<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7065262 - Fast high-accuracy multi-dimensional pattern inspection - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Fast high-accuracy multi-dimensional pattern inspection"><meta name="DC.contributor" content="William Silver" scheme="inventor"><meta name="DC.contributor" content="Aaron Wallack" scheme="inventor"><meta name="DC.contributor" content="Adam Wagman" scheme="inventor"><meta name="DC.contributor" content="Cognex Corporation" scheme="assignee"><meta name="DC.date" content="2003-11-10" scheme="dateSubmitted"><meta name="DC.description" content="A method and apparatus are provided for identifying differences between a stored pattern and a matching image subset, where variations in pattern position, orientation, and size do not give rise to false differences. The invention is also a system for analyzing an object image with respect to a model pattern so as to detect flaws in the object image. The system includes extracting pattern features from the model pattern; generating a vector-valued function using the pattern features to provide a pattern field; extracting image features from the object image; evaluating each image feature, using the pattern field and an n-dimensional transformation that associates image features with pattern features, so as to determine at least one associated feature characteristic; and using at least one feature characteristic to identify at least one flaw in the object image. The invention can find at least two distinct kinds of flaws: missing features, and extra features. The invention provides pattern inspection that is faster and more accurate than any known prior art method by using a stored pattern that represents an ideal example of the object to be found and inspected, and that can be translated, rotated, and scaled to arbitrary precision much faster than digital image re-sampling, and without pixel grid quantization errors. Furthermore, since the invention does not use digital image re-sampling, there are no pixel quantization errors to cause false differences between the pattern and image that can limit inspection performance."><meta name="DC.date" content="2006-6-20" scheme="issued"><meta name="DC.relation" content="US:3069654" scheme="references"><meta name="DC.relation" content="US:3986007" scheme="references"><meta name="DC.relation" content="US:4146924" scheme="references"><meta name="DC.relation" content="US:4581762" scheme="references"><meta name="DC.relation" content="US:4618989" scheme="references"><meta name="DC.relation" content="US:4707647" scheme="references"><meta name="DC.relation" content="US:4972359" scheme="references"><meta name="DC.relation" content="US:5245674" scheme="references"><meta name="DC.relation" content="US:5343390" scheme="references"><meta name="DC.relation" content="US:5351310" scheme="references"><meta name="DC.relation" content="US:5515453" scheme="references"><meta name="DC.relation" content="US:5694482" scheme="references"><meta name="DC.relation" content="US:5703960" scheme="references"><meta name="DC.relation" content="US:5828769" scheme="references"><meta name="citation_reference" content="Akinori Kawamura, Koji Yura, Tatsuya Hayama, Yutaka Hidai, Tadatashi Minamikawa, Akio Tanaka and Shoichi Masuda, On-line Recognition of Freely Handwritten Japanese Characters Using Directional Features Densities, IEEE 1992."><meta name="citation_reference" content="Ballard, D.H., &quot;Generalizing the Hough Transform to Detect Arbitrary Shapes,&quot; Pattern Recognition, 1981, pp. 111-122, vol. 13, No. 2, Pergaman Press Ltd., UK."><meta name="citation_reference" content="Cognex Corporation, &quot;Chapter 7 CONLPAS,&quot; Cognex 3000/4000/5000 Programmable Vision Engines, Vision Tools, 1996, pp. 307-340, Revision 7.4 590-0136, Natick, MA USA."><meta name="citation_reference" content="Cognex Corporation, &quot;Description Sobel Search,&quot; Natick, MA USA, 1998 but public before the above-referenced filing date."><meta name="citation_reference" content="Daniel P. Huttenlocher and William J. Rucklidge, A Multi-Resolution Technique for Comparing Images Using the Hausdorff Distance, Department of Computer Science, Cornell University, Ithaca, NY 14853."><meta name="citation_reference" content="Daniel P. Huttenlocher, Gregory A. Klanderman and William J. Rucklidge, Comparing Images Unsing the Hausdorff Distance, IEEE Transaction on Pattern Analysis and Machine Intelligence, Vo. 15, No. 9, Sep. 1993."><meta name="citation_reference" content="Gunilla Borgefors, Hierarchical Chamfer Matching: A Parametric Edge Matching Algorithm, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 10, No. 6, Nov. 1988."><meta name="citation_reference" content="Hu, et al, &quot;Expanding the Range of Convergence of the CORDIC Algorithm,&quot; IEEE Transactions on computers, Jan. 1991, pp. 13-21, vol. 40, No. 1, USA."><meta name="citation_reference" content="Hu, Yu Hen, &quot;CORDIC-Based VLSI Architectures for Digital Signal Processing,&quot; IEE Signal Processing Magazine, Jul. 1992, pp. 16-35, 1053-5888/92, USA."><meta name="citation_reference" content="I.J. Cox and J.B. Kruskal (AT&amp;T Bell Laboratories, Murray Hill, NJ), On the Congruence of Noisy Images to Line Segment Models, IEEE, 1988."><meta name="citation_reference" content="James D. Foley, Andries Van Dam, Steven K. Feiner, John F. Hughes, Second Edition in C, Introduction to Computer Graphics, pp. 36-49, Addison-Wesley Publishing Company, 1994; USA."><meta name="citation_reference" content="Lin, et al., &quot;On-Line CORDIC Algorithms,&quot; IEEE Transactions on Computers, pp. 1038-1052, vol. 39, No. 8, USA."><meta name="citation_reference" content="Lisa Gottesfeld Brown, A Survey of Image Registration Techniques, Department of Computer Science, Columbia University, New York, NY 10027, ACM Computing Surveys, vol. 24, No. 4, Dec. 1992."><meta name="citation_reference" content="Wallack, Aaron Samuel, &quot;Chapter 4 Robust Algorithms for Object Localization,&quot; Algorithms and Techniques for Manugacturing, 1995, pp. 97-148 (and Bibliography pp. 324-335) PhD thesis, University of California at Berkeley, USA."><meta name="citation_patent_number" content="US:7065262"><meta name="citation_patent_application_number" content="US:10/705,563"><link rel="canonical" href="http://www.google.com/patents/US7065262"/><meta property="og:url" content="http://www.google.com/patents/US7065262"/><meta name="title" content="Patent US7065262 - Fast high-accuracy multi-dimensional pattern inspection"/><meta name="description" content="A method and apparatus are provided for identifying differences between a stored pattern and a matching image subset, where variations in pattern position, orientation, and size do not give rise to false differences. The invention is also a system for analyzing an object image with respect to a model pattern so as to detect flaws in the object image. The system includes extracting pattern features from the model pattern; generating a vector-valued function using the pattern features to provide a pattern field; extracting image features from the object image; evaluating each image feature, using the pattern field and an n-dimensional transformation that associates image features with pattern features, so as to determine at least one associated feature characteristic; and using at least one feature characteristic to identify at least one flaw in the object image. The invention can find at least two distinct kinds of flaws: missing features, and extra features. The invention provides pattern inspection that is faster and more accurate than any known prior art method by using a stored pattern that represents an ideal example of the object to be found and inspected, and that can be translated, rotated, and scaled to arbitrary precision much faster than digital image re-sampling, and without pixel grid quantization errors. Furthermore, since the invention does not use digital image re-sampling, there are no pixel quantization errors to cause false differences between the pattern and image that can limit inspection performance."/><meta property="og:title" content="Patent US7065262 - Fast high-accuracy multi-dimensional pattern inspection"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("20_sU7SJC4OGogTproCAAw"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("CRI"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("20_sU7SJC4OGogTproCAAw"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("CRI"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7065262?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7065262"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=HJF0BAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7065262&amp;usg=AFQjCNGwmjYILC-Drup3ojdWwkcIrdstGQ" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7065262.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7065262.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7065262" style="display:none"><span itemprop="description">A method and apparatus are provided for identifying differences between a stored pattern and a matching image subset, where variations in pattern position, orientation, and size do not give rise to false differences. The invention is also a system for analyzing an object image with respect to a model...</span><span itemprop="url">http://www.google.com/patents/US7065262?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7065262 - Fast high-accuracy multi-dimensional pattern inspection</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7065262 - Fast high-accuracy multi-dimensional pattern inspection" title="Patent US7065262 - Fast high-accuracy multi-dimensional pattern inspection"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7065262 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 10/705,563</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Jun 20, 2006</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Nov 10, 2003</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Nov 26, 1997</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6836567">US6836567</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6975764">US6975764</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6985625">US6985625</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6993192">US6993192</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7006712">US7006712</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7043081">US7043081</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7058225">US7058225</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7088862">US7088862</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7164796">US7164796</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7251366">US7251366</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">10705563, </span><span class="patent-bibdata-value">705563, </span><span class="patent-bibdata-value">US 7065262 B1, </span><span class="patent-bibdata-value">US 7065262B1, </span><span class="patent-bibdata-value">US-B1-7065262, </span><span class="patent-bibdata-value">US7065262 B1, </span><span class="patent-bibdata-value">US7065262B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22William+Silver%22">William Silver</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Aaron+Wallack%22">Aaron Wallack</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Adam+Wagman%22">Adam Wagman</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Cognex+Corporation%22">Cognex Corporation</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7065262.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7065262.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7065262.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (14),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (14),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (3),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (12),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (5)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=HJF0BAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7065262&usg=AFQjCNFUMOSLswy4xt7UGG2tVKfyeSNXNQ">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=HJF0BAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7065262&usg=AFQjCNFBIhrRJowogr1IXVzi01rxKrU2EQ">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=HJF0BAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7065262B1%26KC%3DB1%26FT%3DD&usg=AFQjCNErJ66plCXAGgfetYJWSqOvOYAdZQ">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55594898" lang="EN" load-source="patent-office">Fast high-accuracy multi-dimensional pattern inspection</invention-title></span><br><span class="patent-number">US 7065262 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA50995346" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">A method and apparatus are provided for identifying differences between a stored pattern and a matching image subset, where variations in pattern position, orientation, and size do not give rise to false differences. The invention is also a system for analyzing an object image with respect to a model pattern so as to detect flaws in the object image. The system includes extracting pattern features from the model pattern; generating a vector-valued function using the pattern features to provide a pattern field; extracting image features from the object image; evaluating each image feature, using the pattern field and an n-dimensional transformation that associates image features with pattern features, so as to determine at least one associated feature characteristic; and using at least one feature characteristic to identify at least one flaw in the object image. The invention can find at least two distinct kinds of flaws: missing features, and extra features. The invention provides pattern inspection that is faster and more accurate than any known prior art method by using a stored pattern that represents an ideal example of the object to be found and inspected, and that can be translated, rotated, and scaled to arbitrary precision much faster than digital image re-sampling, and without pixel grid quantization errors. Furthermore, since the invention does not use digital image re-sampling, there are no pixel quantization errors to cause false differences between the pattern and image that can limit inspection performance.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(30)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00018.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00018.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00019.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00019.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00020.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00020.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00021.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00021.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00022.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00022.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00023.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00023.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00024.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00024.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00025.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00025.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00026.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00026.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00027.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00027.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00028.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00028.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-D00029.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7065262B1/US07065262-20060620-D00029.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(55)</span></span></div><div class="patent-text"><div mxw-id="PCLM9034641" lang="EN" load-source="patent-office" class="claims">
    <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
      <div class="claim-text">1. A geometric pattern matching method for refining an estimate of a true pose of an object in a run-time image, the method comprising:
<div class="claim-text">generating a low-resolution model pattern using a training image, the low-resolution model pattern including a geometric description of the expected shape of the object at a low spatial resolution, each geometric description including a list of pattern boundary points;</div>
<div class="claim-text">generating a high-resolution model pattern using the training image, the high-resolution model pattern including a geometric description of the expected shape of the object at a high spatial resolution, each geometric description including a list of pattern boundary points;</div>
<div class="claim-text">receiving a starting pose, the starting pose representing an initial estimate of the true pose of the object in the run-time image;</div>
<div class="claim-text">receiving a run-time image;</div>
<div class="claim-text">using the low-resolution model pattern, and the starting pose, analyzing the run-time image so as to provide a low-resolution pose that is a more refined estimate of the true pose than the starting pose; and</div>
<div class="claim-text">using the high-resolution model pattern, and the low-resolution pose, analyzing the run-time image so as to provide a high-resolution pose that is a more refined estimate of the true pose than the low-resolution pose.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
      <div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein analyzing the run-time image so as to provide a low-resolution pose includes:
<div class="claim-text">producing a low-resolution error value;</div>
<div class="claim-text">producing a low-resolution aggregate clutter value; and</div>
<div class="claim-text">producing a low-resolution aggregate coverage value.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
      <div class="claim-text">3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the low-resolution error value is a low-resolution root-mean-squares error value.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
      <div class="claim-text">4. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, after analyzing the run-time image so as to provide a low-resolution pose, and before analyzing the run-time image so as to provide a high-resolution pose, further comprising:
<div class="claim-text">examining the low-resolution error value, the low-resolution aggregate clutter value, and the low-resolution aggregate coverage value; and</div>
<div class="claim-text">forgoing analyzing the run-time image so as to provide a high-resolution pose if these low-resolution values do not indicate an acceptable match between the run-time image and the low-resolution model pattern.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
      <div class="claim-text">5. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, after analyzing the run-time image so as to provide a low-resolution pose, and instead of analyzing the run-time image so as to provide a high-resolution pose, further comprising:
<div class="claim-text">examining the low-resolution error value, the low-resolution aggregate clutter value, and the low-resolution aggregate coverage value; and</div>
<div class="claim-text">analyzing the run-time image so as to provide a high-resolution pose if these low-resolution values indicate an acceptable match between the run-time image and the low-resolution model pattern.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
      <div class="claim-text">6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:
<div class="claim-text">examining the high-resolution error value, the high-resolution aggregate clutter value, and the high-resolution aggregate coverage value; and</div>
<div class="claim-text">advising a user that the run-time image is out of focus, if these high-resolution values do not indicate an acceptable match between the run-time image and the high-resolution model pattern.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
      <div class="claim-text">7. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further comprising:
<div class="claim-text">examining the high-resolution error value, the high-resolution aggregate clutter value, and the high-resolution aggregate coverage value; and</div>
<div class="claim-text">using the results of analyzing the run-time image so as to provide a low-resolution pose, if these high-resolution values do not indicate an acceptable match between the run-time image and the high-resolution model pattern.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
      <div class="claim-text">8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein analyzing the run-time image so as to provide a high-resolution pose includes:
<div class="claim-text">producing a high-resolution error value;</div>
<div class="claim-text">producing a high-resolution aggregate clutter value; and</div>
<div class="claim-text">producing a high-resolution aggregate coverage value.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
      <div class="claim-text">9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the high-resolution error value is a high-resolution root-mean-squares error value.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
      <div class="claim-text">10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising receiving a coordinate transformation that maps points in an orthonormal coordinate system to points in the run-time image, wherein the coordinate transformation is used in analyzing the run-time image so as to provide a low-resolution pose.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
      <div class="claim-text">11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising receiving a coordinate transformation that maps points in an orthonormal coordinate system to points in the run-time image, wherein the coordinate transformation is used in analyzing the run-time image so as to provide a high-resolution pose.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
      <div class="claim-text">12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating low-resolution model pattern using a training image includes:
<div class="claim-text">low-pass filtering and image sub-sampling so as to attenuate fine detail in the training image, thereby providing a low-resolution model pattern.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
      <div class="claim-text">13. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating high-resolution model pattern using the training image includes:
<div class="claim-text">low-pass filtering and image sub-sampling so as to pass fine detail in the training image, thereby providing a high-resolution model pattern.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
      <div class="claim-text">14. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein analyzing the run-time image so as to provide a high-resolution pose includes:
<div class="claim-text">providing an evaluated pattern boundary point list that identifies boundary points in the high-resolution model pattern that are not present in the run-time image.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
      <div class="claim-text">15. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein analyzing the run-time image so as to provide a high-resolution pose includes:
<div class="claim-text">providing an evaluated image boundary point list that identifies boundary points in the run-time image that are not present in the high-resolution model pattern.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
      <div class="claim-text">16. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>,
<div class="claim-text">wherein analyzing the run-time image so as to provide a low-resolution pose includes: producing a low-resolution aggregate clutter value, and a low-resolution aggregate coverage value; and</div>
<div class="claim-text">wherein analyzing the run-time image so as to provide a high-resolution pose includes: producing a high-resolution aggregate clutter value, and a high-resolution aggregate coverage value;</div>
<div class="claim-text">the method further including:</div>
<div class="claim-text">computing a low-resolution overall match score that is equal to the low-resolution aggregate coverage value minus half the low-resolution aggregate clutter value;</div>
<div class="claim-text">computing a high-resolution overall match score that is equal to the high-resolution aggregate coverage value minus half the high-resolution aggregate clutter value; and</div>
<div class="claim-text">using the results of analyzing the run-time image so as to provide a low-resolution pose, instead of using the results of analyzing the run-time image so as to provide high resolution pose, if the high-resolution overall match score is less than a particular fraction of the low-resolution overall match score.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
      <div class="claim-text">17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the particular fraction is equal to 0.9.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
      <div class="claim-text">18. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating a low-resolution model pattern using a training image includes:
<div class="claim-text">sub-sampling by an equal amount in x and y, the amount being the largest integer that is not greater than sqrt((sqrt(wh)/8)), where w and h are width and height of the training image.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
      <div class="claim-text">19. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein generating a low-resolution model pattern using a training image further includes:
<div class="claim-text">low-pass filtering using a filter size parameter equal to one less than the sub-sampling amount.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
      <div class="claim-text">20. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein generating a low-resolution model pattern using a training image further includes:
<div class="claim-text">multiplying gradient magnitude values by 2.0.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00021" num="00021" class="claim">
      <div class="claim-text">21. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the high-resolution model pattern using the training image includes:
<div class="claim-text">low-pass filtering and image sub-sampling so as to pass the training image unmodified.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00022" num="00022" class="claim">
      <div class="claim-text">22. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the low-resolution model pattern using the training image includes:
<div class="claim-text">setting a default noise threshold for a peak detector used to create the low-resolution model pattern by using a contrast value that is the median gradient magnitude of the pixels in the training image.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00023" num="00023" class="claim">
      <div class="claim-text">23. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the high-resolution model pattern using the training image includes:
<div class="claim-text">setting a default noise threshold for a peak detector used to create the high-resolution model pattern by using a contrast value that is the median gradient magnitude of the pixels in the training image.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00024" num="00024" class="claim">
      <div class="claim-text">24. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the low-resolution model pattern using the training image includes:
<div class="claim-text">setting a noise threshold for a peak detector used to create the low-resolution model pattern by using a contrast value that is equal to 10 gray levels.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00025" num="00025" class="claim">
      <div class="claim-text">25. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the high-resolution model pattern using the training image includes:
<div class="claim-text">setting a noise threshold for a peak detector used to create the high-resolution model pattern by using a contrast value that is one quarter of a saved train-time contrast value.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00026" num="00026" class="claim">
      <div class="claim-text">26. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein analyzing the run-time image so as to provide a low-resolution pose includes:
<div class="claim-text">using a noise threshold for a peak detector used to create the low-resolution model pattern by using a contrast value that is equal to 10 gray levels, multiplied by the ratio of run-time contrast to a saved train-time contrast.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00027" num="00027" class="claim">
      <div class="claim-text">27. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein analyzing the run-time image so as to provide a high-resolution pose includes:
<div class="claim-text">using a noise threshold for a peak detector used to create the high-resolution model pattern by using a contrast value that is one quarter of a saved train-time contrast value, multiplied by the ratio of run-time contrast to a saved train-time contrast.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00028" num="00028" class="claim">
      <div class="claim-text">28. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the starting pose is a coordinate transform that includes non-translational degrees of freedom.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00029" num="00029" class="claim">
      <div class="claim-text">29. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the a high-resolution pose is a coordinate transform that includes non-translational degrees of freedom.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00030" num="00030" class="claim">
      <div class="claim-text">30. A geometric pattern matching method for refining an estimate of a true pose of an object in a run-time image, the method comprising:
<div class="claim-text">storing a plurality of model patterns, each model pattern including a geometric description of the expected shape of the object at a respective spatial resolution, each geometric description including a list of pattern boundary points;</div>
<div class="claim-text">providing a starting pose that represents an initial estimate of the true pose of the object in the run-time image;</div>
<div class="claim-text">using the starting pose and a lowest-resolution model pattern to determine an intermediate estimate of the true pose of the object in the run-time image;</div>
<div class="claim-text">using the intermediate estimate of the true pose from the previous step, and the next-higher resolution stored model pattern, to determine a further refined intermediate estimate of the true pose of the object in the run-time image; and</div>
<div class="claim-text">using a further refined intermediate estimate of the true pose as the final estimate of the true pose of the object in the run-time image.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00031" num="00031" class="claim">
      <div class="claim-text">31. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein using the starting pose and a lowest-resolution model pattern to determine an intermediate pose includes:
<div class="claim-text">producing a intermediate-resolution error value;</div>
<div class="claim-text">producing a intermediate-resolution aggregate clutter value; and</div>
<div class="claim-text">producing a intermediate-resolution aggregate coverage value.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00032" num="00032" class="claim">
      <div class="claim-text">32. The method of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein the intermediate-resolution error value is a intermediate-resolution root-mean-squares error value.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00033" num="00033" class="claim">
      <div class="claim-text">33. The method of <claim-ref idref="CLM-00032">claim 32</claim-ref>, further comprising:
<div class="claim-text">examining the final-resolution error value, the final-resolution aggregate clutter value, and the final-resolution aggregate coverage value; and</div>
<div class="claim-text">using the results of determining an intermediate estimate of the true pose, if these final-resolution values do not indicate an acceptable match between the run-time image and the final-resolution model pattern.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00034" num="00034" class="claim">
      <div class="claim-text">34. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein using a further refined intermediate estimate of the true pose as the final estimate of the true pose includes:
<div class="claim-text">producing a final-resolution error value;</div>
<div class="claim-text">producing a final-resolution aggregate clutter value; and</div>
<div class="claim-text">producing a final-resolution aggregate coverage value.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00035" num="00035" class="claim">
      <div class="claim-text">35. The method of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein the final-resolution error value is a final-resolution root-mean-squares error value.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00036" num="00036" class="claim">
      <div class="claim-text">36. The method of <claim-ref idref="CLM-00034">claim 34</claim-ref>, further comprising:
<div class="claim-text">examining the final-resolution error value, the final-resolution aggregate clutter value, and the final-resolution aggregate coverage value; and</div>
<div class="claim-text">advising a user that the run-time image is out of focus, if these final-resolution values do not indicate an acceptable match between the run-time image and the final-resolution model pattern.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00037" num="00037" class="claim">
      <div class="claim-text">37. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, further comprising receiving a coordinate transformation that maps points in an orthonormal coordinate system to points in the run-time image, wherein the coordinate transformation is used to determine an intermediate estimate of the true pose.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00038" num="00038" class="claim">
      <div class="claim-text">38. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, further comprising receiving a coordinate transformation that maps points in an orthonormal coordinate system to points in the run-time image, wherein the coordinate transformation is used to a further refined intermediate estimate of the true pose.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00039" num="00039" class="claim">
      <div class="claim-text">39. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein storing a plurality of model patterns includes:
<div class="claim-text">low-pass filtering and image sub-sampling a different amount for each model pattern so as to provide a plurality of respective spatial resolutions, and so as to attenuate detail differently in each model pattern of the plurality of model patterns.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00040" num="00040" class="claim">
      <div class="claim-text">40. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein using a further refined intermediate estimate of the true pose as the final estimate of the true pose includes:
<div class="claim-text">providing an evaluated pattern boundary point list that identifies boundary points in the final-resolution model pattern that are not present in the run-time image.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00041" num="00041" class="claim">
      <div class="claim-text">41. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein using a further refined intermediate estimate of the true pose as the final estimate of the true pose includes:
<div class="claim-text">providing an evaluated image boundary point list that identifies boundary points in the run-time image that are not present in the final-resolution model pattern.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00042" num="00042" class="claim">
      <div class="claim-text">42. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>,
<div class="claim-text">wherein using the intermediate estimate of the true pose includes:</div>
<div class="claim-text">producing an intermediate-resolution aggregate clutter value, and an intermediate-resolution aggregate coverage value; and</div>
<div class="claim-text">wherein using a further refined intermediate estimate of the true pose as the final pose includes: producing a final-resolution aggregate clutter value, and a final-resolution aggregate coverage value;</div>
<div class="claim-text">the method further including:</div>
<div class="claim-text">computing a intermediate-resolution overall match score that is equal to the intermediate-resolution aggregate coverage value minus half the intermediate-resolution aggregate clutter value;</div>
<div class="claim-text">computing a final-resolution overall match score that is equal to the final-resolution aggregate coverage value minus half the final-resolution aggregate clutter value; and</div>
<div class="claim-text">using the results of analyzing the run-time image so as to provide a intermediate-resolution pose, instead of using the results of analyzing the run-time image so as to provide final resolution pose, if the final-resolution overall match score is less than a particular fraction of the intermediate-resolution overall match score.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00043" num="00043" class="claim">
      <div class="claim-text">43. The method of <claim-ref idref="CLM-00042">claim 42</claim-ref>, wherein the particular fraction is equal to 0.9.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00044" num="00044" class="claim">
      <div class="claim-text">44. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein generating an intermediate-resolution model pattern using a training image includes:
<div class="claim-text">sub-sampling by an equal amount in x and y, the amount being the largest integer that is not greater than sqrt((sqrt(wh)/8)), where w and h are width and height of the training image.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00045" num="00045" class="claim">
      <div class="claim-text">45. The method of <claim-ref idref="CLM-00044">claim 44</claim-ref>, wherein generating an intermediate-resolution model pattern using a training image further includes:
<div class="claim-text">low-pass filtering using a filter size parameter equal to one less than the sub-sampling amount.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00046" num="00046" class="claim">
      <div class="claim-text">46. The method of <claim-ref idref="CLM-00044">claim 44</claim-ref>, wherein generating an intermediate-resolution model pattern using a training image further includes:
<div class="claim-text">multiplying gradient magnitude values by 2.0.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00047" num="00047" class="claim">
      <div class="claim-text">47. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein generating the final-resolution model pattern using the training image includes:
<div class="claim-text">low-pass filtering and image sub-sampling so as to pass the training image unmodified.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00048" num="00048" class="claim">
      <div class="claim-text">48. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein generating an intermediate-resolution model pattern using the training image includes:
<div class="claim-text">setting a default noise threshold for a peak detector used to create the intermediate-resolution model pattern by using a contrast value that is the median gradient magnitude of the pixels in the training image.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00049" num="00049" class="claim">
      <div class="claim-text">49. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein generating the final-resolution model pattern using the training image includes:
<div class="claim-text">setting a default noise threshold for a peak detector used to create the final-resolution model pattern by using a contrast value that is the median gradient magnitude of the pixels in the training image.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00050" num="00050" class="claim">
      <div class="claim-text">50. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein generating the intermediate-resolution model pattern using the training image includes:
<div class="claim-text">setting a noise threshold for a peak detector used to create the intermediate-resolution model pattern by using a contrast value that is equal to 10 gray levels.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00051" num="00051" class="claim">
      <div class="claim-text">51. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein generating the final-resolution model pattern using the training image includes:
<div class="claim-text">setting a noise threshold for a peak detector used to create the final-resolution model pattern by using a contrast value that is one quarter of a saved train-time contrast value.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00052" num="00052" class="claim">
      <div class="claim-text">52. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein determining a further refined intermediate estimate of the true pose includes:
<div class="claim-text">using a noise threshold for a peak detector used to create the low-resolution model pattern by using a contrast value that is equal to 10 gray levels, multiplied by the ratio of run-time contrast to a saved train-time contrast.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00053" num="00053" class="claim">
      <div class="claim-text">53. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein determining a further refined intermediate estimate of the true pose includes:
<div class="claim-text">using a noise threshold for a peak detector used to create the high-resolution model pattern by using a contrast value that is one quarter of a saved train-time contrast value, multiplied by the ratio of run-time contrast to a saved train-time contrast.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00054" num="00054" class="claim">
      <div class="claim-text">54. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the starting pose is a coordinate transform that includes non-translational degrees of freedom.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00055" num="00055" class="claim">
      <div class="claim-text">55. The method of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the a final-resolution pose is a coordinate transform that includes non-translational degrees of freedom.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES16057185" lang="EN" load-source="patent-office" class="description">
    <heading>CROSS REFERENCE TO RELATED APPLICATIONS</heading> <p num="p-0002">This is a continuation of U.S. patent application Ser. No. 09/746,147 filed Dec. 22, 2000 which is now U.S. Pat. No. 6,658,145 issued on Dec. 2, 2003, which is a continuation of U.S. patent application Ser. No. 09/001,869, filed Dec. 31, 1997 (now abandoned), which is a continuation-in-part to U.S. patent application Ser. No. 08/979,588, filed Nov. 26, 1997 (now abandoned).</p>
    <heading>FIELD OF THE INVENTION</heading> <p num="p-0003">This invention relates to machine vision, and particularly to systems for pattern inspection in an image.</p>
    <heading>BACKGROUND OF THE INVENTION</heading> <p num="p-0004">Digital images are formed by many devices and used for many practical purposes. Devices include TV cameras operating on visible or infrared light, line-scan sensors, flying spot scanners, electron microscopes, X-ray devices including CT scanners, magnetic resonance imagers, and other devices known to those skilled in the art. Practical applications are found in industrial automation, medical diagnosis, satellite imaging for a variety of military, civilian, and scientific purposes, photographic processing, surveillance and traffic monitoring, document processing, and many others.</p>
    <p num="p-0005">To serve these applications the images formed by the various devices are analyzed by digital devices to extract appropriate information. One form of analysis that is of considerable practical importance is determining the position, orientation, and size of patterns in an image that correspond to objects in the field of view of the imaging device. Pattern location methods are of particular importance in industrial automation, where they are used to guide robots and other automation equipment in semiconductor manufacturing, electronics assembly, pharmaceuticals, food processing, consumer goods manufacturing, and many others.</p>
    <p num="p-0006">Another form of digital image analysis of practical importance is identifying differences between an image of an object and a stored pattern that represents the ideal appearance of the object. Methods for identifying these differences are generally referred to as pattern inspection methods, and are used in industrial automation for assembly, packaging, quality control, and many other purposes.</p>
    <p num="p-0007">One early, widely-used method for pattern location and inspection is known as blob analysis. In this method, the pixels of a digital image are classified as object or background by some means, typically by comparing pixel gray-levels to a threshold. Pixels classified as object are grouped into blobs using the rule that two object pixels are part of the same blob if they are neighbors; this is known as connectivity analysis. For each such blob we determine properties such as area, perimeter, center of mass, principal moments of inertia, and principal axes of inertia. The position, orientation, and size of a blob is taken to be its center of mass, angle of first principal axis of inertia, and area, respectively. These and the other blob properties can be compared against a known ideal for proposes of inspection.</p>
    <p num="p-0008">Blob analysis is relatively inexpensive to compute, allowing for fast operation on inexpensive hardware. It is reasonably accurate under ideal conditions, and well-suited to objects whose orientation and size are subject to change. One limitation is that accuracy can be severely degraded if some of the object is missing or occluded, or if unexpected extra features are present. Another limitation is that the values available for inspection purposes represent coarse features of the object, and cannot be used to detect fine variations. The most severe limitation, however, is that except under limited and well-controlled conditions there is in general no reliable method for classifying pixels as object or background. These limitations forced developers to seek other methods for pattern location and inspection.</p>
    <p num="p-0009">Another method that achieved early widespread use is binary template matching. In this method, a training image is used that contains an example of the pattern to be located. The subset of the training image containing the example is thresholded to produce a binary pattern and then stored in a memory. At run-time, images are presented that contain the object to be found. The stored pattern is compared with like-sized subsets of the run-time image at all or selected positions, and the position that best matches the stored pattern is considered the position of the object. Degree of match at a given position of the pattern is simply the fraction of pattern pixels that match their corresponding image pixel, thereby providing pattern inspection information.</p>
    <p num="p-0010">Binary template matching does not depend on classifying image pixels as object or background, and so it can be applied to a much wider variety of problems than blob analysis. It also is much better able to tolerate missing or extra pattern features without severe loss of accuracy, and it is able to detect finer differences between the pattern and the object. One limitation, however, is that a binarization threshold is needed, which can be difficult to choose reliably in practice, particularly under conditions of poor signal-to-noise ratio or when illumination intensity or object contrast is subject to variation. Accuracy is typically limited to about one whole pixel due to the substantial loss of information associated with thresholding. Even more serious, however, is that binary template matching cannot measure object orientation and size. Furthermore, accuracy degrades rapidly with small variations in orientation and/or size, and if larger variations are expected the method cannot be used at all.</p>
    <p num="p-0011">A significant improvement over binary template matching came with the advent of relatively inexpensive methods for the use of gray-level normalized correlation for pattern location and inspection. The methods are similar, except that no threshold is used so that the full range of image gray-levels are considered, and the degree of match becomes the correlation coefficient between the stored pattern and the image subset at a given position.</p>
    <p num="p-0012">Since no binarization threshold is needed, and given the fundamental noise immunity of correlation, performance is not significantly compromised under conditions of poor signal-to-noise ratio or when illumination intensity or object contrast is subject to variation. Furthermore, since there is no loss of information due to thresholding, position accuracy down to about  pixel is practical using well-known interpolation methods. The situation regarding orientation and size, however, is not much improved with respect to binary template matching. Another limitation is that in some applications, contrast can vary locally across an image of an object, resulting in poor correlation with the stored pattern, and consequent failure to correctly locate it.</p>
    <p num="p-0013">More recently, improvements to gray-level correlation have been developed that allow it to be used in applications where significant variation in orientation and/or size is expected. In these methods, the stored pattern is rotated and/or scaled by digital image re-sampling methods before being matched against the image. By matching over a range of angles, sizes, and x-y positions, one can locate an object in the corresponding multidimensional space. Note that such methods would not work well with binary template matching, due the much more severe pixel quantization errors associated with binary images.</p>
    <p num="p-0014">One problem with these methods is the severe computational cost, both of digital re-sampling and of searching a space with more than 2 dimensions. To manage this cost, the search methods break up the problem into two or more phases. The earliest phase uses a coarse, subsampled version of the pattern to cover the entire search space quickly and identify possible object locations in the n-dimensional space. Subsequent phases use finer versions of the pattern to refine the locations determined at earlier phases, and eliminate locations that the finer resolution reveals are not well correlated with the pattern. Note that variations of these coarse-fine methods have also been used with binary template matching and the original 2-dimensional correlation, but are even more important with the higher-dimensional search space.</p>
    <p num="p-0015">The location accuracy of these methods is limited both by how finely the multidimensional space is searched, and by the ability of the discrete pixel grid to represent small changes in position, orientation, and scale. The fineness of the search can be chosen to suit a given application, but computational cost grows so rapidly with resolution and number of dimensions that practical applications often cannot tolerate the cost or time needed to achieve high accuracy. The limitations of the discrete pixel grid are more fundamentalno matter how finely the space is searched, for typical patterns one cannot expect position accuracy to be much better than about  pixel, orientation better than a degree or so, and scale better than a percent or so.</p>
    <p num="p-0016">A similar situation holds when gray-level pixel-grid-based methods are used for pattern inspection. Once the object has been located in the multidimensional space, pixels in the pattern can be compared to each corresponding pixel in the image to identify differences. Some differences, however, will result from the re-sampling process itself, because again the pixel grid cannot accurately represent small variations in orientation and scale. These differences are particularly severe in regions where image gray levels are changing rapidly, such as along object boundaries. Often these are the most important regions of an object to inspect. Since in general, differences due to re-sampling cannot be distinguished from those due to object defects, inspection performance is compromised.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p num="p-0017">In one general aspect, the invention is a method and apparatus for identifying differences between a stored pattern and a matching image subset, where variations in pattern position, orientation, and size do not give rise to false differences. The process of identifying differences is called inspection. Generally, an object image must be precisely located prior to inspection. In another general aspect, the invention is a system for analyzing an object image with respect to a model pattern, wherein the system includes extracting pattern features from the model pattern; generating a vector-valued function using the pattern features to provide a pattern field; extracting image features from the object image; evaluating each image feature, using the pattern field and an n-dimensional transformation that associates image features with pattern features, so as to determine at least one associated feature characteristic; and using at least one feature characteristic to identify at least one flaw in the object image. In a preferred embodiment, at least one associated feature characteristic includes a probability value that indicates the likelihood that an associated image feature does not correspond to a feature in the model pattern. In an alternate preferred embodiment, the at least one associated feature characteristic includes a probability value that indicates the likelihood that an associated image feature does correspond to a feature in the model pattern.</p>
    <p num="p-0018">In another preferred embodiment, at least one pattern feature includes a probability value indicating the likelihood that the pattern feature does not correspond to at least one feature in the object image.</p>
    <p num="p-0019">When using at least one feature characteristic, it is preferred to transfer a feature characteristic from the at least one image feature to an element of the pattern field, where in a preferred embodiment, the element of the pattern field is the nearest element of the pattern field.</p>
    <p num="p-0020">When using at least one feature characteristic, it is also preferred to use a plurality of image features: and transfer a plurality of the feature characteristics from the plurality of image features to a plurality of elements of the pattern field, wherein some of the plurality of elements of the pattern field can include at least one link to a neighboring element of the pattern field. Further, after transferring a plurality of the feature characteristics from the plurality of image features to a plurality of elements of the pattern field, it is preferred that each element of the plurality of elements of the pattern field receive a feature characteristic equal to the maximum of its own feature characteristic and the feature characteristic of each neighboring element of the pattern field.</p>
    <p num="p-0021">In another preferred embodiment, to use at least one feature characteristic includes identifying the nearest element of the pattern field; transferring a feature characteristic from the at least one image feature to the nearest element of the pattern field; and computing a coverage value using at least the transferred feature characteristic.</p>
    <p num="p-0022">In a further preferred embodiment, evaluating each image feature includes comparing the direction of each image feature with the direction of an element of the pattern field. It is preferable to assign a higher weight to the image feature if the difference in the direction of the image feature from the direction of an element of the pattern field is less than a specified direction parameter. The specified direction parameter can be determined by a characteristic of the element of the pattern field, such as a flag indicating corner or non-corner.</p>
    <p num="p-0023">It is also possible for a lower weight to be assigned to the image feature if the difference in the direction of the image feature from the direction of an element of the pattern field is greater than a specified direction parameter, wherein the specified direction parameter can be determined by a characteristic of the element of the pattern field, such as a flag indicating corner or non-corner.</p>
    <p num="p-0024">In yet another preferred embodiment, to evaluate each image feature includes comparing, modulo 180 degrees, the direction of each image feature with the direction of an element of the pattern field.</p>
    <p num="p-0025">It is also possible that to evaluate each image feature includes assigning a weight of zero when the image feature is at a position that corresponds to an element of the pattern field that specified that no image feature is expected at that position.</p>
    <p num="p-0026">Moreover, to evaluate each image feature can include comparing the distance of each image feature with a specified distance parameter, where a lower weight can be assigned to the image feature if the distance of the image feature is greater than a specified distance parameter, or alternatively, where a higher weight can be assigned to the image feature if the distance of the image feature is less than a specified distance parameter.</p>
    <p num="p-0027">In another preferred embodiment, to evaluate each image feature includes comparing the direction of each image feature with the direction of an element of the pattern field, and comparing the distance of each image feature with a specified distance parameter.</p>
    <p num="p-0028">To avoid ambiguity we will call the location of a pattern in a multidimensional space its pose. More precisely, a pose is a coordinate transform that maps points in an image to corresponding points in a stored pattern. In a preferred embodiment, a pose is a general six degree of freedom linear coordinate transform. The six degrees of freedom can be represented by the four elements of a 22 matrix, plus the two elements of a vector corresponding to the two translational degrees of freedom. Alternatively and equivalently, the four non-translational degrees of freedom can be represented in other ways, such as orientation, scale, aspect ratio, and skew, or x-scale, y-scale, x-axis-angle, and y-axis-angle.</p>
    <p num="p-0029">The invention can serve as a replacement for the fine resolution phase of any coarse-fine method for pattern inspection, such as the prior art method of correlation search followed by Golden Template Analysis. In combination with the coarse location phases of any such method, the invention results in an overall method for pattern inspection that is faster and more accurate than any known prior art method.</p>
    <p num="p-0030">In a preferred embodiment, the PatQuick tool, sold by Cognex Corporation, Natick MA, is used for producing an approximate object pose.</p>
    <p num="p-0031">The invention uses a stored pattern that represents an ideal example of the object to be found and inspected. The pattern can be created from a training image or synthesized from a geometric description. According to the invention, patterns and images are represented by a feature-based description that can be translated, rotated, and scaled to arbitrary precision much faster than digital image re-sampling, and without pixel grid quantization errors. Thus accuracy is not limited by the ability of a grid to represent small changes in position, orientation, or size (or other degrees of freedom). Furthermore, pixel quantization errors due to digital re-sampling will not cause false differences between the pattern and image that can limit inspection performance, since no re-sampling is done.</p>
    <p num="p-0032">Accuracy is also not limited by the fineness with which the space is searched, because the invention does not test discrete positions within the space to determine the pose with the highest degree of match. Instead the invention determines an accurate object pose from an approximate starting pose in a small, fixed number of increments that is independent of the number of dimensions of the space (i.e. degrees of freedom) and independent of the distance between the starting and final poses, as long as the starting pose is within some capture range of the true pose. Thus one does not need to sacrifice accuracy in order to keep execution time within the bounds allowed by practical applications.</p>
    <p num="p-0033">Unlike prior art methods where execution time grows rapidly with number of degrees of freedom, with the method of the invention execution time grows at worst very slowly, and in some embodiments not at all. Thus one need not sacrifice degree of freedom measurements in order to keep execution time within practical bounds. Furthermore, allowing four or more degrees of freedom to be refined will often result in better matches between the pattern and image, and thereby improved accuracy.</p>
    <p num="p-0034">The invention processes images with a feature detector to generate a description that is not tied to a pixel grid. The description is a list of elements called dipoles (also called features) that represent points (positions) along object boundaries. A dipole includes the coordinates of the position of a point along an object boundary and a direction pointing substantially normal to the boundary at that point. Object boundaries are defined as places where image gradient (a vector describing rate and direction of gray-level change at each point in an image) reaches a local maximum. In a preferred embodiment, gradient is estimated at an adjustable spatial resolution. In another preferred embodiment, the dipole direction is the gradient direction. In another preferred embodiment, a dipole, i.e., a feature, contains additional information as further described in the drawings. In yet another preferred embodiment, dipoles are generated not from an image but from a geometric description of an object, such as might be found in a CAD system.</p>
    <p num="p-0035">The stored model pattern to be used by the invention for localization and subsequent inspection is the basis for generating a dipole list that describes the objects to be found by representing object boundaries. The dipole list derived from the model pattern is called the field dipole list. It can be generated from a model training image containing an example object using a feature detector, or lt can be synthesized from a geometric description. The field dipole list is used to generate a 2-dimensional vector-valued function called a field. For each point within the region of the stored model pattern, the field gives a vector that indicates the distance and direction to the nearest point along a model object boundary. The vector is called the force at the specified point within the stored model pattern.</p>
    <p num="p-0036">Note that the nearest point along a model object boundary is not necessarily one of the model object boundary points represented by the field dipoles, but in general may lie between field dipole positions. Note further that the point within the stored model pattern is not necessarily an integer grid position, but is in general a real-valued position, known to within the limits of precision of the apparatus used to perform the calculations. Note that since the force vector points to the nearest boundary point, it must be normal to the boundary (except at discontinuities).</p>
    <p num="p-0037">In a preferred embodiment, if no model object boundary point lies within a certain range of a field position, then a special code is given instead of a force vector. In another preferred embodiment, the identity of the nearest field dipole is given in addition to the force. In another preferred embodiment, one additional bit of information is given that indicates whether the gradient direction at the boundary pointed to by the force is the same or 180 opposite from the force direction (both are normal to the boundary). In another preferred embodiment, additional information is given as further described in the drawings. In another embodiment, the field takes a direction in addition to a position within the pattern, and the force returned is the distance and direction to the nearest model object boundary point in approximately the given direction.</p>
    <p num="p-0038">The stored model pattern used by the invention includes the field dipole list, the field, and a set of operating parameters as appropriate to a given embodiment, and further described throughout the specification.</p>
    <p num="p-0039">Given an object image and an approximate starting pose, pattern localization proceeds as follows. The object image is processed by a feature detector to produce a dipole list, called the image dipole list. The starting pose is refined in a sequence of incremental improvements called attraction steps. Each such step results in a significantly more accurate pose in all of the degrees of freedom that are allowed to vary. The sequence can be terminated after a fixed number of steps, and/or when no significant change in pose results from the last step, or based on any reasonable criteria. In a preferred embodiment, the sequence is terminated after four steps.</p>
    <p num="p-0040">For each attraction step, the image dipoles are processed in any convenient order. The position and direction of each image dipole is mapped by the current pose transformation to convert image coordinates to model pattern (field) coordinates. The field is used to determine the force at the point to which the image dipole was mapped. Since each image dipole is presumed to be located on an object boundary, and the force gives the distance and direction to the nearest model object boundary of the stored model pattern, the existence of the image dipole at the mapped position is taken as evidence that the pose should be modified so that the image dipole moves in the force direction by an amount equal to the force distance.</p>
    <p num="p-0041">It is important to note that object boundaries generally provide position information in a direction normal to the boundary, which as noted above is the force direction, but no information in a direction along the boundary. Thus the evidence provided by an image dipole constrains a single degree of freedom only, specifically position along the line of force, and provides no evidence in the direction normal to the force.</p>
    <p num="p-0042">If the current pose is a fair approximation to the true object position, then many image dipoles will provide good evidence as to how the pose should be modified to bring the image boundaries into maximum agreement with the boundaries of the stored model pattern. For a variety of reasons, however, many other image dipoles may provide false or misleading evidence. Thus, it is important to evaluate the evidence provided by each image dipole, and assign a weighting factor to each image dipole to indicate the relative reliability of the evidence.</p>
    <p num="p-0043">In one embodiment, the direction (as mapped to the pattern coordinate system) of an image dipole is compared with the force direction, and the result, modulo <b>1800</b>, is used to determine the weight of the image dipole. If the directions agree to within some specified parameter, the dipole is given a high weight; if they disagree beyond some other specified parameter, the dipole is given zero weight; if the direction difference falls between the two parameters, intermediate weights are assigned.</p>
    <p num="p-0044">In another embodiment, the image dipole direction is compared to the gradient direction of the model pattern boundary to which the force points. A parameter is used to choose between making the comparison modulo 180, in which case gradient polarity is effectively ignored, or making it modulo 360, in which case gradient polarity is considered. In a preferred embodiment, the field itself indicates at each point within the stored model pattern whether to ignore polarity, consider polarity, or defer the decision to a global parameter.</p>
    <p num="p-0045">In one embodiment, the force distance is used to determine the dipole weight. In a preferred embodiment, if the force distance is larger than some specified parameter, the dipole is given zero weight, on the assumption that the dipole is too far away to represent valid evidence. If the force distance is smaller than some other specified parameter, the dipole is given a high weight, and if it falls between the two parameters, intermediate weights are assigned.</p>
    <p num="p-0046">In a preferred embodiment, the parameters specifying the weight factor as a function of force distance are adjusted for each attraction step to account for the fact that the pose is becoming more accurate, and therefore that one should expect image dipoles representing valid evidence to be closer to the pattern boundaries.</p>
    <p num="p-0047">In one embodiment, the gradient magnitude of the image dipoles is used to determine the dipole weight. In a preferred embodiment, a combination of dipole direction, force distance, and gradient magnitude is used to determine the weight.</p>
    <p num="p-0048">For each attraction step, the invention determines a new pose that best accounts for the evidence contributed by each image dipole, and taking into account the dipole's weight. In a preferred embodiment, a least-squares method is used to determine the new pose.</p>
    <p num="p-0049">The evaluation of each image dipole to produce a weight can also provide information for inspection purposes. It is desirable to look for two distinct kinds of errors: missing features, which are pattern features for which no corresponding image feature can be found, and extra features, sometimes called clutter, which are image features that correspond to no pattern feature. In one embodiment, image dipoles with low weights are considered to be clutter. In a preferred embodiment, a specific clutter value is computed for each image dipole, as further described in the drawings below.</p>
    <p num="p-0050">In an embodiment of the invention that can identify missing pattern features, the field at each point gives identity of the nearest field dipole, if any, in addition to the force vector. Each field dipole contains an evaluation, which is initialized to zero. Each image dipole transfers its evaluation (also called weight or feature characteristic) to that of the nearest field dipole as indicated by the field. Since in general the correspondence between image and field dipoles is not one-to-one, some field dipoles may receive evaluations (feature characteristics) from more than one image dipole, and others may receive evaluations from none. Those field dipoles that receive no evaluation may represent truly missing features, or may simply represent gaps in the transfer due to quantization effects.</p>
    <p num="p-0051">When more than one evaluation is transferred to a given field dipole, the evaluations can be combined by any reasonable means. In a preferred embodiment, the largest such evaluation is used and the others are discarded. Gaps in the transfer can be closed by considering neighboring field dipoles. In one embodiment, methods known in the art as gray-level mathematical morphology are used to close the gaps. In the case of the invention, one-dimensional versions of morphological operations are used, since field dipoles lie along one-dimensional boundaries. In a preferred embodiment, a morphological dilation operation is used.</p>
    <p num="p-0052">If the starting pose is too far away from the true pose, there may be insufficient good evidence from the image dipoles to move the pose in the right direction. The set of starting poses that result in attraction to the true pose defines the capture range of the pattern. The capture range depends on the specific pattern in use, and determines the accuracy needed from whatever method is used to determine the starting pose.</p>
    <p num="p-0053">In a preferred embodiment, the feature detector that is used to generate dipoles is tunable over a wide range of spatial frequencies. If the feature detector is set to detect very fine features at a relatively high resolution, the accuracy will be high but the capture range will be relatively small. If on the other hand the feature detector is set to detect coarse features at a relatively lower resolution, the accuracy will be lower but the capture range will be relatively large. This suggests a multi-resolution method where a coarse, low resolution step is followed by a fine, high resolution step. With this method, the capture range is determined by the coarse step and is relatively large, while the accuracy is determined by the fine step and is high.</p>
    <description-of-drawings> <heading>BRIEF DESCRIPTION OF THE DRAWING</heading> <p num="p-0054">The invention will be more fully understood from the following detailed description, in conjunction with the following figures, wherein:</p>
      <p num="p-0055"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a high-level block diagram of an embodiment of the invention;</p>
      <p num="p-0056"> <figref idrefs="DRAWINGS">FIG. 1A</figref> is an illustration of a pixel array having a 2:1 aspect ratio;</p>
      <p num="p-0057"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a block diagram of the training block of <figref idrefs="DRAWINGS">FIG. 1</figref>;</p>
      <p num="p-0058"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a block diagram of the feature detection block of <figref idrefs="DRAWINGS">FIG. 2</figref>;</p>
      <p num="p-0059"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a diagram showing bit assignments of a 32-bit word, and optional nearest dipole bits;</p>
      <p num="p-0060"> <figref idrefs="DRAWINGS">FIG. 5</figref> is an illustration of a field element array, including a border of field elements:</p>
      <p num="p-0061"> <figref idrefs="DRAWINGS">FIG. 6</figref> illustrates field seeding, showing some of the field elements of <figref idrefs="DRAWINGS">FIG. 5</figref>, including a plurality of straight line segments of a pattern boundary, and the associated field dipoles;</p>
      <p num="p-0062"> <figref idrefs="DRAWINGS">FIG. 7A</figref> illustrates field dipole connecting, showing some of the field elements of <figref idrefs="DRAWINGS">FIG. 6</figref>, including a plurality of straight line segments of a pattern boundary, and a plurality of associated right and left links;</p>
      <p num="p-0063"> <figref idrefs="DRAWINGS">FIGS. 7B and 7C</figref> are diagrams illustrating the order in which neighboring field elements are examined;</p>
      <p num="p-0064"> <figref idrefs="DRAWINGS">FIG. 8</figref> illustrates chain segmentation of <figref idrefs="DRAWINGS">FIG. 2</figref>, showing some of the field elements of <figref idrefs="DRAWINGS">FIG. 5</figref>, including a plurality of straight line segments, and a plurality of left and right links;</p>
      <p num="p-0065"> <figref idrefs="DRAWINGS">FIGS. 9A</figref>, <b>9</b>B, and <b>9</b>C illustrate part of the analysis that is performed by the propagate phase of the field generation module of <figref idrefs="DRAWINGS">FIG. 2</figref>;</p>
      <p num="p-0066"> <figref idrefs="DRAWINGS">FIG. 10</figref> shows further details of the propagate phase of the field generation module of <figref idrefs="DRAWINGS">FIG. 2</figref>;</p>
      <p num="p-0067"> <figref idrefs="DRAWINGS">FIG. 11</figref> shows the same portion of the field array that was shown after seeding in <figref idrefs="DRAWINGS">FIG. 6</figref>, but with new force vectors resulting from one propagation;</p>
      <p num="p-0068"> <figref idrefs="DRAWINGS">FIG. 12</figref> shows the same portion of the field array as <figref idrefs="DRAWINGS">FIG. 11</figref>, after two propagations;</p>
      <p num="p-0069"> <figref idrefs="DRAWINGS">FIG. 13</figref> is a block diagram of the run-time module of the preferred embodiment of <figref idrefs="DRAWINGS">FIG. 1</figref>;</p>
      <p num="p-0070"> <figref idrefs="DRAWINGS">FIG. 14</figref> is a diagram illustrating a least squares method for determining a pose that best accounts for the evidence of the image dipoles at each attraction step;</p>
      <p num="p-0071"> <figref idrefs="DRAWINGS">FIG. 15</figref> is a block diagram of the attraction module of <figref idrefs="DRAWINGS">FIG. 13</figref>;</p>
      <p num="p-0072"> <figref idrefs="DRAWINGS">FIG. 16</figref> is a block diagram of the map module of <figref idrefs="DRAWINGS">FIG. 15</figref>;</p>
      <p num="p-0073"> <figref idrefs="DRAWINGS">FIG. 17</figref> is a block diagram of the field module of <figref idrefs="DRAWINGS">FIG. 15</figref>;</p>
      <p num="p-0074"> <figref idrefs="DRAWINGS">FIG. 18</figref> is a block diagram of the rotate module of <figref idrefs="DRAWINGS">FIG. 15</figref>;</p>
      <p num="p-0075"> <figref idrefs="DRAWINGS">FIGS. 19A</figref>, <b>19</b>B, and <b>19</b>C show output as a function of input for three fuzzy logic processing elements;</p>
      <p num="p-0076"> <figref idrefs="DRAWINGS">FIG. 20A</figref> is a schematic diagram of the portion of the evaluate module of <figref idrefs="DRAWINGS">FIG. 15</figref>, showing a preferred system for calculating weight and eval;</p>
      <p num="p-0077"> <figref idrefs="DRAWINGS">FIG. 20B</figref> is a schematic diagram of the portion of the evaluate module of <figref idrefs="DRAWINGS">FIG. 15</figref>, showing a preferred system for calculating clutter;</p>
      <p num="p-0078"> <figref idrefs="DRAWINGS">FIG. 21</figref> is a schematic diagram of the sum module of <figref idrefs="DRAWINGS">FIG. 15</figref>;</p>
      <p num="p-0079"> <figref idrefs="DRAWINGS">FIGS. 22AD</figref> are block diagram of the solve module of <figref idrefs="DRAWINGS">FIG. 15</figref>, showing the equations and inputs for providing motion and rms error for various degrees of freedom</p>
      <p num="p-0080"> <figref idrefs="DRAWINGS">FIG. 23</figref> is a block diagram of the equations of the compose module of <figref idrefs="DRAWINGS">FIG. 15</figref>;</p>
      <p num="p-0081"> <figref idrefs="DRAWINGS">FIG. 24</figref> is a block diagram of the equations of the Normal Tensor module of <figref idrefs="DRAWINGS">FIG. 15</figref>;</p>
      <p num="p-0082"> <figref idrefs="DRAWINGS">FIG. 25</figref> is a graphical illustration of a plurality of image dipoles and a plurality of connected field dipoles, showing field dipole evaluation;</p>
      <p num="p-0083"> <figref idrefs="DRAWINGS">FIG. 26</figref> is a high-level block diagram of a multi-resolution embodiment of the invention;</p>
      <p num="p-0084"> <figref idrefs="DRAWINGS">FIGS. 27A and 27B</figref> are flow diagrams illustrating the sequence of operations performed by the modules of <figref idrefs="DRAWINGS">FIG. 15</figref>; and</p>
      <p num="p-0085"> <figref idrefs="DRAWINGS">FIG. 28</figref> is a flow diagram illustrating a multi-resolution mode of operation of the invention.</p>
    </description-of-drawings> <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading> <p num="p-0086">In the following figures, modules can be implemented as software, firmware, or hardware. Moreover, each module may include sub-modules, or steps, each of which can be implemented as either hardware, software, or some combination thereof. <figref idrefs="DRAWINGS">FIG. 1</figref> is a high-level block diagram of one embodiment of the invention. A training (model) image <b>100</b> containing an example of a pattern <b>105</b> to be used for localization and/or inspection is presented. A training module <b>110</b> analyzes the training image and produces a stored model pattern <b>120</b> for subsequent use. At least one run-time image <b>130</b> is presented, each such image containing zero or more instances of patterns <b>135</b> similar in shape, but possibly different in size and orientation, to the training (model) pattern <b>105</b>.</p>
    <p num="p-0087">Each run-time image <b>130</b> has an associated client map <b>131</b>, chosen by a user for a particular application. A client map is a coordinate transformation that maps, i.e., associates points in an orthonormal but otherwise arbitrary coordinate system to points in the image. An orthonormal coordinate system has perpendicular axes, each axis having a unit scale. The client map provides an orthonormal reference that is necessary to properly handle the orientation degree of freedom, as well as the skew, scale, and aspect ratio degrees of freedom. In practical applications, the images themselves are almost never orthonormal, since practical image sensors almost never have perfectly square pixels. For example, pixels having an aspect ration of 2:1 are possible, as shown in <figref idrefs="DRAWINGS">FIG. 1A</figref>. In this case, the client map would be a 22 matrix:</p>
    <p num="p-0088">
      <maths id="MATH-US-00001" num="00001"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mn>0.5</mn> </mtd> <mtd> <mn>0.0</mn> </mtd> </mtr> <mtr> <mtd> <mn>0.0</mn> </mtd> <mtd> <mn>1.0</mn> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0089">If the pixels are square, the client map is the identity transform, i.e., each diagonal entry in the transform matrix is 1.0, and each off-diagonal element is 0.0.</p>
    <p num="p-0090">Furthermore, it is sometimes useful to have a significantly non-orthonormal field. For example, a field generated from a square pattern can be used to localize and inspect a rectangular or even parallelogram-shaped instance of the pattern by using an appropriate starting pose. These cases can only be handled if an orthonormal reference is available.</p>
    <p num="p-0091">For each run-time image, a starting pose <b>132</b> is determined by any suitable method, such as coarse gray-level correlation with orientation and size re-sampling, a coarse generalized Hough transform, or the Cognex PatQuick tool. The starting pose <b>132</b> is a six-degree-of-freedom coordinate transformation that maps points in the pattern <b>10</b>, to approximately corresponding points <b>135</b> in the run-time image <b>130</b>. A run-time module <b>140</b> analyzes the image <b>130</b>, using the stored pattern <b>120</b>, the starting pose <b>132</b>, and the client map <b>131</b>. As a result of the analysis, the run-time module <b>140</b> produces a pose <b>134</b> that maps pattern points <b>105</b> to accurately corresponding image points <b>135</b>.</p>
    <p num="p-0092">The run-time module <b>140</b> produces an rms error value <b>136</b> that is a measure of the degree of match between the pattern <b>105</b> and the image <b>130</b>. The rms error value <b>136</b> is the root-mean-square error from the least squares solution, or other error minimization solution, that is used to determine a pose that best fits the evidence of the image dipoles, to be described in more detail below. A value of zero represents a perfect fit, while higher values represent poorer fits.</p>
    <p num="p-0093">The run-time module <b>140</b> produces a coverage value <b>138</b> that is a measure of the fraction of the pattern <b>105</b> to which corresponding image features have been found. The coverage value <b>138</b> is computed by summing the field dipole evaluations and dividing by the number of field dipoles, to be described in more detail below.</p>
    <p num="p-0094">The run-time module <b>140</b> produces a clutter value that is a measure of extra features found in the image that do not correspond to pattern features. In a preferred embodiment, the clutter value is computed by summing the individual image dipole clutter values and dividing by the number of field dipoles.</p>
    <p num="p-0095">The run-time module <b>140</b> produces an evaluated image dipole list <b>150</b> and an evaluated field dipole list <b>160</b>. The evaluated image dipole list <b>150</b> identifies features in the image <b>130</b> not present in the pattern <b>105</b>, and the evaluated field dipole list <b>160</b> identifies features in the pattern <b>105</b> not present in the image <b>130</b>. The differences between the image and pattern so identified can be used for inspection purposes.</p>
    <p num="p-0096"> <figref idrefs="DRAWINGS">FIG. 2</figref> shows a block diagram of the training module <b>10</b> and pattern-related data <b>120</b>. A training image <b>100</b> containing an example of a pattern <b>105</b> to be localized and/or inspected is presented to training module <b>110</b>, which analyzes the training image <b>100</b> and produces a stored pattern <b>120</b> for subsequent use. The training module <b>110</b> consists of two modules, a feature detection module <b>200</b> and a field generation module <b>210</b>. Both modules use various parameters <b>220</b> to control operation as appropriate for the application. These parameters, as well as those needed for subsequent run-time modules, are pattern-dependent and therefore are collected and stored in the pattern <b>120</b> as shown.</p>
    <p num="p-0097">The feature detection module <b>200</b> processes the training image <b>100</b>, and using the parameters <b>220</b>, to produce a field dipole set <b>230</b>, which is stored in the pattern <b>120</b> as shown. The field generation module <b>210</b> uses the field dipole set <b>230</b> and parameters <b>220</b> to produce a field <b>240</b>, stored in the pattern <b>120</b> as shown.</p>
    <p num="p-0098">As described in the summary above, the field <b>240</b> produces information as a function of theoretically real-valued position within the region of the pattern <b>105</b>. In practice, since all the field values cannot be computed analytically and stored, a 2-dimensional array is used that stores field values at discrete points on a grid. Given a real-valued position (to some precision determined by the particular details of the embodiment), an interpolation method, such as the method shown in <figref idrefs="DRAWINGS">FIG. 17</figref>, is used to compute field values at intermediate points between grid elements. Since the field grid is never translated, rotated, or scaled, no re-sampling is needed and grid quantization effects are small. Instead, the image dipoles, which are not grid-based, are mapped to the fixed field coordinates in accordance with the map of <figref idrefs="DRAWINGS">FIG. 16</figref>.</p>
    <p num="p-0099">Thus the purpose of the field generation module <b>210</b> is to compute the elements of the 2-dimensional array that encodes the field <b>240</b>. The field generation module <b>210</b> is itself composed of many steps or submodules, as shown in <figref idrefs="DRAWINGS">FIG. 2</figref>. Each of these steps modifies the field in some way, generally based on the results of the previous steps. Some of the steps also add information to the field dipole set <b>230</b>. The specific sequence of steps shown in <figref idrefs="DRAWINGS">FIG. 2</figref> corresponds to a preferred embodiment; many other variations are possible within the spirit of the invention; the essential requirement is that the stored pattern <b>120</b> be able to provide certain information as a function of position within the region of the pattern <b>105</b>.</p>
    <p num="p-0100">In a preferred embodiment as shown in <figref idrefs="DRAWINGS">FIG. 2</figref>, the field generation module <b>210</b> consists of the following steps. An initialization step <b>250</b> loads predefined codes into the field array elements <b>520</b>, <b>540</b>, shown in <figref idrefs="DRAWINGS">FIG. 5</figref>. A seed step <b>252</b> sets up field array elements at positions corresponding to the dipoles in the field dipole set <b>230</b>. A connect step <b>254</b> uses the seeded field array to identify neighboring dipoles for each field dipole. Once identified, the field dipoles are connected to neighboring ones to form chains by storing the identity of left and right neighbors along pattern boundaries, if any. A chain step <b>256</b> scans the connected field dipoles to identify and catalog discrete chains. For each chain, the starting and ending dipoles, length, total gradient magnitude, and whether the chain is open or closed is determined and stored.</p>
    <p num="p-0101">A filter step <b>258</b> removes weak chains from the pattern by removing the dipoles they contain from the field array (i.e. reversing the seeding step <b>252</b> for those dipoles). A variety of criteria can be used to identify weak chains. In a preferred embodiment, chains whose total gradient magnitude or average gradient magnitude are below some specified parameter are considered weak.</p>
    <p num="p-0102">A segment step <b>260</b> divides chains into segments of low curvature, separated by zones of high curvature called corners. Corner dipoles are marked in the field array for use as described in subsequent figures. Curvature can be determined by a variety of methods; in a preferred embodiment, a dipole is considered a corner if its direction differs from that of either neighbor by more than some specified parameter, e.g., as further described in conjunction with FIG. S.</p>
    <p num="p-0103">A sequence of zero or more propagate steps <b>262</b> extend the field out from the seeded positions. The result is that force vectors pointing to pattern boundaries, as well as other information needed by the run-time steps, can be obtained at some distance from the boundaries. The number of propagate steps <b>262</b> is controlled by a parameter and determines the distance from pattern boundaries that the field will contain valid force vectors, as well as the computation time needed for pattern training. In a preferred embodiment, four propagation steps are used. Field elements beyond the range of propagation will contain the code set during the initialization step <b>250</b>.</p>
    <p num="p-0104"> <figref idrefs="DRAWINGS">FIG. 3</figref> shows a preferred embodiment of a feature detector to be used for practice of the invention. The feature detector processes a source image <b>300</b>, which can be either a training image or a run-time image. A low-pass filter <b>310</b> and image sub-sampler <b>320</b> are used to attenuate fine detail in the source image that for a variety of reasons we wish to ignore. For example we may wish to attenuate noise or fine texture, or we may wish to expand the capture range of the pattern by focusing on coarse pattern features. Also, we may wish to decrease the number of image dipoles, thereby reducing processing time.</p>
    <p num="p-0105">The response of the filter <b>310</b> and sub-sampler <b>320</b> are controlled by parameters <b>220</b> stored in the pattern (not shown in this figure). One setting of the parameters effectively disables the filter and sub-sampler, allowing the source image <b>300</b> to pass without modification for maximum resolution.</p>
    <p num="p-0106">Methods for low-pass filtering and sub-sampling of digital images are well known in the art. In a preferred embodiment, a constant-time second-order approximately Gaussian filter is used, as described in [U.S. patent pending Efficient, Flexible Digital Filtering].</p>
    <p num="p-0107">The filtered, sub-sampled image is processed by a gradient estimation module <b>330</b> to produce an estimate of the x (horizontal) and y (vertical) components of image gradient at each pixel. A Cartesian-to-polar conversion module <b>340</b> converts the x and y components of gradient to magnitude and direction. A peak detection module <b>350</b> identifies points where the gradient magnitude exceeds a noise threshold and is a local maximum along a 1-dimensional profile that lies in approximately the gradient direction, and produces a list of the grid coordinates (row and column number), gradient magnitude, and gradient direction for each such point.</p>
    <p num="p-0108">A sub-pixel interpolation module <b>360</b> interpolates the position of maximum gradient magnitude along said 1-dimensional profile to determine real-valued (to some precision) coordinates (x<sub>i</sub>, y<sub>i</sub>) of the point. The result is a list of points that lie along boundaries in the image, which includes the coordinates, direction, and magnitude of each point. This list can be used as the basis for either a field or image dipole list, to which additional information may be added as appropriate.</p>
    <p num="p-0109">Methods for identifying points along image boundaries are well-known in the art. Any such method can be used for practice of the invention, whether based on gradient estimation or other techniques. Methods for gradient estimation, Cartesian-to-polar conversion, peak detection, and interpolation are also well-known. In a preferred embodiment, the methods described in [U.S. patent pending Method and Apparatus for Fast, Inexpensive, Subpixel Edge Detection] are used.</p>
    <p num="p-0110">In a preferred embodiment, the source image has eight bits of gray-scale per pixel. The low-pass <b>13</b> filter produces a 16-bit image, taking advantage of the inherent noise-reduction properties of a low-pass filter. The gradient estimation module uses the well-known Sobel kernels and operates on either a 16-bit filtered image, if the parameters are set so as to enable the filter <b>310</b>, or an 8-bit unfiltered image if the parameters are set so as to disable the filter <b>310</b>. The x and y components of gradient are always calculated to 16 bits to avoid loss of precision, and the gradient magnitude and direction are calculated to at least six bits preferably using the well-known CORDIC algorithm.</p>
    <p num="p-0111">Several parameter values are needed for feature extraction, both in the training module <b>110</b> and in the run-time module <b>140</b>. Generally these parameters include those controlling the response of the low-pass filter <b>310</b>, the sub-sampling amount used by sub-sampler <b>320</b>, and the noise threshold used by peak detector <b>350</b>. Other values may be needed depending on the exact details of the feature extractor used to practice the invention.</p>
    <p num="p-0112">Appropriate settings for the parameter values depend on the nature of the patterns and images to be analyzed. In a preferred embodiment certain defaults are used that work well in many cases, but in general no rules can be given that work well in all cases. Said preferred embodiment is further described below in conjunction with <figref idrefs="DRAWINGS">FIG. 26</figref>.</p>
    <p num="p-0113"> <figref idrefs="DRAWINGS">FIG. 4</figref> shows an element of the field array as used in a preferred embodiment of the invention. Information is packed into a 32-bit word <b>400</b>, both to conserve memory and to speed up access on conventional computers by maximizing the number of elements that will fit in data cache and using a word size that keeps all elements properly aligned on appropriate address boundaries. Fixed point representations are used for the force vector, both because they are more compact than floating point representations and to allow best use to be made of the signal processing capabilities of modern processors such as the Texas Instruments TMS320C80 and the Intel Pentium-MMX.</p>
    <p num="p-0114">In a preferred embodiment, a field element stores a force vector that gives the distance and direction to the nearest point along a pattern boundary, and one bit that specifies whether the gradient direction at that boundary point is in the same, or 180 opposite, direction as the force vector. This is accomplished as shown in <figref idrefs="DRAWINGS">FIG. 4</figref> by storing a signed force magnitude <b>410</b> and a gradient direction <b>420</b>. If the force direction is the same as the gradient direction, the force magnitude <b>410</b> will be positive; if the force direction is opposite from the gradient direction, the force magnitude <b>410</b> will be negative.</p>
    <p num="p-0115">The magnitude/direction representation for the force vector is preferred over an x-y component representation because it is necessary to be able to represent vectors that have zero length but a well-defined direction. Such vectors are called pseudo-null vectors. The equivalent x-y components can be calculated by the well-known formula</p>
    <p num="p-0116">
      <maths id="MATH-US-00002" num="00002"> <math overflow="scroll"> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>force</mi> <mi>x</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>force</mi> <mi>y</mi> </msub> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo>=</mo> <mrow> <mi>magnitude</mi> <mo></mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mrow> <mi>cos</mi> <mo></mo> <mrow> <mo>(</mo> <mi>direction</mi> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>sin</mi> <mo></mo> <mrow> <mo>(</mo> <mi>direction</mi> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </mrow> </math> </maths> </p>
    <p num="p-0117">Note that gradient direction can be used in the above formula, since the stored magnitude is negative if the force direction is opposite the gradient direction.</p>
    <p num="p-0118">In the preferred embodiment shown, the force magnitude <b>410</b> is in units of field grid increments, using a two's complement representation of 16 total bits, of which the least significant <b>11</b> are to the right of the binary point and the most significant is the sign bit. Thus the maximum force vector length is just under 16 field grid units, and the resolution is 1/2048<sup>th </sup>of a grid unit.</p>
    <p num="p-0119">The gradient direction <b>420</b> is preferably represented as a 12-bit binary angle in the range 0 to 360, with a resolution of 360/4096=0.0880. In other embodiments, the bits of field element <b>400</b> are divided between force <b>410</b> and direction <b>420</b> to provide greater or lesser precision and range, as needed for each particular application.</p>
    <p num="p-0120">A 4 bit flags element <b>430</b> is also stored in the field element <b>400</b>. An 2-bit eval code <b>440</b> determines how an image dipole is to be evaluated if the current pose maps it to a field position within the region covered by this element <b>400</b>. The don't care code specifies that the image dipole should be ignored. The expect blank code specifies that no features are expected in this region of the pattern, and so if any image dipoles map here they should be given a low evaluation for inspection purposes, should be given a high clutter rating, and should not be used as evidence for localization. The evaluate only code specifies that the image dipole should be evaluated by the usual criteria for inspection purposes, but should not contribute evidence for localization purposes. The attract code specifies that the image dipole should be evaluated and used both for localization and inspection.</p>
    <p num="p-0121">If the eval code <b>440</b> is either don't care or expect blank, the force vector is undefined and is said to be invalid. If the eval code is evaluate only or attract, the force vector is said to be valid.</p>
    <p num="p-0122">A 1-bit corner code <b>450</b> specifies whether or not the pattern boundary point pointed to by the force vector is in a high-curvature zone (is corner) or a low-curvature segment (not corner). If the force vector is invalid, the corner code is set to not corner.</p>
    <p num="p-0123">A 1-bit polarity code <b>460</b> specifies whether the image dipole evaluation should consider or ignore gradient direction, as described above in the summary section and further described below. A parameter is used to specify whether or not to override the polarity flags stored in the field element <b>400</b>, and if so, whether to force polarity to be considered or ignored.</p>
    <p num="p-0124">In a preferred embodiment, the field element <b>400</b> also specifies the identity of the nearest field dipole <b>401</b> in addition to the force vector <b>400</b>. The identity <b>401</b> can be represented as a index into the field dipole list. In a preferred embodiment, a 16-bit index is used, which is stored in a separate array so as to satisfy data alignment guidelines of conventional computers.</p>
    <p num="p-0125"> <figref idrefs="DRAWINGS">FIG. 5</figref> shows details of the initialization step <b>250</b> of the field generation step <b>210</b>. A 2-dimensional array <b>500</b> of field elements <b>400</b> is used. Any reasonable grid spacing can be used; in a preferred embodiment, the grid spacing is the same as that of the image that is input to the gradient estimation module <b>330</b> of the feature detector <b>200</b>.</p>
    <p num="p-0126">Field elements <b>520</b>, (indicated as white in <figref idrefs="DRAWINGS">FIG. 5</figref> and having the same structure as field element <b>400</b>) cover the region of the training pattern <b>105</b>, together forming a training region. The field elements <b>520</b> are initialized so that the eval code <b>440</b> is set to expect blank. As described above, in this state the force vector is considered invalid and need not be initialized. In one embodiment, however, further described below in conjunction with <figref idrefs="DRAWINGS">FIG. 20</figref> <i>b</i>, the gradient direction field <b>420</b> of these field elements <b>520</b> are set equal to the corresponding gradient directions of the training image. A border of additional field elements <b>540</b>, (indicated as gray in <figref idrefs="DRAWINGS">FIG. 5</figref> and having the same structure as field element <b>400</b>) are initialized so that the eval code <b>440</b> is set to don't care. This reflects the fact that in general we don't know what features might lie beyond the bounds of the training region. These don't care values will be replicated inwards during each propagation step <b>262</b>, so that image features lying just outside the training pattern <b>105</b> don't attract to pattern features just inside.</p>
    <p num="p-0127">A separate corresponding array of field dipole indices <b>401</b>, identical in size to the white-shaded field elements <b>520</b>, is also used, but need not be initialized. The values in this array are considered valid only if the force vector of the corresponding field element of array <b>500</b> is valid.</p>
    <p num="p-0128"> <figref idrefs="DRAWINGS">FIG. 6</figref> shows details of the seed step <b>252</b> of the field generation module <b>210</b>. Shown is a subset of the field elements <b>520</b> of the field array <b>500</b>. Each field dipole is located within some field element. For example, the field dipole at point <b>600</b> falls within field element <b>620</b>, indicated as gray in <figref idrefs="DRAWINGS">FIG. 6</figref>. Also shown is a small straight-line section of pattern boundary <b>660</b> corresponding to the example field dipole at point <b>600</b>. This section of boundary is shown primarily to aid in understanding the figure. Its orientation, and position along a line normal to its orientation, are significant, but its length is essentially arbitrary.</p>
    <p num="p-0129">The field element <b>620</b> is set to have force vector <b>640</b>. The force vector points from the center of element <b>620</b> to a point on boundary section <b>660</b> and either in the direction, or opposite to the direction of the dipole (i.e. normal to the boundary), whichever is required to bring the head of the vector to the boundary <b>660</b>. In the example shown, the point on the boundary to which the vector points is coincident with the dipole position <b>600</b>, but in general it need not be. <figref idrefs="DRAWINGS">FIG. 6</figref> shows several other examples of seeded force vectors.</p>
    <p num="p-0130">It also may happen that a field dipole's position falls exactly at the center of a field element, so that the length of the force vector is zero. In this case the force vector is pseudo-nullits direction is well-defined and must be set properly.</p>
    <p num="p-0131">In a preferred embodiment for each field element that receives a seed force vector, the eval code is set to attract, the corner code is set to no corner, and the polarity code is set to consider polarity. Other schemes may be devised within the spirit of the invention to suit specific applications.</p>
    <p num="p-0132">For each field element that receives a seed force vector, the corresponding element <b>401</b> of the array of field dipole indices is set to identify the field dipole used to seed the field element.</p>
    <p num="p-0133">In a preferred embodiment using the feature detector of <figref idrefs="DRAWINGS">FIG. 3</figref>, as further described in [U.S. patent pending Method and Apparatus for Fast, Inexpensive, Subpixel Edge Detection], and where the field grid has the same geometry as the image that is input to the gradient estimation module <b>330</b>, no more than one field dipole will fall within any given field element, and there will be no gaps in the boundary due to grid quantization effects. In a less preferred embodiment using different methods for feature detection, various schemes can be used to handle multiple dipoles that fall within a given field element, or gaps in the boundary due to quantization effects. The preferred method for multiple dipoles within one field element is to choose the one whose force vector is shortest, and discard the others. The preferred method for gaps in the boundary is to do nothing and let the propagation steps fill in the gaps.</p>
    <p num="p-0134"> <figref idrefs="DRAWINGS">FIG. 7</figref> shows details of the connect step <b>254</b> of the field generation module <b>210</b>. <figref idrefs="DRAWINGS">FIG. 7</figref> <i>a </i>shows the same subset of field elements <b>520</b> of the field array <b>500</b> as was shown in <figref idrefs="DRAWINGS">FIG. 6</figref>. Also shown is the example field element <b>620</b>, indicated as light gray.</p>
    <p num="p-0135">For every field dipole, the seeded field is examined to identify neighboring positions that contain dipoles to which the dipole should be connected. For the example field element <b>620</b>, the neighboring positions <b>700</b> are shown, shaded medium gray. The neighboring positions <b>700</b> are examined in two steps of four neighboring positions each, each step in a particular order, determined by the direction of the field dipole corresponding to field element <b>620</b>.</p>
    <p num="p-0136">In one step, a left neighbor field element <b>710</b> is identified, and a left link <b>715</b> is stored in the field dipole corresponding to field element <b>620</b> identifying the field dipole corresponding to field element <b>710</b> as its left neighbor. In the other step, a right neighbor field element <b>720</b> is identified, and a right link <b>725</b> is stored to identify the field dipole's right neighbor. If a given neighbor cannot be found, a null link is stored. Note that left and right are defined arbitrarily but consistently by an imaginary observer looking along the dipole gradient direction.</p>
    <p num="p-0137"> <figref idrefs="DRAWINGS">FIG. 7</figref> <i>b </i>shows the order in which neighboring field elements are examined for a dipole whose direction falls between arrows <b>740</b> and <b>742</b>, corresponding to a pattern boundary that falls between dotted lines <b>744</b> and <b>746</b>. The sequence for identifying the left neighbor is +1, +2, +3, and +4. The first neighbor in said sequence that contains a dipole (seeded field element), if any, is the left neighbor. Similarly, the sequence for identifying the right neighbor is 1, 2, 3, and 4.</p>
    <p num="p-0138"> <figref idrefs="DRAWINGS">FIG. 7</figref> <i>c </i>shows another example, where the dipole direction falls between arrows <b>760</b> and <b>762</b>, corresponding to a pattern boundary that falls between dotted lines <b>764</b> and <b>766</b>. The sequences of neighbors are as shown. The sequences for all other dipole directions are simply rotations of the two cases of <figref idrefs="DRAWINGS">FIGS. 7</figref> <i>b </i>and <b>7</b> <i>c. </i> </p>
    <p num="p-0139">Note that the sequences given in <figref idrefs="DRAWINGS">FIGS. 7</figref> <i>b </i>and <b>7</b> <i>c </i>show a preference for orthogonal neighbors over diagonal neighbors, even when diagonal neighbors are closer to the direction of the pattern boundary. This preference insures that the chains will properly follow a stair-step pattern for boundaries not aligned with the grid axes. Clearly this preference is somewhat dependent on the specific details of how the feature detector chooses points along the boundary.</p>
    <p num="p-0140">Once connections have been established for all field dipoles, a consistency check is performed. Specifically, the right neighbor of a dipole's left neighbor should be the dipole itself, and the left neighbor of a dipole's right neighbor should also be the dipole itself. If any links are found for which these conditions do not hold, the links are broken by replacing them with a null link. At the end of the connect step, only consistent chains remain.</p>
    <p num="p-0141">Many alternate methods can be used to connect dipoles within the spirit and scope of the invention. In some embodiments, particularly where no inspection is to be performed, the connect <b>254</b> is omitted entirely.</p>
    <p num="p-0142"> <figref idrefs="DRAWINGS">FIG. 8</figref> shows details of the segment step <b>260</b> of the field generation step <b>210</b>. Field elements <b>800</b> shaded medium gray are identified as corners because the dipole directions differs from that of their left and/or right neighbors by more than some specified parameter. In a preferred embodiment, the parameter is 16.875 degrees. For these elements the corner code <b>450</b> is set to is corner. Other field elements <b>820</b>, shaded light gray, lie along one chain segment, while field elements <b>840</b>, also shaded light gray, lie along another chain segment. For these elements it is not necessary to set the corner code, because it was set to no corner when the field was seeded.</p>
    <p num="p-0143"> <figref idrefs="DRAWINGS">FIG. 9</figref> shows examples of part of the analysis that is performed by the propagate step <b>262</b> of the field generation step <b>210</b>. In the example of <figref idrefs="DRAWINGS">FIG. 9</figref> <i>a</i>, field element <b>900</b> initially does not have a valid force vector; its eval code is expect blank-, as set by the initialization step <b>250</b>. Neighboring element <b>902</b> has a valid force vector <b>904</b>, which points to a segment of pattern boundary <b>906</b> that is assumed to be an approximately straight line.</p>
    <p num="p-0144">A vector <b>908</b> is constructed from the center <b>916</b> of element <b>900</b> to the center of the neighbor <b>902</b>. The projection <b>910</b> of vector <b>908</b> onto force vector <b>904</b> is constructed. A new force vector <b>912</b> is constructed from the center <b>916</b> of field element <b>900</b> to the boundary <b>906</b> by adding the neighbor's force vector <b>904</b> to the projection <b>910</b>. An offset value is computed whose magnitude is equal to the length <b>914</b> of the difference between vector <b>908</b> and projection <b>910</b>, and whose sign is determined by the direction <b>918</b> by which vector <b>908</b> must be rotated to coincide with projection <b>910</b>, where anti-clockwise is positive as shown and clockwise is negative. The result of this analysis is the new force vector <b>912</b> and offset value of magnitude <b>914</b> and sign <b>918</b>.</p>
    <p num="p-0145">A similar example but for a diagonal neighbor <b>932</b> of element <b>930</b> is shown in <figref idrefs="DRAWINGS">FIG. 9</figref> <i>b</i>. The projection <b>940</b> of vector <b>938</b> onto force vector <b>934</b> is constructed. A new force vector <b>942</b> is constructed from the center <b>946</b> of field element <b>930</b> to the boundary <b>936</b> by adding the neighbor's force vector <b>934</b> to the projection <b>940</b>. An offset value is computed whose magnitude is equal to the length <b>944</b> of the difference between vector <b>938</b> and projection <b>940</b>, and whose sign is negative since vector <b>938</b> must be rotated clockwise <b>948</b> to coincide with projection <b>940</b>.</p>
    <p num="p-0146">Another example is shown in <figref idrefs="DRAWINGS">FIG. 9</figref> <i>c</i>, where in this case the boundary <b>966</b> passes between field element <b>960</b> and its neighbor <b>962</b>. The projection <b>970</b> of vector <b>968</b> onto force vector <b>964</b> is constructed. A new force vector <b>972</b> is constructed from the center <b>976</b> of field element <b>960</b> to the boundary <b>966</b> by adding the neighbor's force vector <b>964</b> to the projection <b>970</b>. An offset value is computed whose magnitude is equal to the length <b>974</b> of the difference between vector <b>968</b> and projection <b>970</b>, and whose sign is negative since vector <b>968</b> must be rotated clockwise <b>978</b> to coincide with projection <b>970</b>.</p>
    <p num="p-0147">Further details of propagate step <b>262</b> of the field generation step <b>210</b> are shown in <figref idrefs="DRAWINGS">FIG. 10</figref>. Each element of the field array is examined. Any element whose eval code <b>440</b> is expect blank is considered for possible propagation of the field to that element. All other field elements are already in a final state and are skipped. For each field element so considered, the eight neighbors are examined. If two or more adjacent neighbors have valid force vectors or have eval codes equal to don't care, the field will be propagated to the said field element; otherwise, the field element will be skipped and possibly considered again on a subsequent propagate step.</p>
    <p num="p-0148">The rule specifying two or more adjacent neighbors is used to insure that there is sufficient information to be able to interpolate the field between neighbors. Adjacent means either sharing an edge, such as elements <b>1010</b> and <b>1012</b> of <figref idrefs="DRAWINGS">FIG. 10</figref>, or sharing a corner, such as elements <b>1010</b> and <b>1014</b>.</p>
    <p num="p-0149">In <figref idrefs="DRAWINGS">FIG. 10</figref> element <b>1000</b> shaded light gray has eval code expect blank, and neighbors <b>1010</b>, <b>1012</b>, and <b>1014</b>, shaded medium gray, have valid force vectors (the field has been seeded at or already propagated to the neighbors). Neighboring element <b>1010</b> has force vector <b>1030</b>, and following the method of <figref idrefs="DRAWINGS">FIG. 9</figref> new force vector <b>1032</b> and positive offset <b>1034</b> are computed. Neighboring element <b>1012</b> has force vector <b>1040</b>, and following the method of <figref idrefs="DRAWINGS">FIG. 9</figref> new force vector <b>1042</b> and negative offset <b>1044</b> are computed. Neighboring element <b>1014</b> has force vector <b>1050</b>, and following the method of <figref idrefs="DRAWINGS">FIG. 9</figref> new force vector <b>1052</b> and negative offset <b>1054</b> are computed.</p>
    <p num="p-0150">The neighbors of field element <b>1000</b> are scanned anti-clockwise in sequence <b>1070</b>. The starting and ending points of sequence <b>1070</b> are arbitrary. If exactly one positive to negative offset transition between adjacent neighbors is found, the field is propagated to element <b>1000</b> by constructing a force vector <b>1080</b> by interpolating between new force vectors <b>1032</b> and <b>1042</b>.</p>
    <p num="p-0151">In a preferred embodiment, the interpolation is a weighted average of vectors <b>1032</b> and <b>1042</b>. The vector <b>1032</b> is weighted by the magnitude of offset <b>1044</b>, and the vector <b>1042</b> is weighted by the magnitude of offset <b>1034</b>. The effect is that the weight of a vector is proportional to the other vector's offset and inversely proportional to its own offset, so that vectors are more heavily weighted if they pass closer to their corresponding neighbor's force vector. As shown in <figref idrefs="DRAWINGS">FIG. 10</figref>, the offset corresponding to vector <b>1032</b> has the smaller magnitude, so it is more heavily weighted and therefore force vector <b>1080</b> passes closer to vector <b>1032</b> than to vector <b>1042</b>.</p>
    <p num="p-0152">One method for constructing a weighted average of vectors is to scale each vector by its corresponding weight, add the results, and then scale by the inverse of the sum of the weights. This is equivalent to an independent weighted average of the x and v components. In a preferred embodiment, an independent weighted average of the magnitude and direction is used.</p>
    <p num="p-0153">If the vectors <b>1032</b> and <b>1042</b> participating in the interpolation are pointing to distant points along a pattern boundary, or to different boundaries, the field is considered indeterminate at element <b>1000</b> and the eval code is set to don't care. In a preferred embodiment, the vectors are considered to be pointing to distant points or different boundaries if either their magnitudes differ by more than 3 grid units or their directions differ by more than 135.</p>
    <p num="p-0154">In a preferred embodiment, a special case rule is used to propagate the field at the ends of an open chain. If a neighboring element with a valid force produces a small positive offset, and no anti-clockwise adjacent neighbor has a valid force, the field will propagate without interpolation by using the new force vector as constructed by the method of <figref idrefs="DRAWINGS">FIG. 9</figref>. Similarly, if a neighboring element with a valid force produces a small negative offset, and no clockwise adjacent neighbor has a valid force, the field will propagate without interpolation by using the new force vector as constructed by the method of <figref idrefs="DRAWINGS">FIG. 9</figref>. In a preferred embodiment, a small offset is one whose magnitude is less than 1/10<sup>th </sup>of a grid unit.</p>
    <p num="p-0155">If more than one positive to negative offset transition between adjacent neighbors, or application of the special case rule, is found, or if none are found, the field is considered indeterminate at element <b>1000</b> and the eval code is set to don't care. One reason that no such transitions might be found is that neighboring field elements are themselves set to don't care, for example the border elements <b>540</b> set by the initialization step <b>250</b>.</p>
    <p num="p-0156">If a valid force is propagated to element <b>1000</b>, then corner code <b>450</b>, polarity code <b>460</b>, and the index of the nearest field dipole are also propagated by copying from whichever of the neighboring elements participating in the interpolation has the smallest offset magnitude (greatest weight). In the example of <figref idrefs="DRAWINGS">FIG. 10</figref> the values would be copied from element <b>1010</b>. If the special case rule was applied, the values are copied from the neighbor with small offset used to construct the new force vector.</p>
    <p num="p-0157">Many variations on the above rules can be used within the spirit of the invention to achieve similar results. Indeed any method that produces force vectors that point to the nearest point along a pattern boundary can be used to practice this invention.</p>
    <p num="p-0158"> <figref idrefs="DRAWINGS">FIG. 11</figref> shows the same portion of the field array that was shown after seeding in <figref idrefs="DRAWINGS">FIG. 6</figref>, but with new force vectors resulting from one propagation step. <figref idrefs="DRAWINGS">FIG. 12</figref> shows the same portion after two propagation steps. Note in <figref idrefs="DRAWINGS">FIG. 12</figref> field element <b>1200</b> whose eval code is set to don't care because more than one positive to negative offset transition between adjacent neighbors was found.</p>
    <p num="p-0159"> <figref idrefs="DRAWINGS">FIG. 13</figref> shows a block diagram of the run-time module <b>140</b> of a preferred embodiment. Run-time module <b>140</b> analyzes the image <b>130</b>, using the stored pattern <b>120</b>, the starting pose <b>132</b>, and the client map <b>131</b>. As a result of the analysis, the run-time module produces a pose <b>134</b> that maps pattern points to accurately corresponding image points.</p>
    <p num="p-0160">The run-time module <b>140</b> produces an rms error value <b>136</b> that is a measure of the degree of match between the pattern and the image, a coverage value <b>138</b> that is a measure of the fraction of the pattern to which corresponding image features have been found, and a clutter value <b>137</b> that is a measure of extra features found in the image that do not correspond to pattern features.</p>
    <p num="p-0161">The run-time module <b>140</b> produces an evaluated image dipole list <b>150</b> and an evaluated field dipole list <b>160</b>, e.g., as shown in <figref idrefs="DRAWINGS">FIG. 25</figref>. The clutter values of the evaluated image dipole list <b>150</b> can be used to identify features in the image <b>130</b> not present in the pattern <b>105</b> (shown in <figref idrefs="DRAWINGS">FIG. 1</figref>). These probability values range from 0 to 1 and indicate the likelihood that the image feature is not present in the pattern. The eval<b>2</b> values (<figref idrefs="DRAWINGS">FIG. 25</figref>) of the evaluated field dipole list <b>160</b> can be used to identify features in the pattern <b>105</b> not present in the image <b>130</b>. These probability values range from 0 to 1, and indicate the likelihood that the pattern feature was found in the image.</p>
    <p num="p-0162">The run-time module <b>140</b> uses a feature detection module <b>200</b> to process the image <b>130</b> to produce an image dipole list <b>1300</b>. In a preferred embodiment, the feature detection module <b>200</b> is identical to that used by training module <b>110</b>, and is controlled by the same parameter settings stored in pattern parameters <b>220</b>, and is further described in conjunction with <figref idrefs="DRAWINGS">FIG. 26</figref>. In other embodiments, different methods or different parameters setting are used as appropriate for a specific application.</p>
    <p num="p-0163">At least one attraction module <b>1350</b> uses pattern parameters <b>220</b>, field dipole set <b>230</b>, field <b>240</b>, image dipole list <b>1300</b>, and the starting pose <b>132</b> and client map <b>131</b>, to refine the starting pose <b>132</b> and produce the other outputs <b>136</b>, <b>138</b>, <b>160</b>, and <b>150</b>.</p>
    <p num="p-0164"> <figref idrefs="DRAWINGS">FIG. 14</figref> is a diagram that is used to derive the mathematical basis for a preferred embodiment that uses a least-squares method to determine a pose that best accounts for the evidence of the image dipoles at each attraction step. An image dipole, mapped by the current pose to field point <b>1400</b> and with direction <b>1402</b>, is considered. A small section of image boundary <b>1404</b> is also shown as an aid in understanding the diagram.</p>
    <p num="p-0165">The field has force f <b>1430</b> at mapped image dipole point <b>1400</b>, pointing to the nearest point <b>1435</b> along pattern boundary <b>1410</b>. Note that the force <b>1430</b> is normal to the pattern boundary <b>1410</b> at point <b>1435</b>, and the mapped image dipole direction <b>1402</b> is similar but not equal, modulo <b>180</b>, to that of the force.</p>
    <p num="p-0166">The mapped image dipole point <b>1400</b> has position vector p <b>1450</b> relative to the field origin <b>1420</b>. The existence of an image dipole at field point <b>1400</b>, with force <b>1430</b> and mapped dipole-direction similar to the force direction (in a preferred embodiment, similar modulo <b>180</b>), is taken as evidence that the current pose should be modified so that the image dipole is mapped so as to lie somewhere along line <b>1440</b> tangent to pattern boundary <b>1410</b> at point <b>1435</b>. The dipole provides no evidence as to position normal to the force, that is along tangent <b>1440</b>. The position vector p <b>1470</b> defines a point on tangent <b>1440</b>, and the difference vector pp <b>1460</b> indicates how the mapped dipole position might move as a result of the force.</p>
    <p num="p-0167">Suppose that [C, t] is a six-degree-of-freedom coordinate transform that maps the current pose into a new, hopefully more accurate, pose. This transform is called the motion transform, because it tells how the image dipoles will move with respect to the field under the influence of the forces of the field. Here C is a 22 matrix and t is a translation vector. The evidence under consideration suggests that this transform should map p to p:
<br> <i>p=Cp+t.</i>(1)</p>
    <p num="p-0168">Let I be the identity matrix and define
<br> <i>f=|f|</i>(2)</p>
    <p num="p-0169"> <maths id="MATH-US-00003" num="00003"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mover> <mi>f</mi> <mo>^</mo> </mover> <mo>=</mo> <mfrac> <mi>f</mi> <mrow> <mo></mo> <mi>f</mi> <mo></mo> </mrow> </mfrac> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>3</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br> <i>=Cl</i>(4)</p>
    <p num="p-0170">From the diagram of <figref idrefs="DRAWINGS">FIG. 14</figref> it can be seen that</p>
    <p num="p-0171">
      <maths id="MATH-US-00004" num="00004"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mi>f</mi> <mo>=</mo> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>p</mi> <mi></mi> </msup> <mo>-</mo> <mi>p</mi> </mrow> <mo>)</mo> </mrow> <mo></mo> <mover> <mi>f</mi> <mo>^</mo> </mover> </mrow> </mrow> </mtd> <mtd> <mrow> <mstyle> <mspace width="18.3em" height="18.3ex"> </mspace> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <mn>5</mn> <mo></mo> <mi>a</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mo>=</mo> <mrow> <mrow> <mo>(</mo> <mrow> <mi>Cp</mi> <mo>+</mo> <mi>t</mi> <mo>-</mo> <mi>p</mi> </mrow> <mo>)</mo> </mrow> <mo></mo> <mover> <mi>f</mi> <mo>^</mo> </mover> </mrow> </mrow> </mtd> <mtd> <mrow> <mstyle> <mspace width="21.7em" height="21.7ex"> </mspace> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <mn>5</mn> <mo></mo> <mi>b</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mo>=</mo> <mrow> <mrow> <mo>[</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mi>C</mi> <mo>-</mo> <mi>I</mi> </mrow> <mo>)</mo> </mrow> <mo></mo> <mi>p</mi> </mrow> <mo>+</mo> <mi>t</mi> </mrow> <mo>]</mo> </mrow> <mo></mo> <mover> <mi>f</mi> <mo>^</mo> </mover> </mrow> </mrow> </mtd> <mtd> <mrow> <mstyle> <mspace width="21.4em" height="21.4ex"> </mspace> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <mn>5</mn> <mo></mo> <mi>c</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mo>=</mo> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mover> <mi>C</mi> <mo>.</mo> </mover> <mo></mo> <mi>p</mi> </mrow> <mo>+</mo> <mi>t</mi> </mrow> <mo>)</mo> </mrow> <mo></mo> <mover> <mi>f</mi> <mo>^</mo> </mover> </mrow> </mrow> </mtd> <mtd> <mrow> <mstyle> <mspace width="24.7em" height="24.7ex"> </mspace> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <mn>5</mn> <mo></mo> <mi>d</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0172">Thus, given an image dipole that maps to field point p with force f=, we have one equation in the six unknowns [C, t] that tells us how to map the current pose to get a new pose. With six dipoles we can solve for the six-degrees-of-freedom, but in practice the evidence obtained from only six dipoles is generally not sufficient to get an accurate or even meaningful solution. In practice we use many dipoles, typically anywhere from a few dozen to a few thousand, and some method for solving an over-determined set of equations.</p>
    <p num="p-0173">In a preferred embodiment, a least-squares method is used. An error term for the i<sup>th </sup>dipole can be defined as
<br> <i>e</i> <sub>i</sub>=(<i>p</i> <sub>i</sub> <i>+t</i>)<i>{circumflex over (f)}</i> <sub>i</sub><sub>i</sub>(6)</p>
    <p num="p-0174">With this definition a least-squares problem can be set up and solved by methods well-known in the art. If weights w<sub>i </sub>are determined for each dipole, we can write the sum squared error as</p>
    <p num="p-0175">
      <maths id="MATH-US-00005" num="00005"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mi>E</mi> <mo>=</mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <msup> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <mo>[</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mover> <mi>C</mi> <mo>.</mo> </mover> <mo></mo> <msub> <mi>p</mi> <mi>i</mi> </msub> </mrow> <mo>+</mo> <mi>t</mi> </mrow> <mo>)</mo> </mrow> <mo></mo> <msub> <mover> <mi>f</mi> <mo>^</mo> </mover> <mi>i</mi> </msub> </mrow> <mo>-</mo> <msub> <mi>f</mi> <mi>i</mi> </msub> </mrow> <mo>]</mo> </mrow> </mrow> <mn>2</mn> </msup> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>7</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0176">In practice it is usually desirable to solve for fewer than six-degrees-of-freedom. Some patterns would result in a singular or unstable solutions if certain degrees of freedom are included. For example, circles cannot be solved for orientation and corners cannot be solved for size. In other cases, a solution would be possible, but some degrees of freedom, particularly aspect ratio and skew, are known not to vary and might cause problems if included. Perhaps the most serious such problem is that unreliable evidence, present to some degree in all images, will have a more serious effect when more degrees of freedom are allowed to vary. Another problem is that somewhat more computation is needed to solve for the additional degrees of freedom.</p>
    <p num="p-0177">In a preferred embodiment the least-squares problem is set up in 4 degrees of freedom corresponding to x translation, y translation, orientation, and size. Sums needed for a least-squares solution in the 4 degrees of freedom are computed, and pattern parameters <b>220</b> specify which of the degrees of freedom will be solved for.</p>
    <p num="p-0178">In an orthonormal coordinate system we can constrain the matrix  to orientation and size variation by writing it as</p>
    <p num="p-0179">
      <maths id="MATH-US-00006" num="00006"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mover> <mi>C</mi> <mo>.</mo> </mover> <mo>=</mo> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mi>p</mi> </mtd> <mtd> <mi>q</mi> </mtd> </mtr> <mtr> <mtd> <mrow> <mo>-</mo> <mi>q</mi> </mrow> </mtd> <mtd> <mi>p</mi> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo></mo> <mrow> <mstyle> <mspace width="27.5em" height="27.5ex"> </mspace> </mstyle> <mo></mo> <mstyle> <mspace width="1.7em" height="1.7ex"> </mspace> </mstyle> </mrow> <mo></mo> <mstyle> <mtext>(8a)</mtext> </mstyle> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mo>=</mo> <mrow> <mi>p1</mi> <mo>+</mo> <mrow> <mrow> <mi>q</mi> <mo></mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>1</mn> </mtd> </mtr> <mtr> <mtd> <mrow> <mo>-</mo> <mn>1</mn> </mrow> </mtd> <mtd> <mn>0</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <mstyle> <mspace width="24.2em" height="24.2ex"> </mspace> </mstyle> <mo></mo> <mstyle> <mspace width="1.1em" height="1.1ex"> </mspace> </mstyle> <mo></mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> </mrow> <mo></mo> <mstyle> <mtext>(8b)</mtext> </mstyle> </mrow> </mrow> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0180">In practical applications, however, the images themselves are almost never orthonormal. CCD cameras, for example, typically have pixels that are non-square by a percent or so. For line scan cameras, the angle between the coordinate axes depends on mechanical alignment and so the coordinate axes may not be perfectly orthogonal. The variations from square are small, but easily detectable given the accuracy that can be achieved with the invention. Furthermore, it is sometimes useful to have a significantly non-orthonormal field. For example, a field generated from a square pattern can be used to localize and inspect a rectangular or even parallelogram-shaped instance of the pattern by using an appropriate starting pose.</p>
    <p num="p-0181">In these cases we generally want the orientation degree of freedom defined by an orthonormal, real-world coordinate system rather than image or field coordinates. We re-write </p> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-P00001.png"> <img id="CUSTOM-CHARACTER-00001" he="3.13mm" wi="2.12mm" file="US07065262-20060620-P00001.TIF" alt="Figure US07065262-20060620-P00001" img-content="character" img-format="tif" src="//patentimages.storage.googleapis.com/US7065262B1/US07065262-20060620-P00001.png" class="patent-full-image" width="8" height="12"> </a> </div> as
<br> <i>=pl+qN</i>(9)<br>
where the elements of matrix N are the components of the normal tensor in the field coordinate system. The normal tensor is a mixed 2<sup>nd</sup>-rank tensor, a vector-valued function of vectors that, informally, tells how to rotate a vector 90. In an orthonormal coordinate system, of course, the components of the normal tensor are

    <p num="p-0182">
      <maths id="MATH-US-00007" num="00007"> <math overflow="scroll"> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>1</mn> </mtd> </mtr> <mtr> <mtd> <mrow> <mo>-</mo> <mn>1</mn> </mrow> </mtd> <mtd> <mn>0</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo></mo> <mrow> <mo></mo> <mo>.</mo> </mrow> </mrow> </math> </maths> </p>
    <p num="p-0183">The components of the normal tensor are computed from the current pose and from a coordinate transform called the client map that transforms points in an orthonormal but otherwise arbitrary reference coordinate system to points in the run-time image.</p>
    <p num="p-0184">We can now re-write equation 7, the sum squared error, as</p>
    <p num="p-0185">
      <maths id="MATH-US-00008" num="00008"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mi>E</mi> <mo>=</mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msup> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <mo>[</mo> <mrow> <mrow> <mrow> <mo>[</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mi>p1</mi> <mo>+</mo> <mi>qN</mi> </mrow> <mo>)</mo> </mrow> <mo></mo> <msub> <mi>p</mi> <mi>i</mi> </msub> </mrow> <mo>+</mo> <mi>t</mi> </mrow> <mo>]</mo> </mrow> <mo></mo> <msub> <mover> <mi>f</mi> <mo>^</mo> </mover> <mi>i</mi> </msub> </mrow> <mo>-</mo> <msub> <mi>f</mi> <mi>i</mi> </msub> </mrow> <mo>]</mo> </mrow> </mrow> <mn>2</mn> </msup> <mo></mo> <mrow> <mstyle> <mspace width="12.8em" height="12.8ex"> </mspace> </mstyle> <mo></mo> <mstyle> <mspace width="1.1em" height="1.1ex"> </mspace> </mstyle> </mrow> <mo></mo> <mstyle> <mtext>(10a)</mtext> </mstyle> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mo>=</mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo>[</mo> <mrow> <mo>[</mo> <mrow> <msup> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>pp</mi> <mi>i</mi> </msub> <mo></mo> <msub> <mover> <mi>f</mi> <mo>^</mo> </mover> <mi>i</mi> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>qNp</mi> <mi>i</mi> </msub> <mo></mo> <msub> <mover> <mi>f</mi> <mo>^</mo> </mover> <mi>i</mi> </msub> </mrow> <mo>+</mo> <mrow> <mi>t</mi> <mo></mo> <msub> <mover> <mi>f</mi> <mo>^</mo> </mover> <mi>i</mi> </msub> </mrow> <mo>-</mo> <msub> <mi>f</mi> <mi>i</mi> </msub> </mrow> <mo>]</mo> </mrow> <mn>2</mn> </msup> <mo></mo> <mrow> <mstyle> <mspace width="8.3em" height="8.3ex"> </mspace> </mstyle> <mo></mo> <mstyle> <mspace width="2.2em" height="2.2ex"> </mspace> </mstyle> </mrow> <mo></mo> <mstyle> <mtext>(10b)</mtext> </mstyle> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0186">Now we can substitute
<br> <i>r</i> <sub>i</sub> <i>=p</i> <sub>i</sub> <i>{circumflex over (f)}</i> <sub>i</sub>(11)<br> <i>s</i> <sub>i</sub> <i>=Np</i> <sub>i</sub> <i>{circumflex over (f)}</i> <sub>i</sub>(12)</p>
    <p num="p-0187"> <maths id="MATH-US-00009" num="00009"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mi>t</mi> <mo>=</mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mi>x</mi> </mtd> </mtr> <mtr> <mtd> <mi>y</mi> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>13</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mover> <mi>f</mi> <mo>^</mo> </mover> <mi>i</mi> </msub> <mo>=</mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>u</mi> <mi>i</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>v</mi> <mi>i</mi> </msub> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>14</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br>
into equation 10b and finally we have
</p>
    <p num="p-0188">
      <maths id="MATH-US-00010" num="00010"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mi>E</mi> <mo>=</mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo>[</mo> <msup> <mrow> <mo>(</mo> <mrow> <msub> <mi>xu</mi> <mi>i</mi> </msub> <mo>+</mo> <msub> <mi>yv</mi> <mi>i</mi> </msub> <mo>+</mo> <msub> <mi>pr</mi> <mi>i</mi> </msub> <mo>+</mo> <msub> <mi>qs</mi> <mi>i</mi> </msub> <mo>-</mo> <msub> <mi>f</mi> <mi>i</mi> </msub> </mrow> <mo>]</mo> </mrow> <mn>2</mn> </msup> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>15</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0189">A least-squares problem based on equation 15 is easy to set up and solve for x, y, p, and q by well-known methods. The desired motion transform [C, t] is obtained from said solution using equations 4, 9, and 13. The current pose is composed with the motion transform to obtain the new pose.</p>
    <p num="p-0190"> <figref idrefs="DRAWINGS">FIG. 15</figref> is a block diagram of the attraction module <b>1350</b> for a preferred embodiment based on a least-squares method of best accounting for the evidence of the image dipoles. In addition, to further clarify a preferred sequence of operation of the modules of <figref idrefs="DRAWINGS">FIG. 15</figref>, a flow chart is provided in <figref idrefs="DRAWINGS">FIGS. 27A and 27B</figref>. Steps of the flow chart include reference numbers from <figref idrefs="DRAWINGS">FIG. 15</figref> in parentheses to help cross-correlate the figures. A current pose <b>1500</b> is initially set to the start pose <b>2702</b> and updated at the end of each attraction step <b>2724</b>. After the sum module <b>1535</b> is initialized to zero <b>2704</b>, the Normal tensor computation module <b>1510</b> uses the current pose and client map to compute the normal tensor N <b>2706</b> for the current attraction step.</p>
    <p num="p-0191">Each image dipole <b>1515</b> of the image dipole list <b>1300</b> (<figref idrefs="DRAWINGS">FIG. 13</figref>) is processed <b>2708</b>. The position and direction of dipole <b>1515</b> are mapped <b>2710</b> from image coordinates to field coordinates by map module <b>1520</b>, using the current pose <b>1500</b>. The mapped position is used by field module <b>1525</b> to determine the force, flags <b>430</b>, and index of nearest field dipole <b>2712</b>. The force, flags, and index are stored in the image dipole <b>1515</b> for later use. The normal tensor, force, and image dipole position in field coordinates are used by a rotate module <b>1530</b> to obtain the dipole's position in force coordinates (r, s) <b>2714</b> as specified by equations 11 and 12.</p>
    <p num="p-0192">An evaluate module <b>1545</b> examines the force, flags, image dipole direction in field coordinates, and dipole gradient magnitude and computes a weight for attraction (localization) purposes and evaluation and clutter values for inspection purposes <b>2714</b>. In some embodiments, the evaluate module <b>1545</b> also considers the rms error from the previous attraction step in determining the weight and evaluation. The evaluation and clutter values are stored in the image dipole <b>1515</b> for later use.</p>
    <p num="p-0193">A sum module <b>1535</b> uses the force, dipole position in force coordinates, and weight to compute sums needed for the least-squares solution <b>2716</b>. If there are no more dipoles <b>2718</b>, a solve module <b>1540</b> uses the sums and the normal tensor to solve for the motion transform and compute the rms error <b>2720</b>. A compose module <b>1505</b> composes the current pose with the motion transform to produce a new pose <b>2722</b>, which will be the current pose for the next attraction step, or the final pose if this is the last attraction step <b>2726</b>.</p>
    <p num="p-0194">In a preferred embodiment where inspection is being performed, at the end of the last attraction step, the field dipole evaluation module <b>1550</b> evaluates the image dipole list <b>2728</b>, which has now been evaluated by evaluate module <b>1545</b>, and the Field dipole set <b>230</b> stored in the pattern <b>120</b>, and produces an evaluated field dipole list, coverage rating, and clutter rating <b>2730</b>.</p>
    <p num="p-0195"> <figref idrefs="DRAWINGS">FIG. 16</figref> gives details for the map module <b>1520</b> of <figref idrefs="DRAWINGS">FIG. 15</figref>. Inputs are image dipole position in image coordinates <b>1600</b>, image dipole direction with respect to image coordinates <b>1610</b>, and the current pose <b>1620</b>. One output is the dipole position in field coordinates <b>1630</b>, computed as shown.</p>
    <p num="p-0196">The other output is the dipole direction with respect to field coordinates <b>1640</b>, computed as shown. The formula for output dir(field) <b>1640</b> effectively does the following, reading the vector and matrix operations right to left:
</p> <ul> <li id="ul0001-0001" num="0000"> <ul> <li id="ul0002-0001" num="0196">Construct a unit vector in the dipole direction, with respect to image coordinates, by computing the cosine and sine of the angle <sub>d</sub>.</li> <li id="ul0002-0002" num="0197">Rotate the unit vector 90 to get a direction along the boundary that contains the dipole.</li> <li id="ul0002-0003" num="0198">Map the rotated unit vector to field coordinates using C<sub>p </sub>to get a boundary direction in field coordinates.</li> <li id="ul0002-0004" num="0199">Rotate the mapped rotated unit vector 90 to get a direction normal to the boundary in field coordinates.</li> <li id="ul0002-0005" num="0200">If the determinant of the pose matrix is negative, the transform changes the left- or right-handedness of the coordinate system, so rotate the vector 180 because the 90 of the previous step should have been +90.</li> <li id="ul0002-0006" num="0201">Compute the angle of the resulting vector using the well-known version of the arctangent function of two arguments whose result is in the range 0 to 360.</li> </ul> </li> </ul> <p num="p-0197">Note as shown in output dir(field) <b>1640</b> that these calculations can be simplified considerably. In a preferred embodiment, the simplified formula is used at the beginning of each attraction step to compute a 256-element lookup table, indexed by an 8-bit binary angle, for use by the map block <b>1520</b>. This allows the direction mapping operation to be executed at high speed for each dipole.</p>
    <p num="p-0198">When computing the lookup table, the symmetry of the formula requires us to compute only 128 elements of the table; the other elements are the negative of the computed ones. As a further improvement in computation time, 64 even-indexed elements are computed, and the odd-indexed elements are determined by interpolation from the even-indexed elements. Thus the formula need only be applied 64 times. The arctangent function is computed using the well-known CORDIC method.</p>
    <p num="p-0199">Note that in computing output dir(field) <b>1640</b> we map the boundary direction instead of the dipole direction. This is because, in the embodiment described herein, directions are determined by a gradient estimation method <b>330</b> and Cartesian to polar conversion method <b>340</b> that assumes square pixels. This is not a problem except when mapping directions between non-orthonormal coordinate systems. In that case, the boundary direction must be used.</p>
    <p num="p-0200"> <figref idrefs="DRAWINGS">FIG. 17</figref> shows a block diagram of the field module <b>1525</b> of <figref idrefs="DRAWINGS">FIG. 15</figref>, and a corresponding geometric diagram that illustrates the computation being performed. Image dipole position in field coordinates <b>1754</b> is input to the field block <b>1525</b>. The coordinates <b>1754</b> fall within field grid cell <b>1750</b>.</p>
    <p num="p-0201">The coordinates are rounded to integer field grid position <b>1758</b> by integer rounding module <b>1700</b>. The integer field grid coordinates <b>1758</b> are used by address generation module <b>1704</b> to produce a memory address used to look up field element <b>1708</b>, corresponding co grid cell <b>1750</b>. In the embodiment shown, the index of the nearest field dipole is stored with the other field information, but in some embodiments, as described herein, the index is kept in a separate array.</p>
    <p num="p-0202">The force direction of, flags, and index obtained from field element <b>1708</b> are direct outputs of field module <b>1525</b>, but the force magnitude <b>1774</b> is interpolated so that the force vector is a reasonably smooth function of real-valued position within the field.</p>
    <p num="p-0203">Force interpolation is based on the assumption that the force stored in field element <b>1708</b>, corresponding to integer grid position <b>1758</b>, points to an approximately straight-line section of pattern boundary <b>1782</b>. This is a fast and accurate interpolation we can use with the information available. A more compute intensive interpolation could use neighboring field elements as well.</p>
    <p num="p-0204">To interpolate force magnitude, the integer position <b>1758</b> is subtracted by module <b>1712</b> from the real-valued position <b>1754</b> to produce sub-grid position vector <b>1762</b>. A unit vector <b>1766</b> in the force direction is constructed by cosine/sine module <b>1716</b>, implemented as a lookup table in a preferred embodiment. The dot product <b>1770</b> of sub-grid position vector <b>1762</b> and unit vector <b>1766</b> is computed by dot product module <b>1720</b>. The dot product <b>1770</b> is subtracted from force magnitude <b>1774</b> by the subtraction module <b>1724</b> to produce interpolated force magnitude <b>1778</b>.</p>
    <p num="p-0205">The interpolated force magnitude <b>1778</b>, unit vector in the force direction <b>1766</b>, and force direction angle stored in field element <b>1708</b>, are collected in output module <b>1728</b> and become part of the force vector <b>1526</b> produced by field module <b>1525</b>.</p>
    <p num="p-0206">In another embodiment, not shown, at least one force vector is stored in each field element, pointing to the nearest points along at least one pattern boundary. The field module <b>1525</b> examines image dipole direction in addition to position, and uses the stored force vector that is closest to the dipole direction for interpolation and output to subsequent steps.</p>
    <p num="p-0207"> <figref idrefs="DRAWINGS">FIG. 18</figref> gives details for the rotate module <b>1530</b> of <figref idrefs="DRAWINGS">FIG. 15</figref>. Inputs are the normal tensor <b>1800</b>, force <b>1810</b>, and image dipole position in field coordinates <b>1820</b>. Output is image dipole position in force coordinates pos(force) <b>1830</b>, computed as shown, and as described above by equations 11 and 12.</p>
    <p num="p-0208"> <figref idrefs="DRAWINGS">FIG. 19</figref> shows various preferred fuzzy logic processing modules that are used in evaluate module <b>1545</b> of a preferred embodiment illustrated in <figref idrefs="DRAWINGS">FIG. 20</figref>.</p>
    <p num="p-0209"> <figref idrefs="DRAWINGS">FIG. 19</figref> <i>a </i>shows a fuzzy greater than module, which takes a real-valued input <b>1900</b> and a fuzzy threshold <b>1904</b>, and produces a fuzzy logic value <b>1912</b>. The fuzzy threshold <b>1904</b> is an ordered pair that specifies points along the x axis of graph <b>1908</b>. The graph <b>1908</b> shows the fuzzy logic output <b>1912</b> as a function of input <b>1900</b>. As can be seen, the fuzzy logic value falls within the range 0.0 to 1.0, inclusive.</p>
    <p num="p-0210"> <figref idrefs="DRAWINGS">FIG. 19</figref> <i>b </i>shows a fuzzy less than module, which takes a real-valued input <b>1930</b> and a fuzzy threshold <b>1934</b>, and produces a fuzzy logic value <b>1942</b>. The fuzzy threshold <b>1934</b> is an ordered pair that specifies points along the x axis of graph <b>1938</b>. The graph <b>1938</b> shows the fuzzy logic output <b>1942</b> as a function of input <b>1930</b>. As can be seen, the fuzzy logic value falls within the range 0.0 to 1.0, inclusive.</p>
    <p num="p-0211"> <figref idrefs="DRAWINGS">FIG. 19</figref> <i>c </i>shows a fuzzy not module. Fuzzy logic value input <b>1960</b> is inverted by subtracting it from 1 to produce fuzzy logic value output <b>1964</b>.</p>
    <p num="p-0212"> <figref idrefs="DRAWINGS">FIG. 20</figref> is a blocs diagram of a preferred embodiment of the evaluate module <b>1545</b> of <figref idrefs="DRAWINGS">FIG. 15</figref>. <figref idrefs="DRAWINGS">FIG. 20</figref> <i>a </i>shows the portion responsible for computing the weight and eval values, and <figref idrefs="DRAWINGS">FIG. 20</figref> <i>b </i>shows the portion responsible for computing the clutter value.</p>
    <p num="p-0213">Referring to <figref idrefs="DRAWINGS">FIG. 20</figref> <i>a</i>, the computation of weight and eval is based on the force magnitude f, a comparison of the force direction of, and image dipole direction dir(field), and the image dipole's gradient magnitude mag. For each of these three factors in the evaluation, a fuzzy logic value is produced by fuzzy logic modules <b>2004</b>, <b>2040</b>, <b>2064</b>, respectively, that indicates confidence in the reliability of the evidence provided by the image dipole being evaluated. The three fuzzy confidence factors so-produced are combined into a single confidence score by a combination module <b>2080</b> in the range 0.0 to 1.0. The weight and eval outputs are obtained by using the eval code <b>440</b> (<figref idrefs="DRAWINGS">FIG. 4</figref>) to select either the confidence score or the value 0.0.</p>
    <p num="p-0214">Absolute value module <b>2002</b> computes the length of the force vector from force magnitude , which in the preferred embodiment being described may be negative if the force and gradient directions differ. Fuzzy less than module <b>2004</b> compares the force length to a field strength threshold <b>2000</b>, to produce a strength confidence factor that indicates high confidence for force lengths below the field strength threshold.</p>
    <p num="p-0215">The field strength threshold <b>2000</b> is set based on pattern parameters <b>220</b> for the first attraction step. In a preferred embodiment, the first attraction step uses field strength threshold values t<sub>zero</sub> <b>=2.0 field grid units, and t</b> <sub>one</sub>=3.0 field grid units.</p>
    <p num="p-0216">In the embodiment shown in <figref idrefs="DRAWINGS">FIG. 20</figref>, the field strength threshold <b>2000</b> is modified after each attraction step based on the rms error from the previous step. The modification is accomplished by addition module <b>2012</b>, which adds the rms error to both the t<sub>zero </sub>and t<sub>one </sub>components of a field strength margin parameter <b>2008</b> to produce the new field strength threshold <b>2000</b>. As a result, the field strength threshold <b>2000</b> is matched to how well the particular run-time image being analyzed fits the stored pattern at each attraction step.</p>
    <p num="p-0217">The method of adjusting the field strength threshold based on the rms error of the previous step is effective in some applications, but in other cases it has been observed to result in some oscillation of the attraction rather than convergence on one solution. In a preferred embodiment, not shown, the field strength threshold is reduced in equal steps after each attraction step. Thus, as the attraction converges to a solution, image dipoles must be closer to pattern boundaries to be given high confidence.</p>
    <p num="p-0218">The image dipole direction dir(field) is compared with the pattern boundary gradient direction by subtract module <b>2024</b>. Recall that in the embodiment being described, the force direction <sub>f </sub>reported by the field is actually boundary gradient direction, which is the same as or opposite of the true force direction. If pattern parameters <b>220</b> and polarity code, <b>460</b> of flags <b>430</b> specify that gradient polarity is to be ignored, the angle difference from subtract module <b>2024</b> is constrained to the range 90 to +90 by mod 180 module <b>2028</b>; otherwise, the angle <sub>f </sub>is passed unmodified. The magnitude of the resulting angle difference is determined by absolute value module <b>2032</b>.</p>
    <p num="p-0219">The angle difference magnitude is compared to one of two fuzzy thresholds by fuzzy less than module <b>2040</b> to produce a direction confidence factor. If corner code <b>450</b> of flags <b>430</b> indicates no corner, the field direction threshold <b>2044</b> is chosen by selection module <b>2036</b>. If the corner code indicates is corner, the field corner threshold <b>2048</b> is chosen. In a preferred embodiment, the field direction threshold <b>2044</b> has values t<sub>zero</sub>=11.25 and t<sub>one</sub>=22.5, and the field corner threshold <b>2048</b> has values t<sub>zero</sub>=39.375 and t<sub>one</sub>=50.625, reflecting the fact that a wider range of image dipole directions can reasonably correspond to a pattern boundary corner. In an alternate embodiment, a real-valued measure of curvature can be used instead of the binary is corner code, with multiple values of the field direction threshold possible.</p>
    <p num="p-0220">The image dipole's gradient magnitude mag is compared to a fuzzy magnitude threshold <b>2060</b> by fuzzy greater than module <b>2064</b> to produce a magnitude confidence factor. The magnitude threshold is intended to throw out very weak dipoles that are likely due to image noise or other artifacts, but the use of a fuzzy threshold gives more stable results than the more traditional hard threshold. In a preferred embodiment, the magnitude threshold <b>2060</b> uses the same value for t<sub>zero </sub> as the noise threshold chosen for the peak detector <b>350</b>, and uses a value of t<sub>one </sub>equal to twice the value of t<sub>zero</sub>.</p>
    <p num="p-0221">The strength, direction, and magnitude confidence factors are combined by multiply module <b>2080</b> to produce an overall confidence score in the range 0 to 1. Based on eval code <b>440</b> of flags <b>430</b>, the selection module <b>2084</b> chooses a value for weight and the selection module <b>2088</b> chooses a value for eval. If the eval code is attract, the confidence score is chosen for the weight; otherwise the constant 0 is chosen so that the dipole is ignored for localization purposes. If the eval code is attract or evaluate only, the confidence score is chosen for eval; otherwise the constant 0 is chosen to indicate that the dipole does not correspond to any portion of the pattern.</p>
    <p num="p-0222"> <figref idrefs="DRAWINGS">FIG. 20</figref> <i>b </i>shows a preferred embodiment for the calculation of the clutter value. The direction confidence factor produced by fuzzy less than module <b>2040</b> is inverted by fuzzy not element <b>2042</b>. The image dipole's gradient magnitude is compared to a fuzzy clutter threshold <b>2070</b> by fuzzy greater than module <b>2074</b> to produce a clutter confidence factor <b>2075</b>. The clutter confidence factor <b>2075</b> is multiplied by the inverted direction confidence factor by multiplier <b>2090</b> to produce a tentative clutter value <b>2091</b>. If the eval code <b>440</b> is anything but don't care, selection module <b>2092</b> chooses this tentative clutter value <b>2091</b> as the dipole's clutter value; otherwise the constant 0 is chosen.</p>
    <p num="p-0223">If the eval code <b>440</b> is expect blank, the force magnitude mag is meaningless, but in a preferred embodiment, the force direction <sub>71 </sub> encodes the gradient direction from the training image as described above in conjunction with <figref idrefs="DRAWINGS">FIG. 5</figref>. In this case, the computation of clutter uses this direction as it would the force direction <sub></sub>. This mode of operation is appropriate when it is desirable to minimize false alarms. Alternatively, if it is appropriate to minimize the chances of missing clutter, one can consider only the clutter confidence factor <b>275</b> when the eval code is expect blank.</p>
    <p num="p-0224">In a preferred embodiment, the clutter threshold <b>2070</b> has values of t<sub>zero </sub>and t<sub>one</sub>, each value being equal to 1.5 times the t<sub>zero </sub>and t<sub>one </sub>values used for the magnitude threshold <b>2060</b>.</p>
    <p num="p-0225">In a preferred embodiment, the magnitude confidence factors and clutter confidence factors for all of the image dipoles are computed once and stored in the image dipole list <b>1300</b>, rather than being recomputed for each attract step. This can be done because these confidence factors are independent of the current pose <b>1500</b>.</p>
    <p num="p-0226"> <figref idrefs="DRAWINGS">FIG. 21</figref> is a block diagram of sums module <b>1535</b> of <figref idrefs="DRAWINGS">FIG. 15</figref>. This module accumulates the weighted sums needed for the solution of the least-squares problem of equation 15. Five multiply modules <b>2100</b> perform the weighting. Fifteen multiply-accumulate modules <b>2130</b> and one accumulate module <b>2160</b> compute and store the sums needed. The sixteen accumulators are set to zero at the beginning of each attraction step. This four degree-of-freedom case is an exemplary embodiment, other numbers of degrees of freedom being possible.</p>
    <p num="p-0227"> <figref idrefs="DRAWINGS">FIGS. 22</figref> <i>ad </i>give details of the solve module <b>1540</b> of <figref idrefs="DRAWINGS">FIG. 15</figref>, which produces the motion transform and the rms error value. The formulas shown are based on the solution of the least-squares problem of equation <b>15</b>. Pattern parameters <b>220</b> specify which degrees of freedom are to be determined.</p>
    <p num="p-0228"> <figref idrefs="DRAWINGS">FIG. 22</figref> <i>a </i>shows the solution for the 2 translation degrees of freedom onlysize and orientation are as specified in the start pose. <figref idrefs="DRAWINGS">FIG. 22</figref> <i>b </i>shows the solution for translation and orientation. This preferred solution is based on an approximation that assumes a small angle of rotation. If the assumption is violated, some size variation will be introduced. <figref idrefs="DRAWINGS">FIG. 22</figref> <i>c </i>shows the solution for translation and size, holding orientation fixed, and <figref idrefs="DRAWINGS">FIG. 22</figref> <i>d </i>shows the solution for all 4 degrees of freedom.</p>
    <p num="p-0229"> <figref idrefs="DRAWINGS">FIG. 23</figref> gives details of the compose module <b>1505</b> of <figref idrefs="DRAWINGS">FIG. 15</figref>, which composes the current pose <b>2300</b> with the motion transform <b>2330</b> computed by the solve module <b>1540</b> to produce the new pose <b>2360</b>.</p>
    <p num="p-0230"> <figref idrefs="DRAWINGS">FIG. 24</figref> gives details of the normal tensor module <b>1510</b> of <figref idrefs="DRAWINGS">FIG. 15</figref>, which computes the normal tensor <b>2460</b> from the current pose <b>2400</b> and the client map <b>2430</b>.</p>
    <p num="p-0231"> <figref idrefs="DRAWINGS">FIG. 25</figref> shows an example of field dipole evaluation performed as part of field dipole evaluation module <b>1550</b> of <figref idrefs="DRAWINGS">FIG. 15</figref>. In the example, a first image dipole <b>2500</b>, second image dipole <b>2510</b>, and third image dipole <b>2520</b> have received evaluations 0.85, 0.93, and 0.88 respectively. Four field dipoles labeled <b>2540</b>, <b>2550</b>, <b>2560</b>, and <b>2570</b> lie along a chain as determined by connect step <b>254</b> during training module <b>110</b>. The chain is defined by the left links <b>2580</b> and right links <b>2585</b>.</p>
    <p num="p-0232">For image dipole <b>2500</b>, an index <b>2505</b> was determined by field module <b>1525</b> to identify the nearest field dipole <b>2540</b>. The evaluation <b>0</b>.<b>85</b> is transferred from image dipole <b>2500</b> to the eval<b>1</b> slot of field dipole <b>2540</b>.</p>
    <p num="p-0233">No image dipole identified field dipole <b>2550</b> as nearest, so its eval<b>1</b> slot holds its initial value 0.</p>
    <p num="p-0234">For image dipole <b>2510</b>, an index <b>2515</b> was determined to identify the nearest field dipole <b>2560</b>. For image dipole <b>2520</b>, the same index <b>2515</b> was determined to identify the nearest field dipole <b>2560</b>. The larger of image dipole <b>2510</b> evaluation 0.93 and image dipole <b>2520</b> evaluation 0.88 is transferred to the eval<b>1</b> slot of field dipole <b>2560</b>.</p>
    <p num="p-0235">Field dipole <b>2570</b> has evaluation 0.90 transferred from some image dipole not shown.</p>
    <p num="p-0236">To fill in the gap at field dipole <b>2550</b>, a dilation operation is performed, wherein all field dipoles receive an evaluation equal to the maximum of their own evaluation and that of their left and right neighbors. The dilated evaluations are shown in the eval<b>2</b> slot of each field dipole. Note that it is not actually necessary to store both eval<b>1</b> and eval<b>2</b> values; <figref idrefs="DRAWINGS">FIG. 25</figref> shows them for clarity.</p>
    <p num="p-0237">Once the field dipoles have been evaluated, the coverage value produced by field dipole evaluation module <b>1550</b> is computed by averaging all of the field dipole evaluations.</p>
    <p num="p-0238">In a preferred embodiment, the field dipoles are evaluated and coverage is computed only after the last attraction step, and only if pattern inspection is desired.</p>
    <p num="p-0239"> <figref idrefs="DRAWINGS">FIG. 26</figref> shows how the invention can be operated in a multi-resolution mode designed to increase the capture range without sacrificing accuracy. Two pattern training modules (not shown) are run on a single training image, with different settings of low-pass filter module <b>310</b> and image sub-sample module <b>320</b>. In a first setting designed to attenuate fine detail, a low resolution pattern <b>2600</b> is generated. In a second setting designed to pass fine detail, a high resolution pattern <b>2610</b> is generated.</p>
    <p num="p-0240">A low resolution run-time module <b>2620</b> uses the low resolution pattern <b>2600</b>, and a start pose and client map, to analyze run-time image <b>130</b> to produce a low resolution pose that is much more accurate than the start pose but not as accurate as can be achieved at higher resolution. A high resolution run-time module <b>2630</b> uses the high resolution pattern <b>2610</b>, the low resolution pose as a start pose, and the same client map, to analyze run-time image <b>130</b> to produce the final pose, rms error, coverage, and evaluated dipole lists.</p>
    <p num="p-0241">The multi-resolution mode is supervised by overall control module <b>2640</b>, as illustrated by the flow chart in <figref idrefs="DRAWINGS">FIG. 28</figref>. As part of its operation <b>2800</b>, the low resolution rms error, coverage, and clutter values are examined, and if <b>2802</b> they do not indicate a reasonable match between image and stored pattern, the operation is aborted <b>2803</b> without attempting to run the high resolution module. If the low resolution module produces a good match, high resolution module is run <b>2804</b>. If the high resolution module does not produce good results <b>2806</b>, it usually means that the image is out of focus, and the user is so-warned. In some embodiments, when this happens, the low resolution results are used instead of the high resolution results <b>2808</b>. If the results of the high resolution module are acceptable, the results of the high resolution module are provided to the user <b>2810</b> for interpretation, or further processing, according to the particular application.</p>
    <p num="p-0242">In a preferred embodiment, an overall match score is computed for each resolution step that is equal to the coverage value minus half the clutter value. The low resolution results are used instead of the high resolution results if the high resolution match score is less than some fraction of the low resolution match score. In a preferred embodiment, the fraction used is 0.9.</p>
    <p num="p-0243">In a preferred embodiment, the methods of U.S. Pat. No. 6,457,032, issued Sep. 24, 2002, entitled Efficient, Flexible Digital Filtering, and U.S. Pat. No. 6,408,109, issued Jun. 18, 2002, entitled Apparatus and Method for Detecting and Sub-Pixel Location of Edges in a Digital Image are used for feature extraction, Cognex Corporation's PatQuick tool is used to determine the starting pose, and the multi-resolution style of <figref idrefs="DRAWINGS">FIG. 26</figref> is used. The following parameter settings are used for feature extraction by default. Many other strategies can be devised to suit specific applications.</p>
    <p num="p-0244">For training the low resolution pattern <b>2600</b>, and corresponding run-time module <b>2620</b>, the image is sub-sampled by sub-sampler <b>320</b> by an equal amount in x and y given by the formula</p>
    <p num="p-0245">
      <maths id="MATH-US-00011" num="00011"> <math overflow="scroll"> <mrow> <mi>floor</mi> <mo></mo> <mrow> <mo>(</mo> <msqrt> <mfrac> <msqrt> <mi>wh</mi> </msqrt> <mn>8</mn> </mfrac> </msqrt> <mo>)</mo> </mrow> </mrow> </math> </maths> </p> <ul> <li id="ul0003-0001" num="0000">
          <ul> <li id="ul0004-0001" num="0251">where w and h are the width and height, respectively, of the pattern <b>100</b> in pixels and the floor function gives the largest integer that is less than or equal to its argument. Note that sub-sampling by n means taking every n<sup>th </sup>pixel. The low-pass filter <b>310</b> uses a filter size parameter (s in U.S. Pat. No. 6,457,032, issued Sep. 24, 2002, entitled Efficient Flexible Digital Filtering) equal to one less than the computed sub-sample amount. The Cartesian to polar conversion module <b>340</b> multiplies the gradient magnitude values by 2.0 to improve precision at the low end, where most gradient values lie.</li>
          </ul> </li>
      </ul> <p num="p-0246">For training the high resolution pattern <b>2610</b>, and corresponding run-time module <b>2630</b>, the low-pass filter <b>310</b> and the sub-sampler <b>320</b> are set to pass the source image <b>300</b> unmodified.</p>
    <p num="p-0247">As part of its operation, the PatQuick tool reports a contrast value in gray levels that is the median gradient magnitude of the pixels in the image on which it is run that correspond to the trained pattern. In a preferred embodiment, this contrast value is used to set the default noise threshold for the peak detector <b>350</b>. Many other schemes for setting noise thresholds are known in the art that can be used to achieve equivalent results.</p>
    <p num="p-0248">In said preferred embodiment, PatQuick is run on the training image <b>100</b> and the contrast value reported by the tool is saved as part of the pattern parameters <b>220</b>. For training the low resolution pattern <b>2600</b>, the peak detection module <b>350</b> uses a noise threshold equal to 10 gray levels. For training the high resolution pattern <b>2610</b>, the peak detection module <b>350</b> uses a noise threshold equal to one-quarter of said saved contrast.</p>
    <p num="p-0249">For the run-time image <b>130</b>, when the PatQuick tool is used to determine the starting pose the contrast value it reports is examined. For both the low resolution run-time module <b>2620</b>, and the high resolution run-time module <b>2630</b>, the peak detection module <b>350</b> uses a noise threshold equal to that used for the corresponding pattern <b>2600</b> or <b>2610</b>, but in each case multiplied by the ratio of run-time contrast to the saved train-time contrast.</p>
    <p num="p-0250">The preferred embodiments described herein use a six-degree-of-freedom coordinate transform to represent the mapping between points in the image and points in the pattern (i.e. the pose), and a least-squares fitting to determine how to use the information provided by the field to modify a given pose so as to produce a new pose that represents a better correspondence between image and pattern features. Many other arrangements can be devised by those of ordinary skill in the art for achieving similar results within the scope of the invention. These other arrangements may have advantages in certain specific applications.</p>
    <p num="p-0251">For example, the six degree of freedom coordinate transform can be replaced with other analytic models of the mapping between points in the image and points in the pattern. One useful such model is the well-known perspective transform. Another useful model is one that corrects for lens distortions, such as that produced by so-called fisheye lenses. In these cases a different least squares solution would be used, and appropriate changes would be made to the pose element <b>1500</b>, the compose module <b>1505</b>, the normal tensor module <b>1510</b>, the map module <b>1520</b>, the rotate module <b>1530</b>, the sums module <b>1535</b>, and the solve module <b>1540</b>. The image dipole <b>1515</b>, field module <b>1525</b>, evaluate module <b>1545</b>, and field dipole evaluation module <b>1550</b> need not change.</p>
    <p num="p-0252">In other arrangements, the least squares method can be replaced with other well-known methods for fitting data. In such arrangements, appropriate changes might be made to the rotate module <b>1530</b>, the sums module <b>1535</b>, and the solve module <b>1540</b>. Alternatively, one or more of these modules might be replaced by different modules that are required for the fitting method to be used.</p>
    <p num="p-0253">In still other arrangements, a non-analytic mapping between points in the image and points in the pattern, such as a 2-dimensional lookup table with interpolation, may be used. In such an arrangement, the pose <b>1500</b> is a lookup table mapping image points to pattern points, and the map module <b>1520</b> does the lookup and interpolation. The field module <b>1525</b> and evaluate module <b>1545</b> can be used without modification. The compose module <b>1505</b>, normal tensor module <b>1510</b>, rotate module <b>1530</b>, sums module <b>1535</b>, and solve module <b>1540</b> are not used. Instead, an intermediate lookup table is produced as follows. For every image dipole <b>1515</b>, an entry is made in the intermediate lookup table by adding the force vector obtained from the field module <b>1525</b> to the mapped position from map module <b>1520</b>. Along with this field-corrected position, the weight obtained from the evaluate module <b>1545</b> is also stored in the intermediate table entry.</p>
    <p num="p-0254">The intermediate table thus produced may be sparse, in that many points will not have been filled in, and it may have errors caused by the occasional unreliable image dipole. It can be used, however, to produce a new pose <b>1500</b> by applying a smoothness constraint. For example, each element of the new pose can be determined by a weighted mean or median of some neighborhood of corresponding elements of the intermediate table. Other methods for using smoothness as a constraint are well-known in the machine vision literature.</p>
    <p num="p-0255">Other modifications and implementations will occur to those skilled in the art without departing from the spirit and the scope of the invention as claimed. Accordingly, the above description is not intended to limit the invention, except as indicated in the following claims.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3069654">US3069654</a></td><td class="patent-data-table-td patent-date-value">Mar 25, 1960</td><td class="patent-data-table-td patent-date-value">Dec 18, 1962</td><td class="patent-data-table-td ">Hough Paul V C</td><td class="patent-data-table-td ">Method and means for recognizing complex patterns</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3986007">US3986007</a></td><td class="patent-data-table-td patent-date-value">Nov 28, 1975</td><td class="patent-data-table-td patent-date-value">Oct 12, 1976</td><td class="patent-data-table-td ">The Bendix Corporation</td><td class="patent-data-table-td ">Method and apparatus for calibrating mechanical-visual part manipulating system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4146924">US4146924</a></td><td class="patent-data-table-td patent-date-value">Sep 22, 1975</td><td class="patent-data-table-td patent-date-value">Mar 27, 1979</td><td class="patent-data-table-td ">Board Of Regents For Education Of The State Of Rhode Island</td><td class="patent-data-table-td ">System for visually determining position in space and/or orientation in space and apparatus employing same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4581762">US4581762</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 1984</td><td class="patent-data-table-td patent-date-value">Apr 8, 1986</td><td class="patent-data-table-td ">Itran Corporation</td><td class="patent-data-table-td ">Vision inspection system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4618989">US4618989</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 1984</td><td class="patent-data-table-td patent-date-value">Oct 21, 1986</td><td class="patent-data-table-td ">Michio Kawata, Director-General of Agency of Industrial Science and Technology</td><td class="patent-data-table-td ">Method and system for detecting elliptical objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4707647">US4707647</a></td><td class="patent-data-table-td patent-date-value">May 19, 1986</td><td class="patent-data-table-td patent-date-value">Nov 17, 1987</td><td class="patent-data-table-td ">Gmf Robotics Corporation</td><td class="patent-data-table-td ">Gray scale vision method and system utilizing same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4972359">US4972359</a></td><td class="patent-data-table-td patent-date-value">Apr 3, 1987</td><td class="patent-data-table-td patent-date-value">Nov 20, 1990</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Digital image processing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5245674">US5245674</a></td><td class="patent-data-table-td patent-date-value">Jul 30, 1991</td><td class="patent-data-table-td patent-date-value">Sep 14, 1993</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Image processing using distance as a function of direction</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5343390">US5343390</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 1992</td><td class="patent-data-table-td patent-date-value">Aug 30, 1994</td><td class="patent-data-table-td ">Arch Development Corporation</td><td class="patent-data-table-td ">Method and system for automated selection of regions of interest and detection of septal lines in digital chest radiographs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5351310">US5351310</a></td><td class="patent-data-table-td patent-date-value">Dec 8, 1993</td><td class="patent-data-table-td patent-date-value">Sep 27, 1994</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Generalized shape autocorrelation for shape acquisition and recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5515453">US5515453</a></td><td class="patent-data-table-td patent-date-value">Jan 21, 1994</td><td class="patent-data-table-td patent-date-value">May 7, 1996</td><td class="patent-data-table-td ">Beacon System, Inc.</td><td class="patent-data-table-td ">Apparatus and method for image processing in symbolic space</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5694482">US5694482</a></td><td class="patent-data-table-td patent-date-value">Jan 6, 1997</td><td class="patent-data-table-td patent-date-value">Dec 2, 1997</td><td class="patent-data-table-td ">Universal Instruments Corporation</td><td class="patent-data-table-td ">System and method for locating solder bumps on semiconductor chips or chip carriers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5703960">US5703960</a></td><td class="patent-data-table-td patent-date-value">Sep 16, 1996</td><td class="patent-data-table-td patent-date-value">Dec 30, 1997</td><td class="patent-data-table-td ">U.S. Natural Resources, Inc.</td><td class="patent-data-table-td ">Lumber defect scanning including multi-dimensional pattern recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5828769">US5828769</a></td><td class="patent-data-table-td patent-date-value">Oct 23, 1996</td><td class="patent-data-table-td patent-date-value">Oct 27, 1998</td><td class="patent-data-table-td ">Autodesk, Inc.</td><td class="patent-data-table-td ">Method and apparatus for recognition of objects via position and orientation consensus of local image encoding</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Akinori Kawamura, Koji Yura, Tatsuya Hayama, Yutaka Hidai, Tadatashi Minamikawa, Akio Tanaka and Shoichi Masuda, On-line Recognition of Freely Handwritten Japanese Characters Using Directional Features Densities, IEEE 1992.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ballard, D.H., "<a href='http://scholar.google.com/scholar?q="Generalizing+the+Hough+Transform+to+Detect+Arbitrary+Shapes%2C"'>Generalizing the Hough Transform to Detect Arbitrary Shapes,</a>" Pattern Recognition, 1981, pp. 111-122, vol. 13, No. 2, Pergaman Press Ltd., UK.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, "<a href='http://scholar.google.com/scholar?q="Chapter+7+CONLPAS%2C"'>Chapter 7 CONLPAS,</a>" Cognex 3000/4000/5000 Programmable Vision Engines, Vision Tools, 1996, pp. 307-340, Revision 7.4 590-0136, Natick, MA USA.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, "<a href='http://scholar.google.com/scholar?q="Description+Sobel+Search%2C"'>Description Sobel Search,</a>" Natick, MA USA, 1998 but public before the above-referenced filing date.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Daniel P. Huttenlocher and William J. Rucklidge, A Multi-Resolution Technique for Comparing Images Using the Hausdorff Distance, Department of Computer Science, Cornell University, Ithaca, NY 14853.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Daniel P. Huttenlocher, Gregory A. Klanderman and William J. Rucklidge, Comparing Images Unsing the Hausdorff Distance, IEEE Transaction on Pattern Analysis and Machine Intelligence, Vo. 15, No. 9, Sep. 1993.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Gunilla Borgefors, Hierarchical Chamfer Matching: A Parametric Edge Matching Algorithm, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 10, No. 6, Nov. 1988.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Hu, et al, "<a href='http://scholar.google.com/scholar?q="Expanding+the+Range+of+Convergence+of+the+CORDIC+Algorithm%2C"'>Expanding the Range of Convergence of the CORDIC Algorithm,</a>" IEEE Transactions on computers, Jan. 1991, pp. 13-21, vol. 40, No. 1, USA.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Hu, Yu Hen, "<a href='http://scholar.google.com/scholar?q="CORDIC-Based+VLSI+Architectures+for+Digital+Signal+Processing%2C"'>CORDIC-Based VLSI Architectures for Digital Signal Processing,</a>" IEE Signal Processing Magazine, Jul. 1992, pp. 16-35, 1053-5888/92, USA.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">I.J. Cox and J.B. Kruskal (AT&amp;T Bell Laboratories, Murray Hill, NJ), On the Congruence of Noisy Images to Line Segment Models, IEEE, 1988.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">James D. Foley, Andries Van Dam, Steven K. Feiner, John F. Hughes, Second Edition in C, Introduction to Computer Graphics, pp. 36-49, Addison-Wesley Publishing Company, 1994; USA.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Lin, et al., "<a href='http://scholar.google.com/scholar?q="On-Line+CORDIC+Algorithms%2C"'>On-Line CORDIC Algorithms,</a>" IEEE Transactions on Computers, pp. 1038-1052, vol. 39, No. 8, USA.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Lisa Gottesfeld Brown, A Survey of Image Registration Techniques, Department of Computer Science, Columbia University, New York, NY 10027, ACM Computing Surveys, vol. 24, No. 4, Dec. 1992.</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Wallack, Aaron Samuel, "<a href='http://scholar.google.com/scholar?q="Chapter+4+Robust+Algorithms+for+Object+Localization%2C"'>Chapter 4 Robust Algorithms for Object Localization,</a>" Algorithms and Techniques for Manugacturing, 1995, pp. 97-148 (and Bibliography pp. 324-335) PhD thesis, University of California at Berkeley, USA.</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8457390">US8457390</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 10, 2008</td><td class="patent-data-table-td patent-date-value">Jun 4, 2013</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and apparatus for training a probe model based machine vision system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8705851">US8705851</a></td><td class="patent-data-table-td patent-date-value">Jan 3, 2013</td><td class="patent-data-table-td patent-date-value">Apr 22, 2014</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and apparatus for training a probe model based machine vision system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE102012223047A1?cl=en">DE102012223047A1</a></td><td class="patent-data-table-td patent-date-value">Dec 13, 2012</td><td class="patent-data-table-td patent-date-value">Jul 11, 2013</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Mehrteil-Korrespondierer fr mehrere Kameras</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=HJF0BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S291000">382/291</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=HJF0BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S209000">382/209</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=HJF0BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S203000">382/203</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=HJF0BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S199000">382/199</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=HJF0BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0007000000">G06T7/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=HJF0BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009360000">G06K9/36</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=HJF0BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/32">G06K9/32</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=HJF0BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/6203">G06K9/6203</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=HJF0BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06T7/0046">G06T7/0046</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06T7/00P1M</span>, <span class="nested-value">G06K9/62A1A</span>, <span class="nested-value">G06K9/32</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Aug 27, 2013</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 14, 2012</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 14, 2009</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 21, 2009</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090610</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 11, 2003</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COGNEX CORPORATION, MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SILVER, WILLIAM;WALLACK, AARON S.;WAGMAN, ADAM;REEL/FRAME:014702/0137</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19980603</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U05txkcTjRtRQI9c5MS_Ka7Na7IGg\u0026id=HJF0BAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U34PGKMGXWIKr9arx63EuPqJdspIQ\u0026id=HJF0BAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U3hI8dmMFTNwdKSE4HhuDiSCMYWCg","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Fast_high_accuracy_multi_dimensional_pat.pdf?id=HJF0BAABERAJ\u0026output=pdf\u0026sig=ACfU3U2Y87TuAq3O0BWHBFqZnrVLPuONQQ"},"sample_url":"http://www.google.com/patents/reader?id=HJF0BAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>