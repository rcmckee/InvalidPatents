<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6766295 - Adaptation of a speech recognition system across multiple remote sessions ... - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Adaptation of a speech recognition system across multiple remote sessions with a speaker"><meta name="DC.contributor" content="Hy Murveit" scheme="inventor"><meta name="DC.contributor" content="Ashvin Kannan" scheme="inventor"><meta name="DC.contributor" content="Nuance Communications" scheme="assignee"><meta name="DC.date" content="1999-5-10" scheme="dateSubmitted"><meta name="DC.description" content="A technique for adaptation of a speech recognizing system across multiple remote communication sessions with a speaker. The speaker can be a telephone caller. An acoustic model is utilized for recognizing the speaker&#39;s speech. Upon initiation of a first remote session with the speaker, the acoustic model is speaker-independent. During the first session, the speaker is uniquely identified and speech samples are obtained from the speaker. In the preferred embodiment, the samples are obtained without requiring the speaker to engage in a training session. The acoustic model is then modified based upon the samples thereby forming a modified model. The model can be modified during the session or after the session is terminated. Upon termination of the session, the modified model is then stored in association with an identification of the speaker. During a subsequent remote session, the speaker is identified and, then, the modified acoustic model is utilized to recognize the speaker&#39;s speech. Additional speech samples are obtained during the subsequent session and, then, utilized to further modify the acoustic model. In this manner, an acoustic model utilized for recognizing the speech of a particular speaker is cumulatively modified according to speech samples obtained during multiple sessions with the speaker. As a result, the accuracy of the speech recognizing system improves for the speaker even when the speaker only engages in relatively short remote sessions."><meta name="DC.date" content="2004-7-20" scheme="issued"><meta name="DC.relation" content="EP:0651372:A2" scheme="references"><meta name="DC.relation" content="EP:1022725:A1" scheme="references"><meta name="DC.relation" content="JP:H1079785" scheme="references"><meta name="DC.relation" content="US:4489434" scheme="references"><meta name="DC.relation" content="US:4590604" scheme="references"><meta name="DC.relation" content="US:4866778" scheme="references"><meta name="DC.relation" content="US:5127055" scheme="references"><meta name="DC.relation" content="US:5208897" scheme="references"><meta name="DC.relation" content="US:5239586" scheme="references"><meta name="DC.relation" content="US:5293452" scheme="references"><meta name="DC.relation" content="US:5528731" scheme="references"><meta name="DC.relation" content="US:5568540" scheme="references"><meta name="DC.relation" content="US:5617486" scheme="references"><meta name="DC.relation" content="US:5651054" scheme="references"><meta name="DC.relation" content="US:5717743" scheme="references"><meta name="DC.relation" content="US:5742905" scheme="references"><meta name="DC.relation" content="US:5778338" scheme="references"><meta name="DC.relation" content="US:5794192" scheme="references"><meta name="DC.relation" content="US:5799065" scheme="references"><meta name="DC.relation" content="US:5822405" scheme="references"><meta name="DC.relation" content="US:5842161" scheme="references"><meta name="DC.relation" content="US:5848130" scheme="references"><meta name="DC.relation" content="US:5897616" scheme="references"><meta name="DC.relation" content="US:6044346" scheme="references"><meta name="DC.relation" content="US:6070140" scheme="references"><meta name="DC.relation" content="US:6151571" scheme="references"><meta name="DC.relation" content="US:6181780" scheme="references"><meta name="DC.relation" content="US:6185535" scheme="references"><meta name="DC.relation" content="US:6208713" scheme="references"><meta name="DC.relation" content="US:6219407" scheme="references"><meta name="DC.relation" content="US:6275806" scheme="references"><meta name="DC.relation" content="US:6327343" scheme="references"><meta name="DC.relation" content="US:6334103" scheme="references"><meta name="DC.relation" content="US:6363348" scheme="references"><meta name="citation_reference" content="&quot;A Fast Algorithm For Unsupervised Incremental Speaker Adaption&quot; Michael Schubler, Florian Gallwitz, Stefan Harbeck, Apr. 21, 1997, pp. 1019-1022, IEEE."><meta name="citation_reference" content="&quot;Integration Of Speaker And Speech Recognition Systems&quot; D.A. Reynolds and L. P. Heck, 1991, pp. 869-872, IEEE."><meta name="citation_reference" content="&quot;Iterative Self-Learning Speaker And Channel Adaptation Under Various Initial Conditions&quot; Yunxin Zhao, May 9, 1995, pp. 712-715, IEEE."><meta name="citation_reference" content="&quot;Vector-Field-Smoothed Bayesian Learning For Fast And Incremental Speaker/Telephone-Channel Adaption&quot; Jun-ichi Takahashi And Shigeki Sagayama, 1997, pp. 127-146, Computer Speech And Language."><meta name="citation_reference" content="Reynolds et al.; Integration of speaker and speech recognition systems; IEEE; pp. 869-872 vol. 2.*"><meta name="citation_patent_number" content="US:6766295"><meta name="citation_patent_application_number" content="US:09/309,211"><link rel="canonical" href="http://www.google.com/patents/US6766295"/><meta property="og:url" content="http://www.google.com/patents/US6766295"/><meta name="title" content="Patent US6766295 - Adaptation of a speech recognition system across multiple remote sessions with a speaker"/><meta name="description" content="A technique for adaptation of a speech recognizing system across multiple remote communication sessions with a speaker. The speaker can be a telephone caller. An acoustic model is utilized for recognizing the speaker&#39;s speech. Upon initiation of a first remote session with the speaker, the acoustic model is speaker-independent. During the first session, the speaker is uniquely identified and speech samples are obtained from the speaker. In the preferred embodiment, the samples are obtained without requiring the speaker to engage in a training session. The acoustic model is then modified based upon the samples thereby forming a modified model. The model can be modified during the session or after the session is terminated. Upon termination of the session, the modified model is then stored in association with an identification of the speaker. During a subsequent remote session, the speaker is identified and, then, the modified acoustic model is utilized to recognize the speaker&#39;s speech. Additional speech samples are obtained during the subsequent session and, then, utilized to further modify the acoustic model. In this manner, an acoustic model utilized for recognizing the speech of a particular speaker is cumulatively modified according to speech samples obtained during multiple sessions with the speaker. As a result, the accuracy of the speech recognizing system improves for the speaker even when the speaker only engages in relatively short remote sessions."/><meta property="og:title" content="Patent US6766295 - Adaptation of a speech recognition system across multiple remote sessions with a speaker"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("WFfsU4rjAtDMsASQmoDICA"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("USA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("WFfsU4rjAtDMsASQmoDICA"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("USA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6766295?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6766295"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=1nFnBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6766295&amp;usg=AFQjCNHWB_9tQOR8mFRVC1-vj3nwaadodA" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6766295.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6766295.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6766295" style="display:none"><span itemprop="description">A technique for adaptation of a speech recognizing system across multiple remote communication sessions with a speaker. The speaker can be a telephone caller. An acoustic model is utilized for recognizing the speaker&#39;s speech. Upon initiation of a first remote session with the speaker, the acoustic model...</span><span itemprop="url">http://www.google.com/patents/US6766295?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6766295 - Adaptation of a speech recognition system across multiple remote sessions with a speaker</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6766295 - Adaptation of a speech recognition system across multiple remote sessions with a speaker" title="Patent US6766295 - Adaptation of a speech recognition system across multiple remote sessions with a speaker"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6766295 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/309,211</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Jul 20, 2004</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">May 10, 1999</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">May 10, 1999</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/WO2000068933A1">WO2000068933A1</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">09309211, </span><span class="patent-bibdata-value">309211, </span><span class="patent-bibdata-value">US 6766295 B1, </span><span class="patent-bibdata-value">US 6766295B1, </span><span class="patent-bibdata-value">US-B1-6766295, </span><span class="patent-bibdata-value">US6766295 B1, </span><span class="patent-bibdata-value">US6766295B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Hy+Murveit%22">Hy Murveit</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Ashvin+Kannan%22">Ashvin Kannan</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Nuance+Communications%22">Nuance Communications</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6766295.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6766295.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6766295.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (34),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (5),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (55),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (9),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (9)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=1nFnBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6766295&usg=AFQjCNEXVqV1ljekF1hgojGTtq9157diZw">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=1nFnBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6766295&usg=AFQjCNEYSYhT0t4zh-qDTG1dGXWWG1pcLw">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=1nFnBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6766295B1%26KC%3DB1%26FT%3DD&usg=AFQjCNH9X8ywQAok1rdc1RfjgxIpV3PKNw">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55289163" lang="EN" load-source="patent-office">Adaptation of a speech recognition system across multiple remote sessions with a speaker</invention-title></span><br><span class="patent-number">US 6766295 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA50695181" lang="EN" load-source="patent-office"> <div class="abstract">A technique for adaptation of a speech recognizing system across multiple remote communication sessions with a speaker. The speaker can be a telephone caller. An acoustic model is utilized for recognizing the speaker's speech. Upon initiation of a first remote session with the speaker, the acoustic model is speaker-independent. During the first session, the speaker is uniquely identified and speech samples are obtained from the speaker. In the preferred embodiment, the samples are obtained without requiring the speaker to engage in a training session. The acoustic model is then modified based upon the samples thereby forming a modified model. The model can be modified during the session or after the session is terminated. Upon termination of the session, the modified model is then stored in association with an identification of the speaker. During a subsequent remote session, the speaker is identified and, then, the modified acoustic model is utilized to recognize the speaker's speech. Additional speech samples are obtained during the subsequent session and, then, utilized to further modify the acoustic model. In this manner, an acoustic model utilized for recognizing the speech of a particular speaker is cumulatively modified according to speech samples obtained during multiple sessions with the speaker. As a result, the accuracy of the speech recognizing system improves for the speaker even when the speaker only engages in relatively short remote sessions.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(6)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6766295B1/US06766295-20040720-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6766295B1/US06766295-20040720-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6766295B1/US06766295-20040720-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6766295B1/US06766295-20040720-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6766295B1/US06766295-20040720-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6766295B1/US06766295-20040720-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6766295B1/US06766295-20040720-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6766295B1/US06766295-20040720-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6766295B1/US06766295-20040720-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6766295B1/US06766295-20040720-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6766295B1/US06766295-20040720-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6766295B1/US06766295-20040720-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(55)</span></span></div><div class="patent-text"><div mxw-id="PCLM8704169" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is: </claim-statement> <div class="claim"> <div num="1" id="US-6766295-B1-CLM-00001" class="claim">
      <div class="claim-text">1. A method of adapting a speech recognition system, wherein the method comprises steps of:</div>
      <div class="claim-text">a. obtaining an identification of a speaker; </div>
      <div class="claim-text">b. obtaining a sample of a speaker's speech during a first remote session; </div>
      <div class="claim-text">c. recognizing the speaker's speech utilizing the speech recognition system during the first remote session; </div>
      <div class="claim-text">d. modifying the speech recognition system by incorporating the sample into the speech recognition system thereby forming a speaker-specific modified speech recognition system; </div>
      <div class="claim-text">e. storing a representation of the speaker-specific modified speech recognition system in association with the identification of the speaker; and </div>
      <div class="claim-text">f. using the representation of the speaker-specific modified speech recognition system to recognize speech during a subsequent remote session with the speaker. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" id="US-6766295-B1-CLM-00002" class="claim">
      <div class="claim-text">2. The method according to <claim-ref idref="US-6766295-B1-CLM-00001">claim 1</claim-ref> further comprising a step of cumulatively modifying the speech recognition system according to speech samples obtained during one or more remote sessions with the speaker.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" id="US-6766295-B1-CLM-00003" class="claim">
      <div class="claim-text">3. The method according to <claim-ref idref="US-6766295-B1-CLM-00001">claim 1</claim-ref> wherein the speaker is a telephone caller.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" id="US-6766295-B1-CLM-00004" class="claim">
      <div class="claim-text">4. The method according to <claim-ref idref="US-6766295-B1-CLM-00001">claim 1</claim-ref> wherein the step of modifying the speech recognition system comprises a step of modifying an acoustic model thereby forming a speaker-specific modified acoustic model and wherein the step of storing a representation of the speaker-specific modified speech recognition system comprises a step of storing a representation of the modified acoustic model.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" id="US-6766295-B1-CLM-00005" class="claim">
      <div class="claim-text">5. The method according to <claim-ref idref="US-6766295-B1-CLM-00004">claim 4</claim-ref> wherein the representation of the speaker-specific modified acoustic model is a set of statistics which can be utilized to modify a pre-existing acoustic model.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" id="US-6766295-B1-CLM-00006" class="claim">
      <div class="claim-text">6. The method according to <claim-ref idref="US-6766295-B1-CLM-00004">claim 4</claim-ref> wherein the representation of the speaker-specific modified acoustic model is a set of statistics which can be utilized to modify incoming acoustic speech.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" id="US-6766295-B1-CLM-00007" class="claim">
      <div class="claim-text">7. The method according to <claim-ref idref="US-6766295-B1-CLM-00001">claim 1</claim-ref> further comprising a step of utilizing the speaker-specific modified speech recognition system during the first remote session with the speaker.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" id="US-6766295-B1-CLM-00008" class="claim">
      <div class="claim-text">8. The method according to <claim-ref idref="US-6766295-B1-CLM-00001">claim 1</claim-ref> wherein the speech recognition system is speaker-independent prior to the first remote session.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" id="US-6766295-B1-CLM-00009" class="claim">
      <div class="claim-text">9. The method according to <claim-ref idref="US-6766295-B1-CLM-00001">claim 1</claim-ref> wherein the step of modifying the speech recognition system is performed during the first remote session.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" id="US-6766295-B1-CLM-00010" class="claim">
      <div class="claim-text">10. The method according to <claim-ref idref="US-6766295-B1-CLM-00001">claim 1</claim-ref> wherein the step of modifying the speech recognition system is performed after termination of the first remote session.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" id="US-6766295-B1-CLM-00011" class="claim">
      <div class="claim-text">11. The method according to <claim-ref idref="US-6766295-B1-CLM-00001">claim 1</claim-ref> further comprising a step of authenticating the speaker's identification by the speaker's speech.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" id="US-6766295-B1-CLM-00012" class="claim">
      <div class="claim-text">12. The method according to <claim-ref idref="US-6766295-B1-CLM-00002">claim 2</claim-ref> wherein the speech recognition system is speaker-independent prior to the first remote session.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" id="US-6766295-B1-CLM-00013" class="claim">
      <div class="claim-text">13. The method according to <claim-ref idref="US-6766295-B1-CLM-00002">claim 2</claim-ref> wherein the step of modifying the speech recognition system is performed during the first remote session.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" id="US-6766295-B1-CLM-00014" class="claim">
      <div class="claim-text">14. The method according to <claim-ref idref="US-6766295-B1-CLM-00002">claim 2</claim-ref> wherein the step of modifying the speech recognition system is performed after termination of the first remote session.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" id="US-6766295-B1-CLM-00015" class="claim">
      <div class="claim-text">15. The method according to <claim-ref idref="US-6766295-B1-CLM-00002">claim 2</claim-ref> further comprising a step of authenticating the speaker's identification by the speaker's speech.</div>
    </div>
    </div> <div class="claim"> <div num="16" id="US-6766295-B1-CLM-00016" class="claim">
      <div class="claim-text">16. A method of adapting a speech recognition system, wherein the method comprises steps of:</div>
      <div class="claim-text">a. obtaining an identification of a cluster of speakers; </div>
      <div class="claim-text">b. obtaining a sample of a speaker's speech during a first remote session; </div>
      <div class="claim-text">c. recognizing the speaker's speech utilizing the speech recognition system during the first remote session; </div>
      <div class="claim-text">d. modifying the speech recognition system by incorporating the sample into the speech recognition system thereby forming a cluster-specific modified speech recognition system; </div>
      <div class="claim-text">e. storing a representation of the cluster-specific modified speech recognition system in association with the identification of a cluster of speakers wherein the speaker is a member of the cluster; and </div>
      <div class="claim-text">f. using the representation of the cluster-specific modified speech recognition system to recognize speech during a subsequent remote session with a member of the cluster of speakers. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" id="US-6766295-B1-CLM-00017" class="claim">
      <div class="claim-text">17. The method according to <claim-ref idref="US-6766295-B1-CLM-00016">claim 16</claim-ref> further comprising a step of cumulatively modifying the speech recognizing system according to speech samples obtained during one or more remote sessions with one or more members of the cluster of speakers.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" id="US-6766295-B1-CLM-00018" class="claim">
      <div class="claim-text">18. The method according to <claim-ref idref="US-6766295-B1-CLM-00016">claim 16</claim-ref> wherein the speaker is a telephone caller.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" id="US-6766295-B1-CLM-00019" class="claim">
      <div class="claim-text">19. The method according to <claim-ref idref="US-6766295-B1-CLM-00016">claim 16</claim-ref> wherein the step of modifying the speech recognition system comprises a step of modifying an acoustic model thereby forming a cluster-specific modified acoustic model and wherein the step of storing a representation of the cluster-specific modified speech recognition system comprises a step of storing a representation of the cluster-specific modified acoustic model.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" id="US-6766295-B1-CLM-00020" class="claim">
      <div class="claim-text">20. The method according to <claim-ref idref="US-6766295-B1-CLM-00019">claim 19</claim-ref> wherein the representation of the cluster-specific modified acoustic model is a set of statistics which can be utilized to modify a pre-existing acoustic model.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" id="US-6766295-B1-CLM-00021" class="claim">
      <div class="claim-text">21. The method according to <claim-ref idref="US-6766295-B1-CLM-00019">claim 19</claim-ref> wherein the representation of the cluster-specific modified acoustic model is a set of statistics which can be utilized to modify incoming acoustic speech.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="22" id="US-6766295-B1-CLM-00022" class="claim">
      <div class="claim-text">22. The method according to <claim-ref idref="US-6766295-B1-CLM-00016">claim 16</claim-ref> further comprising a step of utilizing the cluster-specific modified speech recognition system during the first remote session with the speaker.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="23" id="US-6766295-B1-CLM-00023" class="claim">
      <div class="claim-text">23. The method according to <claim-ref idref="US-6766295-B1-CLM-00016">claim 16</claim-ref> wherein the speech recognition system is speaker-independent prior to the first remote session.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="24" id="US-6766295-B1-CLM-00024" class="claim">
      <div class="claim-text">24. The method according to <claim-ref idref="US-6766295-B1-CLM-00016">claim 16</claim-ref> wherein the step of modifying the speech recognition system is performed during the first remote session.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" id="US-6766295-B1-CLM-00025" class="claim">
      <div class="claim-text">25. The method according to <claim-ref idref="US-6766295-B1-CLM-00016">claim 16</claim-ref> wherein the step of modifying the speech recognition system is performed after termination of the first remote session.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="26" id="US-6766295-B1-CLM-00026" class="claim">
      <div class="claim-text">26. The method according to <claim-ref idref="US-6766295-B1-CLM-00017">claim 17</claim-ref> wherein the speech recognition system is speaker-independent prior to the first remote session.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="27" id="US-6766295-B1-CLM-00027" class="claim">
      <div class="claim-text">27. The method according to <claim-ref idref="US-6766295-B1-CLM-00017">claim 17</claim-ref> wherein the step of modifying the speech recognition system is performed during the first remote session.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="28" id="US-6766295-B1-CLM-00028" class="claim">
      <div class="claim-text">28. The method according to <claim-ref idref="US-6766295-B1-CLM-00017">claim 17</claim-ref> wherein the step of modifying the speech recognition system is performed after termination of the first remote session.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="29" id="US-6766295-B1-CLM-00029" class="claim">
      <div class="claim-text">29. The method according to <claim-ref idref="US-6766295-B1-CLM-00017">claim 17</claim-ref> further comprising a step of authenticating the speaker's identification by the speaker's speech.</div>
    </div>
    </div> <div class="claim"> <div num="30" id="US-6766295-B1-CLM-00030" class="claim">
      <div class="claim-text">30. A method of adapting a speech recognition system, wherein the method comprises steps of:</div>
      <div class="claim-text">a. obtaining an identification of each of a plurality of speakers during a corresponding first remote session with each speaker; </div>
      <div class="claim-text">b. obtaining a sample of speech made by each of the plurality of speakers during a corresponding first remote session with each speaker; </div>
      <div class="claim-text">c. recognizing speech made by each speaker during the corresponding first remote session utilizing the speech recognition system configured to be speaker-independent; </div>
      <div class="claim-text">d. modifying the speech recognition system by individually incorporating the sample from each speaker into the speech recognition system thereby forming a speaker-specific modified speech recognition system corresponding to each speaker; </div>
      <div class="claim-text">e. storing a representation of the speaker-specific modified speech recognition system corresponding to each speaker in association with the identification of the corresponding speaker; and </div>
      <div class="claim-text">f. using the representation of the speaker-specific modified speech recognition system corresponding to a speaker to recognize speech during a subsequent remote session with the speaker. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="31" id="US-6766295-B1-CLM-00031" class="claim">
      <div class="claim-text">31. The method according to <claim-ref idref="US-6766295-B1-CLM-00030">claim 30</claim-ref> further comprising a step of cumulatively modifying the speech recognition system for each speaker according to speech samples obtained during one or more remote sessions with the corresponding speaker.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="32" id="US-6766295-B1-CLM-00032" class="claim">
      <div class="claim-text">32. The method according to <claim-ref idref="US-6766295-B1-CLM-00030">claim 30</claim-ref> wherein each of the plurality of speakers is a telephone caller.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="33" id="US-6766295-B1-CLM-00033" class="claim">
      <div class="claim-text">33. The method according to <claim-ref idref="US-6766295-B1-CLM-00030">claim 30</claim-ref> wherein the step of modifying the speech recognition system comprises a step of modifying an acoustic model thereby forming a speaker-specific modified acoustic model corresponding to each speaker and wherein the step of storing a representation of the modified speech recognition system comprises a step of storing a representation of the modified acoustic model corresponding to each speaker.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="34" id="US-6766295-B1-CLM-00034" class="claim">
      <div class="claim-text">34. The method according to <claim-ref idref="US-6766295-B1-CLM-00033">claim 33</claim-ref> wherein the representation of the speaker-specific modified acoustic model corresponding to each speaker is a set of statistics which can be utilized to modify a pre-existing acoustic model.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="35" id="US-6766295-B1-CLM-00035" class="claim">
      <div class="claim-text">35. The method according to <claim-ref idref="US-6766295-B1-CLM-00033">claim 33</claim-ref> wherein the representation of the speaker-specific modified acoustic model corresponding to each speaker is a set of statistics which can be utilized to modify incoming acoustic speech.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="36" id="US-6766295-B1-CLM-00036" class="claim">
      <div class="claim-text">36. The method according to <claim-ref idref="US-6766295-B1-CLM-00030">claim 30</claim-ref> further comprising a step of utilizing the speaker-specific modified speech recognition system corresponding to each speaker during the first remote session with the corresponding speaker.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="37" id="US-6766295-B1-CLM-00037" class="claim">
      <div class="claim-text">37. The method according to <claim-ref idref="US-6766295-B1-CLM-00030">claim 30</claim-ref> wherein the step of modifying the speech recognition system for each speaker is performed during the first remote session with the corresponding speaker.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="38" id="US-6766295-B1-CLM-00038" class="claim">
      <div class="claim-text">38. The method according to <claim-ref idref="US-6766295-B1-CLM-00030">claim 30</claim-ref> wherein the step of modifying the speech recognition system for each speaker is performed after termination of the first remote session with the corresponding speaker.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="39" id="US-6766295-B1-CLM-00039" class="claim">
      <div class="claim-text">39. The method according to <claim-ref idref="US-6766295-B1-CLM-00030">claim 30</claim-ref> further comprising a step of authenticating each speaker's identification by the speaker's speech.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="40" id="US-6766295-B1-CLM-00040" class="claim">
      <div class="claim-text">40. The method according to <claim-ref idref="US-6766295-B1-CLM-00031">claim 31</claim-ref> wherein the step of modifying the speech recognition system for each speaker is performed during the first remote session with the corresponding speaker.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="41" id="US-6766295-B1-CLM-00041" class="claim">
      <div class="claim-text">41. The method according to <claim-ref idref="US-6766295-B1-CLM-00031">claim 31</claim-ref> wherein the step of modifying the speech recognition system for each speaker is performed after termination of the first remote session with the corresponding speaker.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="42" id="US-6766295-B1-CLM-00042" class="claim">
      <div class="claim-text">42. The method according to <claim-ref idref="US-6766295-B1-CLM-00031">claim 31</claim-ref> further comprising a step of authenticating each speaker's identification by the speaker's speech.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="43" id="US-6766295-B1-CLM-00043" class="claim">
      <div class="claim-text">43. The method according to <claim-ref idref="US-6766295-B1-CLM-00030">claim 30</claim-ref> further comprising a step of deleting the representation of the speaker-specific modified speech recognition system corresponding to a speaker.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="44" id="US-6766295-B1-CLM-00044" class="claim">
      <div class="claim-text">44. The method according to <claim-ref idref="US-6766295-B1-CLM-00043">claim 43</claim-ref> wherein the step of deleting the representation of the speaker-specific modified speech recognition system corresponding to a speaker is performed when a predetermined period of time has elapsed since the corresponding speaker last engaged in a remote session.</div>
    </div>
    </div> <div class="claim"> <div num="45" id="US-6766295-B1-CLM-00045" class="claim">
      <div class="claim-text">45. A speech recognition system comprising:</div>
      <div class="claim-text">a. an interface coupled to receive a remote session from a speaker; and </div>
      <div class="claim-text">b. a processing system coupled to the interface to obtain an identification of the speaker and to recognize the speaker's speech wherein the processing system is cumulatively modified by incorporating speech samples obtained during a plurality of remote sessions with the speaker into the speech recognition system, thereby forming a speaker-specific modified processing system associated with the identification of the speaker. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="46" id="US-6766295-B1-CLM-00046" class="claim">
      <div class="claim-text">46. The speech recognition system according to <claim-ref idref="US-6766295-B1-CLM-00045">claim 45</claim-ref> wherein the speaker is a telephone caller.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="47" id="US-6766295-B1-CLM-00047" class="claim">
      <div class="claim-text">47. The speech recognition system according to <claim-ref idref="US-6766295-B1-CLM-00045">claim 45</claim-ref> wherein the processing system is modified by modifying an acoustic model, thereby forming a speaker-specific acoustic model.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="48" id="US-6766295-B1-CLM-00048" class="claim">
      <div class="claim-text">48. The speech recognition system according to <claim-ref idref="US-6766295-B1-CLM-00047">claim 47</claim-ref> wherein the processing system includes a memory for storing the speaker-specific acoustic model in association with the identification of the telephone caller.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="49" id="US-6766295-B1-CLM-00049" class="claim">
      <div class="claim-text">49. The speech recognition system according to <claim-ref idref="US-6766295-B1-CLM-00048">claim 48</claim-ref> wherein the memory stores a plurality of speaker-specific acoustic models, one for each of a plurality of telephone callers and wherein each speaker-specific acoustic model is stored in association with the identification of the corresponding telephone caller.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="50" id="US-6766295-B1-CLM-00050" class="claim">
      <div class="claim-text">50. The speech recognition system according to <claim-ref idref="US-6766295-B1-CLM-00049">claim 49</claim-ref> wherein the selected ones of the plurality of speaker-specific acoustic models are deleted when a predetermined period of time has elapsed since the corresponding speaker last engaged in a remote session with the voice recognizer.</div>
    </div>
    </div> <div class="claim"> <div num="51" id="US-6766295-B1-CLM-00051" class="claim">
      <div class="claim-text">51. A method of adapting an acoustic model utilized for speech recognition, wherein the method comprises steps of:</div>
      <div class="claim-text">a. obtaining an identification of a speaker; </div>
      <div class="claim-text">b. obtaining a speech utterance from the speaker during a remote session; </div>
      <div class="claim-text">c. recognizing the speaker's speech utilizing an acoustic model during the remote session; </div>
      <div class="claim-text">d. making a determination relative to the speech utterance; and </div>
      <div class="claim-text">e. only when indicated by the determination, performing steps of: </div>
      <div class="claim-text">i. modifying the acoustic model by incorporating the speech utterance into the acoustic model thereby forming a speaker-specific modified acoustic model; and </div>
      <div class="claim-text">ii. storing a representation of the speaker-specific modified acoustic model in association with the identification of the speaker. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="52" id="US-6766295-B1-CLM-00052" class="claim">
      <div class="claim-text">52. The method according to <claim-ref idref="US-6766295-B1-CLM-00051">claim 51</claim-ref> wherein the step of making the determination assigns a confidence level to the speech utterance.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="53" id="US-6766295-B1-CLM-00053" class="claim">
      <div class="claim-text">53. The method according to <claim-ref idref="US-6766295-B1-CLM-00051">claim 51</claim-ref> wherein the step of making the determination assigns a confidence level to each of a plurality of portions of the speech utterance.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="54" id="US-6766295-B1-CLM-00054" class="claim">
      <div class="claim-text">54. The method according to <claim-ref idref="US-6766295-B1-CLM-00051">claim 51</claim-ref> wherein the step of making a determination determines a level of resources available for storing the representation of the speaker-specific modified acoustic model.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="55" id="US-6766295-B1-CLM-00055" class="claim">
      <div class="claim-text">55. The method according to <claim-ref idref="US-6766295-B1-CLM-00051">claim 51</claim-ref> wherein the step of making a determination determines a level of processing resources available for performing the step of modifying the acoustic model.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES54268337" lang="EN" load-source="patent-office" class="description">
    <heading>FIELD OF THE INVENTION</heading> <p>The present invention relates to the field of speech recognition. More particularly, the present invention relates to the field of adaptation of a speech recognition system across multiple remote sessions with a speaker.</p>
    <heading>BACKGROUND OF THE INVENTION</heading> <p>Speech recognition systems are known which permit a user to interface with a computer system using spoken language. The speech recognition system receives spoken input from the user, interprets the input, and then translates the input into a form that the computer system understands.</p>
    <p>Speech recognition systems typically recognize spoken words or utterances based upon an acoustic model of a person who is speaking (the speaker). Acoustic models are typically generated based upon samples of speech. When the acoustic model is constructed based upon samples of speech obtained from a number of persons rather than a specific speaker, this is called speaker-independent modeling. When a speaker-independent model is then modified for recognizing speech of a particular person based upon samples of that person's speech, this is called adaptive modeling. When a model is constructed based solely on the speech of a particular person, this is termed speaker-dependent modeling.</p>
    <p>Speaker-independent modeling generally enables a number of speakers to interface with the same recognition system without having obtained prior samples of the speech of the particular speakers. In comparison to speaker-independent modeling, adaptive modeling and speaker-dependent modeling generally enable a speech recognition system to more accurately recognize a speaker's speech, especially if the speaker has a strong accent, has a phone line which produces unusual channel characteristics or for some other reason is not well modeled by speaker independent models.</p>
    <p>FIG. 1 illustrates a plurality of speaker-dependent acoustic models M<sub>1</sub>, M<sub>2</sub>, and M<sub>n</sub>, in accordance with the prior art. For each speaker, <b>1</b> through n, a corresponding speaker-dependent acoustic model M<sub>1 </sub>through M<sub>n</sub>, is stored. Thus, speech <b>10</b> of speaker <b>1</b> is recognized using the model M<sub>1 </sub>and the results <b>12</b> are outputted. Similarly, speech <b>14</b> of speaker <b>2</b> is recognized using the model M<sub>2 </sub>and the results <b>16</b> are outputted. And, speech <b>18</b> of speaker n is recognized using the model M<sub>n </sub>and the results are outputted.</p>
    <p>A speech recognition application program called NaturallySpeaking™, which adapts to a particular user, is available from Dragon Systems, Inc. This application program enables a user to enter text into a written document by speaking the words to be entered into a microphone attached to the user's computer system. The spoken words are interpreted and translated into typographical characters which then appear in the written document displayed on the user's computer screen. To adapt the application program to the particular user and to background noises of his or her environment, the user is asked to complete two initial training sessions during which the user is prompted to read textual passages aloud. A first training session requires that the user read several paragraphs aloud, while a second training session requires 25 to 30 to minutes for speaking and 15 to 20 minutes for processing the speech.</p>
    <p>Other speech recognition systems are known which adapt to an individual speaker based upon samples of speech obtained while the speaker is using the system, without requiring a training session. The effectiveness of this type of adaptation, however, is diminished when only a small sample of speech is available.</p>
    <p>Speech recognition systems are known which provide a telephonic interface between a caller and a customer service application. For example, the caller may obtain information regarding flight availability and pricing for a particular airline and may purchase tickets utilizing spoken language and without requiring assistance from an airline reservations clerk. Such customer service applications are typically intended to be accessed by a diverse population of callers and with various background noises. In such applications, it would be impractical to ask the callers to engage in a training session prior to using the customer service application. Accordingly, an acoustic model utilized for such customer service applications must be generalized so as to account for variability in the speakers. Thus, speaker-independent modeling is utilized for customer service applications. A result of using speaker-independent modeling is that the recognition system is less accurate than may be desired. This is particularly true for speakers with strong accents and those who have a phone line which produces unusual channel characteristics.</p>
    <p>Therefore, what is needed is a technique for improving the accuracy of speech recognition for a speech recognition system.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>The invention is a method and apparatus for adaptation of a speech recognition system across multiple remote sessions with a speaker. The speaker can remotely access a speech recognition system, such as via a telephone or other remote communication system. An acoustic model is utilized for recognizing speech utterances made by the speaker. Upon initiation of a first remote session with the speaker, the acoustic model is speaker-independent. During the first remote session, the speaker is uniquely identified and speech samples are obtained from the speaker. In the preferred embodiment, the samples are obtained without requiring the speaker to engage in a training session. The acoustic model is then modified based upon the samples thereby forming a modified model. The model can be modified during the remote session or after the session is terminated. Upon termination of the remote session, the modified model is then stored in association with an identification of the speaker. Alternately, rather than storing the modified model, statistics that can be used to modify a pre-existing acoustic model are stored in association with an identification of the speaker.</p>
    <p>During a subsequent remote session, the speaker is identified and, then, the modified acoustic model is utilized to recognize speech utterances made by the speaker. Additional speech samples are obtained during the subsequent session and, then, utilized to further modify the acoustic model. In this manner, an acoustic model utilized for recognizing the speech of a particular speaker is cumulatively modified according to speech samples obtained during multiple remote sessions with the speaker. As a result, the accuracy of the speech recognizing system improves for the speaker even when the speaker only engages in relatively short remote sessions.</p>
    <p>For each speaker to remotely access the speech recognizing system, a modified acoustic model, or a set of statistics that can be used to modify the acoustic model or incoming acoustic speech, is formed and stored along with the speaker's unique identification. Accordingly, multiple different acoustic models or sets of statistics are stored, one for each speaker.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p>FIG. 1 illustrates a plurality of speaker-dependent acoustic models in accordance with the prior art.</p>
    <p>FIG. 2 illustrates a speech recognizing system in conjunction with a remote communication system in accordance with the present invention.</p>
    <p>FIG. 3 illustrates a flow diagram for adapting an acoustic model utilized for speech recognition in accordance with the present invention.</p>
    <p>FIG. 4 illustrates a plurality of sets of transform statistics for use in conjunction with an acoustic model in accordance with the present invention.</p>
    <p>FIG. 5 illustrates a flow diagram for adapting an acoustic model utilized for speech recognition in accordance with an alternate embodiment of the present invention.</p>
    <heading>DETAILED DESCRIPTION OF A PREFERRED EMBODIMENT</heading> <p>FIG. 2 illustrates a speech recognizing system <b>100</b> in conjunction with a remote communication system <b>150</b> in accordance with the present invention. The remote communication system <b>150</b> can be a telephone system (e.g., a central office, a private branch exchange or cellular telephone system). Alternately, the remote communication system <b>150</b> can be a communication network (e.g., a wireless network), a local area network (e.g., an Ethernet LAN) or a wide area network (e.g., the World Wide Web). The speech recognition system <b>100</b> includes a processing system, such as a general purpose processor <b>102</b>, a system memory <b>104</b>, a mass storage medium <b>106</b>, and input/output devices <b>108</b>, all of which are interconnected by a system bus <b>110</b>. The processor <b>102</b> operates in accordance with machine readable computer software code stored in the system memory <b>104</b> and mass storage medium <b>106</b> so as to implement the present invention. The input/output devices <b>108</b> can include a display monitor, a keyboard and an interface coupled to the remote system <b>150</b> for receiving speech input therefrom. Though the speech recognizing system <b>100</b> illustrated in FIG. 2 is implemented as a general purpose computer, it will be apparent that the speech recognizing system can be implemented so as to include a special-purpose computer or dedicated hardware circuits. In which case, one or more of the hardware elements illustrated in FIG. 2 can be omitted or substituted by another.</p>
    <p>The invention is a method and apparatus for adaptation of a speech recognizing system across multiple remote sessions with a speaker. FIG. 3 illustrates a flow diagram for adapting an acoustic model utilized for speech recognition in accordance with the present invention. The flow diagram of FIG. 3 illustrates graphically operation of the speech recognizing system <b>100</b> in accordance with the present invention. Program flow begins in a start state <b>200</b>. From the state <b>200</b>, program flow moves to a state <b>202</b>. In the state <b>202</b>, a remote session between the speaker and the voice recognition system <b>100</b> is initiated. For example, a telephone call placed by the speaker initiates the session; in which case, the speaker is a telephone caller. Alternately, the remote session is conducted via another remote communication medium. Then, program flow moves to a state <b>204</b>.</p>
    <p>In the state <b>204</b>, an identification of the speaker is obtained. For example, the speaker can be prompted to speak his or her name, enter a personal identification number (pin), enter an account number, or the like. Alternately, the speaker can be automatically identified, such as by receiving the speaker's caller ID for a telephone call. The speaker's identification can also be authenticated utilizing voice identification techniques assuming a voice sample of the speaker has previously been obtained by the speech recognition system <b>100</b>. From the state <b>204</b>, program flow moves to a state <b>206</b>. In the state <b>206</b>, a determination is made as to whether the particular speaker is a first-time speaker or if samples of the speaker's speech have been previously obtained. This is accomplished by attempting to match the speaker's identification obtained in the state <b>204</b> to a prior entry stored in the memory <b>104</b> or mass storage <b>106</b> of the speech recognizing system <b>100</b> made in response to a prior session with the same speaker. It will be apparent that the prior entries can also be stored remotely from the speech recognition system <b>100</b>, such as in a centralized database which is accessible to the speech recognition system <b>100</b> via a network connection which can be provided by a local area network or the World Wide Web.</p>
    <p>Assuming the speaker is a first time speaker, program flow moves from the state <b>206</b> to a state <b>208</b>. In the state <b>208</b>, a speaker-independent model is retrieved from the memory <b>104</b> or mass storage <b>106</b> to be utilized for recognizing speech made by the speaker. The speaker-independent model is a generalized acoustic model generated based upon samples of speech taken from multiple different representative persons.</p>
    <p>The program flow then moves to a state <b>210</b>. In the state <b>210</b>, the speaker-independent acoustic model retrieved in the state <b>208</b> is utilized for recognizing speech made by the speaker as the speaker interacts with the speech recognition system <b>100</b> during the remote session. For example, the speaker-independent model is utilized to recognize when the speaker wishes to obtain a flight schedule, a bank account balance, and so forth. In addition, during the state <b>210</b> samples of the speaker's speech are taken. Preferably, these samples are taken without prompting the speaker to speak certain words or phrases, as in a training session. It will be apparent, however, that the speaker can be prompted to speak certain words or phrases. In which case, prompting of the speaker is preferably performed so as to minimize inconvenience to the speaker.</p>
    <p>Then program flow moves to a state <b>212</b>. In the state <b>212</b>, the speech recognition system <b>100</b> is modified. More particularly, the speaker-independent acoustic model utilized in the state <b>210</b> to recognize the speaker's speech is modified based upon the samples of the speaker's speech taken in the state <b>210</b>, thereby forming a modified acoustic model.</p>
    <p>In the preferred embodiment, the acoustic model is modified prior to termination of the remote session so that the modified model can immediately be put to use. Alternately, the acoustic model is modified after termination of the remote session. In the preferred embodiment, the acoustic model is modified and put to use for speech recognition during the first and subsequent remote sessions. The acoustic model can also be modified between remote sessions. Thus, the states <b>210</b> and <b>212</b> can be performed repeatedly, one after the other or concurrently, during a single session. For example, assuming a predetermined amount of speech (e.g., three seconds) is received (state <b>210</b>), but the remote session has not yet been terminated, then the acoustic model can be modified (state <b>212</b>) while a next predetermined amount of speech is received (state <b>210</b>). Once the next predetermined amount of speech is received, the acoustic model is again modified (state <b>212</b>). For simplicity of illustration, however, the states <b>210</b> and <b>212</b> are shown in FIG. 3 as occurring in a simple succession. Once the session terminates, program flow moves to a state <b>214</b>.</p>
    <p>In the state <b>214</b>, a representation of the modified acoustic model, such as the modified model itself or a set of statistics that can be used to modify a pre-existing acoustic model or that can be used to modify incoming acoustic speech, is stored in the memory <b>104</b> or mass storage <b>106</b> or in a centralized network database. Note that rather than modifying an acoustic model, the present invention can be utilized to modify measurements of the speech such as features vectors to achieve the principle advantages of the present invention. It will be understood that modification of phonetic features is within the scope of the present invention and that use of the term “acoustic model” herein includes phonetic features.</p>
    <p>Thus, in the preferred embodiment, only a set of statistics which can be used to modify a pre-existing acoustic model, is stored. For example, FIG. 4 illustrates a plurality of sets of transform statistics for use in conjunction with a pre-existing acoustic model M in accordance with the present invention. For each speaker, <b>1</b> through n, a corresponding set of statistics X<sub>1</sub>, through X<sub>n</sub>, is stored. For each speaker <b>1</b> through n, the modified model can be considered to include the corresponding set of statistics X<sub>1 </sub>through X<sub>n </sub>together with the preexisting model M. Only one copy of pre-existing model M need be stored.</p>
    <p>Thus, during a subsequent telephone session, speech <b>300</b> of speaker <b>1</b> is recognized using the corresponding set of transform statistics X<sub>1 </sub>in conjunction with the pre-existing model M to recognize the speaker's speech for forming an output <b>302</b>. Similarly, speech of speaker <b>2</b> is recognized using the corresponding set of statistics X<sub>2 </sub>and the same pre-existing model M to recognize the speaker's speech for forming the output <b>302</b>. And, speech of speaker n is recognized using the corresponding set of statistics X<sub>n </sub>and the model M to recognize the speaker's speech for forming an output <b>302</b>. As a result, memory is conserved in comparison with the prior technique illustrated in FIG. <b>1</b>.</p>
    <p>The modified model, or set of statistics, is stored in association with the identification of the speaker for utilization for recognizing the speaker's speech in a subsequent session with the speaker. For example, assume that in the state <b>206</b>, the speech recognition system <b>100</b> looks up the speaker's identification and determines that a sample of the speaker's speech has previously been obtained. In which case, program flow moves from the state <b>206</b> to a state <b>216</b>.</p>
    <p>In the state <b>216</b>, the modified model or set of statistics stored in response to the speaker's previous remote session is retrieved from the memory <b>104</b> or mass storage <b>106</b> to be utilized for recognizing speech utterances made by the speaker during the current session. From the state <b>216</b>, program flow then moves to the state <b>210</b>. In the state <b>210</b>, the modified acoustic model or set of statistics retrieved in the state <b>216</b> is utilized for recognizing speech utterances made by the speaker as the speaker interacts with the speech recognition system during the remote session. Additional samples of the speaker's speech are taken in the state <b>210</b> and utilized to further modify the acoustic model for the speaker.</p>
    <p>In this manner, an acoustic model utilized for recognizing the speech of a particular speaker is cumulatively modified according to speech samples obtained during multiple remote sessions with the speaker. As a result, the accuracy of the speech recognizing system improves for the speaker across multiple remote sessions even when the remote sessions are of relatively short duration.</p>
    <p>FIG. 5 illustrates a flow diagram for adapting an acoustic model utilized for speech recognition in accordance with an alternate embodiment of the present invention. The flow diagram of FIG. 5 illustrates graphically operation of the speech recognizing system <b>100</b> in accordance with an alternate embodiment of the present invention. Portions of FIG. 5 which have a one-to-one functional correspondence with those of FIG. 3 are given the same reference numeral and are not discussed further.</p>
    <p>The flow diagram of FIG. 5 differs from that of FIG. 3 in that from the state <b>210</b>, program flow moves to a state <b>400</b>. In the state <b>400</b>, a determination is made relative to the incoming speech utterance. This determination preferably assigns a confidence level related to the accuracy of the speech recognition performed in the state <b>210</b>. This can be accomplished by the speech recognizing system <b>100</b> assigning each speech utterance, such as a word, a phoneme, a phrase or a sentence, a certainty or score, where the assigned certainty or score is related to the probability that the corresponding identified speech correctly corresponds to the spoken input, and, then, comparing the certainty or score to one or more predetermined thresholds. If the speech recognition confidence is consistently extremely high, there may be no need to modify (or further modify) the acoustic model for the particular speaker. By avoiding modification of the acoustic model, this saves processing capacity and memory of the speech recognition system <b>100</b> which can be devoted to other tasks. Conversely, if the speech recognition accuracy is extremely low, any modifications made to acoustic model based upon incorrectly recognized speech utterances or words is not expected to improve the accuracy of speech recognition based upon such a modified acoustic model. Accordingly, if the determination made in the state <b>400</b> suggests a high accuracy (e.g., the certainty exceeds a first threshold) or a low accuracy (e.g., the certainty is below a second threshold that is lower than the first threshold), then program flow returns to the state <b>202</b> upon termination of the remote session. In which case, no modifications to the acoustic model are performed.</p>
    <p>Alternately, assuming the speech recognition accuracy is determined to be moderate (e.g, the certainty falls between the first and second thresholds), then it is expected that modifications to the acoustic model will improve accuracy. In which case, program flow moves from the state <b>400</b>, to the state <b>212</b>. As discussed relative to FIG. 3, in the state <b>212</b>, the speaker-independent acoustic model utilized in the state <b>210</b> to recognize the speaker's speech is modified based upon the samples of the speaker's speech taken in the state <b>210</b>, thereby forming a modified acoustic model.</p>
    <p>In addition, because each portion of an utterance, such as a word or a phoneme, can be associated with a different confidence level, a single utterance can have several confidence levels associated with it. Thus, if some levels are above a threshold and others are below, only those portions having a confidence level above the threshold can be used to update the model.</p>
    <p>Note that criteria other than, or in addition to, confidence levels can be utilized for making the determination in the state <b>400</b> of whether or not to modify the acoustic model. For example, a level of available resources in the speech recognition system <b>100</b>, such as a low level of available memory or available processing power, may indicate that modification of the model is undesirable.</p>
    <p>In the state <b>214</b>, a representation of the modified acoustic model, such as the modified model itself or a set of statistics that can be used to modify a pre-existing acoustic model, is stored in the memory <b>104</b> or mass storage <b>106</b> or in a centralized network database in association with the identification of the speaker for utilization for recognizing the speaker's speech in a subsequent remote session with the speaker.</p>
    <p>In an alternate embodiment, the determination made in the state <b>400</b> can be supervised. For example, the speech recognition system <b>100</b> can inform the speaker of the word or words it has recognized and, then, ask the speaker to verify whether the speaker's speech has been correctly recognized. Assuming the speaker confirms that the speaker's speech has been correctly recognized, then program flow moves from the state <b>400</b> to the state <b>212</b>. Accordingly, the correctly identified speech utterances or words are utilized to modify the acoustic model. Conversely, if the speaker indicates that the speech utterances or words were incorrectly identified, then the acoustic model is not modified based upon such incorrectly identified speech utterances or words.</p>
    <p>As described in relation to FIGS. 2-5, an acoustic model utilized for recognizing the speech of a particular speaker is cumulatively modified according to speech samples obtained during multiple remote sessions with the speaker. As a result, the accuracy of the speech recognizing system improves for the speaker across multiple remote sessions even when the sessions are of relatively short duration.</p>
    <p>A feature of the present invention provides an acoustic model that uniquely corresponds to each of a plurality of speakers. During a first remote session with each of the speakers, the speaker-independent acoustic model is initially utilized. This model is then modified according to speech samples taken for each particular speaker. Preferably, the model is modified during the first and subsequent remote sessions and between sessions. Each modified model is then stored in association with the corresponding speaker's identification. For subsequent remote sessions, the speech recognizing system <b>100</b> retrieves an appropriate acoustic model from the memory <b>104</b> or mass storage <b>106</b> based upon the speaker's identification. Accordingly, each acoustic model is modified based upon samples of the corresponding speaker's speech across multiple remote sessions with the speaker.</p>
    <p>To conserve memory, acoustic models that are specific to a particular speaker can be deleted from the memory <b>104</b> or mass storage <b>106</b> when no longer needed. For example, when a particular speaker has not engaged in a remote session with the service application for a predetermined period of time, then the acoustic model corresponding to that speaker is deleted. Should the speaker initiate a remote session after deletion of the acoustic model corresponding to that speaker, the speaker-independent model is initially utilized and then modified according to newly acquired samples of the speaker's speech, as described above.</p>
    <p>According to yet another embodiment of the present invention, rather than modifying an acoustic model across a plurality of remote sessions based upon speech of an individual speaker such that the model is speaker specific, the acoustic model can be modified based upon speech of a group of speakers such that the model is speaker-cluster specific. For example, speakers from different locales, each locale being associated with a corresponding accent (or lack thereof), can be clustered and a model or set of statistics can be stored corresponding to each cluster. Thus, speakers from Minnesota can be included in a cluster, while speakers from Georgia can be included in another cluster. As an example, when the remote connection is via telephone, the speaker's telephone area code can be used to place the speaker into an appropriate cluster. It will be apparent that clusters can be based upon criteria other than locale.</p>
    <p>The present invention has been described in terms of specific embodiments incorporating details to facilitate the understanding of principles of construction and operation of the invention. Such reference herein to specific embodiments and details thereof is not intended to limit the scope of the claims appended hereto. It will be apparent to those skilled in the art that modifications may be made in the embodiment chosen for illustration without departing from the spirit and scope of the invention.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4489434">US4489434</a></td><td class="patent-data-table-td patent-date-value">Oct 5, 1981</td><td class="patent-data-table-td patent-date-value">Dec 18, 1984</td><td class="patent-data-table-td ">Exxon Corporation</td><td class="patent-data-table-td ">Speech recognition method and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4590604">US4590604</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 13, 1983</td><td class="patent-data-table-td patent-date-value">May 20, 1986</td><td class="patent-data-table-td ">Westinghouse Electric Corp.</td><td class="patent-data-table-td ">Voice-recognition elevator security system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4866778">US4866778</a></td><td class="patent-data-table-td patent-date-value">Aug 11, 1986</td><td class="patent-data-table-td patent-date-value">Sep 12, 1989</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Interactive speech recognition apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5127055">US5127055</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 11, 1991</td><td class="patent-data-table-td patent-date-value">Jun 30, 1992</td><td class="patent-data-table-td ">Kurzweil Applied Intelligence, Inc.</td><td class="patent-data-table-td ">Speech recognition apparatus &amp; method having dynamic reference pattern adaptation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5208897">US5208897</a></td><td class="patent-data-table-td patent-date-value">Sep 28, 1990</td><td class="patent-data-table-td patent-date-value">May 4, 1993</td><td class="patent-data-table-td ">Emerson &amp; Stern Associates, Inc.</td><td class="patent-data-table-td ">Method and apparatus for speech recognition based on subsyllable spellings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5239586">US5239586</a></td><td class="patent-data-table-td patent-date-value">Nov 20, 1991</td><td class="patent-data-table-td patent-date-value">Aug 24, 1993</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Voice recognition system used in telephone apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5293452">US5293452</a></td><td class="patent-data-table-td patent-date-value">Jul 1, 1991</td><td class="patent-data-table-td patent-date-value">Mar 8, 1994</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Voice log-in using spoken name input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5528731">US5528731</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 19, 1993</td><td class="patent-data-table-td patent-date-value">Jun 18, 1996</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Method of accommodating for carbon/electret telephone set variability in automatic speaker verification</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5568540">US5568540</a></td><td class="patent-data-table-td patent-date-value">Apr 14, 1995</td><td class="patent-data-table-td patent-date-value">Oct 22, 1996</td><td class="patent-data-table-td ">Active Voice Corporation</td><td class="patent-data-table-td ">Method and apparatus for selecting and playing a voice mail message</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5617486">US5617486</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 27, 1995</td><td class="patent-data-table-td patent-date-value">Apr 1, 1997</td><td class="patent-data-table-td ">Apple Computer, Inc.</td><td class="patent-data-table-td ">For pattern recognition of data input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5651054">US5651054</a></td><td class="patent-data-table-td patent-date-value">Apr 13, 1995</td><td class="patent-data-table-td patent-date-value">Jul 22, 1997</td><td class="patent-data-table-td ">Active Voice Corporation</td><td class="patent-data-table-td ">Method and apparatus for monitoring a message in a voice mail system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5717743">US5717743</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 23, 1996</td><td class="patent-data-table-td patent-date-value">Feb 10, 1998</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Transparent telephone access system using voice authorization</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5742905">US5742905</a></td><td class="patent-data-table-td patent-date-value">Sep 19, 1994</td><td class="patent-data-table-td patent-date-value">Apr 21, 1998</td><td class="patent-data-table-td ">Bell Communications Research, Inc.</td><td class="patent-data-table-td ">Personal communications internetworking</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5778338">US5778338</a></td><td class="patent-data-table-td patent-date-value">Jan 23, 1997</td><td class="patent-data-table-td patent-date-value">Jul 7, 1998</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Apparatus for masking frame errors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5794192">US5794192</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 12, 1996</td><td class="patent-data-table-td patent-date-value">Aug 11, 1998</td><td class="patent-data-table-td ">Panasonic Technologies, Inc.</td><td class="patent-data-table-td ">Self-learning speaker adaptation based on spectral bias source decomposition, using very short calibration speech</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5799065">US5799065</a></td><td class="patent-data-table-td patent-date-value">May 6, 1996</td><td class="patent-data-table-td patent-date-value">Aug 25, 1998</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Call routing device employing continuous speech</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5822405">US5822405</a></td><td class="patent-data-table-td patent-date-value">Sep 16, 1996</td><td class="patent-data-table-td patent-date-value">Oct 13, 1998</td><td class="patent-data-table-td ">Toshiba America Information Systems, Inc.</td><td class="patent-data-table-td ">Automated retrieval of voice mail using speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5842161">US5842161</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 25, 1996</td><td class="patent-data-table-td patent-date-value">Nov 24, 1998</td><td class="patent-data-table-td ">Lucent Technologies Inc.</td><td class="patent-data-table-td ">Telecommunications instrument employing variable criteria speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5848130">US5848130</a></td><td class="patent-data-table-td patent-date-value">Dec 31, 1996</td><td class="patent-data-table-td patent-date-value">Dec 8, 1998</td><td class="patent-data-table-td ">At&amp;T Corp</td><td class="patent-data-table-td ">System and method for enhanced intelligibility of voice messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5897616">US5897616</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 11, 1997</td><td class="patent-data-table-td patent-date-value">Apr 27, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Apparatus and methods for speaker verification/identification/classification employing non-acoustic and/or acoustic models and databases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6044346">US6044346</a></td><td class="patent-data-table-td patent-date-value">Mar 9, 1998</td><td class="patent-data-table-td patent-date-value">Mar 28, 2000</td><td class="patent-data-table-td ">Lucent Technologies Inc.</td><td class="patent-data-table-td ">System and method for operating a digital voice recognition processor with flash memory storage</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6070140">US6070140</a></td><td class="patent-data-table-td patent-date-value">Nov 12, 1998</td><td class="patent-data-table-td patent-date-value">May 30, 2000</td><td class="patent-data-table-td ">Tran; Bao Q.</td><td class="patent-data-table-td ">Speech recognizer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6151571">US6151571</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 1999</td><td class="patent-data-table-td patent-date-value">Nov 21, 2000</td><td class="patent-data-table-td ">Andersen Consulting</td><td class="patent-data-table-td ">System, method and article of manufacture for detecting emotion in voice signals through analysis of a plurality of voice signal parameters</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6181780">US6181780</a></td><td class="patent-data-table-td patent-date-value">Nov 24, 1997</td><td class="patent-data-table-td patent-date-value">Jan 30, 2001</td><td class="patent-data-table-td ">Worldvoice Licensing, Inc.</td><td class="patent-data-table-td ">Telephonic voice message store and forward method having network address and voice authentication</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6185535">US6185535</a></td><td class="patent-data-table-td patent-date-value">Oct 16, 1998</td><td class="patent-data-table-td patent-date-value">Feb 6, 2001</td><td class="patent-data-table-td ">Telefonaktiebolaget Lm Ericsson (Publ)</td><td class="patent-data-table-td ">Voice control of a user interface to service applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6208713">US6208713</a></td><td class="patent-data-table-td patent-date-value">Dec 5, 1996</td><td class="patent-data-table-td patent-date-value">Mar 27, 2001</td><td class="patent-data-table-td ">Nortel Networks Limited</td><td class="patent-data-table-td ">Method and apparatus for locating a desired record in a plurality of records in an input recognizing telephone directory</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6219407">US6219407</a></td><td class="patent-data-table-td patent-date-value">Jan 16, 1998</td><td class="patent-data-table-td patent-date-value">Apr 17, 2001</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Apparatus and method for improved digit recognition and caller identification in telephone mail messaging</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6275806">US6275806</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 1999</td><td class="patent-data-table-td patent-date-value">Aug 14, 2001</td><td class="patent-data-table-td ">Andersen Consulting, Llp</td><td class="patent-data-table-td ">System method and article of manufacture for detecting emotion in voice signals by utilizing statistics for voice signal parameters</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6327343">US6327343</a></td><td class="patent-data-table-td patent-date-value">Jan 16, 1998</td><td class="patent-data-table-td patent-date-value">Dec 4, 2001</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and methods for automatic call and data transfer processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6334103">US6334103</a></td><td class="patent-data-table-td patent-date-value">Sep 1, 2000</td><td class="patent-data-table-td patent-date-value">Dec 25, 2001</td><td class="patent-data-table-td ">General Magic, Inc.</td><td class="patent-data-table-td ">Voice user interface with personality</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6363348">US6363348</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 16, 1998</td><td class="patent-data-table-td patent-date-value">Mar 26, 2002</td><td class="patent-data-table-td ">U.S. Philips Corporation</td><td class="patent-data-table-td ">User model-improvement-data-driven selection and update of user-oriented recognition model of a given type for word recognition at network server</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0651372A2?cl=en">EP0651372A2</a></td><td class="patent-data-table-td patent-date-value">Oct 19, 1994</td><td class="patent-data-table-td patent-date-value">May 3, 1995</td><td class="patent-data-table-td ">AT&amp;amp;T Corp.</td><td class="patent-data-table-td ">Automatic speech recognition (ASR) processing using confidence measures</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1022725A1?cl=en">EP1022725A1</a></td><td class="patent-data-table-td patent-date-value">Jan 20, 1999</td><td class="patent-data-table-td patent-date-value">Jul 26, 2000</td><td class="patent-data-table-td ">Sony International (Europe) GmbH</td><td class="patent-data-table-td ">Selection of acoustic models using speaker verification</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="http://www.google.com/url?id=1nFnBAABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3DH1079785A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNHRZ0s_gBBKFqod8BFscuO6RnggUg">JPH1079785A</a></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">Title not available</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="A+Fast+Algorithm+For+Unsupervised+Incremental+Speaker+Adaption"'>A Fast Algorithm For Unsupervised Incremental Speaker Adaption</a>" Michael Schubler, Florian Gallwitz, Stefan Harbeck, Apr. 21, 1997, pp. 1019-1022, IEEE.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Integration+Of+Speaker+And+Speech+Recognition+Systems"'>Integration Of Speaker And Speech Recognition Systems</a>" D.A. Reynolds and L. P. Heck, 1991, pp. 869-872, IEEE.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Iterative+Self-Learning+Speaker+And+Channel+Adaptation+Under+Various+Initial+Conditions"'>Iterative Self-Learning Speaker And Channel Adaptation Under Various Initial Conditions</a>" Yunxin Zhao, May 9, 1995, pp. 712-715, IEEE.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Vector-Field-Smoothed+Bayesian+Learning+For+Fast+And+Incremental+Speaker%2FTelephone-Channel+Adaption"'>Vector-Field-Smoothed Bayesian Learning For Fast And Incremental Speaker/Telephone-Channel Adaption</a>" Jun-ichi Takahashi And Shigeki Sagayama, 1997, pp. 127-146, Computer Speech And Language.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Reynolds et al.; Integration of speaker and speech recognition systems; IEEE; pp. 869-872 vol. 2.*</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6941264">US6941264</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 16, 2001</td><td class="patent-data-table-td patent-date-value">Sep 6, 2005</td><td class="patent-data-table-td ">Sony Electronics Inc.</td><td class="patent-data-table-td ">Retraining and updating speech models for speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6996526">US6996526</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 2, 2002</td><td class="patent-data-table-td patent-date-value">Feb 7, 2006</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and apparatus for transcribing speech when a plurality of speakers are participating</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7200565">US7200565</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 17, 2001</td><td class="patent-data-table-td patent-date-value">Apr 3, 2007</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and method for promoting the use of a selected software product having an adaptation module</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7403766">US7403766</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 16, 2004</td><td class="patent-data-table-td patent-date-value">Jul 22, 2008</td><td class="patent-data-table-td ">Value-Added Communications, Inc.</td><td class="patent-data-table-td ">Telecommunication call management and monitoring system with voiceprint verification</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7457745">US7457745</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 3, 2003</td><td class="patent-data-table-td patent-date-value">Nov 25, 2008</td><td class="patent-data-table-td ">Hrl Laboratories, Llc</td><td class="patent-data-table-td ">Method and apparatus for fast on-line automatic speaker/environment adaptation for speech/speaker recognition in the presence of changing environments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7505903">US7505903</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 21, 2004</td><td class="patent-data-table-td patent-date-value">Mar 17, 2009</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Speech recognition dictionary creation method and speech recognition dictionary creating device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7630898">US7630898</a></td><td class="patent-data-table-td patent-date-value">Sep 27, 2005</td><td class="patent-data-table-td patent-date-value">Dec 8, 2009</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">System and method for preparing a pronunciation dictionary for a text-to-speech voice</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7664636">US7664636</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 17, 2000</td><td class="patent-data-table-td patent-date-value">Feb 16, 2010</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">System and method for indexing voice mail messages by speaker</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7693716">US7693716</a></td><td class="patent-data-table-td patent-date-value">Sep 27, 2005</td><td class="patent-data-table-td patent-date-value">Apr 6, 2010</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">System and method of developing a TTS voice</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7711562">US7711562</a></td><td class="patent-data-table-td patent-date-value">Sep 27, 2005</td><td class="patent-data-table-td patent-date-value">May 4, 2010</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">System and method for testing a TTS voice</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7742919">US7742919</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 27, 2005</td><td class="patent-data-table-td patent-date-value">Jun 22, 2010</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">System and method for repairing a TTS voice database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7742921">US7742921</a></td><td class="patent-data-table-td patent-date-value">Sep 27, 2005</td><td class="patent-data-table-td patent-date-value">Jun 22, 2010</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">System and method for correcting errors when generating a TTS voice</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7827032">US7827032</a></td><td class="patent-data-table-td patent-date-value">Oct 6, 2006</td><td class="patent-data-table-td patent-date-value">Nov 2, 2010</td><td class="patent-data-table-td ">Vocollect, Inc.</td><td class="patent-data-table-td ">Methods and systems for adapting a model for a speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7853448">US7853448</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 16, 2007</td><td class="patent-data-table-td patent-date-value">Dec 14, 2010</td><td class="patent-data-table-td ">Funai Electric Co., Ltd.</td><td class="patent-data-table-td ">Electronic instrument for speech recognition with standby time shortening and acoustic model deletion</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7865362">US7865362</a></td><td class="patent-data-table-td patent-date-value">Feb 4, 2005</td><td class="patent-data-table-td patent-date-value">Jan 4, 2011</td><td class="patent-data-table-td ">Vocollect, Inc.</td><td class="patent-data-table-td ">Method and system for considering information about an expected response when performing speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7895039">US7895039</a></td><td class="patent-data-table-td patent-date-value">Mar 21, 2007</td><td class="patent-data-table-td patent-date-value">Feb 22, 2011</td><td class="patent-data-table-td ">Vocollect, Inc.</td><td class="patent-data-table-td ">Methods and systems for optimizing model adaptation for a speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7917364">US7917364</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 23, 2003</td><td class="patent-data-table-td patent-date-value">Mar 29, 2011</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">System and method using multiple automated speech recognition engines</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7949533">US7949533</a></td><td class="patent-data-table-td patent-date-value">Mar 21, 2007</td><td class="patent-data-table-td patent-date-value">May 24, 2011</td><td class="patent-data-table-td ">Vococollect, Inc.</td><td class="patent-data-table-td ">Methods and systems for assessing and improving the performance of a speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7996226">US7996226</a></td><td class="patent-data-table-td patent-date-value">Dec 15, 2009</td><td class="patent-data-table-td patent-date-value">Aug 9, 2011</td><td class="patent-data-table-td ">AT&amp;T Intellecutal Property II, L.P.</td><td class="patent-data-table-td ">System and method of developing a TTS voice</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8073694">US8073694</a></td><td class="patent-data-table-td patent-date-value">Dec 23, 2009</td><td class="patent-data-table-td patent-date-value">Dec 6, 2011</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">System and method for testing a TTS voice</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8166297">US8166297</a></td><td class="patent-data-table-td patent-date-value">Jul 2, 2008</td><td class="patent-data-table-td patent-date-value">Apr 24, 2012</td><td class="patent-data-table-td ">Veritrix, Inc.</td><td class="patent-data-table-td ">Systems and methods for controlling access to encrypted data stored on a mobile device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8185392">US8185392</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 2011</td><td class="patent-data-table-td patent-date-value">May 22, 2012</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Adapting enhanced acoustic models</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8185646">US8185646</a></td><td class="patent-data-table-td patent-date-value">Oct 29, 2009</td><td class="patent-data-table-td patent-date-value">May 22, 2012</td><td class="patent-data-table-td ">Veritrix, Inc.</td><td class="patent-data-table-td ">User authentication for social networks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8200495">US8200495</a></td><td class="patent-data-table-td patent-date-value">Jan 13, 2006</td><td class="patent-data-table-td patent-date-value">Jun 12, 2012</td><td class="patent-data-table-td ">Vocollect, Inc.</td><td class="patent-data-table-td ">Methods and systems for considering information about an expected response when performing speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8255219">US8255219</a></td><td class="patent-data-table-td patent-date-value">Mar 9, 2011</td><td class="patent-data-table-td patent-date-value">Aug 28, 2012</td><td class="patent-data-table-td ">Vocollect, Inc.</td><td class="patent-data-table-td ">Method and apparatus for determining a corrective action for a speech recognition system based on the performance of the system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8311822">US8311822</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 2, 2004</td><td class="patent-data-table-td patent-date-value">Nov 13, 2012</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Method and system of enabling intelligent and lightweight speech to text transcription through distributed environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8346539">US8346539</a></td><td class="patent-data-table-td patent-date-value">Dec 29, 2009</td><td class="patent-data-table-td patent-date-value">Jan 1, 2013</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">System and method for indexing voice mail messages by speaker</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8347370">US8347370</a></td><td class="patent-data-table-td patent-date-value">Aug 18, 2011</td><td class="patent-data-table-td patent-date-value">Jan 1, 2013</td><td class="patent-data-table-td ">Veritrix, Inc.</td><td class="patent-data-table-td ">Multi-channel multi-factor authentication</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8374870">US8374870</a></td><td class="patent-data-table-td patent-date-value">Mar 9, 2011</td><td class="patent-data-table-td patent-date-value">Feb 12, 2013</td><td class="patent-data-table-td ">Vocollect, Inc.</td><td class="patent-data-table-td ">Methods and systems for assessing and improving the performance of a speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8438025">US8438025</a></td><td class="patent-data-table-td patent-date-value">Jul 30, 2012</td><td class="patent-data-table-td patent-date-value">May 7, 2013</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Method and system of enabling intelligent and lightweight speech to text transcription through distributed environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8468358">US8468358</a></td><td class="patent-data-table-td patent-date-value">Nov 9, 2010</td><td class="patent-data-table-td patent-date-value">Jun 18, 2013</td><td class="patent-data-table-td ">Veritrix, Inc.</td><td class="patent-data-table-td ">Methods for identifying the guarantor of an application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8474014">US8474014</a></td><td class="patent-data-table-td patent-date-value">Aug 16, 2011</td><td class="patent-data-table-td patent-date-value">Jun 25, 2013</td><td class="patent-data-table-td ">Veritrix, Inc.</td><td class="patent-data-table-td ">Methods for the secure use of one-time passwords</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8516562">US8516562</a></td><td class="patent-data-table-td patent-date-value">Aug 18, 2011</td><td class="patent-data-table-td patent-date-value">Aug 20, 2013</td><td class="patent-data-table-td ">Veritrix, Inc.</td><td class="patent-data-table-td ">Multi-channel multi-factor authentication</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8536976">US8536976</a></td><td class="patent-data-table-td patent-date-value">Jun 11, 2008</td><td class="patent-data-table-td patent-date-value">Sep 17, 2013</td><td class="patent-data-table-td ">Veritrix, Inc.</td><td class="patent-data-table-td ">Single-channel multi-factor authentication</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8555066">US8555066</a></td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td patent-date-value">Oct 8, 2013</td><td class="patent-data-table-td ">Veritrix, Inc.</td><td class="patent-data-table-td ">Systems and methods for controlling access to encrypted data stored on a mobile device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8612235">US8612235</a></td><td class="patent-data-table-td patent-date-value">Jun 8, 2012</td><td class="patent-data-table-td patent-date-value">Dec 17, 2013</td><td class="patent-data-table-td ">Vocollect, Inc.</td><td class="patent-data-table-td ">Method and system for considering information about an expected response when performing speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8635243">US8635243</a></td><td class="patent-data-table-td patent-date-value">Aug 27, 2010</td><td class="patent-data-table-td patent-date-value">Jan 21, 2014</td><td class="patent-data-table-td ">Research In Motion Limited</td><td class="patent-data-table-td ">Sending a communications header with voice recording to send metadata for use in speech recognition, formatting, and search mobile search application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8639508">US8639508</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 14, 2011</td><td class="patent-data-table-td patent-date-value">Jan 28, 2014</td><td class="patent-data-table-td ">General Motors Llc</td><td class="patent-data-table-td ">User-specific confidence thresholds for speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8700396">US8700396</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 8, 2012</td><td class="patent-data-table-td patent-date-value">Apr 15, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Generating speech data collection prompts</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8719017">US8719017</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 15, 2008</td><td class="patent-data-table-td patent-date-value">May 6, 2014</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">Systems and methods for dynamic re-configurable speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8750464">US8750464</a></td><td class="patent-data-table-td patent-date-value">Nov 28, 2012</td><td class="patent-data-table-td patent-date-value">Jun 10, 2014</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">System and method for indexing voice mail messages by speaker</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8751227">US8751227</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 10, 2009</td><td class="patent-data-table-td patent-date-value">Jun 10, 2014</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Acoustic model learning device and speech recognition device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8756059">US8756059</a></td><td class="patent-data-table-td patent-date-value">Dec 30, 2010</td><td class="patent-data-table-td patent-date-value">Jun 17, 2014</td><td class="patent-data-table-td ">Vocollect, Inc.</td><td class="patent-data-table-td ">Method and system for considering information about an expected response when performing speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20080221901">US20080221901</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 3, 2007</td><td class="patent-data-table-td patent-date-value">Sep 11, 2008</td><td class="patent-data-table-td ">Joseph Cerra</td><td class="patent-data-table-td ">Mobile general search environment speech processing facility</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20080235024">US20080235024</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 20, 2007</td><td class="patent-data-table-td patent-date-value">Sep 25, 2008</td><td class="patent-data-table-td ">Itzhack Goldberg</td><td class="patent-data-table-td ">Method and system for text-to-speech synthesis with personalized voice</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110046952">US20110046952</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 10, 2009</td><td class="patent-data-table-td patent-date-value">Feb 24, 2011</td><td class="patent-data-table-td ">Takafumi Koshinaka</td><td class="patent-data-table-td ">Acoustic model learning device and speech recognition device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110054896">US20110054896</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 27, 2010</td><td class="patent-data-table-td patent-date-value">Mar 3, 2011</td><td class="patent-data-table-td ">Phillips Michael S</td><td class="patent-data-table-td ">Sending a communications header with voice recording to send metadata for use in speech recognition and formatting in mobile dictation application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110054899">US20110054899</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 27, 2010</td><td class="patent-data-table-td patent-date-value">Mar 3, 2011</td><td class="patent-data-table-td ">Phillips Michael S</td><td class="patent-data-table-td ">Command and control utilizing content information in a mobile voice-to-speech application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110054900">US20110054900</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 27, 2010</td><td class="patent-data-table-td patent-date-value">Mar 3, 2011</td><td class="patent-data-table-td ">Phillips Michael S</td><td class="patent-data-table-td ">Hybrid command and control between resident and remote speech recognition facilities in a mobile voice-to-speech application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110060587">US20110060587</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 27, 2010</td><td class="patent-data-table-td patent-date-value">Mar 10, 2011</td><td class="patent-data-table-td ">Phillips Michael S</td><td class="patent-data-table-td ">Command and control utilizing ancillary information in a mobile voice-to-speech application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110066433">US20110066433</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 16, 2009</td><td class="patent-data-table-td patent-date-value">Mar 17, 2011</td><td class="patent-data-table-td ">At&amp;T Intellectual Property I, L.P.</td><td class="patent-data-table-td ">System and method for personalization of acoustic models for automatic speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120130709">US20120130709</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 23, 2010</td><td class="patent-data-table-td patent-date-value">May 24, 2012</td><td class="patent-data-table-td ">At&amp;T Intellectual Property I, L.P.</td><td class="patent-data-table-td ">System and method for building and evaluating automatic speech recognition via an application programmer interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120209609">US20120209609</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 14, 2011</td><td class="patent-data-table-td patent-date-value">Aug 16, 2012</td><td class="patent-data-table-td ">General Motors Llc</td><td class="patent-data-table-td ">User-specific confidence thresholds for speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2189976A1?cl=en">EP2189976A1</a></td><td class="patent-data-table-td patent-date-value">Nov 21, 2008</td><td class="patent-data-table-td patent-date-value">May 26, 2010</td><td class="patent-data-table-td ">Harman Becker Automotive Systems GmbH</td><td class="patent-data-table-td ">Method for adapting a codebook for speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2013169232A1?cl=en">WO2013169232A1</a></td><td class="patent-data-table-td patent-date-value">May 8, 2012</td><td class="patent-data-table-td patent-date-value">Nov 14, 2013</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Differential acoustic model representation and linear transform-based adaptation for efficient user profile update techniques in automatic speech recognition</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=1nFnBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S243000">704/243</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=1nFnBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S244000">704/244</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=1nFnBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc379/defs379.htm&usg=AFQjCNEr2i5HiMlkBIt1vZADj0MjdHVCTw#C379S088010">379/88.01</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=1nFnBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704SE15047">704/E15.047</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=1nFnBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0015280000">G10L15/28</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=1nFnBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0015060000">G10L15/06</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=1nFnBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L15/30">G10L15/30</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=1nFnBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L2015/0638">G10L2015/0638</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G10L15/30</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 1-55 IS CONFIRMED. NEW CLAIMS 56-154 ARE ADDED AND DETERMINED TO BE PATENTABLE.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 21, 2011</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 28, 2011</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20110510</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 9, 2010</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100730</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 17, 2008</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 24, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG. STAMFORD BRANCH, CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:018160/0909</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20060331</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:018160/0909</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG. STAMFORD BRANCH,CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;US-ASSIGNMENT DATABASE UPDATED:20100216;REEL/FRAME:18160/909</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:18160/909</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 7, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG, STAMFORD BRANCH, CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:017435/0199</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20060331</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG, STAMFORD BRANCH,CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;US-ASSIGNMENT DATABASE UPDATED:20100216;REEL/FRAME:17435/199</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;US-ASSIGNMENT DATABASE UPDATED:20100309;REEL/FRAME:17435/199</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 12, 2001</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">NUANCE COMMUNICATIONS INC. (DELAWARE CORP.), CALIF</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:NUANCE COMMUNICATIONS (CALIFORNIA CORP.);REEL/FRAME:011518/0967</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20010201</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">NUANCE COMMUNICATIONS INC. (DELAWARE CORP.) 1380 W</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:NUANCE COMMUNICATIONS (CALIFORNIA CORP.) /AR;REEL/FRAME:011518/0967</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 10, 1999</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">NUANCE COMMUNICATIONS, CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MURVEIT, HY;KANNAN, ASHVIN;REEL/FRAME:009964/0515</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19990506</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U1aJKHtsYp-KLMHV48L0xpgv9bSUA\u0026id=1nFnBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U1o11Idl8B-RwP6p3mWpmMwirsGeQ\u0026id=1nFnBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U1uabtzScV_47H3EElDxYujHwjSCg","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Adaptation_of_a_speech_recognition_syste.pdf?id=1nFnBAABERAJ\u0026output=pdf\u0026sig=ACfU3U0Hki2YUF1cv-_LZdEB3Qzo_GZ2NQ"},"sample_url":"http://www.google.com/patents/reader?id=1nFnBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>