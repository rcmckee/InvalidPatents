<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7568628 - Bar code reading device with global electronic shutter control - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Bar code reading device with global electronic shutter control"><meta name="DC.contributor" content="Ynjiun Wang" scheme="inventor"><meta name="DC.contributor" content="William H. Havens" scheme="inventor"><meta name="DC.contributor" content="Hand Held Products, Inc." scheme="assignee"><meta name="DC.date" content="2005-3-11" scheme="dateSubmitted"><meta name="DC.description" content="The invention features an image reader and a corresponding method for capturing a sharp distortion free image of a target, such as a one or two-dimensional bar code. In one embodiment, the image reader comprises a two-dimensional CMOS based image sensor array, a timing module, an illumination module, and a control module. The time during which the target is illuminated is referred to as the illumination period. The capture of the image by the image sensor array is driven by the timing module that, in one embodiment, is able to simultaneously expose substantially all of the pixels in the array. The time during which the pixels are collectively activated to photo-convert incident light into charge defines the exposure period for the sensor array. In one embodiment, at least a portion of the exposure period occurs during the illumination period."><meta name="DC.date" content="2009-8-4" scheme="issued"><meta name="DC.relation" content="US:20050103854:A1" scheme="references"><meta name="DC.relation" content="US:3684868" scheme="references"><meta name="DC.relation" content="US:3716699" scheme="references"><meta name="DC.relation" content="US:4253447" scheme="references"><meta name="DC.relation" content="US:4261344" scheme="references"><meta name="DC.relation" content="US:4350418" scheme="references"><meta name="DC.relation" content="US:4491865" scheme="references"><meta name="DC.relation" content="US:4516017" scheme="references"><meta name="DC.relation" content="US:4523224" scheme="references"><meta name="DC.relation" content="US:4546379" scheme="references"><meta name="DC.relation" content="US:4793689" scheme="references"><meta name="DC.relation" content="US:4806776" scheme="references"><meta name="DC.relation" content="US:4853774" scheme="references"><meta name="DC.relation" content="US:4854302" scheme="references"><meta name="DC.relation" content="US:4862253" scheme="references"><meta name="DC.relation" content="US:4877949" scheme="references"><meta name="DC.relation" content="US:4941456" scheme="references"><meta name="DC.relation" content="US:4957346" scheme="references"><meta name="DC.relation" content="US:4963756" scheme="references"><meta name="DC.relation" content="US:5019699" scheme="references"><meta name="DC.relation" content="US:5059146" scheme="references"><meta name="DC.relation" content="US:5144190" scheme="references"><meta name="DC.relation" content="US:5222477" scheme="references"><meta name="DC.relation" content="US:5278642" scheme="references"><meta name="DC.relation" content="US:5291008" scheme="references"><meta name="DC.relation" content="US:5305122" scheme="references"><meta name="DC.relation" content="US:5308962" scheme="references"><meta name="DC.relation" content="US:5337361" scheme="references"><meta name="DC.relation" content="US:5393965" scheme="references"><meta name="DC.relation" content="US:5399846" scheme="references"><meta name="DC.relation" content="US:5401944" scheme="references"><meta name="DC.relation" content="US:5406062" scheme="references"><meta name="DC.relation" content="US:5410141" scheme="references"><meta name="DC.relation" content="US:5471533" scheme="references"><meta name="DC.relation" content="US:5504322" scheme="references"><meta name="DC.relation" content="US:5513264" scheme="references"><meta name="DC.relation" content="US:5521366" scheme="references"><meta name="DC.relation" content="US:5527262" scheme="references"><meta name="DC.relation" content="US:5541777" scheme="references"><meta name="DC.relation" content="US:5572006" scheme="references"><meta name="DC.relation" content="US:5576529" scheme="references"><meta name="DC.relation" content="US:5591955" scheme="references"><meta name="DC.relation" content="US:5602377" scheme="references"><meta name="DC.relation" content="US:5637849" scheme="references"><meta name="DC.relation" content="US:5646390" scheme="references"><meta name="DC.relation" content="US:5654533" scheme="references"><meta name="DC.relation" content="US:5662586" scheme="references"><meta name="DC.relation" content="US:5691773" scheme="references"><meta name="DC.relation" content="US:5702058" scheme="references"><meta name="DC.relation" content="US:5702059" scheme="references"><meta name="DC.relation" content="US:5703349" scheme="references"><meta name="DC.relation" content="US:5714745" scheme="references"><meta name="DC.relation" content="US:5717195" scheme="references"><meta name="DC.relation" content="US:5739518" scheme="references"><meta name="DC.relation" content="US:5756981" scheme="references"><meta name="DC.relation" content="US:5763864" scheme="references"><meta name="DC.relation" content="US:5763866" scheme="references"><meta name="DC.relation" content="US:5770847" scheme="references"><meta name="DC.relation" content="US:5773810" scheme="references"><meta name="DC.relation" content="US:5783811" scheme="references"><meta name="DC.relation" content="US:5784102" scheme="references"><meta name="DC.relation" content="US:5786582" scheme="references"><meta name="DC.relation" content="US:5786586" scheme="references"><meta name="DC.relation" content="US:5793033" scheme="references"><meta name="DC.relation" content="US:5811774" scheme="references"><meta name="DC.relation" content="US:5811784" scheme="references"><meta name="DC.relation" content="US:5811828" scheme="references"><meta name="DC.relation" content="US:5814801" scheme="references"><meta name="DC.relation" content="US:5815200" scheme="references"><meta name="DC.relation" content="US:5818023" scheme="references"><meta name="DC.relation" content="US:5818028" scheme="references"><meta name="DC.relation" content="US:5821518" scheme="references"><meta name="DC.relation" content="US:5831254" scheme="references"><meta name="DC.relation" content="US:5834754" scheme="references"><meta name="DC.relation" content="US:5837987" scheme="references"><meta name="DC.relation" content="US:5841121" scheme="references"><meta name="DC.relation" content="US:5877487" scheme="references"><meta name="DC.relation" content="US:5914476" scheme="references"><meta name="DC.relation" content="US:5917913" scheme="references"><meta name="DC.relation" content="US:5940163" scheme="references"><meta name="DC.relation" content="US:5949052" scheme="references"><meta name="DC.relation" content="US:5979763" scheme="references"><meta name="DC.relation" content="US:5986297" scheme="references"><meta name="DC.relation" content="US:6010070" scheme="references"><meta name="DC.relation" content="US:6010073" scheme="references"><meta name="DC.relation" content="US:6019286" scheme="references"><meta name="DC.relation" content="US:6042012" scheme="references"><meta name="DC.relation" content="US:6045047" scheme="references"><meta name="DC.relation" content="US:6045238" scheme="references"><meta name="DC.relation" content="US:6053407" scheme="references"><meta name="DC.relation" content="US:6060722" scheme="references"><meta name="DC.relation" content="US:6062475" scheme="references"><meta name="DC.relation" content="US:6073851" scheme="references"><meta name="DC.relation" content="US:6075240" scheme="references"><meta name="DC.relation" content="US:6102295" scheme="references"><meta name="DC.relation" content="US:6123261" scheme="references"><meta name="DC.relation" content="US:6142934" scheme="references"><meta name="DC.relation" content="US:6152368" scheme="references"><meta name="DC.relation" content="US:6311895" scheme="references"><meta name="DC.relation" content="US:6637658" scheme="references"><meta name="DC.relation" content="US:7083098" scheme="references"><meta name="DC.relation" content="US:7128266" scheme="references"><meta name="DC.relation" content="US:RE31289" scheme="references"><meta name="DC.relation" content="US:RE31290" scheme="references"><meta name="citation_reference" content="Agilent Technologies, Pulsed Operating Ranges for AlInGap LEDs vs. Projected Long Term Light Output Performance, Nov. 1999, pp. 1-6."><meta name="citation_reference" content="Claim set of U.S. Appl. No. 12/132,462, filed Jun. 3, 2008, 15 pages."><meta name="citation_reference" content="Claim set of U.S. Appl. No. 12/132,480, filed Jun. 3, 2008, 8 pages."><meta name="citation_reference" content="Eastman Kodak Company, Fully Integrated Timing, Analog Signal Processing &amp; 10 bit ADC, Technical Data, Aug. 5, 2002, pp. 1-56."><meta name="citation_reference" content="Eastman Kodak Company, Ultra Sensitive Global Shutter 580 fps Monochrome CIS, Device Performance Specification, Sep. 2004, pp. 1-22."><meta name="citation_reference" content="International Search Report of PCT/US2006/008113, (6 pages), Aug. 7, 2006."><meta name="citation_reference" content="International Search Report of PCT/US2006/008113, (9 pages), Oct. 26, 2006."><meta name="citation_reference" content="International Search Report of PCT/US2006/008114, (4 pgs.), Jul. 7, 2006."><meta name="citation_reference" content="Kodak Image Sensor Solutions, Color Better through CMY Filters, www.kodak.com/go/ccd, pp. 1-2. Month and year unavailable but known to be published prior to earliest priority date of Jun. 3, 2005."><meta name="citation_reference" content="Kodak Image Sensor Solutions, Device Performance Specification for Kodak KAC-9630 CMOS Image Sensor, pp. 1-22, Revision 1.1, Sep. 2004."><meta name="citation_reference" content="Kodak Image Sensor Solutions, Device Performance Specification for Kodak KAI-0340S and Kodak KAI-0340D Image Sensor, pp. 1-54, Revision 1.0, Aug. 6, 2004."><meta name="citation_reference" content="Micron Technology, Inc., MT9M111 SOC Megapixel Digital Image Sensor, Products and Specifications Preliminary Manual, pp. 1-61, 2004."><meta name="citation_reference" content="Micron Technology, Inc., MT9M413 1.3-Megapixel CMOS Active Pixel Digital Image Sensor, Specification manual, Version.3.0, pp. 1-30, Jan. 2004."><meta name="citation_reference" content="Office action for Chinese Patent Application No. 2006800160235, Office action dated Jan. 23, 2009, 6 pages (including English translation thereof, 7 pages)."><meta name="citation_reference" content="PCT/US2006/008113, PCT Written Opinion (9 pages), Oct. 26, 2006."><meta name="citation_reference" content="PCT/US2006/008114, PCT Written Opinion (6 pgs.), Jul. 7, 2006."><meta name="citation_reference" content="STMicroelectronics, STMicroelectronics Introduces Low-Cost High-Quality Mega Pixel CMOS Sensor For Digital Still Cameras and Camcorders, Introduction article on website http://www.st.com/stonline/press/news/year2002/p1239p.htm, p. 1, Oct. 9, 2002."><meta name="citation_reference" content="WWW.FILLFACTORY.COM, Dual Slope Dynamic Range Expansion, Website, Feb. 28, 2005, pp. 1-3."><meta name="citation_reference" content="WWW.MICRON.COM, Introducing A CMOS Image Sensor Specifically Designed for Automotive Scene-Understanding Systems, Website, Oct. 2, 2004, pp. 1-2."><meta name="citation_reference" content="WWW.PHOTONFOCUS.COM, LINOG(TM) Technology The Key To Programmable Linear, Logarithmic, or Combined Linear And Logarithmic Response Without Image Lag Or Distortion, Website, Feb. 28, 2005, pp. 1-6."><meta name="citation_patent_number" content="US:7568628"><meta name="citation_patent_application_number" content="US:11/077,975"><link rel="canonical" href="http://www.google.com/patents/US7568628"/><meta property="og:url" content="http://www.google.com/patents/US7568628"/><meta name="title" content="Patent US7568628 - Bar code reading device with global electronic shutter control"/><meta name="description" content="The invention features an image reader and a corresponding method for capturing a sharp distortion free image of a target, such as a one or two-dimensional bar code. In one embodiment, the image reader comprises a two-dimensional CMOS based image sensor array, a timing module, an illumination module, and a control module. The time during which the target is illuminated is referred to as the illumination period. The capture of the image by the image sensor array is driven by the timing module that, in one embodiment, is able to simultaneously expose substantially all of the pixels in the array. The time during which the pixels are collectively activated to photo-convert incident light into charge defines the exposure period for the sensor array. In one embodiment, at least a portion of the exposure period occurs during the illumination period."/><meta property="og:title" content="Patent US7568628 - Bar code reading device with global electronic shutter control"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("QkTsU_3sENLMsQTfhwI"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("GBR"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("QkTsU_3sENLMsQTfhwI"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("GBR"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7568628?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7568628"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=5AvPBQABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7568628&amp;usg=AFQjCNHHmuh7F0C4oMkQo5uyYzGF_ZytNw" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7568628.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7568628.pdf"></a><a class="appbar-application-grant-link" data-label="Application" href="/patents/US20060202036"></a><a class="appbar-application-grant-link" data-selected="true" data-label="Grant" href="/patents/US7568628"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7568628" style="display:none"><span itemprop="description">The invention features an image reader and a corresponding method for capturing a sharp distortion free image of a target, such as a one or two-dimensional bar code. In one embodiment, the image reader comprises a two-dimensional CMOS based image sensor array, a timing module, an illumination module,...</span><span itemprop="url">http://www.google.com/patents/US7568628?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7568628 - Bar code reading device with global electronic shutter control</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7568628 - Bar code reading device with global electronic shutter control" title="Patent US7568628 - Bar code reading device with global electronic shutter control"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7568628 B2</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 11/077,975</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Aug 4, 2009</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Mar 11, 2005</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Mar 11, 2005</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CN101069190A">CN101069190A</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN101069190B">CN101069190B</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN101171597A">CN101171597A</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN101171597B">CN101171597B</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7909257">US7909257</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8733660">US8733660</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20060202036">US20060202036</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20100090007">US20100090007</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20110163166">US20110163166</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20140204257">US20140204257</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">077975, </span><span class="patent-bibdata-value">11077975, </span><span class="patent-bibdata-value">US 7568628 B2, </span><span class="patent-bibdata-value">US 7568628B2, </span><span class="patent-bibdata-value">US-B2-7568628, </span><span class="patent-bibdata-value">US7568628 B2, </span><span class="patent-bibdata-value">US7568628B2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Ynjiun+Wang%22">Ynjiun Wang</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22William+H.+Havens%22">William H. Havens</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Hand+Held+Products,+Inc.%22">Hand Held Products, Inc.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7568628.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7568628.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7568628.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (104),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (20),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (23),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (9),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (5)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=5AvPBQABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7568628&usg=AFQjCNGNVSnHcvqAc6h2r156SRIQWyyR2Q">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=5AvPBQABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7568628&usg=AFQjCNEyYbPwt_BwX-bVQOV_Bp4FC2Pdyg">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=5AvPBQABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7568628B2%26KC%3DB2%26FT%3DD&usg=AFQjCNGomriryRVKRviGhibU2sEQbWmP_g">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT76386502" lang="EN" load-source="patent-office">Bar code reading device with global electronic shutter control</invention-title></span><br><span class="patent-number">US 7568628 B2</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA62596578" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">The invention features an image reader and a corresponding method for capturing a sharp distortion free image of a target, such as a one or two-dimensional bar code. In one embodiment, the image reader comprises a two-dimensional CMOS based image sensor array, a timing module, an illumination module, and a control module. The time during which the target is illuminated is referred to as the illumination period. The capture of the image by the image sensor array is driven by the timing module that, in one embodiment, is able to simultaneously expose substantially all of the pixels in the array. The time during which the pixels are collectively activated to photo-convert incident light into charge defines the exposure period for the sensor array. In one embodiment, at least a portion of the exposure period occurs during the illumination period.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(31)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00018.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00018.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00019.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00019.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00020.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00020.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00021.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00021.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00022.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00022.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00023.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00023.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00024.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00024.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00025.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00025.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00026.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00026.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00027.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00027.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00028.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00028.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00029.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00029.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7568628B2/US07568628-20090804-D00030.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7568628B2/US07568628-20090804-D00030.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(35)</span></span></div><div class="patent-text"><div mxw-id="PCLM23064037" lang="EN" load-source="patent-office" class="claims">
  <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
    <div class="claim-text">1. A complementary metal oxide semiconductor (CMOS) based image reader for collecting image data from a target, the CMOS based image reader comprising:
<div class="claim-text">a CMOS based image sensor array, the CMOS based image sensor array comprising a plurality of rows of pixels, each of the pixels of the CMOS based image sensor array being an active pixel comprising a pixel amplifier, a photosensitive region and an opaque shielded data storage region;</div>
<div class="claim-text">a hand held housing encapsulating said image sensor array;</div>
<div class="claim-text">a timing module in electrical communication with the CMOS based image sensor array, the timing module configured to simultaneously expose a full frame of pixels of the CMOS based image sensor array during an exposure period;</div>
<div class="claim-text">an illumination module configured to illuminate the target during an illumination period, the illumination module in electrical communication with the timing module;</div>
<div class="claim-text">a control module in electrical communication with the timing module and the illumination module, the control module configured to cause at least a portion of the exposure period to occur during the illumination period;</div>
<div class="claim-text">wherein said CMOS based image sensor array is capable of being controlled so that exposure of pixels of a plurality of different rows of said CMOS based image sensor array is initiated simultaneously, and wherein said illumination period is less than a frame time of said CMOS based image reader; and</div>
<div class="claim-text">a bar code processing module capable of identifying representations of bar code symbols in collected image data.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
    <div class="claim-text">2. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said illumination module for illuminating said target overdrives a light source of said illumination module.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
    <div class="claim-text">3. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said illumination module comprises a light emitting diode.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
    <div class="claim-text">4. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said exposure period is shorter than said illumination period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
    <div class="claim-text">5. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said illumination period is shorter than said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
    <div class="claim-text">6. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said illumination period commences before the start of said exposure period and said illumination period ends before the end of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
    <div class="claim-text">7. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said illumination period commences before the start of said exposure period and said illumination period ends after the end of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
    <div class="claim-text">8. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said illumination period commences after the start of said exposure period and said illumination period ends before the end of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
    <div class="claim-text">9. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said illumination period commences after the start of said exposure period and said illumination period ends after the end of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
    <div class="claim-text">10. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said illumination period commences before the start of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
    <div class="claim-text">11. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said illumination period commences after the start of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
    <div class="claim-text">12. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said illumination period ends before the end of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
    <div class="claim-text">13. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said illumination period ends after the end of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
    <div class="claim-text">14. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the exposure period is less than 3.7 milliseconds.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
    <div class="claim-text">15. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the illumination period is less than 3.7 milliseconds.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
    <div class="claim-text">16. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said illumination module during said illumination period is controlled utilizing a DC driving signal.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
    <div class="claim-text">17. The CMOS based image reader according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said illumination module during said illumination period is controlled utilizing a strobed pulse.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00018" num="00018" class="claim">
    <div class="claim-text">18. A complementary metal oxide semiconductor (CMOS) based bar code reading device for collecting image data from a target, the CMOS based bar code reading device comprising:
<div class="claim-text">an integrated circuit including at least a CMOS based image sensor array, the CMOS based image sensor array comprising a first plurality of pixels and a second plurality of pixels, the bar code reading device being programmable to selectively address and read out the first plurality of pixels of the CMOS based image sensor array independently of the second plurality of pixels of the CMOS based image sensor array, each of the pixels of the CMOS based image sensor array comprising a photosensitive region, an amplifier, and an opaque shielded data storage region;</div>
<div class="claim-text">a portable housing encapsulating said image sensor array;</div>
<div class="claim-text">a global electronic shutter control circuitry, the global electronic shutter control circuitry configured to generate an exposure control timing pulse that is capable of causing the simultaneous exposure of all or substantially all of an entire frame of pixels of the CMOS based image sensor array;</div>
<div class="claim-text">at least one light source configured to illuminate the target in response to an illumination control timing pulse, the at least one light source in electrical communication with the integrated circuit,</div>
<div class="claim-text">wherein at least a portion of the illumination control timing pulse overlaps with at least a portion of the exposure control timing pulse;</div>
<div class="claim-text">wherein said CMOS based image sensor array is capable of being controlled so that exposure of pixels of a plurality of different rows of said CMOS based image sensor array is initiated simultaneously, and wherein said illumination control timing pulse has a duration of less than a frame time of said CMOS based bar code reading device; and</div>
<div class="claim-text">a bar code processing module capable of identifying representations of bar code symbols in collected image data.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
    <div class="claim-text">19. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein said CMOS based bar code reading device is configured so that for illumination of said target said CMOS based bar code reading device overdrives said at least one light source.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
    <div class="claim-text">20. The CMOS based bar code reading device according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein said at least one light source comprises a light emitting diode.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00021" num="00021" class="claim">
    <div class="claim-text">21. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the exposure control timing pulse has a shorter duration than the illumination control timing pulse.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00022" num="00022" class="claim">
    <div class="claim-text">22. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the illumination control timing pulse has a shorter duration than the exposure control timing pulse.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00023" num="00023" class="claim">
    <div class="claim-text">23. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the illumination control timing pulse starts before the start of the exposure control timing pulse and the illumination control timing pulse ends before the end of the exposure control timing pulse.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00024" num="00024" class="claim">
    <div class="claim-text">24. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the exposure control timing pulse has a duration of less than 3.7 milliseconds.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00025" num="00025" class="claim">
    <div class="claim-text">25. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein said illumination control timing pulse is a DC driving signal.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00026" num="00026" class="claim">
    <div class="claim-text">26. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein said illumination control timing pulse is a strobed pulse.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00027" num="00027" class="claim">
    <div class="claim-text">27. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein said illumination control timing pulse has a duration of less than 3.7 milliseconds.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00028" num="00028" class="claim">
    <div class="claim-text">28. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein said illumination period commences before the start of said exposure period and said illumination period ends after the end of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00029" num="00029" class="claim">
    <div class="claim-text">29. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein said illumination period commences after the start of said exposure period and said illumination period ends before the end of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00030" num="00030" class="claim">
    <div class="claim-text">30. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein said illumination period commences after the start of said exposure period and said illumination period ends after the end of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00031" num="00031" class="claim">
    <div class="claim-text">31. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein said illumination period commences before the start of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00032" num="00032" class="claim">
    <div class="claim-text">32. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein said illumination period commences after the start of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00033" num="00033" class="claim">
    <div class="claim-text">33. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein said illumination period ends before the end of said exposure period.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00034" num="00034" class="claim">
    <div class="claim-text">34. The CMOS based bar code reading device according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein said illumination period ends after the end of said exposure period.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00035" num="00035" class="claim">
    <div class="claim-text">35. A bar code reading device for collecting and processing bar code data from a bar code symbol, the bar code reading device comprising:
<div class="claim-text">a two-dimensional array of pixels for receiving light radiation reflected from the bar code symbol, the two-dimensional array of pixels comprising a first plurality of pixels and a second plurality of pixels, the bar code reading device being configured so that said bar code reading device can be controlled to selectively address and read out from said two-dimensional array image data from a first plurality of pixels of said array independently of a second plurality of pixels of said array, each of the pixels comprising a photosensitive region and an opaque shielded data storage region;</div>
<div class="claim-text">a hand held housing encapsulating said two-dimensional array of pixels;</div>
<div class="claim-text">an illumination module configured to illuminate a target during an illumination period, the illumination module in electrical communication with a timing module;</div>
<div class="claim-text">optics for directing light radiation reflected from the bar code symbol onto the two-dimensional array of pixels;</div>
<div class="claim-text">a global electronic shutter associated with the two-dimensional array of pixels, the global electronic shutter capable of simultaneously exposing all or substantially all of the pixels in the two-dimensional array, said global electronic shutter being capable of controlling said two-dimensional array so that exposure of pixels of a plurality of different rows of said two-dimensional array is initiated simultaneously, and wherein said illumination period is less than a frame time of said bar code reading device; and</div>
<div class="claim-text">a bar code processing module, the bar code processing module in electronic communication with the two-dimensional array of pixels, the bar code processing module capable of identifying representations of bar code symbols in collected image data.</div>
</div>
  </div>
</div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES28042769" lang="EN" load-source="patent-office" class="description">
<heading>CROSS-REFERENCE TO RELATED APPLICATIONS</heading> <p num="p-0002">This application is related to the application enumerated below, filed with the United States Patent and Trademark Office contemporaneously with the present application on Mar. 11, 2005 by Express Mail and subject to assignment to the same assignee of this application, the disclosure of which is incorporated herein by reference in its entirety: U.S. patent application Ser. No. 11/077,976, entitled “System And Method To Automatically Focus An Image Reader.”</p>
  <heading>FIELD OF THE INVENTION</heading> <p num="p-0003">The invention relates to image data collection in general and particularly to an image data collector with coordinated illumination and global shutter control.</p>
  <heading>BACKGROUND OF THE INVENTION</heading> <p num="p-0004">Many traditional imager readers, such as hand held and fixed mounted bar code and machine code readers, employ charge-coupled device (CCDs) based image sensors. A CCD based image sensor contains an array of electrically coupled light sensitive photodiodes that convert incident light energy into packets of electric charge. In operation, the charge packets are shifted out of the CCD imager sensor for subsequent processing.</p>
  <p num="p-0005">Some image readers employ CMOS based image sensors as an alternative imaging technology. As with CCDs, CMOS based image sensors contain arrays of light sensitive photodiodes that convert incident light energy into electric charge. Unlike CCDs, however, CMOS based image sensors allow each pixel in a two-dimensional array to be directly addressed. One advantage of this is that sub-regions of a full frame of image data can be independently accessed. Another advantage of CMOS based image sensors is that in general they have lower costs per pixel. This is primarily due to the fact that CMOS image sensors are made with standard CMOS processes in high volume wafer fabrication facilities that produce common integrated circuits such as microprocessors and the like. In addition to lower cost, the common fabrication process means that a CMOS pixel array can be integrated on a single circuit with other standard electronic devices such as clock drivers, digital logic, analog/digital converters and the like. This in turn has the further advantage of reducing space requirements and lowering power usage.</p>
  <p num="p-0006">CMOS based image readers have traditionally employed rolling shutters to expose pixels in the sensor array. In a rolling shutter architecture, rows of pixels are activated and read out in sequence. The exposure or integration time for a pixel is the time between a pixel being reset and its value being read out. This concept is presented in <figref idrefs="DRAWINGS">FIG. 2A</figref>. In <figref idrefs="DRAWINGS">FIG. 2A</figref>, the exposure for each of the rows “a” though “n” is diagrammatically represented by the bars <b>4</b> <i>a </i>. . . <b>4</b> <i>n </i>(generally <b>4</b>). The horizontal extent <b>8</b> of each bar is intended to correspond to the exposure period for a particular row. The horizontal displacement of each bar <b>4</b> is suggestive of the shifting time period during which each row of pixels is exposed. As can be seen in <figref idrefs="DRAWINGS">FIG. 2A</figref>, the exposure period for sequential rows overlap. This is shown in more detail with respect to the timing diagrams for a rolling shutter architecture shown in <figref idrefs="DRAWINGS">FIG. 2B</figref>. The second 12 and third 16 lines of the timing diagram represent the reset timing signal and the read out timing signal, respectively, for row “a.” The fourth 20 and fifth 24 lines represent the reset and the read out timing signals, respectively for row “b.” As shown in both <figref idrefs="DRAWINGS">FIGS. 2A and 2B</figref>, the exposure for row “b” is initiated before the values for row “a” are read out. The exposure periods for adjacent rows of pixels typically overlap substantially as several hundred rows of pixels must be exposed and read during the capture of a frame of data. As shown by the illumination timing signal on the first line <b>28</b>, the rolling shutter architecture with its overlapping exposure periods requires that the illumination source remain on during substantially all of the time required to capture a frame of data so that illumination is provided for all of the rows.</p>
  <p num="p-0007">In operation, the rolling shutter architecture suffers from at least two disadvantages: image distortion and image blur. Image distortion is an artifact of the different times at which each row of pixels is exposed. The effect of image distortion is most pronounced when fast moving objects are visually recorded. The effect is demonstrated in the image shown in <figref idrefs="DRAWINGS">FIG. 3</figref> that shows a representation of an image taken with a rolling shutter of a bus image pixels <b>50</b> passing through the field of view from right to left. As the top row of bus image pixels <b>54</b> of the bus was taken earlier than the bottom row of pixels <b>58</b>, and as the bus was traveling to the left, the bottom row of bus image pixels <b>58</b> is displaced to the left relative to the top row of bus image pixels <b>54</b>.</p>
  <p num="p-0008">Image blur is an artifact of the long exposure periods typically required in a rolling shutter architecture in an image reader. As indicated above, in a rolling shutter architecture the illumination source must remain on during substantially all of the time required to capture a frame of data. Due to battery and/or illumination source limitations, the light provided during the capture of an entire frame of data is usually not adequate for short exposure times. Without a short exposure time, blur inducing effects become pronounced. Common examples of blur inducing effects include the displacement of an image sensor due to, for example, hand shake with a hand held image reader.</p>
  <p num="p-0009">What is needed is an image reader that overcomes the drawbacks of current CMOS image readers including image distortion and image blur.</p>
  <heading>SUMMARY OF THE INVENTION</heading> <p num="p-0010">In one aspect, the invention features a complementary metal oxide semiconductor (CMOS) based image reader for collecting image data from a target. The CMOS based imager reader comprises a CMOS based image sensor array; a timing module in electrical communication with the CMOS based image sensor array. The timing module is capable of simultaneously exposing an entire frame of pixels of the CMOS based image sensor array during an exposure period. The CMOS based image reader also comprises an illumination module capable of illuminating the target during an illumination period. The illumination module is in electrical communication with the timing module. The CMOS based image reader further comprises a control module in electrical communication with the timing module and the illumination module. The control module is capable of causing at least a portion of the exposure period to occur during the illumination period. In one embodiment of the CMOS based image reader, illuminating the target comprises overdriving light sources in the illumination module. In another embodiment of the CMOS based image reader, the light sources comprise light emitting diodes. In a further embodiment of the CMOS based image reader, the exposure period starts after the start of the illumination period and the exposure period ends before the end of the illumination period. In yet another embodiment of the CMOS based image reader, the illumination period starts after the start of the exposure period and the illumination period ends before the end of the exposure period. In yet an additional embodiment of the CMOS based image reader, the illumination period starts before the start of the exposure period and the illumination period ends before the end of the exposure period. In yet a further embodiment of the CMOS based image reader, the exposure period has a duration of less than 3.7 milliseconds. In various embodiments of the CMOS based image reader, the target includes a symbology such as a one-dimensional bar code such as a Code 39 or a UPC code or a two-dimensional bar code such as a PDF417 bar code, an Aztec symbol or Datamatrix symbol.</p>
  <p num="p-0011">In another aspect the invention features a complementary metal oxide semiconductor (CMOS) based image reader for collecting image data from a target. The CMOS based imager reader comprises an integrated circuit including at least a CMOS based image sensor array and global electronic shutter control circuitry. The global electronic shutter control circuitry is capable of generating an exposure control timing pulse that is capable of causing the simultaneous exposure of substantially all of an entire frame of pixels of the CMOS based image sensor array. The CMOS based image reader also comprises light sources in electrical communication with the integrated circuit. The light sources are capable of illuminating the target including the symbology in response to an illumination control timing pulse. At least a portion of the illumination control timing pulse occurs during the exposure control timing pulse. In one embodiment of the CMOS based image reader, illuminating the target comprises overdriving light sources. In another embodiment of the CMOS based image reader, the light sources comprise light emitting diodes. In a further embodiment of the CMOS based image reader, the exposure period starts after the start of the illumination period and the exposure period ends before the end of the illumination period. In yet another embodiment of the CMOS based image reader, the illumination period starts after the start of the exposure period and the illumination period ends before the end of the exposure period. In yet an additional embodiment of the CMOS based image reader, the illumination period starts before the start of the exposure period and the illumination period ends before the end of the exposure period. In yet a further embodiment of the CMOS based image reader, the exposure period has a duration of less than 3.7 milliseconds. In various embodiments of the CMOS based image reader, the target includes a symbology such as a one-dimensional bar code such as a Code 39 or a UPC code or a two-dimensional bar code such as a PDF417 bar code, an Aztec symbol or Datamatrix symbol.</p>
  <p num="p-0012">In a further aspect, the invention features an image reader for collecting image data from a target. The imager reader comprises an integrated circuit including at least an image sensor array and exposure timing control circuitry. The exposure timing control circuitry is capable of generating an exposure control timing pulse that is capable of simultaneously exposing substantially all of the pixels in the image sensor array. The image reader also comprises an illumination module in electrical communication with the integrated circuit. The illumination module comprises light sources that are capable of illuminating the target in response to an illumination control timing pulse. At least a portion of the illumination control timing pulse occurs during the exposure control timing pulse. In one embodiment of the image reader, the illumination control timing pulse is generated by an illumination module. In another embodiment of the image reader, the overlap between the illumination control timing pulse and the exposure control timing pulse is coordinated by a control module that is in electrical communication with the integrated circuit and the illumination module. In a further embodiment of the image reader, the control module comprises a microprocessor. In one embodiment of the image reader, illuminating the target comprises overdriving light sources. In another embodiment of the image reader, the light sources comprise light emitting diodes. In a further embodiment of the image reader, the exposure period starts after the start of the illumination period and the exposure period ends before the end of the illumination period. In yet another embodiment of the image reader, the illumination period starts after the start of the exposure period and the illumination period ends before the end of the exposure period. In yet an additional embodiment of the image reader, the illumination period starts before the start of the exposure period and the illumination period ends before the end of the exposure period. In yet a further embodiment of the image reader, the exposure period has a duration of less than 3.7 milliseconds. In various embodiments of the CMOS based image reader, the target includes a symbology such as a one-dimensional bar code such as a Code 39 or a UPC code or a two-dimensional bar code such as a PDF417 bar code, an Aztec symbol or Datamatrix symbol.</p>
  <p num="p-0013">In another aspect, the invention features a method for collecting image data from a target. The method comprises activating light sources to illuminate the target in response to an illumination control timing pulse. The activation of the light sources occurs for the duration of the illumination control timing pulse. The method also comprises simultaneously activating a plurality of pixels to photoconvert incident radiation. The activation of the plurality of pixels occurs in response to an exposure control timing pulse. The method additionally comprises storing image data collected by each of the plurality of pixels in a shielded portion of each of the plurality of pixels. The storing of the image data occurs in response to the exposure control timing pulse. The method further comprises reading out image data from the plurality of pixels wherein at least a portion of the exposure control timing pulse occurs during the illumination control timing pulse. In one embodiment, the method further comprises coordinating the overlap between the illumination control timing pulse and the exposure control timing pulse. The coordination is directed by a control module. In one such embodiment of the method, the control module comprises a microprocessor. In another embodiment of the method, illuminating the target comprises overdriving light sources in an illumination module. In an additional embodiment of the method, the light sources comprise light emitting diodes. In a further embodiment of the method, the storing of image data occurs in response to a stop portion of the exposure control timing pulse. In an additional embodiment of the method, the exposure period starts after the start of the illumination period and the exposure period ends before the end of the illumination period. In yet another embodiment of the method, the illumination period starts after the start of the exposure period and the illumination period ends before the end of the exposure period. In yet an additional embodiment of the method, the illumination period starts before the start of the exposure period and the illumination period ends before the end of the exposure period. In yet a further embodiment of the method, the exposure period has a duration of less than 3.7 milliseconds. In various embodiments of the CMOS based image reader, the target includes a symbology such as a one-dimensional bar code such as a Code 39 or a UPC code or a two-dimensional bar code such as a PDF417 bar code, an Aztec symbol or Datamatrix symbol.</p>
  <p num="p-0014">In another aspect, the invention features a bar code image reader for collecting and processing bar code data from a bar code symbol. The image reader comprises a two-dimensional array of pixels for receiving light radiation reflected from the bar code symbol, the two-dimensional array of pixels comprising a first plurality of pixels and a second plurality of pixels, the two-dimensional array capable of reading out the first plurality of pixels independently of reading out the second plurality, each of the pixels comprising a photosensitive region and an opaque shielded data storage region. The image reader also comprising an optics assembly for directing light radiation reflected from the bar code symbol onto the two-dimensional array of pixels. The image reader further comprising a global electronic shutter associated with the two-dimensional array of pixels, the global electronic shutter capable of simultaneously exposing substantially all of the pixels in the two-dimensional array. The image reader additionally comprising a processor module, the processor module in electronic communication with the two-dimensional array of pixels, the processor module capable of processing image data from the two-dimensional array of pixels to generate decoded bar code data. In one embodiment of the bar code image reader, the two-dimensional image sensor array is a complementary metal oxide (CMOS) image sensor. In another embodiment of the bar code image reader, processing the image data to generate output data comprises automatically discriminating between a plurality of bar code types.</p>
  <p num="p-0015">In another aspect, the invention features a complementary metal oxide semiconductor (CMOS) based image reader for collecting image data from a target. The CMOS based imager reader comprises a CMOS based image sensor array, the CMOS based image sensor array comprising a first plurality of pixels and a second plurality of pixels, the CMOS based image sensor array capable of reading out the first plurality of pixels independently of reading out the second plurality, each of the pixels of the CMOS based image sensor array comprising a photosensitive region and an opaque shielded data storage region. The CMOS based image reader also comprising a timing module in electrical communication with the CMOS based image sensor array, the timing module configured to simultaneously expose an entire frame of pixels of the CMOS based image sensor array during an exposure period. The CMOS based image sensor array further comprising an illumination module configured to illuminate the target during an illumination period, the illumination module in electrical communication with the timing module. The CMOS based image sensor array additionally comprising a control module in electrical communication with the timing module and the illumination module, the control module configured to cause at least a portion of the exposure period to occur during the illumination period.</p>
  <p num="p-0016">In a further aspect, the invention features a complementary metal oxide semiconductor (CMOS) based image reader for collecting image data from a target. The CMOS based imager reader comprising an integrated circuit including at least a CMOS based image sensor array, the CMOS based image sensor array comprising a first plurality of pixels and a second plurality of pixels, the CMOS based image sensor array capable of reading out the first plurality of pixels independently of reading out the second plurality, each of the pixels of the CMOS based image sensor array comprising a photosensitive region and an opaque shielded data storage region. The CMOS based image sensor array also comprising a global electronic shutter control circuitry, the global electronic shutter control circuitry configured to generate an exposure control timing pulse that is capable of causing the simultaneous exposure of substantially all of an entire frame of pixels of the CMOS based image sensor array. The CMOS based image sensor array further comprising light sources configured to illuminate the target in response to an illumination control timing pulse, the light sources in electrical communication with the integrated circuit. In operation of the CMOS based image reader at least a portion of the illumination control timing pulse overlaps with at least a portion of the exposure control timing pulse. In one embodiment of the CMOS based image reader, illuminating the target comprises overdriving light sources in the illumination module. In another embodiment of the CMOS based reader the light sources comprise light emitting diodes. In a further embodiment of the CMOS based image reader, the exposure control timing pulse has a shorter duration than the illumination control timing pulse. In an additional embodiment of the CMOS based image reader, the illumination control timing pulse has a shorter duration than the exposure control timing pulse. In still another embodiment of the CMOS based imager reader, the illumination control timing pulse starts before the start of the exposure control timing pulse and the illumination control timing pulse ends before the end of the exposure control timing pulse. In still a further embodiment of the CMOS based imager reader, the exposure control timing pulse has a duration of less than 3.7 milliseconds. In still an additional embodiment of the CMOS based imager reader, the target includes a symbology. In one such embodiment, the symbology is a one-dimensional bar code. In another such embodiment, the symbology is a two-dimensional bar code. In one such embodiment, the two-dimensional bar code is a PDF417 bar code.</p>
  <p num="p-0017">In a further aspect, the invention features a bar code image reader for collecting image data from a bar code. The imager reader comprises an integrated circuit including at least a two-dimensional image sensor array, the two-dimensional image sensor array including a plurality of active pixels, each active pixel including at least a shielded data storage area, the two-dimensional image sensor array capable of employing a transfer function to convert an incident light intensity into an output voltage, the transfer function having a first region with a first slope and a second region with a second slope, the two-dimensional image sensor array capable of employing the second region of the transfer function when the incident light intensity is above a specified level and the two-dimensional image sensor array capable of employing the first region of the transfer function when the incident intensity is below a specified level. The bar code image reader also comprises an exposure timing control circuitry, the exposure timing control circuitry configured to generate an exposure control timing pulse that is capable of simultaneously exposing all or substantially all of the pixels in the image sensor array to photoconvert incident radiation. In one embodiment, the exposure control timing pulse has a duration of less than 3.7 milliseconds. In another embodiment, a dynamic range of the two-dimensional image array sensor is greater than 65 decibels.</p>
  <p num="p-0018">In yet another aspect, the invention features a method for automatically focusing an image reader. The method comprises directing with an optical system light energy reflected from a target onto an image sensor. The method also comprises exposing sequentially a plurality of rows of pixels in the image sensor during a frame exposure period, the frame exposure period being defined as a time duration extending from the beginning of the exposure of the first of the plurality of rows to the end of the exposure of the last of the plurality of rows. The method further comprising varying in incremental steps an optical system from a first setting where a distinct image of objects located at a first distance from the image reader is formed on the image sensor to a second setting where a distinct image of objects located at a second distance from the image reader is formed on the image sensor. The method additionally comprising reading out a plurality of rows of image data from the plurality of rows of pixels in the image sensor, wherein the varying in incremental steps the optical system occurs during at least a portion of the frame exposure period. In one embodiment, the method further comprises analyzing the plurality of rows of image data to determine a proper setting for the optical system corresponding to a distinct image of the target being formed on the image sensor. In an additional embodiment, the method also comprises simultaneously exposing the plurality of rows in the image sensor to generate an image of the target. In one embodiment of the method, the exposure period for adjacent lines of pixels in image reader overlap. In another embodiment of the method, the target includes a symbology. In one such embodiment, the symbology is a one-dimensional bar code. In another such embodiment, the symbology is a two-dimensional bar code.</p>
  <p num="p-0019">In another aspect, the invention features an image reader with an automatic focusing capability. The imager reader comprising an integrated circuit including at least an image sensor array. The image reader also comprising an optical system capable of directing light reflected from a target onto the image sensor array, the optical system having a plurality of focus settings, a first focus setting corresponding to distinct images of objects located at a first distance from the image reader being formed on the image sensor array and a second focus setting corresponding to distinct images of objects located at a second distance from the image reader being formed on the image sensor array. The image reader further comprising a rolling shutter control module configured to sequentially expose a plurality of rows of pixels in the image sensor array to collect focusing image data. The imager reader additionally comprising an automatic focusing module configured to analyze the focusing image data to determine a focus setting for the target corresponding to a distinct image of the target being formed on the image sensor, wherein the optical system is capable of being varied in incremental steps from the first focus setting to the second focus setting during at least a portion of a time period during which the rolling shutter control module is sequentially exposing the plurality of rows of pixels. In one embodiment, the imager reader further comprises a global electronic shutter control module configured to simultaneously expose the plurality of lines of pixels in the image sensor array to collect a frame of image data once the focus setting for the target has been determined. In another embodiment of the image reader, the rolling shutter control module and the global electronic shutter control module are integrated on the same integrated circuit containing the image sensor array. In a further embodiment of the image reader, the rolling shutter control module and the global electronic shutter control module are combined in a single image array control module. In an additional embodiment of the image reader, the rolling shutter control module is capable of causing exposure periods for adjacent rows of pixels to overlap.</p>
  <p num="p-0020">In another aspect, the invention features an image reader for minimizing ambient light image degradation. The image reader comprises an integrated circuit including at least an image sensor array, the image sensor array providing a signal suitable for light intensity determination. The image reader also comprises a rolling shutter control module configured to sequentially expose a plurality line of pixels in the image sensor array. The image reader further comprises a global electronic shutter control module configured to simultaneously expose the plurality of lines of pixels in the image sensor array, wherein one of the rolling shutter control module and the global electronic shutter control module is capable of being selected to control the image sensor array in response to the signal suitable for light intensity determination. In one embodiment of the image reader, the signal suitable for light intensity determination includes information related to an intensity of a light source of the image reader. In another embodiment of the image reader, the signal suitable for light intensity determination is useful for determining whether a minimum integration time is satisfied. In a further embodiment of the image reader, the signal suitable for light intensity determination is useful for determining whether the exposure time (also known as the integration time) for the current environmental condition is less than a calculated minimum integration time. In yet another embodiment of the image reader, the rolling shutter control module and the global electronic shutter control module are integrated on the same integrated circuit containing the image sensor array.</p>
  <p num="p-0021">In still another aspect, the invention features a method for minimizing image data degradation collected by an image reader. The method comprises determining at least one parameter related to an ambient light intensity and analyzing the at least one parameter. The method also comprises switching control of an image sensor array in the image reader from a global electronic shutter control module to a rolling shutter control module in response to the analysis of the at least one parameter. In one embodiment of the method, the at least one parameter includes an exposure time for current environmental conditions. In another embodiment of the method, the analyzing the at least one parameter includes calculating a ratio of the exposure time for current environmental conditions to a predetermined exposure time. In one such embodiment, the predetermined exposure time is based on illumination supplied by light sources of the image reader. In another embodiment of the method, analyzing the at least one parameter includes determining whether a ratio of the ambient light intensity to an intensity of a light source of the image reader exceeds a specified threshold.</p>
  <p num="p-0022">The foregoing and other objects, aspects, features, and advantages of the invention will become more apparent from the following description and from the claims.</p>
<description-of-drawings> <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p num="p-0023">The objects and features of the invention can be better understood with reference to the drawings described below, and the claims. The drawings are not necessarily to scale, emphasis instead generally being placed upon illustrating the principles of the invention. In the drawings, like numerals are used to indicate like parts throughout the various views.</p>
    <p num="p-0024"> <figref idrefs="DRAWINGS">FIG. 1A</figref> is a block diagram of one embodiment of an image reader constructed in accordance with the principles of the invention;</p>
    <p num="p-0025"> <figref idrefs="DRAWINGS">FIG. 1B</figref> is a schematic block diagram of an autodiscrimination module which may be utilized with the invention;</p>
    <p num="p-0026"> <figref idrefs="DRAWINGS">FIG. 1C</figref> is a process for practicing principles of the invention including automatically discriminating between different dataform types;</p>
    <p num="p-0027"> <figref idrefs="DRAWINGS">FIG. 2A</figref> illustrates the operation of an image sensor employing a rolling shutter architecture according to the prior art;</p>
    <p num="p-0028"> <figref idrefs="DRAWINGS">FIG. 2B</figref> is a timing diagram used in the prior art rolling shutter architecture presented with respect to <figref idrefs="DRAWINGS">FIG. 2A</figref>;</p>
    <p num="p-0029"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a representation of an image taken by a prior art image sensor;</p>
    <p num="p-0030"> <figref idrefs="DRAWINGS">FIG. 4A</figref> is a block electrical diagram corresponding to a specific embodiment of the invention;</p>
    <p num="p-0031"> <figref idrefs="DRAWINGS">FIG. 4B</figref> is a block electrical diagram corresponding to another specific embodiment of the invention;</p>
    <p num="p-0032"> <figref idrefs="DRAWINGS">FIG. 5A</figref> is a block diagram of one embodiment of an illumination module in an image reader constructed in accordance with the principles of the invention;</p>
    <p num="p-0033"> <figref idrefs="DRAWINGS">FIG. 5B</figref> is a block diagram of one embodiment of an image collection module in an image reader constructed in accordance with the principles of the invention;</p>
    <p num="p-0034"> <figref idrefs="DRAWINGS">FIG. 6</figref> is a perspective drawing of one embodiment of a hand held image reader constructed in accordance with the principles of the invention;</p>
    <p num="p-0035"> <figref idrefs="DRAWINGS">FIG. 7</figref> is a schematic block diagram of one embodiment of an image reader constructed in accordance with the principles of the invention;</p>
    <p num="p-0036"> <figref idrefs="DRAWINGS">FIG. 8A</figref> is a schematic diagram of a portion of one embodiment of an image sensor array from the prior art that can be employed in one embodiment of the image reader of <figref idrefs="DRAWINGS">FIG. 7</figref>;</p>
    <p num="p-0037"> <figref idrefs="DRAWINGS">FIGS. 8B and 8C</figref> are cross-sectional details of pixel architectures from the prior art that can be employed in one embodiment of the image reader of <figref idrefs="DRAWINGS">FIG. 7</figref>;</p>
    <p num="p-0038"> <figref idrefs="DRAWINGS">FIG. 9</figref> is a flow chart illustrating one embodiment of a process for collecting image data according to the principles of the invention;</p>
    <p num="p-0039"> <figref idrefs="DRAWINGS">FIGS. 10A</figref>, <b>10</b>B, <b>10</b>C, and <b>10</b>D are timing diagrams for various embodiments of the process of <figref idrefs="DRAWINGS">FIG. 9</figref>;</p>
    <p num="p-0040"> <figref idrefs="DRAWINGS">FIG. 10E</figref> illustrates an illumination control timing pulse including a plurality of individual pulses;</p>
    <p num="p-0041"> <figref idrefs="DRAWINGS">FIG. 11</figref> is a schematic diagram of a portion of an image sensor according to the prior art;</p>
    <p num="p-0042"> <figref idrefs="DRAWINGS">FIG. 12</figref> is a timing diagram for the prior art image sensor of <figref idrefs="DRAWINGS">FIG. 11</figref>;</p>
    <p num="p-0043"> <figref idrefs="DRAWINGS">FIG. 13</figref> is a flow chart illustrating one embodiment of a process for automatic focusing according to the principles of the invention;</p>
    <p num="p-0044"> <figref idrefs="DRAWINGS">FIG. 14</figref> is a flow chart illustrating one embodiment of a process for changing operational modes according to the principles of the invention;</p>
    <p num="p-0045"> <figref idrefs="DRAWINGS">FIGS. 15A</figref>, <b>15</b>B, and <b>15</b>C are various views of one embodiment of portable data terminal image reader constructed in accordance with the principles of the invention;</p>
    <p num="p-0046"> <figref idrefs="DRAWINGS">FIG. 16</figref> is an electrical block diagram of one embodiment of the portable data terminal image reader of <figref idrefs="DRAWINGS">FIGS. 15A</figref>, <b>15</b>B, and <b>15</b>C;</p>
    <p num="p-0047"> <figref idrefs="DRAWINGS">FIG. 17A</figref> shows one embodiment of a plurality of curvelent detector maps which may be utilized with the invention;</p>
    <p num="p-0048"> <figref idrefs="DRAWINGS">FIG. 17B</figref> shows another embodiment of a plurality of curvelent detector maps which may be utilized with the invention;</p>
    <p num="p-0049"> <figref idrefs="DRAWINGS">FIG. 18</figref> is a diagrammatic representation of a histogram analysis which may be performed in one embodiment of the invention;</p>
    <p num="p-0050"> <figref idrefs="DRAWINGS">FIGS. 19A-19D</figref> are diagrammatic representations of an image data segmentation process according to embodiments of the invention;</p>
    <p num="p-0051"> <figref idrefs="DRAWINGS">FIG. 20</figref> is a schematic block diagram of one embodiment of a lens driver constructed in accordance with the principles of the invention;</p>
    <p num="p-0052"> <figref idrefs="DRAWINGS">FIGS. 21</figref>, <b>22</b>A and <b>22</b>B are diagram illustrations of a focus level detection process according to an embodiment of the invention;</p>
    <p num="p-0053"> <figref idrefs="DRAWINGS">FIGS. 23</figref>, <b>24</b>, <b>25</b>, <b>26</b> and <b>27</b> are flow diagrams illustrating various focusing processes which may be practiced according to the invention;</p>
    <p num="p-0054"> <figref idrefs="DRAWINGS">FIGS. 28A</figref>, <b>28</b>B and <b>28</b>C are representations of image sensor pixel array, wherein shaded regions indicate groups of positionally contiguous pixels that may be selectively addressed and read out when the image sensor array is operated in a windowed frame operating mode;</p>
    <p num="p-0055"> <figref idrefs="DRAWINGS">FIGS. 29</figref>, <b>30</b>A and <b>30</b>B are diagrams illustrating a focus level detection process which may be utilized in an embodiment of the invention;</p>
    <p num="p-0056"> <figref idrefs="DRAWINGS">FIGS. 31 and 32</figref> are flow diagrams illustrating additional processes which may be practiced in accordance with the invention;</p>
    <p num="p-0057"> <figref idrefs="DRAWINGS">FIG. 33</figref> is an exploded assembly view of an imaging module according to the invention;</p>
    <p num="p-0058"> <figref idrefs="DRAWINGS">FIG. 34</figref> is a front view of the imaging module shown in <figref idrefs="DRAWINGS">FIG. 33</figref>;</p>
    <p num="p-0059"> <figref idrefs="DRAWINGS">FIG. 35</figref> is a side view of an assembled imaging module as shown in <figref idrefs="DRAWINGS">FIG. 33</figref>;</p>
    <p num="p-0060"> <figref idrefs="DRAWINGS">FIG. 36</figref> is a view of a substrate bearing a bar code symbol and having projected thereon an illumination pattern and an aiming pattern and having delineated thereon a full frame field of view of an image reader according to the invention that projects the illumination pattern and the aiming pattern; and</p>
    <p num="p-0061"> <figref idrefs="DRAWINGS">FIG. 37</figref> is a chart describing various embodiments of the invention having LEDs which emit light in different wavelength bands.</p>
  </description-of-drawings> <heading>DETAILED DESCRIPTION OF THE INVENTION</heading> <p num="p-0062">The invention features an image reader and a corresponding method for capturing a sharp non-distorted image of a target. In one embodiment, the image reader comprises a two-dimensional CMOS based image sensor array, a timing module, an illumination module, and a control module all in electrical communication with each other. The illumination module shines light on the target, such as a symbology such as one or two-dimensional bar code, so that reflected light that can be collected and processed by the image sensor array. The time during which the target is illuminated is referred to as the illumination period. The capture of the image by the image sensor array is driven by the timing module that, in one embodiment, is able to simultaneously expose all or substantially all of the pixels in the array. The simultaneous exposure of the pixels in the sensor array enables the image reader to capture a distortion free image. The time during which the pixels are collectively activated to photo-convert incident light into charge defines the exposure period for the sensor array. At the end of the exposure period, the collected charge is transferred to a shielded storage area until the data is read out. In one embodiment, the exposure period and the illumination period are under the control of the control module. In one such embodiment, the control module causes at least a portion of the exposure period to occur during the illumination period. By adequately shortening either the illumination period or the exposure period in an environment of low ambient lighting or the exposure period in an environment of high ambient lighting, the image reader of the present invention is able to capture an image substantially free of blurring.</p>
  <p num="p-0063">Referring to <figref idrefs="DRAWINGS">FIG. 1A</figref>, a block diagram of a general image reader <b>100</b> constructed in accordance with the invention is shown. The general image reader includes one or more of: an illumination module <b>104</b>, an image collection module <b>108</b>, a control module <b>112</b>, a memory module <b>116</b>, an I/O module <b>120</b>, an actuation module <b>124</b>, a user feedback module <b>128</b>, a display module <b>132</b>, a user interface module <b>134</b>, a radio frequency identification (RFID) module <b>136</b>, a smart card module <b>140</b>, magnetic stripe card module <b>144</b>, a decoder module <b>150</b>, an autodiscriminating module <b>152</b>, and/or one or more power modules <b>168</b> and a lens driver module <b>165</b>. In various embodiments each of the modules is in combination with one or more of the other modules. In one embodiment, the image reader <b>100</b> comprises a bar code image reader with a full frame electronic global shutter based image sensor that is capable of simultaneously exposing substantially all of the pixels in the image sensor. In one such embodiment, the image sensor is a CMOS based image sensor. In another such embodiment, the image sensor is a CCD based image sensor.</p>
  <p num="p-0064">Dataform decode module <b>150</b> (which may be a bar code symbol dataform decode module) when receiving image data transferred by control module <b>112</b> may search the image data for markers, such as a quiet zone, indicative of the presence of a dataform, such as a one or two-dimensional bar code. If a potential dataform is located, the dataform decode module <b>150</b> applies one or more dataform decoding algorithms to the image data. If the decode attempt is successful, the image reader outputs decoded dataform data through I/O module <b>120</b> and signals a successful read with an alert, such as a beep tone through user interface module <b>134</b>.</p>
  <p num="p-0065">Image reader <b>100</b> may also include an autodiscriminating module <b>152</b>. Referring to <figref idrefs="DRAWINGS">FIG. 1B</figref>, autodiscriminating module <b>152</b> may incorporate a dataform decode module <b>150</b> and an image processing and analysis module <b>1208</b>, that are in communication with one another.</p>
  <p num="p-0066">As shown in this embodiment, the image processing and analysis module <b>1208</b> comprises a feature extraction module <b>1212</b>, a generalized classifier module <b>1216</b>, a signature data processing module <b>1218</b>, an OCR decode module <b>1222</b>, and a graphics analysis module <b>1224</b> that are in communication with each other. In addition as shown in <figref idrefs="DRAWINGS">FIG. 1B</figref>, the feature extraction module <b>1212</b> comprises a binarizer module <b>1226</b>, a line thinning module <b>1228</b>, and a convolution module <b>1230</b> that are in communication with each other.</p>
  <p num="p-0067"> <figref idrefs="DRAWINGS">FIG. 1C</figref> shows a process <b>1300</b> for employing one embodiment of the invention utilizing the autodiscrimination module shown in <figref idrefs="DRAWINGS">FIG. 1B</figref>. The process <b>1300</b> comprises an image reader recording an actuation event (step <b>1302</b>), such as a trigger pull as sensed by actuation module <b>124</b>, and in response collecting (step <b>1304</b>) image data from a target with the image reader <b>100</b>. The collecting of image data step may be in accordance with e.g., process <b>300</b>, process <b>400</b>, (this process is used twice, see <figref idrefs="DRAWINGS">FIG. 13</figref> and <figref idrefs="DRAWINGS">FIGS. 23 and 24</figref>), process <b>600</b> or process <b>800</b>. After collection, the image data is transferred (step <b>1308</b>) to the dataform decode module <b>150</b>. The dataform decode module searches (step <b>1310</b>) the image data for markers, such as a quiet zone, indicative of the presence of a dataform, such as a one or two-dimensional bar code. If a potential dataform is located, the dataform decode module <b>150</b> applies (step <b>1314</b>) one or more dataform decoding algorithms to the ensuing image data. If the decode attempt is successful, the image reader <b>100</b> outputs (step <b>1318</b>) decoded dataform data and signals (step <b>1322</b>) a successful read with an alert, such as a beep tone.</p>
  <p num="p-0068">In one embodiment if the decode attempt is not successful, the image data is transferred (step <b>1326</b>) to the image processing and analysis module <b>1208</b>. In another embodiment, the image data is processed in parallel with the attempt to decode the dataform data. In one such embodiment, the process that completes first (i.e., dataform decode attempt or the image processing) outputs its data (e.g., a decoded bar code or a captured signature) and the other parallel process is terminated. In a further embodiment, the image data is processed in response to the decoding of the dataform. In one such embodiment, a bar code encodes item information such as shipping label number and information indicating that a signature should be captured.</p>
  <p num="p-0069">Within the image processing and analysis module <b>1208</b>, the image data is processed by the feature extraction module <b>1212</b>. In general, the feature extraction module generates numeric outputs that are indicative of the texture of the image data. As indicated above, the texture of the image data refers to the characteristics of the type of data contained in the image data. Common types of texture include one or two-dimensional bar code texture, signature texture, graphics texture, typed text texture, hand-written text texture, drawing or image texture, photograph texture, and the like. Within any category of textures, sub-categories of texture are sometime capable of being identified.</p>
  <p num="p-0070">As part of the processing of the image data by the feature extraction module <b>1212</b>, the image data is processed (step <b>1328</b>) by the binarizer module <b>1226</b>. The binarizer module <b>1226</b> binarizes the grey level image into a binary image according to the local thresholding and target image size normalization. With the image data binarized, the image data is processed (step <b>1332</b>) by the line thinning module <b>1228</b> to reduce multi-pixel thick line segments into single pixel thick lines. With binarized line thinned image data, the image data is processed (step <b>1336</b>) by the convolution module <b>1230</b>.</p>
  <p num="p-0071">In general, the convolution module <b>1230</b> convolves the processed image data with one or more detector maps designed according to the invention to identify various textural features in the image data. In one embodiment, the convolution module <b>1230</b> generates a pair of numbers, the mean and variance (or standard deviation), for each convolved detector map. <figref idrefs="DRAWINGS">FIG. 17A</figref> shows a set of 12 2×3 binary curvelet detector maps <b>1250</b> used to detect curved elements present in image data. As each of the curvelet detector maps <b>1250</b> is convolved with the image data, the mean value and the variance generated provide an indication of the presence or density of elements in the binarized line thinned image data having similar shapes to the curvelet detector maps <b>1250</b>. As each pixel map generates a pair of numbers, the 12 curvelet detector maps <b>1250</b> generate a total of 24 numbers. According to one embodiment, these 24 numbers are representative of the curved or signature texture of the processed image data.</p>
  <p num="p-0072">Further processing of the image data includes the outputs from the feature extraction module <b>1212</b> being fed (step <b>1340</b>) into the generalized classified module <b>1216</b>. The generalized classifier module <b>1216</b> uses the numbers generated by the feature extraction module as inputs to a neural network, a mean square error classifier or the like. These tools are used to classify the image data into general categories. In embodiments employing neural networks, different neural network configurations are contemplated in accordance with the invention to achieve different operational optimizations and characteristics. In one embodiment employing a neural network, the generalized classifier module <b>1212</b> includes a 24+12+6+1=43 nodes Feedforward, Back Propagation Multilayer neural network. The input layer has 24 nodes for the 12 pairs of mean and variance outputs generated by a convolution module <b>1230</b> employing the 12 curvelet detector maps <b>1250</b>. In the neural network of this embodiment, there are two hidden layers of 12 nodes and 6 nodes respectively. There is also one output node to report the positive or negative existence of a signature.</p>
  <p num="p-0073">In another embodiment employing a neural network, the 20 curvelet detector maps <b>1260</b> shown in <figref idrefs="DRAWINGS">FIG. 17B</figref> are used by the convolution module <b>1230</b>. As shown, the 20 curvelet detector maps <b>1260</b> include the original 12 curvelet detector maps <b>1250</b> of <figref idrefs="DRAWINGS">FIG. 17A</figref>. The additional 8 pixel maps <b>1260</b> are used to provide orientation information regarding the signature. In one embodiment employing the 20 curvelet detector maps <b>1260</b>, the generalized classifier module <b>216</b> is a 40+40+20+9=109 nodes Feedforward, Back Propagation Multiplayer neural network. The input layer has 40 nodes for the 20 pairs of mean and variance outputs generated by a convolution module <b>1230</b> employing the 20 curvelet detector maps <b>1260</b>. In the neural network of this embodiment, there are two hidden layers of 40 nodes and 20 nodes respectively, one output node to report the positive or negative existence of a signature, and 8 output nodes to report the degree of orientation of the signature. The eight output nodes provide 2<sup>8</sup>=256 possible orientation states. Therefore, the orientation angle is given in degrees between 0 and 360 in increments of 1.4 degrees.</p>
  <p num="p-0074">In some embodiments, the generalized classifier module <b>1216</b> is capable of classifying data into an expanded collection of categories. For example in some embodiments, the generalized classifier module <b>1216</b> specifies whether the image data contains various data types such as a signature; a dataform; handwritten text; typed text; machine readable text; OCR data; graphics; pictures; images; forms such as shipping manifest, bill of lading, ID cards, and the like; fingerprints, biometrics such as fingerprints, facial images, retinal scans and the like, and/or other types of identifiers. In further additional embodiments, the generalized classifier module <b>1216</b> specifies whether the image data includes various combinations of these data types. In some embodiments, the general classifier module <b>1216</b> specifies whether the image data contains a specified type of data or not. In one such embodiment, the image processing and analysis module <b>1208</b> is contained within an identification module that outputs an affirmative or negative response depending on the presence or absence of the specified data type, such as a signature or a biometric, in the image data.</p>
  <p num="p-0075">In one embodiment once the presence of a signature has been confirmed and its general orientation determined, image data is transferred (step <b>1344</b>) to the signature data processing module <b>1218</b>. In one embodiment, the signature data processing module <b>1218</b> is used to detect the boundaries of the signature in the image data. In one embodiment, the signature boundary is detected using a histogram analysis. As shown in <figref idrefs="DRAWINGS">FIG. 18</figref>, a histogram analysis consists of a series of one-dimensional slices along horizontal and vertical directions defined relative to the orientation of the signature. In one embodiment, the value for each one-dimensional slice corresponds to the number of black (i.e., zero valued) pixels along that pixel slice. In some embodiments if no bar codes have been decoded, then some specified region of the full frame of image data, such as a central region is captured for signature analysis. Once completed, the histogram analysis provides a two-dimensional plot of the density of data element pixels in the image data. The boundary of the signature is determined with respect to a minimum density that must be achieved for a certain number of sequential slices. In one embodiment, the histogram analysis searches inwardly along both horizontal and vertical directions until the pixel density rises above a predefined cutoff threshold. So that the signature data is not inadvertently cropped, it is common to use low cutoff threshold values.</p>
  <p num="p-0076">In one embodiment, once the boundaries of the signature have been determined, the signature data processing module <b>1218</b> crops the image data and extracts the signature image data. In one such embodiment, the cropping is performed by an image modification module that generates modified image data in which a portion of the image data not including the signature has been deleted. In other embodiments, various compression techniques are employed to reduce the memory requirements for the signature image data. One such technique includes the encoding of the signature image data by run length encoding. According to this technique, the length of each run of similar binarized values (i.e., the length of each run of 1 or 0) for each scan line is recorded as a means of reconstructing a bit map. Another encoding technique treats the signature image data as a data structure where the elements of the data structure consist of vectors. According this encoding technique, the signature is broken down into a collection of vectors. The position of each vector in combination with the length and orientation of each vector is used to reconstruct the original signature. In one such embodiment, the encoding process generates a new vector whenever the curvature for a continuous pixel run exceeds a specified value. A further compression technique employs B-Spline curve fitting. This technique has the capacity to robustly accommodate curvature and scaling issues.</p>
  <p num="p-0077">In various embodiments, the signature image data or a compressed or encoded version of the signature image data is stored locally on a dedicated memory device. In one such embodiment, the local memory device can be a detachable memory device such as a CompactFlash memory card or the like described in more detail below. In another embodiment, the signature image data is stored in a volatile or non-volatile portion of general purpose memory and downloaded at a future time. In a further embodiment, the signature image data can be transmitted via wired or wireless means either at the time of capture or at a later point, such as when a data collection session has been completed.</p>
  <p num="p-0078">In another embodiment, the signature data processing module <b>218</b> does not perform a histogram analysis but simply stores in memory the entire image or a compressed version once the presence of a signature has been determined. In a further embodiment to save processing time, the initial image analysis is performed on a lower resolution image. Once the presence of a signature is determined in this embodiment, a higher resolution image is taken. In one embodiment, a signature extraction histogram analysis is performed on this image. Next, the image is stored in memory in either compressed or original format. In some embodiments, the image data is combined with other data to form a record for a particular item such as a package or shipping envelope. As mentioned above, some of the additional data that can be collected by the image reader <b>100</b> and stored with or separate from the signature data includes but is not limited to dataform data, handwritten text data, typed text data, graphics data, image or picture data, and the like.</p>
  <p num="p-0079">As part of its operations, the image processing and analysis module <b>1208</b> can be designed to perform specialized tasks for different data types. For example, if the generalized classifier module <b>1216</b> determines that the image data contains typed or machine readable text, the image data can be collected, possibly histogram analyzed, and stored or alternatively the image data can be transferred to the OCR decoding module <b>1222</b>. Similarly, if the generalized classifier module <b>1216</b> determines that the image data includes a graphic element, the image data can be transferred to the graphics analysis module <b>1224</b> for processing. In one embodiment, the graphics analysis module <b>1224</b> is configured to recognize and decode predefined graphics. In one such embodiment, the graphics analysis can include determining which, if any, boxes have been selected in the billing and shipping instructions on a shipping label. In a further embodiment, the graphics analysis can include locating and decoding the typed or handwritten text contained in the zip code box on a shipping label. In an alternative embodiment, the image reader <b>100</b> can be configured to automatically attempt decode operations in addition to the dataform decode, such as OCR decoding or graphics decoding, prior to the activation of the feature extraction module <b>1212</b>.</p>
  <p num="p-0080">In another embodiment, the image processing and analysis module <b>1208</b> segments the image data into regions and performs a feature extraction and general classification analysis on each region. In one embodiment as shown in <figref idrefs="DRAWINGS">FIG. 19A</figref>, the standard rectangular image data window is divided into four equal sized sub-rectangles. In another embodiment shown in <figref idrefs="DRAWINGS">FIG. 19B</figref>, the segmentation consists of overlapping regions so that the total area of the segmented regions is larger than that of the complete field of the image data. In <figref idrefs="DRAWINGS">FIG. 8B</figref> there are seven shown overlapping regions where each identifying numeral is shown in the center of its region. In a further embodiment shown in <figref idrefs="DRAWINGS">FIGS. 19C and 19D</figref>, the segmentation consists of sample regions (shown as cross-hatched) within the complete field of the image data. In another embodiment, the sampled regions can be based on a preloaded user template that, for example, identifies regions of interest such as a signature region and/or a bar code region, in for example, a shipping label.</p>
  <p num="p-0081">In one embodiment, the segmentation process is used to identify the location of a signature in image data the might include additional elements such as dataforms including bar code dataforms, text, graphics, images and the like. In one such embodiment the generalized classifier module <b>1216</b> classifies the contents of each region of the segmented image data. The region containing the signature is then extracted by the signature data processing module <b>1218</b>. In one embodiment if multiple regions are indicated as containing signature data, the signature data processing module <b>1218</b> analyzes the arrangement of these regions to identify the region most likely to contain the image data. In a further embodiment when multiple regions are indicated as containing signature data, the image processing and analysis module establishes a feedback loop where additional segmented regions are generated and analyzed until a single segmented region containing signature data is located.</p>
  <p num="p-0082">Additional image processing operations which may be carried out by image reader <b>100</b> are described in U.S. patent application Ser. No. 10/958,779 (now U.S. Pat. No. 7,293,712), filed Oct. 5, 2004 entitled, “System And Method To Automatically Discriminate Between A Signature And A Bar code” and incorporated herein by reference in its entirety.</p>
  <p num="p-0083">Referring to additional components of image reader <b>100</b> indicated in <figref idrefs="DRAWINGS">FIG. 1A</figref> and <figref idrefs="DRAWINGS">FIG. 5A</figref>, illumination module <b>104</b> can include light sources <b>160</b>, an illumination control module <b>164</b>, an illumination power module <b>168</b> <i>a</i>, and an interface module <b>172</b>. In various embodiments, the light sources <b>160</b> can include white or colored LEDs, such as 660 nm illumination LEDs, infrared LED, ultra-violet LED, lasers, halogen lights, arc lamps, or incandescent lights, capable of producing adequate intensity light given image reader power constraints and image sensor exposure/sensitivity requirements. In many embodiments, LEDs are chosen for the light source as their efficient operation enables relatively low power consumption. The illumination control module <b>164</b> controls the operation of illumination module <b>104</b> and can include timing and light source activation and deactivation circuitry. The illumination power module <b>168</b> <i>a </i>supplies the energy necessary to drive the light sources <b>160</b> and can include batteries, capacitors, inductors, transformers, semiconductors, integrated circuits and the like. In an alternative embodiment, some or all of the elements of the illumination power module <b>168</b> <i>a </i>are located external to the illumination module. An image reader <b>100</b> with a single common power source is one such embodiment. The interface module <b>172</b> is used for communication with the other modules of the image reader <b>100</b> such as those required to synchronize operations. This can include, for example, the coordination of the illumination and exposure periods discussed above.</p>
  <p num="p-0084">Referring to the physical form views of <figref idrefs="DRAWINGS">FIGS. 33-36</figref>, various components of illumination module <b>104</b> and image collection module <b>108</b> according to one embodiment of the invention are shown and described. An image reader <b>100</b> of the invention, as shown in the embodiment of <figref idrefs="DRAWINGS">FIGS. 15A-15C</figref>, may include an imaging module such as imaging module <b>1802</b>. Imaging module <b>1802</b> as shown in <figref idrefs="DRAWINGS">FIGS. 33-35</figref> incorporates certain features of an IT4000 imaging module as referenced herein and additional features. Imaging module <b>1802</b> includes first circuit board <b>1804</b> carrying light sources <b>160</b> <i>a</i>, <b>160</b> <i>b</i>, while second circuit board <b>1806</b> carries light sources <b>160</b> <i>c</i>, <b>160</b> <i>d</i>, <b>160</b> <i>e</i>, <b>160</b> <i>f</i>, <b>160</b> <i>g</i>, <b>160</b> <i>h</i>, <b>160</b> <i>i</i>, <b>160</b> <i>j</i>, <b>160</b> <i>k</i>, <b>160</b> <i>l</i>, <b>160</b> <i>m</i>, <b>160</b> <i>n</i>, <b>160</b> <i>o</i>, <b>160</b> <i>p</i>, <b>160</b> <i>q</i>, <b>160</b> <i>r</i>, <b>160</b> <i>s</i>, and <b>160</b> <i>t </i>(hereinafter <b>160</b> <i>c </i>through <b>160</b> <i>t</i>). First circuit board <b>1804</b> also carries image sensor array <b>182</b>. Imaging module <b>1802</b> also includes support assembly <b>1810</b> including lens holder <b>1812</b>, which holds lens barrel <b>1814</b> that carries imaging lens <b>212</b>. Light sources <b>160</b> <i>a</i>, <b>160</b> <i>b </i>are aiming illumination light sources whereas light sources <b>160</b> <i>c </i>through <b>160</b> <i>t </i>are illumination light sources. Referring to <figref idrefs="DRAWINGS">FIG. 36</figref>, illumination light sources <b>160</b> <i>c </i>through <b>160</b> <i>t </i>project a two-dimensional illumination pattern <b>1830</b> over a substrate, s, that carries a decodable indicia such as a bar code symbol <b>1835</b> whereas aiming illumination light sources <b>160</b> <i>a</i>, <b>160</b> <i>b </i>project an aiming pattern <b>1838</b>. In the embodiments shown and described in connection with <figref idrefs="DRAWINGS">FIGS. 33-36</figref>, light from aiming illumination light sources <b>160</b> <i>a</i>, <b>160</b> <i>b </i>is shaped by slit apertures <b>1840</b> in combination with lenses <b>1842</b> which image slits <b>1840</b> onto substrate, s, to form aiming pattern <b>1838</b> which in the embodiment of <figref idrefs="DRAWINGS">FIGS. 33-36</figref> is a line pattern <b>1838</b>. Illumination pattern <b>1830</b> substantially corresponds to a full frame field of view of image reader <b>100</b> designated by box <b>1850</b>. Aiming pattern <b>1838</b> is in the form of a line that extends horizontally across a center of field of view of image reader <b>100</b>. Illumination pattern <b>1830</b> may be projected when all of illumination light sources <b>160</b> <i>c </i>through <b>160</b> <i>t </i>are operated simultaneously. Illumination pattern <b>1830</b> may also be projected when a subset of light sources <b>160</b> <i>c </i>through <b>160</b> <i>t </i>are simultaneously energized. Illumination pattern <b>1830</b> may also be projected when only one of light sources <b>160</b> <i>c </i>through <b>160</b> <i>t </i>is energized such as LED <b>160</b> <i>s </i>or LED <b>160</b>t. LEDs <b>160</b> <i>s </i>and <b>160</b> <i>t </i>of imaging module <b>1802</b> have a wider projection angle than LEDs <b>160</b> <i>c </i>through <b>160</b> <i>t. </i> </p>
  <p num="p-0085">As shown in <figref idrefs="DRAWINGS">FIG. 5B</figref>, the image collection module <b>108</b> in one embodiment includes an optics module <b>178</b>, a sensor array module <b>182</b>, and a sensor array control module <b>186</b> all in electrical communication with each other. The optics module <b>178</b> includes an imaging lens or other optical elements used to direct and focus reflected radiation. In some embodiments, the optics module <b>178</b> includes associated circuitry and processing capacity that can be used, for example, as part of automatically determining the proper focus for an object, being imaged.</p>
  <p num="p-0086">The sensor array control module <b>186</b> includes a global electronic shutter control module <b>190</b>, a row and column address and decode module <b>194</b>, and a read out module <b>198</b>, each of which modules is in electrical communication with one or more of the other modules in the sensor array control module <b>186</b>. In one embodiment, the sensor array module <b>182</b> includes components of an integrated circuit chip <b>1082</b> as shown in <figref idrefs="DRAWINGS">FIG. 4A</figref> with a two-dimensional CMOS based image sensor array <b>182</b>. In various embodiments, associated circuitry such as analog-to-digital converters and the like can be discrete from the image sensor array or integrated on the same chip as the image sensor array. In an alternative embodiment, the sensor array module <b>182</b> can include a CCD sensor array capable of simultaneous exposure and storage of a full frame of image data. As indicated above in one embodiment, the global electronic shutter control module <b>190</b> is capable of globally and simultaneously exposing all or substantially all of the pixels in the image sensor array. In one embodiment, the global electronic shutter control module <b>190</b> includes a timing module. The row and column address and decode module <b>194</b> is used to select particular pixels for various operations such as collection activation, electronic shutter data storage and data read out. The read out module <b>198</b> organizes and processes the reading out of data from the sensor array. In some embodiments, the sensor array control module <b>186</b> further includes a rolling shutter control module <b>202</b> that is capable of sequentially exposing and reading out the lines of pixels in the image sensor array.</p>
  <p num="p-0087">A specific embodiment of image reader <b>100</b> is described with reference to <figref idrefs="DRAWINGS">FIG. 4A</figref>. In the embodiment of <figref idrefs="DRAWINGS">FIG. 4A</figref> and image sensor array <b>182</b>, <b>182</b> <i>a </i>having a two-dimensional array of pixels <b>250</b> is incorporated onto CMOS integrated circuit (IC) chip <b>1082</b>, <b>1082</b> <i>a</i>. As is described later with reference to <figref idrefs="DRAWINGS">FIG. 8A</figref>, image sensor array <b>182</b> <i>a </i>is a CMOS image sensor array adapted to operate in a global shutter operating mode. Each pixel <b>250</b> of CMOS image sensor array <b>182</b> <i>a </i>has an on-chip pixel amplifier <b>254</b> (shown in <figref idrefs="DRAWINGS">FIG. 8A</figref>) and an on-chip optically shielded storage area <b>286</b> (shown in <figref idrefs="DRAWINGS">FIG. 8B</figref> and <figref idrefs="DRAWINGS">FIG. 8C</figref>). Image sensor array <b>182</b> <i>a </i>may also have a two-dimensional grid of electrical interconnects <b>262</b> as shown in <figref idrefs="DRAWINGS">FIG. 8A</figref> that are in electrical communication with pixels <b>250</b>. Image sensor array <b>182</b> <i>a </i>may also have an on-chip row circuitry <b>296</b> and column circuitry <b>270</b>. Row circuitry <b>296</b> and the column circuitry <b>270</b> may enable one or more various processing and operational tasks such as addressing pixels, decoding signals, amplification of signals, analog-to-digital signal conversion, applying timing, read out and reset signals and the like. Referring to further aspects of CMOS image sensor IC chip <b>182</b> <i>a</i>, CMOS image sensor IC chip <b>182</b> <i>a </i>includes, on the same chip as pixels <b>250</b> row circuitry <b>296</b>, column circuitry <b>270</b>, processing and control circuitry <b>254</b> including pixel amplifiers <b>255</b>, optically shields storage area <b>258</b>, interconnects <b>262</b>, a gain circuit <b>1084</b>, an analog-to-digital conversion circuit <b>1086</b> and line driver circuit <b>1090</b>, which generates a multi-bit (e.g., 8 bit 10 bit) signal indicative of light incident on each pixel <b>250</b> of array, the output being presented on a set of output pins of chip <b>1082</b> <i>a</i>. Referring to additional on-chip elements of image sensor IC chip <b>1082</b> <i>a</i>, CMOS image sensor IC chip <b>1082</b> <i>a </i>includes timing/control circuit <b>1092</b> which may include such components as a bias circuit, a clock/timing generation circuit, and an oscillator. Timing/control circuit <b>1092</b> may form part of sensor array control module <b>108</b> as described in connection with <figref idrefs="DRAWINGS">FIG. 5B</figref>.</p>
  <p num="p-0088">Referring to further aspects of image reader <b>100</b> of <figref idrefs="DRAWINGS">FIG. 4A</figref>, image reader <b>100</b> includes a main processor IC chip <b>548</b>, memory module <b>116</b>, illumination module <b>104</b>, and actuation module <b>124</b>. Main processor IC chip <b>548</b> may be a multifunctional IC chip having an integrated frame grabber circuit <b>549</b> and central processing unit (CPU) <b>552</b>. Processor IC chip <b>548</b> with an integrated frame grabber may be an e.g., an XSCALE PXA27X processor IC chip with “Quick Capture Camera Interface” available from INTEL. Image reader <b>100</b> further includes actuation module <b>124</b> which generates a trigger signal that initiates a bar code decode process. Actuation module <b>124</b> may include a manually actuated trigger <b>216</b>. Image reader <b>100</b> further includes imaging lens <b>212</b> and memory module <b>116</b> including such memory devices as a RAM, EPROM, flash memory. Memory module <b>116</b> is in communication with processor IC chip <b>548</b> via a system bus <b>584</b>. Processor IC chip <b>548</b> may be programmed or otherwise configured to carry out various functions required of modules <b>104</b>, <b>108</b>, <b>112</b>, <b>120</b>, <b>124</b>, <b>128</b>, <b>132</b>, <b>134</b>, <b>136</b>, <b>140</b>, <b>144</b>, <b>150</b>, <b>152</b>, <b>168</b>, <b>165</b> described with reference to <figref idrefs="DRAWINGS">FIG. 1A</figref>. In the embodiment of <figref idrefs="DRAWINGS">FIG. 4A</figref>, the functions of dataform decode module <b>150</b> and autodiscrimination module <b>152</b> are carried by processor IC chip <b>548</b> operating in accordance with specific software stored in memory module <b>116</b>. The combination of processor IC chip <b>548</b> and memory module <b>116</b> is, therefore, labeled <b>150</b>, <b>152</b> in the embodiment of <figref idrefs="DRAWINGS">FIG. 4A</figref>.</p>
  <p num="p-0089">Referring to <figref idrefs="DRAWINGS">FIG. 4B</figref>, an embodiment of image reader <b>100</b> is shown which has a CCD image sensor chip <b>1082</b>, <b>1082</b> <i>b</i>. CCD image sensor IC chip <b>1082</b> <i>b</i>. CCD image sensor IC chip <b>1082</b> <i>b </i>includes an area array of pixels <b>250</b>, a register <b>1094</b> and an output amplifier <b>1096</b> incorporated onto chip <b>1082</b> <i>b</i>. Output register <b>1094</b> and associated circuitry sequentially converts the charge associated with each pixel into a voltage and sends pixel image signals to a component external to chip <b>1082</b> <i>b</i>. When actuated to read out image data, charges on a first row of pixels <b>250</b> are sequentially transferred to output register <b>1094</b>. Output register <b>1094</b> sequentially feeds charges to amplifier <b>1096</b> which converts pixel charges into voltages and supplies signals to image processing circuitry <b>1070</b>. When charges are transferred from a first row of pixels to output register <b>1094</b>, charges from a next row move down one row so that when a first row of charges has been converted into voltages, output register <b>1094</b> receives charges from a second row of pixels. The process continues until image data corresponding to pixels from all of the rows of image sensor array <b>182</b> <i>b </i>are read out. Image reader <b>100</b> further includes image signal processing circuit <b>1070</b> external to chip <b>1082</b> <i>b</i>. Image signal processing circuit <b>1070</b> includes such elements as a gain circuit <b>1072</b> an analog-to-digital converter <b>1074</b> and a line driver <b>1076</b>. Timing and control circuit <b>1078</b> of circuit <b>1070</b> may include such elements as a bias generator, an oscillator, a clock and timing generator. The gain circuit <b>1072</b> may also implement additional functionality such as correlated double sampling to reduce to effects of pixel offsets and noise. Additional components of image reader <b>100</b> are as shown in <figref idrefs="DRAWINGS">FIG. 4A</figref>. Image signal processing circuit <b>1070</b> may be included in an integrated circuit chip (IC chip) external to image sensor IC chip <b>1082</b> <i>b. </i> </p>
  <p num="p-0090">In one embodiment, components of image collection module <b>108</b> and illumination module <b>104</b> are provided by any one of the IMAGETEAM™ area (2D) imaging engines, such as the 4000 OEM 2D Image Engine available from Hand Held Products, Inc. of 700 Visions Drive, P.O. Box 208, Skaneateles Falls, N.Y., constructed in accordance with the principles of the invention.</p>
  <p num="p-0091">Referring to <figref idrefs="DRAWINGS">FIG. 6</figref>, a perspective drawing of a hand held image reader <b>100</b> <i>a </i>constructed in accordance with one embodiment of the invention is shown. The hand held image reader <b>100</b> <i>a </i>includes a housing <b>208</b>, a plurality of light sources <b>160</b>, a lens <b>212</b>, a trigger <b>216</b>, and an interface cable <b>200</b>. In various embodiments, the functionality of the image reader <b>100</b> <i>a </i>can be provided by any one of the area (2D) IMAGETEAM™ image readers such as the models 4410, 4600, or 4800 available from Hand Held Products, Inc. and constructed in accordance with the invention. All of the modules <b>104</b>, <b>108</b>, <b>112</b>, <b>116</b>, <b>120</b>, <b>124</b>, <b>128</b>, <b>132</b>, <b>134</b>, <b>136</b>, <b>140</b>, <b>144</b>, <b>150</b>, <b>152</b>, <b>165</b>, and <b>168</b> described in connection with <figref idrefs="DRAWINGS">FIG. 1A</figref> may be incorporated into, and may be supported by hand held housing <b>208</b> or alternative housing <b>506</b> shown in <figref idrefs="DRAWINGS">FIG. 15A</figref> such that housing <b>208</b> or housing <b>506</b> encapsulate and support the various modules. Likewise, all of the components shown in <figref idrefs="DRAWINGS">FIGS. 4A and 4B</figref> and <figref idrefs="DRAWINGS">FIG. 16</figref> may be incorporated into and may be supported by housing <b>208</b> or housing <b>506</b> such that housing <b>208</b> or housing <b>506</b> encapsulate and support the various components. Lens <b>212</b> may comprise glass and/or polycarbonate. Lens <b>212</b> may be a lens singlet or else comprise a plurality of lens components; that is, lens <b>212</b> may be a lens doublet or lens triplet, etc.</p>
  <p num="p-0092">Referring to <figref idrefs="DRAWINGS">FIG. 7</figref>, a diagrammatic cross sectional view in combination with a schematic block diagram for the image reader <b>100</b> is shown. The image reader <b>100</b> includes the light sources <b>160</b>, an illumination control module <b>164</b>, a power module <b>168</b> <i>b</i>, and an interface module <b>172</b> all in electrical communication with each other. The light sources <b>160</b> direct light energy <b>162</b> towards a target <b>166</b> including a symbology <b>170</b>. Reflected radiation <b>174</b> from the target <b>166</b> is focused by a lens <b>212</b> onto an image sensor array <b>182</b> in electrical communication with a sensor array control module <b>186</b> and the power module <b>168</b> <i>b</i>. In one embodiment, the image sensor array <b>182</b> is a CMOS based image sensor array. In another embodiment, the image sensor array <b>182</b> is a CCD based image sensor array. The sensor array control module <b>186</b> is further in electrical communication with a memory module <b>116</b> and a control module <b>112</b> also in electrical communication with the power module <b>168</b> <i>b </i>and the interface module <b>172</b>. Often an optical window (not shown) is placed on the front of the scanner to reduce the likelihood of damage to the unit.</p>
  <p num="p-0093">Referring to <figref idrefs="DRAWINGS">FIG. 8A</figref>, a diagram of a portion of a CMOS based image sensor array <b>182</b> <i>a </i>is shown in more detail. The image sensor array <b>182</b> <i>a </i>includes a two-dimensional array of pixels <b>250</b>. Each pixel includes a photosensitive sensitive region <b>252</b>, processing and control circuitry <b>254</b> including amplifier <b>255</b> and a shielded storage area <b>258</b> (for clarity of presentation, the reference numerals <b>252</b>, <b>254</b>, <b>255</b>, and <b>258</b> are provided only with respect to a single pixel). The presence of amplifier <b>255</b> means that the CMOS image array <b>182</b> <i>a </i>is considered an active pixel array; that is, each pixel of the CMOS image array <b>182</b> <i>a </i>is able to amplify the signal generated from the photo-conversion of incident light energy. The charge-to-voltage conversion circuitry allows the CMOS image array <b>182</b> <i>a </i>to convert the collected charge into an output signal. The shielded storage area <b>258</b> stores collected pixel values until read out so that additional incident radiation impinging on the CMOS image array <b>182</b> <i>a </i>does not corrupt the value read during the defined exposure period. In addition to pixel amplifier <b>255</b>, the processing and control circuitry <b>254</b> for each pixel <b>250</b> may include, among other elements, a reset and select transistor.</p>
  <p num="p-0094">In one embodiment, the dynamic range of the CMOS based image sensor array <b>182</b> <i>a </i>is extended by providing additional intelligence in the processing and control circuitry <b>254</b>. In particular, the processing circuitry is augmented to include the capacity to dynamically change the conversion factor between the incident radiation input intensity and the output voltage. That is, the processing circuitry employs a transfer curve with multiple slopes. The particular form of the transfer curve with its multiple slopes can take various forms including a series of linear relations joined at knee points, a linear section at low intensity connected to a logarithmic transfer curve at higher intensity, or a completely continuous curve of arbitrary shape with steeper slopes for low intensity and higher slopes at greater intensities.</p>
  <p num="p-0095">In the multiple slope embodiment, the dynamic range of the CMOS based image sensor <b>182</b> <i>a </i>is significantly extended as each individual pixel is capable of independently employing a different section of the transfer curve depending on the intensity of radiation incident upon it. In operation, regions of the CMOS based image sensor <b>182</b> <i>a </i>that are receiving less incident radiation employ a steeper conversion slope corresponding to greater sensitivity and regions that are receiving more incident radiation employ a shallower conversion slope corresponding to less sensitivity. With a multiple slope transfer function, the CMOS based image sensor <b>182</b> <i>a </i>can achieve a dynamic range of 65 to 120 dB. The operation of image sensors with transfer curves with multiple slopes are described in more detail in the technical document entitled “Dual Slope Dynamic Range Expansion” from FillFactory NV, Schaliënhoevedreef 20B, B-2800 Mechelen, Belgium. This document is available from the Fill Factory (www.fillfactory.com), for example at http://www.fillfactory.com/htm/technology/htm/dual-slope.htm and is hereby herein incorporated in its entirety. The operation of image sensors with transfer curves with logarithmic slopes are described in more detail in the technical document entitled “LinLog Technology” from Photonfocus AG, Bahnhofplatz 10, CH-8853 Lachen, Switzerland. This document is available from the Photonfocus (www.photonfocus.com), for example at http://www.photonfocus.com/html/eng/cmos/linlog.php and is hereby herein incorporated in its entirety.</p>
  <p num="p-0096">Overlaying the pixels <b>250</b> in <figref idrefs="DRAWINGS">FIG. 8A</figref> is a two-dimensional grid of electrical interconnects <b>262</b> that are in electrical communication with the pixels <b>250</b>, the row circuitry <b>296</b> (see also <figref idrefs="DRAWINGS">FIG. 4A</figref>) and the column circuitry <b>270</b>. The row circuitry <b>296</b> and the column circuitry <b>270</b> enable one or more processing and operational tasks such as addressing pixels, decoding signals, amplification of signals, analog-to-digital signal conversion, applying timing, read out and reset signals and the like. With on-chip row circuitry <b>296</b> and column circuitry <b>270</b>, CMOS based image sensor array <b>182</b> <i>a </i>may be operated to selectively address and read out data from individual pixels in an X-Y coordinate system. CMOS based image sensor array <b>182</b> <i>a </i>may also be operated by way of appropriate programming of image reader <b>100</b>, to selectively address and read out a portion of the full frame of pixels. For example, in these embodiments the portion of pixels read out can exclude undesired pixels external to a desired pixel region. The portion of pixels read can also represent a sampling of pixels in a region so that individual pixels, rows of pixels, or columns of pixels in the region of interest are not read out. Further details of image reader <b>100</b> operating in a windowed frame operating mode in which image reader <b>100</b> selectively addresses and reads out image data from less than all pixels of image sensor array <b>182</b> is described in connection with <figref idrefs="DRAWINGS">FIGS. 28A</figref>, <b>28</b>B, and <b>28</b>C. In general, image reader <b>100</b> can be programmed or otherwise configured to selectively address and read out from CMOS based image sensor array <b>182</b> <i>a </i>image data from a first plurality of pixels in the array independently of selectively addressing and reading out a second plurality of pixels in the array.</p>
  <p num="p-0097">In one embodiment, the pixel architecture can be as described in U.S. Pat. No. 5,986,297 assigned to Eastman Kodak Company and entitled “Color Active Pixel Sensor with Electronic Shuttering, Anti-blooming and Low Cross-talk.” In particular at column 3 lines 35 to 55 and at column 5 lines 25 to 55, the patent describes the cross sections of the relevant regions of the pixel architectures shown in the patent's <figref idrefs="DRAWINGS">FIGS. 1A and 2A</figref> (herein reproduced as <figref idrefs="DRAWINGS">FIGS. 8B and 8C</figref>). The disclosure states that the pixel in <figref idrefs="DRAWINGS">FIG. 8B</figref> comprises a photodiode <b>270</b> with a vertical overflow drain <b>274</b>, transfer gate <b>276</b>, floating diffusion <b>280</b>, reset gate <b>282</b>, reset drain <b>284</b>, and a light shield <b>286</b>. A light shield aperture <b>288</b>, color filter <b>290</b>, and micro lens <b>292</b> are placed over the photodetector such that light is focused through micro lens <b>292</b> into light shield aperture <b>288</b> after passing through color filter <b>290</b>. Therefore, the light entering photodiode <b>270</b> has a wavelength that is within a predetermined bandwidth as determined by the color filter <b>290</b>. The patent describes <figref idrefs="DRAWINGS">FIG. 8C</figref> as showing a second pixel architecture that is similar in many respects to the embodiment shown in <figref idrefs="DRAWINGS">FIG. 8B</figref> except that there are two transfer gates <b>294</b>, <b>296</b>, and a storage region <b>298</b>. In both cases the light shield is constructed by effectively covering all regions except the photodetectors (photodiode <b>270</b> in this case), with an opaque layer or overlapping layers, so that incident light falls only on the photodiode area. Creation of an aperture in a light shield that limits the creation of photoelectrons to the photodetector region suppresses cross-talk between pixels. In <figref idrefs="DRAWINGS">FIG. 8C</figref>, the floating diffusion is labeled <b>281</b>, the reset gate is labeled <b>283</b>, and the reset drain is labeled <b>285</b>. In some embodiments employing the pixel architecture described in U.S. Pat. No. 5,986,297, the color filter <b>290</b> may be omitted, and in other embodiments the microlens <b>292</b> may be omitted.</p>
  <p num="p-0098">A process <b>300</b> for collecting image data from a target with the image reader <b>100</b> is presented with respect to <figref idrefs="DRAWINGS">FIGS. 9</figref>, <b>10</b>A, <b>10</b>B, <b>10</b>C and <b>10</b>D. In various embodiments the target can contain a symbology such as a one or two-dimensional bar code. At step <b>302</b>, actuation module <b>124</b> initiates process <b>300</b> in response e.g., to trigger <b>216</b> being depressed or to a sensing of a presence of an object in a field of view of image reader <b>100</b>. In one embodiment, control module <b>112</b> may receive a trigger signal in response to a depressing of trigger <b>216</b> or the sensing of an object and responsively present a series of control signals to various modules, e.g., illumination module <b>104</b> and image collection module <b>108</b> in accordance with process <b>300</b>. The process <b>300</b> includes activating (step <b>304</b>) an illumination source to illuminate the target with illumination light <b>162</b>. In one embodiment, the activation of the illumination source occurs in response to an illumination control timing pulse <b>350</b>. The illumination of the target by the activated illumination source occurs for the duration of the illumination control timing pulse <b>350</b>. In one embodiment, the illumination source is the light source <b>160</b> and the illumination control timing pulse <b>350</b> is generated by the illumination control module <b>164</b> in the illumination module <b>104</b>. The process <b>300</b> also includes activating the global electronic shutter to simultaneously expose (step <b>312</b>) a plurality of pixels in a plurality of rows in an image sensor array to photoconvert incident radiation into electric charge. The simultaneous activation of the plurality of pixels occurs in response to an exposure control timing pulse <b>354</b>. In one embodiment, the simultaneous activation of the plurality of pixels occurs in response to a start portion <b>360</b> of the exposure control timing pulse <b>354</b>. In a further embodiment, the exposure control timing pulse <b>354</b> is generated by the global electronic shutter control module <b>190</b> (<figref idrefs="DRAWINGS">FIG. 5B</figref>) of the sensor array control module <b>186</b>.</p>
  <p num="p-0099">In one embodiment for collecting an image of a target that minimizes translational image distortion, the target is illuminated by overdriving the illumination sources, such as LEDs, to generate illumination several times brighter than standard operation. Referring to an example of the invention wherein image reader <b>100</b> includes imaging module <b>1802</b> as shown in <figref idrefs="DRAWINGS">FIGS. 33-35</figref>, LEDs <b>160</b> <i>c </i>through <b>160</b> <i>t </i>(that is, <b>160</b> <i>c</i>, <b>160</b> <i>d</i>, <b>160</b> <i>e</i>, <b>160</b> <i>f</i>, <b>160</b> <i>g</i>, <b>160</b> <i>h</i>, <b>160</b> <i>i</i>, <b>160</b> <i>j</i>, <b>160</b> <i>k</i>, <b>160</b> <i>l</i>, <b>160</b> <i>m</i>, <b>160</b> <i>n</i>, <b>160</b> <i>o</i>, <b>160</b> <i>p</i>, <b>160</b> <i>q</i>, <b>160</b> <i>r</i>, <b>160</b> <i>s</i>, and <b>160</b> <i>t</i>) each may have a standard recommend maximum DC operation current draw rating of 40 mA (100% LED current) but may be overdriven to draw more than e.g., 60 mA (150% current), or 80 mA (200% current) throughout the duration of illumination timing pulse <b>350</b>, or any one of pulses <b>350</b>′, <b>350</b>″, <b>350</b>′″ described herein. LEDs <b>160</b> <i>c </i>through <b>160</b> <i>t</i>, where LEDs <b>160</b> <i>c </i>through <b>160</b> <i>t </i>have a standard recommended maximum DC operating current draw rating of 40 mA, may also be overdriven to draw more than e.g., 120 mA (300% current), 160 mA (400% current), 200 mA (500% current), or 500 mA (1,250% current) throughout the duration of timing pulse <b>350</b> or any one of pulses <b>350</b>′, <b>350</b>″, <b>350</b>′″ described herein. Illumination timing pulse <b>350</b>, <b>350</b>′, <b>350</b>″, <b>350</b>′″ are shown as DC drive current pulses. However, according to the invention as indicated by <figref idrefs="DRAWINGS">FIG. 10E</figref>, pulses <b>350</b>, <b>350</b>′, <b>350</b>″, <b>350</b>′″ can also be pulse modulated or “strobed” pulses such that each pulse <b>350</b>, <b>350</b>′, <b>350</b>″, <b>350</b>′″ comprise a series of short duration individual pulses for driving LEDs <b>160</b>. Substituting a pulsed driving signal for a DC driving signal reduces the duty cycle of LEDs, and thus the power dissipated in the LEDs. Since in many cases the LED operating life is determined by the maximum junction temperature of the LED die, reduced power dissipation reduces the junction temperature. The net effect is that a higher peak current can be tolerated while not exceeding the maximum operating junction temperature limit for the LED die. In general, reducing the duty cycle of LEDs <b>160</b> increases the amount of current that can be safely driven through LEDs. The strobing rate of a “strobed” or “pulsed” illumination control pulses as described herein may be, e.g., 1,000 Hz to 10,000 Hz. According to this embodiment, the overdriven illumination sources in combination with the electronic global shutter allows for short exposure periods. That is, the bright illumination allows for a short integration time for each pixel and the global electronic shutter allows for all of the pixels in the image sensor to be simultaneously sensitive. With a short exposure period for a brightly illuminated target, an image reader of the present invention is able to collect a sharp non-distorted image even when the target is moving relative to the image reader. In one embodiment, the exposure period is less than 3.7 milliseconds. In one embodiment in which the light sources are overdriven, light sources with different colors are employed. For example, in one such embodiment the image reader includes white and red LEDs, red and green LEDs, white, red, and green LEDs, or some other combination chosen in response to, for example, the color of the symbols most commonly imaged by the image reader. In this embodiment, the different colored LEDs are each alternatively pulsed at a level in accordance with the overall power budget. In another such embodiment, both colored LEDs are pulsed each time but each at a relatively lower power level so that the overall power budget is again maintained. In a further embodiment, red, green, and blue LED's can be interleaved to simulate white light.</p>
  <p num="p-0100">Various embodiments of imaging module <b>1802</b> of image reader <b>100</b> are described with reference to <figref idrefs="DRAWINGS">FIG. 37</figref>. LEDs <b>160</b> of imaging module <b>1802</b> may be divided into banks as indicated in the chart of <figref idrefs="DRAWINGS">FIG. 37</figref>. Image reader <b>100</b> can be configured so that LEDs of each bank emits light in a certain emission wavelength band. In embodiment 8 depicted in the chart of <figref idrefs="DRAWINGS">FIG. 37</figref>, image reader <b>100</b> is configured so that aiming LEDs <b>160</b> <i>a</i>, <b>160</b> <i>b </i>emit green light and all illumination LEDs <b>160</b> <i>c </i>through <b>160</b> <i>t </i>emit red light. Additional embodiments are described in the chart of <figref idrefs="DRAWINGS">FIG. 37</figref>. Image reader <b>100</b> can be configured so that the light sources for the various banks may be energized simultaneously (e.g., bank <b>1</b>, bank <b>2</b>, bank <b>3</b>, bank <b>4</b> simultaneously energized) or sequentially (e.g., bank <b>1</b>, then bank <b>2</b>, then bank <b>3</b>, then bank <b>4</b>) by the illumination timing control pulse <b>350</b>, <b>350</b>′, <b>350</b>″, <b>350</b>′″.</p>
  <p num="p-0101">Referring again to <figref idrefs="DRAWINGS">FIGS. 9</figref>, <b>10</b>A, <b>10</b>B, <b>10</b>C, and <b>10</b>D the process <b>300</b> also includes processing (step <b>316</b>) the photoconversion generated electric charge to produce image data. As discussed above, the processing can include, for example, amplifying the data generated from the incident radiation. The processing further includes storing the generated image data values in a shielded portion of each of the plurality of pixels. The process <b>300</b> additionally includes reading out and processing (step <b>320</b>) the stored image data values from the plurality of pixels. As discussed above, the processing can include amplifying the data generated from the incident radiation and converting the generated data into a digital signal. The processing (step <b>320</b>) can also include storing a set of digital signal values corresponding to incident light on the plurality of pixels of image sensor array module <b>182</b> as a frame of image data. Image reader <b>100</b> at step <b>320</b> may store into memory module <b>116</b> a frame of image data including a plurality of N-bit (grey scale) pixel values, each pixel value representing light incident at one of the plurality of pixels. In one embodiment, the reading out of the plurality of pixels is controlled by a read out timing control pulse <b>368</b> generated by the read out module <b>198</b> of the sensor array control module <b>186</b>. In one embodiment, the read out timing control pulse <b>368</b> includes a plurality of pulses transmitted to each of the plurality of pixels. In one embodiment, at least a portion of the illumination control timing pulse <b>350</b> occurs during the exposure control timing pulse <b>354</b>. In one such embodiment, the operation of the image collection module <b>104</b> including the sensor array control module <b>186</b> with the global electronic shutter control module <b>190</b> is coordinated with the operation of the illumination module <b>104</b> including the illumination control module <b>164</b> by the control module <b>112</b> to achieve the overlap in the illumination <b>350</b> and exposure <b>354</b> control timing signals.</p>
  <p num="p-0102">In one embodiment as shown in <figref idrefs="DRAWINGS">FIG. 10A</figref>, the exposure control timing pulse <b>354</b> begins after and finishes before the illumination control timing pulse <b>350</b>. The read out control timing pulse <b>368</b> begins at the conclusion of the illumination control timing pulse <b>350</b>. In another embodiment as shown in <figref idrefs="DRAWINGS">FIG. 10B</figref>, the illumination control timing pulse <b>350</b>′ begins after and finishes before the exposure control timing pulse <b>354</b>′. In this embodiment, the read out control timing pulse <b>368</b>′ begins at the conclusion of the exposure control timing pulse <b>354</b>′. In further embodiments the exposure control timing pulse and the illumination control timing pulse overlap each other while occurring sequentially. In one such embodiment as shown in <figref idrefs="DRAWINGS">FIG. 10C</figref>, this sequential operation can include the illumination control timing pulse <b>350</b>″ starting, the exposure control timing pulse <b>354</b>″ starting, the illumination control timing signal pulse <b>350</b>″ ending, and then the exposure control timing pulse <b>354</b>″ ending. In this embodiment, the read out control timing pulse <b>368</b>″ begins at the conclusion of the exposure control timing pulse <b>354</b>″. In a further such embodiment as shown in <figref idrefs="DRAWINGS">FIG. 10D</figref>, the sequential operation can include the exposure control timing pulse <b>354</b>′″ starting, the illumination control timing pulse <b>350</b>′″ starting, the exposure control timing pulse <b>354</b>′″ ending, and then the illumination control timing signal pulse <b>350</b>′″ ending. In this embodiment, the read out control timing pulse <b>368</b>′″ begins at the conclusion of the illumination control timing signal pulse <b>350</b>′″. As discussed in connection with <figref idrefs="DRAWINGS">FIG. 10E</figref>, each illumination control timing pulse <b>350</b>, <b>350</b>′, <b>350</b>″, <b>350</b>′″ described herein may comprise a plurality of short duration individual pulses.</p>
  <p num="p-0103">Referring again to imaging module <b>1802</b>, an image reader <b>100</b> having imaging module <b>1802</b> may have an operating mode in which aiming LEDs <b>160</b> <i>a</i>, <b>160</b> <i>b </i>are controlled to be off or de-energized during exposure control timing pulse <b>354</b>, <b>354</b>′, <b>354</b>″, or <b>354</b>′″ so that light from LEDs <b>160</b> <i>a</i>, <b>160</b> <i>b </i>does not influence an image that is collected and transferred to decode module <b>150</b> or autodiscrimination module <b>152</b>. In another embodiment, aiming illumination LEDs <b>160</b> <i>a</i>, <b>160</b> <i>b</i>, in addition to illumination LEDs <b>160</b> <i>c </i>through <b>160</b> <i>t</i>, are controlled to be energized during exposure control timing pulse <b>354</b>, <b>354</b>′, <b>354</b>″, or <b>354</b>′″. Controlling aiming illumination LEDs <b>160</b> <i>c </i>through <b>160</b> <i>t </i>to be energized during exposure control timing pulse <b>354</b>, <b>354</b>′, <b>354</b>″, or <b>354</b>′″ increases a signal strength of image data corresponding regions of substrate, s, onto which aiming pattern <b>1838</b> is projected.</p>
  <p num="p-0104">With reference to process <b>300</b> (<figref idrefs="DRAWINGS">FIG. 9</figref>), image reader <b>100</b> may be configured so that illumination control pulse <b>350</b>, <b>350</b>′, <b>350</b>″, or <b>350</b>′″ at step <b>304</b> simultaneously energizes at least one of aiming LED <b>160</b> <i>a </i>or <b>160</b> <i>b </i>and at least one of illumination LEDs <b>160</b> <i>c </i>through <b>160</b> <i>t </i>so as to increase the intensity of illumination on substrate, s, and specifically the regions of substrate, s, onto which illumination pattern <b>1830</b> and aiming pattern <b>1838</b> are simultaneously projected. A decoding process carried out by decode module <b>150</b> or autodiscrimination module <b>152</b> where an image is collected pursuant to an exposure period wherein aiming LEDs <b>160</b> <i>a</i>, <b>160</b> <i>b </i>and illumination LEDs <b>160</b> <i>c </i>through <b>160</b> <i>t </i>are simultaneously energized may include a process wherein image data corresponding pattern <b>1838</b> (that is, image data corresponding to pixels of array onto which pattern <b>1838</b> is imaged) is selectively subjected to a decoding process such as a finder pattern locating process, a linear bar code symbol decode attempt or a quiet zone locating process. For example, with aiming pattern <b>1838</b> horizontally extending across a field of view, decode module <b>150</b> processing a collected full frame image may selectively analyze image data corresponding to center rows of image sensor <b>182</b> (i.e., image data corresponding to rows <b>2802</b> shown in <figref idrefs="DRAWINGS">FIG. 28</figref> <i>a</i>) for purposes of locating a finder pattern, decoding a linear bar code symbol, or locating a quiet zone where an image is collected pursuant to a frame exposure period wherein at least one aiming LED <b>160</b> <i>a</i>, <b>160</b> <i>b </i>and at least one illumination LED <b>160</b> <i>c </i>through <b>160</b> <i>t </i>are simultaneously energized. At step <b>320</b> of process <b>300</b> carried out with illumination control pulse <b>350</b>, <b>350</b>′, <b>350</b>″, or <b>350</b>′″ simultaneously energizing at least one aiming illumination LED e.g., <b>160</b> <i>a </i>and at least one illumination LED, e.g., <b>160</b> <i>t</i>, image reader <b>100</b> may collect either a full frame or a “windowed frame” of image data as is described in greater detail in connection with <figref idrefs="DRAWINGS">FIGS. 28A-28C</figref>. Image reader <b>100</b> may be configured so that where image reader <b>100</b> at step <b>320</b> collects a windowed frame of image data and at step <b>304</b> simultaneously illuminates at least one aiming illumination LED and at least one illumination LED, the windowed frame corresponds to the size and shape of illumination pattern <b>1838</b>. For example, where image reader <b>100</b> projects horizontal line aiming pattern <b>1838</b>, the windowed frame of image data readout at step <b>320</b> may be a windowed frame of image data corresponding to rows <b>2802</b> shown in <figref idrefs="DRAWINGS">FIG. 28A</figref> onto which pattern <b>1838</b> is imaged which is then processed as described herein (e.g., by attempting to decode linear bar code symbol by locating a quiet zone or by locating a finder pattern). In embodiments of the invention wherein aiming illumination LEDs and illumination LEDs are simultaneously driven by illumination control pulse <b>350</b>, <b>350</b>′, <b>350</b>″, or <b>350</b>′″, the aiming LEDs <b>160</b> <i>a</i>, <b>160</b> <i>b </i>and illumination LEDs <b>160</b> <i>c </i>through <b>160</b> <i>t </i>may be overdriven throughout the duration of pulse <b>350</b>, <b>350</b>′, <b>350</b>″, or <b>350</b>′″ as has been described herein.</p>
  <p num="p-0105">In one embodiment the CMOS image array <b>182</b> <i>a </i>can be implemented with a KAC-0331 640×480 VGA CMOS image sensor available from the Eastman Kodak Company. The KAC-0311 is more fully described in a technical description entitled, “KAC-0311 640×480 VGA CMOS IMAGE SENSOR Fully Integrated Timing, Analog Signal Processing &amp; 10 bit ADC.” Revision 1 dated Aug. 5, 2002 and available at http://www.kodak.com/global/plugins/acrobat/en/digital/ccd/products/cmos/KAC-0311 LongSpec.pdf, hereby incorporated by reference in its entirety. The following is an edited summary of the operation of the KAC-0311 taken from the aforementioned “Full Specification.” As summarized in this technical description, the KAC-0311 is a solid state active CMOS imager that integrates analog image acquisition, digitization, and digital signal processing on a single chip. The image sensor comprises a VGA format pixel array with 640×480 active elements. The image size is programmable by a user to define a window of interest. In particular, by programming the row and column start and stop operations, a user can define a window of interest down to a resolution of 1×1 pixel. In one embodiment of the KAC-0311 image sensor, the window can be used to enable a digital zoom operation of a viewport that can be panned. In another embodiment of the KAC-0311 image sensor, a constant field of view is maintained while subsampling is used to reduce the resolution the collected image.</p>
  <p num="p-0106">The pixels of the KAC-0311 image sensor are on a 7.8 μm pitch. The pixel architecture is Kodak's pinned photodiode architecture. The KAC-0311 image sensor is available in a Monochrome version without microlenses, or with Bayer (CMY) patterned Color Filter Arrays without microlenses. In one embodiment of the KAC-0311 image sensor, integrated timing and programming controls are used to enable progressive scan modes in either video or still image capture operation. In a further embodiment of KAC-0311 image sensor, a user can program the frame rates while maintaining a constant master clock rate.</p>
  <p num="p-0107">In the KAC-0311 image sensor, the analog video output of the pixel array is processed by an on-chip analog signal pipeline. In one embodiment of the KAC-0311 image sensor, correlated double sampling is used to eliminate pixel reset temporal and fixed pattern noise. In a further embodiment of the KAC-0311 image sensor, a frame rate clamp is used to enable contemporaneous optical black level calibration and offset correction. In yet another embodiment, the programmable analog gain of the KAC-0311 image sensor includes a global exposure gain to map the signal swing to the analog-to-digital converter input range. The programmable analog gain further includes white balance gain to perform color balance in the analog domain. In an additional embodiment, the analog signal processing chain of the KAC-0311 image sensor consists of column op-amp processing, column digital offset voltage adjustment, white balancing, programmable gain amplification, global programmable gain amplification, and global digital offset voltage adjustment. In one embodiment, the digitally programmable amplifiers are used to provide contemporaneous color gain correction for auto white balance as well as exposure gain adjustment. The offset calibration in various embodiments is done on a per column basis and globally. In addition, the per column offset correction can be applied by using stored values in the on-chip registers, and a ten-bit redundant signed digit analog-to-digital converter converts the analog data to a ten-bit digital word stream. In various embodiments of the KAC-0311 image sensor, the differential analog signal processing pipeline is used to improve noise immunity, the signal to noise ratio, and the system's dynamic range. In one embodiment, the serial interface of the KAC-0311 is an industry standard two line I<sup>2</sup>C compatible serial interface. In another embodiment, power for the KAC-0311 image sensor is provided by a single 3.3V power supply. In various embodiments, the KAC-0311 image sensor has a single master clock and operates at speeds up to 20 MHz.</p>
  <p num="p-0108">The operational and physical details of image sensors that can be used in the present invention and that are assigned to Eastman Kodak Company are also described in the U.S. Pat. No. 6,714,239 entitled “Active Pixel Sensor with Programmable Color Balance” and U.S. Pat. No. 6,552,323 entitled “Image Sensor with Shared Output Signal Line,” each of which is hereby herein incorporated by reference in its entirety. The following provides a brief summary of material from U.S. Pat. No. 6,522,323. In particular U.S. Pat. No. 6,552,323 discloses an image sensor comprising a plurality of pixels arranged in a plurality of rows and columns. The image sensor is further disclosed to include a global electronic shutter. Pixels in the same row of the disclosed image sensor share a pixel output node and an output signal line. Further, the disclosure indicates that image signal separation within a row is achieved by having two separate row select signal lines per row, one for every other pixel within a row, and a 1:2 column output signal line de-multiplexing scheme for each pair of columns. A schematic diagram, here reproduced as <figref idrefs="DRAWINGS">FIG. 11</figref>, shows two adjacent pixels <b>5</b>. Identifiers used in the schematic include the following: reset transistor with a reset gate (RG), transfer gate (TG), signal transistor (SIG), row select transistor with a row select gate (RSEL), photodetector (PD), and floating diffusion (FD). The operation of the global shutter is described at column 3 lines 25-45 of U.S. Pat. No. 6,552,323 with respect to the embodiment presented in <figref idrefs="DRAWINGS">FIG. 11</figref> and timing diagrams, here reproduced as <figref idrefs="DRAWINGS">FIG. 12</figref>. The disclosure indicates that readout commences by transfer of the integrated signal charge from the photodetectors <b>30</b> <i>a</i>, <b>30</b> <i>b </i>to the floating diffusions <b>10</b> <i>a</i>, <b>10</b> <i>b </i>in each pixel of the sensor simultaneously. Next, row select <b>1</b> (<b>15</b>) is turned on and the signal level of floating diffusion <b>1</b> (<b>10</b> <i>a</i>) is sampled and held by the column circuit <b>20</b> <i>a </i>by pulsing SS1. Row select <b>1</b> (<b>15</b>) is then turned off and row select <b>2</b> (<b>25</b>) is turned on and the signal level of floating diffusion <b>2</b> (<b>10</b> <i>b</i>) is sampled and held by the column circuit <b>20</b> <i>b </i>by pulsing SS2. The floating diffusions <b>10</b> <i>a</i>, <b>10</b> <i>b </i>in the row being read out are then reset by pulsing RG. Next row select <b>2</b> (<b>25</b>) is turned off and row select <b>1</b> (<b>15</b>) is turned on and the reset level of floating diffusion <b>1</b> (<b>10</b> <i>a</i>) is sampled and held by the column circuit <b>20</b> <i>a </i>by pulsing SR<b>1</b>. Row select <b>1</b> (<b>15</b>) is then turned off and row select <b>2</b> (<b>25</b>) turned on and the reset level of floating diffusion <b>2</b> (<b>10</b> <i>b</i>) is sampled and held by pulsing SR<b>2</b>. The readout of the sampled and held signals of the column circuits <b>20</b> <i>a</i>, <b>20</b> <i>b </i>is then done prior to the same pixel readout scheme commencing in the next row of the image sensor.</p>
  <p num="p-0109">In another embodiment, the CMOS image array <b>182</b> <i>a </i>can be implemented with a KAC-9630 128(H)×98(V) CMOS image sensor. The KAC-9630 is more fully described in a technical specification entitled, “Device Performance Specification—Kodak KAC-9630 CMOS Image Sensor,” September 2004, revision 1.1. This document is hereby herein incorporated by reference in its entirety. This document is available from Eastman Kodak (www.kodak.com), for example at http://www.kodak.com/global/plugins/acrobat/en/digital/ccd/products/cmos/KAC-9630LongSpec.pdf. This technical specification describes the KAC-9630 image sensor as a low power CMOS active pixel image sensor capable of capturing monochrome images at 580 frames per second. In addition the KAC-9630 image sensor is described as including an on-chip eight-bit analog-to-digital converter, fixed pattern noise elimination circuits and a video gain amplifier. The KAC-9630 is further described as having integrated programmable timing and control circuitry that allows for the adjustment of integration time and frame rate. The read out circuit in the KAC-9630 image sensor is described as capable of supporting a full frame read out on a single eight-bit digital data bus in less than 2 milliseconds. As indicated above, the KAC-9630 image sensor is described as including an integrated electronic shutter.</p>
  <p num="p-0110">In another embodiment, the CMOS image array <b>182</b> <i>a </i>can be implemented with a Micron image sensor such as the Wide VGA MT9V022 image sensor from Micron Technology, Inc., 8000 South Federal Way, Post Office Box 6, Boise, Id. 83707-0006. The MT9V022 image sensor is describe in more detail in the product MT9V099 product flyer available from Micron Technology (www.micron.com), for example at http://download.micron.com/pdf/flyers/mt9v022_(mi-0350)_flyer.pdf. This document is hereby herein incorporated by reference in its entirety.</p>
  <p num="p-0111">In some embodiments, the image reader <b>100</b> is capable of operating in either a rolling shutter mode or a global electronic shutter mode. In one such embodiment, the rolling shutter mode is used as part of an automatic focusing operation and the global electronic shutter mode is used to collect image data once the proper focus has been determined. The process of determining the proper focus and collecting a subsequent image is described by the process <b>400</b> shown in <figref idrefs="DRAWINGS">FIG. 13</figref>. Actuation module <b>124</b> may generate a trigger signal to initiate process <b>400</b> in response to e.g., a depressing of a trigger <b>216</b> by an operator or in response to an object being moved into a field of view of image reader <b>100</b>. In operation when a new image is collected by the image reader <b>100</b>, the image reader <b>100</b> illuminates (step <b>404</b>) a target containing an object, such as a bar code, and enters (step <b>408</b>) a rolling shutter operational mode in which a plurality of rows in the image reader's image sensor are sequentially exposed. As part of this operation, a frame exposure period can be defined as the time from the beginning of the exposure of the first row of the plurality of rows to the end of the exposure of the last row of the plurality of rows. In one embodiment, an imaging lens <b>212</b> of the image reader <b>100</b> is controlled to be in one of continuous motion or in stepwise continuous motion (step <b>414</b>) during at least a portion of the frame exposure period. As shown in the embodiment of <figref idrefs="DRAWINGS">FIG. 20</figref>, image reader <b>100</b> may have a lens driver module <b>165</b> controlled by control module <b>112</b> or another module for moving imaging lens <b>212</b> to change a focus setting of image reader <b>100</b>. In one such embodiment, the optical system has a plurality of discrete settings. For each discrete setting, lens <b>212</b> forms a distinct image on the image sensor for objects located at a particular distance from the image reader <b>100</b>. In one embodiment, one extreme of the optical system's focusing range corresponds to focusing incident radiation from objects located at infinity. An object is considered to be at “infinity” if its incident light rays are essentially parallel. In one embodiment, another extreme of the optical system's focusing range is the near point of the optical system. The near point of the optical system is the closest distance an object can be brought with respect to the optical system where the optical system is still able to create a distinct image of the object. In another embodiment, the variation of in the focus of the optical system does not cover the entire range of the optical system. For example in one such embodiment, a focus setting of image reader <b>100</b> is varied between focus settings that are millimeters apart. In another embodiment, a focus setting of image reader <b>100</b> is varied between focus settings that are centimeters apart. Configuring reader <b>100</b> to include lens driver module <b>165</b> allows a scanner to operate over an extended depth of field.</p>
  <p num="p-0112">With further reference to lens driver module <b>165</b>, various lens driving technologies and methods can be implemented. U.S. Pat. No. 4,350,418, incorporated by reference herein in its entirety, discloses a lens focus adjustment system including a distance adjusting ring, wherein position adjustment of a lens is achieved by rotation of the adjustment ring. U.S. Pat. No. 4,793,689, also incorporated herein by reference in its entirety, discloses a lens barrel having a hollow rotary ring rotatable about an optical axis that is disposed within a hollow of a hollow fixed cylinder with a bearing interposed there between, a moving cylinder moveable in response to rotation of the rotary ring, and a vibration wave motor disposed between the diametrical directions of the fixed cylinder and the rotary ring. U.S. Pat. No. 5,541,777, also incorporated herein by reference in its entirety, discloses an electromagnetic lens driver having a fixed member including an inside yoke and an outside yoke, an operationally disposed magnet, a moveable member for holding a body to be driven, a coil wound in an axial direction between the outside yoke and the inside yoke and position detector which detects the magnetic field of the operationally disposed magnet to generate a position indicating signal.</p>
  <p num="p-0113">The process <b>400</b> also includes reading out (step <b>420</b>) image data from the plurality of exposed rows. This image data is analyzed (step <b>424</b>) by an automatic focusing algorithm, such as the contrast detection method or the phase detection method. Using the row focus image information, the image reader <b>100</b> establishes (step <b>428</b>) a proper focus setting of lens <b>212</b> e.g., by determining a proper focus setting based on collected data and then moving the lens <b>212</b> to that setting or by assessing the present row image data to determine whether at the present focus setting, the image reader is acceptably focused. In various embodiments, the analysis of the image data can be performed by the image collection module <b>108</b>, the optics module, the control module <b>112</b>, or a dedicated auto-focusing module (e.g., an ASIC or FPGA dedicated for purposes of performing focus calculations). With the position of lens <b>212</b> properly established, the image reader <b>100</b> enters (step <b>432</b>) a global electronic shutter operational mode. It will be seen that in certain instances according to process <b>400</b>, image reader <b>100</b> may cease operation in a rolling shutter and commence operation in a global electronic shutter operational mode prior to reading out image data from each pixel of image sensor array module <b>182</b>. In the global electronic shutter operational mode, the image reader <b>100</b> collects (step <b>436</b>) a full frame of image data that is stored in memory module <b>116</b> and subsequently transferred to decode module <b>150</b> or autodiscriminating module <b>152</b> by control module <b>112</b>. According to this embodiment in which row image information is read out and analyzed during a time that the reader imaging lens <b>112</b> is controlled to be in motion, automatically focusing the image reader to image the target may be achieved within one frame of data. In various embodiments, the automatic focusing operations can be handled by a dedicated automatic focusing module or the focusing module can be incorporated into other modules such as the image collection module <b>108</b> and/or the control module <b>112</b>.</p>
  <p num="p-0114">With further reference to the steps of process <b>400</b>, the step <b>424</b> of analyzing row image data to determine focus characteristics is further described with reference to the flow diagram of <figref idrefs="DRAWINGS">FIG. 21</figref>, and the histogram plots of <figref idrefs="DRAWINGS">FIG. 22</figref> <i>a </i>and <figref idrefs="DRAWINGS">FIG. 22</figref> <i>b</i>. At step <b>2102</b> image reader <b>100</b> may construct a histogram plot of pixel values of the present row of image data read out at step <b>420</b>. <figref idrefs="DRAWINGS">FIG. 22A</figref> is a histogram plot of pixel values of a row of data corresponding to a bi-tonal image (such as in a bar code symbol on a monochrome substrate) that is acceptably focused. Histogram plot <b>2108</b> represents a high contrast image and includes numerous pixel values at the high end of the grey scale, numerous pixel values at the low end of the grey scale, and few pixel values at the center grey scale range. <figref idrefs="DRAWINGS">FIG. 22B</figref> is a histogram plot of pixel values of a row of data corresponding to a poorly focused bi-tonal image. The image data summarized by histogram <b>2110</b> is “flatter” lower contrast image data, meaning that it has fewer pixel values at extremes of the grey scale and a larger number of pixel values at a center of the grey scale. Accordingly, it can be seen that a focus level of an image can readily be determined utilizing image contrast information.</p>
  <p num="p-0115">At step <b>2104</b> image reader <b>100</b> assesses the collected histogram data. At step <b>2104</b> image reader <b>100</b> may either determine an appropriate in-focus setting for lens <b>212</b> or else determine whether the histogram data extracted from the present row of image data indicates that the image reader is acceptably focused at the present lens setting or position. Where image reader <b>100</b> at step <b>2104</b> determines a proper setting for lens <b>212</b> based on the collected histogram data, the histogram data may be from the present row or based on a combination of present row data and preceding row data. In a further aspect, position or setting values of lens <b>212</b> are recorded so that the histogram information of each row of image data that is read out has associated lens position data indicating a position of lens <b>212</b> at the time at which the row information was collected. At step <b>2104</b>, a transfer function for determining an in-focus lens setting may utilize row contrast information as summarized in histogram plots, as well as lens position data indicating a position of lens <b>212</b> associated with each set of row data.</p>
  <p num="p-0116">Referring to further steps of process <b>400</b>, image reader <b>100</b> at step <b>414</b> may control lens <b>212</b> to be in either continuous motion or in stepwise continuous motion. When controlled to be in continuous motion, lens <b>212</b> moves continuously throughout a time that sequentive rows of pixels of image sensor array module <b>182</b> are exposed and read out. When controlled to be in stepwise continuous motion, lens <b>212</b> repeatedly moves and stops throughout the time that rows of pixels of sensor module <b>182</b> are exposed and read out. In one embodiment of an image reader controlling lens <b>212</b> to be in stepwise continuous motion, image reader <b>100</b> continuously moves lens between two extreme points, a first, further field position and second, a nearer field position. In another embodiment of an image reader <b>100</b>, controlling lens <b>212</b> to be in stepwise continuous motion, image reader <b>100</b> continuously moves lens <b>212</b> between two extreme positions and intermittently stops lens <b>212</b> at one or more positions between the extreme positions. A lens <b>212</b> controlled to be in stepwise continuous motion can be considered to have motion periods, i.e., the times during which the lens moves, and stop periods, i.e., the times during which the lens is temporarily idle. In one embodiment of the invention, the motion of the lens <b>212</b> and a reading out of image data from rows of pixels are coordinated. For example, the lens movement and control of image sensor array module <b>182</b> can be coordinated such that an exposure period for one or more rows of image sensor array module <b>182</b> occurs during a stop period of lens <b>212</b> so that lens <b>212</b> is idle during an entire row exposure period. Further, while processing of image data corresponding to pixels exposed during motion phases of lens <b>212</b> is useful in certain embodiments, image reader <b>100</b> can be configured so that image data corresponding to pixels exposed during motion periods of lens <b>212</b> are discarded, e.g., during row analysis step <b>424</b>.</p>
  <p num="p-0117">Specific embodiments of the process <b>400</b> generically described with reference to <figref idrefs="DRAWINGS">FIG. 13</figref> are described with reference to the flow diagrams of <figref idrefs="DRAWINGS">FIGS. 23 and 24</figref>. In the embodiment of <figref idrefs="DRAWINGS">FIG. 23</figref>, image reader <b>100</b> at step <b>424</b> attempts to determine an in-focus lens setting based on collected row image data collected to that point. If at step <b>428</b> <i>a</i>, image reader <b>100</b> determines that enough information has been collected to determine an in-focus position of lens <b>212</b>, image reader <b>100</b> determines an in-focus setting for lens <b>212</b> and proceeds to step <b>428</b> <i>b </i>to move lens <b>212</b> to the determined in-focus position. If sufficient information has not been collected, image reader <b>100</b> returns to step <b>432</b> to collect additional row information. Image reader <b>100</b> may continue to read and process row image data while moving lens <b>212</b> at step <b>428</b> <i>b</i>, e. g., for purposes of confirming that the determined in-focus position is correct. When lens <b>212</b> has been moved to the determined in-focus position, image reader <b>100</b> proceeds to step <b>432</b> to enter a global electronic shutter operational mode of operation. At the time that image reader <b>100</b> enters the global shutter operating mode (step <b>432</b>) image reader <b>100</b> may halt the motion of lens <b>212</b>. The image reader then proceeds to step <b>436</b> to collect a full frame of image data, and then to step <b>438</b> to transfer image data to one of the dataform decode module <b>150</b> or autodiscriminating module <b>152</b>.</p>
  <p num="p-0118">In the embodiment of process <b>400</b> described with reference to <figref idrefs="DRAWINGS">FIG. 24</figref>, image reader <b>100</b> establishes an in-focus setting of lens <b>212</b> by assessing at step <b>424</b> present row data (the most recent collected row data) to determine whether the present row data indicates that image reader <b>100</b> is presently in-focus. If image reader <b>100</b> at step <b>428</b> <i>d </i>determines that image reader <b>100</b> is presently not in focus, image reader <b>100</b> returns to step <b>420</b> to collect additional row information. If at step <b>420</b> image reader <b>100</b> determines that the reader is presently in an in-focus position, image reader <b>100</b> proceeds to step <b>432</b> to enter a global electronic shutter mode of operation. At the time that image reader <b>100</b> enters the global shutter operating mode, (step <b>432</b>) image reader <b>100</b> may halt the motion of lens <b>212</b>. The image reader <b>100</b> then proceeds to step <b>436</b> to collect a full frame of image data, and then to step <b>438</b> to transfer image data to one of the dataform decode module <b>150</b> or autodiscriminating module <b>152</b>.</p>
  <p num="p-0119">It will be understood with reference to process <b>400</b> or process <b>800</b> that image reader <b>100</b> in establishing an “in focus” position may designate a prospective or present position of lens <b>212</b> to be “in focus” on the basis of the prospective or present lens position rendering indicia in better focus that other available lens focus positions. Thus, where a lens focus position is not highly focused in a general sense, reader <b>100</b> may, nevertheless, designate the position as being “in focus” if it renders indicia more in focus than other available lens position. In one specific embodiment, lens <b>100</b> may be “toggled” between a limited number of discrete positions (e.g., two positions) when it is controlled to be in stepwise continuous motion. In such an embodiment, image reader <b>100</b> may designate one of the limited number of possible discrete positions to be the “in focus” positions if the lens position renders indicia more in focus than the remaining possible positions. Particularly in the configuration where lens <b>212</b> is “toggled” between a limited number of discrete positions, the focus determining steps may be omitted and the image data transferred directly to the decode module <b>150</b> or autodiscrimination module <b>152</b>. Particularly when there are a limited number of alternate focus positions, the in-focus position can readily be discriminated based on which position the results in a successful decode. Discriminating an in-focus position by way of decode attempts may reduce average decode time.</p>
  <p num="p-0120">In a variation of the invention, image reader <b>100</b> at step <b>420</b> reads out a predetermined number of rows of image data and analyzes the predetermined number of rows at step <b>424</b>. The predetermined number of rows may be e.g., 2 rows, 3 rows, 10 rows or all of the rows (100+) rows of image sensor array <b>182</b>. Image reader <b>100</b> at step <b>424</b> may select the best focused (e.g., highest contrast) row out of the plurality of rows and determine that the recorded focus setting associated with the best focused row is the “in-focus” setting of image reader <b>100</b>. Alternatively, image reader <b>100</b> may calculate -in-focus setting data utilizing data image collected over several rows. When a focus setting has been determined, in any one of the above variations, image reader <b>100</b> may first enter global electronic shutter operational mode at step <b>432</b>, and then move lens <b>212</b> into the determined focus position setting or else image reader <b>100</b> may alternatively move lens <b>212</b> to the determined lens setting prior to entering the global electronic shutter operational mode at step <b>432</b> or these two operations may occur at the same time.</p>
  <p num="p-0121">In another embodiment of the automatic focusing operation, as described later in connection with <figref idrefs="DRAWINGS">FIGS. 25-30B</figref>, the global electronic shutter operational mode may be used during both the focusing period and the data collection period. According to process <b>800</b> as described herein, during the autofocusing period a limited, “windowed” frame of image data may be collected for each variation in the focus setting or position. For example, only the central region, or a central group of scan lines—such as the middle ten scan lines, of the image sensor is read out and analyzed by the focus determination algorithm. According to this embodiment, the limited frame of data provides adequate information for the focus determination algorithm while significantly decreasing the time required to collect the series of frames required to focus the image reader.</p>
  <p num="p-0122">In alternative embodiments, the specific order of the steps in the process <b>400</b> or process <b>800</b> can be altered without departing from the inventive concepts contained therein. In various other embodiments, the circuitry implementing the rolling shutter operation and the circuitry implementing the global electronic shutter operation can be implemented on the same CMOS chip or one or both of the circuitry components can be implemented on separate dedicated chips. In an additional embodiment, the rolling shutter functionality and the global electronic shutter operation can be combined in a single module that includes hardware, software, and/or firmware.</p>
  <p num="p-0123">In another embodiment of the image reader <b>100</b> that operates in either a rolling shutter or a global electronic shutter mode, the image reader <b>100</b> is able to dynamically shift between the global electronic shutter operational mode and the rolling shutter operational mode. In one such embodiment, the image reader <b>100</b> shifts from the default global electronic shutter operational mode to the rolling shutter operational mode when the integration time is shorter than a given threshold. Many commercially available imagers are implemented with light shields that allow some amount of light leakage into the storage element or with electronic switches that do not completely isolate the storage element from the photosensitive element. As a result of this, the contents of the storage element can be adversely influenced by the ambient illumination incident upon the imager after the charge has been transferred to the storage element. The following provides a numeric example of such operation.</p>
  <p num="p-0124">In general, the shutter efficiency of a CMOS image sensor with global electronic shutter capabilities specifies the extent to which the storage area on the image sensor is able to shield stored image data. For example, if a shutter has an efficiency of 99.9%, then it takes an integration time (also known as exposure time) that is 1,000 times longer to generate the same amount of charge in the shielded portion as in the unshielded portion of the image sensor. Therefore, in an image capture cycle, the following equation provides an indication of the light irradiance on the imager from the ambient light that can be tolerated during the time period after the image is shifted into the storage region relative to the light irradiance on the imager from the object illuminated with the ambient illumination and the light sources <b>160</b> during the time period before the image is shifted into the storage region while not exceeding a desired degradation percentage. The equation can also address the case where the light incident upon the imager is the same during the entire imaging cycle. In both instances, one needs to know the minimum integration that can be used without the introduction of a maximum degradation.
<br>(Amb. Irrad)*<i>T</i>frame*(100%−% eff)=(Amb. Irrad+Light Source Irrad)*<i>T</i>exposure*(% deg)</p>
  <p num="p-0125">In many instances the light on the imager is unchanged during the exposure period and during the remainder of the frame. In this situation the light irradiance on the imager is constant, and it is possible to solve for the minimum integration time that can be used without the light leakage excessively perturbing the desired image. Solving the equation in this case, allows the calculation of the minimum integration period for a specific degradation. The following constant irradiance numeric example is for a shutter efficiency of 99.9%, a frame rate of 50 fps (frame time=20 ms), and a maximum tolerated degradation of 5%.
<br>20 ms*(100%−99.9%)=(<i>T</i>exposure*5%)<br>
or solving for the minimum exposure time that can be used without incurring a degration of more than 5%:
<br>Texposure=0.4 ms.<br>
Thus if the integration time during image capture is shorter than 0.4 ms, then the degradation leakage (both optical or electrical) will cause an error to be introduced of 5% or greater.
</p>
  <p num="p-0126">In one embodiment that addresses image degradation introduced by excessive ambient light, the image reader <b>100</b> shifts to rolling shutter operation when the integration time becomes shorter than a level determined with respect to the frame rate, maximum allowable degradation and shutter efficiency of the image reader. A process <b>600</b> for shifting operational modes in response to short integration times is shown in <figref idrefs="DRAWINGS">FIG. 14</figref>. Actuation module <b>124</b> may generate a trigger signal to initiate process <b>600</b> in response to e.g., a depressing of a trigger <b>216</b> by operator or in response to an object being provided into a field of view of image reader <b>100</b>. The process <b>600</b> includes storing (step <b>604</b>) a calculated minimum integration time. In one embodiment, this threshold is determined in accordance with the equations presented above. Some of the inputs to these equations, such as the shutter efficiency, maximum acceptable image degradation leakage, and frame rate, can be configured in the image reader <b>100</b> as part of its initial setup or at a later time. The process <b>600</b> also includes collecting (step <b>608</b>) image data. As part of the collection of image data, an exposure time for the current environmental conditions is established (step <b>612</b>) by the sensor array control module <b>186</b>. In various embodiments, this exposure time is established by the global electronic shutter control module <b>190</b>, the optics module <b>178</b>, or another appropriate module in the image reader <b>100</b>. To determine whether the operational mode of the image reader <b>100</b> should shift from global shutter to rolling shutter, the established exposure time is compared (step <b>616</b>) with the minimum integration time threshold. If the established integration time is shorter than the calculated minimum integration time threshold, then the operational mode of the image reader <b>100</b> is shifted (step <b>620</b>) from global electronic shutter to rolling shutter. If the established integration time is greater than or equal to the calculated minimum integration time threshold, then the global electronic shutter operational mode (step <b>628</b>) is maintained.</p>
  <p num="p-0127">Further embodiments of the invention are described with reference to <figref idrefs="DRAWINGS">FIG. 15A</figref>, and the flow diagrams of <figref idrefs="DRAWINGS">FIGS. 31 and 32</figref>. As shown in <figref idrefs="DRAWINGS">FIG. 15A</figref>, image reader <b>100</b> can be configured to have user selectable configuration settings. For example, as shown in <figref idrefs="DRAWINGS">FIG. 15A</figref>, image reader <b>100</b> may present on display <b>504</b> a graphical user interface (GUI) menu option display screen <b>3170</b> which presents to an operator the user selectable configuration options of a rolling shutter operational mode and a global shutter operational mode. GUI display screens may be configured with tool kits associated with certain available operating systems such as WINDOWS CE, which may be installed on image reader <b>100</b>. When reader <b>100</b> is configured to include a browser or is otherwise configured with suitable parsers and interpreters, GUI <b>3170</b> can be created using various open standard languages (e.g., HTML/JAVA, XML/JAVA). In the embodiment of <figref idrefs="DRAWINGS">FIG. 15A</figref>, GUI icon <b>3152</b> is a rolling shutter selection button and GUI icon <b>3154</b> is a global electronic shutter menu option. When icon <b>3152</b> is selected, image reader <b>100</b> is configured so that when image reader <b>100</b> receives a next trigger signal as described herein to initiate a decode attempt, image reader <b>100</b> collects image data utilizing a rolling shutter operating mode without utilizing the global electronic operational mode. When icon <b>3154</b> is selected, image reader <b>100</b> is configured so that when image reader <b>100</b> receives a next trigger signal to initiate a decode attempt, image reader <b>100</b> collects image data utilizing the global electronic shutter operational mode without utilizing the rolling shutter operational mode. GUI <b>3170</b> can be created to permit additional user selectable configuration options. In the embodiment of <figref idrefs="DRAWINGS">FIG. 15A</figref>, selection of button <b>3156</b> (which may be in text or icon form) configures image reader <b>100</b> so that process <b>300</b> is executed the next time a trigger signal is received. Selection of button <b>3158</b> configures image reader <b>100</b> so that process <b>400</b> is executed a next time a trigger signal is received. Selection of button <b>3160</b> configures image reader <b>100</b> so that process <b>600</b> is executed a next time a trigger signal is received. Selection of button <b>3162</b> configures image reader <b>100</b> so that process <b>800</b> is executed a next time a trigger signal is received. Selection of button <b>3164</b> configures image reader <b>100</b> so that image reader <b>100</b> is in “image capture” mode of operation such that a next time a trigger signal is received, image reader collects image data such as a 2D full frame of image data and outputs an image (e.g., to display <b>504</b> or a spaced apart device) without transferring the collected image data to module <b>150</b> or module <b>152</b>. In shipping applications, it may be beneficial to capture images in an “image capture” mode corresponding to moving objects (e.g., a moving delivery vehicle, a package on an assembly line). Accordingly, it will be seen that execution of an image capture mode utilizing a global shutter operational mode of operation yields significant advantages, in that image distortion is reduced using a global shutter operational mode. The selection between a rolling shutter configuration and a global electronic shutter configuration or the configurations associated with buttons <b>3156</b>, <b>3158</b>, <b>3160</b>, <b>3162</b>, and <b>3164</b> can also be made with use of commands of a software development kit (SDK). A system can be created so that SDK-created commands (e.g., a “ROLLING SHUTTER” and a “GLOBAL SHUTTER” command) causing image reader <b>100</b> to be in one of a rolling shutter configuration and a global electronic shutter configuration can be selected at a host terminal spaced apart from image reader <b>100</b> and transmitted to image reader <b>100</b> to reconfigure reader <b>100</b>.</p>
  <p num="p-0128">Referring to the flow diagram of <figref idrefs="DRAWINGS">FIG. 31</figref>, an operator selects between a rolling shutter configuration and a global electronic shutter configuration at step <b>3102</b>. If an operator selects the rolling shutter configuration, image reader <b>100</b> proceeds to step <b>3104</b>. At step <b>3104</b> image reader <b>100</b> is driven from an idle state to an active reading state by the generation of a trigger signal (e.g., by manual actuation of trigger <b>216</b> or another method) and then automatically executes steps <b>3106</b> and <b>3108</b>. At step <b>3106</b> image reader <b>100</b> collects image data utilizing a rolling shutter operational mode and at step <b>3108</b> the image data collected at step <b>3106</b> is transferred to dataform decode module <b>150</b> or autodiscrimination module <b>152</b> to decode or otherwise process the image data. If at step <b>3102</b> a global electronic shutter mode is selected, image reader <b>100</b> proceeds to step <b>3118</b>. At step <b>3118</b> image reader <b>100</b> is driven from an idle state to an active reading state by the generation of a trigger signal (e.g., by manual actuation of trigger <b>216</b> or another method) and then automatically executes steps <b>3118</b> and <b>3120</b>. At step <b>3118</b> image reader <b>100</b> collects image data utilizing a global electronic shutter operational mode and at step <b>3122</b> the image data collected at step <b>3118</b> is transferred to dataform decode module <b>150</b> or autodiscrimination module <b>152</b> to decode or otherwise process the image data.</p>
  <p num="p-0129">Another embodiment of the invention is described with reference to the flow diagram of <figref idrefs="DRAWINGS">FIG. 32</figref>. In the embodiment described with reference to the flow diagram of <figref idrefs="DRAWINGS">FIG. 32</figref>, image reader <b>100</b> is configured to collect image data and attempt to decode image data utilizing a rolling shutter operational mode and a global shutter operational mode. At step <b>3202</b> a trigger signal is generated as described herein (e.g., by manual actuation of trigger <b>216</b> or another method) to drive image reader <b>100</b> from an idle state to an active reading state and all of steps <b>3204</b>, <b>3206</b> may be automatically executed thereafter. At step <b>3204</b> image reader <b>100</b> enters a rolling shutter operational mode. At step <b>3206</b> image reader <b>100</b> collects image data such as a full frame of image data or a windowed frame of image data utilizing the rolling shutter operational mode. At step <b>3208</b> image reader <b>100</b> transfers the image data collected at step <b>3206</b> to dataform decode module <b>150</b> and/or autodiscrimination module <b>152</b>. Dataform decode module <b>150</b> or autodiscrimination module <b>152</b> may decode or otherwise process the image data collected and output a result (e.g., output a decoded bar code message to display <b>504</b> and or a spaced apart device). At step <b>3118</b> image reader <b>100</b> enters a global electronic shutter operational mode. At step <b>3212</b> image reader <b>100</b> collects image data utilizing the global electronic shutter operational mode. The image data collected at step <b>3212</b> may be full frame or a windowed frame image data. At step <b>3214</b> image reader <b>100</b> transfers the image data collected at step <b>3212</b> to dataform decode module <b>150</b> or autodiscrimination module <b>152</b>. Dataform decode module <b>150</b> or autodiscrimination module <b>152</b> may decode or otherwise process the image data collected and output a result (e.g., output a decoded bar code message to display <b>540</b> and/or a spaced apart device). As indicated by control loop arrow <b>3216</b>, image reader <b>100</b> may automatically repeat steps <b>3204</b>, <b>3206</b>, <b>3208</b>, <b>3210</b>, <b>3212</b>, and <b>3214</b> until a stop condition is satisfied. A stop condition may be e.g., the generation of a trigger stop signal (as may be generated by the release of trigger <b>216</b>) or the successful decoding a predetermined number of bar code symbols.</p>
  <p num="p-0130">Another process according to the invention is described with reference to the flow diagram of <figref idrefs="DRAWINGS">FIG. 25</figref>. Process <b>800</b> is similar to process <b>400</b> in that it involves the processing of a limited amount of image data collected during a time that lens <b>212</b> is controlled to be in motion. With process <b>400</b> and with process <b>800</b> an in-focus position of lens <b>212</b> is quickly established. Whereas process <b>400</b> involves utilization of an image sensor array module <b>182</b> operated, at different times during the course of the process, in a first rolling shutter operational mode and a second, subsequently executed global electronic operational mode, process <b>800</b> may be implemented with use of one of the selectively addressable image sensor array modules described herein operated throughout the process in either one of a rolling shutter mode of operation or in a global electronic mode of operation.</p>
  <p num="p-0131">With further reference to process <b>800</b>, actuation module <b>124</b> at step <b>802</b> initiates process <b>800</b> by generating a trigger signal, e.g., in response to a depression of a trigger <b>216</b>, a sensing of an object in a field of view of image reader or receipt of a command from a spaced apart device. At step <b>814</b> image reader <b>100</b> sets lens <b>212</b> into motion. At step <b>814</b> image reader <b>100</b> may control lens <b>212</b> to be in one of continuous motion or stepwise continuous motion.</p>
  <p num="p-0132">At step <b>820</b> image reader <b>100</b> reads out a “windowed frame” of image data from image sensor array module <b>182</b>. CMOS image sensors can be operated in a windowed frame operating mode. In a windowed frame operating mode, image data corresponding to only a selectively addressed subset of all pixels of an image sensor array is read out. Examples of image reader <b>100</b> operating in windowed frame operating modes are described with reference to <figref idrefs="DRAWINGS">FIGS. 28A</figref>, <b>28</b>B and <b>28</b>C, wherein image sensor arrays are represented with each square of the grid representing a 10×10 block of pixels, and wherein shaded regions <b>2802</b>, <b>2804</b>, and <b>2806</b> represent pixels that are selectively addressed and selectively subjected to readout. In the embodiment of <figref idrefs="DRAWINGS">FIG. 28A</figref>, a windowed frame operating mode is illustrated wherein windowed image data is read out of image sensor array <b>182</b> by selectively addressing and reading out only centerline pattern of pixels consisting of a set of rows of pixels at a center of image sensor array module <b>182</b>. Alternatively, in a windowed frame operating mode image reader <b>100</b> may selectively address and selectively read out image data from a single row of pixels of image sensor array module <b>182</b>. Further, in a windowed frame operating mode, image reader <b>100</b> may selectively address, and selectively read out image data from rows <b>2802</b> <i>a </i>and <b>2802</b> <i>b</i>. In the embodiment of <figref idrefs="DRAWINGS">FIG. 28B</figref>, a windowed frame operating mode is illustrated wherein windowed image data is read out of image sensor array module <b>182</b> by selectively addressing and reading out only a collection of positionally contiguous pixels (i.e., a collection of pixels that are adjacent to one another) at a center of image sensor array module <b>182</b>. In the embodiment of <figref idrefs="DRAWINGS">FIG. 28C</figref>, a windowed frame operating mode is illustrated wherein windowed image data is read out of image sensor array module <b>182</b> by selectively reading out spaced apart clusters of 10×10 blocks of positionally contiguous pixels. In all of the windowed frame operating modes described with reference to <figref idrefs="DRAWINGS">FIGS. 28A</figref>, <b>28</b>B and <b>28</b>C, image data corresponding to less than half of the pixels of the image sensor is selectively addressed and read out. When operating in a windowed frame operating mode image reader <b>100</b> may collect image data corresponding to light incident on pixels in one or more of the patterns illustrated in <figref idrefs="DRAWINGS">FIGS. 28A</figref>, <b>28</b>B or <b>28</b>C or another pattern. Such collections of image data may include a collection of gray scale values and may be termed windowed frames of image data.</p>
  <p num="p-0133">A windowed frame operating mode described herein is contrasted with an alternative operating mode in which a full frame of image data is stored into memory module <b>116</b>, and then a portion of that full frame of image data is designated as a region of interest (i.e., a “sample” region) which is subject to further processing. In a windowed frame operating mode a frame of image data may collected in a fraction of the time required to collect a full frame of image data.</p>
  <p num="p-0134">With further reference to process <b>800</b> image reader <b>100</b> at step <b>824</b> analyzes a windowed frame of image data to determine focus characteristics of image reader <b>100</b>. The step of analyzing windowed frame image data to determine focus characteristics is further described with reference to the flow diagram of <figref idrefs="DRAWINGS">FIG. 29</figref>, and the histogram plots of <figref idrefs="DRAWINGS">FIG. 30A</figref> and <figref idrefs="DRAWINGS">FIG. 30B</figref>. At step <b>4102</b> image reader <b>100</b> may construct a histogram plot of pixel values of the present windowed frame of image data read out at step <b>820</b>. <figref idrefs="DRAWINGS">FIG. 30A</figref> is a histogram plot of pixel values of a row of data corresponding to a bi-tonal image (such as in a bar code symbol on a monochrome substrate) that is acceptably focused. Histogram plot <b>4108</b> represents a high contrast image and includes numerous pixel values at the high end of the grey scale, numerous pixel values at the low end of the grey scale, and few pixel values at the center grey scale range. <figref idrefs="DRAWINGS">FIG. 30B</figref> is a histogram plot of pixel values of a windowed frame of image data corresponding to a poorly focused bi-tonal image. The image data summarized by histogram <b>4110</b> is “flatter,” lower contrast image meaning that it has fewer pixel values at extremes of the grey scale and a larger number of pixel values at a center of the grey scale. Accordingly, it can be seen that a focus level of an image can readily be determined utilizing image contrast information.</p>
  <p num="p-0135">At step <b>4104</b>, image reader <b>100</b> assesses the collected histogram data. At block <b>4104</b> image reader <b>100</b> may either determine an appropriate in-focus setting for lens <b>212</b> or else determine whether the histogram data extracted from the present row of image data indicates that the image reader <b>100</b> is acceptably focused at the present lens position. Where image reader <b>100</b> at step <b>4104</b> determines a proper setting for lens <b>212</b> based on the collected histogram data, the histogram data may be from the present windowed frame of image data or based on a combination of present windowed frame of image data and preceding data of previously collected one or more frames of windowed image data. In a further aspect, position or setting values of lens <b>212</b> are recorded so that the histogram information of each row of image data that is read out and analyzed has associated lens position data indicating a position of lens <b>212</b> at the time at which the windowed frame of image data information was collected. At step <b>4104</b> a transfer function for determining an in-focus lens setting may utilize windowed frame contrast information as summarized in histogram plots, as well as lens position data indicating a position of lens <b>212</b> associated with each collected windowed frame of image data.</p>
  <p num="p-0136">Referring to further steps of process <b>800</b>, image reader <b>100</b> at step <b>814</b> may control lens <b>212</b> to be in either continuous motion or in stepwise continuous motion. When controlled to be in continuous motion, lens <b>212</b> moves continuously throughout a time that pixels corresponding to a windowed frame of image data are exposed and read out. When controlled to be in stepwise continuous motion, lens <b>212</b> repeatedly moves and stops throughout the time that pixels corresponding to a windowed frame of image data are exposed and read out. In one embodiment of an image reader <b>100</b> controlling lens <b>212</b> to be in stepwise continuous motion, image reader <b>100</b> continuously moves lens between two extreme points, a first further-field position and second, a nearer-field position. In another embodiment of an image reader <b>100</b> controlling lens <b>212</b> to be in stepwise continuous motion, image reader <b>100</b> continuously moves lens <b>212</b> between two extreme positions and intermittently stops lens <b>212</b> at one or more positions between the extreme positions. A lens <b>212</b> controlled to be stepwise continuous motion can be considered to have motion periods, i.e., the times during which the lens moves, and stop periods corresponding the time the lens is temporarily idle. In one embodiment of the invention, the motion of the lens <b>212</b> and a reading out of image data from rows of pixels are coordinated. For example, the stepwise movement of lens <b>212</b> and control of image sensor array module <b>182</b> can be coordinated such that a stop period of a lens in stepwise continuous motion occurs during an exposure period for exposing pixels corresponding to a windowed frame of image data and motion periods occur before and after such an exposure period. Further, while processing of image data corresponding to pixels exposed during motion periods of lens <b>212</b> is useful in certain embodiments, image reader <b>100</b> can be configured so that image data corresponding to pixels exposed during motion periods of lens <b>212</b> are discarded, e.g., during analysis step <b>824</b>.</p>
  <p num="p-0137">Specific embodiments of the process <b>800</b> generically described with reference to <figref idrefs="DRAWINGS">FIG. 25</figref> are described with reference to the flow diagrams of <figref idrefs="DRAWINGS">FIGS. 26 and 27</figref>. In the embodiment of <figref idrefs="DRAWINGS">FIG. 26</figref>, image reader <b>100</b> at step <b>824</b> attempts to determine an in-focus setting based on collected windowed frame image data collected to that point. If at block <b>828</b> <i>a </i>image reader <b>100</b> determines that enough information has been collected to determine an in-focus position of image reader <b>100</b>, image reader <b>100</b> proceeds to step <b>828</b> <i>b </i>to move the lens to the determined in-focus position. If sufficient information has not been collected, image reader returns to step <b>820</b> to collect additional windowed frame information. Image reader <b>100</b> may continue to read and process windowed frame image data while moving lens <b>212</b> at step <b>828</b> <i>b</i>, e.g., for purposes of confirming that the determined in-focus position is correct. When lens <b>212</b> has been moved to the determined in-focus position image reader <b>100</b> proceeds to step <b>836</b> to collect a full frame of image data (e.g., in accordance with process <b>300</b>), and then proceeds to step <b>838</b> to transfer the collected image data to one of dataform decode module <b>150</b> or autodiscriminating module <b>152</b>.</p>
  <p num="p-0138">In the embodiment of process <b>800</b> described with reference to <figref idrefs="DRAWINGS">FIG. 27</figref>, image reader <b>100</b> establishes an in-focus setting of lens <b>212</b> by assessing at step <b>824</b> present windowed frame image data (the most recent collected windowed frame data) to determine whether the present windowed frame image data indicates that image reader <b>100</b> is presently in-focus. If image reader <b>100</b> at step <b>828</b> <i>c </i>determines that image reader <b>100</b> is presently not in focus, image reader <b>100</b> returns to step <b>820</b> to collect additional windowed frame information. If at step <b>828</b> image reader <b>100</b> determines that the reader is presently in an in-focus position, image reader <b>100</b> proceeds to step <b>836</b> to collect a full frame of image data, (e.g., in accordance with process <b>300</b>), and then proceeds to step <b>838</b> to transfer the collected image data to one of dataform decode module <b>150</b> or autodiscriminating module <b>152</b>.</p>
  <p num="p-0139">In a variation of the invention, image reader <b>100</b> at step <b>820</b> may read out a predetermined number of windowed frames of image data, and at step <b>824</b> may analyze a predetermined number of windowed frames of image data. The windowed frames of image data may have the same pattern (e.g., always the pattern of <figref idrefs="DRAWINGS">FIG. 28A</figref>) or may have alternating patterns (e.g., first the pattern of <figref idrefs="DRAWINGS">FIG. 28A</figref>, next the pattern of <figref idrefs="DRAWINGS">FIG. 28B</figref>, and next the pattern of <figref idrefs="DRAWINGS">FIG. 28C</figref>). In another variation, image reader <b>100</b> may transfer each collected windowed frame of image data, subsequent to collection, to dataform decode module <b>150</b> and/or autodiscrimination module <b>152</b>. At step <b>824</b>, image reader <b>100</b> analyzes the predetermined number of frames of image data in order to determine an in-focus setting of image reader <b>100</b>. In determining an in-focus setting, image reader <b>100</b> may select the in-focus setting associated with the best focused (highest contrast) windowed frame of image data out of the plurality of windowed frames of image data or else image reader <b>100</b> may calculate a focus setting utilizing image data from the plurality of windowed frames collected. In any of the variations of process <b>800</b>, image reader <b>100</b> may collect a full frame of image data at step <b>836</b> after determining an in-focus setting of image reader <b>100</b> before or after moving lens <b>212</b> to the determined setting position to establish an in-focus setting.</p>
  <p num="p-0140">It will be understood with reference to process <b>400</b> and process <b>800</b> that image reader <b>828</b> in establishing an “in focus” position may designate a prospective or present position of lens <b>212</b> to be “in focus” on the basis of the prospective or present lens position rendering target indicia in better focus than other available lens focus positions. Thus, where a lens focus position is not highly focused in a general sense reader <b>100</b> may, nevertheless, designate the position as being “in focus” if it renders target indicia more in focus than other available lens position. In one specific embodiment, lens <b>212</b> may be “toggled” between a limited number of discrete positions (e.g., two positions) when it is controlled to be in stepwise continuous motion. In such an embodiment, image reader <b>100</b> may designate one of the limited number of possible discrete positions to be the “in-focus” position if the lens position renders target indicia more in focus than the remaining possible positions. Particularly in the configuration where lens <b>212</b> is “toggled” between a limited number of discrete positions, the focus determining steps may be omitted and the image data transferred directly to the decode module <b>150</b> or autodiscrimination module <b>152</b>. Particularly when there are a limited number of alternate focus positions, the in-focus position can readily be discriminated based on which position the results in a successful decode. Discriminating an in-focus position by way of decode attempts may reduce average decode time.</p>
  <p num="p-0141">It is recognized that some available image sensor arrays have configurations or operation modes in which a limited number of edge columns/and or rows are not read out because of packaging concerns (e.g., edge pixels are covered by packaging material of the chip) or because of a configuration to a particular aspect ratio. Where image data from an image sensor is read out from all of the pixels of the image sensor or substantially all the pixels excluding a limited number of row and/or column edge pixels, such image data collecting is regarded herein as a collecting of a full frame of image data.</p>
  <p num="p-0142">With reference to process <b>400</b> and process <b>800</b>, it has been described that lens <b>212</b> can be controlled to be in one of continuous motion or stepwise continuous motion. It will be seen that when lens <b>212</b> is controlled to be in continuous motion, a focus setting of image reader <b>100</b> is controlled to vary over time. When lens <b>212</b> is controlled to be in stepwise continuous motion, a focus setting of lens <b>212</b> and, therefore, of image reader <b>100</b> is controlled to vary stepwise over time. Further, when lens <b>212</b> in accordance with process <b>400</b> or process <b>800</b> is in a motion period while being controlled to be in stepwise continuous motion, a focus setting of lens <b>212</b> is in a varying state. During a stop period of lens <b>212</b> while lens <b>212</b> is being controlled to be in stepwise continuous motion, a focus setting of image reader <b>100</b> is in a temporarily idle state.</p>
  <p num="p-0143">Referring again to <figref idrefs="DRAWINGS">FIG. 1A</figref>, the following description provides additional details on modules in the image reader <b>100</b> presented above. In various embodiments, the control module <b>112</b> can include a central processing unit including on-chip fast accessible memory, application specific integrated circuits (ASICs) for performing specialized operations, as well as software, firmware and digitally encoded logic. The memory module <b>116</b> can comprise any one or more of read-only (ROM), random access (RAM) and non-volatile programmable memory for data storage. The ROM-based memory can be used to accommodate security data and image reader operating system instructions and code for other modules. The RAM-based memory can be used to facilitate temporary data storage during image reader operation. Non-volatile programmable memory may take various forms, erasable programmable ROM (EPROM) and electrically erasable programmable ROM (EEPROM) being typical. In some embodiments, non-volatile memory is used to ensure that the data is retained when the image reader <b>100</b> is in its quiescent or power-saving “sleep” state.</p>
  <p num="p-0144">The I/O module <b>120</b> is used to establish potentially bi-directional communications between the image reader <b>100</b> and other electronic devices. Examples of elements that can comprise a portion of the I/O module <b>120</b> include a wireless or wired Ethernet interface, a dial-up or cable modem interface, a USB interface, a PCMCIA interface, a RS232 interface, an IBM Tailgate Interface RS485 interface, a PS/2 keyboard/mouse port, a specialized audio and/or video interface, a CompactFlash interface, a PC Card Standard interface, a Secure Digital standard for memory, a Secure Digital Input Output for input/output devices and/or any other standard or proprietary device interface. A CompactFlash interface is an interface designed in accordance with the CompactFlash standard as described in the CompactFlash Specification version 2.0 maintained at the website http://www.compactflash.org. The CompactFlash Specification version 2.0 document is herein incorporated by reference in its entirety. A PC Card Standard interface is an interface designed in accordance with the PC Card Standard as described by, for example, the PC Card Standard 8.0 Release—April 2001 maintained by the Personal Computer Memory Card International Association (PCMCIA) and available through the website at http://www.pcmcia.org. The PC Card Standard 8.0 Release—April 2001 Specification version 2.0 document is hereby herein incorporated by reference in its entirety.</p>
  <p num="p-0145">The actuation module <b>124</b> is used to initiate the operation of various aspects of the image reader <b>100</b> such as data collection and processing in accordance with process <b>300</b>, process <b>400</b>, process <b>600</b> or process <b>800</b> as described herein. All of the steps of process <b>300</b>, process <b>400</b>, process <b>600</b> and process <b>800</b> may be automatically executed in response to an initiation of the respective process by actuation module <b>124</b>. Image reader <b>100</b> may be configured so that the steps of process <b>300</b>, process <b>400</b>, process <b>600</b>, and process <b>800</b> continue automatically when initiated until a stop condition is satisfied. A stop condition may be e.g., the generation of a trigger stop signal (as may be generated by the release of trigger <b>216</b>) or the successful decoding a predetermined number of bar code symbols. In the hand held image reader <b>100</b> <i>a </i>discussed above, the actuation module comprises the trigger <b>216</b> which, when depressed, generates a trigger signal received by control module <b>112</b> which, in turn, sends control signals to appropriate other modules of image reader <b>100</b>. In one embodiment of a fixed mounted embodiment of the image reader <b>100</b>, the actuation module <b>124</b> comprises an object sensing module that generates a trigger signal to initiate the operation of the image reader <b>100</b> when the presence of an object to be imaged is detected. When a trigger signal is generated, image reader <b>100</b> is driven from an idle state to an active reading state. Actuation module <b>124</b> may also generate a trigger signal in response to receipt of a command from a local or remote spaced apart device.</p>
  <p num="p-0146">The user feedback module <b>128</b> is used to provide sensory feedback to an operator. In various embodiments, the feedback can include an auditory signal such as a beep alert, a visual display such as an LED flashing indicator, a mechanical sensation such as vibration in the image reader <b>100</b>, or any other sensory feedback capable of indicating to an operator the status of operation of the image reader <b>100</b> such as a successful image capture.</p>
  <p num="p-0147">The display module <b>132</b> is used to provide visual information to an operator such as the operational status of the image reader <b>100</b> including, for example, a remaining battery and/or memory capacity, a mode of operation, and/or other operational or functional details. In various embodiments, the display module <b>132</b> can be provided by a LCD flat panel display with an optional touch-pad screen overlay for receiving operator tactile input coordinated with the display.</p>
  <p num="p-0148">The user interface module <b>134</b> is used to provide an interface mechanism for communication between an operator and the image reader <b>100</b>. In various embodiments, the user interface module <b>134</b> comprises a keypad, function specific or programmable buttons, a joystick or toggle switch and the like. If the display module <b>132</b> includes a touch-pad screen overlay as mentioned above, the display module can incorporate some of the input functionality alternatively provided by elements in the user interface module <b>134</b>.</p>
  <p num="p-0149">In some embodiments, the RFID module <b>136</b> is an ISO/IEC 14443 compliant RFID interrogator and reader that can interrogate a RFID contactless device and that can recover the response that a RFID tag emits. The International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) are bodies that define the specialized system for worldwide standardization. In other embodiments, the RFID module <b>136</b> operates in accordance with ISO/IEC 10536 or ISO/IEC 15963. Contactless Card Standards promulgated by ISO/IEC cover a variety of types as embodied in ISO/IEC 10536 (Close coupled cards), ISO/IEC 14443 (Proximity cards), and ISO/IEC 15693 (Vicinity cards). These are intended for operation when very near, nearby and at a longer distance from associated coupling devices, respectively. In some embodiments, the RFID module <b>136</b> is configured to read tags that comprise information recorded in accordance with the Electronic Product Code (EPC), a code format proposed by the Auto-ID Center at MIT. In some embodiments, the RFID module <b>136</b> operates according to a proprietary protocol. In some embodiments, the RFID module <b>136</b> communicates at least a portion of the information received from an interrogated RFID tag to a computer processor that uses the information to access or retrieve data stored on a server accessible via the Internet. In some embodiments, the information is a serial number of the RFID tag or of the object associated with the RFID tag.</p>
  <p num="p-0150">In some embodiments, the smart card module <b>140</b> is an ISO/IEC 7816 compliant smart card reader with electrical contact for establishing communication with a suitably designed contact chip based smart card. The smart card module <b>140</b> is able to read and in some cases write data to attached smart cards.</p>
  <p num="p-0151">In some embodiments, the magnetic stripe card module <b>144</b> is a magnetic stripe reader capable of reading objects such as cards carrying information encoded in magnetic format on one or more tracks, for example, the tracks used on credit cards. In other embodiments, the magnetic stripe card module <b>144</b> is a magnetic character reading device, for reading characters printed using magnetic ink, such as is found on bank checks to indicate an American Bankers Association routing number, an account number, a check sequence number, and a draft amount. In some embodiments, both types of magnetic reading devices are provided.</p>
  <p num="p-0152">In some embodiments of the image reader <b>100</b>, the functionality of the RFID module <b>136</b>, the smart card module <b>140</b>, and the magnetic stripe card module <b>144</b> are combined in a single tribrid reader module such as the Panasonic's Integrated Smart Card Reader model number ZU-9A36CF4 available from the Matsushita Electrical Industrial Company, Ltd. The ZU-9A36CF4 is described in more detail in the Panasonic Specification number MIS-DG60C194 entitled, “Manual Insertion Type Integrated Smart Reader” dated March 2004 (revision 1.00). This document is hereby herein incorporated by reference in its entirety.</p>
  <p num="p-0153">The decoder module <b>150</b> is used to decode target data such as one and two-dimensional bar codes such as UPC/EAN, Code 11, Code 39, Code 128, Codabar, Interleaved 2 of 5, MSI, PDF417, MicroPDF417, Code 16K, Code 49, MaxiCode, Aztec, Aztec Mesa, Data Matrix, Qcode, QR Code, UCC Composite, Snowflake, Vericode, Dataglyphs, RSS, BC 412, Code 93, Codablock, Postnet (US), BPO4 State, Canadian 4 State, Japanese Post, KIX (Dutch Post), Planet Code, OCR A, OCR B, and the like. In some embodiments, the decoder module also includes autodiscrimination functionality that allows it to automatically discriminate between a plurality of bar code such as those listed above. Certain functionality of the decoder <b>150</b>, such as the measurement of characteristics of decodable indicia, is described in the related U.S. applications Ser. No. 10/982,393 (now U.S. Pat. No. 7,219,841), filed Nov. 5, 2004, entitled “Device and System for Verifying Quality of Bar Codes.” This application is hereby herein incorporated by reference in its entirety.</p>
  <p num="p-0154">Another example of an image reader <b>100</b> constructed in accordance with the principles of the invention is the portable data terminal <b>100</b> <i>b </i>shown in different perspective drawings in <figref idrefs="DRAWINGS">FIGS. 15A</figref>, <b>15</b>B, and <b>15</b>C. <figref idrefs="DRAWINGS">FIG. 15A</figref> shows a top perspective, <figref idrefs="DRAWINGS">FIG. 15B</figref> shows a front perspective view, and <figref idrefs="DRAWINGS">FIG. 15C</figref> shows a back perspective view. As shown, the portable data terminal <b>100</b> <i>b </i>in one embodiment includes interface elements including a display <b>504</b>, a keyboard <b>508</b>, interface buttons <b>512</b> for example for positioning a cursor, a trigger <b>216</b>, and a stylus <b>520</b> with a stylus holder <b>524</b> (not shown). The portable data terminal <b>100</b> <i>b </i>further includes a lens <b>212</b> <i>b </i>and light sources <b>160</b> <i>b</i>. In additional embodiments, the portable data terminal can have its functionality enhanced with the addition of multiple detachable computer peripherals. In various embodiments, the computer peripherals can include one or more of a magnetic stripe reader, a biometric reader such as a finger print scanner, a printer such as a receipt printer, a RFID tag or RF payment reader, a smart card reader, and the like. In various embodiments, the portable data terminal <b>100</b> <i>b </i>can be a Dolphin 7200, 7300, 7400, 7900, or 9500 Series Mobile Computer available from Hand Held Products, Inc., of 700 Visions Drive, P.O. Box 208, Skaneateles Falls, N.Y. and constructed in accordance with the invention. Various details of a hand held computer device, in particular the device's housing, are described in more detail in the related U.S. application Ser. No. 10/938,416, filed Sep. 10, 2004, entitled “Hand Held Computer Device.” This application is hereby herein incorporated by reference in its entirety.</p>
  <p num="p-0155">The portable data terminal <b>100</b> <i>b </i>further includes an electro-mechanical interface <b>532</b> such as a dial-up or cable modem interface, a USB interface, a PCMCIA interface, an Ethernet interface, a RS232 interface, an IBM Tailgate Interface RS485 interface, a CompactFlash interface, a PC Card Standard interface, a Secure Digital standard for memory interface, a Secure Digital Input Output for input/output devices interface and/or any other appropriate standard or proprietary device interface. In various embodiments, the electro-mechanical interface <b>532</b> can be used as part of attaching computer peripherals.</p>
  <p num="p-0156">An electrical block diagram of one embodiment of the portable data terminal <b>100</b> <i>b </i>is shown in <figref idrefs="DRAWINGS">FIG. 16</figref>. In the embodiment of <figref idrefs="DRAWINGS">FIG. 16</figref>, an image collection module <b>108</b> <i>b </i>includes an image engine including two-dimensional image sensor <b>536</b> provided on image sensor chip <b>546</b> and associated imaging optics <b>544</b>. The associated imaging optics <b>544</b> includes the lens <b>212</b> <i>b </i>(not shown). Image sensor chip <b>546</b> may be provided in an IT4000 or IT4200 image engine of the type available from Hand Held Products, Inc. of Skaneateles Falls, N.Y. constructed in accordance with the invention and may be a suitable commercially available chip such as the Kodak KAC-0311 or the Micron MT9V022 image sensor array described above. The portable data terminal <b>100</b> <i>b </i>also includes an illumination module <b>104</b> <i>b </i>including the light sources <b>160</b> <i>b </i>and an illumination control module <b>164</b> <i>b</i>. These illumination modules are also an integral part of the IT4000 and IT4200 image engines referenced above. The portable data terminal <b>100</b> <i>b </i>further includes a processor integrated circuit (<b>1</b>C) chip <b>548</b> such as may be provided by, for example, an INTEL Strong ARM RISC processor or an INTEL PXA255 Processor. Processor IC chip <b>548</b> includes a central processing unit (CPU) <b>552</b>. For capturing images, the processor IC chip <b>548</b> sends appropriate control and timing signals to image sensor chip <b>546</b>, as described above. The processor IC chip <b>548</b> further manages the transfer of image data generated by the chip <b>546</b> into RAM <b>576</b>. Processor IC chip <b>548</b> may be configured to partially or entirely carry out the functions of one or more of the modules, e.g., modules <b>104</b>, <b>108</b>, <b>112</b>, <b>116</b>, <b>120</b>, <b>124</b>, <b>128</b>, <b>132</b>, <b>134</b>, <b>136</b>, <b>140</b>, <b>144</b>, <b>150</b>, <b>152</b>, <b>165</b>, <b>168</b> as described in connection with <figref idrefs="DRAWINGS">FIG. 1A</figref>.</p>
  <p num="p-0157">As indicated above, the portable data terminal <b>100</b> <i>b </i>may include a display <b>504</b>, such as a liquid crystal display, a keyboard <b>508</b>, a plurality of communication or radio transceivers such as a 802.11 radio communication link <b>556</b>, a Global System for Mobile Communications/General Packet Radio Service (GSM/GPRS) radio communication link <b>560</b>, and/or a Bluetooth radio communication link <b>564</b>. In additional embodiments, the portable data terminal <b>100</b> <i>b </i>may also have the capacity to transmit information such as voice or data communications via Code Division Multiple Access (CDMA), Cellular Digital Packet Data (CDPD), Mobitex cellular phone and data networks and network components. In other embodiments, the portable data terminal <b>100</b> <i>b </i>can transmit information using a DataTAC™ network or a wireless dial-up connection.</p>
  <p num="p-0158">The portable data terminal <b>100</b> <i>b </i>may further include an infrared (IR) communication link <b>568</b>. The keyboard <b>508</b> may communicate with IC chip <b>548</b> via microcontroller chip <b>572</b>. The portable data terminal I <b>10</b> <i>b </i>may further include RFID circuitry <b>578</b> as described above for reading or writing data to a RFID tag or token and smart card circuitry <b>586</b> including electrical contacts <b>590</b> for establishing electrical communication with a smart card such as a circuitry enabled credit card. The portable data terminal <b>100</b> <i>b </i>further includes a memory <b>574</b> including a volatile memory and a non-volatile memory. The volatile memory in one embodiment is provided in part by the RAM <b>576</b>. The non-volatile memory may be provided in part by flash ROM <b>580</b>. Processor IC chip <b>548</b> is in communication with the RAM <b>576</b> and ROM <b>580</b> via a system bus <b>584</b>. Processor IC chip <b>548</b> and microcontroller chip <b>572</b> also include areas of volatile and non-volatile memory. In various embodiments where at least some of the modules discussed above, such as the elements in the control module <b>112</b>, are implemented at least in part in software, the software components can be stored in the non-volatile memories such as the ROM <b>580</b>. In one embodiment, the processor IC chip <b>548</b> includes a control circuit that itself employs the CPU <b>552</b> and memory <b>574</b>. Non-volatile areas of the memory <b>574</b> can be used, for example, to store program operating instructions.</p>
  <p num="p-0159">In various embodiments, the processor IC chip <b>548</b> may include a number of I/O interfaces (not all shown in <figref idrefs="DRAWINGS">FIG. 16</figref>) including several serial interfaces (e.g., general purpose, Ethernet, blue tooth), and parallel interfaces (e.g., PCMCIA, Compact Flash).</p>
  <p num="p-0160">In one embodiment, the processor IC chip <b>548</b> processes frames of image data to, for example, decode a one or two-dimensional bar code or a set of OCR characters. Various bar code and/or OCR decoding algorithms are commercially available, such as by the incorporation of an IT4250 image engine with decoder board, available from Hand Held Products, Inc. In one embodiment, the decoder board decodes symbologies such as UPC/EAN, Code 11, Code 39, Code 128, Codabar, Interleaved 2 of 5, MSI, PDF417, MicroPDF417, Code 16K, Code 49, MaxiCode, Aztec, Aztec Mesa, Data Matrix, Qcode, QR Code, UCC Composite, Snowflake, Vericode, Dataglyphs, RSS, BC 412, Code 93, Codablock, Postnet (US), BPO4 State, Canadian 4 State, Japanese Post, KIX (Dutch Post), Planet Code, OCR A, OCR B, and the like.</p>
  <p num="p-0161">Among other operations, the infrared transceiver <b>568</b> facilitates infrared copying of data from a portable data terminal <b>100</b> <i>b </i>in a broadcasting mode to a portable data terminal <b>100</b> <i>b </i>in a receiving mode. Utilization of infrared transceiver <b>568</b> during a data copying session allows data broadcast from a single broadcast device to be simultaneously received by several receiving devices without any of the receiving devices being physically connected to the broadcasting device.</p>
  <p num="p-0162">In an additional further embodiment, the image reader <b>100</b> can be contained in a transaction terminal such as the Transaction Terminal Image Kiosk 8870 available from Hand Held Products, Inc., of 700 Visions Drive, P.O. Box 208, Skaneateles Falls, N.Y. and constructed in accordance with the invention. In a further embodiment, the image reader <b>100</b> can be contained in a fixed mount system such as the IMAGETEAM 3800E linear image engine or the IMAGETEAM 4710 two-dimensional reader available from Hand Held Products, Inc. of 700 Visions Drive, P.O. Box 208, Skaneateles Falls, N.Y.</p>
  <p num="p-0163">In various embodiments, the modules discussed above including the illumination module <b>104</b>, the image collection module <b>108</b>, the control module <b>112</b>, the memory module <b>116</b>, the I/O module <b>120</b>, the actuation module <b>124</b>, the user feedback module <b>128</b>, the display module <b>132</b>, the user interface module <b>134</b>, the RFID module <b>136</b>, the smart card module <b>140</b>, the magnetic stripe card module <b>144</b>, the decoder module <b>150</b>, the illumination control module <b>164</b>, the power module <b>168</b>, the interface module <b>172</b>, the optics module <b>178</b>, the sensor array module <b>182</b>, the sensor array control module <b>186</b>, the global electronic shutter control module <b>190</b>, the row and column address and decode module <b>194</b>, and the read out module <b>198</b>, the rolling shutter control module <b>202</b>, and the auto-focusing module can be implemented in different combinations of software, firmware, and/or hardware.</p>
  <p num="p-0164">Machine readable storage media that can be used in the invention include electronic, magnetic and/or optical storage media, such as magnetic floppy disks and hard disks, a DVD drive, a CD drive that in some embodiments can employ DVD disks, any of CD-ROM disks (i.e., read-only optical storage disks), CD-R disks (i.e., write-once, read-many optical storage disks), and CD-RW disks (i.e., rewriteable optical storage disks); and electronic storage media, such as RAM, ROM, EPROM, Compact Flash cards, PCMCIA cards, or alternatively SD or SDI<b>0</b> memory; and the electronic components (e.g., floppy disk drive, DVD drive, CD/CD-R/CD-RW drive, or Compact Flash/PCMCIA/SD adapter) that accommodate and read from and/or write to the storage media. As is known to those of skill in the machine-readable storage media arts, new media and formats for data storage are continually being devised, and any convenient, commercially available storage medium and corresponding read/write device that may become available in the future is likely to be appropriate for use, especially if it provides any of a greater storage capacity, a higher access speed, a smaller size, and a lower cost per bit of stored information. Well known older machine-readable media are also available for use under certain conditions, such as punched paper tape or cards, magnetic recording on tape or wire, optical or magnetic reading of printed characters (e.g., OCR and magnetically encoded symbols) and machine-readable symbols such as one and two-dimensional bar codes.</p>
  <p num="p-0165">Those of ordinary skill will recognize that many functions of electrical and electronic apparatus can be implemented in hardware (for example, hard-wired logic), in software (for example, logic encoded in a program operating on a general purpose processor), and in firmware (for example, logic encoded in a non-volatile memory that is invoked for operation on a processor as required). The present invention contemplates the substitution of one implementation of hardware, firmware and software for another implementation of the equivalent functionality using a different one of hardware, firmware and software. To the extent that an implementation can be represented mathematically by a transfer function, that is, a specified response is generated at an output terminal for a specific excitation applied to an input terminal of a “black box” exhibiting the transfer function, any implementation of the transfer function, including any combination of hardware, firmware and software implementations of portions or segments of the transfer function, is contemplated herein.</p>
  <p num="p-0166">While the present invention has been explained with reference to the structure disclosed herein, it is not confined to the details set forth and this invention is intended to cover any modifications and changes as may come within the scope and spirit of the following claims.</p>
</div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3684868">US3684868</a></td><td class="patent-data-table-td patent-date-value">Oct 29, 1970</td><td class="patent-data-table-td patent-date-value">Aug 15, 1972</td><td class="patent-data-table-td ">Ncr Co</td><td class="patent-data-table-td ">Color bar code tag reader with light-emitting diodes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3716699">US3716699</a></td><td class="patent-data-table-td patent-date-value">Apr 2, 1971</td><td class="patent-data-table-td patent-date-value">Feb 13, 1973</td><td class="patent-data-table-td ">A Eckert</td><td class="patent-data-table-td ">Method and apparatus for optical code reading</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4253447">US4253447</a></td><td class="patent-data-table-td patent-date-value">Oct 16, 1978</td><td class="patent-data-table-td patent-date-value">Mar 3, 1981</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Color endoscope with charge coupled device and television viewing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4261344">US4261344</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 1979</td><td class="patent-data-table-td patent-date-value">Apr 14, 1981</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Color endoscope</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4350418">US4350418</a></td><td class="patent-data-table-td patent-date-value">Sep 11, 1979</td><td class="patent-data-table-td patent-date-value">Sep 21, 1982</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Camera provided with automatic focus adjusting device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4491865">US4491865</a></td><td class="patent-data-table-td patent-date-value">Sep 29, 1982</td><td class="patent-data-table-td patent-date-value">Jan 1, 1985</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">For use in the viewing head of an endoscope</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4516017">US4516017</a></td><td class="patent-data-table-td patent-date-value">Jan 14, 1983</td><td class="patent-data-table-td patent-date-value">May 7, 1985</td><td class="patent-data-table-td ">Nippondenso Co., Ltd.</td><td class="patent-data-table-td ">High-sensitive optical reading apparatus and method of reading optical information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4523224">US4523224</a></td><td class="patent-data-table-td patent-date-value">Dec 27, 1982</td><td class="patent-data-table-td patent-date-value">Jun 11, 1985</td><td class="patent-data-table-td ">Welch Allyn Inc.</td><td class="patent-data-table-td ">Color filter wheel synchronizer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4546379">US4546379</a></td><td class="patent-data-table-td patent-date-value">Apr 21, 1983</td><td class="patent-data-table-td patent-date-value">Oct 8, 1985</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Independent color adjustment for a video system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4793689">US4793689</a></td><td class="patent-data-table-td patent-date-value">Dec 23, 1986</td><td class="patent-data-table-td patent-date-value">Dec 27, 1988</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Lens barrel with vibration wave motor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4806776">US4806776</a></td><td class="patent-data-table-td patent-date-value">Aug 5, 1985</td><td class="patent-data-table-td patent-date-value">Feb 21, 1989</td><td class="patent-data-table-td ">Kley Victor B</td><td class="patent-data-table-td ">Electrical illumination and detecting apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4853774">US4853774</a></td><td class="patent-data-table-td patent-date-value">Oct 28, 1988</td><td class="patent-data-table-td patent-date-value">Aug 1, 1989</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Auxiliary light apparatus for borescope</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4854302">US4854302</a></td><td class="patent-data-table-td patent-date-value">Nov 12, 1987</td><td class="patent-data-table-td patent-date-value">Aug 8, 1989</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Video equipped endoscope with needle probe</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4862253">US4862253</a></td><td class="patent-data-table-td patent-date-value">Jul 20, 1988</td><td class="patent-data-table-td patent-date-value">Aug 29, 1989</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Apparatus for converting a video processor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4877949">US4877949</a></td><td class="patent-data-table-td patent-date-value">Aug 8, 1986</td><td class="patent-data-table-td patent-date-value">Oct 31, 1989</td><td class="patent-data-table-td ">Norand Corporation</td><td class="patent-data-table-td ">Hand-held instant bar code reader system with automated focus based on distance measurements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4941456">US4941456</a></td><td class="patent-data-table-td patent-date-value">Oct 5, 1989</td><td class="patent-data-table-td patent-date-value">Jul 17, 1990</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Portable color imager borescope</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4957346">US4957346</a></td><td class="patent-data-table-td patent-date-value">Oct 6, 1989</td><td class="patent-data-table-td patent-date-value">Sep 18, 1990</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Illumination system for portable color imager borescope</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4963756">US4963756</a></td><td class="patent-data-table-td patent-date-value">Oct 13, 1989</td><td class="patent-data-table-td patent-date-value">Oct 16, 1990</td><td class="patent-data-table-td ">Hewlett-Packard Company</td><td class="patent-data-table-td ">Focused line identifier for a bar code reader</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5019699">US5019699</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 1988</td><td class="patent-data-table-td patent-date-value">May 28, 1991</td><td class="patent-data-table-td ">Norand Corporation</td><td class="patent-data-table-td ">Hand-held optical character reader with means for instantaneously reading information from a predetermined area at an optical sensing area</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5059146">US5059146</a></td><td class="patent-data-table-td patent-date-value">Feb 22, 1990</td><td class="patent-data-table-td patent-date-value">Oct 22, 1991</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Method of adjusting a light source for color temperature and chromaticity</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5144190">US5144190</a></td><td class="patent-data-table-td patent-date-value">May 10, 1991</td><td class="patent-data-table-td patent-date-value">Sep 1, 1992</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Light source having desired color temperature and chromaticity</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5222477">US5222477</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 1991</td><td class="patent-data-table-td patent-date-value">Jun 29, 1993</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Endoscope or borescope stereo viewing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5278642">US5278642</a></td><td class="patent-data-table-td patent-date-value">Feb 26, 1992</td><td class="patent-data-table-td patent-date-value">Jan 11, 1994</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Color imaging system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5291008">US5291008</a></td><td class="patent-data-table-td patent-date-value">Jan 10, 1992</td><td class="patent-data-table-td patent-date-value">Mar 1, 1994</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Optical assembly and apparatus employing same using an aspherical lens and an aperture stop</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5305122">US5305122</a></td><td class="patent-data-table-td patent-date-value">Dec 16, 1991</td><td class="patent-data-table-td patent-date-value">Apr 19, 1994</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Image reading and processing apparatus suitable for use as a color hand-held scanner</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5308962">US5308962</a></td><td class="patent-data-table-td patent-date-value">Jul 28, 1993</td><td class="patent-data-table-td patent-date-value">May 3, 1994</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Reduced power scanner for reading indicia</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5337361">US5337361</a></td><td class="patent-data-table-td patent-date-value">Jun 1, 1992</td><td class="patent-data-table-td patent-date-value">Aug 9, 1994</td><td class="patent-data-table-td ">Symbol Technologies, Inc.</td><td class="patent-data-table-td ">Record with encoded data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5393965">US5393965</a></td><td class="patent-data-table-td patent-date-value">Aug 3, 1992</td><td class="patent-data-table-td patent-date-value">Feb 28, 1995</td><td class="patent-data-table-td ">Symbol Technologies, Inc.</td><td class="patent-data-table-td ">Flexible merchandise checkout and inventory management system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5399846">US5399846</a></td><td class="patent-data-table-td patent-date-value">Sep 27, 1993</td><td class="patent-data-table-td patent-date-value">Mar 21, 1995</td><td class="patent-data-table-td ">Symbol Technologies, Inc.</td><td class="patent-data-table-td ">Systems utilizing a high density two dimensional bar code symbology</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5401944">US5401944</a></td><td class="patent-data-table-td patent-date-value">Aug 3, 1992</td><td class="patent-data-table-td patent-date-value">Mar 28, 1995</td><td class="patent-data-table-td ">Symbol Technologies, Inc.</td><td class="patent-data-table-td ">Traveler security and luggage control system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5406062">US5406062</a></td><td class="patent-data-table-td patent-date-value">Jul 19, 1993</td><td class="patent-data-table-td patent-date-value">Apr 11, 1995</td><td class="patent-data-table-td ">Alps Electric Co., Ltd.</td><td class="patent-data-table-td ">Sensitivity adjustment circuit for bar code scanner and method therefor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5410141">US5410141</a></td><td class="patent-data-table-td patent-date-value">Jun 7, 1990</td><td class="patent-data-table-td patent-date-value">Apr 25, 1995</td><td class="patent-data-table-td ">Norand</td><td class="patent-data-table-td ">Hand-held data capture system with interchangable modules</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5471533">US5471533</a></td><td class="patent-data-table-td patent-date-value">May 20, 1994</td><td class="patent-data-table-td patent-date-value">Nov 28, 1995</td><td class="patent-data-table-td ">Symbol Technologies, Inc.</td><td class="patent-data-table-td ">Record with encoded data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5504322">US5504322</a></td><td class="patent-data-table-td patent-date-value">Oct 26, 1994</td><td class="patent-data-table-td patent-date-value">Apr 2, 1996</td><td class="patent-data-table-td ">Symbol Technologies, Inc.</td><td class="patent-data-table-td ">High density two dimensional bar code symbology</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5513264">US5513264</a></td><td class="patent-data-table-td patent-date-value">Apr 5, 1994</td><td class="patent-data-table-td patent-date-value">Apr 30, 1996</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Visually interactive encoding and decoding of dataforms</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5521366">US5521366</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 1994</td><td class="patent-data-table-td patent-date-value">May 28, 1996</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Dataform readers having controlled and overlapped exposure integration periods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5527262">US5527262</a></td><td class="patent-data-table-td patent-date-value">Sep 22, 1995</td><td class="patent-data-table-td patent-date-value">Jun 18, 1996</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Hand-held diagnostic dental probe with video imaging</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5541777">US5541777</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 1994</td><td class="patent-data-table-td patent-date-value">Jul 30, 1996</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Electromagnetic driving device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5572006">US5572006</a></td><td class="patent-data-table-td patent-date-value">Jul 26, 1994</td><td class="patent-data-table-td patent-date-value">Nov 5, 1996</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Automatic exposure single frame imaging systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5576529">US5576529</a></td><td class="patent-data-table-td patent-date-value">Sep 19, 1994</td><td class="patent-data-table-td patent-date-value">Nov 19, 1996</td><td class="patent-data-table-td ">Norand Technology Corporation</td><td class="patent-data-table-td ">Hand-held optically readable information set reader focus with operation over a range of distances</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5591955">US5591955</a></td><td class="patent-data-table-td patent-date-value">Nov 16, 1994</td><td class="patent-data-table-td patent-date-value">Jan 7, 1997</td><td class="patent-data-table-td ">Laser; Vadim</td><td class="patent-data-table-td ">For receiving an image of an object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5602377">US5602377</a></td><td class="patent-data-table-td patent-date-value">Mar 1, 1995</td><td class="patent-data-table-td patent-date-value">Feb 11, 1997</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Bar code dataform scanning and labeling apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5637849">US5637849</a></td><td class="patent-data-table-td patent-date-value">May 31, 1995</td><td class="patent-data-table-td patent-date-value">Jun 10, 1997</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Maxicode data extraction using spatial domain features</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5646390">US5646390</a></td><td class="patent-data-table-td patent-date-value">Mar 25, 1996</td><td class="patent-data-table-td patent-date-value">Jul 8, 1997</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Dataform readers and methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5654533">US5654533</a></td><td class="patent-data-table-td patent-date-value">Jul 17, 1995</td><td class="patent-data-table-td patent-date-value">Aug 5, 1997</td><td class="patent-data-table-td ">Kabushiki Kaisha Tec</td><td class="patent-data-table-td ">Apparatus and method for reading two-dimensional symbols</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5662586">US5662586</a></td><td class="patent-data-table-td patent-date-value">Apr 18, 1996</td><td class="patent-data-table-td patent-date-value">Sep 2, 1997</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Hand held diagnostic instrument with video imaging</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5691773">US5691773</a></td><td class="patent-data-table-td patent-date-value">Sep 12, 1995</td><td class="patent-data-table-td patent-date-value">Nov 25, 1997</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Anti-hand-jittering dataform readers and methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5702058">US5702058</a></td><td class="patent-data-table-td patent-date-value">May 20, 1996</td><td class="patent-data-table-td patent-date-value">Dec 30, 1997</td><td class="patent-data-table-td ">Calmar Inc.</td><td class="patent-data-table-td ">Dual foamer nozzle assembly for trigger sprayer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5702059">US5702059</a></td><td class="patent-data-table-td patent-date-value">Oct 18, 1995</td><td class="patent-data-table-td patent-date-value">Dec 30, 1997</td><td class="patent-data-table-td ">Meta Holding Corp.</td><td class="patent-data-table-td ">Extended working range dataform reader including fuzzy logic image control circuitry</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5703349">US5703349</a></td><td class="patent-data-table-td patent-date-value">Dec 20, 1995</td><td class="patent-data-table-td patent-date-value">Dec 30, 1997</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Portable data collection device with two dimensional imaging assembly</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5714745">US5714745</a></td><td class="patent-data-table-td patent-date-value">Mar 1, 1996</td><td class="patent-data-table-td patent-date-value">Feb 3, 1998</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Portable data collection device with color imaging assembly</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5717195">US5717195</a></td><td class="patent-data-table-td patent-date-value">Mar 29, 1996</td><td class="patent-data-table-td patent-date-value">Feb 10, 1998</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Imaging based slot dataform reader</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5739518">US5739518</a></td><td class="patent-data-table-td patent-date-value">May 17, 1995</td><td class="patent-data-table-td patent-date-value">Apr 14, 1998</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Autodiscrimination for dataform decoding and standardized recording</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5756981">US5756981</a></td><td class="patent-data-table-td patent-date-value">Aug 1, 1996</td><td class="patent-data-table-td patent-date-value">May 26, 1998</td><td class="patent-data-table-td ">Symbol Technologies, Inc.</td><td class="patent-data-table-td ">Optical scanner for reading and decoding one- and-two-dimensional symbologies at variable depths of field including memory efficient high speed image processing means and high accuracy image analysis means</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5763864">US5763864</a></td><td class="patent-data-table-td patent-date-value">Oct 23, 1995</td><td class="patent-data-table-td patent-date-value">Jun 9, 1998</td><td class="patent-data-table-td ">Meta Holding Corporation</td><td class="patent-data-table-td ">For reading a target dataform in a target area</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5763866">US5763866</a></td><td class="patent-data-table-td patent-date-value">May 14, 1996</td><td class="patent-data-table-td patent-date-value">Jun 9, 1998</td><td class="patent-data-table-td ">Asahi Kogaku Kogyo Kabushiki Kaisha</td><td class="patent-data-table-td ">Optical reader for information pattern representing coded data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5770847">US5770847</a></td><td class="patent-data-table-td patent-date-value">Dec 23, 1994</td><td class="patent-data-table-td patent-date-value">Jun 23, 1998</td><td class="patent-data-table-td ">Spectra-Physics Scanning Systems, Inc.</td><td class="patent-data-table-td ">Bar code reader with multi-focus lens</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5773810">US5773810</a></td><td class="patent-data-table-td patent-date-value">Mar 29, 1996</td><td class="patent-data-table-td patent-date-value">Jun 30, 1998</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Method for generating real time degree of focus signal for handheld imaging device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5783811">US5783811</a></td><td class="patent-data-table-td patent-date-value">Feb 26, 1996</td><td class="patent-data-table-td patent-date-value">Jul 21, 1998</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Portable data collection device with LED targeting and illumination assembly</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5784102">US5784102</a></td><td class="patent-data-table-td patent-date-value">Feb 27, 1997</td><td class="patent-data-table-td patent-date-value">Jul 21, 1998</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">For reading data from an indicia of the two dimensional type</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5786582">US5786582</a></td><td class="patent-data-table-td patent-date-value">Dec 8, 1995</td><td class="patent-data-table-td patent-date-value">Jul 28, 1998</td><td class="patent-data-table-td ">Symbol Technologies, Inc.</td><td class="patent-data-table-td ">Optical scanner for reading and decoding one- and two-dimensional symbologies at variable depths of field</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5786586">US5786586</a></td><td class="patent-data-table-td patent-date-value">Oct 28, 1996</td><td class="patent-data-table-td patent-date-value">Jul 28, 1998</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Hand-held optical reader having a detachable lens-guide assembly</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5793033">US5793033</a></td><td class="patent-data-table-td patent-date-value">Mar 29, 1996</td><td class="patent-data-table-td patent-date-value">Aug 11, 1998</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">For reading a dataform positioned in a target area</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5811774">US5811774</a></td><td class="patent-data-table-td patent-date-value">Aug 15, 1996</td><td class="patent-data-table-td patent-date-value">Sep 22, 1998</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Extended working range dataform reader with reduced power consumption</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5811784">US5811784</a></td><td class="patent-data-table-td patent-date-value">Jun 26, 1995</td><td class="patent-data-table-td patent-date-value">Sep 22, 1998</td><td class="patent-data-table-td ">Telxon Corporation</td><td class="patent-data-table-td ">For reading a dataform positioned in a target area</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5811828">US5811828</a></td><td class="patent-data-table-td patent-date-value">Jun 5, 1997</td><td class="patent-data-table-td patent-date-value">Sep 22, 1998</td><td class="patent-data-table-td ">Norand Corporation</td><td class="patent-data-table-td ">Portable reader system having an adjustable optical focusing means for reading optical information over a substantial range of distances</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5814801">US5814801</a></td><td class="patent-data-table-td patent-date-value">Nov 1, 1996</td><td class="patent-data-table-td patent-date-value">Sep 29, 1998</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Maxicode data extraction using spatial domain features exclusive of fourier type domain transfer processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5815200">US5815200</a></td><td class="patent-data-table-td patent-date-value">Jul 25, 1995</td><td class="patent-data-table-td patent-date-value">Sep 29, 1998</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Extended working range dataform reader with reduced power consumption</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5818023">US5818023</a></td><td class="patent-data-table-td patent-date-value">Mar 5, 1996</td><td class="patent-data-table-td patent-date-value">Oct 6, 1998</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Portable ID card verification apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5818028">US5818028</a></td><td class="patent-data-table-td patent-date-value">Jan 23, 1997</td><td class="patent-data-table-td patent-date-value">Oct 6, 1998</td><td class="patent-data-table-td ">Telxon Corporation</td><td class="patent-data-table-td ">Portable data collection device with two dimensional imaging assembly</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5821518">US5821518</a></td><td class="patent-data-table-td patent-date-value">Sep 23, 1996</td><td class="patent-data-table-td patent-date-value">Oct 13, 1998</td><td class="patent-data-table-td ">United Parcel Service Of America, Inc.</td><td class="patent-data-table-td ">Method and apparatus for a portable non-contact label imager</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5831254">US5831254</a></td><td class="patent-data-table-td patent-date-value">Dec 18, 1995</td><td class="patent-data-table-td patent-date-value">Nov 3, 1998</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Apparatus for optically scanning encoded data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5834754">US5834754</a></td><td class="patent-data-table-td patent-date-value">Dec 12, 1996</td><td class="patent-data-table-td patent-date-value">Nov 10, 1998</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Imaging assembly</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5837987">US5837987</a></td><td class="patent-data-table-td patent-date-value">Nov 19, 1996</td><td class="patent-data-table-td patent-date-value">Nov 17, 1998</td><td class="patent-data-table-td ">Norand Technology Corporation</td><td class="patent-data-table-td ">Hand-held optically readable character set reader having automatic focus control for operating over a range of distances</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5841121">US5841121</a></td><td class="patent-data-table-td patent-date-value">Nov 19, 1996</td><td class="patent-data-table-td patent-date-value">Nov 24, 1998</td><td class="patent-data-table-td ">Norand Technology Corporation</td><td class="patent-data-table-td ">Hand-held optically readable character set reader having automatic focus control for operation over a range of distances</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5877487">US5877487</a></td><td class="patent-data-table-td patent-date-value">Jun 19, 1996</td><td class="patent-data-table-td patent-date-value">Mar 2, 1999</td><td class="patent-data-table-td ">Asahi Kogaku Kogyo Kabushiki Kaisha</td><td class="patent-data-table-td ">Data symbol reading device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5914476">US5914476</a></td><td class="patent-data-table-td patent-date-value">Nov 4, 1997</td><td class="patent-data-table-td patent-date-value">Jun 22, 1999</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Optical reader configured to accurately and rapidly read multiple symbols</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5917913">US5917913</a></td><td class="patent-data-table-td patent-date-value">Dec 4, 1996</td><td class="patent-data-table-td patent-date-value">Jun 29, 1999</td><td class="patent-data-table-td ">Wang; Ynjiun Paul</td><td class="patent-data-table-td ">Portable electronic authorization devices and methods therefor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5940163">US5940163</a></td><td class="patent-data-table-td patent-date-value">Apr 7, 1997</td><td class="patent-data-table-td patent-date-value">Aug 17, 1999</td><td class="patent-data-table-td ">Electro Plasma Inc.</td><td class="patent-data-table-td ">Photon coupled color flat panel display and method of manufacture</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5949052">US5949052</a></td><td class="patent-data-table-td patent-date-value">Oct 17, 1997</td><td class="patent-data-table-td patent-date-value">Sep 7, 1999</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Object sensor system for stationary position optical reader</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5979763">US5979763</a></td><td class="patent-data-table-td patent-date-value">Oct 13, 1995</td><td class="patent-data-table-td patent-date-value">Nov 9, 1999</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Sub-pixel dataform reader with dynamic noise margins</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5986297">US5986297</a></td><td class="patent-data-table-td patent-date-value">Feb 13, 1997</td><td class="patent-data-table-td patent-date-value">Nov 16, 1999</td><td class="patent-data-table-td ">Eastman Kodak Company</td><td class="patent-data-table-td ">Color active pixel sensor with electronic shuttering, anti-blooming and low cross-talk</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6010070">US6010070</a></td><td class="patent-data-table-td patent-date-value">Oct 9, 1997</td><td class="patent-data-table-td patent-date-value">Jan 4, 2000</td><td class="patent-data-table-td ">Nippon Electric Industry Co., Ltd.</td><td class="patent-data-table-td ">Code reading device and method with variable light signal storage time</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6010073">US6010073</a></td><td class="patent-data-table-td patent-date-value">Jul 31, 1997</td><td class="patent-data-table-td patent-date-value">Jan 4, 2000</td><td class="patent-data-table-td ">Datalogic S.P.A.</td><td class="patent-data-table-td ">Portable apparatus for reading an optical code</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6019286">US6019286</a></td><td class="patent-data-table-td patent-date-value">Feb 22, 1996</td><td class="patent-data-table-td patent-date-value">Feb 1, 2000</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Portable data collection device with dataform decoding and image capture capability</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6042012">US6042012</a></td><td class="patent-data-table-td patent-date-value">Sep 28, 1998</td><td class="patent-data-table-td patent-date-value">Mar 28, 2000</td><td class="patent-data-table-td ">Spectra-Physics Scanning Systems, Inc.</td><td class="patent-data-table-td ">Method and apparatus for reading images without need for self-generated illumination source</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6045047">US6045047</a></td><td class="patent-data-table-td patent-date-value">Jun 19, 1998</td><td class="patent-data-table-td patent-date-value">Apr 4, 2000</td><td class="patent-data-table-td ">Welch Allyn Data Collection, Inc.</td><td class="patent-data-table-td ">Two-dimensional part reader having a focussing guide</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6045238">US6045238</a></td><td class="patent-data-table-td patent-date-value">Oct 9, 1998</td><td class="patent-data-table-td patent-date-value">Apr 4, 2000</td><td class="patent-data-table-td ">Welch Allyn Inc.</td><td class="patent-data-table-td ">Illumination assembly for an optical viewing device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6053407">US6053407</a></td><td class="patent-data-table-td patent-date-value">Nov 1, 1996</td><td class="patent-data-table-td patent-date-value">Apr 25, 2000</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Maxicode data extraction using spatial domain features</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6060722">US6060722</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 1997</td><td class="patent-data-table-td patent-date-value">May 9, 2000</td><td class="patent-data-table-td ">Havens; William H.</td><td class="patent-data-table-td ">Optical reader having illumination assembly including improved aiming pattern generator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6062475">US6062475</a></td><td class="patent-data-table-td patent-date-value">Jun 25, 1997</td><td class="patent-data-table-td patent-date-value">May 16, 2000</td><td class="patent-data-table-td ">Metanetics Corporation</td><td class="patent-data-table-td ">Portable data collection device including color imaging dataform reader assembly</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6073851">US6073851</a></td><td class="patent-data-table-td patent-date-value">Sep 28, 1998</td><td class="patent-data-table-td patent-date-value">Jun 13, 2000</td><td class="patent-data-table-td ">Spectra-Physics Scanning Systems, Inc.</td><td class="patent-data-table-td ">Multi-focus optical reader with masked or apodized lens</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6075240">US6075240</a></td><td class="patent-data-table-td patent-date-value">Jul 30, 1998</td><td class="patent-data-table-td patent-date-value">Jun 13, 2000</td><td class="patent-data-table-td ">Nec Usa, Inc.</td><td class="patent-data-table-td ">Hand-held plastic optical fiber linear scanner for reading color images formed on a surface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6102295">US6102295</a></td><td class="patent-data-table-td patent-date-value">Jan 14, 1998</td><td class="patent-data-table-td patent-date-value">Aug 15, 2000</td><td class="patent-data-table-td ">Intermec Ip Corp.</td><td class="patent-data-table-td ">Method and apparatus for decoding symbols by declaring erasures of element characteristics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6123261">US6123261</a></td><td class="patent-data-table-td patent-date-value">May 5, 1998</td><td class="patent-data-table-td patent-date-value">Sep 26, 2000</td><td class="patent-data-table-td ">Roustaei; Alexander R.</td><td class="patent-data-table-td ">Optical scanner and image reader for reading images and decoding optical information including one and two dimensional symbologies at variable depth of field</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6142934">US6142934</a></td><td class="patent-data-table-td patent-date-value">Dec 28, 1998</td><td class="patent-data-table-td patent-date-value">Nov 7, 2000</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Objective lens system for imaging instrument</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6152368">US6152368</a></td><td class="patent-data-table-td patent-date-value">Oct 29, 1998</td><td class="patent-data-table-td patent-date-value">Nov 28, 2000</td><td class="patent-data-table-td ">Psc Inc.</td><td class="patent-data-table-td ">Optical reader with addressable pixels</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6311895">US6311895</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 7, 1999</td><td class="patent-data-table-td patent-date-value">Nov 6, 2001</td><td class="patent-data-table-td ">Psc, Inc.</td><td class="patent-data-table-td ">Optical reader with condensed CMOS circuitry</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6637658">US6637658</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 22, 2001</td><td class="patent-data-table-td patent-date-value">Oct 28, 2003</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Optical reader having partial frame operating mode</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7083098">US7083098</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 24, 2004</td><td class="patent-data-table-td patent-date-value">Aug 1, 2006</td><td class="patent-data-table-td ">Symbol Technologies, Inc.</td><td class="patent-data-table-td ">Motion detection in imaging reader</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7128266">US7128266</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 13, 2003</td><td class="patent-data-table-td patent-date-value">Oct 31, 2006</td><td class="patent-data-table-td ">Metrologic Instruments. Inc.</td><td class="patent-data-table-td ">Hand-supportable digital imaging-based bar code symbol reader supporting narrow-area and wide-area modes of illumination and image capture</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20050103854">US20050103854</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 13, 2003</td><td class="patent-data-table-td patent-date-value">May 19, 2005</td><td class="patent-data-table-td ">Metrologic Instruments, Inc.</td><td class="patent-data-table-td ">Hand-supportable digital imaging-based bar code symbol reader supporting narrow-area and wide-area modes of illumination and image capture</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/USRE31289">USRE31289</a></td><td class="patent-data-table-td patent-date-value">Jul 10, 1981</td><td class="patent-data-table-td patent-date-value">Jun 28, 1983</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Color endoscope with charge coupled device and television viewing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/USRE31290">USRE31290</a></td><td class="patent-data-table-td patent-date-value">Jul 10, 1981</td><td class="patent-data-table-td patent-date-value">Jun 28, 1983</td><td class="patent-data-table-td ">Welch Allyn, Inc.</td><td class="patent-data-table-td ">Color endoscope</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Agilent Technologies, Pulsed Operating Ranges for AlInGap LEDs vs. Projected Long Term Light Output Performance, Nov. 1999, pp. 1-6.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Claim set of U.S. Appl. No. 12/132,462, filed Jun. 3, 2008, 15 pages.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Claim set of U.S. Appl. No. 12/132,480, filed Jun. 3, 2008, 8 pages.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Eastman Kodak Company, Fully Integrated Timing, Analog Signal Processing &amp; 10 bit ADC, Technical Data, Aug. 5, 2002, pp. 1-56.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Eastman Kodak Company, Ultra Sensitive Global Shutter 580 fps Monochrome CIS, Device Performance Specification, Sep. 2004, pp. 1-22.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">International Search Report of PCT/US2006/008113, (6 pages), Aug. 7, 2006.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">International Search Report of PCT/US2006/008113, (9 pages), Oct. 26, 2006.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">International Search Report of PCT/US2006/008114, (4 pgs.), Jul. 7, 2006.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kodak Image Sensor Solutions, Color Better through CMY Filters, www.kodak.com/go/ccd, pp. 1-2. Month and year unavailable but known to be published prior to earliest priority date of Jun. 3, 2005.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kodak Image Sensor Solutions, Device Performance Specification for Kodak KAC-9630 CMOS Image Sensor, pp. 1-22, Revision 1.1, Sep. 2004.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kodak Image Sensor Solutions, Device Performance Specification for Kodak KAI-0340S and Kodak KAI-0340D Image Sensor, pp. 1-54, Revision 1.0, Aug. 6, 2004.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Micron Technology, Inc., MT9M111 SOC Megapixel Digital Image Sensor, Products and Specifications Preliminary Manual, pp. 1-61, 2004.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Micron Technology, Inc., MT9M413 1.3-Megapixel CMOS Active Pixel Digital Image Sensor, Specification manual, Version.3.0, pp. 1-30, Jan. 2004.</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Office action for Chinese Patent Application No. 2006800160235, Office action dated Jan. 23, 2009, 6 pages (including English translation thereof, 7 pages).</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">PCT/US2006/008113, PCT Written Opinion (9 pages), Oct. 26, 2006.</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">PCT/US2006/008114, PCT Written Opinion (6 pgs.), Jul. 7, 2006.</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">STMicroelectronics, STMicroelectronics Introduces Low-Cost High-Quality Mega Pixel CMOS Sensor For Digital Still Cameras and Camcorders, Introduction article on website http://www.st.com/stonline/press/news/year2002/p1239p.htm, p. 1, Oct. 9, 2002.</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">WWW.FILLFACTORY.COM, Dual Slope Dynamic Range Expansion, Website, Feb. 28, 2005, pp. 1-3.</td></tr><tr><td class="patent-data-table-td ">19</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">WWW.MICRON.COM, Introducing A CMOS Image Sensor Specifically Designed for Automotive Scene-Understanding Systems, Website, Oct. 2, 2004, pp. 1-2.</td></tr><tr><td class="patent-data-table-td ">20</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">WWW.PHOTONFOCUS.COM, LINOG(TM) Technology The Key To Programmable Linear, Logarithmic, or Combined Linear And Logarithmic Response Without Image Lag Or Distortion, Website, Feb. 28, 2005, pp. 1-6.</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7866557">US7866557</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 27, 2007</td><td class="patent-data-table-td patent-date-value">Jan 11, 2011</td><td class="patent-data-table-td ">Symbol Technologies, Inc.</td><td class="patent-data-table-td ">Imaging-based bar code reader utilizing modified rolling shutter operation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7918398">US7918398</a></td><td class="patent-data-table-td patent-date-value">Jun 3, 2008</td><td class="patent-data-table-td patent-date-value">Apr 5, 2011</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Indicia reading terminal having multiple setting imaging lens</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7934660">US7934660</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 17, 2006</td><td class="patent-data-table-td patent-date-value">May 3, 2011</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Data collection system having reconfigurable data collection terminal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8061613">US8061613</a></td><td class="patent-data-table-td patent-date-value">Sep 1, 2009</td><td class="patent-data-table-td patent-date-value">Nov 22, 2011</td><td class="patent-data-table-td ">Cognex Technology And Investment Corporation</td><td class="patent-data-table-td ">Method and apparatus for providing omnidirectional lighting in a scanning device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8089542">US8089542</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 27, 2011</td><td class="patent-data-table-td patent-date-value">Jan 3, 2012</td><td class="patent-data-table-td ">Micron Technology, Inc.</td><td class="patent-data-table-td ">CMOS imager with integrated circuitry</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8162218">US8162218</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 17, 2007</td><td class="patent-data-table-td patent-date-value">Apr 24, 2012</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Using a 2D imager for rastering scanning</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8256678">US8256678</a></td><td class="patent-data-table-td patent-date-value">Aug 12, 2009</td><td class="patent-data-table-td patent-date-value">Sep 4, 2012</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Indicia reading terminal having image sensor and variable lens assembly</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8292183">US8292183</a></td><td class="patent-data-table-td patent-date-value">Apr 4, 2011</td><td class="patent-data-table-td patent-date-value">Oct 23, 2012</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Indicia reading terminal having multiple setting imaging lens</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8295601">US8295601</a></td><td class="patent-data-table-td patent-date-value">Aug 12, 2009</td><td class="patent-data-table-td patent-date-value">Oct 23, 2012</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Indicia reading terminal having multiple exposure periods and methods for same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8305691">US8305691</a></td><td class="patent-data-table-td patent-date-value">Apr 29, 2009</td><td class="patent-data-table-td patent-date-value">Nov 6, 2012</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Fluid lens element for use in changing thermal operating environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8366002">US8366002</a></td><td class="patent-data-table-td patent-date-value">May 26, 2010</td><td class="patent-data-table-td patent-date-value">Feb 5, 2013</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Solid elastic lens element and method of making same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8384814">US8384814</a></td><td class="patent-data-table-td patent-date-value">Dec 30, 2011</td><td class="patent-data-table-td patent-date-value">Feb 26, 2013</td><td class="patent-data-table-td ">Micron Technology, Inc.</td><td class="patent-data-table-td ">CMOS imager with integrated circuitry</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8387881">US8387881</a></td><td class="patent-data-table-td patent-date-value">Dec 1, 2010</td><td class="patent-data-table-td patent-date-value">Mar 5, 2013</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Terminal with screen reading mode</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8464952">US8464952</a></td><td class="patent-data-table-td patent-date-value">Nov 18, 2009</td><td class="patent-data-table-td patent-date-value">Jun 18, 2013</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Optical reader having improved back-illuminated image sensor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8474723">US8474723</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 26, 2008</td><td class="patent-data-table-td patent-date-value">Jul 2, 2013</td><td class="patent-data-table-td ">Symbol Technologies, Inc.</td><td class="patent-data-table-td ">Method for optimizing auto-exposure performance of an imaging device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8537245">US8537245</a></td><td class="patent-data-table-td patent-date-value">Mar 4, 2011</td><td class="patent-data-table-td patent-date-value">Sep 17, 2013</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Imaging and decoding device with quantum dot imager</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8596539">US8596539</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 31, 2012</td><td class="patent-data-table-td patent-date-value">Dec 3, 2013</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Imaging terminal having image sensor and lens assembly</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8628013">US8628013</a></td><td class="patent-data-table-td patent-date-value">Dec 13, 2011</td><td class="patent-data-table-td patent-date-value">Jan 14, 2014</td><td class="patent-data-table-td ">Honeywell International Inc.</td><td class="patent-data-table-td ">Apparatus comprising image sensor array and illumination control</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8636215">US8636215</a></td><td class="patent-data-table-td patent-date-value">Jun 27, 2011</td><td class="patent-data-table-td patent-date-value">Jan 28, 2014</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Decodable indicia reading terminal with optical filter</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8640958">US8640958</a></td><td class="patent-data-table-td patent-date-value">Jan 21, 2010</td><td class="patent-data-table-td patent-date-value">Feb 4, 2014</td><td class="patent-data-table-td ">Honeywell International, Inc.</td><td class="patent-data-table-td ">Indicia reading terminal including optical filter</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8640960">US8640960</a></td><td class="patent-data-table-td patent-date-value">Dec 1, 2011</td><td class="patent-data-table-td patent-date-value">Feb 4, 2014</td><td class="patent-data-table-td ">Honeywell International Inc.</td><td class="patent-data-table-td ">Optical filter for image and barcode scanning</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8646692">US8646692</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 2011</td><td class="patent-data-table-td patent-date-value">Feb 11, 2014</td><td class="patent-data-table-td ">Hand Held Products, Inc.</td><td class="patent-data-table-td ">Devices and methods employing dual target auto exposure</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8777108">US8777108</a></td><td class="patent-data-table-td patent-date-value">Mar 23, 2012</td><td class="patent-data-table-td patent-date-value">Jul 15, 2014</td><td class="patent-data-table-td ">Honeywell International, Inc.</td><td class="patent-data-table-td ">Cell phone reading mode using image timer</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=5AvPBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc235/defs235.htm&usg=AFQjCNE4qW8wy3R4_i_voHj55rF5UTzd0Q#C235S462450">235/462.45</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=5AvPBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc235/defs235.htm&usg=AFQjCNE4qW8wy3R4_i_voHj55rF5UTzd0Q#C235S462250">235/462.25</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=5AvPBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc235/defs235.htm&usg=AFQjCNE4qW8wy3R4_i_voHj55rF5UTzd0Q#C235S462110">235/462.11</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=5AvPBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc235/defs235.htm&usg=AFQjCNE4qW8wy3R4_i_voHj55rF5UTzd0Q#C235S462070">235/462.07</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=5AvPBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc235/defs235.htm&usg=AFQjCNE4qW8wy3R4_i_voHj55rF5UTzd0Q#C235S462410">235/462.41</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=5AvPBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc235/defs235.htm&usg=AFQjCNE4qW8wy3R4_i_voHj55rF5UTzd0Q#C235S462100">235/462.1</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=5AvPBQABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0007100000">G06K7/10</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=5AvPBQABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K7/10722">G06K7/10722</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06K7/10S4D</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Oct 29, 2013</td><td class="patent-data-table-td ">IPR</td><td class="patent-data-table-td ">Aia trial proceeding filed before the patent and appeal board: inter partes review</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Opponent name: </span><span class="nested-value">FUJIAN NEWLAND COMPUTER CO., LTD.</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">TRIAL NO: IPR2013-00595</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20130920</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 2, 2013</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 1-35 IS CONFIRMED.NEW CLAIMS 36-46 ARE ADDED AND DETERMINED TO BE PATENTABLE.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 25, 2013</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 19, 2012</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20120427</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 18, 2007</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">HAND HELD PRODUCTS, INC., NEW YORK</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:WANG, YNJIUN;HAVENS, WILLIAM H.;REEL/FRAME:019981/0752</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20050609</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U0Yl0dOCGJIKwaf06AyMUKPZTtHBQ\u0026id=5AvPBQABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U00witSt-xgMsxXSnqLt8s758A0Eg\u0026id=5AvPBQABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U0zIPs-Uor76yLdsHtllalT7NZFpw","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Bar_code_reading_device_with_global_elec.pdf?id=5AvPBQABERAJ\u0026output=pdf\u0026sig=ACfU3U3lZ-hoZXXsNm_p7khToabbjp0AIA"},"sample_url":"http://www.google.com/patents/reader?id=5AvPBQABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>