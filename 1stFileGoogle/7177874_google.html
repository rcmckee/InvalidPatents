<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7177874 - System and method for generating and processing results data in a ... - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="System and method for generating and processing results data in a distributed system"><meta name="DC.contributor" content="Cary A. Jardin" scheme="inventor"><meta name="DC.contributor" content="Jardin Cary A" scheme="assignee"><meta name="DC.date" content="2004-3-23" scheme="dateSubmitted"><meta name="DC.description" content="Embodiments of the systems and methods provide the superior performance of high-speed distributed processing in a clustered system environment. The distributed computing system stores data tables or distributes jobs or tasks on multiple processors that execute on one or more nodes. For the case of multiple nodes, the nodes communicate over an inter-nodal communication link, for example, via a proprietary communication protocol, or alternatively via a standard protocol such as SQL database command protocol. By distributing the data storage and task processing over a potentially large number of processors and nodes, the distributed computing system returns processing results to the requestor in significantly reduced times as compared to conventional computing systems."><meta name="DC.date" content="2007-2-13" scheme="issued"><meta name="DC.relation" content="EP:1164498:A1" scheme="references"><meta name="DC.relation" content="EP:1363189:A2" scheme="references"><meta name="DC.relation" content="US:20040122845:A1" scheme="references"><meta name="DC.relation" content="US:20050060292:A1" scheme="references"><meta name="DC.relation" content="US:6098157" scheme="references"><meta name="DC.relation" content="US:6968335" scheme="references"><meta name="DC.relation" content="US:7054852" scheme="references"><meta name="DC.relation" content="US:7113957" scheme="references"><meta name="DC.relation" content="WO:2001002935:A2" scheme="references"><meta name="citation_reference" content="Jaiwei et al., Join Index Hierarchy: An Indexing Structure for Efficient Navigation in Object-Oriented Database, IEEE, 1999, pp. 1-33."><meta name="citation_reference" content="Mohammed et al., Novel Parallel Join Algorithms for Grid Files, IEEE, 1996, pp. 144-149."><meta name="citation_reference" content="Taniar et al., Aggregate-Join Query Processing in Parallel Database System, IEEE, 2000, pp. 824-829."><meta name="citation_patent_number" content="US:7177874"><meta name="citation_patent_application_number" content="US:10/808,177"><link rel="canonical" href="http://www.google.com/patents/US7177874"/><meta property="og:url" content="http://www.google.com/patents/US7177874"/><meta name="title" content="Patent US7177874 - System and method for generating and processing results data in a distributed system"/><meta name="description" content="Embodiments of the systems and methods provide the superior performance of high-speed distributed processing in a clustered system environment. The distributed computing system stores data tables or distributes jobs or tasks on multiple processors that execute on one or more nodes. For the case of multiple nodes, the nodes communicate over an inter-nodal communication link, for example, via a proprietary communication protocol, or alternatively via a standard protocol such as SQL database command protocol. By distributing the data storage and task processing over a potentially large number of processors and nodes, the distributed computing system returns processing results to the requestor in significantly reduced times as compared to conventional computing systems."/><meta property="og:title" content="Patent US7177874 - System and method for generating and processing results data in a distributed system"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("2qDtU4iADbHUsATS0ID4Cg"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("USA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("2qDtU4iADbHUsATS0ID4Cg"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("USA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7177874?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7177874"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=LQJ6BAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7177874&amp;usg=AFQjCNGN7hqEZVdb8tXgKiDLiu2bPfOWaA" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7177874.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7177874.pdf"></a><a class="appbar-application-grant-link" data-label="Application" href="/patents/US20040181523"></a><a class="appbar-application-grant-link" data-selected="true" data-label="Grant" href="/patents/US7177874"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7177874" style="display:none"><span itemprop="description">Embodiments of the systems and methods provide the superior performance of high-speed distributed processing in a clustered system environment. The distributed computing system stores data tables or distributes jobs or tasks on multiple processors that execute on one or more nodes. For the case of multiple...</span><span itemprop="url">http://www.google.com/patents/US7177874?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7177874 - System and method for generating and processing results data in a distributed system</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7177874 - System and method for generating and processing results data in a distributed system" title="Patent US7177874 - System and method for generating and processing results data in a distributed system"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7177874 B2</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 10/808,177</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Feb 13, 2007</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Mar 23, 2004</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Jan 16, 2003</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US20040181523">US20040181523</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">10808177, </span><span class="patent-bibdata-value">808177, </span><span class="patent-bibdata-value">US 7177874 B2, </span><span class="patent-bibdata-value">US 7177874B2, </span><span class="patent-bibdata-value">US-B2-7177874, </span><span class="patent-bibdata-value">US7177874 B2, </span><span class="patent-bibdata-value">US7177874B2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Cary+A.+Jardin%22">Cary A. Jardin</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Jardin+Cary+A%22">Jardin Cary A</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7177874.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7177874.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7177874.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (9),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (3),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (6),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (22),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (5)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7177874&usg=AFQjCNGya7irA--Me-TecVtiWHbu8Tr-ag">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7177874&usg=AFQjCNHM8N7XHQwDS1V4cupKPRDvrNZaIw">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7177874B2%26KC%3DB2%26FT%3DD&usg=AFQjCNE-w1TOneLodOzH0SKyGOlo3-HUrA">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55708872" lang="EN" load-source="patent-office">System and method for generating and processing results data in a distributed system</invention-title></span><br><span class="patent-number">US 7177874 B2</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA51108826" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">Embodiments of the systems and methods provide the superior performance of high-speed distributed processing in a clustered system environment. The distributed computing system stores data tables or distributes jobs or tasks on multiple processors that execute on one or more nodes. For the case of multiple nodes, the nodes communicate over an inter-nodal communication link, for example, via a proprietary communication protocol, or alternatively via a standard protocol such as SQL database command protocol. By distributing the data storage and task processing over a potentially large number of processors and nodes, the distributed computing system returns processing results to the requestor in significantly reduced times as compared to conventional computing systems.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(14)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7177874B2/US07177874-20070213-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7177874B2/US07177874-20070213-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(21)</span></span></div><div class="patent-text"><div mxw-id="PCLM9161367" lang="EN" load-source="patent-office" class="claims">
  <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
    <div class="claim-text">1. A method of processing a query command in a distributed computing system in which a plurality of database tables are stored on a plurality of nodes, different portions of at least one database table being stored on at least two of the nodes, the method comprising:
<div class="claim-text">storing a first portion of a first database table and a first portion of a second database table on a first node, and storing a second portion of a first database table and a second portion of a second database table on a second node;</div>
<div class="claim-text">determining a join table definition in response to a query command, said join table definition identifying a subset of said first database table to include in executing said database query command;</div>
<div class="claim-text">generating a first join table from said first portion of said first database table in accordance with said join table definition, and generating a second join table from said second portion of said first database table in accordance with said join table definition;</div>
<div class="claim-text">transmitting said first join table to said second node, and transmitting said second join table to said first node;</div>
<div class="claim-text">comparing said first portion of said second database table with said first join table, and comparing said second portion of said second database table with said second join table to generate a first intermediate results file;</div>
<div class="claim-text">comparing said first portion of said second database table with said second join table, and comparing said second portion of said second database table with said first join table to generate a second intermediate results file; and</div>
<div class="claim-text">generating a final results file from said first intermediate results file and said second intermediate results file.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
    <div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said storing of said first portion of said first database table and said first portion of said second database table on said first node is stored in substantially equal portions.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
    <div class="claim-text">3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein said storing of said first portion of said first database table and said first portion of said second database table on said first node is stored in substantially equal portions according to a round robin distribution.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
    <div class="claim-text">4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein said storing of said second portion of said first database table and said second portion of said second database table on said second node is stored in substantially equal portions according to a round robin distribution.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
    <div class="claim-text">5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising executing post-processing operations on said final results file.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
    <div class="claim-text">6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said storing of said first portion of said first database table and said first portion of said second database table is stored on a volatile memory of said first node, and said storing of said second portion of said first database table and said first portion of said second database table is stored on a volatile memory of said second node.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
    <div class="claim-text">7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising storing said first and second database tables on a persistent storage device.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00008" num="00008" class="claim">
    <div class="claim-text">8. A distributed database system for processing a database query command in which a plurality of database tables are stored on a plurality of nodes, different portions of at least one database table being stored on at least two of the nodes, the system comprising:
<div class="claim-text">means for storing a first portion of a first database table and a first portion of a second database table on a first node, and storing a second portion of a first database table and a second portion of a second database table on a second node;</div>
<div class="claim-text">means for determining a join table definition in response to a database query command, said join table definition identifying a subset of said first database table to include in executing said database query command;</div>
<div class="claim-text">means for generating a first join table from said first portion of said first database table in accordance with said join table definition, and generating a second join table from said second portion of said first database table in accordance with said join table definition;</div>
<div class="claim-text">means for transmitting said first join table to said second node, and transmitting said second join table to said first node;</div>
<div class="claim-text">means for comparing said first portion of said second database table with said first join table, and comparing said second portion of said second database table with said second join table to generate a first intermediate results file;</div>
<div class="claim-text">means for comparing said first portion of said second database table with said second join table, and comparing said second portion of said second database table with said first join table to generate a second intermediate results file; and</div>
<div class="claim-text">means for generating a final results file from said first intermediate results file and said second intermediate results file.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
    <div class="claim-text">9. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein storing of said first portion of said first database table and said first portion of said second database table on said first node comprises storing in substantially equal portions.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
    <div class="claim-text">10. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein storing of said first portion of said first database table and said first portion of said second database table on said first node comprises storing in substantially equal portions according to a round robin distribution.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
    <div class="claim-text">11. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein storing of said second portion of said first database table and said second portion of said second database table on said second node comprises storing in substantially equal portions according to a round robin distribution.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
    <div class="claim-text">12. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising means for executing post-processing operations on said final results file.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
    <div class="claim-text">13. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein said means for storing of said first portion of said first database table and said first portion of said second database table comprises a volatile memory of said first node, and said means for storing of said second portion of said first database table and said first portion of said second database table comprises a volatile memory of said second node.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
    <div class="claim-text">14. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising means for storing said first and second database tables on a persistent storage device.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00015" num="00015" class="claim">
    <div class="claim-text">15. A method of processing a query command in a distributed computing system in which a plurality of database tables are stored on a plurality of nodes, the method comprising:
<div class="claim-text">storing a first database table and a second database table on a first node;</div>
<div class="claim-text">storing a third database table and a fourth database table on a second node;</div>
<div class="claim-text">determining a first join table definition in response to a query command, said first join table definition identifying a subset of said first database table, and generating a second join table definition in response to said query command, said second join table definition identifying a subset of said third database table;</div>
<div class="claim-text">generating a first join table from said first database table in accordance with said first join table definition, and generating a second join table from said third database table in accordance with said second join table definition;</div>
<div class="claim-text">comparing said second database table with said first join table to generate a first intermediate results file;</div>
<div class="claim-text">comparing said fourth database table with said second join table to generate a second intermediate results file; and</div>
<div class="claim-text">generating a final results file from said first intermediate results file and said second intermediate results file.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
    <div class="claim-text">16. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising executing post-processing operations on said final results file.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
    <div class="claim-text">17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein said post-processing operations comprise removing duplicate matching records from said final results file.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
    <div class="claim-text">18. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein said first database table and said second database table on said first node are stored on a volatile memory of said first node.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
    <div class="claim-text">19. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein said third database table and said fourth database table are stored on a volatile memory of said second node.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
    <div class="claim-text">20. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising storing said first and second database tables on a persistent storage device.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00021" num="00021" class="claim">
    <div class="claim-text">21. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, further comprising storing said third and fourth database tables on a persistent storage device.</div>
  </div>
</div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES16183911" lang="EN" load-source="patent-office" class="description">
<heading>RELATED APPLICATIONS</heading> <p num="p-0002">This application is a continuation-in-part of, and claims priority to, U.S. patent application Ser. No. 10/345,811, filed Jan. 16, 2003 now abandoned and titled “SYSTEM AND METHOD FOR DISTRIBUTED DATABASE PROCESSING IN A CLUSTERED ENVIRONMENT,” and U.S. patent application Ser. No. 10/345,504, filed Jan. 16, 2003 now abandoned and titled “SYSTEM AND METHOD FOR COOPERATIVE DATABASE ACCELERATION,” which are hereby incorporated by reference in their entireties. This application is related to U.S. patent application Ser. No. 10/807,816 titled “SYSTEM AND METHOD FOR COOPERATIVE DATABASE ACCELERATION,” U.S. patent application Ser. No. 10/808,199 titled “SYSTEM AND METHOD FOR DISTRIBUTED PROCESSING IN A NODE ENVIRONMENT,” U.S. patent application Ser. No. 10/808,176 titled “SYSTEM AND METHOD FOR CONTROLLING PROCESSING IN A DISTRIBUTED SYSTEM,” and U.S. patent application Ser. No. 10/808,175 titled “SHARED MEMORY ROUTER SYSTEM AND METHOD FOR NODE COMMUNICATION IN A DISTRIBUTED SYSTEM,” which are filed on even date herewith and are all hereby incorporated by reference in their entireties.</p>
<heading>BACKGROUND OF THE INVENTION</heading> <p num="p-0003">1. Field of the Invention</p>
  <p num="p-0004">The present invention generally relates to distributed processing in computer systems. More particularly, the invention relates to systems and methods for increasing the performance of computer systems by distributing the data processing load among multiple processors in a clustered environment.</p>
  <p num="p-0005">2. Description of the Related Technology</p>
  <p num="p-0006">Database systems have become a central and critical element of business infrastructure with the development and widespread use of computer systems and electronic data. Businesses typically rely on computer databases to be the safe harbor for storage and retrieval of very large amounts of vital information. The speed and storage capacities of computer systems have grown exponentially over the years, as has the need for larger and faster database systems.</p>
  <p num="p-0007">A database (DB) is a collection of information organized in such a way that a computer program can quickly access desired pieces of data. Traditional databases are organized by fields, records and tables or files. A field is a category or item of information, a record is one complete set of fields, and a table or file is a collection of records. For example, a telephone book is analogous to a table or file. It contains a list of records that is analogous to the entries of people or businesses in the phone book, each record consisting of three fields: name, address, and telephone number.</p>
  <p num="p-0008">In its simplest form, a database is a repository for the storage and retrieval of information. The early database systems simply provided batch input command data for programs, and stored the programmatic output. As computing technologies have advanced greatly over the years, so too have database systems progressed from an internal function supporting the execution of computer programs to complex and powerful stand-alone data storage systems. Client applications executing on computer systems can connect to or communicate with the database system via a network, or by other programmatic means, to store and retrieve data.</p>
  <p num="p-0009">A database management system (DBMS) can be used to access information in a database. The DBMS is a collection of programs that enables the entry, organization and selection of data in a database. There are many different types of DBMSs, ranging from small systems that run on personal computers to very large systems that run on mainframe computers or serve the data storage and retrieval needs of many computers connected to a computer network. The term “database” is often used as shorthand to refer to a “database management system.”</p>
  <p num="p-0010">While database system applications are numerous and varied, following are several examples:
</p> <ul> <li id="ul0001-0001" num="0000"> <ul> <li id="ul0002-0001" num="0010">computerized library systems;</li> <li id="ul0002-0002" num="0011">automated teller machines and bank account data;</li> <li id="ul0002-0003" num="0012">customer contact and account information;</li> <li id="ul0002-0004" num="0013">flight reservation systems; and</li> <li id="ul0002-0005" num="0014">computerized parts inventory systems.</li> </ul> </li> </ul> <p num="p-0011">From a technical standpoint, DBMSs can vary widely. For example, a DBMS can organize information internally in a relational, network, flat, or hierarchical manner. The internal organization can affect how quickly and flexibly information can be extracted from the database system. A relational database is one which stores data in two or more tables and enables the user to define relationships between the tables. The link between the tables is based on field values common to both tables.</p>
  <p num="p-0012">Requests for information from a database are often presented in the form of a query, which is essentially a stylized or structured question. For example, the following query requests all records from the current database table in which the NAME field is SMITH and the AGE field contains a number greater than 35.
<br>SELECT ALL WHERE NAME=“SMITH” AND AGE&gt;35</p>
  <p num="p-0013">The set of rules or standards for constructing queries is generally referred to as a query language. Different DBMSs support different query languages, although there is a semi-standardized query language called structured query language (SQL). In addition, more sophisticated languages for managing database systems are referred to as fourth generation languages, or 4GLs for short.</p>
  <p num="p-0014">SQL is used to communicate with a database system. SQL is the ANSI (American National Standards Institute) standard language for relational database management systems. SQL statements are used to perform tasks such as update data on a database or retrieve data from a database. Although there are different variations or dialects of SQL, it is nevertheless the closest thing to a standard query language that currently exists. Some examples of relational database management systems that use SQL include the following: Oracle, Sybase, Microsoft SQL Server, Access, and Ingres. Although most database systems use SQL, many also have their own additional proprietary extensions that are usually only used on that system. However, the standard SQL commands such as “Select,” “Insert,” “Update,” “Delete,” “Create,” and “Drop” can be used to accomplish most operations that the user needs to do with a database.</p>
  <p num="p-0015">Distributed database systems are databases in which the data storage and processing load is spread out over multiple database systems and connected by a communication. Distributed databases enable multiple users on a network such as a local area network (LAN) to access the same database system simultaneously.</p>
  <p num="p-0016">However, existing database systems are often the bottleneck of computer systems, and the ever-growing power and speed of modern computing systems exacerbate this problem as computer processors are able to receive and process data ever more quickly. Therefore, what is needed is a distributed database system that provides very high-speed data retrieval.</p>
  <heading>SUMMARY OF CERTAIN INVENTIVE ASPECTS</heading> <p num="p-0017">The systems and methods of the invention have many features, no single one of which is solely responsible for its desirable attributes. Without limiting the scope of the invention as expressed by the claims that follow, some prominent features will now be discussed briefly. After considering this discussion, and particularly after reading the section entitled “Detailed Description of Certain Embodiments,” one will understand how the features of the system and methods provide advantages over traditional systems.</p>
  <p num="p-0018">Embodiments of the present invention provide the superior performance of high-speed distributed computing systems in a clustered environment. The distributed computing system stores data tables or distributes jobs or tasks on multiple processors that execute on one or more nodes. For the case of multiple nodes, the nodes communicate over an inter-nodal communication link, for example, via a proprietary communication protocol, or alternatively via a standard protocol such as SQL database command protocol. By distributing the data storage and task processing over a potentially large number of processors and nodes, the distributed computing system returns processing results to the requestor in significantly reduced times as compared to conventional distributed computing systems.</p>
  <p num="p-0019">Embodiments of the systems and methods include processing a query command in a distributed computing system in which a plurality of database tables are stored on a plurality of nodes. The systems and methods include storing a first database table and a second database table on a first node, and storing a third database table and a fourth database table on a second node. This also includes determining a first join table definition in response to a query command, the first join table definition identifying a subset of the first database table. Also included is generating a second join table definition in response to the query command, the second join table definition identifying a subset of the third database table. This further includes generating a first join table from the first database table in accordance with the first join table definition, and generating a second join table from the third database table in accordance with the second join table definition. Additionally included is comparing the second database table with the first join table to generate a first intermediate results file, and comparing the fourth database table with the second join table to generate a second intermediate results file. This also includes generating a final results file from the first intermediate results file and the second intermediate results file. Additionally, this can also include executing post-processing operations on the final results file and wherein the post-processing operations include removing duplicate matching records from the final results file.</p>
<description-of-drawings> <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p num="p-0020">The above and other aspects, features and advantages of the invention will be better understood by referring to the following detailed description, which should be read in conjunction with the accompanying drawings. These drawings and the associated description are provided to illustrate certain embodiments of the invention, and not to limit the scope of the invention.</p>
    <p num="p-0021"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a block diagram illustrating one example of a database system.</p>
    <p num="p-0022"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a block diagram illustrating components or modules of the nodes of the distributed database system shown in <figref idrefs="DRAWINGS">FIG. 1</figref>.</p>
    <p num="p-0023"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a block diagram illustrating an example of database table storage in the logical processor storage areas shown in <figref idrefs="DRAWINGS">FIG. 2</figref>.</p>
    <p num="p-0024"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a diagram illustrating a representation of storage of two database tables in a single node embodiment of the distributed database system.</p>
    <p num="p-0025"> <figref idrefs="DRAWINGS">FIG. 5A</figref> is a diagram illustrating an example of one phase of a database query command in the single node embodiment of the distributed database system.</p>
    <p num="p-0026"> <figref idrefs="DRAWINGS">FIG. 5B</figref> is a diagram illustrating an example of an additional phase of a database query command in the single node embodiment of the distributed database system.</p>
    <p num="p-0027"> <figref idrefs="DRAWINGS">FIG. 5C</figref> is a diagram illustrating an example of an additional phase of a database query command in the single node embodiment of the distributed database system.</p>
    <p num="p-0028"> <figref idrefs="DRAWINGS">FIG. 6</figref> is a block diagram illustrating components or modules of the controller of the primary node shown in <figref idrefs="DRAWINGS">FIG. 2</figref>.</p>
    <p num="p-0029"> <figref idrefs="DRAWINGS">FIG. 7</figref> is a block diagram illustrating components or modules of the controller of the secondary nodes shown in <figref idrefs="DRAWINGS">FIG. 2</figref>.</p>
    <p num="p-0030"> <figref idrefs="DRAWINGS">FIG. 8</figref> is a block diagram illustrating components or modules of the logical processors of the nodes shown in <figref idrefs="DRAWINGS">FIG. 2</figref>.</p>
    <p num="p-0031"> <figref idrefs="DRAWINGS">FIG. 9</figref> is a flowchart illustrating a database command process as performed by the table distribution processing module shown in <figref idrefs="DRAWINGS">FIG. 6</figref>.</p>
    <p num="p-0032"> <figref idrefs="DRAWINGS">FIG. 10</figref> is a flowchart illustrating a process of determining the target node for a database write command as performed by the primary FEP shown in <figref idrefs="DRAWINGS">FIG. 2</figref>.</p>
    <p num="p-0033"> <figref idrefs="DRAWINGS">FIG. 11</figref> is a flowchart illustrating a single set query process as performed by the distributed database system shown in <figref idrefs="DRAWINGS">FIG. 1</figref>.</p>
    <p num="p-0034"> <figref idrefs="DRAWINGS">FIG. 12</figref> is a flowchart illustrating a multiple set query process as performed by the distributed database system shown in <figref idrefs="DRAWINGS">FIG. 1</figref>.</p>
    <p num="p-0035"> <figref idrefs="DRAWINGS">FIG. 13</figref> is a flowchart illustrating a shared memory router process as performed by the shared memory router shown in <figref idrefs="DRAWINGS">FIG. 6</figref>.</p>
  </description-of-drawings> <heading>DETAILED DESCRIPTION OF CERTAIN EMBODIMENTS</heading> <p num="p-0036">The following detailed description is directed to certain specific embodiments of the invention. However, the invention can be embodied in a multitude of different ways as defined and covered by the claims. The scope of the invention is to be determined with reference to the appended claims. In this description, reference is made to the drawings wherein like parts are designated with like numerals throughout.</p>
  <p num="p-0037">The distributed computing system described herein can be implemented in different embodiments as various modules as discussed in detail below. The components or modules can be implemented as, but are not limited to, software, hardware or firmware components, or any combination of such components, that perform certain functions, steps or tasks as described herein. Thus, for example, a component or module may include software components, firmware, microcode, circuitry, an application specific integrated circuit (ASIC), and may further include data, databases, data structures, tables, arrays, and variables. In the case of a software embodiment, each of the modules can be separately compiled and linked into a single executable program, or may be run in an interpretive manner, such as a macro. The functions, steps or tasks associated with each of the modules may be redistributed to one of the other modules, combined together in a single module, or made available in, for example, a shareable dynamic link library. Furthermore, the functionality provided for in the components or modules may be combined into fewer components, modules, or databases or further separated into additional components, modules, or databases. Additionally, the components or modules may be implemented to execute on one or more computers.</p>
  <p num="p-0038">The distributed computing systems are described herein primarily in the example context of distributed database systems. However, other embodiments of the invention include many types of distributed task systems, for example, systems having disjoint, distributed processing of tasks, jobs, or operations. Generally, the distributed computing systems are configured to break jobs down into a series of smaller functions or tasks that can be distributed among processors or nodes of a distributed computing system. In such distributed systems, the data is usually disjoint in that a particular piece of data is primarily associated with a single processor at any point in time, and processors do not directly access the data of other processors. The distributed database systems described herein are only example embodiments of the distributed computing systems.</p>
  <p num="p-0039">Referring to the figures, <figref idrefs="DRAWINGS">FIG. 1</figref> is a block diagram illustrating one example of a database system <b>100</b>. The database system <b>100</b> includes an accelerated database system <b>105</b>, which in turn includes a database (DB) server <b>130</b> that is connected to a persistent storage device <b>140</b> and a distributed database system <b>160</b> as shown in <figref idrefs="DRAWINGS">FIG. 1</figref>. The accelerated database system <b>105</b> can store data reliably and long-term on the persistent storage device <b>140</b>, and simultaneously store data for fast retrieval on the distributed database system <b>160</b>. In some embodiments, the DB server <b>130</b> stores data both on the persistent storage device <b>140</b> and on the distributed database system <b>160</b>, such that the data stored on the databases are copies of one another. In this way, the accelerated database system <b>105</b> stores data reliably and retrieves data very rapidly. In other embodiments, the accelerated database system <b>105</b> does not include the persistent storage device <b>140</b> and data is stored just on the distributed database system <b>160</b>.</p>
  <p num="p-0040">The database system <b>100</b> can include a client computer system <b>110</b>. The client computer system <b>110</b> can be one or more computers and associated input devices. The client computer system <b>110</b> is used by clients or users of the database system <b>100</b> to access the accelerated database system <b>105</b>. The client can access the accelerated database system <b>105</b> by entering database commands and viewing database information in a logical and easy to use manner via a graphical user interface (GUI) that executes on the client computer system <b>110</b>. The client computer system <b>110</b> can also employ other types of user interfaces, such as scripting language files or command line interfaces.</p>
  <p num="p-0041">The DB server <b>130</b> can be implemented in a computer or a computer system. For example, such servers are available from Oracle and Microsoft. The DB server <b>130</b> receives database commands, for example, read and write commands, transmitted by the client computer system <b>110</b> via a network <b>120</b>. The DB server <b>130</b> also determines whether to send the database commands to the persistent storage device <b>140</b>, or to the distributed database system <b>160</b>, or to both. The DB server <b>130</b> additionally receives responses from the database read commands, for example, results data from a database query command. The DB server <b>130</b> can be a SQL server that conforms or approximately conforms to the SQL standard for database query language. The database commands can be initiated by user input or other user actions on the client computer system <b>110</b>, or programmatically generated by an application running on the client computer system <b>110</b>.</p>
  <p num="p-0042">The network <b>120</b> is represented in <figref idrefs="DRAWINGS">FIG. 1</figref> as a cloud-shaped symbol to illustrate that a multitude of network configurations are possible and that the client computer system <b>110</b> and the DB server <b>130</b> can be indirectly connected via multiple server computers and network connections (not shown). Alternatively, the DB server <b>130</b> can be directly connected to the client computer system <b>110</b>, or the DB server <b>130</b> can be incorporated within the client computer system <b>110</b>, in which case the network <b>120</b> is not needed.</p>
  <p num="p-0043">The DB server <b>130</b> communicates with the persistent storage device <b>140</b>, if present, via a communication link <b>150</b>. The communication link <b>150</b> can be a direct connection or a network connection. Characteristics of embodiments of the persistent storage device <b>140</b> include the capability to store data, for example, database entries or records, through cycles in power (e.g., power on/power off transitions) and for long periods of time in a reliable way. The persistent storage device <b>140</b> can be, for example, one or more computer hard disk drives, tape drives, or other long-term storage devices and combinations of the foregoing.</p>
  <p num="p-0044">The accelerated database system <b>105</b> further includes the distributed database system <b>160</b> that communicates with the DB server <b>130</b> via a communication link <b>154</b>. The distributed database system <b>160</b> provides distributed storage of database information and can provide very high-speed data retrieval. The distributed database system <b>160</b> can conform to a standard database protocol, for example, a SQL compliant database, in which case the distributed database system <b>160</b> can be directly connected to the network <b>120</b> without the use of the DB server <b>130</b>. In one embodiment, the distributed database system <b>160</b> is a processor with a main memory. Alternatively, multiple processors, each with a main memory, can be used. The memory or data storage area of the distributed database system <b>160</b> can be, for example, solid state memory such as random access memory (RAM). In one embodiment, the memory of the distributed database system <b>160</b> is volatile memory, which means that stored data is lost when power is removed. Alternatively, the memory can be other types of volatile memory, as well as nonvolatile memory, such as a disk drive or a combination of volatile and nonvolatile memory. However, in the description that follows, only volatile memory examples are described, but both types of memory can be used. The communication link <b>154</b> can be an Ethernet network connection that conforms to the TCP/IP network protocol, for example, the Internet, a local area network (LAN), a wide area network (WAN), an Intranet, or other network links and protocols.</p>
  <p num="p-0045">As shown in <figref idrefs="DRAWINGS">FIG. 1</figref>, the distributed database system <b>160</b> can include multiple nodes <b>164</b>, <b>170</b>, <b>174</b>, <b>180</b> connected via an inter-nodal communication link <b>190</b>. Each node can store a portion of the database information, for example, an approximately equal portion. High-speed retrieval can be improved when the nodes <b>164</b>, <b>170</b>, <b>174</b>, <b>180</b> process database read commands simultaneously in a parallel fashion on the portion of the database stored at each of the nodes. The inter-nodal communication link <b>190</b> transfers data between the nodes <b>164</b>, <b>170</b>, <b>174</b>, <b>180</b>, and is preferably a high throughput, low latency communication interface link. The inter-nodal communication link <b>190</b> can be a commercially available communication link, or a custom-built, proprietary communication link. As designated by the label “NODE N” for the node <b>180</b>, any number of nodes can be utilized, typically determined by the storage size and performance requirements of the particular database system. Alternatively, the distributed database system <b>160</b> can include only a single node, in which case the inter-nodal communication link <b>190</b> is not needed.</p>
  <p num="p-0046">In embodiments having more than one node, one of the nodes communicates directly with the DB server <b>130</b> via the communication link <b>154</b>. In this case, as shown in <figref idrefs="DRAWINGS">FIG. 1</figref>, the node <b>164</b> that communicates directly with the DB server <b>130</b> is referred to as the primary node. The nodes <b>170</b>, <b>174</b>, <b>180</b> in <figref idrefs="DRAWINGS">FIG. 1</figref>, referred to as secondary nodes, are not in direct communication with the DB server <b>130</b>, but communicate with the other nodes and with the primary node <b>164</b> via the inter-nodal communication link <b>190</b>. In other embodiments, multiple nodes <b>164</b>, <b>170</b>, <b>174</b>, <b>180</b> can be connected to the DB server <b>130</b> via the communication link <b>154</b>, up to a maximum of all the nodes. The internal components and functionality of the nodes are described in greater detail below.</p>
  <p num="p-0047"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a block diagram illustrating components or modules of the nodes of the distributed database system <b>160</b> shown in <figref idrefs="DRAWINGS">FIG. 1</figref>. Except as noted, all of the nodes <b>1</b>–N operate in the same manner and include the same elements. Therefore, the other nodes will not be described in detail. Each node can be a processor with a main memory. Alternatively, each node can be a computer with a main memory. The main memory can be segmented in multiple areas, physically or logically, as shown in <figref idrefs="DRAWINGS">FIG. 2</figref>.</p>
  <p num="p-0048">A significant portion of the database storage and retrieval can be shared by the nodes, thereby distributing the data storage and processing load approximately equally among the nodes. The database storage and retrieval can be performed concurrently in a substantially parallel fashion by the nodes, thereby significantly increasing the performance of the distributed database system <b>160</b>. In addition, the distributed database system <b>160</b> is easily expandable when additional performance is desired by simply adding nodes.</p>
  <p num="p-0049">The primary node <b>164</b> includes a database server interface processing module <b>220</b> that communicates with the DB server <b>130</b> via the communication link <b>154</b>. The database server interface processing module <b>220</b> transmits and receives data between the primary node <b>164</b> and the DB server <b>130</b> in conformance with the applicable communication protocol. The data received from the DB server <b>130</b> includes database commands and data to be stored by the distributed database system <b>160</b>, and the data transmitted to the DB server <b>130</b> includes the results of database query commands. In embodiments in which only the primary node <b>164</b> communicates directly with the DB server <b>130</b>, the secondary nodes <b>170</b>, <b>174</b>, <b>180</b> can be configured without the database server interface processing module <b>220</b>. Alternatively, the database server interface processing module <b>220</b> can be included but not used in such embodiments to maintain commonality between the primary node <b>164</b> and the secondary nodes <b>170</b>, <b>174</b>, <b>180</b>.</p>
  <p num="p-0050">The primary node <b>164</b> communicates with the secondary nodes <b>170</b>, <b>174</b>, <b>180</b> that are present in the distributed database system <b>160</b> via the inter-nodal communication link <b>190</b>, which is connected to a communication link interface module <b>210</b> that is in turn connected to a communication link <b>224</b>. In some embodiments, the inter-nodal link <b>190</b> and the communication link interface module <b>210</b> conform to the Scalable Coherent Interface (SCI) protocol as specified by the. Institute of Electrical and Electronics Engineers (IEEE) 1596 standard. Other types of communication interface links can also be used for the inter-nodal communication of the nodes <b>164</b>, <b>170</b>, <b>174</b>, <b>180</b> in the distributed database system <b>160</b>. The inter-nodal communication link <b>190</b> can be, for example, fiber optic, Ethernet, small computer system interface (SCSI), VersaModule Eurocard bus (VME), peripheral component interconnect (PCI), or universal serial bus (USB).</p>
  <p num="p-0051">The primary node <b>164</b> includes at least one processor represented by the dashed box <b>226</b> for performing at least some of the operations of the primary node <b>164</b>. The processor <b>226</b> can be a general-purpose single- or multi-chip processor, or a special purpose processor such as an application specific integrated circuit (ASIC). The processor <b>226</b> can include at least one actual or physical processor and at least one logical processing unit or task. For example, in some embodiments, the processor <b>226</b> can include two or more physical processors for performing the operations of at least four logical central processing units (LCPU's). In such an example, the four logical central processing units shown in <figref idrefs="DRAWINGS">FIG. 2</figref> are a first logical central processing unit (LCPU<b>1</b>) <b>230</b>, a second logical central processing unit (LCPU<b>2</b>) <b>240</b>, a third logical central processing unit (LCPU<b>3</b>) <b>250</b>, and a fourth logical central processing unit (LCPU<b>4</b>) <b>260</b>. Alternatively, each logical central processing unit can be implemented as a separate physical processor or different numbers of physical processors can be used to implement the logical central processing units and the modules depicted in <figref idrefs="DRAWINGS">FIG. 2</figref>. In addition, each logical central processing unit can be a virtual processor. The virtual processors can include programs, or programs not necessarily executing simultaneously.</p>
  <p num="p-0052">An LCPU<b>1</b> <b>230</b> and an LCPU<b>2</b> <b>240</b> communicate with one another via a processor communication link <b>244</b>. Similarly, the LCPU<b>1</b> <b>230</b> communicates with an LCPU<b>3</b> <b>250</b> via a processor communication link <b>254</b>. The LCPU<b>1</b> <b>230</b> additionally communicates with an LCPU<b>4</b> <b>260</b> via a processor communication link <b>264</b>. The LCPU<b>1</b> <b>230</b> communicates with the database server interface processing module <b>220</b> via a communication link <b>214</b>. In the example shown in <figref idrefs="DRAWINGS">FIG. 2</figref>, the LCPU<b>1</b> <b>230</b> and the LCPU<b>2</b> <b>240</b> can be executed by a first physical processor (not shown), and the LCPU<b>3</b> <b>250</b> and the LCPU<b>4</b> <b>260</b> can be executed by a second physical processor (not shown). The LCPU<b>1</b> <b>230</b> communicates with the DB server <b>130</b> through the database server interface processing module <b>220</b>. Other configurations of physical processors and logical processing units can be used, for example, with more or fewer physical processors and logical processing units.</p>
  <p num="p-0053">The LCPU<b>1</b> <b>230</b> can also be referred to as the controller <b>230</b> to indicate that in some embodiments it performs some or all of the management and control operations of the primary node <b>164</b>. The LCPU<b>1</b> <b>230</b> can additionally include the functionality of the database server interface processing module <b>220</b>. The LCPU<b>2</b> <b>240</b> can also be referred to as the Tstore<b>1</b> <b>240</b>, the LCPU<b>3</b> <b>250</b> can be referred to as the Tstore<b>2</b> <b>250</b>, and the LCPU<b>4</b> <b>260</b> can be referred to as the Tstore<b>3</b> <b>260</b>. The Tstore<b>1</b> <b>240</b>, the Tstore<b>2</b> <b>250</b>, and the Tstore<b>3</b> <b>260</b>, each store a portion of the total database information in response to database write commands and respond to database read or query commands.</p>
  <p num="p-0054">The Tstores <b>240</b>, <b>250</b>, <b>260</b> perform write operations, such as inserting, updating and deleting records from the Tstores' portion of the database. The Tstores <b>240</b>, <b>250</b>, <b>260</b> also perform read operations, such as receiving a database command or a data table, process data, and produce output based on the commands and the processing. For example, the Tstores <b>240</b>, <b>250</b>, <b>260</b> receive other Tstores' join tables, compare the join tables against the Tstore's own pivot table, and produce a corresponding intermediate results file. Although various types of data are described herein as being in the form of files, the data can also be in the form of data streams, raw data, or blocks of memory. The join tables, pivot tables, intermediate results files, and Tstore operation are described in more detail below.</p>
  <p num="p-0055">The LCPU<b>2</b> <b>240</b>, the LCPU<b>3</b> <b>250</b>, and the LCPU<b>4</b> <b>260</b> are referred to as logical processing units or logical CPUs to indicate that each can execute on a separate physical CPU, or that multiple logical CPUs can execute on a single physical CPU. The logical CPUs can be thought of as a collection of functionally related tasks. <figref idrefs="DRAWINGS">FIG. 2</figref> shows the LCPU<b>1</b> <b>230</b> communicating with the communication link interface module <b>210</b> to transmit and receive data via the inter-nodal communication link <b>190</b>. Alternatively, the primary node <b>164</b> can be configured so that any of the logical processing units LCPU<b>2</b> <b>240</b>, LCPU<b>3</b> <b>250</b>, or LCPU<b>4</b> <b>260</b> of the processor <b>226</b> communicates with the communication link interface module <b>210</b>.</p>
  <p num="p-0056">In <figref idrefs="DRAWINGS">FIG. 2</figref>, each of the logical CPUs, LCPU<b>1</b> <b>230</b>, LCPU<b>2</b> <b>240</b>, LCPU<b>3</b> <b>250</b>, and LCPU<b>4</b> <b>260</b>, have an associated storage area in the memory <b>270</b> of the node <b>164</b>. In some embodiments, the link between the processor <b>226</b> and the memory <b>270</b> can be the main memory bus of the processor, which provides high-speed memory data access. The LCPU<b>1</b> <b>230</b> stores data in an area of the memory <b>270</b> referred to as a storage area <b>1</b> <b>274</b>. The LCPU<b>2</b> <b>240</b> stores data in an area of the memory <b>270</b> referred to as a storage area <b>2</b> <b>280</b>. The LCPU<b>3</b> <b>250</b> stores data in an area of the memory <b>270</b> referred to as a storage area <b>3</b> <b>284</b>. The LCPU<b>4</b> <b>260</b> stores data in an area of the memory <b>270</b> referred to as a storage area <b>4</b> <b>290</b>.</p>
  <p num="p-0057">The storage area <b>1</b> <b>274</b>, the storage area <b>2</b> <b>280</b>, the storage area <b>3</b> <b>284</b>, and the storage area <b>4</b> <b>290</b> are shown in <figref idrefs="DRAWINGS">FIG. 2</figref> as separate, non-contiguous, non-overlapping areas for ease of illustration. However, the actual physical locations of the storage area <b>1</b> <b>274</b>, the storage area <b>2</b> <b>280</b>, the storage area <b>3</b> <b>284</b>, and the storage area <b>4</b> <b>290</b> may be contiguous or may overlap. Alternatively, there can be fewer or more data storage areas than those shown in <figref idrefs="DRAWINGS">FIG. 2</figref>. For example, there can be only one storage area that is shared by all the processors or tasks, or each processor or task can have multiple memory storage areas. In this example, the memory <b>270</b> is random access memory (RAM) such as static RAM (SRAM) or dynamic RAM (DRAM). However, other types of data storage can be utilized, such as flash memory or read-only memory (ROM). Alternatively, the data storage can be nonvolatile memory, or a combination of volatile memory and nonvolatile memory.</p>
  <p num="p-0058"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a block diagram illustrating an example of database table storage in the storage areas <b>280</b>, <b>284</b>, <b>290</b> shown in <figref idrefs="DRAWINGS">FIG. 2</figref>. As described above, each node can store a portion of the database information, preferably an approximately equal portion as for the other nodes. Typically, a database refers to a collection of data tables. By storing the database tables in approximately equal portions among the available nodes, the processing load of performing query commands is spread among the nodes, allowing for significantly faster database retrieval. In this way, database retrieval speed is improved as the nodes <b>164</b>, <b>170</b>, <b>174</b>, <b>180</b> process database query commands concurrently in a parallel fashion for the portion of the database table stored at each the respective nodes.</p>
  <p num="p-0059"> <figref idrefs="DRAWINGS">FIG. 3</figref> represents a single node example with three of the logical processing units storing two tables. In this example, the LCPU<b>2</b> <b>240</b>, LCPU<b>3</b> <b>250</b> and LCPU<b>4</b> <b>260</b> (see <figref idrefs="DRAWINGS">FIG. 2</figref>), and their associated memory storage areas, storage area <b>2</b> <b>280</b>, storage area <b>3</b> <b>284</b> and storage area <b>4</b> <b>290</b>, respectively, store Table A and Table B. Each of the tables is stored in approximately equal portions at each of the storage areas <b>280</b>, <b>284</b>, <b>290</b>. For example, approximately one-third of Table A is stored in each of the storage areas <b>280</b>, <b>284</b>, <b>290</b>, and likewise for Table B. Thus, as shown in <figref idrefs="DRAWINGS">FIG. 3</figref>, a first portion of Table A, denominated TableA/<b>3</b>-<b>1</b> <b>310</b>, is stored on the storage area <b>280</b>. Similarly, a second portion of Table A, denominated TableA/<b>3</b>-<b>2</b> <b>320</b>, is stored on the storage area <b>284</b>. A third portion of Table A, denominated TableA/<b>3</b>-<b>3</b> <b>330</b>, is stored on the storage area <b>290</b>. Likewise, a first portion of Table B, denominated TableB/<b>3</b>-<b>1</b> <b>340</b>, is stored on the storage area <b>280</b>. A second portion of Table B, denominated TableB/<b>3</b>-<b>2</b> <b>350</b>, is stored on the storage area <b>284</b>. A third portion of Table B, denominated TableB/<b>3</b>-<b>3</b> <b>360</b>, is stored in the storage area <b>290</b>.</p>
  <p num="p-0060">In an example having two nodes each with three logical processing units for a total of six logical processing units (not shown), each database table can be distributed among the storage areas associated with each Tstore in approximately equal one-sixth portions. Likewise, in a three-node example in which each node has three logical processing units for a total of nine logical processing units (not shown), each database table is distributed among the storage areas in approximately equal one-ninth portions. Thus, where N represents the total number of logical processing units, the database data and processing load is distributed in approximately equal 1/N portions. In other embodiments, the data is distributed in a non-uniform fashion in which the data are stored on the nodes or Tstores in unequal portions. For example, one table can be stored on a single Tstore, or one table can be stored on a group of Tstores. As additional examples, a group of related tables can be stored on a single Tstore, or a group of related tables can be stored on a group of Tstores. In addition, the data distribution can be a combination of uniform and non-uniform distribution. For example, certain tables can be distributed in one fashion, such as uniformly, while other tables in the same system can be distributed in another fashion, such as non-uniformly. In these embodiments, each Tstore stores some subset of the total database.</p>
  <p num="p-0061">In the uniform distribution embodiments, one way of distributing the database tables in approximately equal portions, for example, is round robin distribution, in which successive database write operations are directed to the individual Tstores one at a time in circular succession. In some embodiments, database write commands that involve writing a new record to a database table are performed in a round robin manner. Round robin distribution refers to writing successive new records to a database table such that the records are written in approximately equal portions in a sequential and circular fashion.</p>
  <p num="p-0062">For example, in a three-Tstore system, write commands could be sent to the Tstores according to round robin distribution in the following sequential order: Tstore <b>1</b>, Tstore <b>2</b>, Tstore <b>3</b>, Tstore <b>1</b>, Tstore <b>2</b>, Tstore <b>3</b>, etc. Of course, there are many other ways of distributing the records of the database tables to multiple Tstores or processing units in approximately equal amounts. In addition, although the number of nodes present in the distributed database system <b>160</b> and the number of Tstores present in each node can vary, in this example the distributed database system <b>160</b> distributes the database tables approximately equally over the total number of Tstores present.</p>
  <p num="p-0063"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a diagram illustrating a representation of storage of two database tables in a single node embodiment of the distributed database system <b>160</b>. Database tables can be visualized as having a row by column configuration. The columns of a database table represent the various categories of information that are capable of being stored for each record in the database table. Each row of a database table represents a record or an entry for which some or all of the column data can be stored. As a simple example, a database table can be configured to have three columns to store name, address and telephone number information for customers. Each row of the database table represents a list of the individual customers for which the name, address and telephone number information can be stored. In this example, if the user has 100 customers stored in the database, the corresponding database table would have 100 rows and three columns.</p>
  <p num="p-0064">In the single node, multiple Tstore embodiment, since the rows or records of the database tables are stored on multiple Tstores, the distributed database system <b>160</b> cannot search for all matching records in the database tables in a single step. One of the more interesting types of searches involves the intersection of two tables in which database records are stored. The process of searching and comparing multiple tables for matching records in the single node embodiment is described below with regard to <figref idrefs="DRAWINGS">FIGS. 5A</figref>, <b>5</b>B and <b>5</b>C. First, an overall, top-level description of the database table storage is provided.</p>
  <p num="p-0065">As shown in <figref idrefs="DRAWINGS">FIG. 4</figref>, in the single node embodiment having three Tstores, a database table, designated as Table A <b>460</b>, is stored in approximately equal portions at each of the three Tstores. The approximately one-third portions (in this example, 1/N, where N=3) are designated in <figref idrefs="DRAWINGS">FIG. 4</figref> as TableA-DB<b>1</b> <b>450</b>, TableA-DB<b>2</b> <b>454</b>, and TableA-DB<b>3</b> <b>458</b>. The Table A <b>460</b> is shown as a pie-shaped object to illustrate that the three portions of the Table A <b>460</b>, while stored separately at each of the Tstores, make up the entire database table when taken together as a whole. Similarly, a second database table, designated as Table B <b>480</b> in <figref idrefs="DRAWINGS">FIG. 4</figref>, is also stored in approximately equal one-third portions labeled TableB-DB<b>1</b> <b>470</b>, TableB-DB<b>2</b> <b>474</b>, and TableB-DB<b>3</b> <b>478</b>.</p>
  <p num="p-0066">In multi-node embodiments, or embodiments in which each node has more or fewer than three Tstores present, each database table is divided into a number of portions that represents the total number (N) of the Tstores present in all the nodes of the distributed database system <b>160</b>. For example, if the distributed database system <b>160</b> has two nodes each with three Tstores, the records of the database tables would be distributed as approximately equal one-sixth portions. Likewise, if the distributed database system <b>160</b> has three nodes each with four Tstores, the records of the database tables would be distributed as approximately equal one-twelfth portions.</p>
  <p num="p-0067"> <figref idrefs="DRAWINGS">FIG. 5A</figref> is a diagram illustrating a representation of an example of one phase of a database query command in the single node embodiment of the distributed database system <b>160</b>. In this two table example, the intersection of the tables, shown in <figref idrefs="DRAWINGS">FIG. 5A</figref> by the overlapping portion of the circles, represents the records in each of the tables that match the database query command. For example, such a database query command could be requesting a listing of all new customers (stored in the new customer Table A) who made purchases in the month of July (all July purchase records are stored in Table B). Therefore, the overlap or intersection of Tables A and B in <figref idrefs="DRAWINGS">FIG. 5A</figref> represents new customers who made purchases in July. To compile a table of records matching the search criteria, referred to as a results file, each of the three portions of the Table B <b>480</b> is compared in conjunction with each of the three portions of Table A <b>460</b>. <figref idrefs="DRAWINGS">FIG. 5A</figref> shows one phase of the three-phase search, while <figref idrefs="DRAWINGS">FIGS. 5B and 5C</figref> show the two additional phases.</p>
  <p num="p-0068">The queries can be conducted on the entire portion of each of the database tables, or a subset thereof having one or more fields or columns removed. For example, if the database query involves searching for customers having a certain name, the database tables that are searched and compared can be a single column subset of the full column table with the other fields removed and only the name field included. The database tables that are actually compared, whether the full tables or a subset thereof, are referred to as join tables. Therefore, the Table A <b>460</b> and the Table B <b>480</b> can represent join tables that are smaller than the full database tables in that one or more columns can be removed in producing the corresponding join tables. Pairing down the join tables to include only the data items necessary to perform the particular query command improves the performance of the distributed database system <b>160</b> by requiring less data to be transferred between the nodes and Tstores, as will become apparent as the query operation is described below. A database query can involve the generation and processing of multiple join tables. However, the following examples describe the case in which the database query involves a single join table.</p>
  <p num="p-0069">As shown in <figref idrefs="DRAWINGS">FIG. 5A</figref>, the portion TableB-DB<b>1</b> <b>470</b> of the Table B <b>480</b> is compared with each of the three portions of Table A to determine the records that are in all these portions matching the search criteria. The database table portion TableB-DB<b>1</b> <b>470</b> and the portion TableA-DB<b>1</b> <b>450</b>, both stored at the same Tstore, are compared for records present in both portions that match the search criteria. The portion TableB-DB<b>1</b> <b>470</b>, stored at one Tstore, and the portion TableA-DB<b>3</b> <b>458</b>, stored at another Tstore, are compared for records present in both portions that match the search criteria. The portion TableB-DB<b>1</b> <b>470</b> and the portion TableA-DB<b>2</b> <b>454</b>, stored at another Tstore, are compared for records present in both portions that match the search criteria.</p>
  <p num="p-0070">In this way, an intermediate results file compiled from the phase of the query in <figref idrefs="DRAWINGS">FIG. 5A</figref> includes records in the TableB-DB<b>1</b> <b>470</b> portion of the Table B <b>480</b> and the entire Table A <b>460</b> that match the search criteria. This intermediate results file is saved for combining with the other intermediate results files from the phases of <figref idrefs="DRAWINGS">FIGS. 5B and 5C</figref> to produce a single final results file having the records matching the query command for the whole database.</p>
  <p num="p-0071"> <figref idrefs="DRAWINGS">FIG. 5B</figref> is a diagram illustrating a representation of a second phase of a database query command in the single node embodiment of the distributed database system <b>160</b>. The portion TableB-DB<b>2</b> <b>474</b> of the Table B <b>480</b> is compared in combination with each of the three portions of the Table A <b>460</b> to determine the records that are in all these portions that match the search criteria. The database table portion TableB-DB<b>2</b> <b>474</b>, stored at one Tstore, and the portion TableA-DB<b>1</b> <b>450</b>, stored at another Tstore, are compared for records present in both portions that match the search criteria. The portion TableB-DB<b>2</b> <b>474</b> and the portion TableA-DB<b>3</b> <b>458</b>, stored at another Tstore, are compared for records present in both portions that match the search criteria. The portion TableB-DB<b>2</b> <b>474</b> and the portion TableA-DB<b>2</b> <b>454</b>, both stored at the same Tstore, are compared for records present in both portions that match the search criteria.</p>
  <p num="p-0072">In this way, the intermediate results file compiled from the phase of the query in <figref idrefs="DRAWINGS">FIG. 5B</figref> includes records in the TableB-DB<b>2</b> <b>474</b> portion of the Table B <b>480</b> and the entire Table A <b>460</b> that match the search criteria. This intermediate results file is saved for combining with the other intermediate results files from the phases of <figref idrefs="DRAWINGS">FIGS. 5A and 5C</figref> to produce the final results file.</p>
  <p num="p-0073"> <figref idrefs="DRAWINGS">FIG. 5C</figref> is a diagram illustrating a representation of a third phase of a database query command in the single node embodiment of the distributed database system <b>160</b>. The portion TableB-DB<b>3</b> <b>478</b> of Table B <b>480</b> is compared in combination with each of the three portions of Table A to determine the records that are in all these portions that match the search criteria. The database table portion TableB-DB<b>3</b> <b>478</b>, stored at one Tstore, and the portion TableA-DB<b>1</b> <b>450</b>, stored at another Tstore, are compared for records present in both portions that match the search criteria. The portion TableB-DB<b>3</b> <b>478</b> and the portion TableA-DB<b>3</b> <b>458</b>, both stored at the same Tstore, are compared for records present in both portions matching the search criteria. The portion TableB-DB<b>3</b> <b>478</b>, stored at one Tstore, and the portion TableA-DB<b>2</b> <b>454</b>, stored at another Tstore, are compared for records present in both portions that match the search criteria.</p>
  <p num="p-0074">In this way, the intermediate results file compiled from the phase of the query in <figref idrefs="DRAWINGS">FIG. 5C</figref> includes records in the TableB-DB<b>3</b> <b>478</b> portion of the Table B <b>480</b> and the entire Table A <b>460</b> matching the search criteria. This intermediate results file is saved for combining with the other intermediate results files from the phases of <figref idrefs="DRAWINGS">FIGS. 5A and 5B</figref> to produce the final results file. Once the three intermediate results files are produced as described in the example above, they can be combined into a final results file that includes those records that are present in all the portions of both Table A <b>460</b> and Table B <b>480</b> that match the search criteria. The combining of intermediate results files to build the final results file is referred to as gather processing. The distributed database system <b>160</b> returns the final results table to the requestor, for example, the database server <b>130</b> or directly to the user at the client computer system <b>110</b>.</p>
  <p num="p-0075">While the query shown in <figref idrefs="DRAWINGS">FIGS. 5A</figref>, <b>5</b>B and <b>5</b>C are for a single node system, queries for multinode systems can be performed in a similar fashion. In multinode queries, the database tables can be divided into a number of portions equal to the total number of Tstores present in the nodes of the distributed database system <b>160</b>. For example, for a two-node system having three Tstores per node, the database tables are divided into one-sixth (⅙) portions. In multinode embodiments, each of the join tables is sent to each of the other nodes for comparing by each of the remote Tstores with local join tables.</p>
  <p num="p-0076">In addition, <figref idrefs="DRAWINGS">FIGS. 5A</figref>, <b>5</b>B and <b>5</b>C show query commands for single node systems with three Tstores per node, but more or fewer Tstores can be present on each node. Still further, <figref idrefs="DRAWINGS">FIGS. 5A</figref>, <b>5</b>B and <b>5</b>C show comparing of two database tables. However, more than two tables can be compared in performing a query command. Regardless of the number of database tables involved in the query, the number of nodes present, or the number of Tstores on each node, the query command processing can be performed in a manner analogous to that shown in <figref idrefs="DRAWINGS">FIGS. 5A</figref>, <b>5</b>B and <b>5</b>C.</p>
  <p num="p-0077"> <figref idrefs="DRAWINGS">FIG. 6</figref> is a block diagram illustrating components or modules of the LCPU<b>1</b> <b>230</b> of the primary node <b>164</b> shown in <figref idrefs="DRAWINGS">FIG. 2</figref>. The LCPU<b>1</b> <b>230</b> of the primary node <b>164</b>, also referred to as the primary controller <b>230</b>, performs management and data transfer functions for the distributed database system <b>160</b> associated with executing the database write and query commands and returning the query results. These functions can include, but are not limited to, transmitting and receiving database commands and associated data from the DB server <b>130</b>, distributing database commands (including a join table definition specifying how to build the join tables) to the nodes <b>164</b>, <b>170</b>, <b>174</b>, <b>180</b> in the distributed database system <b>160</b>, receiving intermediate results files from the nodes <b>164</b>, <b>170</b>, <b>174</b>, <b>180</b>, building the final results file, and optionally performing post-processing operations on the results files.</p>
  <p num="p-0078">The primary controller <b>230</b> includes a shared memory router module <b>610</b>. The shared memory router module <b>610</b> operates to propagate data more efficiently among the nodes <b>164</b>, <b>170</b>, <b>174</b>, <b>180</b> and the LCPU<b>1</b> <b>230</b>, LCPU<b>2</b> <b>240</b>, LCPU<b>3</b> <b>250</b> and LCPU<b>4</b> <b>260</b> by reducing unnecessary or redundant copying of data. One example of redundant data copying that can be eliminated by the shared memory router <b>610</b> involves join tables that a Tstore sends to the other Tstores. For example, in the case where each node has three Tstores, the shared memory router <b>610</b> only sends one copy of the join table to each node, rather than three copies to each node for each of the three Tstores. The shared memory router at the receiving node makes the single copy of the join table available to all the Tstores on the node. The shared memory router <b>610</b> can also be used more generally to efficiently distribute data, jobs, or tasks among multiple processors in a clustered environment. The operation of the shared memory router <b>610</b> is described in greater detail below, for example, with regard to <figref idrefs="DRAWINGS">FIG. 13</figref>.</p>
  <p num="p-0079">In some embodiments, the Tstores have an inbound queue, or an outbound queue, or both. The shared memory router <b>610</b> can place input data (or a pointer to the data) for the Tstore in the Tstore's inbound queue for processing by the Tstore. In addition, the shared memory router <b>610</b> can send the Tstore's output data (or a pointer to the data) in the Tstore's outbound queue to other nodes or other Tstores.</p>
  <p num="p-0080">If the destination of a message is a remote Tstore, the shared memory router <b>610</b> sends a message with the data to the shared memory router at the node of the destination Tstore. Once the has been received at the destination node, the shared memory router of the destination node updates the memory pointer in the message to point to the local memory location, then queues up a message to the destination Tstore with the updated memory pointer. In addition to the reduction of sending and copying the same data multiple times, a further advantage of the shared memory router <b>610</b> is that the sender of messages does not need to worry about the location of the destination Tstore. The shared memory router <b>610</b> determines the location of the destination of the messages, thus abstracting the communication of message to a simple datagram interface. For example, the sender tells the shared memory router <b>610</b> to send a data message to a particular Tstore without knowing the Tstore's actual location.</p>
  <p num="p-0081">In some embodiments, the sender of the message indicates to the shared memory router <b>610</b> whether the message is to be a broadcast message sent to all nodes or Tstores in the distributed database system <b>160</b>, or a point-to-point message that is sent only to a particular destination or to multiple particular destinations. For example, the sender can indicate whether to broadcast the message or send it to a particular destination based on data included in the message to the shared memory router <b>610</b>. Alternatively, the shared memory router <b>610</b> can determine whether to send the message as a broadcast message or a point-to-point message, for example, based on the type of message being sent.</p>
  <p num="p-0082">The shared memory router <b>610</b> includes a node interface processing module <b>614</b>. The node interface processing module <b>614</b> communicates with the communication link interface module <b>210</b> (see <figref idrefs="DRAWINGS">FIG. 2</figref>) for transferring data between the primary node <b>164</b> and one or more of the secondary nodes <b>170</b>, <b>174</b>, <b>180</b>. Examples of this inter-nodal data, which are described below, include join table definitions, the actual join tables themselves, and the intermediate results files. The node interface processing module <b>614</b> acts as an interface between the communication link interface module <b>210</b> and the modules of the primary controller <b>230</b>. In some embodiments of the primary controller <b>230</b>, the node interface processing module <b>614</b> acts as the interface between the communication link interface module <b>210</b> and various modules of the primary controller <b>230</b> via the communication link <b>224</b> as shown in <figref idrefs="DRAWINGS">FIG. 6</figref>.</p>
  <p num="p-0083">The shared memory router <b>610</b> also includes an intermediate results file receipt module <b>618</b> for receiving the intermediate results files from each of the secondary nodes <b>170</b>, <b>174</b>, <b>180</b> for each Tstore and from the Tstores on the primary node <b>164</b>. The intermediate results files receipt module <b>618</b> stores the intermediate results files for subsequent processing as described herein. In some embodiments, the commands and associated data sent from the primary controller <b>230</b> to the other nodes and the responses received from the other nodes by the primary controller <b>230</b> via the inter-nodal link <b>190</b> can be low-level, non-standard commands. In other embodiments, the commands and associated data sent from the primary controller <b>230</b> to the other nodes and the responses received from the other nodes by the primary controller <b>230</b> via the inter-nodal link <b>190</b> can be standard SQL commands. Therefore, each Tstore includes a database command processing module <b>820</b> (see <figref idrefs="DRAWINGS">FIG. 8</figref>) to parse SQL commands. These latter embodiments utilizing standard SQL commands are sometimes referred to as loosely-coupled architectures. In one example of a loosely-coupled architecture, all of the Tstores are configured to follow a particular communication protocol or job description language. Each Tstore, however, can have different hardware or software than one or more of the other Tstores, so long as each Tstore supports the same communication protocol or job description language.</p>
  <p num="p-0084">The primary controller <b>230</b> additionally includes a front end processor (FEP) <b>620</b>. The FEP <b>620</b> of the primary controller <b>230</b>, also referred to as the primary FEP <b>620</b>, performs processing of incoming data for the primary node <b>164</b>, determines the pivot table, builds the final results file having the matching records for the database query command, and optionally performs post processing operations on the intermediate or final results files.</p>
  <p num="p-0085">The primary FEP <b>620</b> includes a table distribution processing module <b>624</b>, which receives database write commands from the database server interface processing module <b>220</b> (see <figref idrefs="DRAWINGS">FIG. 2</figref>) via the communication link <b>214</b>. For example, the database write commands can include commands to add new records to a database table or update data in an existing database table record. For new records, the table distribution processing module <b>624</b> additionally determines the particular node and/or Tstore to which the record is to be stored. In some embodiments, the table distribution processing module <b>624</b> determines the particular node that is to store the updated data and the particular node determines which Tstore on that node is to store the new record. Alternatively, the table distribution processing module <b>624</b> can determine the individual Tstore on a particular node that is to store the new record. The table distribution processing module <b>624</b> additionally transmits the identification of the target node and/or Tstore to the node interface processing module <b>614</b> for transmittal to the target node or Tstore.</p>
  <p num="p-0086">With regard to commands to write new records to a database table, one way the table distribution processing module <b>624</b> can determine the node and/or Tstore to store the new record is to distribute the records on an approximately equal basis by a round robin distribution process. The round robin processing of the table distribution processing module <b>624</b> is further described below, for example, with regard to <figref idrefs="DRAWINGS">FIG. 10</figref>. Alternatively, for updated records, the table distribution processing module <b>624</b> transmits a broadcast message via the node interface processing module <b>614</b>. A broadcast message is a message that is sent once and is able to be received by all the nodes that are present in the distributed database system <b>160</b>. By sending a broadcast message, the node on which the record to be updated is stored receives the data and the corresponding Tstore updates the record accordingly. The other nodes that do not include the Tstore that is storing the updated record simply ignore the broadcast message.</p>
  <p num="p-0087">The primary FEP <b>620</b> additionally includes a pivot table processing module <b>628</b> for receiving a database query command and determining the database table to be the pivot table for the query. When a query command involving multiple database tables is performed, one of the join tables does not need to be sent to the other Tstores for query processing. In the two-table example, only one of Table A <b>460</b> or Table B <b>480</b> is sent to the other Tstores for comparing by each of the Tstores. The table that is not sent to the other Tstores is referred to as the pivot table. In a three-table example, only two tables are sent to the other Tstores. In some embodiments, to increase performance of the query processing, the pivot table processing module <b>628</b> determines that the smaller of the database tables are sent to the other Tstores, and the largest table is the one that is not sent (the pivot table). However, in other embodiments, different parameters besides performance can be the determining factors, such that another table besides the largest can be selected as the pivot table. In some embodiments, the pivot table processing module <b>628</b> can select the pivot table by maintaining and using tables that indicate, for example, where each database table is stored, the size of each table, and the row/column configuration of each table. In some embodiments, the pivot table processing module <b>628</b> can identify the pivot table to the Tstores in the format of one or more SQL commands. One way to implement these embodiments is for the pivot table processing module <b>628</b> to send SQL query commands to the Tstores telling them which tables to compare. By telling the Tstores to compare the same table to other tables, that same table becomes the pivot table.</p>
  <p num="p-0088">The primary FEP <b>620</b> further includes a final results file delivery module <b>622</b>. The final results file delivery module <b>622</b> sends the final results file of the database query command to the requester, for example, the DB server <b>130</b> or alternatively directly to the user at the client computer system <b>110</b>. The final results file is the single file that represents the final results of the query command based on the current contents of the applicable database table. The building of the final results file, which is referred to as gather processing, is described below.</p>
  <p num="p-0089">The primary controller <b>230</b> additionally includes an internode processing module <b>640</b> for processing and transferring data received by or produced on the primary node <b>164</b> to other Tstores and other nodes. The internode processing module <b>640</b> includes a query distribution processing module <b>644</b> for sending the query command to the secondary nodes <b>170</b>, <b>174</b>, <b>180</b> and to the Tstores of the primary node <b>164</b>. The query command from the primary node <b>164</b> to the secondary nodes <b>170</b>, <b>174</b>, <b>180</b> can be a SQL query command, as is the query command from the DB server <b>130</b> to the primary node <b>164</b>. Alternatively, the query command sent to the secondary nodes <b>170</b>, <b>174</b>, <b>180</b> can be in a non-SQL format, such as a proprietary query format. To distinguish between the potentially different query command formats, the query command to the primary node <b>164</b> is referred to as a primary query command or a primary query. In addition, the query command to the secondary nodes <b>170</b>, <b>174</b>, <b>180</b> is referred to as a secondary query command or a secondary query. In other embodiments, the primary query and the secondary query can be jobs, tasks or operations, often including associated data, for distributed processing.</p>
  <p num="p-0090">One example of different primary and secondary queries is when a part or the whole primary query cannot be accomplished in a distributed manner. For example, if the primary query is to select the value that occurs most frequently in a particular field of a database table, each Tstore in a distributed system can only calculate the most frequently occurring value from the tables stored locally on that Tstore. In this example, the secondary query is to return all values for the field to the primary node. The primary node calculates the most frequently occurring value for the field of the distributed table. In other embodiments, the secondary query can be of the same format as the primary query, or the secondary query can be identical to the primary query. References herein to the query command without indicating whether the primary or secondary query refers to the primary query.</p>
  <p num="p-0091">The query distribution processing module <b>644</b> determines how each Tstore is to build the join table that is compared for records matching the search criteria. In some embodiments, the query distribution processing module <b>644</b> can determine how each Tstore is to build the join table by maintaining internal tables that indicate, for example, where each database table is stored, the size of each table, and the row/column configuration of each table. The query distribution processing module <b>644</b> sends to the secondary nodes <b>170</b>, <b>174</b>, <b>180</b> the join table definition, which includes information on how to build the join table, via the node interface processing module <b>614</b>. In addition, the query distribution processing module <b>644</b> sends to the secondary nodes <b>170</b>, <b>174</b>, <b>180</b> the identification of the table that is designated as the pivot table for the query command as determined by the pivot table processing module <b>628</b>.</p>
  <p num="p-0092">The internode processing module <b>640</b> also includes a join table send processing module <b>648</b> for sending the join tables built by the Tstores to other Tstores. In some embodiments, the join tables are sent via the node interface processing module <b>614</b> of the shared memory router <b>610</b>, over the communication link <b>224</b> to the other nodes for performing the query command.</p>
  <p num="p-0093">The primary controller <b>230</b> additionally includes a results file processing module <b>630</b> for building the final results files for the query command from the intermediate results files generated by the Tstores. The results file processing module <b>630</b> includes an intermediate results file processing module <b>632</b> for accessing the intermediate results files stored by the intermediate results files receipt module <b>618</b> as described above. The results file processing module <b>630</b> additionally includes a final results file build processing module <b>634</b> for processing the intermediate results files and building the single, final results file that represents the final results of the query command based on the current contents of the applicable database tables. Building the final results file from the multiple intermediate results files is referred to as gather processing.</p>
  <p num="p-0094">The intermediate or final results files can optionally be examined by a final results file post-processing module <b>638</b> of the primary FEP <b>620</b> to perform post-processing operations or analysis of the results files. Post-processing can involve operations on, or analysis of, the results files. For example, the query command can require that only unique instances of the records satisfying the query be returned. The individual Tstores are not able to perform this function as each Tstore only has access to its own intermediate results file, not the intermediate results files of other Tstores. Thus, in this example, the final results file post-processing module <b>638</b> scans the final results file produced by the final results file delivery module <b>622</b> and removes any duplicate entries that may exist. The final results file, either after post-processing or without any post processing, is sent to the requester, for example, the DB server <b>130</b> or alternatively directly to the user at the client computer system <b>1</b> <b>10</b>.</p>
  <p num="p-0095"> <figref idrefs="DRAWINGS">FIG. 7</figref> is a block diagram illustrating components or modules of an example controller <b>700</b> of the secondary nodes <b>170</b>, <b>174</b>, <b>180</b> shown in <figref idrefs="DRAWINGS">FIG. 2</figref>. The controller <b>700</b> is analogous to the primary controller <b>230</b> shown in <figref idrefs="DRAWINGS">FIG. 2</figref>, but for the secondary nodes <b>170</b>, <b>174</b>, <b>180</b>. The controller <b>700</b>, also referred to as the secondary controller <b>700</b>, performs management and data transfer functions for the secondary nodes <b>170</b>, <b>174</b>, <b>180</b> associated with executing the database write and query commands and returning the query results. These management functions can include, but are not limited to, receiving database commands and associated data from the primary controller <b>230</b>, sending join tables to other Tstores, and sending intermediate results files from each Tstore on the secondary nodes <b>170</b>, <b>174</b>, <b>180</b> to the primary controller <b>230</b>.</p>
  <p num="p-0096">The secondary controller <b>700</b> includes a shared memory router <b>704</b>. The shared memory router of the secondary controller <b>700</b> operates to propagate data more efficiently among the nodes <b>164</b>, <b>170</b>, <b>174</b>, <b>180</b> and the LCPU<b>1</b> <b>230</b>, LCPU<b>2</b> <b>240</b>, LCPU<b>3</b> <b>250</b> and LCPU<b>4</b> <b>260</b> by reducing unnecessary or redundant copying of data. In some embodiments, the shared memory router <b>704</b> of the secondary nodes and the shared memory router <b>610</b> of the primary node can be interchangeable components that are configured to perform different, additional or fewer functions depending on whether executing on a secondary node or the primary node. The shared memory router <b>704</b> includes a node interface processing module <b>710</b>. The node interface processing module <b>710</b> communicates with the communication link interface module <b>210</b> (see <figref idrefs="DRAWINGS">FIG. 2</figref>) for transferring data between the secondary nodes <b>170</b>, <b>174</b>, <b>180</b> and the primary node <b>164</b>. Examples of this inter-nodal data include the database write and query commands, the join tables, and the intermediate results files. The node interface processing module <b>710</b> acts as an interface between the communication link interface module <b>210</b> and the modules of the secondary controller <b>700</b>.</p>
  <p num="p-0097">The shared memory router <b>704</b> additionally includes a table receipt and storage processing module <b>730</b> for receiving database write commands to update existing records and add new records to database tables. The table receipt and storage processing module <b>730</b> additionally stores the updated data to the appropriate location in memory for the affected database table.</p>
  <p num="p-0098">The secondary controller <b>700</b> also includes a front end processor <b>724</b>, also referred to as the secondary FEP <b>724</b>. In some embodiments, the secondary FEP <b>724</b> and the primary FEP <b>620</b> can be interchangeable components that are configured to perform different, additional or fewer functions depending on whether executing on a secondary node or the primary node. The secondary FEP <b>724</b> includes a query command receipt processing module <b>720</b> for receiving query commands and associated join table definition data from the primary controller <b>230</b>. The query command receipt processing module <b>720</b> makes available the join table definition data for access by the Tstores in building the join tables.</p>
  <p num="p-0099">The secondary controller <b>700</b> also includes an outgoing processing module <b>744</b> for processing and transferring data produced on the local node to other Tstores or nodes. In some embodiments, the outgoing processing module <b>744</b> on the secondary node and the internode processing module <b>640</b> on the primary node can be interchangeable components that are configured to perform different, additional or fewer functions depending on whether executing on the secondary node or the primary node. The outgoing processing module <b>744</b> includes a join table send processing module <b>740</b> for sending the join tables built by the Tstores. The join tables are sent to the other Tstores for performing the query command as described above.</p>
  <p num="p-0100">The outgoing processing module <b>744</b> further includes an intermediate results file send processing module <b>750</b>. The intermediate results files send processing module <b>750</b> reads results file data from memory for each of the local Tstores of the secondary node and sends them to the primary node <b>164</b> for gather processing into the final results file. Alternatively, the gather processing of the intermediate results files of the local Tstores can be performed by each of the secondary controllers <b>700</b> instead of by the primary controller <b>230</b>. In such embodiments, each node sends a single intermediate results file for that node, rather than sending intermediate results files for each Tstore. The primary controller <b>230</b> in this example performs gather processing on each node's results file to produce the final results file.</p>
  <p num="p-0101"> <figref idrefs="DRAWINGS">FIG. 8</figref> is a block diagram illustrating components or modules of the logical processors LCPU<b>2</b> <b>240</b>, LCPU<b>3</b> <b>250</b>, and LCPU<b>4</b> <b>260</b> of the node <b>164</b> shown in <figref idrefs="DRAWINGS">FIG. 2</figref>. In certain embodiments of the distributed database system <b>160</b>, the Tstores executing on the logical processors perform the actual join table comparing illustrated in <figref idrefs="DRAWINGS">FIGS. 5A</figref>, <b>5</b>B, and <b>5</b>C. For ease of illustration and description, the logical processor shown in <figref idrefs="DRAWINGS">FIG. 8</figref> is labeled as the LCPU<b>2</b> <b>240</b>. However, each of the LCPU<b>2</b> <b>240</b>, the LCPU<b>3</b> <b>250</b>, and the LCPU<b>4</b> <b>260</b> can operate in the same manner and can include the same components or modules.</p>
  <p num="p-0102">The LCPU<b>2</b> <b>240</b> includes a controller/Tstore interface processing module <b>810</b> for communicating with the controller and with the other Tstores on the node. In embodiments of the primary controller <b>230</b> that utilize the shared memory router module <b>610</b>, the controller/Tstore interface processing module <b>810</b> can be replaced with a SMR interface processing module (not shown) that handles the interface data between the Tstores and the shared memory router <b>610</b>.</p>
  <p num="p-0103">The LCPU<b>2</b> <b>240</b> additionally includes a database (DB) command processing module <b>820</b>. Database commands include write commands, for example, to write a new record to a database table or update the data in an existing record, and query commands. In the case of database write commands, a write command processing module <b>830</b> receives the data to be written to the database, and writes the data to the appropriate database table in memory.</p>
  <p num="p-0104">The LCPU<b>2</b> <b>240</b> includes a join table compare and build processing module <b>840</b> for processing database query commands. Upon receipt of a database query command, the join table compare and build processing module <b>840</b> builds join tables for use by the other local Tstores and for sending to the other nodes for use by the remote Tstores in carrying out the query command. In some embodiments, the primary FEP <b>620</b> determines how the join table compare and build processing module <b>840</b> is to build the join tables. For example, the primary FEP <b>620</b> can specify that certain columns of the database tables that are not involved in the query processing are to be removed in building the join tables. This results in increased efficiency and performance in performing the query command by not sending unused data in the database tables to the other Tstores in the join tables.</p>
  <p num="p-0105">The join table compare and build processing module <b>840</b> also compares other Tstores' join tables to the portion of the pivot table that is stored in the Tstore in which the join table compare and build processing module <b>840</b> is executing. The join table compare and build processing module <b>840</b> compares its portion of the pivot table and the join tables of other Tstores for database table records that match the search criteria specified in the query command. The join table compare and build processing module <b>840</b> generates and sends the intermediate results file to the primary controller <b>230</b> for gather processing to build the final results file as described above.</p>
  <p num="p-0106"> <figref idrefs="DRAWINGS">FIG. 9</figref> is a flowchart illustrating a database command process <b>1000</b> which can be performed by the table distribution processing module <b>624</b> shown in <figref idrefs="DRAWINGS">FIG. 6</figref>. The database command process <b>1000</b> processes database commands, for example, read and write commands, and initiates the execution of the commands. The database command process <b>1000</b> begins at a start block <b>1010</b>. The database command process <b>1000</b> continues to a block <b>1020</b> where the table distribution processing module <b>624</b> receives the incoming database commands and identifies the database command and the associated data. The database command process <b>1000</b> continues to a decision block <b>1030</b> where the table distribution processing module <b>624</b> determines whether the incoming database command is a read command or a write command, as the processing of the database commands varies based on the type of command.</p>
  <p num="p-0107">If the table distribution processing module <b>624</b> determines at the decision block <b>1030</b> that the database command is a write command, the database command process <b>1000</b> continues to a decision block <b>1032</b> where the table distribution processing module <b>624</b> determines if the write command is to write a new record to the database table or update an existing record in the database table. If the table distribution processing module <b>624</b> determines at the decision block <b>1032</b> that the write command is to update an existing record, the table distribution processing module <b>624</b> continues to a block <b>1050</b> where the table distribution processing module <b>624</b> sends a broadcast command to all the nodes to update the existing record. Each node receives the broadcast command, but only the node on which the record to be updated is stored updates the record with the updated data.</p>
  <p num="p-0108">If the table distribution processing module <b>624</b> determines at the decision block <b>1032</b> that the write command is to write a new record, the database command process <b>1000</b> continues to a block <b>1034</b> where the table distribution processing module <b>624</b> determines the target node for the write command. In some embodiments, the table distribution processing module <b>624</b> determines the target node by a round robin distribution. Round robin distribution refers to approximately equally distributing the write commands sequentially among the nodes until the last node is reached, at which point the next write command is sent to the first node and the process continues. A simple illustrative example involves a distributed database system with three nodes, in which write commands are sent to the nodes in the following order: Node <b>1</b>, Node <b>2</b>, Node <b>3</b>, Node <b>1</b>, Node <b>2</b>, Node <b>3</b>, Node <b>1</b>, etc. The round robin process is described below with regard to <figref idrefs="DRAWINGS">FIG. 10</figref>. In other embodiments, the table distribution processing module <b>624</b> determines the target Tstore instead of the target node. In other words, the table distribution processing module <b>624</b> can determine the specific Tstore on each node to receive the write command, for example, on a round robin basis. In addition, the table distribution processing module <b>624</b> can determine the target node in any of a number of ways in which the new records are distributed to the nodes and Tstores in approximately equal portions.</p>
  <p num="p-0109">The database command process <b>1000</b> continues to a block <b>1040</b> where the table distribution processing module <b>624</b> forwards the command to write a new record to the target node as determined by the block <b>1034</b>. The table distribution processing module <b>624</b> forwards the write command by sending the command to the target node via the inter-nodal communication link <b>190</b>. The target node stores the data associated with the new record in local memory on the target node for incorporation into the database table.</p>
  <p num="p-0110">If the table distribution processing module <b>624</b> determines at the decision block <b>1030</b> that the database command is a read command such as a query, the database command process <b>1000</b> continues to a decision block <b>1060</b> where the table distribution processing module <b>624</b> determines whether the read command is for a single set query or a multiple set query. If the table distribution processing module <b>624</b> determines at the decision block <b>1060</b> that the read command is for a single set query, the database command process <b>1000</b> continues to a block <b>1070</b> where the table distribution processing module <b>624</b> processes the single set query. A single set query command is a query that involves accessing only a single database table to perform the query. For example, an example of a single set query is to return all occurrences of the last name “Jones” in a customer list database table. To perform such a query command, only a single database table, the customer list table, needs to be accessed and compared. The single set query processing of the block <b>1070</b> is described in greater detail below with regard to <figref idrefs="DRAWINGS">FIG. 11</figref>.</p>
  <p num="p-0111">If the table distribution processing module <b>624</b> determines at the decision block <b>1060</b> that the read command is for a multiple set read, the database command process <b>1000</b> continues to a block <b>1080</b> where the table distribution processing module <b>624</b> processes the multiple set query command. A multiple set query command is a query that involves accessing multiple database tables to perform the query. One example of a multiple set query is to return all occurrences of the last name “Jones” in both a customer list database table and a delinquent account database table. To perform such a query command, multiple database tables, e.g., the customer list table and the delinquent account database table, are accessed and compared in order to return those records that are in both tables. The multiple set query processing of the block <b>1080</b> is described in greater detail below with regard to <figref idrefs="DRAWINGS">FIG. 12</figref>. The database command process <b>1000</b> terminates at an end block <b>1090</b>.</p>
  <p num="p-0112"> <figref idrefs="DRAWINGS">FIG. 10</figref> is a flowchart illustrating a process of determining the target node in the block <b>1034</b> of <figref idrefs="DRAWINGS">FIG. 9</figref> as performed by the table distribution processing module <b>624</b> of the primary FEP <b>620</b> shown in <figref idrefs="DRAWINGS">FIG. 6</figref>. The target determination process <b>1034</b> shown in <figref idrefs="DRAWINGS">FIG. 10</figref> is an embodiment of a round robin distribution process. Numerous other distribution processes can be implemented that result in approximately equal distribution of the database table records. The target determination process <b>1034</b> begins at a start block <b>1110</b>. The target determination process <b>1034</b> continues to a block <b>1120</b> where it determines to which node the last new record of the database table was written. One way of determining the last node is to refer to the nodes by a unique index number, and storing the node index number in memory after each write of a new record to the database table. When a subsequent new record write command is received, this stored node index number becomes the last node written to. While the round robin distribution process described for <figref idrefs="DRAWINGS">FIG. 10</figref> distributes write commands among nodes, other embodiments distribute write commands among Tstores.</p>
  <p num="p-0113">The target determination process <b>1034</b> continues to a block <b>1130</b> where it increments the node index to indicate the node to receive the next new record being written to the particular database table. In round robin distribution, each successive new record is written to the next node in the sequence of nodes. The target determination process <b>1034</b> continues to a decision block <b>1140</b> where it determines whether the current node index is greater than the total number of nodes (represented by a maximum node number ‘N’) present in the distributed database system <b>160</b>.</p>
  <p num="p-0114">If the target determination process <b>1034</b> determines at the decision block <b>1140</b> that the current node index is greater than the maximum node number ‘N’, the target determination process <b>1034</b> continues to a block <b>1150</b> where it resets the node index to refer to the first node present in the distributed database system <b>160</b>. For example, the index of the first node can be the number 1. If the target determination process <b>1034</b> determines at the decision block <b>1140</b> that the current node index is not greater than the maximum node number ‘N’, or after the block <b>1150</b>, the target determination process <b>1034</b> continues to a block <b>1160</b> where it sets the target node index to the value of the current node index. The target node index indicates the next node to which the new record write command is written. The target determination process <b>1034</b> terminates at an end block <b>1190</b>.</p>
  <p num="p-0115"> <figref idrefs="DRAWINGS">FIG. 11</figref> is a flowchart illustrating a single set query process <b>1070</b> as performed by the distributed database system <b>160</b> shown in <figref idrefs="DRAWINGS">FIG. 1</figref>. A single set query is a query command that involves accessing and comparing only a single database table to perform the query command. The single set query process <b>1070</b> begins at a start block <b>1210</b>. The single set query process <b>1070</b> continues to a block <b>1220</b> where it forwards the secondary query command to the node or nodes for processing. As described above, the secondary query command corresponds to the primary query command received from the DB server <b>130</b>, and can be the same as the primary query command, or it can be modified, for example, to conform to a different protocol than the primary query command. For example, the primary query command received from the DB server <b>130</b> can be a SQL command, and the secondary query command can be in a proprietary query command protocol. Alternatively, the primary query command received from the DB server <b>130</b> and the secondary query command can both be SQL commands. In some embodiments, the processing at the block <b>1220</b> is performed by the primary controller <b>230</b> (see <figref idrefs="DRAWINGS">FIG. 2</figref>). Alternatively, this can be performed by the shared memory controller <b>610</b>, <b>704</b> as described above.</p>
  <p num="p-0116">The single set query process <b>1070</b> continues to a block <b>1230</b> where it processes the query command as performed by each node present in the distributed database system <b>160</b>. In other embodiments, the processing of the query command at the block <b>1230</b> can be performed by each Tstore present at each node in the distributed database system <b>160</b>. The query command processing at the block <b>1230</b> can include each Tstore at each node comparing the portion of the database table stored by the Tstore for records that match the search criteria specified in the query command. The query command processing at the block <b>1230</b> also can include each Tstore generating an intermediate results file that includes the matching records.</p>
  <p num="p-0117">The single set query process <b>1070</b> continues to a block <b>1240</b> at which the nodes or Tstores send the intermediate results files generated at each node to the primary node <b>164</b> via the inter-nodal communication link <b>190</b>. In some embodiments, the Tstores send the intermediate results files to the primary node via the shared memory router <b>610</b>, <b>704</b>. The single set query process <b>1070</b> continues to a block <b>1250</b> where it performs gather processing, such as by the primary LCPU<b>1</b> <b>230</b>. As described above, gather processing involves building a single results file, referred to as the final results file, by combining the multiple intermediate results files received from each of the nodes as generated by each of the Tstores. The final results file includes the matching records that are identified in the multiple intermediate results files that are generated and sent by each of the nodes or Tstores.</p>
  <p num="p-0118">The single set query process <b>1070</b> continues to a decision block <b>1260</b> where it determines whether to perform post-processing operations on the final results file. While <figref idrefs="DRAWINGS">FIG. 11</figref> shows the post-processing operations being performed on the final results file, post-processing can also be performed on the intermediate results files prior to the gather processing. Post-processing operations can include such processing as removing certain duplicative records from the results files so that all matching records are unique. If the single set query process <b>1070</b> determines at the decision block <b>1260</b> that post processing is to be performed, the single set query process <b>1070</b> continues to a block <b>1270</b> where it performs the post-processing operations. After the block <b>1270</b>, or if the decision block <b>1260</b> determines that post processing is not to be performed, the single set query process <b>1070</b> continues to a block <b>1280</b> where it returns the final results file to the requestor as a single response file. The single set query process <b>1070</b> terminates at an end block <b>1290</b>.</p>
  <p num="p-0119"> <figref idrefs="DRAWINGS">FIG. 12</figref> is a flowchart illustrating a multiple set query process <b>1080</b> as performed by the distributed database system <b>160</b> shown in <figref idrefs="DRAWINGS">FIG. 1</figref>. A multiple set query is a query command that involves accessing and comparing two or more database tables to perform the query command. The multiple set query process <b>1080</b> begins at a start block <b>1310</b>. Continuing at a block <b>1320</b>, the pivot table processing module <b>628</b> of the primary FEP <b>620</b> determines the pivot table. As described above, the primary FEP <b>620</b> can select the pivot table by maintaining internal tables that indicate, for example, where each database table is stored, the size of each table, and the row/column configuration of each table. In some embodiments, the pivot table in multiple set queries is the database table that is kept local on each node, in contrast to the join tables which are sent to the other nodes for performing the query command. The pivot table can be the database table to be compared that is the largest in size. By selecting the largest sized table as the database table to be kept local, the overall performance of the distributed database system <b>160</b> is increased by reducing the amount of data transferred over the inter-nodal communication link <b>190</b>.</p>
  <p num="p-0120">Continuing at a block <b>1324</b>, the primary controller <b>230</b> sends a broadcast message to the other Tstores in the distributed database system <b>160</b> specifying the parameters to be used to generate the join tables. The broadcast message is a single message sent to multiple destinations via the inter-nodal communication link <b>190</b>. At a block <b>1326</b>, the secondary node receives the secondary query, which can include the parameters to be used to generate the join tables. In some embodiments, the shared memory router <b>704</b> receives the secondary query and makes the corresponding data available to the local Tstores on the node. Once the Tstores receive the secondary query data, at the block <b>1330</b> each Tstore builds its join table(s), which will be transmitted to the other Tstores. The secondary query, which can be a SQL query, determines the compare processing that the Tstores perform.</p>
  <p num="p-0121">The multiple set query process <b>1080</b> continues to a block <b>1334</b> at which each Tstore sends its join table(s) to the other Tstores present in the distributed database system <b>160</b> via the inter-nodal communication link <b>190</b>. In some embodiments, the Tstores send the join tables to other Tstores via the shared memory router <b>610</b> (or the shared memory router <b>704</b> of a secondary node) as described above. The shared memory router <b>610</b>, <b>704</b> receives the incoming join tables and stacks the join tables in its associated memory for access by the Tstores on the node.</p>
  <p num="p-0122">At a block <b>1340</b>, in accordance with the secondary query, each Tstore compares the pivot table to the Tstore's own join table to determine the records, if any, in both tables that satisfy the secondary query. At a block <b>1344</b>, each Tstore compares the pivot table to the join tables of the other local Tstores that may be present on the same node to determine records in both tables, if any, that satisfy the secondary query. The comparisons at the block <b>1340</b> and the block <b>1344</b> can be performed while the node is waiting to receive join tables from the Tstores on other nodes that may be present in the distributed database system <b>160</b>. At a block <b>1350</b>, each Tstore processes the join tables from the Tstores of other nodes. In one embodiment, the shared memory router <b>610</b> (or the shared memory router <b>704</b> of a secondary node) makes the received join tables available to its local Tstores in a first in/first out queue.</p>
  <p num="p-0123">Once at least one of the join tables are received from another Tstore, at a block <b>1354</b> the Tstore compares the pivot table to the join tables received from the other Tstores to determine the records from both the database tables, if any, satisfy the secondary query. The multiple set query process <b>1080</b> continues to a block <b>1360</b> where each Tstore builds its intermediate results file. Each intermediate results file includes the records that satisfy the secondary query as determined by each Tstore by comparing each of the join tables with that Tstore's portion of the pivot table.</p>
  <p num="p-0124">The multiple set query process <b>1080</b> continues to a block <b>1364</b> where each node sends the intermediate results file for each of the local Tstores present on the node to the primary node <b>164</b> via the inter-nodal communication link <b>190</b>. In some embodiments, the shared memory router <b>704</b> of a secondary node receives a pointer to the memory with the intermediate results file from each Tstore at the node. The shared memory router <b>704</b> transmits the intermediate results file via the inter-nodal communication link <b>190</b> to the shared memory router <b>610</b> on the primary node <b>164</b> for receipt by the intermediate results file receipt module <b>618</b>. The shared memory router <b>610</b> makes the received intermediate results files available to the primary FEP <b>620</b> for gather processing by the final results file delivery module <b>622</b>.</p>
  <p num="p-0125">Continuing to a block <b>1370</b>, the primary FEP <b>620</b> performs gather processing on the intermediate results files. Gather processing can include building a final results file by combining the multiple intermediate results files received from each of the nodes as generated by each of the Tstores. The final results file includes the records that satisfy the secondary query as identified in the multiple intermediate results files that are generated and sent by the nodes for each Tstore.</p>
  <p num="p-0126">Continuing to a decision block <b>1374</b>, the primary FEP <b>620</b> determines whether to perform post-processing operations on the final results file. Post-processing operations can include any operations that are performed on the final results file after it is built by the primary FEP <b>230</b>, for example, ordering records and eliminating duplicate records. If the primary FEP <b>620</b> determines at the decision block <b>1374</b> that post-processing is to be performed, the multiple set query process <b>1080</b> continues to a block <b>1378</b> where the final results file post processing module <b>638</b> of the primary FEP <b>620</b> performs the post-processing operations. After the block <b>1378</b>, or if the primary FEP <b>620</b> determines that post processing is not to be performed, at a block <b>1380</b> the primary FEP <b>620</b> returns the final results file to the requestor. The multiple set query process <b>1080</b> terminates at an end block <b>1390</b>.</p>
  <p num="p-0127"> <figref idrefs="DRAWINGS">FIG. 13</figref> is a flowchart illustrating a shared memory routing process <b>1400</b> as performed by the shared memory router <b>610</b> or the shared memory router <b>704</b> of a secondary node. For ease of explanation, only the shared memory router <b>610</b> is specifically referred to in the following description. In addition, the specific example of a join table being received by the shared memory router <b>610</b> is described. As described above, join tables can be used to execute database query commands in the distributed database system <b>160</b>. One advantage the shared memory router <b>610</b> can provide is receiving a single copy of a join table and making it available to multiple Tstores on a node. This reduces the number of messages that are sent between nodes and increases the efficiency and performance of query commands in the distributed database system <b>160</b>.</p>
  <p num="p-0128">The shared memory routing process <b>1400</b> begins at a start block <b>1410</b>. The shared memory routing process <b>1400</b> continues to a block <b>1420</b> where the shared memory router <b>610</b> receives a message from another shared memory router on another node. In some embodiments, the message is in the form of a data packet having a header and a body. The header can include message routing data and the body can include the data to be processed by the Tstores on the receiving node, for example, one or more join tables.</p>
  <p num="p-0129">The shared memory routing process <b>1400</b> continues to a block <b>1430</b> where the shared memory router <b>610</b> reads the header of the message and determines the local routing information for the body. For example, a message having a body which is a single copy of a join table can be received and made available (routed) to all of the local Tstores on the receiving node as described below.</p>
  <p num="p-0130">The shared memory routing process <b>1400</b> continues to a block <b>1440</b> where the shared memory router <b>610</b> stores a single copy of the body of the incoming message in common memory of the receiving node. At block <b>1450</b> the shared memory router <b>610</b> makes the single copy of the body in the common memory available to the local Tstores of the receiving node. This can include, for example, sending a message to the local Tstores with a pointer to the location where the body is stored and the size of the body. The shared memory routing process <b>1400</b> terminates at an end block <b>1490</b>.</p>
  <p num="p-0131">While the above detailed description has shown, described, and pointed out novel features of the invention as applied to various embodiments, it will be understood that various omissions, substitutions, and changes in the form and details of the device or process illustrated may be made by those of ordinary skill in the technology without departing from the spirit of the invention. This invention may be embodied in other specific forms without departing from the essential characteristics as described herein. For example, although most of the foregoing embodiments are described in the context of distributed database systems, other embodiments of the invention include many types of distributed task systems, for example, systems having disjoint, distributed processing of tasks, jobs, or operations. The embodiments described above are to be considered in all respects as illustrative only and not restrictive in any manner. The scope of the invention is indicated by the following claims rather than by the foregoing description.</p>
</div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6098157">US6098157</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 24, 1998</td><td class="patent-data-table-td patent-date-value">Aug 1, 2000</td><td class="patent-data-table-td ">Shomiti Systems, Inc.</td><td class="patent-data-table-td ">Method for storing and updating information describing data traffic on a network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6968335">US6968335</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 14, 2002</td><td class="patent-data-table-td patent-date-value">Nov 22, 2005</td><td class="patent-data-table-td ">Sesint, Inc.</td><td class="patent-data-table-td ">Method and system for parallel processing of database queries</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7054852">US7054852</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 23, 2001</td><td class="patent-data-table-td patent-date-value">May 30, 2006</td><td class="patent-data-table-td ">Ncr Corporation</td><td class="patent-data-table-td ">Performance of join operations in parallel database systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7113957">US7113957</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 26, 2002</td><td class="patent-data-table-td patent-date-value">Sep 26, 2006</td><td class="patent-data-table-td ">Ncr Corporation</td><td class="patent-data-table-td ">Row hash match scan join using summary contexts for a partitioned database system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20040122845">US20040122845</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 19, 2002</td><td class="patent-data-table-td patent-date-value">Jun 24, 2004</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and method for automating data partitioning in a parallel database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20050060292">US20050060292</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 11, 2003</td><td class="patent-data-table-td patent-date-value">Mar 17, 2005</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and system for dynamic join reordering</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1164498A1?cl=en">EP1164498A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 11, 2000</td><td class="patent-data-table-td patent-date-value">Dec 19, 2001</td><td class="patent-data-table-td ">R U Sure Ltd.</td><td class="patent-data-table-td ">Object-oriented document change detection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1363189A2?cl=en">EP1363189A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 14, 2003</td><td class="patent-data-table-td patent-date-value">Nov 19, 2003</td><td class="patent-data-table-td ">SGS-THOMSON MICROELECTRONICS, INC. (a Delaware corp.)</td><td class="patent-data-table-td ">Apparatus and method for implementing a rom patch using a lockable cache</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001002935A2?cl=en">WO2001002935A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 11, 2000</td><td class="patent-data-table-td patent-date-value">Jan 11, 2001</td><td class="patent-data-table-td ">Intel Corp</td><td class="patent-data-table-td ">Method and system for managing secure client-server transactions</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Jaiwei et al., Join Index Hierarchy: An Indexing Structure for Efficient Navigation in Object-Oriented Database, IEEE, 1999, pp. 1-33.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Mohammed et al., Novel Parallel Join Algorithms for Grid Files, IEEE, 1996, pp. 144-149.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Taniar et al., Aggregate-Join Query Processing in Parallel Database System, IEEE, 2000, pp. 824-829.</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8005848">US8005848</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 28, 2007</td><td class="patent-data-table-td patent-date-value">Aug 23, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Streamlined declarative parsing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8037096">US8037096</a></td><td class="patent-data-table-td patent-date-value">Jun 29, 2007</td><td class="patent-data-table-td patent-date-value">Oct 11, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Memory efficient data processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8219523">US8219523</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 26, 2007</td><td class="patent-data-table-td patent-date-value">Jul 10, 2012</td><td class="patent-data-table-td ">Sap Ag</td><td class="patent-data-table-td ">Data quality enrichment integration and evaluation system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8458706">US8458706</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 31, 2007</td><td class="patent-data-table-td patent-date-value">Jun 4, 2013</td><td class="patent-data-table-td ">Neutrino Concepts Ltd.</td><td class="patent-data-table-td ">Methods and apparatus for parallel pipelining and width processing and configured to process a multiple task processes apportioned a different section of memory</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090144748">US20090144748</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 31, 2007</td><td class="patent-data-table-td patent-date-value">Jun 4, 2009</td><td class="patent-data-table-td ">Patrick Foody</td><td class="patent-data-table-td ">Methods and apparatus for parallel pipelining and width processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120323947">US20120323947</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 14, 2011</td><td class="patent-data-table-td patent-date-value">Dec 20, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Enriching Database Query Responses using Data from External Data Sources</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S714000">707/714</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S737000">707/737</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999100">707/999.1</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999200">707/999.2</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999006">707/999.006</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999103">707/999.103</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999001">707/999.001</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0017300000">G06F17/30</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0017000000">G06F17/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0007000000">G06F7/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0012000000">G06F12/00</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99931">Y10S707/99931</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99944">Y10S707/99944</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99936">Y10S707/99936</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L29/06">H04L29/06</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30498">G06F17/30498</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L2029/06054">H04L2029/06054</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30575">G06F17/30575</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=LQJ6BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L67/1002">H04L67/1002</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06F17/30S7</span>, <span class="nested-value">G06F17/30S4P4P3J</span>, <span class="nested-value">H04L29/06</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">May 20, 2010</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 15, 2009</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 1-21 IS CONFIRMED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 21, 2008</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080825</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 4, 2007</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">JARDIN, CARY A., CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:XPRIME, INC.;REEL/FRAME:018710/0135</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20050303</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 23, 2004</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">XPRIME, INC., CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:JARDIN, CARY A.;REEL/FRAME:015136/0938</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20040317</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U3eMlOjN-ftujzOe_ax78VQ8xjdnw\u0026id=LQJ6BAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U34G76P0OyushGLAN1rvCvcCDcpEw\u0026id=LQJ6BAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U2dETGVVXAlN_uz4fLulO5rEMYtiA","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/System_and_method_for_generating_and_pro.pdf?id=LQJ6BAABERAJ\u0026output=pdf\u0026sig=ACfU3U2RO5WCeuUI1VVTRy7qq9C76vdgQA"},"sample_url":"http://www.google.com/patents/reader?id=LQJ6BAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>