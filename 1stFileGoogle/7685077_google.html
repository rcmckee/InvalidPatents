<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7685077 - Recursive feature eliminating method based on a support vector machine - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Recursive feature eliminating method based on a support vector machine"><meta name="DC.contributor" content="Eric Q. Li" scheme="inventor"><meta name="DC.contributor" content="Qian Diao" scheme="inventor"><meta name="DC.contributor" content="Intel Corporation" scheme="assignee"><meta name="DC.date" content="2005-8-11" scheme="dateSubmitted"><meta name="DC.description" content="Method, apparatus and system are described to perform a feature eliminating method based on a support vector machine. In some embodiments, a value for each feature in a group of features provided by a training data is determined. At least one feature is eliminated from the group by utilizing the value for each feature in the group. The value for each feature in the group is updated based upon a part of the training data that corresponds to the eliminated feature."><meta name="DC.date" content="2010-3-23" scheme="issued"><meta name="DC.relation" content="US:20030172043:A1" scheme="references"><meta name="DC.relation" content="WO:2004105573:A2" scheme="references"><meta name="citation_reference" content="‘An introduction to support vector machines and other kernel based learning methods’: Cristianini, 2000, Cambridge University Press, p. 18, 29, 30, 45, 125."><meta name="citation_reference" content="‘Gene selection for cancer classification using support vector machines’: Guyon, 2002, Machine Learning, vol. 46, pp. 389-422 (1-39)."><meta name="citation_reference" content="Ambroise et al: Selection Bias in gene Extraction on the Basis of Microarray Gene-Expression Data, Proceedings of the National Academy of Sciences, vol. 99, No. 10, May 14, 2002, 5 pages www.pnas.org/cgi/doi/10.1073/pnas.102102699."><meta name="citation_reference" content="&#39;An introduction to support vector machines and other kernel based learning methods&#39;: Cristianini, 2000, Cambridge University Press, p. 18, 29, 30, 45, 125."><meta name="citation_reference" content="An Introduction to Support Vector Machines and Other Kernel-based Learning Methods 4 pages."><meta name="citation_reference" content="Boser et al: A Training Algorithm for Optimal Margin classifiers, AT &amp;T Bell Labs / Univ. of California, Berkeley, 9 pages."><meta name="citation_reference" content="Chih-Chung Chang, et al: LIBSVM: A Library for Support Vector Machines, De&#39;t of Computer Science and Information Engineering, Nat&#39;l Taiwan Univ, Taipei, Taiwan, Last updated: Mar. 28, 2004."><meta name="citation_reference" content="&#39;Gene selection for cancer classification using support vector machines&#39;: Guyon, 2002, Machine Learning, vol. 46, pp. 389-422 (1-39)."><meta name="citation_reference" content="Golub et al: Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring, Science, vol. 286, Oct. 15, 1999, www.sciencemag.org."><meta name="citation_reference" content="Guyon et al: Gene Selection for Cancer Classification Using Support Vector Machines, Barnhill Bioinformatics, Savannah, GA AT &amp; T Labs, Red Bank, NJ, USA pp. 1-39."><meta name="citation_reference" content="International Preliminary Report on Patentability, International Application No. PCT/CN2005/001242, mailing date Jun. 15, 2006, 4."><meta name="citation_reference" content="International Search Report and Written Opinion, International Application No. PCT/CN2005/001242, mailing date Jun. 15, 2006."><meta name="citation_reference" content="Joachims: Making Large-Scale SVM Learning Practical, chapter 11, 16 pages To be pub&#39;d in: Advances in Kernel Methods-Support Vector Learning, MIT Press, (Cambridge, USA, 1998."><meta name="citation_reference" content="Joachims: Making Large-Scale SVM Learning Practical, chapter 11, 16 pages To be pub&#39;d in: Advances in Kernel Methods—Support Vector Learning, MIT Press, (Cambridge, USA, 1998."><meta name="citation_reference" content="Osuna, et al: Training Support Vector Machines: an Application to Face Detection, To appear in the Proceedings of CVPR &#39;97, Jun. 17-19, 1997, Puerto Rico Center for biological and Computational Learning and Operations Research Center, MIT, Cambridge, MA, USA, 8 pages."><meta name="citation_reference" content="Petricoin et al: Use of Proteomic Patterns in Serum to Identify Ovarian Cancer, The Lancet, vol. 359, Feb. 16, 2002, Mechanisms of Disease, pp. 572-577 www.thelancet.com."><meta name="citation_reference" content="Platt: Fast Training of Support Vector Machines Using Sequential minimal Optimization, Microsoft REsearch, Redmond, WA www.research.microsoft.com/~jplatt."><meta name="citation_reference" content="Platt: Fast Training of Support Vector Machines Using Sequential minimal Optimization, Microsoft REsearch, Redmond, WA www.research.microsoft.com/˜jplatt."><meta name="citation_reference" content="Written Opinion of the International Searching Authority, International Application No. PCT/CN2005/001242, mailing date Jun. 15, 2006, 3."><meta name="citation_patent_number" content="US:7685077"><meta name="citation_patent_application_number" content="US:10/587,094"><link rel="canonical" href="http://www.google.com/patents/US7685077"/><meta property="og:url" content="http://www.google.com/patents/US7685077"/><meta name="title" content="Patent US7685077 - Recursive feature eliminating method based on a support vector machine"/><meta name="description" content="Method, apparatus and system are described to perform a feature eliminating method based on a support vector machine. In some embodiments, a value for each feature in a group of features provided by a training data is determined. At least one feature is eliminated from the group by utilizing the value for each feature in the group. The value for each feature in the group is updated based upon a part of the training data that corresponds to the eliminated feature."/><meta property="og:title" content="Patent US7685077 - Recursive feature eliminating method based on a support vector machine"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("ekzsU_S7A8b4yQTiwIAQ"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("CAN"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("ekzsU_S7A8b4yQTiwIAQ"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("CAN"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7685077?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7685077"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=QKBlBgABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7685077&amp;usg=AFQjCNFjxXopHUG7mwOoTT5EHokA2F987A" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7685077.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7685077.pdf"></a><a class="appbar-application-grant-link" data-label="Application" href="/patents/US20080243728"></a><a class="appbar-application-grant-link" data-selected="true" data-label="Grant" href="/patents/US7685077"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7685077" style="display:none"><span itemprop="description">Method, apparatus and system are described to perform a feature eliminating method based on a support vector machine. In some embodiments, a value for each feature in a group of features provided by a training data is determined. At least one feature is eliminated from the group by utilizing the value...</span><span itemprop="url">http://www.google.com/patents/US7685077?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7685077 - Recursive feature eliminating method based on a support vector machine</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7685077 - Recursive feature eliminating method based on a support vector machine" title="Patent US7685077 - Recursive feature eliminating method based on a support vector machine"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7685077 B2</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 10/587,094</td></tr><tr><td class="patent-bibdata-heading">PCT number</td><td class="single-patent-bibdata">PCT/CN2005/001242</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Mar 23, 2010</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Aug 11, 2005</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Aug 11, 2005</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US20080243728">US20080243728</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO2007016814A1">WO2007016814A1</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">10587094, </span><span class="patent-bibdata-value">587094, </span><span class="patent-bibdata-value">PCT/2005/1242, </span><span class="patent-bibdata-value">PCT/CN/2005/001242, </span><span class="patent-bibdata-value">PCT/CN/2005/01242, </span><span class="patent-bibdata-value">PCT/CN/5/001242, </span><span class="patent-bibdata-value">PCT/CN/5/01242, </span><span class="patent-bibdata-value">PCT/CN2005/001242, </span><span class="patent-bibdata-value">PCT/CN2005/01242, </span><span class="patent-bibdata-value">PCT/CN2005001242, </span><span class="patent-bibdata-value">PCT/CN200501242, </span><span class="patent-bibdata-value">PCT/CN5/001242, </span><span class="patent-bibdata-value">PCT/CN5/01242, </span><span class="patent-bibdata-value">PCT/CN5001242, </span><span class="patent-bibdata-value">PCT/CN501242, </span><span class="patent-bibdata-value">US 7685077 B2, </span><span class="patent-bibdata-value">US 7685077B2, </span><span class="patent-bibdata-value">US-B2-7685077, </span><span class="patent-bibdata-value">US7685077 B2, </span><span class="patent-bibdata-value">US7685077B2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Eric+Q.+Li%22">Eric Q. Li</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Qian+Diao%22">Qian Diao</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Intel+Corporation%22">Intel Corporation</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7685077.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7685077.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7685077.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (2),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (19),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (1),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (8),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (4)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=QKBlBgABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7685077&usg=AFQjCNFZ4PExe73B5SPclaQDX33H8ErwXw">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=QKBlBgABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7685077&usg=AFQjCNH6Fho49xwzZfGK5gjSeYvG_gA6Nw">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=QKBlBgABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7685077B2%26KC%3DB2%26FT%3DD&usg=AFQjCNGITUutHq7yL4zeC07GUcxy71Lr3g">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT94144235" lang="EN" load-source="patent-office">Recursive feature eliminating method based on a support vector machine</invention-title></span><br><span class="patent-number">US 7685077 B2</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA75221005" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">Method, apparatus and system are described to perform a feature eliminating method based on a support vector machine. In some embodiments, a value for each feature in a group of features provided by a training data is determined. At least one feature is eliminated from the group by utilizing the value for each feature in the group. The value for each feature in the group is updated based upon a part of the training data that corresponds to the eliminated feature.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(5)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7685077B2/US07685077-20100323-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7685077B2/US07685077-20100323-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7685077B2/US07685077-20100323-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7685077B2/US07685077-20100323-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7685077B2/US07685077-20100323-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7685077B2/US07685077-20100323-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7685077B2/US07685077-20100323-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7685077B2/US07685077-20100323-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7685077B2/US07685077-20100323-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7685077B2/US07685077-20100323-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(15)</span></span></div><div class="patent-text"><div mxw-id="PCLM30340953" lang="EN" load-source="patent-office" class="claims">
    <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
      <div class="claim-text">1. A method, comprising
<div class="claim-text">determining a value for each feature in a group of features provided by a training data;</div>
<div class="claim-text">eliminating at least one feature with one of a minimum value and a maximum value from the group;</div>
<div class="claim-text">retrieving a kernel data from a buffer;</div>
<div class="claim-text">subtracting a matrix from the kernel data to provide an updated kernel data, each component of the matrix comprising a dot product of two of training samples provided by at least a part of the training data that corresponds to the eliminated feature;</div>
<div class="claim-text">updating the value for each feature of the group based on the updated kernel data;</div>
<div class="claim-text">repeating of eliminating the at least one feature from the group and updating the value for each feature of the group until a number of features in the group reaches a predetermined value to generate a feature ranking list;</div>
<div class="claim-text">recognizing a new data corresponding to the group of features with the feature ranking list.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
      <div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the training data further comprises a plurality of training samples, each of the training samples corresponding to the group of features.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
      <div class="claim-text">3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the value comprises:
<div class="claim-text">computing a kernel data based on the training data;</div>
<div class="claim-text">computing the value for each feature of the group based on the kernel data; and</div>
<div class="claim-text">storing the kernel data in a buffer.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
      <div class="claim-text">4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein computing the kernel data further comprises computing a matrix as the kernel data, each component of the matrix comprising a dot product of two of training samples provided by the training data.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
      <div class="claim-text">5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein eliminating at least one feature comprises:
<div class="claim-text">computing a ranking criterion for each feature of the group based on the value for the each feature;</div>
<div class="claim-text">eliminating the at least one feature with the minimum ranking criterion from the group; and</div>
<div class="claim-text">recording the eliminated feature in a feature ranking list.</div>
</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00006" num="00006" class="claim">
      <div class="claim-text">6. A support vector machine, comprising:
<div class="claim-text">a training logic to determine a value for each feature in a group of features provided by a training data; and</div>
<div class="claim-text">an eliminate logic to eliminate at least one feature with one of a minimum value and a maximum value from the group,</div>
<div class="claim-text">wherein the training logic further comprises a buffer to store a kernel data, a kernel data logic to subtract a matrix from the kernel data to provide an updated kernel data, each component of the matrix comprising a dot product of two of training samples provided by at least a part of the training data that corresponds to the eliminated feature and a value update logic to update the value based on the updated kernel data, and</div>
<div class="claim-text">wherein the apparatus further repeats eliminating the at least one feature from the group and updating the value for each feature of the group until a number of features in the group reaches a predetermined value, to generate a feature ranking list for a use of recognizing a new data corresponding to the group of features.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
      <div class="claim-text">7. The support vector machine of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the training data comprises a plurality of training samples, each of the training samples having the group of features.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
      <div class="claim-text">8. The support vector machine of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:
<div class="claim-text">a decision logic to decide whether to repeat the elimination of the at least one features from the group and update of the value for each feature of the group until a number of features in the group reaches a predetermined value.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
      <div class="claim-text">9. The support vector machine of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the eliminate logic further comprises a ranking criterion logic to compute a ranking criterion for each feature of the group based on the value for the each feature.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
      <div class="claim-text">10. The support vector machine of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the eliminate logic further comprises a feature eliminate logic to eliminate the at least one feature having the minimum ranking criterion from the group.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00011" num="00011" class="claim">
      <div class="claim-text">11. A machine-readable medium comprising a plurality of instructions, that in response to being executed, result in a computing system:
<div class="claim-text">determining a value for each feature in a group of features provided by a training data;</div>
<div class="claim-text">eliminating at least one feature with one of a minimum value and a maximum value from the group;</div>
<div class="claim-text">retrieving a kernel data from a buffer;</div>
<div class="claim-text">subtracting a matrix from the kernel data to provide an updated kernel data, each component of the matrix comprising a dot product of two of training samples provided by at least a part of the training data that corresponds to the eliminated feature;</div>
<div class="claim-text">updating the value for each feature of the group based on the updated kernel data;</div>
<div class="claim-text">repeating of eliminating the at least one feature from the group and updating the value for each feature of the group until a number of features in the group reaches a predetermined value to generate a feature ranking list;</div>
<div class="claim-text">recognizing a new data corresponding to the group of features with the feature ranking list.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
      <div class="claim-text">12. The machine-readable medium of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the training data further comprises a plurality of training samples, each of the training samples corresponding to the group of features.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
      <div class="claim-text">13. The machine-readable of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the plurality of instructions that result in the computing system determining the value, further result in the computing system:
<div class="claim-text">computing a kernel data based on the training data;</div>
<div class="claim-text">computing the value for each feature of the group based on the kernel data; and</div>
<div class="claim-text">storing the kernel data in a buffer.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
      <div class="claim-text">14. The machine-readable of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the plurality of instructions that result in the computing system computing the kernel data, further result in the computing system computing a matrix as the kernel data, each component of the matrix comprising a dot product of two of training samples provided by the training data.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
      <div class="claim-text">15. The machine-readable of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the plurality of instructions that result in the computing system eliminating at least one feature, further result in the computing system:
<div class="claim-text">computing a ranking criterion for each feature of the group based on the value for the each feature;</div>
<div class="claim-text">eliminating the at least feature with the minimum ranking criterion from the group; and</div>
<div class="claim-text">recording the eliminated feature in a feature ranking list.</div>
</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES36800614" lang="EN" load-source="patent-office" class="description">
    <p num="p-0002">This U.S. application claims priority to Pending International Application Number PCT/CN2005-001242, filed PCT China on Aug. 11, 2005.</p>
    <heading>BACKGROUND</heading> <p num="p-0003">A recursive feature eliminating method based on a support vector machine (SVM-RFE) is widely used in data intensive applications, such as disease genes selection, structured data mining, and unstructured data mining, etc. The SVM-RFE method may comprise: SVM training an input training data to classify the training data, wherein the training data may comprise a plurality of training samples corresponding to a group of features and class labels associated with each of the training samples; eliminating at least one feature with a minimum ranking criterion from the group of features; and repeating the aforementioned SVM training and eliminating until the group becomes empty. The SVM-RFE may be used to rank the features, for example, to rank the genes that may cause a disease. Rounds of SVM training and eliminating are independent with each other.</p>
    <description-of-drawings> <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p num="p-0004">The invention described herein is illustrated by way of example and not by way of limitation in the accompanying figures. For simplicity and clarity of illustration, elements illustrated in the figures are not necessarily drawn to scale. For example, the dimensions of some elements may be exaggerated relative to other elements for clarity. Further, where considered appropriate, reference labels have been repeated among the figures to indicate corresponding or analogous elements.</p>
      <p num="p-0005"> <figref idrefs="DRAWINGS">FIG. 1</figref> illustrates an embodiment of a computing system applying a SVM-RFE method.</p>
      <p num="p-0006"> <figref idrefs="DRAWINGS">FIG. 2</figref> illustrates an embodiment of a SVM-RFE machine in the computing system of <figref idrefs="DRAWINGS">FIG. 1</figref>.</p>
      <p num="p-0007"> <figref idrefs="DRAWINGS">FIG. 3</figref> illustrates an embodiment of a SVM-RFE method;</p>
      <p num="p-0008"> <figref idrefs="DRAWINGS">FIG. 4</figref> illustrates an embodiment of a SVM training method involved in the SVM-RFE method of <figref idrefs="DRAWINGS">FIG. 3</figref>.</p>
    </description-of-drawings> <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading> <p num="p-0009">The following description describes techniques for a recursive feature eliminating method based on a support vector machine. In the following description, numerous specific details such as logic implementations, pseudo-code, means to specify operands, resource partitioning/sharing/duplication implementations, types and interrelationships of system components, and logic partitioning/integration choices are set forth in order to provide a more thorough understanding of the current invention. However, the invention may be practiced without such specific details. In other instances, control structures, gate level circuits and full software instruction sequences have not been shown in detail in order not to obscure the invention. Those of ordinary skill in the art, with the included descriptions, will be able to implement appropriate functionality without undue experimentation.</p>
    <p num="p-0010">References in the specification to “one embodiment”, “an embodiment”, “an example embodiment”, etc., indicate that the embodiment described may include a particular feature, structure, or characteristic, but every embodiment may not necessarily include the particular feature, structure, or characteristic. Moreover, such phrases are not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, it is submitted that it is within the knowledge of one skilled in the art to effect such feature, structure, or characteristic in connection with other embodiments whether or not explicitly described.</p>
    <p num="p-0011">Embodiments of the invention may be implemented in hardware, firmware, software, or any combination thereof. Embodiments of the invention may also be implemented as instructions stored on a machine-readable medium, that may be read and executed by one or more processors. A machine-readable medium may include any mechanism for storing or transmitting information in a form readable by a machine (e.g., a computing device). For example, a machine-readable medium may include read only memory (ROM); random access memory (RAM); magnetic disk storage media; optical storage media; flash memory devices and other possible mediums.</p>
    <p num="p-0012"> <figref idrefs="DRAWINGS">FIG. 1</figref> shows a computing system for implementing a recursive feature eliminating method based on a support vector machine (SVM-RFE). A non-exhausive list of examples for the computing system may include distributed computing systems, supercomputers, computing clusters, mainframe computers, mini-computers, client-server systems, personal computers, workstations, servers, portable computers, laptop computers and other devices for transceiving and processing data.</p>
    <p num="p-0013">In an embodiment, the computing system <b>1</b> may comprise one or more processors <b>10</b>, memory <b>11</b>, chipset <b>12</b>, I/O device <b>13</b>, BIOS firmware <b>14</b> and the like. The one or more processors <b>10</b> are communicatively coupled to various components (e.g., the memory <b>11</b>) via one or more buses such as a processor bus as depicted in <figref idrefs="DRAWINGS">FIG. 1</figref>. The processors <b>10</b> may be implemented as an integrated circuit (IC) with one or more processing cores that may execute codes under a suitable architecture, for example, including Intel® Xeon™ MP architecture available from Intel Corporation of Santa Clara, Calif.</p>
    <p num="p-0014">In an embodiment, the memory <b>12</b> may store codes to be executed by the processor <b>10</b>. In an embodiment, the memory <b>12</b> may store training data <b>110</b>, SVM-RFE <b>111</b> and operation system (OS) <b>112</b>. A non-exhausive list of examples for the memory <b>102</b> may comprise one or a combination of the following semiconductor devices, such as synchronous dynamic random access memory (SDRAM) devices, RAMBUS dynamic random access memory (RDRAM) devices, double data rate (DDR) memory devices, static random access memory (SRAM), flash memory devices, and the like.</p>
    <p num="p-0015">In an embodiment, the chipset <b>12</b> may provide one or more communicative path among the processor <b>10</b>, memory <b>11</b> and various components, such as the I/O device <b>13</b> and BIOS firmware <b>14</b>. The chipset <b>12</b> may comprise a memory controller hub <b>120</b>, an input/output controller hub <b>121</b> and a firmware hub <b>122</b>.</p>
    <p num="p-0016">In an embodiment, the memory controller hub <b>120</b> may provide a communication link to the processor bus that may connect with the processor <b>101</b> and to a suitable device such as the memory <b>11</b>. The memory controller hub <b>120</b> may couple with the I/O controller hub <b>121</b>, that may provide an interface to the I/O devices <b>13</b> or peripheral components (not shown in <figref idrefs="DRAWINGS">FIG. 1</figref>) for the computing system <b>1</b> such as a keyboard and a mouse. A non-exhausive list of examples for the I/O devices <b>13</b> may comprise a network card, a storage device, a camera, a blue-tooth, an antenna, and the like. The I/O controller hub <b>121</b> may further provide communication link to a graphic controller and an audio controller (not shown in <figref idrefs="DRAWINGS">FIG. 1</figref>). The graphic controller may control the display of information on a display device and the audio controller may control the display of information on an audio device.</p>
    <p num="p-0017">In an embodiment, the memory controller hub <b>120</b> may communicatively couple with a firmware hub <b>122</b> via the input/output controller hub <b>121</b>. The firmware hub <b>122</b> may couple with the BIOS firmware <b>14</b> that may store routines that the computing device <b>100</b> executes during system startup in order to initialize the processors <b>10</b>, chipset <b>12</b>, and other components of the computing device <b>1</b>. Moreover, the BIOS firmware <b>14</b> may comprise routines or drivers that the computing device <b>1</b> may execute to communicate with one or more components of the computing device <b>1</b>.</p>
    <p num="p-0018">In an embodiment, the training data <b>110</b> may be input from a suitable devices, such as the I/O component <b>13</b>, or the BIOS firmware. Examples for the training data <b>110</b> may comprise data collected for a feature selection/ranking task, such as gene expression data from a plurality of human beings or other species, or text data from web or other sources. The data format may be structured data, such as a database or table, or unstructured data, such as matrix or vector. The SVM-RFE <b>111</b> may be implemented between the training data <b>110</b> and the operation system <b>112</b>. In an embodiment, the operation system <b>112</b> may include, but not limited to, different versions of LINUX, Microsoft Windows™ Server 2003, and real time operating systems such as VxWorkS™, etc. In an embodiment, the SVM-RFE <b>111</b> may implement operations of: SVM training the training data <b>110</b> that corresponds to a group of features; eliminating at least one feature from the group according to a predetermined ranking criterion; and repeating the SVM training and feature eliminating until the number of features in the group reaches a predetermined value, for example, until the group becomes empty, wherein the rounds of SVM training and eliminating dependent with each other. The SVM-RFE <b>111</b> may output a feature elimination history or a feature ranking list.</p>
    <p num="p-0019">Other embodiments may implement other modifications or variations to the structure of the aforementioned computing system <b>1</b>. For example, the SVM-RFE <b>111</b> may be implemented as an integrated circuit with various functional logics as depicted in <figref idrefs="DRAWINGS">FIG. 2</figref>. For another example, the memory <b>11</b> may further comprise a validation software (not show in <figref idrefs="DRAWINGS">FIG. 1</figref>) to validate the SVM-RFE classification by the SVM-RFE <b>111</b>. More specifically, the validation software may determine whether a person has a disease by checking his/her gene expression with a gene ranking list output by the SVM-RFE <b>111</b>.</p>
    <p num="p-0020">An embodiment of the SVM-RFE <b>111</b> is shown in <figref idrefs="DRAWINGS">FIG. 2</figref>. As shown, the SVM-RFE <b>111</b> may comprise a decision logic <b>21</b>, a SVM learning machine <b>22</b>, a ranking criterion logic <b>23</b> and an eliminating logic <b>24</b>.</p>
    <p num="p-0021">In an embodiment, the training data <b>110</b> input to the SVM-RFE <b>111</b> may comprise a plurality of training samples [x<sub>1</sub>, x<sub>2</sub>, . . . , x<sub>m</sub>] corresponding to a group of embodiment, the operation system <b>112</b> may include, but not limited to, different versions of LINUX, Microsoft Windows™ Server 2003, and real time operating systems such as VxWorkS™, etc. In an embodiment, the SVM-RFE <b>111</b> may implement operations of: SVM training the training data <b>110</b> that corresponds to a group of features; eliminating at least one feature from the group according to a predetermined ranking criterion; and repeating the SVM training and feature eliminating until the number of features in the group reaches a predetermined value, for example, until the group becomes empty, wherein the rounds of SVM training and eliminating dependent with each other. The SVM-RFE <b>111</b> may output a feature elimination history or a feature ranking list.</p>
    <p num="p-0022">Other embodiments may implement other modifications or variations to the structure of the aforementioned computing system <b>1</b>. For example, the SVM-RFE <b>111</b> may be implemented as an integrated circuit with various functional logics as depicted in <figref idrefs="DRAWINGS">FIG. 2</figref>. For another example, the memory <b>11</b> may further comprise a validation software (not show in <figref idrefs="DRAWINGS">FIG. 1</figref>) to validate the SVM-RFE classification by the SVM-RFE <b>111</b>. More specifically, the validation software may determine whether a person has a disease by checking his/her gene expression with a gene ranking list output by the SVM-RFE <b>111</b>.</p>
    <p num="p-0023">An embodiment of the SVM-RFE <b>111</b> is shown in <figref idrefs="DRAWINGS">FIG. 2</figref>. As shown, the SVM-RFE <b>111</b> may comprise a decision logic <b>21</b>, a SVM learning machine <b>22</b>, a ranking criterion logic <b>23</b> and an eliminating logic <b>24</b>.</p>
    <p num="p-0024">In an embodiment, the training data <b>110</b> input to the SVM-RFE <b>111</b> may comprise a plurality of training samples [x<sub>1</sub>, x<sub>2</sub>, . . . , x<sub>m</sub>] corresponding to a group of features, wherein m represents the number of training samples. The training data may further comprise class labels associated with each of the training samples [y<sub>1</sub>, y<sub>2</sub>, . . . , y<sub>m</sub>]. In an embodiment, each of the training samples represents a vector of n dimensions, wherein each dimension corresponds with each feature, and each of the class labels has a number of values. For example, if the training data is gene data collected from a plurality of persons, each of the training samples represents a pattern of n gene expression coefficients for one person, and each of the class labels has two values (i.e., [1, −1]) to represent two-class classification of its associated training sample, e.g., whether the person has a certain decease (y<sub>i</sub>=1) or not (y<sub>i</sub>=−1).</p>
    <p num="p-0025">In an embodiment, the decision logic <b>21</b> may determine whether the group is empty and output a feature ranking list or feature elimination history if so. However, if the group is not empty, the SVM learning machine <b>22</b> may train the training data by setting a normal to a hyperplane where the training data may be mapped to leave the largest possible margin on either side of the normal. The SVM learning machine <b>22</b> may comprise a linear SVM learning machine and non-linear SVM learning machine. In an embodiment for linear SVM learning machine, a normal may comprise a vector ({right arrow over (ω)}) representing a linear combination of the training data. For non-linear SVM learning machine, a normal may comprise a vector ({right arrow over (ω)}) representing a non-linear combination of the training data. Each component of the vector represents a weight for each feature in the group of features.</p>
    <p num="p-0026">In an embodiment, the ranking criterion logic <b>23</b> may compute a predetermined ranking criterion for each feature based upon the weight vector {right arrow over (ω)}. The eliminating logic <b>27</b> may eliminate at least one feature with a certain ranking criterion from the group of features, for example, the at least one feature with a minimum or maximum ranking criterion in the group of features. Then, the decision logic <b>21</b> may determine whether the group becomes empty. If not, then in another round of SVM training and feature eliminating, the SVM learning machine <b>22</b> will retrain the training data corresponding to the group of features without the eliminated ones, the ranking criterion logic <b>23</b> and eliminating logic <b>24</b> may compute the predetermined ranking criterion for each features in the group and eliminate at least one features with a minimum ranking criterion from the group of features. The SVM-RFE <b>111</b> may repeat the rounds of SVM training and feature eliminating as described above until the group becomes empty.</p>
    <p num="p-0027">In an embodiment, the SVM learning machine <b>22</b> may comprise a kernel data logic <b>220</b>, a buffer <b>221</b>, a Lagrange multiplier logic <b>222</b> and a weight logic <b>223</b>. In a first round of SVM training, the kernel data logic <b>22</b> may compute the kernel data based on the training data corresponding to the group of features and store the kernel data in the buffer <b>22</b> and then in each round of SVM training later, the kernel data logic <b>220</b> may retrieve a kernel data from the buffer <b>23</b>, update the kernel data based on a part of the training data corresponding to the at least one feature that may be eliminated in a previous round and store the updated kernel data in the buffer in place of the old one.</p>
    <p num="p-0028">In an embodiment, the Lagrange multiplier logic <b>222</b> may compute a Lagrange multiplier α<sub>i </sub>for each of the training samples by utilizing the kernel data output from the kernel data logic <b>220</b> and the weight logic <b>224</b> may obtain a weight ω<sub>k </sub>for each feature in the group of features, wherein i is an integer in a range of [1, the number of training samples], and k is an integer in a range of [1, the number of features].</p>
    <p num="p-0029"> <figref idrefs="DRAWINGS">FIG. 3</figref> depicts an embodiment of a SVM-RFE method that may be implemented by the SVM-RFE <b>111</b>.</p>
    <p num="p-0030">As depicted, the SVM-RFE <b>111</b> may input the training data <b>110</b> in block <b>301</b>. In an embodiment, the training data may comprise a plurality of training samples [x<sub>1</sub>, x<sub>2</sub>, . . . , x<sub>m</sub>], wherein m represents the number of training samples. The training data may further comprise class labels associated with each of the training samples [y<sub>1</sub>, y<sub>2</sub>, . . . , y<sub>m</sub>]. Each of the training samples may represent a vector of n dimensions, wherein each dimension corresponds to each feature in a group of features (hereinafter, the group is labeled as group G), and each of class labels has a number of values to represent the class that its associated training sample belongs to.</p>
    <p num="p-0031">In block <b>302</b>, the decision logic <b>21</b> of SVM-RFE <b>111</b> may determine whether the number of features in the group G is zero (block <b>301</b>). If the number of features in the group G is greater than zero, then the SVM learning machine <b>22</b> of SVM-RFE <b>111</b> may train the training data corresponding to the features in the group G, so as to obtain a vector ({right arrow over (ω)}) for the training data (block <b>303</b>). Each component of the weight vector represents a weight (e.g., weight (ω<sub>k</sub>)) for a feature (e.g., the k<sup>th </sup>feature) in the group G.</p>
    <p num="p-0032">Then, the ranking criterion logic <b>23</b> may compute a ranking criterion for each feature in the group G based on its weight in block <b>304</b>. In an embodiment, the ranking criterion is a square of the weight, e.g., c<sub>k</sub>=(ω<sub>k</sub>)<sup>2</sup>, wherein c<sub>k </sub>represents the ranking criterion for the k<sup>th </sup>feature. However, in other embodiments, the ranking criterion may be obtained in other ways.</p>
    <p num="p-0033">In block <b>305</b>, the eliminating logic <b>24</b> may eliminate at least one feature with a certain ranking criterion from the group G. In an embodiment, the at least one feature (e.g., the k<sup>th </sup>feature) may correspond to the ranking criterion (e.g., c<sub>k</sub>=(ω<sub>k</sub>)<sup>2</sup>) that is the minimum in the group G. In another embodiment, the at least one feature may correspond to the ranking criterion that is the maximum in the group G. In other embodiments, the at least one feature may be eliminated in other ways.</p>
    <p num="p-0034">In block <b>306</b>, the eliminating logic <b>24</b> of the SVM-RFE <b>111</b> or other suitable logics may optionally update the training data by removing a part of the training data that corresponds to the eliminated features. In an embodiment that the input training data may comprise m training samples and m class labels associated with the training samples, and each of the training samples is a vector of n dimensions wherein each dimension corresponds to each feature of the group G, the updated training data may comprise m training samples and m class labels associated with the training samples, and each of the training samples is a vector of (n−p) dimensions wherein (n−p) represents the number of the features in the group G after p features may be eliminated in block <b>305</b>.</p>
    <p num="p-0035">In block <b>307</b>, the eliminating logic <b>24</b> of the SVM-RFE <b>111</b> or other suitable logics may record the eliminating history, or record the feature ranking list based on the eliminating history. In an embodiment, the at least one features eliminated in block <b>305</b> may be listed as a least important feature in the feature ranking list. In another embodiment, the at least features may be listed as a most important feature in the feature ranking list.</p>
    <p num="p-0036">Then, the decision logic <b>21</b> of the SVM-RFE <b>111</b> may continue to determine whether the number of features in the group G is zero in block <b>302</b>. If not, the round of SVM training and feature eliminating as described with reference to blocks <b>303</b>-<b>307</b> may be repeated until the group G is determined to be empty, namely, the number of features therein is zero.</p>
    <p num="p-0037">If the decision logic <b>21</b> determines the number of features in the group G is zero in block <b>302</b>, then the decision logic <b>21</b> or other suitable logics of SVM-RFE <b>111</b> may output the eliminating history or the feature ranking list.</p>
    <p num="p-0038"> <figref idrefs="DRAWINGS">FIG. 4</figref> depicts an embodiment of SVM training implemented by the SVM learning machine <b>22</b> in block <b>303</b> of <figref idrefs="DRAWINGS">FIG. 3</figref>. In the embodiment, blocks depicted in <figref idrefs="DRAWINGS">FIG. 4</figref> may be implemented in each round of SVM training and feature elimination.</p>
    <p num="p-0039">As depicted, the kernel data logic <b>220</b> of the SVM learning machine or other suitable logics may determine whether it is the first round of SVM training for the training data <b>110</b> (block <b>401</b>). This determination may be accomplished by setting a count number. If it is the first round of SVM training, then the kernel data logic <b>220</b> may compute a kernel data based on the training data <b>110</b> in block <b>402</b>. In an embodiment for linear SVM training, the kernel data may be computed by the following equations (1) and (2):</p>
    <p num="p-0040"> <maths id="MATH-US-00001" num="00001"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <msup> <mi>K</mi> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>1</mn> </mrow> </msup> <mo>=</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <msubsup> <mi>k</mi> <mrow> <mn>1</mn> <mo>,</mo> <mn>1</mn> </mrow> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>1</mn> </mrow> </msubsup> </mtd> <mtd> <mi>…</mi> </mtd> <mtd> <msubsup> <mi>k</mi> <mrow> <mn>1</mn> <mo>,</mo> <mi>m</mi> </mrow> <mi>round1</mi> </msubsup> </mtd> </mtr> <mtr> <mtd> <mi>…</mi> </mtd> <mtd> <msubsup> <mi>k</mi> <msub> <mi>k</mi> <mrow> <mi>i</mi> <mo>,</mo> <mi>j</mi> </mrow> </msub> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>1</mn> </mrow> </msubsup> </mtd> <mtd> <mi>…</mi> </mtd> </mtr> <mtr> <mtd> <msubsup> <mi>k</mi> <mrow> <mi>m</mi> <mo>,</mo> <mn>1</mn> </mrow> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>1</mn> </mrow> </msubsup> </mtd> <mtd> <mi>…</mi> </mtd> <mtd> <msubsup> <mi>k</mi> <mrow> <mi>m</mi> <mo>,</mo> <mi>m</mi> </mrow> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>1</mn> </mrow> </msubsup> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>1</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msubsup> <mi>k</mi> <mi>ij</mi> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>1</mn> </mrow> </msubsup> <mo>=</mo> <mrow> <mrow> <msubsup> <mi>x</mi> <mi>i</mi> <mi>T</mi> </msubsup> <mo>⁢</mo> <msub> <mi>x</mi> <mi>j</mi> </msub> </mrow> <mo>=</mo> <mrow> <munderover> <mo>∑</mo> <mrow> <mi>k</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>n</mi> </munderover> <mo>⁢</mo> <mrow> <msub> <mi>x</mi> <mi>ik</mi> </msub> <mo>⁢</mo> <msub> <mi>x</mi> <mi>jk</mi> </msub> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>2</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br>
wherein, K<sup>round1 </sup>is the kernel data of a matrix with (m·m) components k<sub>ij</sub> <sup>round1</sup>, m represents the number of training samples, x<sub>i</sub> <sup>T </sup>represents a transpose of i<sup>th </sup>training sample that is a vector of n components, x<sub>j </sub>represents j<sup>th </sup>training sample that is another vector of n components, n represents the number of features in the group G. Other embodiments may implement other modifications and variations to block <b>406</b>. For example, for non-linear SVM training, the kernel data may be obtained in a different way, e.g., the Gaussian RBF kernel:
</p>
    <p num="p-0041">
      <maths id="MATH-US-00002" num="00002"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <msubsup> <mi>k</mi> <mrow> <mi>i</mi> <mo>,</mo> <mi>j</mi> </mrow> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>1</mn> </mrow> </msubsup> <mo>=</mo> <mrow> <msup> <mi>ⅇ</mi> <mfrac> <mrow> <mo>-</mo> <msup> <mrow> <mo></mo> <mrow> <msub> <mi>x</mi> <mi>i</mi> </msub> <mo>-</mo> <msub> <mi>x</mi> <mi>j</mi> </msub> </mrow> <mo></mo> </mrow> <mn>2</mn> </msup> </mrow> <mrow> <mn>2</mn> <mo>⁢</mo> <msup> <mi>σ</mi> <mn>2</mn> </msup> </mrow> </mfrac> </msup> <mo>.</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>3</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0042">Then, the kernel data logic <b>220</b> stores the kernel data in the buffer <b>221</b> of the SVM learning machine <b>22</b> in block <b>403</b>. The Lagrange multiplier logic <b>222</b> may compute a Lagrange multiplier matrix based upon the kernel data in blocks <b>408</b>-<b>412</b> and the weight logic <b>223</b> may compute a weight vector based on the Lagrange multiplier matrix in block <b>414</b>. With these implementations, the first round of SVM training for the training data <b>110</b> is completed.</p>
    <p num="p-0043">However, if the kernel data logic <b>220</b> or other suitable logics determines that it is not the first round of SVM training for the training data <b>110</b> in block <b>401</b>, then in block <b>404</b>, the kernel data logic <b>220</b> or other suitable logics may input the at least one feature eliminated in a previous round of feature elimination implemented in block <b>305</b> of <figref idrefs="DRAWINGS">FIG. 3</figref>. For example, if it is q<sup>th </sup>round of SVM training (q&gt;1), then the kernel data logic or other suitable logics may input the at least one feature eliminated in a (q−1)<sup>th </sup>round of feature elimination (e.g., the p<sup>th </sup>feature that is eliminated from the group of n features in the (q−1)<sup>th </sup>round of feature elimination). Then, the kernel data logic <b>220</b> may retrieve the kernel data stored in the buffer <b>221</b> in a previous round of SVM training (block <b>405</b>), and update the kernel data based on a part of the training data corresponding to the at least one eliminated feature (block <b>406</b>). In an embodiment for linear SVM training, the kernel data may be updated by the following equations (4) and (5):</p>
    <p num="p-0044"> <maths id="MATH-US-00003" num="00003"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <msup> <mi>K</mi> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mo>(</mo> <mi>q</mi> <mo>)</mo> </mrow> </mrow> </msup> <mo>=</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <msubsup> <mi>k</mi> <mrow> <mn>1</mn> <mo>,</mo> <mn>1</mn> </mrow> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mo>(</mo> <mi>q</mi> <mo>)</mo> </mrow> </mrow> </msubsup> </mtd> <mtd> <mi>…</mi> </mtd> <mtd> <msubsup> <mi>k</mi> <mrow> <mn>1</mn> <mo>,</mo> <mi>m</mi> </mrow> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mo>(</mo> <mi>q</mi> <mo>)</mo> </mrow> </mrow> </msubsup> </mtd> </mtr> <mtr> <mtd> <mi>…</mi> </mtd> <mtd> <msubsup> <mi>k</mi> <mrow> <mi>i</mi> <mo>,</mo> <mi>j</mi> </mrow> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mo>(</mo> <mi>q</mi> <mo>)</mo> </mrow> </mrow> </msubsup> </mtd> <mtd> <mi>…</mi> </mtd> </mtr> <mtr> <mtd> <msubsup> <mi>k</mi> <mrow> <mi>m</mi> <mo>,</mo> <mn>1</mn> </mrow> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mo>(</mo> <mi>q</mi> <mo>)</mo> </mrow> </mrow> </msubsup> </mtd> <mtd> <mi>…</mi> </mtd> <mtd> <msubsup> <mi>k</mi> <mrow> <mi>m</mi> <mo>,</mo> <mi>m</mi> </mrow> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mo>(</mo> <mi>q</mi> <mo>)</mo> </mrow> </mrow> </msubsup> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>4</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msubsup> <mi>k</mi> <mi>ij</mi> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mo>(</mo> <mi>q</mi> <mo>)</mo> </mrow> </mrow> </msubsup> <mo>=</mo> <mrow> <msubsup> <mi>k</mi> <mi>ij</mi> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mo>(</mo> <mrow> <mi>q</mi> <mo>-</mo> <mn>1</mn> </mrow> <mo>)</mo> </mrow> </mrow> </msubsup> <mo>-</mo> <mrow> <msub> <mi>x</mi> <mi>ip</mi> </msub> <mo>⁢</mo> <msub> <mi>x</mi> <mi>jp</mi> </msub> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>5</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br>
wherein, k<sub>ij</sub> <sup>round(q) </sup>represents a component of the kernel data K in q<sup>th </sup>round of SVM training, k<sub>ij</sub> <sup>round(q−1) </sup>represents a component of the kernel data K in a (q−1)<sup>th </sup>round of SVM training, x<sub>ip </sub>represents the i<sup>th </sup>training sample with p<sup>th </sup>feature that is eliminated in (q−1)<sup>th </sup>round of feature elimination, x<sub>jp </sub>represents the j<sup>th </sup>training sample with p<sup>th </sup>feature that is eliminated in (q−1)<sup>th </sup>round of feature elimination.
</p>
    <p num="p-0045">Other embodiments may implement other modifications and variations to block <b>406</b>. For example, for non-linear SVM training, the kernel data may be updated in a different way, e.g., for the Gaussian RBF kernel, a component for the kernel data K in q<sup>th </sup>round may be updated by</p>
    <p num="p-0046">
      <maths id="MATH-US-00004" num="00004"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <msubsup> <mi>k</mi> <mi>ij</mi> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mo>(</mo> <mi>q</mi> <mo>)</mo> </mrow> </mrow> </msubsup> <mo>=</mo> <mrow> <msubsup> <mi>k</mi> <mi>ij</mi> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mo>(</mo> <mrow> <mi>q</mi> <mo>-</mo> <mn>1</mn> </mrow> <mo>)</mo> </mrow> </mrow> </msubsup> <mo>×</mo> <mrow> <msup> <mi>ⅇ</mi> <mfrac> <mrow> <mo>-</mo> <msup> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mi>ip</mi> </msub> <mo>-</mo> <msub> <mi>x</mi> <mi>jp</mi> </msub> </mrow> <mo>)</mo> </mrow> <mn>2</mn> </msup> </mrow> <mrow> <mn>2</mn> <mo>⁢</mo> <msup> <mi>σ</mi> <mn>2</mn> </msup> </mrow> </mfrac> </msup> <mo>.</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>6</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0047">Then, in block <b>407</b>, the kernel data logic <b>220</b> may replace the kernel data in the buffer <b>221</b> with the updated kernel data obtained in block <b>406</b>. The Lagrange multiplier logic <b>222</b> may compute a Lagrange multiplier matrix based on the kernel data in blocks <b>408</b>-<b>412</b> and the weight logic <b>223</b> may compute a weight vector based on the Lagrange multiplier matrix in block <b>414</b>. With these implementations, the q<sup>th </sup>round of SVM training is completed.</p>
    <p num="p-0048">More specifically, in block <b>408</b>, the Lagrange multiplier logic <b>222</b> may initialize a Lagrange multiplier matrix α in each round of SVM training, wherein each component of the α matrix represents a Lagrange multiplier (e.g. α<sub>i</sub>) corresponding to a training sample x<sub>i</sub>. In an embodiment, the initialization of the Lagrange multiplier matrix may be implemented by setting a predetermined value (e.g., zero) to each component of the Lagrange multiplier matrix.</p>
    <p num="p-0049">Then, in block <b>409</b>, the Lagrange multiplier logic <b>222</b> may determine whether each of the Lagrange multipliers corresponding to each of the training samples (e.g., [α<sub>1</sub>, α<sub>2</sub>, . . . , α<sub>m</sub>]) fulfill the Karush-Kuhn-Tucker (KKT) conditions. More specifically, whether each of the Lagrange multipliers fulfills the following five conditions:</p>
    <p num="p-0050"> <maths id="MATH-US-00005" num="00005"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mn>1.</mn> <mo>⁢</mo> <mfrac> <mrow> <mo>∂</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> </mrow> <mrow> <mo>∂</mo> <msub> <mi>w</mi> <mi>v</mi> </msub> </mrow> </mfrac> <mo>⁢</mo> <mrow> <mi>L</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mi>w</mi> <mo>,</mo> <mi>b</mi> <mo>,</mo> <mi>α</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>=</mo> <mrow> <msub> <mi>w</mi> <mi>v</mi> </msub> <mo>-</mo> <mrow> <munderover> <mo>∑</mo> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>m</mi> </munderover> <mo>⁢</mo> <mrow> <msub> <mi>α</mi> <mi>i</mi> </msub> <mo>⁢</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo>⁢</mo> <msub> <mi>x</mi> <mi>iv</mi> </msub> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mrow> <mi>v</mi> <mo>=</mo> <mn>1</mn> </mrow> <mo>,</mo> <mi>…</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>,</mo> <mi>n</mi> </mrow> </mtd> </mtr> </mtable> </math> </maths> <maths id="MATH-US-00005-2" num="00005.2"> <math overflow="scroll"> <mrow> <mrow> <mn>2.</mn> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mfrac> <mrow> <mo>∂</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> </mrow> <mrow> <mo>∂</mo> <mi>b</mi> </mrow> </mfrac> <mo>⁢</mo> <mrow> <mi>L</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mi>w</mi> <mo>,</mo> <mi>b</mi> <mo>,</mo> <mi>α</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>=</mo> <mrow> <mrow> <mo>-</mo> <mrow> <munderover> <mo>∑</mo> <mi>i</mi> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> </munderover> <mo>⁢</mo> <mrow> <msub> <mi>α</mi> <mi>i</mi> </msub> <mo>⁢</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> </mrow> </mrow> </mrow> <mo>=</mo> <mn>0</mn> </mrow> </mrow> </math> </maths> <maths id="MATH-US-00005-3" num="00005.3"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mrow> <mn>3.</mn> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>x</mi> <mi>i</mi> </msub> <mo>·</mo> <mi>w</mi> </mrow> <mo>-</mo> <mi>b</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>-</mo> <mn>1</mn> </mrow> <mo>≥</mo> <mn>0</mn> </mrow> </mtd> <mtd> <mrow> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mo>,</mo> <mi>…</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>,</mo> <mi>m</mi> </mrow> </mtd> </mtr> </mtable> </math> </maths> <maths id="MATH-US-00005-4" num="00005.4"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mn>4.</mn> <mo>⁢</mo> <msub> <mi>α</mi> <mi>i</mi> </msub> </mrow> <mo>≥</mo> <mn>0</mn> </mrow> </mtd> <mtd> <mrow> <mo>∀</mo> <mi>i</mi> </mrow> </mtd> </mtr> </mtable> </math> </maths> <maths id="MATH-US-00005-5" num="00005.5"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mn>5.</mn> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <msub> <mi>α</mi> <mi>i</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>x</mi> <mi>i</mi> </msub> <mo>·</mo> <mi>w</mi> </mrow> <mo>-</mo> <mi>b</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> <mn>1</mn> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>=</mo> <mn>0</mn> </mrow> </mtd> <mtd> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> </mtd> </mtr> </mtable> </math> </maths> <br>
wherein, w<sub>v </sub>represents the weight for the v<sup>th </sup>feature, b represents a bias value, L(w, b, α) represents a Lagrangian with w,b and α as variables:
</p>
    <p num="p-0051">
      <maths id="MATH-US-00006" num="00006"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mi>L</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mi>w</mi> <mo>,</mo> <mi>b</mi> <mo>,</mo> <mi>α</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mrow> <mrow> <mfrac> <mn>1</mn> <mn>2</mn> </mfrac> <mo>⁢</mo> <mrow> <mo>〈</mo> <mrow> <mi>w</mi> <mo>·</mo> <mi>w</mi> </mrow> <mo>〉</mo> </mrow> </mrow> <mo>-</mo> <mrow> <munderover> <mo>∑</mo> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>m</mi> </munderover> <mo>⁢</mo> <mrow> <msub> <mi>α</mi> <mi>i</mi> </msub> <mo>⁡</mo> <mrow> <mo>[</mo> <mrow> <mrow> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mo>〈</mo> <mrow> <mi>w</mi> <mo>·</mo> <msub> <mi>x</mi> <mi>i</mi> </msub> </mrow> <mo>〉</mo> </mrow> <mo>+</mo> <mi>b</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> <mn>1</mn> </mrow> <mo>]</mo> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>7</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0052">If not all of the Lagrange multipliers fulfill the KKT conditions, the Lagrange multiplier logic <b>222</b> may initialize an active set for two Lagrange multipliers in block <b>410</b>. In an embodiment, the initialization of the active set may be implemented by clearing a data fragment in a memory of the computing system to store the active set. In other embodiments, the active set may be initialized in other ways.</p>
    <p num="p-0053">Then, in block <b>411</b>, the Lagrange multiplier logic <b>222</b> may select two Lagrange multipliers (e.g., α<sub>1 </sub>and α<sub>2</sub>) as an active set with heuristics, wherein the two Lagrange multiplier violates the KKT conditions with minimum errors (e.g., errors E<sub>1 </sub>and E<sub>2 </sub>respectively associated with the two Lagrange multipliers α<sub>1 </sub>and α<sub>2</sub>) under a predetermined constraint. In order to do that, the Lagrange multiplier logic <b>222</b> may obtain the errors associated with each of the Lagrange multipliers (e.g., [α<sub>1</sub>, α<sub>2</sub>, . . . , α<sub>m</sub>]) by utilizing the kernel data stored in the buffer <b>221</b>. In an embodiment for linear SVM training, the predetermined constraint may comprise 0≦α<sub>i</sub>≦C wherein C is a predetermined value, and the error associated with each Lagrange multiplier may be obtained by the following equation and then stored in an error cache:</p>
    <p num="p-0054"> <maths id="MATH-US-00007" num="00007"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mtable> <mtr> <mtd> <mrow> <msub> <mi>E</mi> <mi>j</mi> </msub> <mo>=</mo> <mrow> <mo>(</mo> <mrow> <mrow> <munderover> <mo>∑</mo> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>m</mi> </munderover> <mo>⁢</mo> <mrow> <msub> <mi>α</mi> <mi>i</mi> </msub> <mo>⁢</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo>⁢</mo> <msubsup> <mi>k</mi> <mi>ij</mi> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mo>(</mo> <mi>q</mi> <mo>)</mo> </mrow> </mrow> </msubsup> </mrow> </mrow> <mo>-</mo> <msub> <mi>y</mi> <mi>j</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mrow> <mi>j</mi> <mo>=</mo> <mn>1</mn> </mrow> <mo>,</mo> <mi>…</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>,</mo> <mi>m</mi> </mrow> </mtd> </mtr> </mtable> </mtd> <mtd> <mrow> <mo>(</mo> <mn>8</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br>
wherein, E<sub>j </sub>represents an error associated with a Lagrange multiplier α<sub>j </sub>in q<sup>th </sup>round of SVM training, k<sub>ij</sub> <sup>round(q) </sup>my be obtained from the kernel data stored in the buffer <b>221</b>. Other embodiments may implement other modifications and variations to block <b>411</b>. For example, the active set may comprise the number of Lagrange multipliers other than two.
</p>
    <p num="p-0055">Then, in block <b>412</b>, the Lagrange multiplier logic <b>222</b> may update the Lagrange multipliers in the active set by utilizing the kernel data K stored in the buffer <b>221</b>. In an embodiment that the SVM learning machine is a linear learning machine and the active set may comprise two Lagrange multipliers (e.g., α<sub>1 </sub>and α<sub>2</sub>), the Lagrange multipliers may be updated with the following equations:</p>
    <p num="p-0056">
      <maths id="MATH-US-00008" num="00008"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <msubsup> <mi>α</mi> <mn>2</mn> <mi>new</mi> </msubsup> <mo>=</mo> <mrow> <msub> <mi>α</mi> <mn>2</mn> </msub> <mo>+</mo> <mfrac> <mrow> <msub> <mi>y</mi> <mn>2</mn> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>E</mi> <mn>2</mn> </msub> <mo>-</mo> <msub> <mi>E</mi> <mn>1</mn> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mi>η</mi> </mfrac> </mrow> </mrow> <mo>,</mo> <mrow> <mi>η</mi> <mo>≡</mo> <mrow> <mrow> <mn>2</mn> <mo>⁢</mo> <msub> <mi>k</mi> <mn>12</mn> </msub> </mrow> <mo>-</mo> <msub> <mi>k</mi> <mn>11</mn> </msub> <mo>-</mo> <msub> <mi>k</mi> <mn>22</mn> </msub> </mrow> </mrow> <mo>,</mo> <mstyle> <mtext> </mtext> </mstyle> <mo>⁢</mo> <mrow> <msub> <mi>E</mi> <mi>j</mi> </msub> <mo>=</mo> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <munderover> <mo>∑</mo> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>m</mi> </munderover> <mo>⁢</mo> <mrow> <msub> <mi>α</mi> <mi>i</mi> </msub> <mo>⁢</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo>⁢</mo> <msubsup> <mi>k</mi> <mi>ij</mi> <mrow> <mi>round</mi> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mo>(</mo> <mi>q</mi> <mo>)</mo> </mrow> </mrow> </msubsup> </mrow> </mrow> <mo>-</mo> <msub> <mi>y</mi> <mi>j</mi> </msub> </mrow> <mo>)</mo> </mrow> <mo>-</mo> <msub> <mi>y</mi> <mi>j</mi> </msub> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>9</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0057">
      <maths id="MATH-US-00009" num="00009"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <msubsup> <mi>α</mi> <mn>2</mn> <mrow> <mi>new</mi> <mo>,</mo> <mi>clipped</mi> </mrow> </msubsup> <mo>=</mo> <mrow> <mo>{</mo> <mtable> <mtr> <mtd> <mi>H</mi> </mtd> <mtd> <mi>if</mi> </mtd> <mtd> <mrow> <msubsup> <mi>α</mi> <mn>2</mn> <mi>new</mi> </msubsup> <mo>≥</mo> <mi>H</mi> </mrow> </mtd> </mtr> <mtr> <mtd> <msubsup> <mi>α</mi> <mn>2</mn> <mi>new</mi> </msubsup> </mtd> <mtd> <mi>if</mi> </mtd> <mtd> <mrow> <mi>L</mi> <mo>&lt;</mo> <msubsup> <mi>α</mi> <mn>2</mn> <mi>new</mi> </msubsup> <mo>&lt;</mo> <mi>H</mi> </mrow> </mtd> </mtr> <mtr> <mtd> <mi>L</mi> </mtd> <mtd> <mi>if</mi> </mtd> <mtd> <mrow> <msubsup> <mi>α</mi> <mn>2</mn> <mi>new</mi> </msubsup> <mo>≤</mo> <mi>L</mi> </mrow> </mtd> </mtr> </mtable> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>10</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mtable> <mtr> <mtd> <mrow> <mrow> <mi>L</mi> <mo>=</mo> <mrow> <mi>max</mi> <mo>⁢</mo> <mrow> <mo>(</mo> <mrow> <mn>0</mn> <mo>,</mo> <mrow> <msub> <mi>α</mi> <mn>2</mn> </msub> <mo>-</mo> <msub> <mi>α</mi> <mn>1</mn> </msub> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>,</mo> </mrow> </mtd> <mtd> <mrow> <mi>H</mi> <mo>=</mo> <mrow> <mi>min</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mi>C</mi> <mo>,</mo> <mrow> <mi>C</mi> <mo>+</mo> <msub> <mi>α</mi> <mn>2</mn> </msub> <mo>-</mo> <msub> <mi>α</mi> <mn>1</mn> </msub> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mtd> </mtr> </mtable> </mtd> <mtd> <mrow> <mo>(</mo> <mn>11</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <msubsup> <mi>α</mi> <mn>1</mn> <mi>new</mi> </msubsup> <mo>=</mo> <mrow> <msub> <mi>α</mi> <mn>1</mn> </msub> <mo>+</mo> <mrow> <mi>s</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>α</mi> <mn>2</mn> </msub> <mo>-</mo> <msubsup> <mi>α</mi> <mn>2</mn> <mrow> <mi>new</mi> <mo>,</mo> <mi>clipped</mi> </mrow> </msubsup> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> <mo>,</mo> <mrow> <mi>s</mi> <mo>=</mo> <mrow> <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>⁢</mo> <msub> <mi>y</mi> <mn>2</mn> </msub> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>12</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0058">However, other embodiments may implement other modifications and variations to block <b>412</b>.</p>
    <p num="p-0059">Then, in block <b>413</b>, the Lagrange multiplier logic <b>222</b> may update the error cache by computing the errors associated with the updated Lagrange multipliers in the active set with the equation (8).</p>
    <p num="p-0060">Then, the Lagrange multiplier logic <b>222</b> may continue to update other Lagrange multipliers in the Lagrange multiplier matrix in blocks <b>408</b>-<b>413</b>, until all of the Lagrange multipliers in the matrix fulfill KKT conditions.</p>
    <p num="p-0061">Then, the weight logic <b>223</b> may compute the weight vector ({right arrow over (ω)}) based on the Lagrange multipliers obtained in blocks <b>408</b>-<b>413</b>, wherein each component of the vector corresponds to each of the feature. In an embodiment for linear SVM training, weight for each feature may be obtained with the following equation:</p>
    <p num="p-0062"> <maths id="MATH-US-00010" num="00010"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <msub> <mi>w</mi> <mi>k</mi> </msub> <mo>=</mo> <mrow> <munderover> <mo>∑</mo> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>m</mi> </munderover> <mo>⁢</mo> <mrow> <msub> <mi>α</mi> <mi>i</mi> </msub> <mo>⁢</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo>⁢</mo> <msub> <mi>x</mi> <mi>ik</mi> </msub> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>13</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br>
wherein, w<sub>k </sub>represents a weight for k<sup>th </sup>feature, m represent the number of the training samples, x<sub>ik </sub>represents the training samples corresponding to the k<sup>th </sup>feature. However, other embodiments may implement other modifications and variations to block <b>414</b>.
</p>
    <p num="p-0063">Although the present invention has been described in conjunction with certain embodiments, it shall be understood that modifications and variations may be resorted to without departing from the spirit and scope of the invention as those skilled in the art readily understand. Such modifications and variations are considered to be within the scope of the invention and the appended claims.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20030172043">US20030172043</a></td><td class="patent-data-table-td patent-date-value">Jan 24, 2002</td><td class="patent-data-table-td patent-date-value">Sep 11, 2003</td><td class="patent-data-table-td ">Isabelle Guyon</td><td class="patent-data-table-td ">Methods of identifying patterns in biological systems and uses thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2004105573A2?cl=en">WO2004105573A2</a></td><td class="patent-data-table-td patent-date-value">May 21, 2004</td><td class="patent-data-table-td patent-date-value">Dec 9, 2004</td><td class="patent-data-table-td ">Laszlo Kari</td><td class="patent-data-table-td ">Method of diagnosis of cancer based on gene expression profiles in cells</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">‘An introduction to support vector machines and other kernel based learning methods’: Cristianini, 2000, Cambridge University Press, p. 18, 29, 30, 45, 125.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">‘Gene selection for cancer classification using support vector machines’: Guyon, 2002, Machine Learning, vol. 46, pp. 389-422 (1-39).</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ambroise et al: Selection Bias in gene Extraction on the Basis of Microarray Gene-Expression Data, Proceedings of the National Academy of Sciences, vol. 99, No. 10, May 14, 2002, 5 pages www.pnas.org/cgi/doi/10.1073/pnas.102102699.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">'<a href='http://scholar.google.com/scholar?q="An+introduction+to+support+vector+machines+and+other+kernel+based+learning+methods"'>An introduction to support vector machines and other kernel based learning methods</a>': Cristianini, 2000, Cambridge University Press, p. 18, 29, 30, 45, 125.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">An Introduction to Support Vector Machines and Other Kernel-based Learning Methods 4 pages.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Boser et al: A Training Algorithm for Optimal Margin classifiers, AT &amp;T Bell Labs / Univ. of California, Berkeley, 9 pages.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Chih-Chung Chang, et al: LIBSVM: A Library for Support Vector Machines, De'<a href='http://scholar.google.com/scholar?q="t+of+Computer+Science+and+Information+Engineering%2C+Nat"'>t of Computer Science and Information Engineering, Nat</a>'l Taiwan Univ, Taipei, Taiwan, Last updated: Mar. 28, 2004.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">'<a href='http://scholar.google.com/scholar?q="Gene+selection+for+cancer+classification+using+support+vector+machines"'>Gene selection for cancer classification using support vector machines</a>': Guyon, 2002, Machine Learning, vol. 46, pp. 389-422 (1-39).</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Golub et al: Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring, Science, vol. 286, Oct. 15, 1999, www.sciencemag.org.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Guyon et al: Gene Selection for Cancer Classification Using Support Vector Machines, Barnhill Bioinformatics, Savannah, GA AT &amp; T Labs, Red Bank, NJ, USA pp. 1-39.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">International Preliminary Report on Patentability, International Application No. PCT/CN2005/001242, mailing date Jun. 15, 2006, 4.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">International Search Report and Written Opinion, International Application No. PCT/CN2005/001242, mailing date Jun. 15, 2006.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Joachims: Making Large-Scale SVM Learning Practical, chapter 11, 16 pages To be pub'd in: Advances in Kernel Methods-Support Vector Learning, MIT Press, (Cambridge, USA, 1998.</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Joachims: Making Large-Scale SVM Learning Practical, chapter 11, 16 pages To be pub'd in: Advances in Kernel Methods—Support Vector Learning, MIT Press, (Cambridge, USA, 1998.</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Osuna, et al: Training Support Vector Machines: an Application to Face Detection, To appear in the Proceedings of CVPR '97, Jun. 17-19, 1997, Puerto Rico Center for biological and Computational Learning and Operations Research Center, MIT, Cambridge, MA, USA, 8 pages.</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Petricoin et al: Use of Proteomic Patterns in Serum to Identify Ovarian Cancer, The Lancet, vol. 359, Feb. 16, 2002, Mechanisms of Disease, pp. 572-577 www.thelancet.com.</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Platt: Fast Training of Support Vector Machines Using Sequential minimal Optimization, Microsoft REsearch, Redmond, WA www.research.microsoft.com/~jplatt.</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Platt: Fast Training of Support Vector Machines Using Sequential minimal Optimization, Microsoft REsearch, Redmond, WA www.research.microsoft.com/˜jplatt.</td></tr><tr><td class="patent-data-table-td ">19</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Written Opinion of the International Searching Authority, International Application No. PCT/CN2005/001242, mailing date Jun. 15, 2006, 3.</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8095483">US8095483</a></td><td class="patent-data-table-td patent-date-value">Dec 1, 2010</td><td class="patent-data-table-td patent-date-value">Jan 10, 2012</td><td class="patent-data-table-td ">Health Discovery Corporation</td><td class="patent-data-table-td ">Support vector machine—recursive feature elimination (SVM-RFE)</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=QKBlBgABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc706/defs706.htm&usg=AFQjCNEPcb-u7SHyqWIxu4byLMFo1P7uDw#C706S012000">706/12</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=QKBlBgABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0015180000">G06F15/18</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=QKBlBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/6269">G06K9/6269</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=QKBlBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/6231">G06K9/6231</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=QKBlBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06N99/005">G06N99/005</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06N99/00L</span>, <span class="nested-value">G06K9/62B3S</span>, <span class="nested-value">G06K9/62C1B</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Aug 28, 2013</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 9, 2012</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIMS 1, 6 AND 11 ARE DETERMINED TO BE PATENTABLE AS AMENDED.CLAIMS 2-5, 7-10 AND 12-15, DEPENDENTON AN AMENDED CLAIM, ARE DETERMINED TO BE PATENTABLE.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 10, 2012</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20111110</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 22, 2008</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">INTEL CORPORATION, CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LI, ERIC Q.;DIAO, QIAN;REEL/FRAME:020421/0603;SIGNING DATES FROM 20060707 TO 20060712</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">INTEL CORPORATION,CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LI, ERIC Q.;DIAO, QIAN;SIGNED BETWEEN 20060707 AND 20060712;US-ASSIGNMENT DATABASE UPDATED:20100323;REEL/FRAME:20421/603</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LI, ERIC Q.;DIAO, QIAN;SIGNING DATES FROM 20060707 TO 20060712;REEL/FRAME:020421/0603</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U1diXyDNgnxVRgZlbH-mNeiH0lPOQ\u0026id=QKBlBgABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U3pdTc8k8_Voe1XFIZZQadVtzi11A\u0026id=QKBlBgABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U39vN6trPcW9pita-OhNDmyZZYtAQ","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Recursive_feature_eliminating_method_bas.pdf?id=QKBlBgABERAJ\u0026output=pdf\u0026sig=ACfU3U1oFBlq0DjtGZcmkI38xDDr56WCEA"},"sample_url":"http://www.google.com/patents/reader?id=QKBlBgABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>