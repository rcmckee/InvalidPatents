<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7190834 - Methods for finding and characterizing a deformed pattern in an image - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Methods for finding and characterizing a deformed pattern in an image"><meta name="DC.contributor" content="Jason Davis" scheme="inventor"><meta name="DC.contributor" content="Cognex Technology And Investment Corporation" scheme="assignee"><meta name="DC.date" content="2003-7-22" scheme="dateSubmitted"><meta name="DC.description" content="A method is disclosed for finding a deformed pattern in an image using a plurality of sub-patterns. By advantageously restricting sub-pattern search ranges, search speed is improved, and the incidence of spurious matches is reduced. The method also quickly decides which sub-pattern result, of several potential candidates, is most likely to be the correct match for a deformed sub-pattern. Also, a method is provided for characterizing a deformed pattern in an image by using results from feature-based search tools to create a mapping that models the deformation of the pattern. A transform, selectable by a user, is fit to the results from the search tools to create a global deformation mapping. This transformation is fit only to feature points derived from matches resulting from successful sub-pattern search, without including data from areas of the pattern that were blank, not matched, or otherwise didn&#39;t contain information about the pattern&#39;s distorted location."><meta name="DC.date" content="2007-3-13" scheme="issued"><meta name="DC.relation" content="DE:4406020" scheme="references"><meta name="DC.relation" content="US:20020054699:A1" scheme="references"><meta name="DC.relation" content="US:3936800" scheme="references"><meta name="DC.relation" content="US:4115702" scheme="references"><meta name="DC.relation" content="US:4115762" scheme="references"><meta name="DC.relation" content="US:4183013" scheme="references"><meta name="DC.relation" content="US:4200861" scheme="references"><meta name="DC.relation" content="US:4441248" scheme="references"><meta name="DC.relation" content="US:4570180" scheme="references"><meta name="DC.relation" content="US:4685143" scheme="references"><meta name="DC.relation" content="US:4688088" scheme="references"><meta name="DC.relation" content="US:4736437" scheme="references"><meta name="DC.relation" content="US:4763280" scheme="references"><meta name="DC.relation" content="US:4783826" scheme="references"><meta name="DC.relation" content="US:4860374" scheme="references"><meta name="DC.relation" content="US:4876457" scheme="references"><meta name="DC.relation" content="US:4876728" scheme="references"><meta name="DC.relation" content="US:4922543" scheme="references"><meta name="DC.relation" content="US:4955062" scheme="references"><meta name="DC.relation" content="US:4959898" scheme="references"><meta name="DC.relation" content="US:4980971" scheme="references"><meta name="DC.relation" content="US:5060276" scheme="references"><meta name="DC.relation" content="US:5086478" scheme="references"><meta name="DC.relation" content="US:5113565" scheme="references"><meta name="DC.relation" content="US:5161201" scheme="references"><meta name="DC.relation" content="US:5206917" scheme="references"><meta name="DC.relation" content="US:5226095" scheme="references"><meta name="DC.relation" content="US:5268999" scheme="references"><meta name="DC.relation" content="US:5343028" scheme="references"><meta name="DC.relation" content="US:5371690" scheme="references"><meta name="DC.relation" content="US:5471541" scheme="references"><meta name="DC.relation" content="US:5495537" scheme="references"><meta name="DC.relation" content="US:5497451" scheme="references"><meta name="DC.relation" content="US:5500906" scheme="references"><meta name="DC.relation" content="US:5545887" scheme="references"><meta name="DC.relation" content="US:5602937" scheme="references"><meta name="DC.relation" content="US:5621807" scheme="references"><meta name="DC.relation" content="US:5625715" scheme="references"><meta name="DC.relation" content="US:5627912" scheme="references"><meta name="DC.relation" content="US:5627915" scheme="references"><meta name="DC.relation" content="US:5663809" scheme="references"><meta name="DC.relation" content="US:5828769" scheme="references"><meta name="DC.relation" content="US:5845288" scheme="references"><meta name="DC.relation" content="US:5933516" scheme="references"><meta name="DC.relation" content="US:6636634" scheme="references"><meta name="DC.relation" content="US:6691145" scheme="references"><meta name="DC.relation" content="US:6785419" scheme="references"><meta name="DC.relation" content="US:6909798" scheme="references"><meta name="citation_reference" content="Belongie, S. , et al., &quot;Shape Matching and Object Recognition Using Shape Contexts&quot;, IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Inc. New York, vol. 24, No. 4, (Apr. 2003),509-522."><meta name="citation_reference" content="Bileschi, S. , et al., &quot;Advances in Component-based Face Detection&quot;, Lecture notes in Computer Science, Springer Verlag, New York, NY, vol. 2388, (2002),135-143."><meta name="citation_reference" content="Bookstein, F L., &quot;Principal Warps: Thin-Plate Splines and the Decomposition of Deformations&quot;, IEEE Transactions on pattern Analysis and Machine Intelligence, IEEE Inc., New York, vol. 11, No. 6, (Jun. 1, 1989)."><meta name="citation_reference" content="Cognex Corporation, Cognex MVS-8000 Series, CVL Vision Tools Guide, pp. 25-136, Release 5.4 590-6271, Natick, MA USA 2000."><meta name="citation_reference" content="deFigueiredo et al. Model Based Orientation Independent 3-D Machine Vision Techniques, IEEE Transactions on Aerospace and Electronic Systems, vol. 24, No. 5 Sep. 1988, pp. 597-607."><meta name="citation_reference" content="Gdalyahu Y et al.: Self-Organization in Vision: Stochastic Clustering for Image Segmentation Perceptual Grouping, and Image Database Organization: IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Inc. New York, vol. 23, No. 10, Oct. 2001, pp. 1053-1074."><meta name="citation_reference" content="Hoogs et al., &quot;Model-Based Learning of Segmentations&quot;, IEEE, pp. 494-499, 1996."><meta name="citation_reference" content="J, Michael F., et al., &quot;Handbook of Medical Imaging&quot;, vol. 2: Medical image Processing and Analysis, SPIE Press, Bellingham, WA,(2000),Chapter 8."><meta name="citation_reference" content="Jianbo Shi et al: Normalized Cuts and Image Segmentation, Computer Vision and Pattern Recognition, 1997. Proceedings, 1997 IEEE Computer Society Conference, San Juan, Puerto Rico Jun. 17-19, 1997, pp. 731-737."><meta name="citation_reference" content="Medina-Mora, R., &quot;An Incremental Programming Environment,&quot; IEEE Transactions on Software Engineering, Sep. 1981, pp. 472-482, vol. SE-7, No. 5, 1992."><meta name="citation_reference" content="Mehrotra R et al: Feature-based retrieval of similar shapes, Proceeding of the International Conference on Data Engineering. Vienna, Apr. 19-23, 1993, pp. 108-115."><meta name="citation_reference" content="Newman et al., &quot;3D CAD-Based Inspection I: Coarse Verification&quot;, IEEE, pp. 49-52, 1992."><meta name="citation_reference" content="Ohm, Jens-Rainer , &quot;Digitale Bildcodierung&quot;, Springer Verlag, Berlin 217580, XP002303066, Section 6.2 Bewegungschatzung,(1995)."><meta name="citation_reference" content="Pauwels E J et al: Finding Salient Regions in Images-Nonparametric Clustering for Image Segmentation and Grouping Computer Vision and Image Understanding, Academic Press, San Diego, CA, vol. 75, No. 1-2, Jul. 1999, pp. 73-85."><meta name="citation_reference" content="Scanlon J. et al.: Graph-theoretic Algorithms for Image Segmentation, Circuits and Systems, 1999, ISCAS &#39;99. Proceedings of the 1999 IEEE International Symposium, Orlando, FL, May 30, 1999, pp. 141-144."><meta name="citation_reference" content="Stockman, G , et al., &quot;Matching images to models for registration and object detection via clustering&quot;, IEEE Transaction of Pattern Analysis and Machine Intelligence, IEEE Inc., New York, vol. PAMI-4, No. 3,,(1982)."><meta name="citation_reference" content="Ullman, S., &quot;Aligning pictorial descriptions: An approach to object recognition, I: Approaches to Object Recognition,&quot; reprinted from Cognition, pp. 201-214, vol. 32, No. 3, Cambridge, MA USA, Aug. 1989."><meta name="citation_reference" content="Wei, Wen , et al., &quot;Recognition and Insprection of Two-Dimensional Industrial Parts Using Subpolygons&quot;, Pattern Recognition, Elsevier, Kidlington, GB, vol. 25, No. 12, (Dec. 1, 1992),1427-1434."><meta name="citation_reference" content="Xie X L et al. A new fuzzy clustering criterion and its application to color image segmentation, Proceedings of the International Symposium on Intelligent Control, Arlington, Aug. 13-15, 1991, pp. 463-468."><meta name="citation_reference" content="Zhang, Zhengyou , &quot;Parameter estimation techniques: A tutorial with application to conic fitting&quot;, Imag and Vision Comput; Image and Vision computing: Elsevier Science Ltd. Oxford England, vol. 15, No. 1,(Jan. 1, 1997)."><meta name="citation_patent_number" content="US:7190834"><meta name="citation_patent_application_number" content="US:10/625,205"><link rel="canonical" href="http://www.google.com/patents/US7190834"/><meta property="og:url" content="http://www.google.com/patents/US7190834"/><meta name="title" content="Patent US7190834 - Methods for finding and characterizing a deformed pattern in an image"/><meta name="description" content="A method is disclosed for finding a deformed pattern in an image using a plurality of sub-patterns. By advantageously restricting sub-pattern search ranges, search speed is improved, and the incidence of spurious matches is reduced. The method also quickly decides which sub-pattern result, of several potential candidates, is most likely to be the correct match for a deformed sub-pattern. Also, a method is provided for characterizing a deformed pattern in an image by using results from feature-based search tools to create a mapping that models the deformation of the pattern. A transform, selectable by a user, is fit to the results from the search tools to create a global deformation mapping. This transformation is fit only to feature points derived from matches resulting from successful sub-pattern search, without including data from areas of the pattern that were blank, not matched, or otherwise didn&#39;t contain information about the pattern&#39;s distorted location."/><meta property="og:title" content="Patent US7190834 - Methods for finding and characterizing a deformed pattern in an image"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("IYftU_HZPOGusQSI3YCwBA"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("GBR"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("IYftU_HZPOGusQSI3YCwBA"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("GBR"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7190834?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7190834"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=j2Z6BAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7190834&amp;usg=AFQjCNFHd7yL5F7Z0SnoAjtLp0L7Flvwzg" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7190834.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7190834.pdf"></a><a class="appbar-application-grant-link" data-label="Application" href="/patents/US20050018904"></a><a class="appbar-application-grant-link" data-selected="true" data-label="Grant" href="/patents/US7190834"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7190834" style="display:none"><span itemprop="description">A method is disclosed for finding a deformed pattern in an image using a plurality of sub-patterns. By advantageously restricting sub-pattern search ranges, search speed is improved, and the incidence of spurious matches is reduced. The method also quickly decides which sub-pattern result, of several...</span><span itemprop="url">http://www.google.com/patents/US7190834?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7190834 - Methods for finding and characterizing a deformed pattern in an image</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7190834 - Methods for finding and characterizing a deformed pattern in an image" title="Patent US7190834 - Methods for finding and characterizing a deformed pattern in an image"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7190834 B2</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 10/625,205</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Mar 13, 2007</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Jul 22, 2003</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Jul 22, 2003</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US8345979">US8345979</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20050018904">US20050018904</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20070183668">US20070183668</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20130188870">US20130188870</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO2005010803A2">WO2005010803A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO2005010803A3">WO2005010803A3</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">10625205, </span><span class="patent-bibdata-value">625205, </span><span class="patent-bibdata-value">US 7190834 B2, </span><span class="patent-bibdata-value">US 7190834B2, </span><span class="patent-bibdata-value">US-B2-7190834, </span><span class="patent-bibdata-value">US7190834 B2, </span><span class="patent-bibdata-value">US7190834B2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Jason+Davis%22">Jason Davis</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Cognex+Technology+And+Investment+Corporation%22">Cognex Technology And Investment Corporation</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7190834.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7190834.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7190834.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (48),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (20),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (11),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (8),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (5)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=j2Z6BAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7190834&usg=AFQjCNHXpAjrFjBlpKszsZ7XZ-7h5lVBsA">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=j2Z6BAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7190834&usg=AFQjCNG9Tg66BRWtC6uyWGVUUPhoe9G-CA">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=j2Z6BAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7190834B2%26KC%3DB2%26FT%3DD&usg=AFQjCNEc7crspQgHm0pHphr9S2eeC8k6HA">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55714129" lang="EN" load-source="patent-office">Methods for finding and characterizing a deformed pattern in an image</invention-title></span><br><span class="patent-number">US 7190834 B2</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA51121349" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">A method is disclosed for finding a deformed pattern in an image using a plurality of sub-patterns. By advantageously restricting sub-pattern search ranges, search speed is improved, and the incidence of spurious matches is reduced. The method also quickly decides which sub-pattern result, of several potential candidates, is most likely to be the correct match for a deformed sub-pattern. Also, a method is provided for characterizing a deformed pattern in an image by using results from feature-based search tools to create a mapping that models the deformation of the pattern. A transform, selectable by a user, is fit to the results from the search tools to create a global deformation mapping. This transformation is fit only to feature points derived from matches resulting from successful sub-pattern search, without including data from areas of the pattern that were blank, not matched, or otherwise didn't contain information about the pattern's distorted location.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(11)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7190834B2/US07190834-20070313-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7190834B2/US07190834-20070313-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7190834B2/US07190834-20070313-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7190834B2/US07190834-20070313-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7190834B2/US07190834-20070313-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7190834B2/US07190834-20070313-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7190834B2/US07190834-20070313-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7190834B2/US07190834-20070313-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7190834B2/US07190834-20070313-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7190834B2/US07190834-20070313-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7190834B2/US07190834-20070313-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7190834B2/US07190834-20070313-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7190834B2/US07190834-20070313-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7190834B2/US07190834-20070313-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7190834B2/US07190834-20070313-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7190834B2/US07190834-20070313-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7190834B2/US07190834-20070313-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7190834B2/US07190834-20070313-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7190834B2/US07190834-20070313-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7190834B2/US07190834-20070313-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7190834B2/US07190834-20070313-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7190834B2/US07190834-20070313-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(22)</span></span></div><div class="patent-text"><div mxw-id="PCLM9175538" lang="EN" load-source="patent-office" class="claims">
  <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
    <div class="claim-text">1. A method for finding a deformed pattern in an image, the method comprising:
<div class="claim-text">providing a plurality of features that represent the deformed pattern in the image;</div>
<div class="claim-text">dividing the plurality of features into a plurality of sub-pluralities, each sub-plurality representing a sub-pattern in the image, a plurality of the sub-patterns representing the deformed pattern;</div>
<div class="claim-text">determining a distance between each pair of sub-patterns of the plurality of sub-pluralities;</div>
<div class="claim-text">selecting a first sub-pattern to locate in the image;</div>
<div class="claim-text">locating the first sub-pattern in the image so as to provide a first sub-pattern location;</div>
<div class="claim-text">using the first sub-pattern location to select a second sub-pattern to locate in the image; and</div>
<div class="claim-text">locating the second sub-pattern in the image so as to provide a second sub-pattern location; and</div>
<div class="claim-text">using the first sub-pattern location and the second sub-pattern location to determine a location of the deformed pattern.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
    <div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein providing a plurality of features that represent the deformed pattern in the image includes:
<div class="claim-text">detecting features in the image.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
    <div class="claim-text">3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein detecting features in the image includes:
<div class="claim-text">detecting features in the image using a Sobel edge detector.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
    <div class="claim-text">4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining a distance between each pair of sub-patterns of the plurality of sub-patterns includes:
<div class="claim-text">storing each distance for later use.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
    <div class="claim-text">5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining a distance between each pair of sub-patterns of the plurality of sub-patterns includes:
<div class="claim-text">determining the minimum distance between the pair of sub-patterns.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
    <div class="claim-text">6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining a distance between each pair of sub-patterns of the plurality of sub-patterns includes:
<div class="claim-text">determining the distance between a first feature of a first sub-pattern and a second feature of a second sub-pattern.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
    <div class="claim-text">7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining a distance between each pair of sub-patterns of the plurality of sub-patterns includes:
<div class="claim-text">determining the distance between a first center of a first sub-pattern and a second center of a second sub-pattern.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
    <div class="claim-text">8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein locating the first sub-pattern in the image so as to provide a first sub-pattern location includes:
<div class="claim-text">using a feature-based search method for locating the first sub-pattern in the image.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
    <div class="claim-text">9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein locating the second sub-pattern in the image so as to provide a second sub-pattern location includes:
<div class="claim-text">computing a search area using the location of the first sub-pattern.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
    <div class="claim-text">10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein locating the second sub-pattern in the image so as to provide a second sub-pattern location includes:
<div class="claim-text">computing an expected angle and an expected scale of the second sub-pattern.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
    <div class="claim-text">11. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the expected scale is the expected X-dimension scale and the expected Y-dimension scale.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
    <div class="claim-text">12. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further including computing an expected aspect ratio of the second sub-pattern.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
    <div class="claim-text">13. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further including:
<div class="claim-text">using the expected angle and the expected scale of the second sub-pattern so as to provide an angular search range and a scale search range.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
    <div class="claim-text">14. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the scale search range is an X-dimension scale search range, and a Y-dimension scale search range.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
    <div class="claim-text">15. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further including:
<div class="claim-text">using a distance between the first sub-pattern and the second sub-pattern; and</div>
<div class="claim-text">a deformation rate.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
    <div class="claim-text">16. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein locating the second sub-pattern in the image so as to provide a second sub-pattern location includes:
<div class="claim-text">computing a search area using the location of the first sub-pattern;</div>
<div class="claim-text">computing an expected angle of the second sub-pattern and an expected scale of the second sub-pattern using the angle and scale of the first sub-pattern;</div>
<div class="claim-text">using the expected angle of the second sub-pattern, the expected scale of the second sub-pattern, a distance between the first sub-pattern and the second sub-pattern, and a deformation rate so as to provide an expanded search area, an angular search range, and a scale search range; and</div>
<div class="claim-text">locating the second sub-pattern within the expanded search area, the angular search range, and the scale search range so as to provide a second sub-pattern location.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
    <div class="claim-text">17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein using the expected angle of the second sub-pattern, the expected scale of the second sub-pattern, a distance between the first sub-pattern and the second sub-pattern, and a deformation rate so as to provide an expanded search area, an angular search range, and a scale search range includes:
<div class="claim-text">increasing the size of the expanded search area, the angular search range, and the scale search range upon an increase in the distance between the first sub-pattern and the second sub-pattern.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
    <div class="claim-text">18. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein using the expected angle of the second sub-pattern, the expected scale of the second sub-pattern, a distance between the first sub-pattern and the second sub-pattern, and a deformation rate so as to provide an expanded search area, an angular search range, and a scale search range includes:
<div class="claim-text">increasing, in proportion to the deformation rate, the size of the expanded search area, the angular search range, and the scale search range.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
    <div class="claim-text">19. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein using the expected angle of the second sub-pattern, the expected scale of the second sub-pattern, a distance between the first sub-pattern and the second sub-pattern, and a deformation rate so as to provide an expanded search area, an angular search range, and a scale search range includes:
<div class="claim-text">increasing, in proportion to the deformation rate, the size of the expanded search area, the angular search range, and the scale search range upon an increase in the distance between the first sub-pattern and the second sub-pattern.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
    <div class="claim-text">20. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein computing an expected angle of the second sub-pattern and an expected scale of the second sub-pattern using the angle and scale of the first sub-pattern includes:
<div class="claim-text">computing an expected X-dimension scale and an expected Y-dimension scale using the X-dimension scale of the first sub-pattern and the Y-dimension scale of the first sub-pattern, respectively.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00021" num="00021" class="claim">
    <div class="claim-text">21. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein computing an expected angle of the second sub-pattern and an expected scale of the second sub-pattern using the angle and scale of the first sub-pattern includes:
<div class="claim-text">computing an angular uncertainty and a scale uncertainty of the second sub-pattern using an angular uncertainty and a scale uncertainty of the first sub-pattern, respectively.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00022" num="00022" class="claim">
    <div class="claim-text">22. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein computing an angular uncertainty and a scale uncertainty of the second sub-pattern using an angular uncertainty and a scale uncertainty of the first sub-pattern, respectively includes:
<div class="claim-text">computing an X-dimension scale uncertainty and a Y-dimension scale uncertainty of the second sub-pattern using an X-dimension scale uncertainty of the first sub-pattern, and a Y-dimension scale uncertainty of the first sub-pattern, respectively.</div>
</div>
  </div>
</div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES16198082" lang="EN" load-source="patent-office" class="description">
<heading>FIELD OF THE INVENTION</heading> <p num="p-0002">This invention relates to machine vision systems, and particularly to methods for searching for a pattern in an image.</p>
  <heading>BACKGROUND OF THE INVENTION</heading> <p num="p-0003">The problem of how to find a particular pattern in an image is a well-known problem with many known solutions, such as feature-based search methods. Typically, the pattern is assumed to have undergone one or more of a few basic transformations, such as being scaled, or being rotated. However, these known solutions often fail if the pattern has been deformed by being warped, pulled, bent, wrinkled, damaged, or otherwise fundamentally changed from the original un-deformed shape that the search process is adapted to find.</p>
  <p num="p-0004">Nevertheless, even after transformation of the whole pattern, if the deformed pattern is divided into smaller sub-patterns, those sub-patterns are themselves fairly close in form to the corresponding parts of the original undeformed pattern. For example, if the pattern is bent into a “V” or boomerang shape, then the two legs of the boomerang both represent good, easily-findable portions of the pattern. It's only the deformed pattern that is hard to find as an entirety. Therefore, searching for a deformed pattern in an image may be facilitated by dividing the deformed pattern into smaller sub-patterns. For many typical types of deformation encountered, most of those sub-patterns are probably findable by known feature-based search methods (because the sub-patterns are not themselves substantially distorted).</p>
  <p num="p-0005">However, it is then necessary for a subsequent algorithm to combine the sub-pattern search results into a full match of the distorted whole pattern. Unfortunately, searching for a large number of sub-patterns in an image takes much longer than searching for a single whole pattern. Furthermore, sub-patterns are inherently simpler than the whole pattern, so they're more likely to be confused when they are used in searching the target image, potentially yielding many spurious matches in various locations of the target image.</p>
  <p num="p-0006">It is sometimes useful to characterize the deformation of the deformed whole pattern after it has been found. However, even though a deformed whole pattern has been found, a characterization of the deformation may still not be known. The deformation of the whole pattern can be characterized after each of the sub-patterns have been located. In some cases, the deformation may be easily characterized, such as the deformation due to 3D perspective, or the deformation due to the whole pattern being wrapped around a cylinder, such as when a label is wrapped around a can. In other cases, the deformation may be more atypical, representing random wrinkles, folds, bends, dents, and so forth, and is consequently not characterized by a known or standard transformation. In either case, an automated method of characterizing deformation after finding a deformed pattern would be useful in some applications, e.g., inspecting printing on soda cans, inspecting labels on oddly shaped containers such as bags of sugar, or inspecting lot and date codes on medicine bottles. With a deformation transform that maps between the deformed pattern (e.g. the logo on a full bag of sugar or the date code on a curved bottle) and the original undeformed pattern (e.g., the logo as it would appear on a flat piece of paper), tasks using inspection tools suitable for undeformed patterns can be performed, such as inspection of logo print quality. This would be impossible without a characterization, and un-doing based on the characterization, of the deformation of the bag or bottle, because wrinkles or curvature would easily be misclassified as a print defects using inspection tools suitable for undeformed patterns, even though the print quality might otherwise be acceptable.</p>
  <heading>SUMMARY OF THE INVENTION</heading> <p num="p-0007">One general aspect of the invention is a method for finding a deformed pattern in an image. The method includes providing a plurality of features that represent the deformed pattern in the image, and then dividing the plurality of features into a plurality of sub-pluralities, each sub-plurality representing a sub-pattern in the image, a plurality of the sub-patterns representing the deformed pattern. Next a distance is determined between each pair of sub-patterns of the plurality of sub-pluralities. Then, a first sub-pattern is selected to locate in the image, locating the first sub-pattern in the image so as to provide a first sub-pattern location. Next, the first sub-pattern location is used to select a second sub-pattern to locate in the image, which is then located in the image so as to provide a second sub-pattern location. The first sub-pattern location and the second sub-pattern location are then used to determine a location of the deformed patter.</p>
  <p num="p-0008">In a preferred embodiment, providing a plurality of features that represent the deformed pattern in the image includes detecting features in the image. In a further preferred embodiment, detecting features in the image includes detecting features in the image using a Sobel edge detector.</p>
  <p num="p-0009">In another preferred embodiment, determining a distance between each pair of sub-patterns of the plurality of sub-patterns includes storing each distance for later use. In another embodiment, determining a distance between each pair of sub-patterns of the plurality of sub-patterns includes determining the minimum distance between the pair of sub-patterns. In yet another embodiment, determining a distance between each pair of sub-patterns of the plurality of sub-patterns includes determining the distance between a first feature of a first sub-pattern and a second feature of a second sub-pattern. In still another embodiment, determining a distance between each pair of sub-patterns of the plurality of sub-patterns includes determining the distance between a first center of a first sub-pattern and a second center of a second sub-pattern.</p>
  <p num="p-0010">In a preferred embodiment, locating the first sub-pattern in the image so as to provide a first sub-pattern location includes using a feature-based search method for locating the first sub-pattern in the image. In another embodiment, locating the second sub-pattern in the image so as to provide a second sub-pattern location includes computing a search area using the location of the first sub-pattern. In an alternate embodiment, locating, the second sub-pattern in the image so as to provide a second sub-pattern location includes computing an expected angle and an expected scale of the second sub-pattern.</p>
  <p num="p-0011">In a preferred embodiment, the expected scale is the expected X-dimension scale and the expected Y-dimension scale. In a further preferred embodiment, an expected aspect ratio of the second sub-pattern is also computed. In another embodiment, the expected angle and the expected scale of the second sub-pattern is used so as to provide an angular search range and a scale search range. In a further preferred embodiment, the scale search range is an X-dimension scale search range, and a Y-dimension scale search range. In another further embodiment, a distance between the first sub-pattern and the second sub-pattern, and a deformation rate, is used.</p>
  <p num="p-0012">In a preferred embodiment of the method of the invention, locating the second sub-pattern in the image so as to provide a second sub-pattern location includes computing a search area using the location of the first sub-pattern, and then computing an expected angle of the second sub-pattern and an expected scale of the second sub-pattern using the angle and scale of the first sub-pattern. Next, the expected angle of the second sub-pattern, the expected scale of the second sub-pattern, a distance between the first sub-pattern and the second sub-pattern, and a deformation rate are all used so as to provide an expanded search area, an angular search range, and a scale search range. Then, the second sub-pattern is located within the expanded search area, the angular search range, and the scale search range so as to provide a second sub-pattern location.</p>
  <p num="p-0013">In a further preferred embodiment, using the expected angle of the second sub-pattern, the expected scale of the second sub-pattern, a distance between the first sub-pattern and the second sub-pattern, and a deformation rate so as to provide an expanded search area, an angular search range, and a scale search range includes increasing the size of the expanded search area, the angular search range, and the scale search range upon an increase in the distance between the first sub-pattern and the second sub-pattern.</p>
  <p num="p-0014">In an alternate further preferred embodiment, using the expected angle of the second sub-pattern, the expected scale of the second sub-pattern, a distance between the first sub-pattern and the second sub-pattern, and a deformation rate so as to provide an expanded search area, an angular search range, and a scale search range includes increasing, in proportion to the deformation rate, the size of the expanded search area, the angular search range, and the scale search range. In yet another embodiment, using the expected angle of the second sub-pattern, the expected scale of the second sub-pattern, a distance between the first sub-pattern and the second sub-pattern, and a deformation rate so as to provide an expanded search area, an angular search range, and a scale search range includes increasing, in proportion to the deformation rate, the size of the expanded search area, the angular search range, and the scale search range upon an increase in the distance between the first sub-pattern and the second sub-pattern.</p>
  <p num="p-0015">In further embodiments, computing an expected angle of the second sub-pattern and an expected scale of the second sub-pattern using the angle and scale of the first sub-pattern includes computing an expected X-dimension scale and an expected Y-dimension scale using the X-dimension scale of the first sub-pattern and the Y-dimension scale of the first sub-pattern, respectively. In another embodiment, computing an expected angle of the second sub-pattern and an expected scale of the second sub-pattern using the angle and scale of the first sub-pattern includes computing an angular uncertainty and a scale uncertainty of the second sub-pattern using an angular uncertainty and a scale uncertainty of the first sub-pattern, respectively. In a further embodiment, computing an angular uncertainty and a scale uncertainty of the second sub-pattern using an angular uncertainty and a scale uncertainty of the first sub-pattern, respectively includes computing an X-dimension scale uncertainty and a Y-dimension scale uncertainty of the second sub-pattern using an X-dimension scale uncertainty of the first sub-pattern, and a Y-dimension scale uncertainty of the first sub-pattern, respectively.</p>
  <p num="p-0016">Another general aspect of the invention is a method for characterizing a deformed pattern in an image. The method includes providing a plurality of features that represent the deformed pattern in the image; dividing the plurality of features into a plurality of sub-pluralities, each sub-plurality representing a sub-pattern in the image, a plurality of the sub-patterns representing the deformed pattern; determining a distance between each pair of sub-patterns of the plurality of sub-pluralities; locating a first sub-pattern in the image so as to provide a first sub-pattern location; locating a second sub-pattern in the image so as to provide a second sub-pattern location; and then using the first sub-pattern location and the second sub-pattern location for determining a deformation mapping that characterizes the deformed pattern in the image.</p>
  <p num="p-0017">Another general aspect of the invention is another method for characterizing a deformed pattern in an image, wherein the method includes dividing the deformed pattern into at least a first sub-pattern and a second sub-pattern; locating the first sub-pattern in the image so as to provide a first sub-pattern pose; locating the second sub-pattern in the image so as to provide a second sub-pattern pose; and then using the first sub-pattern pose and the second sub-pattern pose for determining a deformation mapping that characterizes the deformed pattern in the image.</p>
  <p num="p-0018">In a preferred embodiment, using the first sub-pattern pose and the second sub-pattern pose for determining a deformation mapping that characterizes the deformed pattern in the image includes deriving a plurality of source points from the first sub-pattern and a plurality of source points from the second sub-pattern; generating a plurality of destination points from the source points and the sub-pattern poses; and then using a transform to fit the plurality of source points and plurality of destination points so as to create the global deformation map. In preferred embodiments, the transform is a perspective transform, or is an affine transform, or is a spline transform, or is a thin-plate spline transform, or is a cylinder transform.</p>
  <p num="p-0019">In further embodiments, the transform is fit using a least-squares-fit method, or a Total Variation method, or a Robust M-estimators method, or a Minimum L<sub>p</sub>-Norm Estimation, or a Least Median of Squares method. In other embodiments, the first sub-pattern and the second sub-pattern each include a plurality of feature points. In yet other embodiments, the first sub-pattern and the second sub-pattern each include a region. In a preferred embodiment, each region is converted into feature points. In further preferred embodiments, each region is converted into feature points by representing each sub-pattern as a region having a boundary and interior area, and then selecting a plurality of boundary points along the boundary. In a yet further preferred embodiment, the boundary points include corners. In an alternate embodiment, the boundary points include a point midway between two vertices of the boundary.</p>
  <p num="p-0020">In another preferred embodiment, deriving a plurality of source points from the first sub-pattern and a plurality of source points from the second sub-pattern includes representing each sub-pattern as a region having a boundary and an interior area, and then selecting at least one interior point. In another preferred embodiment, deriving a plurality of source points from the first sub-pattern and a plurality of source points from the second sub-pattern includes representing each sub-pattern as a region having a boundary and an interior area, and then selecting a plurality of boundary points along the boundary. In a further embodiment, the boundary points include corners. In another embodiment, the boundary points include a point midway between two vertices of the boundary.</p>
  <p num="p-0021">In another preferred embodiment, dividing the deformed pattern into at least a first sub-pattern and a second sub-pattern includes dividing the deformed pattern in the image into a plurality of contiguous rectilinear regions so as to form a grid that extends over at least most of the deformed pattern, thereby providing at least a first sub-pattern and a second sub-pattern; and then deriving a plurality of source points from at least the first sub-pattern and the second sub-pattern.</p>
  <p num="p-0022">In yet another preferred embodiment, deriving a plurality of source points from the first sub-pattern and a plurality of source points from the second sub-pattern includes representing each sub-pattern as a plurality of feature points, and then sub-sampling the plurality of feature points so as to reduce the number of feature points in each sub-pattern.</p>
  <p num="p-0023">The method of the invention effectively restricts sub-pattern search ranges, which both improves search speed and reduces the number of spurious matches. The method of the invention also quickly decides which sub-patterns, out of several potential candidates, is most likely to correctly match a deformed sub-pattern.</p>
  <p num="p-0024">The deformation characterization method of the invention uses results from feature-based search tools (typically the “pose” of each sub-pattern of a deformed whole pattern, the pose including position, angle, x-dimension scale, and y-dimension scale information, for example) to create a mapping that models the deformation of the pattern in the image. A transform, selectable by a user, is fit to the results from the feature-based search tools to create a global deformation mapping. This transformation is fit only to feature points derived from matches resulting from successful sub-pattern search, without including data from areas of the pattern that were blank, not matched, or otherwise didn't contain information about the pattern's distorted location.</p>
<description-of-drawings> <heading>BRIEF DESCRIPTION OF THE DRAWING</heading> <p num="p-0025">The invention will be more fully understood by reference to the detailed description, in conjunction with the following figures, wherein:</p>
    <p num="p-0026"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a flow chart of an embodiment of the method of the invention;</p>
    <p num="p-0027"> <figref idrefs="DRAWINGS">FIG. 1A</figref> is a flow chart of an embodiment of a step <b>170</b> of <figref idrefs="DRAWINGS">FIG. 1</figref>;</p>
    <p num="p-0028"> <figref idrefs="DRAWINGS">FIG. 2A</figref> is an example of a target image having a pattern to be analyzed by the method of the invention, the pattern having three sub-patterns;</p>
    <p num="p-0029"> <figref idrefs="DRAWINGS">FIG. 2B</figref> is a collection of feature points derived from the pattern of <figref idrefs="DRAWINGS">FIG. 2A</figref>;</p>
    <p num="p-0030"> <figref idrefs="DRAWINGS">FIG. 3A</figref> shows a pair of sets of feature points that illustrates the distance between sub-patterns as the minimum distance between any two feature points of each sub-pattern;</p>
    <p num="p-0031"> <figref idrefs="DRAWINGS">FIG. 3B</figref> shows a pair of sets of feature points each within a bounding box that illustrates the distance between sub-patterns as the distance between the centers of the bounding boxes of sub-patterns;</p>
    <p num="p-0032"> <figref idrefs="DRAWINGS">FIG. 3C</figref> shows a pair of sets of feature points each having a computed “center-of-mass” that illustrates the distance between sub-patterns as the distance between the centers-of-mass;</p>
    <p num="p-0033"> <figref idrefs="DRAWINGS">FIG. 4A</figref> is an example of a target image of a pattern having three sub-patterns;</p>
    <p num="p-0034"> <figref idrefs="DRAWINGS">FIG. 4B</figref> is a set of features of a sub-pattern to be located within the target image of <figref idrefs="DRAWINGS">FIG. 4A</figref>;</p>
    <p num="p-0035"> <figref idrefs="DRAWINGS">FIG. 4C</figref> is an illustration of two matches of the set of features of the sub-pattern to be located within the target image of <figref idrefs="DRAWINGS">FIG. 4A</figref>, with the match data for each of the two matches;</p>
    <p num="p-0036"> <figref idrefs="DRAWINGS">FIGS. 5A–5E</figref> show a target image of a pattern having three sub-patterns, one sub-pattern serving as a “local anchor”;</p>
    <p num="p-0037"> <figref idrefs="DRAWINGS">FIGS. 6A–6D</figref> show a target image of a pattern having three sub-patterns, illustrating multiple matches for one of the three sub-patterns and their associated scores;</p>
    <p num="p-0038"> <figref idrefs="DRAWINGS">FIG. 7A</figref> shows three sets of source feature points corresponding to a pattern having three sub-patterns;</p>
    <p num="p-0039"> <figref idrefs="DRAWINGS">FIG. 7B</figref> shows three sets of destination feature points corresponding to a pattern having three sub-patterns;</p>
    <p num="p-0040"> <figref idrefs="DRAWINGS">FIG. 7C</figref> shows three source regions corresponding to a pattern having three sub-patterns;</p>
    <p num="p-0041"> <figref idrefs="DRAWINGS">FIG. 7D</figref> shows three destination regions corresponding to a pattern having three sub-patterns;</p>
    <p num="p-0042"> <figref idrefs="DRAWINGS">FIG. 8A</figref> shows three sets of feature points corresponding to a pattern having three sub-patterns, the feature points being corners of bounding shapes of the three sub-patterns;</p>
    <p num="p-0043"> <figref idrefs="DRAWINGS">FIG. 8B</figref> shows three sets of feature points corresponding to a pattern having three sub-patterns, the feature points being corners of bounding shapes, centers of bounding shapes, and mid-points of the vertices of the bounding shapes;</p>
    <p num="p-0044"> <figref idrefs="DRAWINGS">FIG. 8C</figref> shows three sets of feature points corresponding to a pattern having three sub-patterns, the feature points being a sampling of points along a bounding contour, and along an interior contour;</p>
    <p num="p-0045"> <figref idrefs="DRAWINGS">FIG. 9</figref> is a flow chart of an embodiment of a method for characterizing deformation of a pattern;</p>
    <p num="p-0046"> <figref idrefs="DRAWINGS">FIG. 10A</figref> shows three sets of source feature points corresponding to an undeformed pattern having three sub-patterns;</p>
    <p num="p-0047"> <figref idrefs="DRAWINGS">FIG. 10B</figref> shows three sets of destination feature points corresponding to a deformed version of the pattern of <figref idrefs="DRAWINGS">FIG. 10A</figref>; and</p>
    <p num="p-0048"> <figref idrefs="DRAWINGS">FIG. 10C</figref> is a deformation map of the deformation of the deformed pattern of <figref idrefs="DRAWINGS">FIG. 10B</figref>.</p>
  </description-of-drawings> <heading>DETAILED DESCRIPTION</heading> <p num="p-0049">Referring to <figref idrefs="DRAWINGS">FIGS. 1 and 2</figref>, this method takes as input a target image having a pattern to be analyzed <b>100</b> (the letters A, B, and C, taken together), which is partitioned into a set of two or more sub-patterns <b>110</b> (the letters A, B, C; individually). These sub-patterns <b>110</b> are selected to be “spatially coherent” (a coherent sub-pattern tends to contain areas of the image that are connected to each other, and tends not to contain areas that aren't connected. For example, the region of the image where the “A”, lies in <figref idrefs="DRAWINGS">FIG. 2A</figref> is connected by virtue of being part of the “A”, so that would tend to be one coherent sub-pattern, but elements of the “B” would not be included because there is a gap such as in <b>205</b> between those elements), such that each sub-pattern <b>110</b> represents a particular region of the main pattern, and together the sub-pattern regions cover the majority of the area of the larger pattern <b>100</b> where pattern information, such as boundaries between light and dark regions, is found. In one embodiment, to obtain a plurality of spatially coherent sub-patterns, the main pattern is divided into rectangles by a grid, where each sub-pattern covers the area spanned by a rectangle.</p>
  <p num="p-0050">In a preferred embodiment, to obtain more spatially coherent sub-patterns, the main pattern is converted into feature points <b>210</b> by any method known in the art, such as by using a Sobel edge detector, where each feature point represents a point of pattern information, such as a point along a brightness boundary (i.e., an edge) within the image. These feature points are then clustered into sub-groups <b>220</b> using any known partitioning algorithm, such as simply dividing the points into groups by applying a regular grid to partition the area, or other clustering algorithm, such as the well-known “nearest-neighbor” or “k-means” clustering methods. Each clustered group then represents a sub-pattern. In another preferred embodiment, the feature points are clustered into sub-groups using the methods taught in co-pending U.S. patent application entitled “METHOD FOR PARTITIONING A PATTERN INTO OPTIMIZED SUB-PATTERNS”, filed Jul. 22, 2003. In another preferred embodiment, these feature points <b>115</b> are provided directly as inputs, in lieu of the image <b>100</b>.</p>
  <p num="p-0051">These feature points <b>115</b> need not be restricted to two-dimensional (2D) points. One skilled in the art can readily see that a pattern can be represented in any number of dimensions, for example 3D, with no change to this method. However, 2D points and 2D images are used as example images herein for ease of representation.</p>
  <p num="p-0052">Next, in step <b>120</b>, distances are computed between each pair of sub-patterns <b>110</b>. These distances are stored in a look-up table for later use, in a data storage entity hereinafter referred to as the “distance table”. In one embodiment, shown in <figref idrefs="DRAWINGS">FIG. 2A</figref>, the distance <b>205</b> between a pair of sub-patterns <b>110</b> is the minimum distance between any points within the regions represented by the two sub-patterns <b>110</b>.</p>
  <p num="p-0053">In an alternate embodiment shown in <figref idrefs="DRAWINGS">FIG. 3A</figref>, the minimum distance <b>300</b> computed is between any two features within the sub-patterns, if the sub-patterns are represented by features. In another embodiment shown in <figref idrefs="DRAWINGS">FIG. 3B</figref>, this distance is the distance <b>310</b> between the two centers <b>312</b> of the bounding boxes <b>314</b> of the two sub-patterns shown in <figref idrefs="DRAWINGS">FIG. 3B</figref>. In a preferred embodiment shown in <figref idrefs="DRAWINGS">FIG. 3C</figref>, the distance <b>320</b> is computed between the so-called “centers of mass” <b>316</b> of the two sub-patterns, where the center of mass is the average of the positions of all features in the sub-pattern.</p>
  <p num="p-0054">In step <b>130</b> a sub-pattern is selected for initial search. Referring to <figref idrefs="DRAWINGS">FIG. 4B</figref>, this sub-pattern <b>410</b> must be located in a target image <b>400</b> of <figref idrefs="DRAWINGS">FIG. 4A</figref> without the benefit of any additional information as to where exactly it might be, because it is the first sub-pattern searched. Therefore, this initial sub-pattern <b>410</b>, or “anchor” pattern, should be chosen so as to maximize “findability” in an image, relative to the other potential sub-patterns. To maximize findability, the method of the invention scores all sub-patterns according to a criterion or set of criteria, and chooses the highest-scoring sub-pattern as the anchor pattern. In one embodiment, the score is the area of the region covered by the sub-pattern. In another embodiment, the score is the number of feature points in the sub-pattern. In another embodiment, the score is based on proximity to other sub-patterns computed with the same distance metric discussed above in reference to <b>300</b>, <b>310</b>, and <b>320</b>, where a sub-pattern that is near many other sub-patterns scores relatively high. In a preferred embodiment, the utility of each sub-pattern as a search pattern (“findability”) is judged by using that sub-pattern to do a search on the initial pattern, if available. A sub-pattern scores higher if it provides only a single good match, and does not result in extra matches in other locations of the image, or at other orientations, scales, etc. With this embodiment, a circle, for example, would not be considered a good pattern, because it would yield multiple matches at different angles, since a circle looks the same when rotated in place. In another preferred embodiment, several or all of these scoring methods are combined to give a master score, either by simply adding scores together, or by weighting them and then adding them (where the weights would be chosen according to whatever works best empirically for the particular search method used), or by multiplying them together, or by any other reasonable method for combining multiple scores.</p>
  <p num="p-0055">Per step <b>150</b> of <figref idrefs="DRAWINGS">FIG. 1</figref>, the anchor pattern <b>410</b> is sought in the target image <b>400</b> (also <b>140</b> in <figref idrefs="DRAWINGS">FIG. 1</figref>) using any known feature-based search method that finds a pattern despite changes in appearance, such as changes in position, x-dimension scale, y-dimension scale, aspect ratio, angle, or other degrees of freedom (D.O.F), even including generalized deformation. Such pattern search methods include feature-based search methods, including some “geometric” search methods. See, for example, “A Feature-Based Image Registration Algorithm Using Improved Chain-Code Representation Combined with Invariant Moments”, IEEE Trans. on Geoscience and Remote Sensing, Vol. 37, No. 5, September 1999, and “Multiresolution Feature-Based Image Registration”, Visual Comm. and Image Processing 2000, Proceedings of SPIE vol. 4067 (2000), pp. 1490–1498, Perth, Australia, 20–23 Jun. 2000. The anchor pattern <b>410</b> is searched for over a range of angles, scales, etc., the range being based on a predetermined or user-provided deformation rate estimate, which estimate indicates how much local deformation may occur to the pattern. A larger deformation rate estimate means that the anchor pattern may be dissimilar from versions that may be present in the target image, and therefore a larger range of angles, scales. etc. should be used to search for the anchor. For example, the user may indicate that the search should be done over an angle range of −10 to +20 degrees, in the case of a large deformation rate estimate. The actual search range would then be expanded by, for example, 70%, resulting in a search range of −17 to +34 degrees. For another example, a smaller deformation rate estimate might instead expand the range by only 20%, resulting in a search range of −12 to +24 degrees. Similar increases in range occur for each available degree of freedom, such as x-dimension scale, y-dimension scale, aspect ratio, etc.</p>
  <p num="p-0056">If the chosen anchor pattern <b>410</b> is not found within the target image <b>400</b> at step <b>150</b>, or if it is found but the remaining search steps <b>160</b>,<b>170</b>, <b>172</b>–<b>178</b> are unsuccessful in matching the rest of the pattern, a different anchor pattern is selected using the same methods described above, except the next-highest scoring sub-pattern is chosen instead of the failed sub-pattern. If no anchor pattern is ever found within the target image <b>400</b>, even after a plurality of attempts <b>165</b>, the method is deemed to have failed to locate the main pattern in the image, and consequently it cannot characterize the deformation of the pattern <b>180</b>. In this case, either the deformation was too extreme to allow the pattern <b>410</b> to be found with the method of the invention, or the user-supplied deformation-rate estimate was too low, or the pattern <b>410</b> was not in fact present in the image <b>400</b> at all.</p>
  <p num="p-0057">Referring to <figref idrefs="DRAWINGS">FIG. 4C</figref>, if more than one occurrence <b>420</b> <b>430</b> of the anchor pattern <b>410</b> is found in the target image <b>400</b>, the highest scoring match <b>440</b> (e.g., Match <b>1</b>) is used first for the rest of the steps <b>160</b>, <b>170</b>, <b>172</b>–<b>178</b>,<b>180</b>, according to the value of the scoring function which depends on whatever search method was used. If subsequent patterns cannot be successfully matched using the highest scoring match by the rest of the steps set forth below <b>160</b>, <b>170</b>,<b>172</b>–<b>178</b>, <b>180</b>, the steps <b>160</b>, <b>170</b>, <b>172</b>–<b>178</b>, <b>180</b> are repeated for each anchor match, in order of decreasing score.</p>
  <p num="p-0058">Referring to <figref idrefs="DRAWINGS">FIG. 1</figref>, at step <b>160</b>, to select a next sub-pattern to search, the remaining sub-patterns not including the anchor pattern <b>410</b> are then scored. In one embodiment, the score is the area of the region covered by a sub-pattern. In another embodiment, the score is the number of feature points in a sub-pattern. A preferred embodiment uses the distance (as previously computed above) between the anchor pattern and the associated sub-pattern, with smaller distances scoring higher—that is, the sub-pattern nearest to the anchor pattern will be chosen. A refinement to this embodiment gives higher scores to sub-patterns that are near more than one previously found pattern, if such patterns exist. The reason for this is that having more than one previously located, or “anchored”, pattern nearby increases the confidence in the expected location of the new sub-pattern, as there are a larger number of successfully matched features in the immediately surrounding area, reducing the potential for unexpected deformation. A key assumption here is that as the search area moves away from regions with found matches, the potential for deformation increases, as even small, consistent deformations (such as a line of text printed on a curved path) add up to large deformations across a large gap. In another preferred embodiment, several or all of these scoring methods are combined to give a master score, either by simply adding scores together, or by weighting them and then adding them (where the weights would be chosen according to whatever works best empirically for the particular search method used), or by multiplying them together, or by any other reasonable method for combining multiple scores.</p>
  <p num="p-0059">At step <b>170</b>, the highest-scoring sub-pattern is searched for in the target image <b>400</b>, again using any search method known in the art that finds patterns in the presence of some sort of distortion, including but not limited to distortion of position, angle, x-dimension scale, y-dimension scale, etc.</p>
  <p num="p-0060">With reference to <figref idrefs="DRAWINGS">FIGS. 1A and 5A</figref>, the search range of this sub-pattern search <b>170</b> is restricted, which allows the search <b>170</b> to run in less time than a full search of the image <b>400</b>, and which also reduces the possibility of spurious matches by tending to exclude them from the search range. Inputs <b>172</b> to the sub-pattern search <b>170</b> include the target image <b>400</b>, sub-pattern to be located (for example, <b>410</b>), locations of all previously found sub-patterns (for example, see <figref idrefs="DRAWINGS">FIG. 4C</figref>), and the distance table. The distance table is the list of distances between all pairs of sub-patterns that was computed in <b>120</b>. The initial search ranges are based on the expected position <b>500</b> of the sub-pattern in the image, with a small amount of leeway added to the range to deal with incidental deformation <b>174</b>. Since the pattern is assumed to be deformed in the target image, we must assume that sub-patterns will have small deformations as well. By increasing the ranges by a small amount, we increase the likelihood that the entire sub-pattern will be found within the search area, despite this extra deformation. Examples of this leeway include setting the size of the search window to be the size of the sub-pattern's nominal bounding box <b>500</b>, plus three extra pixels on each side to account for the possibility of ink bleeding in a printed image, or similarly searching at plus or minus three degrees of angle from the nominal angle, though of course other values can be used that may further optimize a particular application of the method of the invention.</p>
  <p num="p-0061">In step <b>176</b>, the initial search ranges for those various degrees of freedom (e.g., search area, angle, scale) are then modified based on a number of factors. In one preferred embodiment, the initial search ranges are expanded further based on the user-provided “deformation rate estimate”, where higher rates increase the ranges more. For example, an estimate of high deformation, arbitrarily chosen as 0.8 for this example, could result in an expansion of a nominal angular search range from 40 to 60 degrees to 20 to 80 degrees, while an estimate of low deformation, such as 0.2, could result in an expansion of the angular search range to only 35 to 65 degrees. In a further preferred embodiment, the deformation rate estimate is multiplied by the distance between the current sub-pattern and the nearest previously located sub-pattern (called the “local anchor”) <b>510</b> <b>560</b>. This results in larger search area ranges <b>570</b> for sub-patterns <b>580</b> that are further away from any known, “locally-anchored” regions <b>560</b> in the target image <b>400</b>, and smaller search area ranges <b>520</b> for sub-patterns <b>530</b> that are closer to a known local anchor <b>510</b>. In the embodiment, described above, where sub-patterns are chosen for searching based on the distances to more than one previously located sub-pattern, those distances can be combined here. If the nearby sub-patterns do not agree on the likely pose of the new sub-pattern, that is evidence that deformation to the pattern has occurred (as otherwise all sub-patterns would be found right where they're predicted to be anyway), so therefore the deformation estimate should be increased, to encompass the expected ranges from all nearby sub-patterns. In the embodiment of step <b>175</b>, the position, as well as the angle, x-dimension scale, y-dimension scale, and other degrees of freedom of the local anchor <b>560</b> are used to determine the nominal, or expected, pose of the sub-pattern <b>550</b>—the sub-pattern search is based on the assumption that the most likely place for the new sub-pattern is where it would be found if it had exactly the same sort of distortion (angle, scale, etc.) as its local anchor <b>560</b>.</p>
  <p num="p-0062">Another embodiment varies the D.O.F. search ranges based on the score obtained by the search for the local anchor, as the score is considered to be a measure of certainty, and when the local anchor's pose is uncertain, the search ranges for nearby sub-patterns can be advantageously enlarged <b>570</b>, as shown in <figref idrefs="DRAWINGS">FIGS. 5D and 5E</figref>. A further preferred embodiment uses all of these factors to modify the D.O.F. search ranges. Another preferred embodiment further uses more than one local anchor, if more than one exists, and combines the search ranges determined using each local anchor to determine an overall set of D.O.F. search ranges. This last embodiment is particularly useful when the deformation is not smooth, and several nearby sub-patterns might have different types of distortion. In that case, the new sub-pattern might be distorted similarly to any of the known sub-patterns, so including all of those potential distortions in the search range is important, though there is of course a speed cost in the search over the widened ranges. An important benefit of the method of the invention is to maximize the likelihood that the D.O.F. search range includes the instance of the sub-pattern in the target image <b>530</b> <b>580</b>, even though that instance may not be where it's expected to be, while also minimizing the computational overhead of the search by limiting D.O.F. search range to some D.O.F. range that is less than the maximum possible range in each D.O.F., such as less than the whole image area in the spatial D.O.F., less than 360 degrees in the angular D.O.F., etc.</p>
  <p num="p-0063">Referring to <figref idrefs="DRAWINGS">FIG. 1A</figref> in step <b>177</b>, a search is performed for any matches of the sub-pattern <b>600</b> using the limited D.O.F. search ranges. If there is more than one match of this new sub-pattern <b>600</b>, shown in <figref idrefs="DRAWINGS">FIGS. 6B–6D</figref>, score the matches and choose the highest scoring one. In one embodiment of this scoring function, the score is simply the score <b>610</b> indicating a measure of match quality returned by the underlying search algorithm. In another embodiment, the score is determined based on the match's similarity to the expected match location given the local anchor. That is, if the match is at the position <b>620</b>, angle <b>630</b>, scale <b>640</b>, etc. that would be expected if the only distortion was that given by the local anchor's pose, it scores higher. The further away a degree-of-freedom gets from any of these expected (or nominal) values, for example by being translated, rotated, or scaled differently than the anchor, the lower the score gets.</p>
  <p num="p-0064">An actual score can most easily be computed by looking at how far away each D.O.F. value is from the expected value, relative to the possible range, and expressing that as a ratio. For example, if the expected angle was 20 degrees, and the search range was 10 to 30 degrees, then if the match angle was at 16 degrees, then the score would be 1.0−(20−16)/(20−10)=1.0−0.4=0.6. If the match angle was at 12 degrees, the score would be 1.0−(20−12)/(20−10)=1.0−0.8=0.2. Subtracting the ratio from 1.0 causes high scores to indicate better matches than low scores, which is what is preferred. A preferred embodiment combines both of these scoring methods into a single score <b>650</b>, either by multiplying them together, or by adding them, or by any reasonable scheme for combining multiple score values into a single score.</p>
  <p num="p-0065">Referring to <figref idrefs="DRAWINGS">FIGS. 1 and 1A</figref>, whether or not any matches were actually found at step <b>178</b> of step <b>170</b>, the method then repeats <b>165</b> the sub-pattern search described above, not searching for previously found sub-patterns, until searches have been performed for all sub-patterns in the original set of sub-patterns <b>110160</b>. On each subsequent iteration <b>165</b>, the step that selects the next sub-pattern for search <b>160</b>, as described above, does not select the best-scoring sub-pattern considering only a single anchor pattern, but rather it selects it considering all of the previously found sub-patterns. That is, the candidate sub-patterns are scored against all “anchored” sub-patterns, and the best score determines which will be the next sub-pattern selected <b>160</b>.</p>
  <p num="p-0066">After the various parts of the target pattern <b>400</b> have been individually located in the image, an aggregate score for the match is computed based on the sub-pattern quality scores (already computed as described above <b>650</b>), where non-located sub-patterns score as zero. In one embodiment, the scores of the sub-patterns are simply averaged. In a preferred embodiment, these sub-scores are combined as a weighted average, being weighted according to the area enclosed by the features of each sub-pattern, or by the size of the bounding-box. In another preferred embodiment, they are weighted by the number of features in each sub-pattern.</p>
  <p num="p-0067">This aggregate score is returned, or a failure message is returned if the pattern could not be matched. The individual poses of the sub-pattern matches can also be returned, and also the individual sub-scores corresponding to the individual poses.</p>
  <p num="p-0068">In a preferred embodiment, the individual poses of the sub-pattern, and the exact found positions of individual features in each sub-pattern are used to define a deformation map <b>180</b>, and that deformation map can be returned as output, either instead of or in addition to other match information. In a refinement to that embodiment, the feature information used to help define the deformation map is a sub-set of the feature points provided by prior steps of the method for finding a deformed pattern. For example, the set of feature points can be sub-sampled, resulting in a less accurate result (as fewer features are considered), but then the deformation map can often be computed much faster from fewer features In another refinement, these points are a representative sampling of the region that each sub-pattern occupies, such as the corners or edges of the bounding box of the region, or a set of points along whatever contour may define the region. In a preferred refinement to this embodiment, individual feature points that were not matched as part of the core search for a deformed pattern are omitted from the list of feature points used to create the deformation map <b>180</b>. The location or pose of the pattern in the image is returned <b>182</b> by the method of the invention, though the exact concept of “location” or “pose” for a deformed pattern permits a variety of definitions, the particular definition being chosen in accordance with the requirements of a particular application, or the convenience of a user or developer. In one embodiment, the location of the pattern is deemed to be the location of a particular designated “location point” in the pattern, the location point being designated by a user. In this embodiment, the returned location is the “location point” mapped through the above-described deformation map, which is a close approximation to the location of that point in the deformed pattern. In another embodiment, the pose of the pattern is deemed to be the location of a user-specified sub-pattern. In this case, the location includes the full pose of the sub-pattern (which was determined by the method of the invention), not just the position information. In another embodiment, the pose is computed by fitting an affine transform to all found feature points using any standard method for fitting such a transform, such as a least-squares fit, or other fitting methods disclosed herein. This yields only a rough approximation of the location of the pattern, as all deformation has been stripped away. However, for patterns that have only minor deformation, this may be the simplest solution. In a preferred embodiment, the deformation map itself is returned, as this map contains the most detailed information available on the pattern's position in the image. In this embodiment, the user can define “location” any way they choose by using the information provided by the deformation map. For example, they could map the four corners of the pattern rectangle through the deformation map to yield a “location quadrilateral” for the deformed pattern.</p>
  <p num="h-0006">Method for Characterizing Deformation of a Pattern</p>
  <p num="p-0069">Referring to <figref idrefs="DRAWINGS">FIGS. 7</figref>, <b>9</b>, and <b>10</b>, to define a deformation map <b>1040</b> as shown in <figref idrefs="DRAWINGS">FIG. 10C</figref>, a set of feature points representing an un-deformed pattern <b>700</b>, <b>1000</b>, and a corresponding set of feature points representing the deformed pattern <b>710</b>,<b>1020</b> are used <b>920</b>. Each feature point <b>720</b> represents a point of information in the pattern or image <b>700</b>, <b>710</b>, such as a point along a brightness boundary within the image <b>700</b>, <b>710</b>. These corresponding sets of feature points <b>700</b>, <b>710</b> can be determined by any method known in the art, including but not limited to the search method of the invention described herein above.</p>
  <p num="p-0070">The above-described search method of the invention employs sub-patterns that each include a plurality of feature points which can be input directly as “undeformed pattern” points <b>700</b>. The above-described search method of the invention also provides a found pose for each matched sub-pattern <b>910</b>. Mapping each matched sub-pattern's set of feature points through the found pose gives a set of “deformed pattern” feature points <b>915</b> to be used as input to the method disclosed herein for characterizing the deformation of a pattern.</p>
  <p num="p-0071">Alternatively, a set of source regions <b>730</b> can be used as initial inputs <b>900</b>. The set of source regions <b>730</b> together represent an un-deformed pattern <b>740</b>, and a corresponding set of regions <b>750</b> represent a deformed pattern <b>760</b>. An example of a set of source regions is a grid of rectangles that cover the full area of an un-deformed pattern, and a corresponding set of destination regions is a collection of contiguous affine rectangles (e.g., rectangles that have undergone rotation, skew, scaling, and/or translation) that represent the approximate deformations of the rectangles of the grid.</p>
  <p num="p-0072">Referring again to <figref idrefs="DRAWINGS">FIGS. 8 and 9</figref>, to use the regions <b>900</b>, a list of feature points <b>920</b> is created from the source regions and destination regions by any reasonable method <b>905</b>. For example, in one embodiment illustrated in <figref idrefs="DRAWINGS">FIG. 8A</figref>, where the regions are rectangles or other regular shapes <b>800</b> in the image <b>805</b>, the feature points are the corners (vertices) <b>802</b> of the shapes <b>800</b>. In another embodiment shown in <figref idrefs="DRAWINGS">FIG. 8B</figref>, where the regions are again rectangles or other regular shapes, the feature points are a representative sampling of points within and along the shapes, such as the corners (vertices) <b>810</b>, the centers (midpoints) of the sides <b>820</b>, and the center of the shape <b>830</b> (or any defined interior point). In another embodiment, set forth in <figref idrefs="DRAWINGS">FIG. 8C</figref>, where the regions are more general contours <b>840</b>, a sampling of points along the contour <b>850</b>, or within the shape itself <b>860</b>, are used.</p>
  <p num="p-0073">A global deformation map <b>1040</b> as shown in <figref idrefs="DRAWINGS">FIG. 10C</figref> is then created by fitting a transform <b>930</b> to the set of source <b>1000</b> and destination <b>1020</b> points <b>920</b>, using any standard method of fitting known to those skilled in the art, such as a least-squares fit, a least median of squares fit, a minimum L<sub>p </sub>norm estimation, a robust m-estimators fit, or a total variation fit. Also, some transforms, such as the “thin-plate spline” discussed below, can map the points exactly without relying on the approximate solution provided by a “fitting” method. In one embodiment, this deformation map <b>1040</b> is an affine map. If the deformation of the pattern is known to be solely or primarily perspective distortion, then a preferred embodiment is to fit a perspective transform to the points. If the deformation of the pattern is known to follow the curve of a cylinder, for example if the pattern were printed on a metal can, a generic cylinder or conic transform is used.</p>
  <p num="p-0074">A preferred embodiment for accurately modeling more detailed deformation is the thin-plate-spline, a mathematical construct described in “Principal Warps: Thin-Plate Splines and the Decomposition of Deformations” by Fred L. Bookstein (IEEE Transactions on Pattern Analysis and Machine Intelligence, June 1989). Depending on the domain, any number of possible models can be fit to these points, including other types of splines, or any other transformation.</p>
  <p num="p-0075">This global deformation map (transform) <b>940</b>, <b>1040</b> is an approximation of the deformation of the pattern, with accuracy limited by whatever level of detail the provided feature points or regions possess, and limited by the appropriateness of the chosen transformation (e.g., perspective, cylinder, or spline) for modeling that deformation.</p>
  <p num="p-0076">The global deformation map of the invention can be used to un-do or otherwise reverse the deformation of the deformed pattern so as to provide an un-deformed pattern.</p>
  <p num="p-0077">Other variants and embodiments will occur to those skilled in the art, without departing from the spirit and scope of the invention. Accordingly, the invention is not intended to be limited by the detailed description, except as set forth in the following claims.</p>
</div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3936800">US3936800</a></td><td class="patent-data-table-td patent-date-value">Mar 27, 1974</td><td class="patent-data-table-td patent-date-value">Feb 3, 1976</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Pattern recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4115702">US4115702</a></td><td class="patent-data-table-td patent-date-value">Apr 25, 1977</td><td class="patent-data-table-td patent-date-value">Sep 19, 1978</td><td class="patent-data-table-td ">Zumback Electronic Ag</td><td class="patent-data-table-td ">Device for measuring at least one dimension of an object and a method of operating said device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4115762">US4115762</a></td><td class="patent-data-table-td patent-date-value">Nov 30, 1977</td><td class="patent-data-table-td patent-date-value">Sep 19, 1978</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Alignment pattern detecting apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4183013">US4183013</a></td><td class="patent-data-table-td patent-date-value">Nov 29, 1976</td><td class="patent-data-table-td patent-date-value">Jan 8, 1980</td><td class="patent-data-table-td ">Coulter Electronics, Inc.</td><td class="patent-data-table-td ">System for extracting shape features from an image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4200861">US4200861</a></td><td class="patent-data-table-td patent-date-value">Sep 1, 1978</td><td class="patent-data-table-td patent-date-value">Apr 29, 1980</td><td class="patent-data-table-td ">View Engineering, Inc.</td><td class="patent-data-table-td ">Pattern recognition apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4441248">US4441248</a></td><td class="patent-data-table-td patent-date-value">Dec 2, 1982</td><td class="patent-data-table-td patent-date-value">Apr 10, 1984</td><td class="patent-data-table-td ">Stanley Electric Company, Ltd.</td><td class="patent-data-table-td ">On-line inspection method and system for bonds made to electronic components</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4570180">US4570180</a></td><td class="patent-data-table-td patent-date-value">May 26, 1983</td><td class="patent-data-table-td patent-date-value">Feb 11, 1986</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method for automatic optical inspection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4685143">US4685143</a></td><td class="patent-data-table-td patent-date-value">Mar 21, 1985</td><td class="patent-data-table-td patent-date-value">Aug 4, 1987</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Method and apparatus for detecting edge spectral features</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4688088">US4688088</a></td><td class="patent-data-table-td patent-date-value">Apr 15, 1985</td><td class="patent-data-table-td patent-date-value">Aug 18, 1987</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">For detecting a mark having edges</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4736437">US4736437</a></td><td class="patent-data-table-td patent-date-value">Apr 21, 1987</td><td class="patent-data-table-td patent-date-value">Apr 5, 1988</td><td class="patent-data-table-td ">View Engineering, Inc.</td><td class="patent-data-table-td ">High speed pattern recognizer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4763280">US4763280</a></td><td class="patent-data-table-td patent-date-value">Apr 29, 1985</td><td class="patent-data-table-td patent-date-value">Aug 9, 1988</td><td class="patent-data-table-td ">Evans &amp; Sutherland Computer Corp.</td><td class="patent-data-table-td ">Curvilinear dynamic image generation system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4783826">US4783826</a></td><td class="patent-data-table-td patent-date-value">Aug 18, 1986</td><td class="patent-data-table-td patent-date-value">Nov 8, 1988</td><td class="patent-data-table-td ">The Gerber Scientific Company, Inc.</td><td class="patent-data-table-td ">Pattern inspection system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4860374">US4860374</a></td><td class="patent-data-table-td patent-date-value">Jul 6, 1988</td><td class="patent-data-table-td patent-date-value">Aug 22, 1989</td><td class="patent-data-table-td ">Nikon Corporation</td><td class="patent-data-table-td ">Apparatus for detecting position of reference pattern</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4876457">US4876457</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 1988</td><td class="patent-data-table-td patent-date-value">Oct 24, 1989</td><td class="patent-data-table-td ">American Telephone And Telegraph Company</td><td class="patent-data-table-td ">Method and apparatus for differentiating a planar textured surface from a surrounding background</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4876728">US4876728</a></td><td class="patent-data-table-td patent-date-value">Nov 20, 1987</td><td class="patent-data-table-td patent-date-value">Oct 24, 1989</td><td class="patent-data-table-td ">Adept Technology, Inc.</td><td class="patent-data-table-td ">Vision system for distinguishing touching parts</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4922543">US4922543</a></td><td class="patent-data-table-td patent-date-value">Dec 13, 1985</td><td class="patent-data-table-td patent-date-value">May 1, 1990</td><td class="patent-data-table-td ">Sten Hugo Nils Ahlbom</td><td class="patent-data-table-td ">Image processing device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4955062">US4955062</a></td><td class="patent-data-table-td patent-date-value">Jul 18, 1989</td><td class="patent-data-table-td patent-date-value">Sep 4, 1990</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Pattern detecting method and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4959898">US4959898</a></td><td class="patent-data-table-td patent-date-value">May 22, 1990</td><td class="patent-data-table-td patent-date-value">Oct 2, 1990</td><td class="patent-data-table-td ">Emhart Industries, Inc.</td><td class="patent-data-table-td ">Surface mount machine with lead coplanarity verifier</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4980971">US4980971</a></td><td class="patent-data-table-td patent-date-value">Dec 14, 1989</td><td class="patent-data-table-td patent-date-value">Jan 1, 1991</td><td class="patent-data-table-td ">At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Using cameras</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5060276">US5060276</a></td><td class="patent-data-table-td patent-date-value">May 31, 1989</td><td class="patent-data-table-td patent-date-value">Oct 22, 1991</td><td class="patent-data-table-td ">At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Technique for object orientation detection using a feed-forward neural network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5086478">US5086478</a></td><td class="patent-data-table-td patent-date-value">Dec 27, 1990</td><td class="patent-data-table-td patent-date-value">Feb 4, 1992</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Finding fiducials on printed circuit boards to sub pixel accuracy</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5113565">US5113565</a></td><td class="patent-data-table-td patent-date-value">Jul 6, 1990</td><td class="patent-data-table-td patent-date-value">May 19, 1992</td><td class="patent-data-table-td ">International Business Machines Corp.</td><td class="patent-data-table-td ">Apparatus and method for inspection and alignment of semiconductor chips and conductive lead frames</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5161201">US5161201</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 7, 1991</td><td class="patent-data-table-td patent-date-value">Nov 3, 1992</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Method of and apparatus for measuring pattern profile</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5206917">US5206917</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 10, 1991</td><td class="patent-data-table-td patent-date-value">Apr 27, 1993</td><td class="patent-data-table-td ">Nippon Sheet Glass Co., Ltd.</td><td class="patent-data-table-td ">Method for collating independent figure elements between images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5226095">US5226095</a></td><td class="patent-data-table-td patent-date-value">May 26, 1992</td><td class="patent-data-table-td patent-date-value">Jul 6, 1993</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Method of detecting the position of an object pattern in an image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5268999">US5268999</a></td><td class="patent-data-table-td patent-date-value">Mar 20, 1992</td><td class="patent-data-table-td patent-date-value">Dec 7, 1993</td><td class="patent-data-table-td ">Ricoh Company, Ltd.</td><td class="patent-data-table-td ">Modeling method and system using solid data having functional structure and normal projection drawing dimensional format</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5343028">US5343028</a></td><td class="patent-data-table-td patent-date-value">Aug 10, 1992</td><td class="patent-data-table-td patent-date-value">Aug 30, 1994</td><td class="patent-data-table-td ">United Parcel Service Of America, Inc.</td><td class="patent-data-table-td ">Method and apparatus for detecting and decoding bar code symbols using two-dimensional digital pixel images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5371690">US5371690</a></td><td class="patent-data-table-td patent-date-value">Jan 17, 1992</td><td class="patent-data-table-td patent-date-value">Dec 6, 1994</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and apparatus for inspection of surface mounted devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5471541">US5471541</a></td><td class="patent-data-table-td patent-date-value">Nov 16, 1993</td><td class="patent-data-table-td patent-date-value">Nov 28, 1995</td><td class="patent-data-table-td ">National Research Council Of Canada</td><td class="patent-data-table-td ">System for determining the pose of an object which utilizes range profiles and synethic profiles derived from a model</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5495537">US5495537</a></td><td class="patent-data-table-td patent-date-value">Jun 1, 1994</td><td class="patent-data-table-td patent-date-value">Feb 27, 1996</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Methods and apparatus for machine vision template matching of images predominantly having generally diagonal and elongate features</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5497451">US5497451</a></td><td class="patent-data-table-td patent-date-value">Jan 22, 1992</td><td class="patent-data-table-td patent-date-value">Mar 5, 1996</td><td class="patent-data-table-td ">Holmes; David</td><td class="patent-data-table-td ">Computerized method for decomposing a geometric model of surface or volume into finite elements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5500906">US5500906</a></td><td class="patent-data-table-td patent-date-value">Jan 14, 1994</td><td class="patent-data-table-td patent-date-value">Mar 19, 1996</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Locating curvilinear objects using feathered fiducials</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5545887">US5545887</a></td><td class="patent-data-table-td patent-date-value">Oct 25, 1994</td><td class="patent-data-table-td patent-date-value">Aug 13, 1996</td><td class="patent-data-table-td ">United Parcel Service Of America, Inc.</td><td class="patent-data-table-td ">Method and apparatus for decoding bar code symbols using subpixel scan lines</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5602937">US5602937</a></td><td class="patent-data-table-td patent-date-value">Jun 1, 1994</td><td class="patent-data-table-td patent-date-value">Feb 11, 1997</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Methods and apparatus for machine vision high accuracy searching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5621807">US5621807</a></td><td class="patent-data-table-td patent-date-value">Oct 13, 1994</td><td class="patent-data-table-td patent-date-value">Apr 15, 1997</td><td class="patent-data-table-td ">Dornier Gmbh</td><td class="patent-data-table-td ">Intelligent range image camera for object measurement</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5625715">US5625715</a></td><td class="patent-data-table-td patent-date-value">Oct 21, 1993</td><td class="patent-data-table-td patent-date-value">Apr 29, 1997</td><td class="patent-data-table-td ">U.S. Philips Corporation</td><td class="patent-data-table-td ">Method and apparatus for encoding pictures including a moving object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5627912">US5627912</a></td><td class="patent-data-table-td patent-date-value">Dec 6, 1995</td><td class="patent-data-table-td patent-date-value">May 6, 1997</td><td class="patent-data-table-td ">Yozan Inc.</td><td class="patent-data-table-td ">Inspection method of inclination of an IC</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5627915">US5627915</a></td><td class="patent-data-table-td patent-date-value">Jan 31, 1995</td><td class="patent-data-table-td patent-date-value">May 6, 1997</td><td class="patent-data-table-td ">Princeton Video Image, Inc.</td><td class="patent-data-table-td ">Pattern recognition system employing unlike templates to detect objects having distinctive features in a video field</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5663809">US5663809</a></td><td class="patent-data-table-td patent-date-value">May 16, 1995</td><td class="patent-data-table-td patent-date-value">Sep 2, 1997</td><td class="patent-data-table-td ">Sharp Kabushiki Kaisha</td><td class="patent-data-table-td ">Image processing apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5828769">US5828769</a></td><td class="patent-data-table-td patent-date-value">Oct 23, 1996</td><td class="patent-data-table-td patent-date-value">Oct 27, 1998</td><td class="patent-data-table-td ">Autodesk, Inc.</td><td class="patent-data-table-td ">Method and apparatus for recognition of objects via position and orientation consensus of local image encoding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5845288">US5845288</a></td><td class="patent-data-table-td patent-date-value">Dec 11, 1995</td><td class="patent-data-table-td patent-date-value">Dec 1, 1998</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Automated system for indexing graphical documents having associated text labels</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5933516">US5933516</a></td><td class="patent-data-table-td patent-date-value">Oct 28, 1997</td><td class="patent-data-table-td patent-date-value">Aug 3, 1999</td><td class="patent-data-table-td ">Lockheed Martin Corp.</td><td class="patent-data-table-td ">Fingerprint matching by estimation of a maximum clique</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6636634">US6636634</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 23, 2002</td><td class="patent-data-table-td patent-date-value">Oct 21, 2003</td><td class="patent-data-table-td ">Coreco Imaging, Inc.</td><td class="patent-data-table-td ">Systems and methods for locating a pattern in an image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6691145">US6691145</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 13, 2000</td><td class="patent-data-table-td patent-date-value">Feb 10, 2004</td><td class="patent-data-table-td ">Semiconductor Technology Academic Research Center</td><td class="patent-data-table-td ">Computing circuit, computing apparatus, and semiconductor computing circuit</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6785419">US6785419</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 22, 2000</td><td class="patent-data-table-td patent-date-value">Aug 31, 2004</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">System and method to facilitate pattern recognition by deformable matching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6909798">US6909798</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 31, 2000</td><td class="patent-data-table-td patent-date-value">Jun 21, 2005</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Method of erasing repeated patterns and pattern defect inspection device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020054699">US20020054699</a></td><td class="patent-data-table-td patent-date-value">Sep 25, 2001</td><td class="patent-data-table-td patent-date-value">May 9, 2002</td><td class="patent-data-table-td ">Peter Roesch</td><td class="patent-data-table-td ">Device and method of computing a transformation linking two images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE4406020C1?cl=en">DE4406020C1</a></td><td class="patent-data-table-td patent-date-value">Feb 24, 1994</td><td class="patent-data-table-td patent-date-value">Jun 29, 1995</td><td class="patent-data-table-td ">Zentrum Fuer Neuroinformatik G</td><td class="patent-data-table-td ">Automatic digital image recognition system</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Belongie, S. , et al., "<a href='http://scholar.google.com/scholar?q="Shape+Matching+and+Object+Recognition+Using+Shape+Contexts"'>Shape Matching and Object Recognition Using Shape Contexts</a>", IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Inc. New York, vol. 24, No. 4, (Apr. 2003),509-522.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bileschi, S. , et al., "<a href='http://scholar.google.com/scholar?q="Advances+in+Component-based+Face+Detection"'>Advances in Component-based Face Detection</a>", Lecture notes in Computer Science, Springer Verlag, New York, NY, vol. 2388, (2002),135-143.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bookstein, F L., "<a href='http://scholar.google.com/scholar?q="Principal+Warps%3A+Thin-Plate+Splines+and+the+Decomposition+of+Deformations"'>Principal Warps: Thin-Plate Splines and the Decomposition of Deformations</a>", IEEE Transactions on pattern Analysis and Machine Intelligence, IEEE Inc., New York, vol. 11, No. 6, (Jun. 1, 1989).</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, Cognex MVS-8000 Series, CVL Vision Tools Guide, pp. 25-136, Release 5.4 590-6271, Natick, MA USA 2000.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">deFigueiredo et al. Model Based Orientation Independent 3-D Machine Vision Techniques, IEEE Transactions on Aerospace and Electronic Systems, vol. 24, No. 5 Sep. 1988, pp. 597-607.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Gdalyahu Y et al.: Self-Organization in Vision: Stochastic Clustering for Image Segmentation Perceptual Grouping, and Image Database Organization: IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Inc. New York, vol. 23, No. 10, Oct. 2001, pp. 1053-1074.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Hoogs et al., "<a href='http://scholar.google.com/scholar?q="Model-Based+Learning+of+Segmentations"'>Model-Based Learning of Segmentations</a>", IEEE, pp. 494-499, 1996.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">J, Michael F., et al., "<a href='http://scholar.google.com/scholar?q="Handbook+of+Medical+Imaging"'>Handbook of Medical Imaging</a>", vol. 2: Medical image Processing and Analysis, SPIE Press, Bellingham, WA,(2000),Chapter 8.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Jianbo Shi et al: Normalized Cuts and Image Segmentation, Computer Vision and Pattern Recognition, 1997. Proceedings, 1997 IEEE Computer Society Conference, San Juan, Puerto Rico Jun. 17-19, 1997, pp. 731-737.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Medina-Mora, R., "<a href='http://scholar.google.com/scholar?q="An+Incremental+Programming+Environment%2C"'>An Incremental Programming Environment,</a>" IEEE Transactions on Software Engineering, Sep. 1981, pp. 472-482, vol. SE-7, No. 5, 1992.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mehrotra R et al: Feature-based retrieval of similar shapes, Proceeding of the International Conference on Data Engineering. Vienna, Apr. 19-23, 1993, pp. 108-115.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Newman et al., "<a href='http://scholar.google.com/scholar?q="3D+CAD-Based+Inspection+I%3A+Coarse+Verification"'>3D CAD-Based Inspection I: Coarse Verification</a>", IEEE, pp. 49-52, 1992.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ohm, Jens-Rainer , "<a href='http://scholar.google.com/scholar?q="Digitale+Bildcodierung"'>Digitale Bildcodierung</a>", Springer Verlag, Berlin 217580, XP002303066, Section 6.2 Bewegungschatzung,(1995).</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Pauwels E J et al: Finding Salient Regions in Images-Nonparametric Clustering for Image Segmentation and Grouping Computer Vision and Image Understanding, Academic Press, San Diego, CA, vol. 75, No. 1-2, Jul. 1999, pp. 73-85.</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Scanlon J. et al.: Graph-theoretic Algorithms for Image Segmentation, Circuits and Systems, 1999, ISCAS '99. Proceedings of the 1999 IEEE International Symposium, Orlando, FL, May 30, 1999, pp. 141-144.</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Stockman, G , et al., "<a href='http://scholar.google.com/scholar?q="Matching+images+to+models+for+registration+and+object+detection+via+clustering"'>Matching images to models for registration and object detection via clustering</a>", IEEE Transaction of Pattern Analysis and Machine Intelligence, IEEE Inc., New York, vol. PAMI-4, No. 3,,(1982).</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ullman, S., "<a href='http://scholar.google.com/scholar?q="Aligning+pictorial+descriptions%3A+An+approach+to+object+recognition%2C+I%3A+Approaches+to+Object+Recognition%2C"'>Aligning pictorial descriptions: An approach to object recognition, I: Approaches to Object Recognition,</a>" reprinted from Cognition, pp. 201-214, vol. 32, No. 3, Cambridge, MA USA, Aug. 1989.</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Wei, Wen , et al., "<a href='http://scholar.google.com/scholar?q="Recognition+and+Insprection+of+Two-Dimensional+Industrial+Parts+Using+Subpolygons"'>Recognition and Insprection of Two-Dimensional Industrial Parts Using Subpolygons</a>", Pattern Recognition, Elsevier, Kidlington, GB, vol. 25, No. 12, (Dec. 1, 1992),1427-1434.</td></tr><tr><td class="patent-data-table-td ">19</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Xie X L et al. A new fuzzy clustering criterion and its application to color image segmentation, Proceedings of the International Symposium on Intelligent Control, Arlington, Aug. 13-15, 1991, pp. 463-468.</td></tr><tr><td class="patent-data-table-td ">20</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Zhang, Zhengyou , "<a href='http://scholar.google.com/scholar?q="Parameter+estimation+techniques%3A+A+tutorial+with+application+to+conic+fitting"'>Parameter estimation techniques: A tutorial with application to conic fitting</a>", Imag and Vision Comput; Image and Vision computing: Elsevier Science Ltd. Oxford England, vol. 15, No. 1,(Jan. 1, 1997).</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7386172">US7386172</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 11, 2005</td><td class="patent-data-table-td patent-date-value">Jun 10, 2008</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Image recognition method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7878402">US7878402</a></td><td class="patent-data-table-td patent-date-value">Dec 20, 2005</td><td class="patent-data-table-td patent-date-value">Feb 1, 2011</td><td class="patent-data-table-td ">Cognex Technology And Investment Corporation</td><td class="patent-data-table-td ">Decoding distorted symbols</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7965899">US7965899</a></td><td class="patent-data-table-td patent-date-value">Jul 3, 2008</td><td class="patent-data-table-td patent-date-value">Jun 21, 2011</td><td class="patent-data-table-td ">Gognex Technology and Investment Corporation</td><td class="patent-data-table-td ">Methods for locating and decoding distorted two-dimensional matrix symbols</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8131055">US8131055</a></td><td class="patent-data-table-td patent-date-value">Jan 31, 2008</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">Caterpillar Inc.</td><td class="patent-data-table-td ">System and method for assembly inspection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8260059">US8260059</a></td><td class="patent-data-table-td patent-date-value">May 5, 2008</td><td class="patent-data-table-td patent-date-value">Sep 4, 2012</td><td class="patent-data-table-td ">Mvtec Software Gmbh</td><td class="patent-data-table-td ">System and method for deformable object recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8270749">US8270749</a></td><td class="patent-data-table-td patent-date-value">Jun 20, 2011</td><td class="patent-data-table-td patent-date-value">Sep 18, 2012</td><td class="patent-data-table-td ">Cognex Technology And Investment Corporation</td><td class="patent-data-table-td ">Method for locating and decoding distorted two-dimensional matrix symbols</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8322620">US8322620</a></td><td class="patent-data-table-td patent-date-value">Jan 6, 2011</td><td class="patent-data-table-td patent-date-value">Dec 4, 2012</td><td class="patent-data-table-td ">Cognex Technology And Investment Corporation</td><td class="patent-data-table-td ">Decoding distorted symbols</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8345979">US8345979</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 1, 2007</td><td class="patent-data-table-td patent-date-value">Jan 1, 2013</td><td class="patent-data-table-td ">Cognex Technology And Investment Corporation</td><td class="patent-data-table-td ">Methods for finding and characterizing a deformed pattern in an image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8437502">US8437502</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 25, 2004</td><td class="patent-data-table-td patent-date-value">May 7, 2013</td><td class="patent-data-table-td ">Cognex Technology And Investment Corporation</td><td class="patent-data-table-td ">General pose refinement and tracking tool</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090041361">US20090041361</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 12, 2008</td><td class="patent-data-table-td patent-date-value">Feb 12, 2009</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Character recognition apparatus, character recognition method, and computer product</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2081133A1?cl=en">EP2081133A1</a></td><td class="patent-data-table-td patent-date-value">Jan 18, 2008</td><td class="patent-data-table-td patent-date-value">Jul 22, 2009</td><td class="patent-data-table-td ">MVTec Software GmbH</td><td class="patent-data-table-td ">System and method for deformable object recognition</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=j2Z6BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S181000">382/181</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=j2Z6BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S191000">382/191</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=j2Z6BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009000000">G06K9/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=j2Z6BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009460000">G06K9/46</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=j2Z6BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009640000">G06K9/64</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=j2Z6BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/6206">G06K9/6206</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=j2Z6BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06T7/0079">G06T7/0079</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06K9/62A1A2</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Dec 20, 2011</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIMS 1-9 ARE CANCELLED. CLAIMS 10-22 WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 24, 2010</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 24, 2010</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 21, 2009</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090608</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 23, 2004</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COGNEX TECHNOLOGY AND INVESTMENT CORPORATION, CALI</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:DAVIS, JASON;REEL/FRAME:014282/0117</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20040123</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U28yuGt53ceejdd-8a6Ue1dpihvRw\u0026id=j2Z6BAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U2hjHaOhSK-5IErww_RZW7cTE8TDw\u0026id=j2Z6BAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U0GFleaJ-8ar_KpnkyTLd5646tXwQ","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Methods_for_finding_and_characterizing_a.pdf?id=j2Z6BAABERAJ\u0026output=pdf\u0026sig=ACfU3U2abfiQzKvm8n2vVSugXnpzPaiexQ"},"sample_url":"http://www.google.com/patents/reader?id=j2Z6BAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>