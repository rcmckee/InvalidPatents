<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7952483 - Human movement measurement system - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_4ff636b3d23669b7103f3b3a3a18b4cd/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_4ff636b3d23669b7103f3b3a3a18b4cd__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Human movement measurement system"><meta name="DC.contributor" content="Kevin Ferguson" scheme="inventor"><meta name="DC.contributor" content="Donald Gronachan" scheme="inventor"><meta name="DC.contributor" content="Motiva Llc" scheme="assignee"><meta name="DC.date" content="2009-2-16" scheme="dateSubmitted"><meta name="DC.description" content="A system for use in playing a video game, the system acting to measure the position of transponders for testing and to train the user to manipulate the position of the transponders while being guided by interactive and sensory feedback. A bidirectional communication link to a processing system supporting the video game provides functional movement assessment."><meta name="DC.date" content="2011-5-31" scheme="issued"><meta name="DC.relation" content="US:4337049" scheme="references"><meta name="DC.relation" content="US:4375674" scheme="references"><meta name="DC.relation" content="US:4627620" scheme="references"><meta name="DC.relation" content="US:4631676" scheme="references"><meta name="DC.relation" content="US:4645458" scheme="references"><meta name="DC.relation" content="US:4695953" scheme="references"><meta name="DC.relation" content="US:4702475" scheme="references"><meta name="DC.relation" content="US:4751642" scheme="references"><meta name="DC.relation" content="US:4787051" scheme="references"><meta name="DC.relation" content="US:4817950" scheme="references"><meta name="DC.relation" content="US:4912638" scheme="references"><meta name="DC.relation" content="US:4925189" scheme="references"><meta name="DC.relation" content="US:4988981" scheme="references"><meta name="DC.relation" content="US:5148154" scheme="references"><meta name="DC.relation" content="US:5184295" scheme="references"><meta name="DC.relation" content="US:5214615" scheme="references"><meta name="DC.relation" content="US:5227985" scheme="references"><meta name="DC.relation" content="US:5229756" scheme="references"><meta name="DC.relation" content="US:5239463" scheme="references"><meta name="DC.relation" content="US:5255211" scheme="references"><meta name="DC.relation" content="US:5288078" scheme="references"><meta name="DC.relation" content="US:5296871" scheme="references"><meta name="DC.relation" content="US:5320538" scheme="references"><meta name="DC.relation" content="US:5347306" scheme="references"><meta name="DC.relation" content="US:5372365" scheme="references"><meta name="DC.relation" content="US:5375610" scheme="references"><meta name="DC.relation" content="US:5385519" scheme="references"><meta name="DC.relation" content="US:5405152" scheme="references"><meta name="DC.relation" content="US:5421590" scheme="references"><meta name="DC.relation" content="US:5423554" scheme="references"><meta name="DC.relation" content="US:5429140" scheme="references"><meta name="DC.relation" content="US:5466200" scheme="references"><meta name="DC.relation" content="US:5469740" scheme="references"><meta name="DC.relation" content="US:5474083" scheme="references"><meta name="DC.relation" content="US:5485402" scheme="references"><meta name="DC.relation" content="US:5495576" scheme="references"><meta name="DC.relation" content="US:5506605" scheme="references"><meta name="DC.relation" content="US:5516105" scheme="references"><meta name="DC.relation" content="US:5524637" scheme="references"><meta name="DC.relation" content="US:5554033" scheme="references"><meta name="DC.relation" content="US:5574479" scheme="references"><meta name="DC.relation" content="US:5577981" scheme="references"><meta name="DC.relation" content="US:5580249" scheme="references"><meta name="DC.relation" content="US:5584700" scheme="references"><meta name="DC.relation" content="US:5587937" scheme="references"><meta name="DC.relation" content="US:5591104" scheme="references"><meta name="DC.relation" content="US:5597309" scheme="references"><meta name="DC.relation" content="US:5602569" scheme="references"><meta name="DC.relation" content="US:5605505" scheme="references"><meta name="DC.relation" content="US:5616078" scheme="references"><meta name="DC.relation" content="US:5638300" scheme="references"><meta name="DC.relation" content="US:5641288" scheme="references"><meta name="DC.relation" content="US:5645077" scheme="references"><meta name="DC.relation" content="US:5656904" scheme="references"><meta name="DC.relation" content="US:5659691" scheme="references"><meta name="DC.relation" content="US:5679004" scheme="references"><meta name="DC.relation" content="US:5702323" scheme="references"><meta name="DC.relation" content="US:5703623" scheme="references"><meta name="DC.relation" content="US:5704837" scheme="references"><meta name="DC.relation" content="US:5711304" scheme="references"><meta name="DC.relation" content="US:5715834" scheme="references"><meta name="DC.relation" content="US:5720619" scheme="references"><meta name="DC.relation" content="US:5741182" scheme="references"><meta name="DC.relation" content="US:5759044" scheme="references"><meta name="DC.relation" content="US:5785630" scheme="references"><meta name="DC.relation" content="US:5785631" scheme="references"><meta name="DC.relation" content="US:5790076" scheme="references"><meta name="DC.relation" content="US:5790124" scheme="references"><meta name="DC.relation" content="US:5792031" scheme="references"><meta name="DC.relation" content="US:5796354" scheme="references"><meta name="DC.relation" content="US:5812257" scheme="references"><meta name="DC.relation" content="US:5830065" scheme="references"><meta name="DC.relation" content="US:5835077" scheme="references"><meta name="DC.relation" content="US:5838816" scheme="references"><meta name="DC.relation" content="US:5846086" scheme="references"><meta name="DC.relation" content="US:5850201" scheme="references"><meta name="DC.relation" content="US:5872438" scheme="references"><meta name="DC.relation" content="US:5875257" scheme="references"><meta name="DC.relation" content="US:5888172" scheme="references"><meta name="DC.relation" content="US:5890995" scheme="references"><meta name="DC.relation" content="US:5897437" scheme="references"><meta name="DC.relation" content="US:5913727" scheme="references"><meta name="DC.relation" content="US:5919149" scheme="references"><meta name="DC.relation" content="US:5929782" scheme="references"><meta name="DC.relation" content="US:5963891" scheme="references"><meta name="DC.relation" content="US:5986644" scheme="references"><meta name="DC.relation" content="US:5989157" scheme="references"><meta name="DC.relation" content="US:6004243" scheme="references"><meta name="DC.relation" content="US:6013007" scheme="references"><meta name="DC.relation" content="US:6028593" scheme="references"><meta name="DC.relation" content="US:6043873" scheme="references"><meta name="DC.relation" content="US:6050822" scheme="references"><meta name="DC.relation" content="US:6050963" scheme="references"><meta name="DC.relation" content="US:6054951" scheme="references"><meta name="DC.relation" content="US:6059576" scheme="references"><meta name="DC.relation" content="US:6066075" scheme="references"><meta name="DC.relation" content="US:6073489" scheme="references"><meta name="DC.relation" content="US:6077201" scheme="references"><meta name="DC.relation" content="US:6088091" scheme="references"><meta name="DC.relation" content="US:6098458" scheme="references"><meta name="DC.relation" content="US:6100896" scheme="references"><meta name="DC.relation" content="US:6119516" scheme="references"><meta name="DC.relation" content="US:6238289" scheme="references"><meta name="DC.relation" content="US:6554707" scheme="references"><meta name="DC.relation" content="US:6585596" scheme="references"><meta name="citation_reference" content="&quot;MEMS enable smart golf clubs&quot;, Small Times, Jan. 6, 2005, 2 pages."><meta name="citation_reference" content="Achenbach, James, &quot;Golfs New Measuring Stick&quot;, Golfweek, Jun. 11, 2005, www.golfweek.com."><meta name="citation_reference" content="Allard, P., et al, Three-Dimensional Analysis of Human Movement, Human Kinetics (1995) 3, 8-14."><meta name="citation_reference" content="Analog Devices Data Sheet, &quot;MicroConverter®, Multichannel 12-Bit ADC with Embedded Flash MCU, ADuC812&quot; (2003) (http://www.analog.com/static/imported-files/data-sheets/ADUC812.pdf) 60 pages."><meta name="citation_reference" content="Analog Devices Data Sheet, “MicroConverter®, Multichannel 12-Bit ADC with Embedded Flash MCU, ADuC812” (2003) (http://www.analog.com/static/imported-files/data—sheets/ADUC812.pdf) 60 pages."><meta name="citation_reference" content="Bachman, E.R., Duman, I., Usta, U.Y., McGhee, R.B., Yun, X.P., Zyda, M.J.,&quot;Orientation Tracking for Humans and Robots Using Inertial Sensors,&quot; 1999 IEEE International Symposium on Computational Intelligence in Robotics and Automation (CIRA99) at 187-94 (1999)."><meta name="citation_reference" content="Ballagas, Rafael; Ringel, Meredith; Stone Maureen; and Borchers, Jan, &quot;iStuff: A Physical User Interface Toolkit For Ubiquitous Computer Environments,&quot; Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, vol. 5, No. 1, at 537-44 (ACM) (Apr. 5-10, 2003) (Gilbert Cockton, Panu Korhonen, eds.)."><meta name="citation_reference" content="Benbsat, Ari Y., Paradiso, Joseph A., &quot;An Inertial Measurement Framework for Gesture Recognition and Applications,&quot; Gesture and Sign Language in Human-Computer Interaction , vol. 2298/2002 at 77-90 (Springer-Verlag Berlin Heibelberg 2002); International Gesture Workshop, GW 2001, London, UK, 2001 Proceedings, LNAI 2298, at 9-20, I. Wachsmuth and T. Sowa (eds.)."><meta name="citation_reference" content="Brownell, Richard, Review of Peripheral-GameCube-G3 Wireless Controller, GAF, Jul. 17, 2003, 2 pages."><meta name="citation_reference" content="Brownstein, B., et al, Functional Movement in Orthopedic and Sports Physical Therapy, Churchill Livingstone (1997), 15."><meta name="citation_reference" content="Brugger, W., et al, Computer-aided tracking of body motions using a c.c.d.-image sensor, Med. Biol. Eng. &amp; Comput, (Mar. 1978), 207-210."><meta name="citation_reference" content="Buchanan, Levi, &quot;Happy Birthday, Rumble Pak&quot;, IGN.com, Apr. 3, 2008, 2 pages."><meta name="citation_reference" content="Codamotion: The science of real-time motion capture and analysis, webpages from http://www.charndyn.com/index.html, (Apr. 17, 2004) 1."><meta name="citation_reference" content="Codella, C., et al, Interactive Simulation in a Multi-Person Virtual World ACM (May 3-7, 1992), 329-334."><meta name="citation_reference" content="CyberGlove product, Immersion Corporation, 1990, http://www.cyberglovesystem.com."><meta name="citation_reference" content="DeLoura, M., et al, Game Programming Gems, Charles River Media, (2000) 200-204."><meta name="citation_reference" content="Europe is Bursting with Virtual Reality Ideas, But Developers Are Critically Strapped for Cash, webpages from https://www/lexis.com/research/retrieve?-m=66d17057c1b77f197aledb9f5fadb87d&amp;-browseType=Text, (Jan. 1993) 1-2."><meta name="citation_reference" content="Europe is Bursting with Virtual Reality Ideas, But Developers Are Critically Strapped for Cash, webpages from https://www/lexis.com/research/retrieve?—m=66d17057c1b77f197aledb9f5fadb87d&amp;—browseType=Text, (Jan. 1993) 1-2."><meta name="citation_reference" content="Eyestone, Dick, &quot;SmartSwing Company: Letter from the CEO&quot;, 3 pages, 2004, retrieved May 19, 2009, http://web.archive.org/web/20040810101314/www.smartsinggolf.com."><meta name="citation_reference" content="Fielder, Lauren, &quot;E3 2001: Nintendo unleashes GameCube software, a new Miyamoto game, and more&quot;, GameSpot, May 16, 2001, 2 pages, http://www.gamespot.com/downloads/2761390."><meta name="citation_reference" content="FrontSide Field Test, &quot;Smart Bomber Get This&quot;, Golf Magazine, Jun. 2005."><meta name="citation_reference" content="Furniss, Maureen, &quot;Motion Capture,&quot; MoCap MIT (Dec. 1999) 12 pages."><meta name="citation_reference" content="Gamecubicle, Jim-New Contributor, Nintendo Wavebird Control, http://www.gamecubicle.com/news-nintendo—gamecube—wavebird—contoller.htm, May 14, 2002."><meta name="citation_reference" content="Greenleaf, W.J., DataGlove, DataSuit, and virtual reality Advanced technology for people with disabilities, Proceedings of the Seventh Annual Conference ‘Technology and Persons with Disabilities,’ (Mar. 1992) 211-214."><meta name="citation_reference" content="Greenleaf, W.J., DataGlove, DataSuit, and virtual reality Advanced technology for people with disabilities, Proceedings of the Seventh Annual Conference &#39;Technology and Persons with Disabilities,&#39; (Mar. 1992) 211-214."><meta name="citation_reference" content="Gyration, Inc., GyroRemote GP240-001 Professional Series, copyrighted 2003, www.gyration.com."><meta name="citation_reference" content="Hinckley Synchronous Gestures Device, Oct. 12-15, 2003, Microsoft Research, USA."><meta name="citation_reference" content="Hinckley, Ken et al, &quot;Sensing Techniques for Mobile Interaction,&quot; Proceedings of the 13th Annual ACM Symposium on User Interface Software and Technology (San Diego, Cal.), ACM UIST 2000 &amp; Technology, CHI Letters 2 (2), at 91-100 (2000)."><meta name="citation_reference" content="Hinckley, Ken, &quot;Haptic Issues for Virtual Manipulation-A Dissertation Presented to the Faculty of the School of Engineering and Applied Science at the University of Virginia&quot; (Dec. 1996)."><meta name="citation_reference" content="Hinckley, Ken, &quot;Synchronous Gestures for Multiple Persons and Computer,&quot; Chi Letter, vol. 5, Issue 2, pp. 149-158, ACM, 2003."><meta name="citation_reference" content="Hinckley, Ken, “Haptic Issues for Virtual Manipulation—A Dissertation Presented to the Faculty of the School of Engineering and Applied Science at the University of Virginia” (Dec. 1996)."><meta name="citation_reference" content="Hinkley et al, Proceedings for the working conference on Advanced visual interfaces, Portal: the guide to computing literature, 1 page, 2004."><meta name="citation_reference" content="Holden, Maureen K., et al Use of Virtual Environments in Motor Learning and Rehabilitation Department of Brain and Cognitive Sciences, Handbook of Virtual Environments: Design, Implementation, and Applications chap 49, pp. 999-1026, Stanney (ed), Lawrence Erlbaum Associates 2002."><meta name="citation_reference" content="Immersion CyberGlove product, Immersion Corporation, 1990, http://www.cyberglovesystem.com."><meta name="citation_reference" content="InterSense, &quot;InterSense Inc.-The New Standard in Motion Tracking&quot; www.isense.com Dec. 18, 2003."><meta name="citation_reference" content="InterSense, &quot;InterSense Inc.-The New Standard in Motion Tracking&quot; www.isense.com Mar. 27, 2004."><meta name="citation_reference" content="InterSense, &quot;InterSense Motion Trackers&quot; www.isense.com Mar. 12, 1998."><meta name="citation_reference" content="InterSense, &quot;IS-900 Precision Motion Trackers&quot; www.isense.com May 16, 2003."><meta name="citation_reference" content="InterSense, &quot;IS-900 Precision Motion Trackers&quot; www.isense.com Sep. 10, 2002."><meta name="citation_reference" content="InterSense, &quot;Technical Overview IS-900 Motion Tracking System&quot; www.isense.com, Apr. 2004."><meta name="citation_reference" content="InterSense, “InterSense Inc.—The New Standard in Motion Tracking” www.isense.com Dec. 18, 2003."><meta name="citation_reference" content="InterSense, “InterSense Inc.—The New Standard in Motion Tracking” www.isense.com Mar. 27, 2004."><meta name="citation_reference" content="IREX, Virtual Reality Technologies, webpages from http://www.irexonline.com/how-it-works.htm, (Apr. 19, 2004) 1-2."><meta name="citation_reference" content="IREX, Virtual Reality Technologies, webpages from http://www.irexonline.com/how—it—works.htm, (Apr. 19, 2004) 1-2."><meta name="citation_reference" content="IS-900 product, InterSense, Inc. 1999."><meta name="citation_reference" content="Kaplan, US S.I.R. H1383, Dec. 6, 1994."><meta name="citation_reference" content="Kasvand, T., et al, Computers and the Kinesiology of Gait, Comput. Biol. Med. Pergamon Press (1976) vol. 6 111-120."><meta name="citation_reference" content="Kenmochi, A., et al, A network virtual reality skiing system-system overview and skiing movement estimation, Symbiosis of Human and Artifact, (Jul. 1995) 423-428."><meta name="citation_reference" content="Kraus, A., Matrices for Engineers, Hemisphere Publishing Corp. (1987) 118-120, 124-126."><meta name="citation_reference" content="Kunz, Andreas and Burri, Adrian &quot;Design and Construction of a New Haptic Interface,&quot; ASME 2000 Design Engineering Technical Conferences and Computers and Information in Engineering Conference, Baltimore, Maryland, Sep. 10-13, 2000; Proceedings of DETC &#39;00."><meta name="citation_reference" content="Lengyel, E., Mathematics for 3D Game Programming &amp; Computer Graphics, Charles River Media (2004) 76-78, 467-468."><meta name="citation_reference" content="Logitech 3D Mouse &amp; Head Tracker Technical Manual, Logitech Inc., Fremont CA, 1992, 88 pages."><meta name="citation_reference" content="Logitech WingMan Cordless Rumblepad, Logitech, Press release Sep. 2, 2001, 2 pages."><meta name="citation_reference" content="MacLean, Karen, &quot;Designing with Haptic Feedback&quot;, Proceedings of IEEE Robotics and Automation (ICRA &#39;2000), at 783-88 (Apr. 22-28 (2000))."><meta name="citation_reference" content="Marrin, Teresa and Paradiso, Joseph, &quot;The Digital Baton: a Versatile Performance Instrument&quot; (1997)."><meta name="citation_reference" content="Marrin, Teresa, &quot;Toward an Understanding of Musical Gesture: Mapping Expressive Intention with the Digital Baton&quot; (1996)."><meta name="citation_reference" content="Marti, Gaetan et al, &quot;Biopsy navigator: a smart haptic interface for interventional radiological gestures,&quot; Swiss Federal Institute of Technology (EPFL), CARS 2003 (Computer Assisted Radiology and Surgery), Proceedings of the 17th Int&#39;l Congress &amp; Exhibition, International Congress Series, vol. 1256 at 788-93 (Jun. 2003)."><meta name="citation_reference" content="Medved, V., Towards a virtual reality-assisted movement diagnostics-an outline, Robotica (Jan.-Feb. 1994) vol. 12, 55-57."><meta name="citation_reference" content="Merians, Alma S. et al &quot;Virtual Reality-Augmented Rehabilitation for Patients Following Stroke,&quot; Physical Therapy, vol. 82, No. 9 (Sep. 2002)."><meta name="citation_reference" content="Merians, Alma S. et al “Virtual Reality—Augmented Rehabilitation for Patients Following Stroke,” Physical Therapy, vol. 82, No. 9 (Sep. 2002)."><meta name="citation_reference" content="Mulder, A., Human movement tracking technology, School of Kinesiology, Simon Fraser University (Jul. 1994) 1-14."><meta name="citation_reference" content="Mulder, Axel, &quot;Human movement tracking technology,&quot; Hand Centered Studies of Human Movement Project, Technical Report 94-1, Simon Fraser University (Jul. 1994), 16 pages."><meta name="citation_reference" content="Nintendo Game Boy Advance Wireless Adapter, Nintendo World Report, Sep. 25, 2003, 2 pages."><meta name="citation_reference" content="Nintendo Game Boy Advance, Nintendo, Sep. 26, 2003, Wikipedia Article http://en.wikipedia.org/wiki/Game-Boy-Advance."><meta name="citation_reference" content="Nintendo Game Boy Advance, Nintendo, Sep. 26, 2003, Wikipedia Article http://en.wikipedia.org/wiki/Game—Boy—Advance."><meta name="citation_reference" content="Nintendo Game Boy Color Game Catridge with Built-In Rumble, Nintendo, Jun. 28, 1999, Wikipedia Article, 2pages http://en.wikipedia.org/wiki/Rumble-Pak."><meta name="citation_reference" content="Nintendo Game Boy Color Game Catridge with Built-In Rumble, Nintendo, Jun. 28, 1999, Wikipedia Article, 2pages http://en.wikipedia.org/wiki/Rumble—Pak."><meta name="citation_reference" content="Nintendo Game Boy Color Nintendo, Nov. 18, 1998, Wikipedia Article http://en.wikipedia.org/wiki/Game-Boy-Color."><meta name="citation_reference" content="Nintendo Game Boy Color Nintendo, Nov. 18, 1998, Wikipedia Article http://en.wikipedia.org/wiki/Game—Boy—Color."><meta name="citation_reference" content="Nintendo Game Boy, Consumer Information and Precautions Booklet, Nintendo,Jul. 31, 1989."><meta name="citation_reference" content="Nintendo Game Boy, Nintendo,Jul. 31, 1989, Wikipedia Article, http://en.wikipedia.org/wiki/Game-Boy."><meta name="citation_reference" content="Nintendo Game Boy, Nintendo,Jul. 31, 1989, Wikipedia Article, http://en.wikipedia.org/wiki/Game—Boy."><meta name="citation_reference" content="Nintendo GameCube Standard Controller, Nintendo, Nov. 18, 2001, Wikipedia Article, http://en.wikipedia.org/wiki/Nintendo-GameCube-Controller."><meta name="citation_reference" content="Nintendo GameCube Standard Controller, Nintendo, Nov. 18, 2001, Wikipedia Article, http://en.wikipedia.org/wiki/Nintendo—GameCube—Controller."><meta name="citation_reference" content="Nintendo GameCube System, Nintendo, Nov. 18, 2001, Wikipedia Article, http://en.wikipedia.org/wiki/Nintendo-GameCube."><meta name="citation_reference" content="Nintendo GameCube System, Nintendo, Nov. 18, 2001, Wikipedia Article, http://en.wikipedia.org/wiki/Nintendo—GameCube."><meta name="citation_reference" content="Nintendo N64 Rumble Pack, Nintendo, Jul. 1997, Wikipedia Article, 12 pages, http://en.wikipedia.org/wiki/Rumble-Pak."><meta name="citation_reference" content="Nintendo N64 Rumble Pack, Nintendo, Jul. 1997, Wikipedia Article, 12 pages, http://en.wikipedia.org/wiki/Rumble—Pak."><meta name="citation_reference" content="Nintendo Wavebird Controller, Nintendo, Jun. 2002 Wikipedia Article, http://en.wikipedia.org/wiki/WaveBird."><meta name="citation_reference" content="Nintendo Wavebird Controller, Operation of the WaveBird Controller Nintendo, Jun. 2002, 2 pages."><meta name="citation_reference" content="Nintendo&quot;Kirby Tilt &#39;n&#39; Tumble&quot; video game Instruction booklet, Nintendo, Apr. 11, 2001, 18 pages."><meta name="citation_reference" content="Nintendo&quot;Kirby Tilt &#39;n&#39; Tumble&quot; video game, Nintendo, Apr. 11, 2001, Wikipedia Article, http:en.wikipedia.org/wiki/Kirby-Tilt-%27%-Tumble."><meta name="citation_reference" content="Nintendo, Nintendo Entertainment System Booth 2002."><meta name="citation_reference" content="Nintendo, Nintendo Entertainment System Consumer Information and Precautions Booklet , Nintendo of America, Inc. 1992."><meta name="citation_reference" content="Nintendo, Nintendo Entertainment System Instruction Manual, Nintendo of America, Inc. 1992."><meta name="citation_reference" content="Nintendo, Nintendo Entertainment System Layout May 9, 2002."><meta name="citation_reference" content="Nintendo, Nintendo Feature: History of Pokemon Part 2, Official Nintendo Magazine, May 17, 2009, http://www.officialnintendomagazine.co.uk/article.php?id=8576."><meta name="citation_reference" content="Nintendo“Kirby Tilt ‘n’ Tumble” video game Instruction booklet, Nintendo, Apr. 11, 2001, 18 pages."><meta name="citation_reference" content="Nintendo“Kirby Tilt ‘n’ Tumble” video game, Nintendo, Apr. 11, 2001, Wikipedia Article, http:en.wikipedia.org/wiki/Kirby—Tilt—%27%—Tumble."><meta name="citation_reference" content="Nintendo“Kirby Tilt ‘n’ Tumble” video game, Nintendo, Apr. 11, 2001."><meta name="citation_reference" content="Paradiso, Joseph A., &quot;The Brain Opera Technology: New Instruments and Gestural Sensors for Musical Interaction and Performance&quot; (Nov. 1998) (&quot;Brain Opera Article&quot;)."><meta name="citation_reference" content="Pelican Accessories G3 Wireless Controller, Pelican Accessories, Sep. 20, 2002."><meta name="citation_reference" content="Perry, Simon, “Nintendo to Launch Wireless Game Boy Adaptor,” Digital Lifestyles, Sep. 26, 2003 http://digital-lifestyles.info/2003/09/26/nintendo-to-launch-wireless-game-boy-adaptor/."><meta name="citation_reference" content="Philips, LPC2104/2105/2106 Single-chip 32 bit microcontrollers; 128 kB ISP/IAP Flash with 64 kB/32 kB/16 kB RAM Product Data, Rev. 05 Dec. 22, 2004."><meta name="citation_reference" content="Pokemon Pinball Game, 1999, Wikipedia Article, http://en.wikipedia.org/wiki/Pok?C3?A9mon—Pinball."><meta name="citation_reference" content="Polhemus, Liberty: The Forerunner in Electromagnetic Tracking Technology, www.polhemus.com, (May 2003) 1-2."><meta name="citation_reference" content="Polhemus, Patriot: The Fast and Affordable Digital Tracker, www.polhemus.com, (Feb. 2004) 1-2."><meta name="citation_reference" content="Polhemus,“Fastrak, The Fast and Easy Digital Tracker” copyrighted 2001, Coldiester, Vermont, 2 pages."><meta name="citation_reference" content="PowerGlove and Nintendo product photo, Mattel, 1989."><meta name="citation_reference" content="PowerGlove product photo, Mattel, 1989."><meta name="citation_reference" content="PowerGlove product Program Guide, Mattel, 1989."><meta name="citation_reference" content="PowerGlove product, Instructions, Mattel, 1989."><meta name="citation_reference" content="PowerGlove product, Mattel, 1989 Wikipedia Article."><meta name="citation_reference" content="Reality built for two: a virtual reality tool, Symposium on Interactive 3D Graphics, ACM Press webpages from http://portal.acm.org/citation.cfm?id=91385.91409&amp;dl+ACM&amp;type=series&amp;i (Jun. 10, 2004) 1-4."><meta name="citation_reference" content="Regan, Mary Beth, &quot;Smart Golf Clubs&quot;, baltimoresun.com, Jun. 17, 2005."><meta name="citation_reference" content="Ruby, D., Biomechanics-how computers extend athletic performance to the body&#39;s far limits, Popular Science (Jan. 1982) 58-60."><meta name="citation_reference" content="Sandweiss, J., et al, Biofeedback and Sports Science, Plenum Press New York (1985) 1-201."><meta name="citation_reference" content="Satterfield, Shane, E3 2002: Nintendo announces new GameCube games, GameSpot, May 21, 2002, http://wwwgamespot.com/gamecube/action.rollorama/new.html?sid=2866974&amp;com—act-convert&amp;om—clk=nesfeatures&amp;tag=newsfeatures%Btitle%3B."><meta name="citation_reference" content="Scarborough, E.L., Enhancement of Audio Localization Cue Synthesis by Adding Environmental and Visual Cues, Air Force Inst. Of Tech., Wright-Patterson AFB, OH. School of Engineering (Dec. 1992) 1-4."><meta name="citation_reference" content="Smartswing, &quot;Register to be Notified When SmartSwing Products are Available for Purchase&quot;, 3 pages, May 2004, retrieved May 19, 2009, http://web.archive.org/web/20040426182437/www.smartswinggolf.com/..."><meta name="citation_reference" content="Smartswing, &quot;SmartSwing: Intelligent Golf Clubs that Build a Better Swing&quot;, 2 pages, 2004, retrieved May 19, 2009, http://web.archive.org/web/20040728221951/http://www.smartswinggolf..."><meta name="citation_reference" content="SmartSwing, SmartSwing Inc.,Apr. 2004, Austin, Texas."><meta name="citation_reference" content="Smartswing, The SmartSwing Learning System Overview, 3 pages, 2004, retrieved May 19, 2009, http://web.archive.org/web/20040810142134/http://www.smartswinggolf.com/t..."><meta name="citation_reference" content="Smartswing, The SmartSwing Learning System: How it works, 3 pages, 2004, retrieved May 19, 2009, http://web.archive.org/web/20040403213108/http://www.smartswinggolf.com/t..."><meta name="citation_reference" content="Smartswing, The SmartSwing Product, 3 pages, 2004, retrieved May 19, 2009, http://web.archive.org/web/200400403204628/http://www.smartswinggolf.com/t..."><meta name="citation_reference" content="Smartswing, The SmartSwing Product: Technical Information, 1 page, 2004, retrieved May 19, 2009, http://web.archive.org/web/200400403205906/http://www.smartswinggolf.com/..."><meta name="citation_reference" content="Smith, J., et al, Virtual Batting Cage and Human Model, Virtual Human http://www.cs.berkeley.edu/ rcdavis/classes/&#39;cs294/, (Jun. 17, 2004)1-5."><meta name="citation_reference" content="Success Story Profile: Innovative Sports Training, Motion Monitor, (2002) 1-2."><meta name="citation_reference" content="Sulic, Ivan, Logitech Wingman Cordless Rumblepad Review, IGN Entertainment Games, Jan. 14, 2002, 3 pages."><meta name="citation_reference" content="Swisher, Kara, &quot;How Science Can Improve Your Golf Game&quot;, The Wall Street Journal, Apr. 18, 2005."><meta name="citation_reference" content="Training Aid, SmartSwing, PGA Magazine, p. 46, Apr. 2005, www.pgamagazine.com."><meta name="citation_reference" content="Verplaetse, C., &quot;Inertial Proprioceptive Devices: Self-Motion-Sensing Toys and Tools,&quot; IBM Systems Journal, vol. 35 Nos. 3 &amp; 4, at 639-50 (IBM Corp.) (1996)."><meta name="citation_reference" content="Villoria, Gerald, Hands on Roll-O-Rama Game Cube, Game Spot, May 29, 2002, http://www.gamespot.com/gamecube/action/rollorama/news.html?sid=2868421&amp;com—act=convert&amp;om—clk=newsfeatures&amp;tag=newsfeatures;title;1&amp;m."><meta name="citation_reference" content="VTI, Mindflux-VTi CyberTouch, 1996, http://www.mindflux.com.au/products/vti/cybertouch.html."><meta name="citation_reference" content="Wiley, M., &quot;Nintendo Wavebird Review&quot; US, Jun. 11, 2002, 21 pages."><meta name="citation_reference" content="Williams II, Robert L. et al, &quot;Implementation and Evaluation of A Haptic Playback System,&quot; vol. 3, No. 3, Haptics-e, 2004, http://www.haptics-e.org."><meta name="citation_reference" content="Williams II, Robert L. et al, &quot;The Virtual Haptic Back Project,&quot; Presented at the Image 2003 Conference, Scottsdale, Arizona, 14-18 Jul. 2003."><meta name="citation_reference" content="Wilson et al, &quot;Gesture Recognition Using the XWand,&quot; Assistive Intelligent Environments Group, Robotics Institute, Carnegie Mellon University, Report CMU-RI-TR-04-57 (Apr. 2004)."><meta name="citation_reference" content="Wilson et al, &quot;XWand: UI for Intelligent Spaces,&quot; Microsoft Research, inProceedings of the SIGCHI Conference on Human Factors in Computing Systems (Ft. Lauderdale, Florida, USA, Apr. 5-10, 2003; CHI &#39;03, ACM, New York, NY, at 545-52 (2003)."><meta name="citation_reference" content="Wilson, Andy, &quot;XWand: UI for Intelligent Environments&quot;, 5 pages retrieved Jan. 11, 2009, http://research.microsoft.com/en-us/um/people/awilson/wand/default.htm."><meta name="citation_reference" content="Wormell, D. and Foxlin, E., &quot;Advancements in 3D Interactive Devices for Virtual Environments,&quot; EGVE, Proceedings of the Workshop on Virtual Environments 2003, vol. 39 at 47-56 (ACM) (2003) (The Eurographics Association)."><meta name="citation_reference" content="Yang, Ungyeon and Kim, Gerard Jounghyun, &quot;Implementation and Evaluation of &#39;Just Follow Me&#39;: An Ilmmersive, VR-Based, Motion-Training System,&quot; Presence: Teleoperators and Virtual Environments vol. 11 No. 3, at 304-23 (MIT Press) (Jun. 2002)."><meta name="citation_reference" content="Yang, Ungyeon and Kim, Gerard Jounghyun, “Implementation and Evaluation of ‘Just Follow Me’: An Ilmmersive, VR-Based, Motion-Training System,” Presence: Teleoperators and Virtual Environments vol. 11 No. 3, at 304-23 (MIT Press) (Jun. 2002)."><meta name="citation_reference" content="Zetu, D., et al, Extended range tracking for remote virtual reality-aided facility management, Department of Mechanical Engineering The University of Illinois at Chicago, http://alpha.me.uic.edu/dan/NsfPaper/nsf2.html, (Apr. 19, 2004)1-9."><meta name="citation_patent_number" content="US:7952483"><meta name="citation_patent_application_number" content="US:12/371,711"><link rel="canonical" href="http://www.google.com/patents/US7952483"/><meta property="og:url" content="http://www.google.com/patents/US7952483"/><meta name="title" content="Patent US7952483 - Human movement measurement system"/><meta name="description" content="A system for use in playing a video game, the system acting to measure the position of transponders for testing and to train the user to manipulate the position of the transponders while being guided by interactive and sensory feedback. A bidirectional communication link to a processing system supporting the video game provides functional movement assessment."/><meta property="og:title" content="Patent US7952483 - Human movement measurement system"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("sVjpU7i9IaqxsQTWr4HgBQ"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407464522.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("EGY"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("sVjpU7i9IaqxsQTWr4HgBQ"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407464522.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("EGY"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7952483?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7952483"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=K_feBgABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7952483&amp;usg=AFQjCNHmMibN7-MA61lPf8sq5e0BxQxOyQ" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7952483.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7952483.pdf"></a><a class="appbar-application-grant-link" data-label="Application" href="/patents/US20090149257"></a><a class="appbar-application-grant-link" data-selected="true" data-label="Grant" href="/patents/US7952483"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7952483" style="display:none"><span itemprop="description">A system for use in playing a video game, the system acting to measure the position of transponders for testing and to train the user to manipulate the position of the transponders while being guided by interactive and sensory feedback. A bidirectional communication link to a processing system supporting...</span><span itemprop="url">http://www.google.com/patents/US7952483?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7952483 - Human movement measurement system</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7952483 - Human movement measurement system" title="Patent US7952483 - Human movement measurement system"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7952483 B2</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 12/371,711</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">May 31, 2011</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Feb 16, 2009</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Jul 29, 2004</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CA2578653A1">CA2578653A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1779344A2">EP1779344A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1779344A4">EP1779344A4</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7292151">US7292151</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7492268">US7492268</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8159354">US8159354</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8427325">US8427325</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20060022833">US20060022833</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20080061949">US20080061949</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20090149257">US20090149257</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20110201428">US20110201428</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20120178534">US20120178534</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20130303286">US20130303286</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO2006014810A2">WO2006014810A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO2006014810A3">WO2006014810A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO2006014810A9">WO2006014810A9</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">12371711, </span><span class="patent-bibdata-value">371711, </span><span class="patent-bibdata-value">US 7952483 B2, </span><span class="patent-bibdata-value">US 7952483B2, </span><span class="patent-bibdata-value">US-B2-7952483, </span><span class="patent-bibdata-value">US7952483 B2, </span><span class="patent-bibdata-value">US7952483B2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Kevin+Ferguson%22">Kevin Ferguson</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Donald+Gronachan%22">Donald Gronachan</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Motiva+Llc%22">Motiva Llc</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7952483.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7952483.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7952483.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (105),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (134),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (11),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (44),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (3)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7952483&usg=AFQjCNG6d_NJukmMCHZ7oaLcNS-Um-nhcQ">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7952483&usg=AFQjCNGLB1F3ijQNMKduC3FJmz62JfhA3w">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7952483B2%26KC%3DB2%26FT%3DD&usg=AFQjCNG5_umwzUhNF0vOjg02sbUQzxw0MQ">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT103216178" lang="EN" load-source="patent-office">Human movement measurement system</invention-title></span><br><span class="patent-number">US 7952483 B2</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA85126251" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">A system for use in playing a video game, the system acting to measure the position of transponders for testing and to train the user to manipulate the position of the transponders while being guided by interactive and sensory feedback. A bidirectional communication link to a processing system supporting the video game provides functional movement assessment.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(11)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7952483B2/US07952483-20110531-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7952483B2/US07952483-20110531-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7952483B2/US07952483-20110531-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7952483B2/US07952483-20110531-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7952483B2/US07952483-20110531-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7952483B2/US07952483-20110531-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7952483B2/US07952483-20110531-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7952483B2/US07952483-20110531-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7952483B2/US07952483-20110531-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7952483B2/US07952483-20110531-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7952483B2/US07952483-20110531-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(50)</span></span></div><div class="patent-text"><div mxw-id="PCLM36066253" lang="EN" load-source="patent-office" class="claims">
    <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
      <div class="claim-text">1. A system for a user to play a video game, comprising:
<div class="claim-text">a first hand-held communication device comprising:
<div class="claim-text">a transmitter for transmitting signals;</div>
<div class="claim-text">a receiver for receiving signals; and</div>
<div class="claim-text">an output device; and</div>
</div>
<div class="claim-text">a processing system, remote from the first hand-held communication device, adapted to wirelessly receive the signals transmitted by the transmitter, to determine movement information for the first hand-held communication device, to determine position information from the signals transmitted by the transmitter, and to send data signals to the receiver to provide feedback data to the user;</div>
<div class="claim-text">an interactive interface such that movement information of the first hand-held communication device controls the movement of at least one object in a computer generated virtual environment; and</div>
<div class="claim-text">wherein the first hand-held communication device receives and processes the received data signals and generates sensory stimuli for the user, based on the received data signals and delivered through the output device.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
      <div class="claim-text">2. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<div class="claim-text">a second hand-held communication device adapted to electrically communicate with the first hand-held communication device, said second hand-held communication device, comprising a transmitter for transmitting signals;</div>
<div class="claim-text">wherein the processing system is adapted to determine movement information of the second hand-held communication device.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
      <div class="claim-text">3. The video game system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the processing system is adapted to provide performance indicators based on the movement information of the first hand-held communication device and the second hand-held communication device.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
      <div class="claim-text">4. The video game system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the feedback data sent to the receiver is derived from the movement information of the first hand-held communication device and the second hand-held communication device.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
      <div class="claim-text">5. The video game system of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the output device provides audible stimuli to the user based on the feedback data.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
      <div class="claim-text">6. The video game system of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the output devices provides tactile stimuli to the user based on the feedback data.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
      <div class="claim-text">7. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system is adapted to provide feedback data proportional to an error signal determined by comparing the movement of the user to a reference movement trajectory.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
      <div class="claim-text">8. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system is adapted to store activity attributes.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
      <div class="claim-text">9. The video game system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein:
<div class="claim-text">the activity attributes stored by the system comprise at least one of:</div>
<div class="claim-text">the number of repetitions performed by the user; and</div>
<div class="claim-text">the energy expenditure of the user.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
      <div class="claim-text">10. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first hand-held communication device is adapted to attach to a modular extension piece.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
      <div class="claim-text">11. The video game system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the modular extension piece is a rod.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
      <div class="claim-text">12. The video game system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the modular extension piece provides force resistive feedback.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
      <div class="claim-text">13. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system is adapted to excite the output device based on the movement information for the first hand-held communication device.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
      <div class="claim-text">14. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system is adapted to excite the output device to indicate the quality of movement information for the first hand-held communication device.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
      <div class="claim-text">15. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system is adapted to proportionally excite the output device based on the movement information for the first hand-held communication device.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
      <div class="claim-text">16. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system is adapted to provide feedback data to the output device when the user has completed an activity.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
      <div class="claim-text">17. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system is adapted to modulate activity challenges based on scoring criteria.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
      <div class="claim-text">18. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system is adapted to visually queue the user to move according to a desired movement trajectory to assess balance control.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
      <div class="claim-text">19. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system is adapted to determine range of motion of a user's joint or body segment.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
      <div class="claim-text">20. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system is adapted to queue the user to control range of motion within prescribed limits.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00021" num="00021" class="claim">
      <div class="claim-text">21. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system is adapted to provide feedback data to the output device when the user deviates from a desired movement trajectory.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00022" num="00022" class="claim">
      <div class="claim-text">22. The video game system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein functional movements can be measured of gait, jumping, cutting, turning, or shuttling.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00023" num="00023" class="claim">
      <div class="claim-text">23. The video game system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the functional movement measured can be balance or stance.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00024" num="00024" class="claim">
      <div class="claim-text">24. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the system is interactive and responds to verbal command input from the user to modulate an activity.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00025" num="00025" class="claim">
      <div class="claim-text">25. The video game system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<div class="claim-text">a second hand-held communication device in electrical communication with the first hand-held communication device, said second hand-held communication device, comprising a transmitter for transmitting signals,</div>
<div class="claim-text">wherein the processing system is adapted to determine movement information of the second hand-held communication device; and</div>
<div class="claim-text">wherein the movement information of the first hand-held communication device and the second hand-held communication device is used to derive performance measurements of functional movements.</div>
</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00026" num="00026" class="claim">
      <div class="claim-text">26. A system for a user to play a video game, comprising:
<div class="claim-text">a first hand-held communication device comprising:
<div class="claim-text">a transmitter for transmitting signals;</div>
<div class="claim-text">a receiver for receiving signals; and</div>
<div class="claim-text">an output device; and</div>
</div>
<div class="claim-text">a processing system, remote from the first hand-held communication device for wirelessly receiving the signals transmitted by the transmitter, determining movement information for first hand-held communication device and sending data signals to the receiver to provide feedback data to the user;</div>
<div class="claim-text">a second hand-held communication device, in wireless communication with the processing system, said second hand-held communication device, comprising a transmitter for transmitting signals;</div>
<div class="claim-text">wherein the processing system is adapted to determine movement information of the second hand-held communication device;</div>
<div class="claim-text">wherein the first hand-held communication device receives and processes the received data signals and generates sensory stimuli for the user, based on the received data signals and delivered through the output device; and</div>
<div class="claim-text">wherein the system is interactive and responds to verbal command input from the user to modulate an activity; and</div>
<div class="claim-text">wherein the system is adapted to provide feedback data to the output device when the user deviates from a desired movement trajectory.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00027" num="00027" class="claim">
      <div class="claim-text">27. The video game system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the system is adapted to modulate activity challenges based on scoring criteria.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00028" num="00028" class="claim">
      <div class="claim-text">28. The video game system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the system is adapted to visually queue the user to move according to a desired movement trajectory to assess balance control.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00029" num="00029" class="claim">
      <div class="claim-text">29. The video game system of <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein the system is adapted to determine range of motion of a user's joint or body segment.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00030" num="00030" class="claim">
      <div class="claim-text">30. A system for a user to play a video game, comprising:
<div class="claim-text">a first hand-held communication device comprising:
<div class="claim-text">a transmitter for transmitting signals;</div>
<div class="claim-text">a receiver for receiving signals; and</div>
<div class="claim-text">an output device; and</div>
</div>
<div class="claim-text">a processing system, remote from the first hand-held communication device, adapted to wirelessly receive the signals transmitted by the transmitter, to determine movement information for the first hand-held communication device, to provide feedback data proportional to an error signal determined by comparing the movement of the user to a reference movement trajectory, and to send data signals to the receiver to provide feedback data to the user;</div>
<div class="claim-text">an interactive interface such that movement information of the first hand-held communication device controls the movement of at least one object in a computer generated virtual environment; and</div>
<div class="claim-text">wherein the first hand-held communication device receives and processes the received data signals and generates sensory stimuli for the user, based on the received data signals and delivered through the output device.</div>
</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00031" num="00031" class="claim">
      <div class="claim-text">31. A system for a user to play a video game, comprising:
<div class="claim-text">a first hand-held communication device comprising:
<div class="claim-text">a transmitter for transmitting signals;</div>
<div class="claim-text">a receiver for receiving signals; and</div>
<div class="claim-text">an output device;</div>
</div>
<div class="claim-text">a second hand-held communication device, the second hand-held communication device comprising a transmitter for transmitting signals; and</div>
<div class="claim-text">a processing system, remote from the first hand-held communication device, adapted to wirelessly receive the signals transmitted by the transmitters of the respective communication devices, to determine movement information for each of the respective communication devices, and to send data signals to the receiver to provide feedback data to the user; and</div>
<div class="claim-text">wherein the first hand-held communication device receives and processes the received data signals and generates sensory stimuli for the user, based on the received data signals and delivered through the output device.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00032" num="00032" class="claim">
      <div class="claim-text">32. The system according to <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein the first hand-held communication device is adapted to attach to a modular extension piece.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00033" num="00033" class="claim">
      <div class="claim-text">33. The system of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein the modular extension piece is a rod.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00034" num="00034" class="claim">
      <div class="claim-text">34. The system of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein the modular extension piece provides force resistive feedback.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00035" num="00035" class="claim">
      <div class="claim-text">35. The system of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein the modular extension piece is a weighted extension.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00036" num="00036" class="claim">
      <div class="claim-text">36. The system according to <claim-ref idref="CLM-00031">claim 31</claim-ref>, further comprising:
<div class="claim-text">a strap connected to the first hand-held communication device for securing the first hand-held communication device to a user's wrist.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00037" num="00037" class="claim">
      <div class="claim-text">37. The system according to <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein:
<div class="claim-text">the first hand-held communication device comprises a user input device adapted for communication with the processing system through the transmitter.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00038" num="00038" class="claim">
      <div class="claim-text">38. The system according to <claim-ref idref="CLM-00037">claim 37</claim-ref>, wherein:
<div class="claim-text">the user input device is adapted for calibrating the first communication device to establish a reference position.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00039" num="00039" class="claim">
      <div class="claim-text">39. The system according to <claim-ref idref="CLM-00031">claim 31</claim-ref>,wherein:
<div class="claim-text">the processing system is adapted to determine position information for each of the respective communication devices.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00040" num="00040" class="claim">
      <div class="claim-text">40. The system according to <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein the processing system is further comprised of:
<div class="claim-text">a memory for storing data representing a reference movement trajectory used for comparison with a detected movement trajectory of the user.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00041" num="00041" class="claim">
      <div class="claim-text">41. The system according to <claim-ref idref="CLM-00031">claim 31</claim-ref>, further comprising:
<div class="claim-text">a memory for storing a user's measured range of motion.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00042" num="00042" class="claim">
      <div class="claim-text">42. The system according to <claim-ref idref="CLM-00031">claim 31</claim-ref>, further comprising:
<div class="claim-text">a coupling attachment to the first hand-held communication device adapted to accept attachment to the second hand-held communication device.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00043" num="00043" class="claim">
      <div class="claim-text">43. The system according to <claim-ref idref="CLM-00031">claim 31</claim-ref>, further comprising:
<div class="claim-text">a coupling attachment to the first hand-held communication device adapted to accept attachment to a modular extension piece.</div>
</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00044" num="00044" class="claim">
      <div class="claim-text">44. A system for a user to play a video game, comprising:
<div class="claim-text">a first hand-held communication device comprising:
<div class="claim-text">a transmitter for transmitting signals;</div>
<div class="claim-text">a receiver for receiving signals; and</div>
<div class="claim-text">an output device;</div>
</div>
<div class="claim-text">a second hand-held communication device adapted to electrically communicate with the first communication device, and adapted for being attached to, in contact with, or held by the user, the second hand-held communication device comprising a transmitter for transmitting signals; and</div>
<div class="claim-text">a processing system, remote from the first hand-held communication device, adapted to wirelessly receive the signals transmitted by the transmitter of the first hand-held communication device, to determine movement information for each of the respective communication devices, and to send data signals to the receiver to provide feedback data to the user;</div>
<div class="claim-text">wherein the first hand-held communication device is adapted to receive and process the received data signals and generate sensory stimuli for the user, based on the received data signals, the sensory stimuli delivered through the output device;</div>
<div class="claim-text">wherein the first hand-held communication device is further comprised of a user input device adapted for communication with the processing system through the transmitter; and</div>
<div class="claim-text">wherein the user input device is adapted for calibrating the first communication device to establish a reference position.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00045" num="00045" class="claim">
      <div class="claim-text">45. The system according to <claim-ref idref="CLM-00044">claim 44</claim-ref>,wherein:
<div class="claim-text">the processing system is adapted to determine position information for each of the respective communication devices.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00046" num="00046" class="claim">
      <div class="claim-text">46. The system according to <claim-ref idref="CLM-00044">claim 44</claim-ref>,wherein:
<div class="claim-text">wherein the output device is a speaker for providing audible stimuli to the user.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00047" num="00047" class="claim">
      <div class="claim-text">47. The system according to <claim-ref idref="CLM-00044">claim 44</claim-ref>,wherein:
<div class="claim-text">the first hand-held communication device is adapted to attach to a modular extension piece.</div>
</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00048" num="00048" class="claim">
      <div class="claim-text">48. A system for a user to play a video game, comprising:
<div class="claim-text">a first hand-held communication device comprising:
<div class="claim-text">a transmitter for transmitting signals;</div>
<div class="claim-text">a receiver for receiving signals; and</div>
<div class="claim-text">an output device;</div>
</div>
<div class="claim-text">a second hand-held communication device adapted to electrically communicate with the first communication device, and adapted for being attached to, in contact with, or held by the user, the second hand-held communication device comprising a transmitter for transmitting signals; and</div>
<div class="claim-text">a processing system, remote from the first hand-held communication device, adapted to wirelessly receive the signals transmitted by the transmitter of the first hand- held communication device, to determine position information for each of the respective communication devices, and to send data signals to the receiver to provide feedback data to the user;</div>
<div class="claim-text">wherein the first hand-held communication device receives and processes the received data signals and generates sensory stimuli for the user, based on the received data signals and delivered through the output device;</div>
<div class="claim-text">wherein the first hand-held communication device is further comprised of a user input device adapted for communication with the processing system through the transmitter; and</div>
<div class="claim-text">wherein the user input device is adapted for calibrating the first communication device to establish a reference position.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00049" num="00049" class="claim">
      <div class="claim-text">49. The system according to <claim-ref idref="CLM-00048">claim 48</claim-ref>,wherein:
<div class="claim-text">the processing system is adapted to determine movement information for each of the respective communication devices.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00050" num="00050" class="claim">
      <div class="claim-text">50. The system according to <claim-ref idref="CLM-00048">claim 48</claim-ref>,wherein:
<div class="claim-text">the first hand-held communication device is adapted to attach to a modular extension piece.</div>
</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES41842479" lang="EN" load-source="patent-office" class="description">
    <p num="p-0002">This application is a continuation of U.S. Ser. No. 11/935,578 filed 6 Nov. 2007, now U.S. Pat. No. 7,492,268, issued 17 Feb. 2009, which is in turn a continuation of U.S. Ser. No. 11/187,373 filed 22 Jul. 2005, now U.S. Pat. No. 7,292,151, issued 6 Nov. 2007, which claims the benefit of U.S. 60/592,092, filed 29 Jul. 2004. Each of these applications is incorporated by reference as if fully recited herein.</p>
    <heading>BACKGROUND OF THE ART AND SUMMARY OF THE INVENTION</heading> <p num="p-0003">This invention relates to a system and methods for setup and measuring the position and orientation (pose) of transponders. More specifically, for training the user to manipulate the pose of the transponders through a movement trajectory, while guided by interactive and sensory feedback means, for the purposes of functional movement assessment for exercise, and physical medicine and rehabilitation.</p>
    <p num="p-0004">Known are commercial tracking and display systems that employ either singularly, or a hybrid fusion thereof, mechanical, inertial, acoustical or electromagnetic radiation sensors to determine a mobile object's position and orientation, referred to collectively as pose.</p>
    <p num="p-0005">The various commercial tracking systems are broadly classified by their relative or absolute position tracking capability, in which system the pose of a mobile object is measured relative to a fixed coordinate system associated with either combination of receiver(s) or passive or active transmitter(s) housing mounted on the user. The tracking system's components may be tethered with obvious inherent movement restrictions, or use wireless communication means to remotely transmit and process the information and allow for greater mobility and range of movement.</p>
    <p num="p-0006">Typically these tracking systems are utilized for biomechanics and gait analysis, motion capture, or performance animation and require the sensors to be precisely mounted on the joints. Various means of presenting the tracking information in a visual display are employed, such as Heads-Up Display (HUD), that provide occluded or see-through visibility of the physical world, or Fixed-Surface Display (FSD), such as computer desktop monitors, depending upon the simulation and immersive quality required for the application. The application may require various degrees of aural, visual, and tactile simulation fidelity and construct direct or composite camera views of the augmented or three dimensional (3D) virtual reality environment to elicit interactive user locomotion and/or object manipulation to enhance the user's performance and perception therein. The tracked object may be represented in the virtual environment in various forms, i.e., as a fully articulated anthropoid or depicted as a less complex graphical primitive. The rendering strategy employed depends upon the degree of photo realism required with consideration to its computational cost and the application's proprioception requirements.</p>
    <p num="p-0007">Tracking technologies possess certain inherent strengths and limitations dependent upon technology, human factors, and environment that need consideration when discussing their performance metrics. Regardless of differentiating resolution and accuracy performance benchmarks, many implementations suffer from varying degrees of static and dynamic errors, including spatial distortion, jitter, stability, latency, or overshoot from prediction algorithms. Some human factors include perceptual stability and task performance transparency, which are more subjective in nature. And environmental issues such as line-of-sight, sensor attachment, range, and multiple-object recognition, need to be considered when selecting the optimal technology for the most robust application development. Irrespective of the intrinsic strengths and weaknesses of the tracking technology employed, ultimately the user's satisfaction with the system's utilization and efficacy, including the production of reliable, easily understood, measurable outcomes, will dictate the overall success of the device.</p>
    <p num="p-0008">This invention's system and methods facilitates biomechanical tracking and analysis of functional movement. In the preferred embodiment, this invention is low cost, robust, easy to deploy, noninvasive, unobtrusive, and conveys intuitive and succinct information to the user to execute movement properly and provides performance indicators of said movement for feedback purposes. One feature of the present invention provides for an interactive tracking system because the sensor functionality, or referred to herein as active transponders or transponders, is integrated with local user input control, and real-time sensory interfaces on the same device. The transponder is a wireless communication and monitoring device that receives a specific signal and automatically responds with a specific reply. In one embodiment, the invention provides functional movement assessment based upon the relative measures of limb pose with respect to two positions defined by the transponders. The transponders can operate independently or work in unison to process and share computational tasks and information between the local databases. This decentralized, distributed processing scheme allows the configuration and coordination of the training session, and processing and analysis of the measurements to occur without requiring expensive auxiliary computer and display systems to manage the same, and without relying on costly software development of complex synthetic environments for visualization purposes. Also, the user can manage the applications and performance databases off-line on a remote computer system with Internet connectivity to customize and configure the system parameters in advance of their session.</p>
    <p num="p-0009">The present invention is designed to provide such system and methods for high-fidelity tracking or registration of the poses of active transponders and engage the user to purposely manipulate the transponders' pose along a prescribed or choreographed movement trajectory in order to train and assess functional movement capability. In the preferred embodiment, the system is comprised of two subsystems: (1) a subsystem comprised of one or more active transponders, which, in its most sophisticated implementation, responds to periodic requests from another component of the system to radiate or transmit a signal for purposes of absolute position tracking; processes an embedded inertial sensor for relative orientation tracking and absolute tracking refinement; and provides an essentially real-time aural, visual, and tactile sensory interfaces to the user, and (2) a subsystem comprised of a centralized position processor system or unit and receiver constellation unit, collectively referred to as the processor unit, which is essentially a signal processor that synchronizes the transponders' periodicity of radiating signal and other operational states; collectively receives and processes the radiated signal; iteratively calculates the transponders instantaneous pose and convolution, thereof; and continually exchanges this information, and its analysis thereof, with the transponders and/or auxiliary host computer system in essentially real-time via a combined wireless and tethered communication means. This real-time bi-directional exchange of information allows for proper transponder identification, coordination, and the accurate measurement of pose, thereof, and timely actuation of the sensory interfaces for optimal user regulated closed-loop control.</p>
    <p num="p-0010">The transponder is broadly classified by its level of hardware and software configuration that define its scope of intelligence, sensory support, and configuration. The degree of intelligence is determined by its capability to locally access, process, and modify the database. Further, either transponder classification can be sub-classified by its manipulative requirements. In one embodiment, where multiple transponders are used, a principle transponder is consciously and deliberately moved along the reference movement trajectory, while a subordinate transponder serves as an anchor or secondary reference point elsewhere on the locomotion system whose kinematics are not necessarily controlled by the user's volition.</p>
    <p num="p-0011">An interactive transponder, preferably, has significant intelligence; supports relative and absolute tracking capabilities; provides complete sensory stimuli support; provides for functional enhancement through attachment of modular, extension pieces; and provides a user display and input system to control the training session. In the preferred embodiment, the interactive transponder is primarily held in the hand to facilitate more complex user input and greater sensory intimacy. Conversely, in another embodiment, the fixed transponder has limited intelligence; supports only the absolute pose tracking capability; provides no sensory stimuli support; and is usually mounted to a fixed site on the limb or trunk.</p>
    <p num="p-0012">A combination of transponder deployment strategies may be required depending on the training session's objectives, such as two interactive transponders grasped by each hand; or alternatively, an interactive transponder, and a fixed transponder attached to the limb or trunk; or lastly, two fixed transponders attached to the limb(s) and/or trunk.</p>
    <p num="p-0013">In one embodiment, this invention proposes to elicit movement strategies based on the deployment of at least two transponders that define the endpoints of a movement vector whose relative translation and rotation is measured and evaluated for the assessment of functional movement capability, including but not limited to, limb range of motion and its control thereof, limb strength conditioning, and overall proprioception and hand-eye coordination skills, and overall body movement. This registration system measures a single movement vector whose endpoints are comprised of an anchor point, i.e. one that is located in a less dynamic frame of reference, e.g., such as the trunk or abdomen, and another more distal location fixed on or held by a limb or extremity, e.g., the hand, arm, or leg. As this movement vector is translated and rotated through space by the act of the user modifying the pose of the principle transponder in concert with the reference movement trajectory, the vector's length will expand and contract relative to the proximity of principle transponder with respect to the subordinate transponder. The vector's length conveys unique and explicit information regarding the user's movement efficiency and biomechanical leverage. For example, by attaching a fixed subordinate transponder at the hips and a fixed principle transponder on the upper arm, the biomechanics of the act of lifting a box or similar object can be elegantly qualified. If the user assumes a poor lifting technique, i.e. legs locked with the trunk severely flexed with head down and the arms stretched out beyond the basis of support, the vector's length would consistently be measured longer than compared to a good lifting technique, i.e., legs bent at knees with the back straight, head gaze up, and arms close to body. Also, the measurement(s) of higher-order derivatives derived from numerical mathematical processes of a reference point described by the vector would provide additional indication of movement control or smoothness. In summary, one embodiment of the present invention is comprised of:</p>
    <p num="p-0014">a means to create a single movement vector whose endpoints are defined by the locations of at least two transponders, wherein, the expansion and contraction of the vector's length is calculated, analyzed, and reported in essentially real-time;</p>
    <p num="p-0015">a means to create a single movement vector whose endpoints are defined by the locations of two transponders, wherein, a representative point along the vector length is referenced and its higher-order derivatives are computed by mathematical numerical processes, wherein the result is calculated, analyzed, and reported in essentially real-time; and,</p>
    <p num="p-0016">a means to correlate said vector's length and at least one other measure consisting of a higher-order derivative, to the reference movement trajectory, wherein the result is calculated, analyzed, and reported in essentially real-time.</p>
    <p num="p-0017">A registration system for practical functional movement applications should clearly convey information to the user regarding his movement quality while he performs the task, without compromising or distracting from said execution by unnecessary head movements or change in eye gaze and normal focus. Poor visualization strategies that distract the user are ineffectual for promoting heads-up, immersive interaction, and the alphanumerical information it imparts often can not be consciously processed fast enough to elicit corrective action. This system provides for both a local, standalone sensory interface as a primary feedback aid, or alternatively, an interface to a remote fixed-surface display for greater visualization and simulation capabilities. The visual stimulus could be modulated to warn of range violations, or provide signals for purposes of movement cadence and directional cueing. A principle interactive transponder is typically hand-held, which is naturally in close proximity to the user's aural and visual sensory field during most upper extremity movements, or, conversely, the visual stimulus may be viewed through a mirrored or reflective means if not in optimal line-of-sight. A remote fixed-surface display might augment the immersive quality of the user's experience by providing control of a view camera of a simulated computer environment, and display of the transponders and/or interactive objects' static or dynamic poses within the computer display's skewed through-the-window perspective projection.</p>
    <p num="p-0018">In summary, one embodiment of the present invention is comprised of:</p>
    <p num="p-0019">a means for modulating an embedded luminescent display organized and oriented into a directional-aiding pattern, by varying its degree of intensity and color, or other physical characteristics, to provide a visual display stimulus. This sensory interface is excited at a rate, repetition, or pattern proportional to the pose error of the transponders' movement trajectory compared to the reference movement trajectory;</p>
    <p num="p-0020">a means to view said visual display stimulus with the aid of a mirror(s) or other reflective means;</p>
    <p num="p-0021">a means for the real-time projection of sound or speech commands through an audio device to provide warning, alarm, instructional, and motivational aid, and/or additional cueing upon encroachment of static and dynamic limit/boundary conditions defined by the reference movement trajectory;</p>
    <p num="p-0022">a means for real-time tactile feedback including, but not limited to, modulation of the rotational properties of a vibrator motor proportional to the pose error of the transponders' movement vector compared to the reference movement trajectory;</p>
    <p num="p-0023">a means for combining the excitation of said stimuli proportional to the pose error of the transponders' movement vector compared to the reference movement trajectory; and,</p>
    <p num="p-0024">a means to coordinate the real-time, periodic parametric update and modulation of the stimuli imparted by the sensory interfaces within the transponders from a processing unit by means of a wireless communication link.</p>
    <p num="p-0025">This invention addresses the need for an intuitive, interactive method to instruct, create, and deliver a movement trajectory command without necessarily relying on pre-programmed, regimented movement trajectories. The registration system can be configured via remote setup at the principle transponder to pre-record and choreograph a free-form movement trajectory of the principle transponder with the intent of the user mimicking the same said path. This impromptu learning modality can expedite the session down time between different users and movement scenarios, and accommodate users' high anthropometric variability in range of movement. In summary, one embodiment of the present invention is comprised of:</p>
    <p num="p-0026">a means is to provide a movement trajectory learning modality that allows the user to calibrate and create the desired endpoints, midpoints, and/or total reference movement trajectory through user programmer entry of an input device resident on the transponder;</p>
    <p num="p-0027">a means to process and save a movement trajectory using a computationally efficient Catmull-Rom spline algorithm or other similar path optimizing algorithms to create control points along key points of the movement trajectory that define the optimally smoothest path intersecting the control points;</p>
    <p num="p-0028">a means to provide database management by a processing unit via a wireless communication link or, alternatively, through user data entry of an input device resident on the interactive transponder; and,</p>
    <p num="p-0029">a means to access, edit, and store the program and/or databases to nonvolatile memory operably coupled to the principle transponders for the purpose of automating the creation, delivery, storage, and processing of movement trajectories. Customized user programs and databases would be downloaded from a central repository or relevant website in advance of the training session to the transponder from the user's home location via the Internet or other convenient locales having networked Internet access, and transported to the systems remote physical location, and uploaded into the system's memory, and executed as the application program.</p>
    <p num="p-0030">This a priori process of remote selection, download, and transfer of programmatic content and database would minimize the user's decision making and input during product utilization by offering only relevant and customized programming material of their choosing targeted for their specific exercise, fitness, or rehabilitation goals. Performance data could be saved indefinitely in the database's nonvolatile memory, until an upload process was performed through the said network so the database could be transferred to another location for purposes of, but not limited to, registration, processing, archival, and normative performance evaluation, etc.</p>
    <p num="p-0031">An exemplary list of specific data structures contributing to or affecting the means for automating the creation, delivery, storage, and processing of movement trajectories described below may be stored within the non-volatile memory of the transponder or position processor which may use high-density serial FLASH, although other types of memory may be used such as SmartMedia, Compact Flash, etc. Additionally, the memory device interface should not be limited to internal, but may include external media devices, such as USB FLASH Key or other portable media means, that may have inter-operability with other computerized devices. The data structures may include:</p>
    <p num="p-0032">Modulation &amp; Feedback Thresholds/Triggers Properties—the aural, visual, tactile interfaces require threshold settings which determine their excitation or stimulation characteristics. These settings can be derived from previous performance data or defaults determined from normative data, or modified in real-time, by algorithmic methods including moving averages, standard deviations, interpolation based upon goal-oriented objectives, etc.</p>
    <p num="p-0033">Normative Performance—performance data collected over a large population of users through controlled studies, that is distilled down into specific user categories based upon certain demographics that the user may compare and rank his/her results. This data may be initially embedded within the transponders or position processor non-volatile memory and may be augmented or modified automatically or by user volition when connected to the Internet.</p>
    <p num="p-0034">Competitive Ranking—applications which have a predominate point goal-oriented purpose would allow access to a global ranking file archive accessed through the Internet or automatically via updated executive files. This ranking file would be created through an analysis of user participation and publishing of his/her results through Internet Web-based services.</p>
    <p num="p-0035">Downloadable Executive Programs &amp; Configurations—new software programs, including new features, enhancements, bug fixes, adjustments, etc., could be downloaded to the transponder through an Internet connection. Graphics images would be stored in compressed or uncompressed binary forms, i.e., bitmap, gif, jpeg, etc. This new programs could be transferred to any suitable computerized position processor unit located at a remote facility via the transponder's wireless link. Therefore, the user's transponder is the node that establishes the portable network capabilities of the system, not necessarily the computerized position processor.</p>
    <p num="p-0036">Custom Menu Interfaces—specialized activities may require more advanced (or simplified) interfaces dependent upon the users' cognitive abilities and interactive specificity. This menu may include interactive queries or solicit information regarding the user's daily goals, subjective opinions or overall impression of the activity and ones performance which could be incorporated in the Motivation Index described below.</p>
    <p num="p-0037">Report Generation Tools and Templates—XML, HTML or other authoring language used to create documents on the Web that would provide an interactive browser-based user interface to access additional performance data analysis and report generation tools and templates that may not be available or offered with the standard product.</p>
    <p num="p-0038">Custom Performance Algorithms—certain application-specific performance analysis may require dynamically linked algorithms that process and calculate non-standard or specialized information, values, units, physical measurements, statistical results, predictive behaviors, filtering, numerical analysis including differentiation and integration, convolution and correlation, linear algebraic matrices operations to compute data pose and scaling transformation, and proprietary types. One example of a proprietary type is Motivation Index, a composite numerical value derived from a weighted average of statistical performance indicators and subjective user input including relative scoring improvements, conformity to ROM pattern, lengthy activity access duration, high access rate, relative skill level improvement, daily goal achievement, etc., that could represent the overall level of enthusiasm and satisfaction, the user has for a particular activity.</p>
    <p num="p-0039">Range of Motion (ROM) Pattern Generator—the ROM pattern requires some key control points to be captured along the desired trajectory and stored in order that the algorithm can calculate an optimally smooth path, in real-time, during the comparative analysis phase.</p>
    <p num="p-0040">ROM Pattern Capture &amp; Replay—the ROM pattern can be can saved to memory in real-time by discrete position samples versus time depending upon the resolution desired and memory limitations and later played back on the transponder or remote display for analysis.</p>
    <p num="p-0041">Activity Specific Attributes—includes Reps/Sets, Duration, Pause, Heart Rate Limits, intra-activity delay, level, point scalars, energy expenditure, task-oriented triggers, etc., and other parametric data that controls intensity, execution rate and scoring criteria for the activity.</p>
    <p num="p-0042">Instructional Information—textual, graphical, or animation-based instruction, advice, coaching, activity description, diagramed transponder deployment and intra-device connectivity, etc. that facilitates the intuitiveness, understanding, and usage of the system. The form of instruction may include music files saved in various formats, including Wave, MP3 or other current or future audio data compression formats, and video files saved in MPEG or other current or future video data compression formats.</p>
    <p num="p-0043">Real-time Data Management—proprietary data management protocols that reside above the communication driver layer that manage the real-time, synchronous and asynchronous exchange of data between transponder(s) and position processor. This would provide an essential real-time sharing of activity data, analysis, and feedback stimulus thresholds, or coordination of multiple transponder configurations, or for a collaboration of same or different user requirements to complete a similar activity objective.</p>
    <p num="p-0044">This invention addresses the need for adaptability of the registration system to different movement measurement scenarios. In one embodiment, it utilizes a versatile, modular configuration and mounting of the transponders onto the user. The efficient deployment of the transducers between different users' and from task to task requires a universal mounting scheme to provide consistent localization and pose of the transponders at the desired measurement sites on user's body. Also, to compensate for the receivers' finite tracking volume when stationary, the receiver constellation unit may be mechanically modified to optimize its tracking properties by conveniently repositioning it in closer proximity to the expected transponders movement trajectories and line-of-sight, thereof. In summary, one embodiment of the present invention is comprised of:</p>
    <p num="p-0045">a means to quickly and efficiently alter the location of the transponders using a fastening system designed to quickly attach and dispose various forms of transponder assemblies;</p>
    <p num="p-0046">a means to augment the physical properties, i.e., weight and length, of the principle transponder with adjunct electromechanical components that provide variations in biomechanical leverage for isotonic and isometric utilization; and,</p>
    <p num="p-0047">a means to allow the user to manually alter the geometry and pose of the receiver constellation unit to facilitate an optimal tracking location based upon collectively maximizing the ultrasonic source's energy received at the transducer interface.</p>
    <p num="p-0048">This invention addresses the practicality and robustness of the registration system when used in either indoor or outdoor environments, and especially when the tracking volume likely contains potentially occluding objects, i.e., an uninvolved limbs or clothing, that become potential sources of competing, reflected paths. The preferred embodiment of the registration system utilizes the time of flight (TOF) measurement of ultrasonic acoustic waves due to its immunity from interference from the visible and near-visible electromagnetic spectrum and its superior ability to overcome most multi-path reflections problems by simple gated timing of the initial wave front. Upon command from the processor unit, the transponders produce a few cycles burst of ultrasonic energy and the transducers of the receiver constellation unit are stimulated and mechanically resonate accordingly, upon the wave front arrival. The processor unit's analog signal processing circuits transform the mechanical energy into electrical signals that resemble tapered sinusoidal waveforms, which another electronic circuit triggers upon using an adaptive threshold technique which, in turn, the processor unit detects and calculates TOF timestamps indicating the wave front arrival. In the preferred embodiment, the system overcomes the ultrasonic technology's intrinsic challenge of precisely triggering on same the waveform location and provides consistent unambiguous trigger detection by complementing the adaptive threshold technique with a software timestamp correction algorithm, which includes in part, a digital over-sampling and averaging timestamp algorithm, a relative timestamp correction scheme utilizing a predictive algorithm of higher-order Taylor series based derivatives, and an absolute timestamp correction scheme that minimizes the range error based upon discrete biasing of timestamps.</p>
    <p num="p-0049">Further, in the preferred embodiment, the processor unit utilizes the absolute and relative trigger timestamps in a multi-modal trilateration algorithm for the measurement of three-dimensional (3D) translations and rotations of the transponders. The primary trilateration calculation is derived by an application of Pythagorean theorem involving a point position solution based-upon range measurements from at least three (3) points, versus the well-known triangulation method which uses bearing angles of two cameras of known pose. Additionally, the system's main accuracy limitation is mostly affected by the temperature variability of outdoor environments and its influence on the speed of sound in air value. This algorithm mitigates this problem by mathematically computing the speed of sound every analysis period provided at least five (5) receivers and a transponder synchronizing means are utilized. If the integrity of the synchronizing signal is temporarily compromised, the system automatically employs a variation of the trilateration algorithm that uses the last known speed of sound value.</p>
    <p num="p-0050">In the preferred embodiment, the maximum update rate, and hence the major contributor to the latency of the position calculation, is determined by the typical acoustical reverberation, typically between 20 to 100 ms, encountered in an indoor environment. Since the transponders are held or fixed on the user's body and, therefore, are mobile, the TOF measurements will experience an additional latency effect. A Kalman filter is used as a prediction/estimation strategy to minimize and compensate for the latency effect. The prediction algorithm uses a higher-order Taylor series based derivatives and augmentative inertial sensor data. Its predictive refinement is dependent upon predefined models of expected movement conditions. Because functional movement is episodic, having periods of stillness interspersed with bursts of motion activity, a multi-modal filtering strategy is preferably employed to handle the unpredictable jerkiness at the start of motion and relatively predictable, smooth motion afterwards. In summary, the preferred embodiment of the present invention is comprised of:</p>
    <p num="p-0051">a means to detect the same carrier wave cycle of ultrasonic energy using a software correction algorithm requiring multiple, consecutive TOF acquisitions as input for the digital over-sampling and averaging algorithm, the calculation of a higher-order numerical differentiation of the past and current TOF information as input for the predictive algorithm of higher-order Taylor series based derivatives used for the relative TOF correction, and a measurement of the intra-pulse time intervals of consecutive TOF acquisitions as input for the absolute TOF correction scheme that minimizes the range error based upon selective biasing of the TOFs;</p>
    <p num="p-0052">a means to utilize a dual matrix formulation of the trilateration algorithm, and a calculation strategy thereof, which decision is dependent upon the integrity of the system's communication link, synchronization condition, and the desired measurement accuracy; and,</p>
    <p num="p-0053">a means to coordinate the information transfer between transponders and the processor unit so that their contribution to the resultant movement vector calculation can be measured without intra-signal interference.</p>
    <p num="p-0054">These goals will be attained by such system and methods that are comprised of the user's interaction described by the following steps as set forth as the preferred embodiment:
</p> <ul> <li id="ul0001-0001" num="0000"> <ul> <li id="ul0002-0001" num="0054">a. Authenticate user access and open user session from a local or remote database;</li> <li id="ul0002-0002" num="0055">b. Setup user training session, i.e., workload limitations, measurement criteria, and audio/visual/tactile stimuli;</li> <li id="ul0002-0003" num="0056">c. Select training program and configure its options;</li> <li id="ul0002-0004" num="0057">d. Deploy the transponders as instructed to predefined locations of users locomotion system to create at least one transponder movement vector;</li> <li id="ul0002-0005" num="0058">e. Calibrate the transponder movement vector to establish its reference pose;</li> <li id="ul0002-0006" num="0059">f. Create a movement trajectory using learn mode, if required;</li> <li id="ul0002-0007" num="0060">g. Initiate the start of session;</li> <li id="ul0002-0008" num="0061">h. Determine the instantaneous pose of transponder movement vector relative to its reference pose from a periodic temporal iteration of this step;</li> <li id="ul0002-0009" num="0062">i. Perform qualitative and quantitative statistical analysis of accumulated measured poses of the transponder movement vector relative to the pattern of instantaneous poses defined by the reference movement trajectory;</li> <li id="ul0002-0010" num="0063">j. Update the major transponders sensory interfaces to modulate said system parameters in a periodic temporal iteration of this step;</li> <li id="ul0002-0011" num="0064">k. End the session once program objectives have been obtained;</li> <li id="ul0002-0012" num="0065">l. Analyze the results by interacting with local and/or remote databases;</li> <li id="ul0002-0013" num="0066">m. Provide numerical, graphical, and/or animated information indicating desired performance measurements.</li> </ul> </li> </ul> <description-of-drawings> <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p num="p-0055">The disclosed embodiments will be better understood when reference is made to the accompanying drawings, wherein identical parts are identified with identical reference numbers and wherein:</p>
      <p num="p-0056"> <figref idrefs="DRAWINGS">FIG. 1A</figref> illustrates one example of a deployment apparatus of the present invention;</p>
      <p num="p-0057"> <figref idrefs="DRAWINGS">FIG. 1B</figref> illustrates one example of hand-held form for the transponder of the present invention;</p>
      <p num="p-0058"> <figref idrefs="DRAWINGS">FIGS. 2A-2D</figref> illustrate example extension pieces for the present invention;</p>
      <p num="p-0059"> <figref idrefs="DRAWINGS">FIGS. 3A-3D</figref> illustrate one example of process flows for the present invention;</p>
      <p num="p-0060"> <figref idrefs="DRAWINGS">FIGS. 4A and 4B</figref> illustrate a sample application of the present invention;</p>
      <p num="p-0061"> <figref idrefs="DRAWINGS">FIG. 5</figref> illustrates a block diagram of the remote processing system of the present invention;</p>
      <p num="p-0062"> <figref idrefs="DRAWINGS">FIGS. 6A-6C</figref> illustrate example receiver configurations of the present invention; and</p>
      <p num="p-0063"> <figref idrefs="DRAWINGS">FIG. 7</figref> illustrates a block diagram of the components of one embodiment of the transponder of the present invention.</p>
    </description-of-drawings> <heading>DETAILED DESCRIPTION OF THE EMBODIMENTS</heading> <p num="p-0064">The present invention provides a practical, versatile measurement tool for the assessment of the user's manipulation strategy of the transponder <b>10</b> or transponders along a reference movement trajectory. Moreover, the system and methods measure and analyze the kinematics of the relative translations and rotations of the limbs or extremities with respect to each other or to a more inertial reference location on or off body as the transponders are manipulated. This information provides useful insight on biomechanical demands and anthropometric factors that influence human movement efficiency and control. Although measurement performance metrics are important design criteria, it's equally important to provide intuitive and motivating program instruction and administration, and to provide comprehensive analysis and integration of the motion data in a form that is objective and easily interpreted. This system improves upon the practicality and user interactive aspects of setup, deployment, calibration, execution, feedback, and data interpretation of a tracking system designed for function human movement.</p>
    <p num="p-0065">Human movement is a response to external environmental forces which requires the accurate coordination of the distal segment(s) to compensate for these forces. Skillful coordination of human movement is dependent upon the cohesive interaction of multiple sensory systems, including visual, vestibular, with the musculoskeletal system. More specifically, the challenges and goals of cognitive spatial mapping, (2) minimization of energy expenditure, (3) maintaining stability, (4) steering and accommodation strategies for various environments, (5) dynamic equilibrium, (6) active propulsion and weight support, and (7) core locomotion pattern should be relationally considered to properly assess human movement. Therefore, it is preferable to engage the interaction of these sensory systems during a training session to promote the desired functional movement outcome. Because many movements persist for 400-500 ms, enough time is allowed for the initiation of the movement and for user correction based upon visual and kinesthetic information acquired during the time of the movement. However, the implemented means of visual feedback should be not be distracting or interfering with the task at hand. In the preferred embodiment, this system engages the sensory systems with non-distracting, intuitive, embedded aural, visual, and tactile stimuli which provide real-time indication of the principle transponder pose error with respect to the reference movement trajectory.</p>
    <p num="p-0066">In order to conduct a time efficient training session, this registration system attempts to minimize the encumbering experimental setup and calibration procedures characteristic of more complex and higher cost motion analysis technology. These complementary systems serve important academic or clinical oriented research needs or for motion capture for computer animation purposes and strive for highly accurate measurement of joint motion data in terms of angular displacement. Therefore, the integrity and reliability of their motion data is dependent upon proper sensor setup and calibration.</p>
    <p num="p-0067">For instance, single axis goniometer-based systems usually require specially designed harnesses to hold the monitor and are firmly strapped or taped over the joint to avoid relative motion artifacts. Usually these devices are tethered and their fit, weight, and constraining mechanical linkages can impose limitations on the joint motion and cause discomfort for the user. Most optical or video-based systems require the placement of numerous active or passive markers over landmarks, such as the joints' center of rotation. These systems should guarantee sufficient environmental illumination and contrast between markers and background to function optimally. Also, these systems are severely affected by occluded markers that may disappear for long periods of time due to rotations and line-of-sight limitations. Other video-based systems do not use markers but require the assignment of the body's joints manually or through computerized automation during data analysis, making real-time analysis arduous and real-time feedback virtually impossible.</p>
    <p num="p-0068">In the preferred embodiment, the system doesn't require complicated, time consuming sensor setup and calibration by virtue of it minimalist sensor requirements and uncomplicated sensor mounting. Instead, it requires only the deployment of a sensor on the body (in one embodiment a dual sensor group on a combination of limb(s) and or trunk) and doesn't enforce stringent movement protocol, but encourages free-form, unrestrictive movement of the transponders.</p>
    <p num="p-0069">The transponder's preferred deployment means, include either insertion into a universal strap and holster apparatus (<figref idrefs="DRAWINGS">FIG. 1A</figref>) that secures on the user's limb, extremity, or trunk, including, but not limited to, the hip, ankle, knee, wrist, upper arm, neck, waist or an augmentative mechanical attachment to one or a combination of modular extension pieces shaped into a hand-held form (<figref idrefs="DRAWINGS">FIG. 1B</figref>). A strap or torx-like clip and holster design provides a firm, yet light weight and comfortable mounting location away from areas that clothing and or uninvolved limbs may occlude.</p>
    <p num="p-0070">The modular extension piece is either an instrumented sensory type designed to support alternative tactile stimulus device or alternative configurations of aural, visual, and tactile feedback types, or non-instrumented, weighted extension pieces as shown in <figref idrefs="DRAWINGS">FIGS. 2A-2D</figref>. All modular extension pieces may be of various physical dimensions and intrinsic weight, with a captive handle design that preferably requires zero grip strength to grasp. Alternatively, the modular extension piece may be coupled to the transponder through a fixed or flexible, segmented, articulated coupling to accommodate attachment of additional transponders and/or other modular extension pieces. These components would quickly assemble to each other using a spherical snap joint or twist snap latch, or similar mechanism, to provide quick alteration of form and function when used for different movement trajectory scenarios.</p>
    <p num="p-0071">In one embodiment, the weighted extension attachments (<figref idrefs="DRAWINGS">FIG. 2A</figref>) are offered in fixed gradations of one (1) kilogram increments or other convenient unit of measure and either be indicated as such with a numerical label, quantitative mark, or color-code feature, or combination thereof. For upper extremity evaluation, the weighted extension piece integrated into a zero-grip handle would enhance the improvement of musculature strength of the limb, while not compromising the user's endurance with a potentially fatiguing hand grasp requirement.</p>
    <p num="p-0072">In one embodiment (<figref idrefs="DRAWINGS">FIG. 2B</figref>), the tactile type provides force feedback functionality by controlling the rotational speed of an embedded vibrator motor in the shaft. Alternatively, the visual type (<figref idrefs="DRAWINGS">FIG. 2C</figref>) may be comprised of a series of light emitting diodes that could be uniformly embedded along the length of the handle or transponder and their intensities variably controlled therein. It should be appreciated that a simple, economical mirrored or reflective surface placed in front of the user's visual field could provide sufficient real-time indication of the user's subjective conformity to the said movement trajectory while allowing non-distracting viewing of this visual stimulus. For example, a program that requires the user to reposition the principle interactive transponder through an arc-like movement trajectory in the midsagittal plane through out a range of motion beginning from the waist upwards until parallel to shoulder height. As the user performs the movement, the visual sensory interface could be proportionally excited if the user moves too quickly, or hesitates too long, or produces shaky or erratic episodic motions, or is beyond the prescribed bounds of the movement arc. The light stimulus is easily viewed in the mirror and would indicate corrective action in his or her movement strategy, while appropriate aural commands may be issued simultaneously to encourage the same correction. Regardless of the sensory interface type, its control and excitation properties will be determined by some statistical aspect of the user's conformity to and progression through the movement trajectory.</p>
    <p num="p-0073">The hand-held transponder may include a modular extension piece with an embedded graphic display device and associated input means to allow the user to setup, operate, provide visual feedback, and view performance results of the device usage without additional remote display means. More specifically, a software-controlled user interface could provide certain visual prompts in a menu oriented presentation, to instruct the user on (1) device setup, i.e., aural, visual, and tactile feedback parameters, types of program start and termination cues, program intensity based on ratio of amount of repetitions, sets, and rest periods or categorical gradation of challenge, learn mode behavior, etc., (2) scrollable program selection with brief descriptions including objective, desired measurement, i.e., range of motion, energy, accuracy, speed, etc., and instructive information, and (3) alphanumeric and/or graphical display of measured performance data and other biophysical data and its analysis thereof, displayed in standard plotted forms including line, bar, and pie charts, etc. It is important to note that the user input process is intuitive and streamlined so as not to detract from the practicality and user friendliness of the system. Only relevant applications and its control thereof will be sequestered from the database and presented to the user.</p>
    <p num="p-0074">In one embodiment, two or more transponders and extension pieces, or combinations thereof, may be assembled at their endpoints with a universal spring coupling. The assembled device could be grasped in both hands and bent in various rotational angles about the spring coupling's axis. Isotonic strength conditioning programs can be developed due to the force resistance feedback supplied by the spring. A multi-transponder assembly in the form of a flexible rod or staff could provide an indication of balance of upper extremity strength and proprioceptive function dependent upon the angular closure rate and rotational imbalance and orientation deviation from initial starting position.</p>
    <p num="p-0075">Additionally, in the preferred embodiment, the modular extension pieces have provisions for other attachable apparatus (<figref idrefs="DRAWINGS">FIG. 2D</figref>) that can augment the program's intensity or difficulty. For example, an eyelet is embedded in the end of the extension piece and is designed to attach an elastomeric band, such as the type manufactured by Theraband®. By securing the other open stirrup end of the band to the user's foot, isotonic strength conditioning programs can be developed due to the force resistance feedback supplied by the elastomeric band. Moving the transponder through a movement trajectory is now made more restrictive and challenging.</p>
    <heading>APPLICATION EXAMPLES</heading> <p num="p-0076">An example training session deploying a dual transponder group is now described that may be designed to improve the range of motion, strength, and coordination of shoulder abduction in a user. The training session would primarily serve as an exercise aid that provides essential feedback to the user so that he/she learns to progressively improve the manipulation of the transponder through the reference movement trajectory, while benefiting from increased shoulder range of motion and strength improvement.</p>
    <p num="p-0077">In advance of the training session, a software application is operated from a host computer that provides a utility for baseline configuration and management of the system's and transponder's local databases, and/or access to other remote databases, and for the real-time interface to the data flow between the system's components. The application's navigation and selections are presented to the user through a typical graphical user interface like Microsoft Windows® XP operating system. A generalized step-wise procedure requires the administrator or user to (1) select the desired program and features from a menu screen list, and (2) to initiate a communication process that causes the program parameters to be transferred to the processor unit through a standard computer communication protocol, i.e. serial, USB, ethernet, etc., whereupon, (3) the information is subsequently processed and transferred into the transponders local memory via a wireless communication link, and, finally, (4) the transponder's software program accesses this database to manage the device utilization and configuration of the local display means. Alternatively, a Compact FLASH-based memory card, embedded serial FLASH, or a similar nonvolatile memory device provides the user an additional specialized database supporting remote data collection capabilities. This database would be preprogrammed in advance and the resultant performance data retained, even if the device's power is lost, or for extended unsupervised exercise sessions conducted remotely from the host computer system or when the host computer system is unattached or unavailable. After the session is completed, the user would be queried if the results are to be saved for later analysis or would automatically be saved, dependent upon device setup. This data could be retrieved at a later time when the system is once again attached to a host computer system, and the software utility could be commanded to upload the database.</p>
    <p num="p-0078">Henceforth, the following procedural description refers to the activity dependencies diagrammed in <figref idrefs="DRAWINGS">FIGS. 3A-3D</figref> that the user would encounter while operating the system.</p>
    <p num="p-0079">During the Security Phase (<figref idrefs="DRAWINGS">FIG. 3A</figref>), the user may be requested to provide a security authentication code for validation, which opens access to his/hers custom programs in the training session. Next, during the Setup Phase (<figref idrefs="DRAWINGS">FIG. 3A</figref>), the user can configure global options or select the desired program. The global options may include, but are not limited to, workload intensity, measurement criteria, sensory interface properties, and reporting features. A program menu list would indicate name, ID, and a brief description, or alternatively, be represented by a detailed graphical icon. When the program is selected, other program-specific options can be setup.</p>
    <p num="p-0080">During the Deployment Phase (<figref idrefs="DRAWINGS">FIG. 3B</figref>), and dependent upon the program's objectives, a suitable combination of transponder types will be mounted on the user's body as instructed by the program. This example requires the assembly of a hand-held interactive transponder with graphical display, and a weighted extension piece coupled therein to be grasped by the hand on the same side as the affected shoulder. Another subordinate transponder <b>12</b> is placed into a holster assembly strapped around the lower quadriceps on the same side. This setup is shown in <figref idrefs="DRAWINGS">FIG. 4</figref>.</p>
    <p num="p-0081">During the Calibration Phase (<figref idrefs="DRAWINGS">FIG. 3C</figref>), a simple calibration procedure may be requested to evaluate transponder function and specific user range of motion constraints. Typically, this information is determined beforehand and saved in the system's database. Also, practicality of this system is claimed for lack of extensive calibration requirements.</p>
    <p num="p-0082">Dependent upon the program's options, a user-defined movement trajectory may be created prior to program start in lieu of executing the predefined version. The learn mode could be utilized to quickly choreograph free-form movement trajectories and save them into the transponder's non-volatile memory for later execution. The learn mode would be accessed through the user interface and instruct the management of the control point assignment by pressing the push button switch at the appropriate junctures of movement discontinuity or, preferably, allowing automated assignment by the software. In the preferred embodiment, a computationally efficient Catmull-Rom spline is used to define a three dimensional (3D) curve that passes through all the control points along the movement trajectory path. If manually interceding, the user is instructed to press the push button once at each major juncture in the movement trajectory, but, preferably, for no more than a few locations, until the desired end of range of motion is reached as shown in <figref idrefs="DRAWINGS">FIG. 4B</figref>. Similarly, the return path may be similarly defined or he/she may elect to use the same forward path in reverse. These control points are registered by the processor unit and transferred and saved to the transponders' memory to serve as the control points for the real-time calculation of a Catmull-Rom spline. The Catmull-Rom spline is calculated in real-time from the desired initial starting point to provide a continuous set of position points representing the “learned” reference movement trajectory.</p>
    <p num="p-0083">After the program is selected or the learn mode complete, the user may be instructed to alter the pose of the transponders to satisfy the initial starting conditions of the program. Either one or a combination of sensory interfaces could be excited by the principle transponder to cause the user to direct or steer it towards the desired start point. For instance, the visual sensory interface could sequentially extinguish or dim its peripheral light sources to converge to a central light source as the principle transponder is positioned closer to the desired start point. Alternatively, the aural sensory interface could change its tonality and loudness as the start point is approached. Or alternatively, the tactile sensory interface could be modulated to provide less force feedback as the start point is approached.</p>
    <p num="p-0084">During the Execution Phase (<figref idrefs="DRAWINGS">FIG. 3D</figref>), the transponders are continually manipulated along the reference movement trajectory to the best of the user's skill and fidelity, within the bounds of the user's physical limitation, until an aural, visual, or tactile response is given that indicates the activity volume has been successfully completed or a sufficient number of conformity violations or failures have been registered. The processor unit calculates the instantaneous pose coordinates of the transponders every analysis interval and periodically communicates this information to the transponders via the wireless communication link. As the principle transponder is moved in mimicry to the reference movement trajectory the conformity error between the actual and reference movement trajectory is calculated periodically in real-time to determine the characteristics of feedback quality to be elicited by the sensory interfaces for the user's closed-loop control to correct his/her manipulation strategy. For example, the conformity error may be calculated from statistical processes based upon the standard deviation of the least mean squared (LSM) principle transponder's position error compared to the reference movement trajectory, or based upon, or combination thereof, a threshold magnitude of some multi-order numerical differentiation of said movement to indicate a “smoothness” quality of translation and rotation along the movement trajectory path.</p>
    <p num="p-0085">Alternatively, a host computer system could provide an auxiliary processing and display means to allow another software program to access the transponder's calculated positional data through an application programmer's interface and use this data to alter the pose of a graphical primitive in proportion to the motions of the transponders within the context of computer generated virtual environment. The dynamic control of objects in the computer generated virtual environment could be used to augment the local sensory interfaces of the transponders through an interactive, goal-oriented video game modality. The video game challenges could be increased over time based upon some scoring criteria of successful manipulation of the principally controlled on-screen graphical object with respect to cueing derived from other secondary static or dynamically moving objects. It is important to note that only primitive forms of video game challenges would be considered, to take into account the user's cognitive awareness and physical limitations, and the economics of software development for photo realistic virtual environments and animation. Also, this auxiliary computer display means would offer an alternative visualization method of interactive and immersive video feedback aid to enhance the application presentation.</p>
    <p num="p-0086">Additional examples of how the present invention may be applied are described as follows:</p>
    <p num="h-0005">Balance</p>
    <p num="p-0087">The body has the ability to maintain balance under both static and dynamic conditions. In static conditions, such as in standing, the body strives to efficiently maintain posture (often referred to as postural stability) with minimal movement. In dynamic conditions such as in walking or sports play, the body strives to maintain balance while performing in an ever changing environment. The ability to maintain balance is a complex process that depends on three major sensory components. The sensory systems include visual, vestibular and proprioception. For example, we rely on our visual system (eyes) to tell us if the environment around us is moving or still; we rely on our vestibular system (inner ears) to tell us if we are upright or leaning, standing still or moving; and we rely on our proprioceptive system (feet and joints) to tell us if the surface we are standing on is uneven or moving. If balance problems develop, they can cause profound disruptions in your daily life. In addition to increased risk for falls, balance disorders can shorten attention span, disrupt normal sleep patterns, cause excessive fatigue, increase dependence on others and reduce quality of life. It is not uncommon for individuals with a history of balance problems to regain their balance control through accurate diagnosis followed by specific medical treatment and/or rehabilitation exercises.</p>
    <p num="p-0088">The present invention described can be used as a testing and training device for balance improvement under both static and dynamic conditions.</p>
    <p num="p-0089">One testing and training scenario for postural stability would be to measure frequency and amplitude of body sway in three dimension (3D) space while feet remain in a fixed position. This task can be performed in both a double or single leg stance to test for bilateral symmetry relating to balance. Another modification of the test would be to perform each test with eyes both open and closed to help determine the contribution of the visual component to overall balance ability. Tracking body sway while creating the illusion of motion through proper visual cueing on a display means would be another test to help determine the reliance on specific sensory components of balance. Delivering repetition of protocols with increasing difficult oscillation thresholds with biofeedback of successes and failures of such predetermined goals is one way to train to improve balance.</p>
    <p num="p-0090">The transponder can deliver aural, visual, and tactile stimuli to queue the individual to the degree of frequency and amplitude of body oscillations. The aural and tactile components provide the only means of feedback when the testing and training are performed with eyes closed or the visual field is compromised. Examples include, but are not limited to, (1) an audio signal increasing and decreasing in volume proportional to the amplitude of body sway, (2) a vibration action proportional to frequency of body oscillations, and (3) a light source illuminated when both frequency and amplitude goals are achieved. Multiple transponders can be used to evaluate and reinforce proper balance posture by communicating position information of certain body segments in relationship to others. An example would be the comparison of position of the head with respect to the hips while generating a vibration action if an excessive forward lean of the head as compared to the hips is recognized.</p>
    <p num="p-0091">Another test for balance would be to test ones Limits of Stability (LOS). This test refers to ones ability to effectively operate within their sway envelope. The sway envelope or LOS is defined as the maximal angle a person's body can achieve from vertical without losing balance. An individual with healthy balance is capable of leaning (swaying) within a known sway envelope and recover back to a centered position without the need for a secondary maneuver such as a step, excessive bend at the torso or arm swinging. LOS for bilateral stance in normal adults is 8 degrees anterior, 4 degrees posterior and 8 degrees laterally in both directions.</p>
    <p num="p-0092">The present invention described can be used as a testing and training device for balance control during movement or perturbations within a desired sway envelope. Through proper visual queuing represented on the display means that defines a normal sway envelope, the amount of body displacement can be measured from vertical stance.</p>
    <p num="p-0093">The transponder can deliver aural, visual, and tactile stimuli to queue the individual as to when he or she has achieved the desired range of their sway envelope, then assess the individual's ability to return back to a vertical stance. Examples include, but are not limited to, (1) a vibration action when the user varies (meanders) from the desired movement path, (2) an array of lights change intensity and pattern as the individual successfully approaches the intended target, (3) an audio signal is generated when the individual has maintained a stable position with respect to proper visual queuing represented on the display means for a selected period of time. Multiple transponders can be used to evaluate and reinforce proper balance posture by communicating position information of certain body segments in relationship to others. An example would be the comparison of position of the head with respect to the hips while generating a vibration action if an excessive forward lean of the head as compared to the hips is recognized.</p>
    <p num="p-0094">Dynamic balance can be evaluated while having the individual perform coordinated movements which specifically challenge the various components of balance in a dynamic nature. Such movements include, but are not limited to jumping, hopping, and walking. These movements can be performed with eyes both open and closed, during interaction with static or dynamic visual queuing on the display means. The ability to perform these dynamic balance tasks with comparisons to others of similar sex, age or disability can be assessed. Example measurements may include, but are not limited to, (1) amount of body sway in three dimension (3D) space, (2) time to complete specific task, and (3) effects of fatigue on balance ability.</p>
    <p num="p-0095">Balance training in both static and dynamic conditions can be easily achieved by providing specific visual queuing on the display means, which challenge the individual to perform repetitive and progressively more difficult balance drills. Performance reports can be generated to establish a baseline, isolate specific strengths and weaknesses within the specific sensory and motor control aspects of balance, and document progression and improvements.</p>
    <p num="p-0096">The transponder can deliver aural, visual, and tactile stimuli to queue the individual as to when he or she has achieved the desired balance task. By example, a vibration action is produced proportional to the frequency of a body segment oscillation after the user lands from a hop test and attempts to stabilize and maintain proper postural balance. When the individual finally stabilizes and achieves correct postural balance, an audio signal indicates the task has successfully completed. Multiple transponders can be used to evaluate and reinforce proper balance posture by communicating position information of certain body segments in relationship to others. An example would be the comparison of position of the head with respect to the hips while generating a vibration action if an excessive forward lean of the head as compared to the hips is recognized.</p>
    <p num="h-0006">Range of Motion (ROM)</p>
    <p num="p-0097">The present invention described can be used as a testing and training device to determine the range of motion within a joint. Range of Motion is the normal distance and direction through which a joint can move. Limited ROM is a relative term indicating that a specific joint or body part cannot move through its normal and full ROM. Motion may be limited by a mechanical problem within the joint that prevents it from moving beyond a certain point, by swelling of tissue around the joint, by spasticity of the muscles, or by pain. Diseases that prevent a joint from fully extending, over time may produce contracture deformities, causing permanent inability to extend the joint beyond a certain fixed position.</p>
    <p num="p-0098">The present invention described can be used to test the starting point and end point which an individual is capable of moving a body part, typically a limb and associated joint(s). Comparisons to age and sex based normative data can be made. Proper visual queuing can be represented on the display means to instruct and motivate the individual through the proper testing procedure.</p>
    <p num="p-0099">The present invention described can be used as a testing and training device for individuals involved in physical rehabilitation or general fitness to improve ROM. Proper visual queuing can be represented on a display means to motivate individuals to extend their range of motion beyond their current capabilities.</p>
    <p num="p-0100">The transponder can deliver aural, visual, and tactile feedback that alerts the individual to successes or failures in proper execution of each repetition. An example of tactile feedback would be the transponders are vibrated if the individual's movement trajectory varied from the intended two dimensional (2D) reference movement trajectory by deviation from the planar path into the uninvolved spatial dimension. An array of light sources could increase illumination in intensity and repetition as the ROM goal was approached and an audio tone could signal the individual they have achieved the desired pause time at the proper ROM.</p>
    <p num="p-0101">Multiple transponders can be deployed to determine the contribution of each joint or anatomical structure where more then one joint is involved in the ROM movement (example; shoulder and scapular in overhead reaching). The vector sum of each transponder movement in a specific axis can be added together to determine the total ROM. The ROM of one joint in a two joint motion can be subtracted from the total ROM to determine the contribution of a single joint in a two joint movement.</p>
    <p num="h-0007">Human Performance Testing and Training</p>
    <p num="p-0102">There are many devices that test the strength and speed of isolated joint movements, for example, the leg extension and bicep curl. This information has value in testing both healthy individuals, athletes and individuals whose strength and speed capabilities may be compromised by injury, disease, poor conditioning or simply age. Recently in the field of human performance, it has been recognized that the analysis of the mobility of the isolated joint, although providing some value, does not offer enough information to determine how the body will perform during functional movements. Functional movements are defined as movements equal to those encountered on the athletic field, in the work environment or while performing activities of daily living. Functional movements involve the movement and coordination of multiple joints and muscle groups acting together to perform a more complex task then a single, isolated joint movement.</p>
    <p num="p-0103">The present invention described can be used as a testing and training device for functional movement improvement. By tracking various registration points on the body with respect to each other or to an off-body registration point, performance measurements of functional movements can be assessed, such as jumping, cutting, turning, bounding, hopping, shuttling, etc.</p>
    <p num="p-0104">The present invention described can be used as a testing and training device for individuals involved in physical rehabilitation, general fitness or sports performance enhancement to improve their functional movement abilities. Proper visual queuing can be represented on the display means to instruct and motivate individuals to perform specific functional movements.</p>
    <p num="p-0105">The transponder can deliver aural, visual, and tactile feedback of proper movement execution. Examples include, but are not limited to, (1) an audio signal alerting the user that the desired performance stance is incorrect, (2) the light sources illuminate when the desired speed is achieved in a first step quickness drill, (3) a vibration action to indicate the limits of tracking range, (4) a vibration action proportional to the magnitude of a biophysical measurement during the interaction with visual queues represented on the display means, (5) a vibration action when the body or limb position does not correlate well to the desired body or limb position of the visual queuing represented on the display means, (6) an audio signal indicating start, stop and pause periods or other controlling events, (7) an audio signal indicating proper body alignment or posture has been compromised, and (8) an audio signal indicating the relationship of desired target heart rate to a desired threshold.</p>
    <p num="h-0008">Hardware Description</p>
    <p num="p-0106">In the preferred embodiment, the processor unit is comprised principally of a constellation of five (5) ultrasonic transducers and signal processing circuitry, thereof, and a signal processor that interfaces to this receiver group, performs the pose calculations, and interfaces to the transponders and host computer databases. The following interface descriptions for the processor unit are based upon the dependency flow represented by <figref idrefs="DRAWINGS">FIG. 5</figref>.</p>
    <p num="p-0107">The sensors <b>14</b> preferably used for the receiver constellation unit are cylindrically-shaped ultrasonic transducers, for example, the model US40KR-01 40 kHz PVDF ultrasonic receivers manufactured by Measurement Specialties Inc., which provide adequate acoustic pressure sensitivity and exhibit 360 degree omnidirectional broad beam response along the horizontal plane. The omnidirectional characteristic, albeit in one plane only, is very desirable to minimize line-of-sight occlusion. Because of its low resonance Q value, the rising and decay times are much faster than conventional ceramic transmitters. This reduces its power requirements since less burst drive duration is needed to achieve sufficient triggering thresholds at the receiver. This transducer type is also utilized similarly in the transponders to provide the potential for the most optimal acoustic coupling.</p>
    <p num="p-0108">The receiver constellation unit is preferably mounted on a fixed support base, and has a pivoting and/or swiveling mechanical linkage which provides an adjustable mechanism for configuration of the receiver constellation unit's inertial frame of reference relative to the tracking field. In the preferred embodiment, it is strategically positioned and oriented in proximity to the tracking field in order (1) to minimize line-of-sight degradation with respect to the expected transponder orientation, (2) to optimize registration resolution with respect to field volume size, and (3) to satisfy the mathematical restrictions of performing trilateration calculations based upon the solution of simultaneous linear equations. It should be noted that the trilateration matrices may be solved if the matrices have a rank of five, and are non-singular, i.e., the matrix determinant is non-zero. In the preferred embodiment, the geometric parameters and their coordinate location of the receiver constellation must insure linear independence of the columns of the matrices and to avoid the matrices from becoming singular.</p>
    <p num="p-0109">One example geometrical permutation of the receiver constellation unit that satisfies these rules is shown in <figref idrefs="DRAWINGS">FIG. 6A</figref>. It occupies a volume of approximately 8 cu. ft. and essentially fixes the transducers in a way that defines two primary orthogonal, bisecting planes defined by three non-collinear points each. Another preferred implementation that occupies nearly the same volume is shown in <figref idrefs="DRAWINGS">FIG. 6B</figref> and is characterized by its S-shaped curve and tilted with respect to the horizontal plane. Another preferred implementation that occupies nearly the same volume is shown in <figref idrefs="DRAWINGS">FIG. 6C</figref> and is characterized by its helical or logarithmic spiral shape oriented perpendicular to the horizontal plane. Further, as indicated in the preceding figures, the transducers vertical axes are oriented 90° with respect to the typical vertical axis orientation of the transponder's transmitter to improve acoustic coupling in the vertical plane, a consideration for overhead, upper extremity tracking. Although this causes some reduction in the lateral registration bounds, the compromise provides a more symmetric field about the middle or primary location of tracking interest.</p>
    <p num="p-0110">In the preferred embodiment, the overall size of the receiver constellation unit is predicated on a phenomenon referred to as Geometric Dilution of Precision (GDOP). The solution of a unique three-dimensional location based upon trilateration requires the precise resolution of the common intersection of multiple spheres circumscribed by the distance between each transmitter and receiver transducer. Each sphere has an inexact radius due to system noise and measurement resolution. Therefore, the intersection becomes a volume instead of a point and the size of the volume is dependent upon the radii of the intersecting spheres as well as the distance between the spheres' centers. As the radii get larger with respect to the distance between the centers, i.e., the transmitter is farther down range, the spheres begin to appear more and more tangential to one another and the intersection volume increases, although not necessarily symmetrical in all dimensions. Therefore, to minimize position uncertainty, the receiver transducers should be separated from each other as much as practical proportions allow with respect to the confines of the tracking field volume as the above said geometric examples provide.</p>
    <p num="p-0111">This receiver constellation unit can be repositioned with respect to the tracking field by a simple mechanical adjustment as shown in the preceding figures. The mechanical adjustment raises and lowers and changes the length and pivot axis of the cantilever arm which is fixed to a ground base support.</p>
    <p num="p-0112">Because the receiver constellation unit operates a distance from the processor unit, each receiver preferably has an associated pre-amplifier circuit to convert the high-input impedance piezoelectric signal into a low-level voltage proportional to the acoustic signal energy impinging the transducers sufficient in order to accurately transmit the signals to the processor unit. In one embodiment, a high-input impedance AC amplifier design with 30 dB gain can be utilized. The preferred operational amplifier is the OPA373 manufactured by Texas Instruments. It was chosen for its low 1 pA input bias current, high 6 MHz GBW, and low-voltage single supply operation. The amplifier is configured as a non-inverting type with the high-pass cutoff frequency set at 1 kHz. The overall circuitry is preferably enclosed in a metal shield to minimize electromagnetic noise coupling into the highly sensitive amplifier inputs. In addition, a local, regulated power supply is included to allow for a wide range of input voltage supply and provide sufficient power supply rejection to compensate for the noise susceptibility of remote power distribution. All the pre-amplifier circuits' power and signal connections preferably originate from the processor unit.</p>
    <p num="p-0113">The processor unit subsystem preferably consists of an analog signal processing interface that provides (1) additional voltage amplification and filtering of base band signal from the preamplifiers, (2) absolute value function, (3) peak detection function, and (4) analog-to-digital comparator function to provide support for an adaptive threshold means. The adaptive threshold technique provides robust triggering of the most proximal ultrasonic source at a precise temporal point along the traversing sinusoidal waveform of the electrical signal. Essentially, a new threshold signal is recalculated each analysis period based upon a small percentage reduction of the last peak waveform detected. Therefore, the tracking range is not necessarily restricted due to an arbitrarily high threshold setting and the noise immunity is improved as the threshold tracks the waveform envelope and not transient disturbances. An alternative automatic gain control strategy for the amplification function is unnecessary since the trigger threshold will adjust to the signal level instead. In the preferred embodiment, the threshold faithfully tracks the peak to minimize integer period phase errors, so the amplifier's gain is set to prevent signal saturation from occurring when the receiver constellation unit and transponders are in closest proximity during normal use.</p>
    <p num="p-0114">In one embodiment, an amplifier and BW (band width) filter circuit receives the output from the sensor and preamplifier circuit and provides additional amplification and low-pass filtering to condition it for reliable threshold triggering and input to other analog signal processing circuitry. A dual amplifier configuration may be used to provide an additional gain of 40 dB, AC coupling to remove DC offsets of the preamplifier outputs and long cable losses, and low-pass filter to reject noise beyond the interest signal's bandwidth. The first stage amplifier may be configured as a non-inverting type with a gain of 20 dB. The low-impedance DC input signal is effectively blocked by the coupling capacitor in series at its non-inverting input with a high-pass frequency cutoff set at 20 kHz. This gain stage feeds a second amplifier configured as low-pass, 2<sup>nd </sup>order Butterworth MFB filter. This filter type provides smooth pass band response and reduced sensitivity to component tolerances. The second stage low-pass frequency cutoff is set at 80 kHz with a pass band gain of 20 dB.</p>
    <p num="p-0115">An absolute value circuit receives the output of the amplifier and BW filter circuit and converts the bipolar signal into a unipolar form for magnitude detection. A dual amplifier configuration may be used to provide highly accurate full wave rectification of the millivolt-level signal. The first stage amplifier feedback switches to control the distribution of input current between the two signal paths dependent upon the input signal polarity. For a positive input voltage the input current will be positive which forward biases D<b>1</b> and reverse biases D<b>2</b>. This configures the 1<sup>st </sup>stage as an inverter driving the inverting input resistor of the 2<sup>nd </sup>stage, which is also configured as an inverter because its non-inverting input is held at virtual ground due to the non-conducting path of D<b>2</b>. This effectively creates a combined circuit of two cascaded inverters for an overall gain of +1. For a negative input signal its input current is negative which forward biases D<b>2</b> and reverse biases D<b>1</b>. This configures the 1<sup>st </sup>stage as an inverter driving the non-inverting input of the 2<sup>nd </sup>stage which changes the sign of the circuit gain. In this mode, the input current is shared between two paths to the input of the 2<sup>nd </sup>stage, where −⅔ of the input current flows around the 1<sup>st </sup>feedback stage and −⅓ flows in the opposite path around the 2<sup>nd </sup>stage feedback path for a net gain of −1.</p>
    <p num="p-0116">In the preferred embodiment, a peak detect and sample-hold circuit receives the output of the absolute value circuit and registers a peak value that is required to set a magnitude threshold precisely at some percentage of full-scale of the peak. A dual amplifier configuration may be used to provide the highest ratio of high output slew rate to low droop. The first stage is typically in negative saturation until the input voltage rises and exceeds the peak previously stored on the sample capacitor at the inverting input. Now the amplifier acts as a unity gain buffer and the input voltage charges the sample capacitor which faithfully tracks the rising voltage. Once the input voltage diminishes in magnitude, the first blocking diode reverse biases and the sample capacitor holds an accurate replica of the highest voltage attained with minimal droop because of the low input bias current of the amplifier and elimination of leakage altogether in the second blocking diode by bootstrapping its cathode at the same potential provided by the low-impedance buffer of the second output stage. An electronic switch and bleed resistor allow the voltage across the sample capacitor to be reset by the processor during power up and after the triggering event is recorded so the adaptive threshold value can be refreshed each cycle. A 1<sup>st </sup>order Butterworth filter may be used at the input to smooth false in-band transients that could disrupt the peak accuracy detection.</p>
    <p num="p-0117">In the preferred embodiment, a comparator circuit receives the output from the peak detect and sample-hold circuit to convert the analog signal to digital form for high-speed triggering operation of the processor. The preferred device is the MAX941 which is manufactured by Maxim. A percentage of the peak threshold is used to set the inverting input. When the non-inverting voltage exceeds the inverting voltage, the comparator's output will trip and produce a high-true logic pulse that triggers the processor. A latch control input allows the processor to disable the comparator action to prevent unnecessary triggering during the reverberation phase and to prevent potentially disruptive noisy output chattering near threshold crossover beyond its hysteretic immunity. The percentage of threshold level is predetermined through the scaling resistors to be set low enough to trigger on the rising edge of the signal's first crest at the furthest range of transponder operation, but high enough above the intrinsic system noise level and external noise caused by reverberation and other ultrasonic sources. Once the first crest is registered, subsequent crests may be triggered at their zero-crossing representing the most precise timing registration by momentarily disabling the sample-hold circuit. Because of the longer duration trigger receptivity window, early multiple reflections are mitigated by transducer placement at least 3.5 cm away from adjacent planar surfaces, so the reflected acoustic energy doesn't produce a canceling effect of the direct acoustic energy of the later crests. Once a sufficient number of crests have been registered, then the triggering window is blanked for the remainder of the analysis period by latching the comparator's value.</p>
    <p num="p-0118">In the preferred embodiment, a digital signal processing interface is connected to the analog signal processing interface to transform the analog trigger processing into digital position information.</p>
    <p num="p-0119">The digital filter circuit receives output from the comparator circuit and preferably consists of a digital low-pass filter implemented in a complex programmable logic device (CPLD) that serves to precondition the comparator circuit's digital outputs. The preferred device is an AT1504ASVL CPLD which is manufactured by Atmel. Base band system noise or other glitches potentially occurring in the analog signal processor interface, but prior to the actually arrival of the ultrasonic signal, could cause a threshold disruption that registers a “runt” pulse as a false trigger condition. The “runt” pulse would be misinterpreted as the actual TOF trigger and cause serious error in the position calculation. An AND/NOR one-hot state machine design may be used to ignore level transitions that are not stable for at least ½ system clock frequency×8 states, so only transitions of 4 μS or greater are passed through. The system clock delays introduced by the digital filter's synchronous state machine affect all channels the same and are, therefore, effectively eliminated by the inherent dependency on relative measurement.</p>
    <p num="p-0120">In the preferred embodiment, the processor and digital filter circuits receive the output from the analog processor and provide controlling signals therein. The preferred processor circuit is the MC9S08 GB60 which is manufactured by Motorola Inc. It is a low-cost, high-performance 8-bit microcontroller device that provides all the aforementioned hardware circuits integrated into one convenient device. The calculation circuit is abstracted from embedded 60 KB FLASH for program memory with in-circuit programmable capability and 4 KB RAM for data memory. The time base circuit is preferably comprised of an external, high-noise immunity, 4.0 MHz system clock, which multiplies this by the internal frequency-locked loop for a bus clock of 40.0 MHz and single instruction execution time of 25 ηS. This clock also provides all the capture and control timing functionality for the other specified circuits. Multiple parallel I/O ports and dedicated asynchronous serial communication signals provide for the digital control of the analog signal processing and communication interfaces, respectively.</p>
    <p num="p-0121">The timing capture-control circuit receives the output from the digital filter circuit representing the arrival of the TOF triggers to determine the relative TOF propagation of the ultrasonic acoustic wave as it passes through the receiver constellation unit. More specifically, it is comprised of a five channel 16-bit timer input capture module with programmable interrupt control that provides edge detection and 50 ηs timing precision to automatically register the TOF triggers timestamps asynchronously without using inefficient and less accurate software polling means.</p>
    <p num="p-0122">The phase-locked loop circuit receives the output from the timing capture-control circuit and is preferably comprised of a three channel, 16-bit timer compare module is implemented as an all-digital phase locked loop (ADPLL), which synchronizes the capture window and blanking functions with respect to the reference input channel. It is comprised primarily of a free-running 16-bit timer configured to periodically interrupt the processor dependent upon a precise convergence of its period and phase to the reference trigger source, by means of an over/under count matching and correction technique.</p>
    <p num="p-0123">The A/D conversion circuit receives the output from the amplifier and BW filter circuit and consists of an eight channel 10-bit analog-to-digital converter used to monitor channel offsets and magnitudes for range and polarity errors and correction. This information is utilized by the calculation circuit as input to the TOF software correction algorithm to determine the slope of the waveform crest.</p>
    <p num="p-0124">In the preferred embodiment, the serial communication circuit is comprised of two asynchronous serial communication interfaces that are connected between the calculation circuit and host link and radio link circuits of the communication interface. The host link provides a 115K bit per second (baud) bi-directional communication link to an auxiliary host computer system through a Serial-to-Universal Serial Bus bridge. The preferred device is the CP2101 which is manufactured by Silicon Laboratories. It supports the conversion of a fully asynchronous serial data bus protocol, with buffering and handshaking support, to an integrated Universal Serial Bus (USB) Function Controller and Transceiver and internal clock providing USB 2.0 full-speed compliancy. An integrated 512 bit EEPROM stores the required USB device descriptors, including the Vendor ID, Product ID, Serial Number, Power Descriptor, Release number and Product Description strings. A host computer may enumerate and access this device utilizing the manufacturer's virtual COM port device drivers using a USB channel.</p>
    <p num="p-0125">In the preferred embodiment, the radio link circuit is comprised of a wireless bi-directional communication interface to preferably (1) broadcast a synchronization signal to control the transponders interoperability, (2) to receive other transponder sensor data, including, but not limited to, accelerometer, heart rate, battery, user I/O status, (3) to provide control messages for the transponders' sensory interfaces, and (4) to provide means to configure transponders' local databases. The preferred wireless communication link is based upon the AT86RF211, a highly integrated, low-power FSK transceiver optimized for license-free ISM band operations from 400 MHz to 950 MHz. and manufactured by Atmel. It supports data rates up to 64 kbps with data clock recovery and no Manchester Encoding required. The device has a three wire microprocessor interface that allows access of read/write registers to setup the frequency selection, transmission mode, power output, etc. or get information about parameters such as battery, PLL lock state, etc. In normal mode, any data entering its input channel is immediately radiated or any desired signal collected by the aerial is demodulated and transferred to the microprocessor as reshaped register bit information. In wake-up mode, the device periodically scans for an expected message sequence and broadcasts an interrupt if a correct message is detected.</p>
    <p num="p-0126">In the preferred embodiment, at least three (3) consecutive TOF timestamps are registered for each receiver during the acquisition phase. Preferably, the transponder's transducer emits a multi-cycle ultrasonic acoustic burst of at least ten cycles in duration so that sufficient energization of the receiver transducer is realized and at least three crests of the waveform can be properly registered. At low signal levels when ultrasonic acoustic coupling is poor, this requirement may fail and an invalid tracking status is asserted. Preferably, the reference receiver transducer of the receiver constellation unit is positioned in closest proximity to the acoustic signal source so that it is the first transducer to be affected by the initial wave front. This reference receiver provides the overall system timing and state machine control for the phase-locked loop circuit, so that the processing, calculation, and communication tasks are executed in a deterministic and efficient fashion.</p>
    <p num="p-0127">It should be appreciated that a high-resolution ultrasonic acoustic tracking system that depends upon threshold detection means has an inherent uncertain trigger dilemma. This uncertainty arises because of the multi-cycle nature of the transmitted signal's waveform and the associated difficulty detecting the exact temporal location for consecutive analysis periods when the signal's magnitude may vary greatly depending upon the efficiency of the acoustic coupling, the distance between transmitter and receiver, and signal-to-noise ratio of the signal processing techniques. If a threshold is set near one of the minor crests of the waveform during the last analysis period, then it is conceivable a slight reduction of magnitude of the waveform during the next analysis period may fall slightly below the set threshold and actually not be triggered until the next larger excursion of the waveform occurs. This would create a TOF error proportional to the period of the acoustic waveform or its intra-pulse interval and have a detrimental affect on the measurement accuracy. This analog processing described above establishes trigger thresholds that allow no more than a single intra-pulse interval of uncertainty, but that is still inadequate for high-resolution measurements. Although a technique is known that controls the largest peak profile of the transmitter acoustic signal and claims to provide an absolute trigger condition, this procedure is difficult to reliably tune and control among different transducer types.</p>
    <p num="p-0128">In the preferred embodiment of the invention, no modulation of the acoustic signal is required. Rather, the adaptive threshold method is augmented with a TOF software correction algorithm that unambiguously determines the correct TOF based upon a means to detect the same carrier wave cycle of ultrasonic energy every period. The software correction algorithm requires multiple, consecutive TOF acquisitions as input for the digital over-sampling and averaging algorithm, the calculation of a higher-order numerical differentiation of the past and current TOF information as input for the predictive algorithm of higher-order Taylor series based derivatives used for the relative TOF correction, and a measurement of the intra-pulse time intervals of consecutive TOF acquisitions as input for the absolute TOF correction scheme that minimizes the range error based upon selective biasing of the TOFs.</p>
    <p num="p-0129">The calculation circuit preferably processes multiple, consecutive TOF acquisitions to effectively improve the timing resolution that proportionally affects position accuracy and precision. The digital filter discussed above introduces quantization errors because of its discrete operation. And minor fluctuations in the acoustical coupling produces timing jitter or uncertainty in the triggered zero-crossings of the acoustic sinusoidal. A Gaussian average or mean value of multiple TOF is a simple and effective filter strategy. Due to the possibility of poor acoustic coupling or misalignment, and distant transponder location from the processor unit, the number of detectable triggered zero-crossings may vary for a fixed duration of multi-cycle ultrasonic acoustic burst. The averaging algorithm automatically adjusts to this condition by only including TOFs whose delta changes fall within the expected range of the nominal intra-pulse interval defined by the transmission properties of the acoustic source. The nominal intra-pulse interval is determined and utilized by the following compensation schemes.</p>
    <p num="p-0130">The calculation circuit preferably processes a relative TOF correction algorithm based upon a predictive tuned algorithm that requires higher-order numerical differentiation calculation of the past and current TOFs. This compensates the TOFs that may have registered one intra-pulse interval earlier or later than the nominally expected time due to the trigger dilemma described above. By formulating these derivatives into a truncated 2<sup>nd </sup>order Taylor series expansion and weighting the terms contribution, an estimate of expected TOF is calculated and compared to the actual TOF through an iterative error minimization calculation. A minimized error that results in a delta time change indicative of a discrete intra-pulse interval increase or decrease due to an early or late TOF, respectively, produces a characteristic value that directs the algorithm to compensate the actual TOF by the intra-pulse interval and restore it to its correct value. In the preferred embodiment, this relative compensation algorithm works most effectively when, (1) the maximally expected inter-period TOF change is less than the discrete intra-pulse interval, (2) the TOF inter-period processing is contiguous, (3) the TOF increase or decrease is no more than a single intra-pulse interval, and (3) the Taylor series terms are suitably weighted in the prediction algorithm.</p>
    <p num="p-0131">The calculation circuit preferably processes an absolute TOF correction algorithm at least once initially, when the phase-locked loop is stable, but may be performed every analysis period depending on computational resources, that determines the initial set of TOF values for the relative correction algorithm. The initial condition that precedes the start of the relative compensation algorithm may be due to the resumption of a stable, locked tracking state after recovery from a fault condition and, therefore, requires computation of a set of reference TOFs producing minimum range error as a starting basis. The algorithm utilizes a wireless synchronization means to determine a reference TOF calculation between the transponder and reference sensor of the receiver constellation. By computing the reference range distance by the product of the reference TOF and speed of sound in air, this reference range may be compared to the range calculated from the matrices solutions described below. By iteratively and sequential increasing and decreasing the TOFs by a single intra-pulse time interval and applying the input to matrices formulations described below, all possible combinations of compensation are permutated and tested, which produces a unique set of TOFs that minimize the error between the calculated range distance with respect to the reference range distance. This unique set of initial TOFs serves as the starting basis for the relative compensation algorithm. In the preferred, embodiment, this absolute compensation algorithm works most effectively when (1) the wireless synchronization means is tightly coupled to the excitation of the acoustic source, (2) the synchronizing signal's arrival is timed by the same mechanism that times the arrival of the reference transducer's acoustic signal, and (3) the coordinate locations of the sensors of the receiver constellation are established to a high degree of accuracy.</p>
    <p num="p-0132">The calculation circuit preferably employs two software methods of trilateration calculation to estimate transponder position, wherein the particular method used depends upon the availability of a synchronizing signal and the accuracy desired. The first method is based on a relative TOF calculation and the speed of sound is treated as a constant estimated at ambient indoor room temperature. The second method requires calculation of an additional TOF timestamp between the transponder and reference receiver, but calculates the speed of sound as an unknown every analysis period, and thus improves measurement accuracy. The first method eliminates the global system timing variances and delays due to the multiplicity of signal conditioning circuitry and eliminates the need for a controlling signal means synchronized at the generation of the transmission of the ultrasonic acoustic wave. The second method also employs relative TOF calculation but requires an additional synchronization signal from the processor unit to determine the absolute TOF between transponder and reference receiver. Since the absolute TOF is based upon a single channel only, its timing latencies can be readily accounted for and easily corrected. This method computes the speed of sound every analysis period, provided the synchronization signal is detected, without need for additional hardware temperature processing or requiring more then five (5) receivers, and automatically accounts for the system's main accuracy limitation of speed of sound in air as defined by Eq. 1.1, if uncorrected, yields a 1.6 mm/m ranging error for every 1° C. temperature shift. If the synchronization signal is not detected and, therefore, the second method is not resolvable, the last calculated speed of sound can be utilized within the first method's calculation to minimize error.
<br> <i>c≈</i>34.6 m/s+0.5813 m/s(<i>T</i> <sub>c</sub>−25° C.)  (1.1)</p>
    <p num="p-0133">The TOF timestamps and speed of sound values are input into linear independent algebraic equations in a matrix formulation to solve for the unknown transponder(s) position, in a form as shown in Eq. 2.1,</p>
    <p num="p-0134">
      <maths id="MATH-US-00001" num="00001"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mtable> <mtr> <mtd> <mrow> <mrow> <mi>A</mi> <mo>·</mo> <mi>X</mi> </mrow> <mo>=</mo> <mi>B</mi> </mrow> </mtd> <mtd> <mrow> <mi>A</mi> <mo>=</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <msub> <mi>a</mi> <mn>11</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>12</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>13</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>14</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>a</mi> <mn>21</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>22</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>23</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>24</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>a</mi> <mn>31</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>32</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>33</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>34</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>a</mi> <mn>41</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>42</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>43</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>44</mn> </msub> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mi>X</mi> <mo>=</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <msub> <mi>x</mi> <mn>1</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>x</mi> <mn>2</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>x</mi> <mn>3</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>x</mi> <mn>4</mn> </msub> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mi>B</mi> <mo>=</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <msub> <mi>b</mi> <mn>1</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>b</mi> <mn>2</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>b</mi> <mn>3</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>b</mi> <mn>4</mn> </msub> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> </mtd> </mtr> </mtable> </mtd> <mtd> <mrow> <mo>(</mo> <mn>2.1</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0135">To solve for the unknowns X, Eq. 2.1 is rearranged as shown in Eq. 3.1, whereas the inverse of A requires computation of the cofactor matrix A<sup>c </sup>for the adjoint and determinant calculations for Eq. 3.2 and Eq. 3.3, respectively,</p>
    <p num="p-0136">
      <maths id="MATH-US-00002" num="00002"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mi>X</mi> <mo>=</mo> <mrow> <mrow> <msup> <mi>A</mi> <mrow> <mo>-</mo> <mn>1</mn> </mrow> </msup> <mo>·</mo> <mi>B</mi> </mrow> <mo>=</mo> <mrow> <mfrac> <msup> <mrow> <mo>(</mo> <msup> <mi>A</mi> <mi>c</mi> </msup> <mo>)</mo> </mrow> <mrow> <mo>-</mo> <mi>T</mi> </mrow> </msup> <mrow> <mo></mo> <mi>A</mi> <mo></mo> </mrow> </mfrac> <mo>·</mo> <mi>B</mi> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>3.1</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msup> <mrow> <mo>(</mo> <msup> <mi>A</mi> <mi>c</mi> </msup> <mo>)</mo> </mrow> <mrow> <mo>-</mo> <mi>T</mi> </mrow> </msup> <mo>=</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <msub> <mi>A</mi> <mn>11</mn> </msub> </mtd> <mtd> <msub> <mi>A</mi> <mn>21</mn> </msub> </mtd> <mtd> <msub> <mi>A</mi> <mn>31</mn> </msub> </mtd> <mtd> <msub> <mi>A</mi> <mn>41</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>A</mi> <mn>12</mn> </msub> </mtd> <mtd> <msub> <mi>A</mi> <mn>22</mn> </msub> </mtd> <mtd> <msub> <mi>A</mi> <mn>32</mn> </msub> </mtd> <mtd> <msub> <mi>A</mi> <mn>42</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>A</mi> <mn>13</mn> </msub> </mtd> <mtd> <msub> <mi>A</mi> <mn>23</mn> </msub> </mtd> <mtd> <msub> <mi>A</mi> <mn>33</mn> </msub> </mtd> <mtd> <msub> <mi>A</mi> <mn>43</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>A</mi> <mn>14</mn> </msub> </mtd> <mtd> <msub> <mi>A</mi> <mn>24</mn> </msub> </mtd> <mtd> <msub> <mi>A</mi> <mn>34</mn> </msub> </mtd> <mtd> <msub> <mi>A</mi> <mn>44</mn> </msub> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>3.2</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mo></mo> <mi>A</mi> <mo></mo> </mrow> <mo>=</mo> <mrow> <mrow> <msub> <mi>a</mi> <mn>11</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>11</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>a</mi> <mn>21</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>21</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>a</mi> <mn>31</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>31</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>a</mi> <mn>41</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>41</mn> </msub> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>3.3</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0137">To setup the coefficient matrix A, the utilization of five (5) receivers produces the following set of relative TOF equations defined by Eqs. 4.1-4,
<br>Δ<i>T</i> <sub>12</sub> <i>=T</i> <sub>2</sub> <i>−T</i> <sub>1</sub>  (4.1)<br>Δ<i>T</i> <sub>13</sub> <i>=T</i> <sub>3</sub> <i>−T</i> <sub>1</sub>  (4.2)<br>Δ<i>T</i> <sub>14</sub> <i>=T</i> <sub>4</sub> <i>−T</i> <sub>1</sub>  (4.3)<br>Δ<i>T</i> <sub>15</sub> <i>=T</i> <sub>5</sub> <i>−T</i> <sub>1</sub>  (4.4)</p>
    <p num="p-0138">The receiver locations are fixed within the system's inertial reference frame, while the transponder(s) are mobile with respect to the same reference frame and are defined as follows,
<br> <i>S</i>(<i>x</i> <sub>1</sub> <i>,y</i> <sub>1</sub> <i>,z</i> <sub>1</sub>) for 5<i>≧i≧</i>1</p> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-P00001.png"> <img id="CUSTOM-CHARACTER-00001" he="2.79mm" wi="3.13mm" file="US07952483-20110531-P00001.TIF" alt="Figure US07952483-20110531-P00001" img-content="character" img-format="tif" src="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-P00001.png" class="patent-full-image" width="12" height="11"> </a> </div>fixed receiver locations<br> <i>S</i>(<i>x</i> <sub>0</sub> <i>,y</i> <sub>0</sub> <i>,z</i> <sub>0</sub>)≡<i>S</i>(<i>u,v,w</i>)<div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-P00001.png"> <img id="CUSTOM-CHARACTER-00002" he="2.79mm" wi="3.13mm" file="US07952483-20110531-P00001.TIF" alt="Figure US07952483-20110531-P00001" img-content="character" img-format="tif" src="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-P00001.png" class="patent-full-image" width="12" height="11"> </a> </div>unknown transponder location<p num="p-0139">Since each receiver is fixed at a distance D<sub>i </sub>from the transponder as determined by the receiver constellation geometry and because the acoustic waves propagate spherically, by using Pythagorean's theorem the following set of range equations are defined in Eqs. 5.1-5,
<br>(<i>x</i> <sub>1</sub> <i>−u</i>)<sup>2</sup>+(<i>y</i> <sub>1</sub> <i>−v</i>)<sup>2</sup>+(<i>z</i> <sub>1</sub> <i>−w</i>)<sup>2</sup> <i>=D</i> <sub>1</sub> <sup>2</sup>  (5.1)<br>(<i>x</i> <sub>2</sub> <i>−u</i>)<sup>2</sup>+(<i>y</i> <sub>2</sub> <i>−v</i>)<sup>2</sup>+(<i>z</i> <sub>2</sub> <i>−w</i>)<sup>2</sup> <i>=D</i> <sub>2</sub> <sup>2</sup>  (5.2)<br>(<i>x</i> <sub>3</sub> <i>−u</i>)<sup>2</sup>+(<i>y</i> <sub>3</sub> <i>−v</i>)<sup>2</sup>+(<i>z</i> <sub>3</sub> <i>−w</i>)<sup>2</sup> <i>=D</i> <sub>3</sub> <sup>2</sup>  (5.3)<br>(<i>x</i> <sub>4</sub> <i>−u</i>)<sup>2</sup>+(<i>y</i> <sub>4</sub> <i>−v</i>)<sup>2</sup>+(<i>z</i> <sub>4</sub> <i>−w</i>)<sup>2</sup> <i>=D</i> <sub>4</sub> <sup>2</sup>  (5.4)<br>(<i>x</i> <sub>5</sub> <i>−u</i>)<sup>2</sup>+(<i>y</i> <sub>5</sub> <i>−v</i>)<sup>2</sup>+(<i>z</i> <sub>5</sub> <i>−w</i>)<sup>2</sup> <i>=D</i> <sub>5</sub> <sup>2</sup>  (5.5)</p>
    <p num="p-0140">Equivocally, the four (4) non-reference receivers are preferably located at an incremental distance relative to the reference receiver, so by substitution of the incremental distance defined by Eq. 6.1, the following set of relativistic range equations are defined by Eqs. 6.2-5,
<br> <i>D</i> <sub>1</sub> <i>=D</i> <sub>1</sub> <i>+cΔT</i> <sub>11 </sub>for 5<i>≧i≧</i>2  (6.1)<br>(<i>x</i> <sub>2</sub> <i>−u</i>)<sup>2</sup>+(<i>y</i> <sub>2</sub> <i>−v</i>)<sup>2</sup>+(<i>z</i> <sub>2</sub> <i>−w</i>)<sup>2</sup>=(<i>D</i> <sub>1</sub> <i>+cΔT</i> <sub>12</sub>)<sup>2</sup>  (6.2)<br>(<i>x</i> <sub>3</sub> <i>−u</i>)<sup>2</sup>+(<i>y</i> <sub>3</sub> <i>−v</i>)<sup>2</sup>+(<i>z</i> <sub>3</sub> <i>−w</i>)<sup>2</sup>=(<i>D</i> <sub>1</sub> <i>+cΔT</i> <sub>13</sub>)<sup>2</sup>  (6.3)<br>(<i>x</i> <sub>4</sub> <i>−u</i>)<sup>2</sup>+(<i>y</i> <sub>4</sub> <i>−v</i>)<sup>2</sup>+(<i>z</i> <sub>4</sub> <i>−w</i>)<sup>2</sup>=(<i>D</i> <sub>1</sub> <i>+cΔT</i> <sub>14</sub>)<sup>2</sup>  (6.4)<br>(<i>x</i> <sub>5</sub> <i>−u</i>)<sup>2</sup>+(<i>y</i> <sub>5</sub> <i>−v</i>)<sup>2</sup>+(<i>z</i> <sub>5</sub> <i>−w</i>)<sup>2</sup>=(<i>D</i> <sub>1</sub> <i>+cΔT</i> <sub>15</sub>)<sup>2</sup>  (6.5)</p>
    <p num="p-0141">By expanding and rearranging the terms of Eqs. 6.2-5, a set of four linear algebraic equations and four unknowns for the first method algorithm, depicted in the matrix form of Eq. 2.1, is defined by Eq. 7.1,</p>
    <p num="p-0142">
      <maths id="MATH-US-00003" num="00003"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mrow> <mn>2</mn> <mo>⁡</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mrow> <msub> <mi>x</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>x</mi> <mn>2</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>y</mi> <mn>2</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>z</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>z</mi> <mn>2</mn> </msub> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>-</mo> <mi>c</mi> </mrow> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>12</mn> </msub> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>x</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>x</mi> <mn>3</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>y</mi> <mn>3</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>z</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>z</mi> <mn>3</mn> </msub> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>-</mo> <mi>c</mi> </mrow> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>13</mn> </msub> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>x</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>x</mi> <mn>4</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>y</mi> <mn>4</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>z</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>z</mi> <mn>4</mn> </msub> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>-</mo> <mi>c</mi> </mrow> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>14</mn> </msub> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>x</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>x</mi> <mn>5</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>y</mi> <mn>5</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>z</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>z</mi> <mn>5</mn> </msub> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>-</mo> <mi>c</mi> </mrow> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>15</mn> </msub> </mrow> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> <mo>⁡</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mi>u</mi> </mtd> </mtr> <mtr> <mtd> <mi>v</mi> </mtd> </mtr> <mtr> <mtd> <mi>w</mi> </mtd> </mtr> <mtr> <mtd> <msub> <mi>D</mi> <mn>1</mn> </msub> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> <mo>=</mo> <mrow> <mo> </mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mrow> <msup> <mrow> <mo>(</mo> <mrow> <mi>c</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>12</mn> </msub> </mrow> <mo>)</mo> </mrow> <mn>2</mn> </msup> <mo>+</mo> <msubsup> <mi>R</mi> <mn>1</mn> <mn>2</mn> </msubsup> <mo>-</mo> <msubsup> <mi>R</mi> <mn>2</mn> <mn>2</mn> </msubsup> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msup> <mrow> <mo>(</mo> <mrow> <mi>c</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>13</mn> </msub> </mrow> <mo>)</mo> </mrow> <mn>2</mn> </msup> <mo>+</mo> <msubsup> <mi>R</mi> <mn>1</mn> <mn>2</mn> </msubsup> <mo>-</mo> <msubsup> <mi>R</mi> <mn>3</mn> <mn>2</mn> </msubsup> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msup> <mrow> <mo>(</mo> <mrow> <mi>c</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>14</mn> </msub> </mrow> <mo>)</mo> </mrow> <mn>2</mn> </msup> <mo>+</mo> <msubsup> <mi>R</mi> <mn>1</mn> <mn>2</mn> </msubsup> <mo>-</mo> <msubsup> <mi>R</mi> <mn>4</mn> <mn>2</mn> </msubsup> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msup> <mrow> <mo>(</mo> <mrow> <mi>c</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>15</mn> </msub> </mrow> <mo>)</mo> </mrow> <mn>2</mn> </msup> <mo>+</mo> <msubsup> <mi>R</mi> <mn>1</mn> <mn>2</mn> </msubsup> <mo>-</mo> <msubsup> <mi>R</mi> <mn>5</mn> <mn>2</mn> </msubsup> </mrow> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>7.1</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mi>where</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <msubsup> <mi>R</mi> <mi>i</mi> <mn>2</mn> </msubsup> </mrow> <mo>=</mo> <mrow> <mrow> <msubsup> <mi>x</mi> <mi>i</mi> <mn>2</mn> </msubsup> <mo>+</mo> <msubsup> <mi>y</mi> <mi>i</mi> <mn>2</mn> </msubsup> <mo>+</mo> <mrow> <msubsup> <mi>z</mi> <mi>i</mi> <mn>2</mn> </msubsup> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>for</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>5</mn> </mrow> </mrow> <mo>≥</mo> <mi>i</mi> <mo>≥</mo> <mn>1</mn> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>7.2</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0143">Alternatively, if the second method algorithm is used, the unknown range of the reference receiver D<sub>1 </sub>can be substituted by Eq. 8.1,
<br> <i>D</i> <sub>1</sub> <i>=cT</i> <sub>01</sub> <i>, </i> </p> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-P00002.png"> <img id="CUSTOM-CHARACTER-00003" he="3.13mm" wi="2.46mm" file="US07952483-20110531-P00002.TIF" alt="Figure US07952483-20110531-P00002" img-content="character" img-format="tif" src="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-P00002.png" class="patent-full-image" width="9" height="12"> </a> </div> <i>T</i> <sub>01</sub> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-P00001.png"> <img id="CUSTOM-CHARACTER-00004" he="2.79mm" wi="3.13mm" file="US07952483-20110531-P00001.TIF" alt="Figure US07952483-20110531-P00001" img-content="character" img-format="tif" src="//patentimages.storage.googleapis.com/US7952483B2/US07952483-20110531-P00001.png" class="patent-full-image" width="12" height="11"> </a> </div>time of flight (TOF) from <i>S</i>(<i>u,v,w</i>) to <i>S</i>(<i>x</i> <sub>1</sub> <i>,y</i> <sub>1</sub> <i>,z</i> <sub>1</sub>)  (8.1)<p num="p-0144">And, by rearranging terms, it is depicted in the matrix form defined by Eq. 9.1,</p>
    <p num="p-0145">
      <maths id="MATH-US-00004" num="00004"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mi>a</mi> <mo>.</mo> </mrow> </mtd> <mtd> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mrow> <mn>2</mn> <mo>⁡</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mrow> <msub> <mi>x</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>x</mi> <mn>2</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>y</mi> <mn>2</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>z</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>z</mi> <mn>2</mn> </msub> </mrow> </mtd> <mtd> <mrow> <mo>-</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>01</mn> </msub> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>12</mn> </msub> </mrow> <mo>+</mo> <mrow> <mn>0.5</mn> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msubsup> <mi>T</mi> <mn>12</mn> <mn>2</mn> </msubsup> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>x</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>x</mi> <mn>3</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>y</mi> <mn>3</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>z</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>z</mi> <mn>3</mn> </msub> </mrow> </mtd> <mtd> <mrow> <mo>-</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>01</mn> </msub> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>13</mn> </msub> </mrow> <mo>+</mo> <mrow> <mn>0.5</mn> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msubsup> <mi>T</mi> <mn>13</mn> <mn>2</mn> </msubsup> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>x</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>x</mi> <mn>4</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>y</mi> <mn>4</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>z</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>z</mi> <mn>4</mn> </msub> </mrow> </mtd> <mtd> <mrow> <mo>-</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>01</mn> </msub> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>14</mn> </msub> </mrow> <mo>+</mo> <mrow> <mn>0.5</mn> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msubsup> <mi>T</mi> <mn>14</mn> <mn>2</mn> </msubsup> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>x</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>x</mi> <mn>5</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>y</mi> <mn>5</mn> </msub> </mrow> </mtd> <mtd> <mrow> <msub> <mi>z</mi> <mn>1</mn> </msub> <mo>-</mo> <msub> <mi>z</mi> <mn>5</mn> </msub> </mrow> </mtd> <mtd> <mrow> <mo>-</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>01</mn> </msub> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>T</mi> <mn>15</mn> </msub> </mrow> <mo>+</mo> <mrow> <mn>0.5</mn> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msubsup> <mi>T</mi> <mn>15</mn> <mn>2</mn> </msubsup> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> <mo>⁡</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mi>u</mi> </mtd> </mtr> <mtr> <mtd> <mi>v</mi> </mtd> </mtr> <mtr> <mtd> <mi>w</mi> </mtd> </mtr> <mtr> <mtd> <msup> <mi>c</mi> <mn>2</mn> </msup> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> <mo>=</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mrow> <msubsup> <mi>R</mi> <mn>1</mn> <mn>2</mn> </msubsup> <mo>·</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msubsup> <mi>R</mi> <mn>1</mn> <mn>2</mn> </msubsup> <mo>·</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msubsup> <mi>R</mi> <mn>1</mn> <mn>2</mn> </msubsup> <mo>·</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msubsup> <mi>R</mi> <mn>1</mn> <mn>2</mn> </msubsup> <mo>·</mo> </mrow> </mtd> </mtr> </mtable> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>9.1</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0146">Although similar results may be obtained by application of more computational efficient processes such as pivotal condensation or Crout's decomposition, the application of Cramer's rule was used to evaluate the first-order determinant in Eq. 3.3 using second-order determinants from Laplace expansion. The final transponder(s) position equations are defined by Eqs. 10.1-8.</p>
    <p num="p-0147">
      <maths id="MATH-US-00005" num="00005"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mi>u</mi> <mo>=</mo> <mfrac> <mrow> <mo></mo> <msub> <mi>A</mi> <mn>2</mn> </msub> <mo></mo> </mrow> <mrow> <mo></mo> <mi>A</mi> <mo></mo> </mrow> </mfrac> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>10.1</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>v</mi> <mo>=</mo> <mfrac> <mrow> <mo></mo> <msub> <mi>A</mi> <mn>2</mn> </msub> <mo></mo> </mrow> <mrow> <mo></mo> <mi>A</mi> <mo></mo> </mrow> </mfrac> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>10.2</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>w</mi> <mo>=</mo> <mfrac> <mrow> <mo></mo> <msub> <mi>A</mi> <mn>3</mn> </msub> <mo></mo> </mrow> <mrow> <mo></mo> <mi>A</mi> <mo></mo> </mrow> </mfrac> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>10.3</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>D</mi> <mn>1</mn> </msub> <mo>=</mo> <mrow> <mrow> <mfrac> <mrow> <mo></mo> <msub> <mi>A</mi> <mn>4</mn> </msub> <mo></mo> </mrow> <mrow> <mo></mo> <mi>A</mi> <mo></mo> </mrow> </mfrac> <mo>⁢</mo> <mrow> <mo>(</mo> <mrow> <mn>1</mn> <mo>⁢</mo> <mi>st</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>method</mi> </mrow> <mo>)</mo> </mrow> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>or</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>c</mi> </mrow> <mo>=</mo> <mrow> <msqrt> <mfrac> <mrow> <mo></mo> <msub> <mi>A</mi> <mn>4</mn> </msub> <mo></mo> </mrow> <mrow> <mo></mo> <mi>A</mi> <mo></mo> </mrow> </mfrac> </msqrt> <mo>⁢</mo> <mrow> <mo>(</mo> <mrow> <mn>2</mn> <mo>⁢</mo> <mi>nd</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>method</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>10.4</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mo></mo> <msub> <mi>A</mi> <mn>1</mn> </msub> <mo></mo> </mrow> <mo>=</mo> <mrow> <mrow> <msub> <mi>b</mi> <mn>1</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>11</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>b</mi> <mn>2</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>21</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>b</mi> <mn>3</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>31</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>b</mi> <mn>4</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>41</mn> </msub> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>10.5</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mo></mo> <msub> <mi>A</mi> <mn>2</mn> </msub> <mo></mo> </mrow> <mo>=</mo> <mrow> <mrow> <msub> <mi>b</mi> <mn>1</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>12</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>b</mi> <mn>2</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>22</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>b</mi> <mn>3</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>32</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>b</mi> <mn>4</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>42</mn> </msub> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>10.6</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mo></mo> <msub> <mi>A</mi> <mn>3</mn> </msub> <mo></mo> </mrow> <mo>=</mo> <mrow> <mrow> <msub> <mi>b</mi> <mn>1</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>13</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>b</mi> <mn>2</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>23</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>b</mi> <mn>3</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>33</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>b</mi> <mn>4</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>43</mn> </msub> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>10.7</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mo></mo> <msub> <mi>A</mi> <mn>4</mn> </msub> <mo></mo> </mrow> <mo>=</mo> <mrow> <mrow> <msub> <mi>b</mi> <mn>1</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>14</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>b</mi> <mn>2</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>24</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>b</mi> <mn>3</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>34</mn> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>b</mi> <mn>4</mn> </msub> <mo>·</mo> <msub> <mi>A</mi> <mn>44</mn> </msub> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>10.8</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0148">If the first method is used, D, the range of the transponder to the reference receiver from Eq. 10.4 may be calculated as a redundant confirmation of the Eqs. 10.1-3 calculations, provided the frame of reference origin and location of the reference receiver are identical or their offsets accounted for. If the second method is used, C, the speed of sound in air, from Eq. 10.4 must be computed every analysis period if its value is anticipated to be used in the first method in the absence of a synchronization signal.</p>
    <p num="p-0149">The orientation of the transponders can be derived from a similar utilization of the above algorithms for a transponder configured with a triad of ultrasonic transmitters. The transducers are preferably arranged in a triangular plane at the transponder of sufficient area for the desired angular resolution. The sequential excitation of each transducer and subsequent calculation of position by the aforementioned methods provides sufficient information to determine orientation by the inverse kinematic calculations of Eqs. 11.1-4, where the analysis is simplified by assuming the origin of rotations occurs about T<sub>1 </sub>and T<sub>123 </sub>represents the initial relative position matrix from this origin and T<sub>123 </sub>is the transformed or forward kinematic position matrix.</p>
    <p num="p-0150">
      <maths id="MATH-US-00006" num="00006"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <msup> <mrow> <msub> <mi>R</mi> <mi>z</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mi>θ</mi> <mo>)</mo> </mrow> </mrow> <mrow> <mo>-</mo> <mn>1</mn> </mrow> </msup> <mo>⁢</mo> <msubsup> <mi>T</mi> <mn>123</mn> <mi>′</mi> </msubsup> </mrow> <mo>=</mo> <mrow> <mrow> <msub> <mi>R</mi> <mi>x</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mi>θ</mi> <mo>)</mo> </mrow> </mrow> <mo>⁢</mo> <mrow> <msub> <mi>R</mi> <mi>y</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mi>θ</mi> <mo>)</mo> </mrow> </mrow> <mo>⁢</mo> <msub> <mi>T</mi> <mn>123</mn> </msub> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>11.1</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mrow> <msub> <mi>R</mi> <mi>x</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mi>θ</mi> <mo>)</mo> </mrow> </mrow> <mo>⁢</mo> <mrow> <msub> <mi>R</mi> <mi>y</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mi>θ</mi> <mo>)</mo> </mrow> </mrow> <mo>⁢</mo> <msub> <mi>T</mi> <mn>123</mn> </msub> </mrow> <mo>≡</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mrow> <mrow> <msub> <mi>x</mi> <mn>2</mn> </msub> <mo>⁢</mo> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>y</mi> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>z</mi> <mn>2</mn> </msub> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>y</mi> </msub> </mrow> </mrow> </mtd> <mtd> <mrow> <msub> <mi>x</mi> <mn>3</mn> </msub> <mo>⁢</mo> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>y</mi> </msub> </mrow> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mrow> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <msub> <mi>θ</mi> <mi>x</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>x</mi> <mn>2</mn> </msub> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>y</mi> </msub> </mrow> <mo>-</mo> <mrow> <msub> <mi>z</mi> <mn>2</mn> </msub> <mo>⁢</mo> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>y</mi> </msub> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <msub> <mi>x</mi> <mn>3</mn> </msub> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>x</mi> </msub> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>y</mi> </msub> </mrow> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mrow> <mrow> <mo>-</mo> <mi>cos</mi> </mrow> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <msub> <mi>θ</mi> <mi>x</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>x</mi> <mn>2</mn> </msub> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>y</mi> </msub> </mrow> <mo>-</mo> <mrow> <msub> <mi>z</mi> <mn>2</mn> </msub> <mo>⁢</mo> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>y</mi> </msub> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>-</mo> <msub> <mi>x</mi> <mn>3</mn> </msub> </mrow> <mo>⁢</mo> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>x</mi> </msub> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>y</mi> </msub> </mrow> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>11.2</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <msup> <mrow> <msub> <mi>R</mi> <mi>z</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mi>θ</mi> <mo>)</mo> </mrow> </mrow> <mrow> <mo>-</mo> <mn>1</mn> </mrow> </msup> <mo>⁢</mo> <msubsup> <mi>T</mi> <mn>123</mn> <mi>′</mi> </msubsup> </mrow> <mo>≡</mo> <mrow> <mo> </mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mrow> <mrow> <msub> <mi>x</mi> <mn>1</mn> </msub> <mo>⁢</mo> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> </mrow> </mtd> <mtd> <mrow> <mrow> <msub> <mi>x</mi> <mn>2</mn> </msub> <mo>⁢</mo> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>y</mi> <mn>2</mn> </msub> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> </mrow> </mtd> <mtd> <mrow> <mrow> <msub> <mi>x</mi> <mn>3</mn> </msub> <mo>⁢</mo> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>y</mi> <mn>3</mn> </msub> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mrow> <mo>-</mo> <msub> <mi>x</mi> <mn>1</mn> </msub> </mrow> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>⁢</mo> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> </mrow> </mtd> <mtd> <mrow> <mrow> <mrow> <mo>-</mo> <msub> <mi>x</mi> <mn>2</mn> </msub> </mrow> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>y</mi> <mn>2</mn> </msub> <mo>⁢</mo> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> </mrow> </mtd> <mtd> <mrow> <mrow> <mrow> <mo>-</mo> <msub> <mi>x</mi> <mn>3</mn> </msub> </mrow> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>y</mi> <mn>3</mn> </msub> <mo>⁢</mo> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <msub> <mi>z</mi> <mn>1</mn> </msub> </mtd> <mtd> <msub> <mi>z</mi> <mn>2</mn> </msub> </mtd> <mtd> <msub> <mi>z</mi> <mn>3</mn> </msub> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>11.3</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mo>∵</mo> <msub> <mi>x</mi> <mn>1</mn> </msub> </mrow> <mo>=</mo> <mrow> <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>=</mo> <mrow> <msub> <mi>z</mi> <mn>1</mn> </msub> <mo>=</mo> <mrow> <mrow> <mn>0</mn> <mo>⋀</mo> <msub> <mi>y</mi> <mn>2</mn> </msub> </mrow> <mo>=</mo> <mrow> <mrow> <mn>0</mn> <mo>⋀</mo> <msub> <mi>y</mi> <mn>3</mn> </msub> </mrow> <mo>=</mo> <mrow> <msub> <mi>z</mi> <mn>3</mn> </msub> <mo>=</mo> <mrow> <mn>0</mn> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>for</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>initial</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>orientation</mi> </mrow> </mrow> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>11.4</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0151">By examination of the matrices element equivalency of Eqs. 11.2-3 and manipulation of terms so that the angles may be found using the inverse tangent function, the following rotation equations Eqs. 12.1-3 are derived,</p>
    <p num="p-0152">
      <maths id="MATH-US-00007" num="00007"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <msub> <mi>θ</mi> <mi>x</mi> </msub> <mo>=</mo> <mrow> <mi>a</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mi>tan</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <mfrac> <mrow> <mrow> <msubsup> <mi>x</mi> <mn>2</mn> <mi>′</mi> </msubsup> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> <mo>-</mo> <mrow> <msubsup> <mi>y</mi> <mn>2</mn> <mi>′</mi> </msubsup> <mo>⁢</mo> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> </mrow> <msubsup> <mi>z</mi> <mn>2</mn> <mi>′</mi> </msubsup> </mfrac> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>12.1</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>θ</mi> <mi>y</mi> </msub> <mo>=</mo> <mrow> <mi>a</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mi>tan</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <mfrac> <mrow> <mo>-</mo> <msubsup> <mi>z</mi> <mn>3</mn> <mi>′</mi> </msubsup> </mrow> <mrow> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <msub> <mi>θ</mi> <mi>z</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>x</mi> <mn>3</mn> </msub> <mo>⁢</mo> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> <mo>+</mo> <mrow> <msub> <mi>y</mi> <mn>3</mn> </msub> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mfrac> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>12.2</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>θ</mi> <mi>z</mi> </msub> <mo>=</mo> <mrow> <mi>a</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mi>tan</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <mfrac> <mrow> <msubsup> <mi>y</mi> <mn>3</mn> <mi>′</mi> </msubsup> <mo>-</mo> <mfrac> <mrow> <msub> <mi>x</mi> <mn>3</mn> </msub> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> <mo>⁢</mo> <mi>sin</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> </mrow> <mrow> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mrow> <mi>z</mi> <mo>-</mo> <mn>1</mn> </mrow> </msub> </mrow> </mfrac> </mrow> <msubsup> <mi>x</mi> <mn>3</mn> <mi>′</mi> </msubsup> </mfrac> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>12.3</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mrow> <mi>z</mi> <mo>-</mo> <mn>1</mn> </mrow> </msub> </mrow> <mo>≡</mo> <mrow> <mi>cos</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msub> <mi>θ</mi> <mi>z</mi> </msub> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>from</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>previous</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>iteration</mi> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>12.4</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>θ</mi> <mi>z</mi> </msub> <mo>=</mo> <mrow> <mi>a</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mrow> <mi>tan</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <mfrac> <msubsup> <mi>y</mi> <mn>3</mn> <mi>′</mi> </msubsup> <msubsup> <mi>x</mi> <mn>3</mn> <mi>′</mi> </msubsup> </mfrac> <mo>)</mo> </mrow> </mrow> <mo>⁢</mo> <mrow> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> </mrow> <mo>⁢</mo> <mi>for</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mn>1</mn> <mo>⁢</mo> <mi>st</mi> <mo>⁢</mo> <mstyle> <mspace width="0.8em" height="0.8ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>iteration</mi> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>12.5</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0153">These calculations are performed through iterative step processes which inherit angular approximations of the preceding steps until the final desired angular accuracy is achieved by assuming the conditions of Eqs. 12.4-5. Therefore the rotation θ<sub>z</sub>, roll, is first approximated by Eq. 12.5; then the rotation θ<sub>x</sub>, pitch, is approximated by Eq. 12.1; and then the final rotation θ<sub>y</sub>, yaw or turn, is approximated by Eq. 12.2. The next approximation of θ<sub>z </sub>utilizes the previous value of θ<sub>z </sub>in Eq. 12.3 and the similar steps are preferably repeated until the desired accuracy is achieved. The transcendental functions may be evaluated through a conventional look-up table or by a power series expansion.</p>
    <p num="p-0154">Preferably, the overall analysis period duration is effectively trebled until the three (3) transducers' positions are calculated, which reduces the system's frequency response and imposes an increased latency effect. Typically, robust absolute orientation processing requires more stringent line-of-sight operation and is reserved for more sensitive, less dynamic, and reduced ROM movement trajectories, e.g., balance and sway. Therefore, the latency effect is less noticeable upon the real-time performance of the sensory interfaces.</p>
    <p num="p-0155">In the preferred embodiment, the interactive hand-held transponders support a dual axis inertial sensor, which is operably configured to provide tilt (pitch and roll) orientation in its horizontal mounting plane. The inertial sensor is mounted in the intended operational horizontal plane with respect to the systems inertial frame of reference. Once the sensors signals has been converted to an acceleration value that varies between +/−1 g the tilt in degrees is calculated as shown in Eqs. 13.1-2, for pitch and roll, respectively.
<br>φ=<i>a </i>sin(<i>A</i> <sub>x</sub>/1g)  (13.1)<br>φ=<i>a </i>sin(<i>A</i> <sub>y</sub>/1g)  (13.2)</p>
    <p num="p-0156">This outside-in ultrasonic tracking implementation, where the transponders are mounted on the mobile object, produces inherent temporal delays due to the finite TOF registration and calculation delays after the transponder has already moved into a different position before the measurement is complete. This overall latency period is compensated and minimized through use of a Kalman filter data processing algorithm to estimate the pose of the transponder by optimally and recursively combining past history, new measurements, and a priori models and information. Generally speaking, the Kalman filter is a digital filter with time-varying gains that are optimally determined through a stochastic dynamical model of the motion. The overall goal is to minimize filter lag while providing sufficient smoothing of the motion data.</p>
    <p num="p-0157">An adaptive, multi dynamic model is developed based upon the kinematic quality of the expected movement trajectory. The predictive kinematic model for the Kalman filter is depicted in matrix form utilizing a truncated 2<sup>nd </sup>order Taylor series expansion as below in Eqs. 14.1-2,</p>
    <p num="p-0158">
      <maths id="MATH-US-00008" num="00008"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <msub> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mi>r</mi> </mtd> </mtr> <mtr> <mtd> <mi>v</mi> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> <mrow> <mi>k</mi> <mo>+</mo> <mn>1</mn> </mrow> </msub> <mo>=</mo> <mrow> <msub> <mrow> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mn>1</mn> </mtd> <mtd> <mrow> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>t</mi> </mrow> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> <mo>⁡</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mi>r</mi> </mtd> </mtr> <mtr> <mtd> <mi>v</mi> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> <mi>k</mi> </msub> <mo>+</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mn>0</mn> </mtd> </mtr> <mtr> <mtd> <mi>w</mi> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>14.1</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mi>r</mi> </mtd> </mtr> <mtr> <mtd> <mi>v</mi> </mtd> </mtr> <mtr> <mtd> <mi>a</mi> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> <mrow> <mi>k</mi> <mo>+</mo> <mn>1</mn> </mrow> </msub> <mo>=</mo> <mrow> <msub> <mrow> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mn>1</mn> </mtd> <mtd> <mrow> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>t</mi> </mrow> </mtd> <mtd> <mrow> <mn>0.5</mn> <mo>⁢</mo> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <msup> <mi>t</mi> <mn>2</mn> </msup> </mrow> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>1</mn> </mtd> <mtd> <mrow> <mi>Δ</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>t</mi> </mrow> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> <mo>⁡</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mi>r</mi> </mtd> </mtr> <mtr> <mtd> <mi>v</mi> </mtd> </mtr> <mtr> <mtd> <mi>a</mi> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> <mi>k</mi> </msub> <mo>+</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mn>0</mn> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> </mtr> <mtr> <mtd> <mi>w</mi> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>14.2</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0159">The Kalman filter is now described for a single dimension, although it is utilized for prediction and smoothing for all position dimensions. The predictor stages consist of the calculation of the state and the error covariance projection equations. The state projector equation, Eq 15.1, utilizes a discrete time-sampled difference equation of r calculated from Eq. 15.2. In other words, the numerically derived velocity and acceleration components of motion are linearly combined with the previously a priori position to estimate the new position. The corrector stages consist of sequential computation of the gain, updated state estimate, and updated error covariance equations. The a posteriori state estimate, Eq. 15.4, is based on a linear combination of the weighted measurement residual and the last state estimate.</p>
    <p num="p-0160">
      <maths id="MATH-US-00009" num="00009"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <msub> <mover> <mover> <mi>x</mi> <mi>︵</mi> </mover> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> </mover> <mover> <mi>k</mi> <mo>~</mo> </mover> </msub> <mo>=</mo> <mrow> <msub> <mi>x</mi> <mrow> <mi>k</mi> <mo>-</mo> <mn>1</mn> </mrow> </msub> <mo>+</mo> <mrow> <mn>0.5</mn> <mo>⁢</mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mrow> <mi>k</mi> <mo>-</mo> <mn>1</mn> </mrow> </msub> <mo>-</mo> <msub> <mi>x</mi> <mrow> <mi>k</mi> <mo>-</mo> <mn>3</mn> </mrow> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>+</mo> <mrow> <mn>0.5</mn> <mo>⁢</mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mrow> <mi>k</mi> <mo>-</mo> <mn>1</mn> </mrow> </msub> <mo>-</mo> <mrow> <mn>2</mn> <mo>⁢</mo> <msub> <mi>x</mi> <mrow> <mi>k</mi> <mo>-</mo> <mn>2</mn> </mrow> </msub> </mrow> <mo>+</mo> <msub> <mi>x</mi> <mrow> <mi>k</mi> <mo>-</mo> <mn>3</mn> </mrow> </msub> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>15.1</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>P</mi> <mover> <mi>k</mi> <mo>~</mo> </mover> </msub> <mo>=</mo> <mrow> <msub> <mi>P</mi> <mrow> <mi>k</mi> <mo>-</mo> <mn>1</mn> </mrow> </msub> <mo>+</mo> <msub> <mi>Q</mi> <mi>k</mi> </msub> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>15.2</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>K</mi> <mi>k</mi> </msub> <mo>=</mo> <mfrac> <msub> <mi>P</mi> <mover> <mi>k</mi> <mo>~</mo> </mover> </msub> <mrow> <mo>(</mo> <mrow> <msub> <mi>P</mi> <mover> <mi>k</mi> <mo>~</mo> </mover> </msub> <mo>+</mo> <msub> <mi>R</mi> <mi>k</mi> </msub> </mrow> <mo>)</mo> </mrow> </mfrac> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>15.3</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mover> <mi>x</mi> <mi>︵</mi> </mover> <mi>k</mi> </msub> <mo>=</mo> <mrow> <msub> <mover> <mi>x</mi> <mi>︵</mi> </mover> <mover> <mi>k</mi> <mo>~</mo> </mover> </msub> <mo>+</mo> <mrow> <msub> <mi>K</mi> <mi>k</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>z</mi> <mrow> <mi>k</mi> <mo>⁢</mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> </mrow> </msub> <mo>-</mo> <msub> <mover> <mi>x</mi> <mi>︵</mi> </mover> <mover> <mi>k</mi> <mo>~</mo> </mover> </msub> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>15.4</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>P</mi> <mi>k</mi> </msub> <mo>=</mo> <mrow> <mrow> <mo>(</mo> <mrow> <mn>1</mn> <mo>-</mo> <msub> <mi>K</mi> <mi>k</mi> </msub> </mrow> <mo>)</mo> </mrow> <mo>⁢</mo> <msub> <mi>P</mi> <mover> <mi>k</mi> <mo>~</mo> </mover> </msub> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>15.5</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0161">The new error covariance projector, Eq. 15.2, is it's previously computed value combined with the current process noise covariance, Q<sub>k</sub>, which is tuned by an example model derived from the measured motion dynamics shown in Eq. 16.1. The gain's measurement noise covariance, R<sub>k</sub>, is defined as a small constant and based upon the actual static timing variance empirically measured. The smaller this value the more confidence there exists in the systems' measurement capability.</p>
    <p num="p-0162">In the preferred embodiment, the product of the numerically-derived 1<sup>st </sup>and 2<sup>nd </sup>order derivatives of the measured position scaled by a frequency dependent gain provides a computationally practical adaptive dynamic process noise estimate model. The derivative product term increases Q<sub>k </sub>proportionally for higher velocity and acceleration components of motion, e.g., quick, abrupt directional changes, which effectively increases the gain and, therefore, means more confidence exists in the measurement rather than the estimate. This provides faithful, low-latency response to high-frequency motions. Conversely, the frequency scaling term decreases the predictive “overshoot” characteristic of lower power, repetitive motion, e.g. slower, cyclic, ROM trajectories, which effectively decreases the gain and, therefore, means more confidence exists in the estimate rather than the measurement. It should be appreciated this filter implementation provides superior tracking fidelity and comparable smoothing characteristics as compared to practical lengths of finite impulse response running-average filters and various low-orders infinite impulse response filters. It achieves enough predictive response to compensate for the inherent TOF and computation latencies, while providing and comparable smoothing properties of other filter types.
<br> <i>Q</i> <sub>k</sub> <i>≡|K</i> <sub>q</sub>[(<i>z</i> <sub>k-1</sub> <i>−z</i> <sub>k-3</sub>)(<i>z</i> <sub>k-1</sub>−2<i>z</i> <sub>k-2</sub> <i>+z</i> <sub>k-3</sub>)sin(<i>z</i> <sub>k-1</sub> <i>−z</i> <sub>k-3</sub>)]|  (16.1)<br>R<sub>k</sub>≡0.005  (16.2)</p>
    <p num="p-0163">In the preferred embodiment, a three dimensional (3D) piecewise cubic curve interpolates a movement trajectory for smoothing and reduced sample storage for greater memory efficiency. Preferably, four (4) sequential discrete control points of the n-length set of control points, the sample resolution dependent upon the desired movement granularity, and corresponding timestamp are needed to calculate in real-time the interpolated position between any pair of control points. A Catmull-Rom spline algorithm is the preferred method in that the path intersects the control points and would best approximate a movement that may have acute directional changes. The Catmull-Rom spline algorithm is defined by Eqs. 17.1-3, where the geometry matrix G<sub>k </sub>represents the matrix of three dimensional (3D) control points.</p>
    <p num="p-0164">
      <maths id="MATH-US-00010" num="00010"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <msub> <mi>C</mi> <mi>k</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mi>μ</mi> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mrow> <msub> <mi>G</mi> <mi>k</mi> </msub> <mo>⁢</mo> <mrow> <mrow> <mfrac> <mn>1</mn> <mn>2</mn> </mfrac> <mo>⁡</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mrow> <mo>-</mo> <mn>1</mn> </mrow> </mtd> <mtd> <mn>2</mn> </mtd> <mtd> <mrow> <mo>-</mo> <mn>1</mn> </mrow> </mtd> </mtr> <mtr> <mtd> <mn>2</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mrow> <mo>-</mo> <mn>5</mn> </mrow> </mtd> <mtd> <mn>3</mn> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>1</mn> </mtd> <mtd> <mn>4</mn> </mtd> <mtd> <mrow> <mo>-</mo> <mn>3</mn> </mrow> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mrow> <mo>-</mo> <mn>1</mn> </mrow> </mtd> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> <mo>⁡</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mn>1</mn> </mtd> </mtr> <mtr> <mtd> <mi>μ</mi> </mtd> </mtr> <mtr> <mtd> <msup> <mi>μ</mi> <mn>2</mn> </msup> </mtd> </mtr> <mtr> <mtd> <msup> <mi>μ</mi> <mn>3</mn> </msup> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>17.1</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <msub> <mi>C</mi> <mi>k</mi> </msub> <mo>⁡</mo> <mrow> <mo>(</mo> <mi>μ</mi> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mrow> <msub> <mi>G</mi> <mi>k</mi> </msub> <mo>⁡</mo> <mrow> <mo>[</mo> <mtable> <mtr> <mtd> <mrow> <mrow> <mrow> <mo>-</mo> <mn>0.5</mn> </mrow> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>μ</mi> </mrow> <mo>+</mo> <msup> <mi>μ</mi> <mn>2</mn> </msup> <mo>-</mo> <mrow> <mn>0.5</mn> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <msup> <mi>μ</mi> <mn>3</mn> </msup> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mn>1</mn> <mo>-</mo> <mrow> <mn>2.5</mn> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <msup> <mi>μ</mi> <mn>2</mn> </msup> </mrow> </mrow> <mo>=</mo> <mrow> <mn>1.5</mn> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <msup> <mi>μ</mi> <mn>3</mn> </msup> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mn>0.5</mn> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <mi>μ</mi> </mrow> <mo>+</mo> <mrow> <mn>2</mn> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <msup> <mi>μ</mi> <mn>2</mn> </msup> </mrow> <mo>-</mo> <mrow> <mn>1.5</mn> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <msup> <mi>μ</mi> <mn>3</mn> </msup> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mrow> <mo>-</mo> <mn>0.5</mn> </mrow> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <msup> <mi>μ</mi> <mn>2</mn> </msup> </mrow> <mo>+</mo> <mrow> <mn>0.5</mn> <mo>⁢</mo> <mstyle> <mspace width="0.6em" height="0.6ex"> </mspace> </mstyle> <mo>⁢</mo> <msup> <mi>μ</mi> <mn>3</mn> </msup> </mrow> </mrow> </mtd> </mtr> </mtable> <mo>]</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>17.2</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>G</mi> <mi>k</mi> </msub> <mo>≡</mo> <mrow> <mo>[</mo> <mrow> <msub> <mi>P</mi> <mrow> <mi>k</mi> <mo>-</mo> <mn>1</mn> </mrow> </msub> <mo>⁢</mo> <msub> <mi>P</mi> <mi>k</mi> </msub> <mo>⁢</mo> <msub> <mi>P</mi> <mrow> <mi>k</mi> <mo>+</mo> <mn>1</mn> </mrow> </msub> <mo>⁢</mo> <msub> <mi>P</mi> <mrow> <mi>k</mi> <mo>+</mo> <mn>2</mn> </mrow> </msub> </mrow> <mo>]</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>17.3</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
    <p num="p-0165">The μ value is normalized and represents the % value between the 2<sup>nd </sup>3<sup>rd </sup>control points. To calculate the interpolated value between the 1<sup>st </sup>and 2<sup>nd </sup>or the n−1<sup>th </sup>and n<sup>th </sup>control points, the value of first control point of the pair and the value of the last control point pair are doubly entered into the geometry matrix, respectively. The appropriate dμ/dt is determined by the desired rate of playback of movement trajectory. To playback at the same rate as the recorded session, and assuming fairly constant velocity, a timestamp should also be saved at each control point registration so that the μ calculation is correctly scaled by the delta time interval. The n-length set of control points would be manually registered by the user pressing a switch or automatically post processed by a sorting method where a control point is registered at the tangents of the trajectory having sufficient magnitude and/or experience sign changes which indicates discontinuous or non-monotonic movement.</p>
    <p num="p-0166">The major functional interfaces of the transponder unit preferably include the sensory interface, transducer interface, processor, and communication interface. The following descriptions of the transponder unit are based upon the dependence flow represented by <figref idrefs="DRAWINGS">FIG. 6</figref>.</p>
    <p num="p-0167">The sensor interface refers to the collective support for the ultrasonic transmitter, heart rate receiver, and accelerometer circuits. The ultrasonic transmitter circuit is preferably gated by a pulse-width modulated (PWM) digital signal at nominally 0.8% duty cycle of the 40 kHz resonant frequency, e.g., a single 250 μs pulse every analysis period, by the processor circuit. The radiated ultrasonic signal strength is controlled by gating a MOSFET transistor switch at a duty cycle which optimally energizes the transducer's series resonant tank circuit for sufficient duration. The resonant circuit's reactive components include an impedance matching inductor, the transducer's intrinsic capacitance, and a small damping resistive load. At resonance, a electrical damped sinusoidal with a potential up to ˜400 V<sub>pk-pk </sub>is developed across the transducer to sufficiently drive it at acoustical power levels practical for the system's intended range of operation. Enabling a lower duty cycle control through means of a software algorithm monitoring the transponders range would effectively lower the transponders power consumption and radiate less ultrasonic acoustic energy for close range operation when signal saturation and clipping is undesirable. Conversely, a higher duty cycle control would radiate greater ultrasonic energy to compensate for less efficient, non-optimal acoustical coupling orientations of the transponder with respect to the receiver constellation. Optionally, two additional transducers may be driven in unison or sequentially from a different transponder assembly to support measurement of absolute rotation about a single or multiple axes, or provide calculated positional redundancy for certain difficult line-of-sight applications.</p>
    <p num="p-0168">The heart rate receiver circuit wirelessly receives a 5 kHz heart rate signal from a POLAR® transmitter belt. The transmitter, worn around the chest, electrically detects the heart beat and starts transmitting a pulse corresponding to each heart beat. The receiver captures the signal and generates a corresponding digital pulse which is received by the timing capture-control circuit of the processor interface. A software algorithm processes the signal with known time-based averaging and an adaptive window filter techniques to remove any extraneous artifact or corruption caused by interfering sources.</p>
    <p num="p-0169">The accelerometer circuit consists of a low cost +/−1.5 g dual axis accelerometer that can measure both dynamic, e.g. vibration, and static, e.g. gravity or tilt, acceleration. If the accelerometer is oriented so both its axes are parallel to the earth's surface it can be used as a two axis tilt sensor with a roll and pitch axis.</p>
    <p num="p-0170">The stimuli interface circuit provides the primary visual sensory interface preferably comprised of a linear array of five (5) bright, white light emitting diodes (LED) and associated drivers. The preferred LED device is a CMD87 manufactured by Chicago Miniature Lamp. These LEDs' intensity is controlled by a white LED driver. The preferred white LED driver device is a MAX1570 manufactured by Maxim. The white LED driver provides a maximum 120 mA constant current source to each LED for optimal uniform luminescence. The drive current can be proportionally regulated through external pulse width modulation (PWM) means from the processor circuit to modulate its brightness level. Additionally, an electronic switch is connected in series to each LED drive to individually control its active state. By simultaneously controlling the PWM duty cycle and active state of each LED, the light strobe can appear to smoothly migrate along the linear array in spite of its discontinuous operation.</p>
    <p num="p-0171">Preferably, the stimuli interface circuit provides the primary aural stimulus by means of a 4 kHz piezo buzzer. The preferred device is SMT-3303-G manufactured by Projects Unlimited. This electro-mechanical buzzer requires an external transistor drive circuit and digital control signal gated at a rate near its resonant frequency. The buzzer inputs are connected to and controlled by PWM means from the processor circuit to provide a gross volume adjustment which is dependent upon the amplitude of the drive signal.</p>
    <p num="p-0172">Additionally, the stimuli interface circuit provides the primary tactile stimulus by means of a vibrator motor. The driver for the vibrator motor enables a 120 mA DC current source to excite the motor armature. The preferred driver device is the MAX1748 manufactured by Maxim. The rotational speed of the motor's armature is controlled by PWM means from the processor circuit.</p>
    <p num="p-0173">The processor circuit preferably receives input from the stimuli interface, sensor interface, and the communication interface and provides controlling signals therein. The preferred processor circuit is the MC9S08 GB60 which is manufactured by Motorola Inc. It is a low-cost, high-performance 8-bit microcontroller device that integrates the specialized hardware circuits into one convenient device. The software calculation engine circuit operates from an embedded 60 KB FLASH for program memory with in-circuit programmable capability and 4 KB RAM for data memory. The time base circuit is preferably comprised of an external, high-noise immunity, 4.0 MHz system clock, which multiplies this value by the internal frequency-locked loop for a bus clock of 40.0 MHz and single instruction execution time of 25 ηS. This clock also provides all the capture and control timing requirements for the other specified circuits. Multiple parallel I/O ports and dedicated asynchronous serial communication signals provide digital control for the circuits of the parallel/serial I/O circuit.</p>
    <p num="p-0174">In the preferred embodiment, the graphic LCD and touch screen circuit is the primary user input device for database management for an interactive transponder configuration. For example, it may be a 128×64 graphical liquid crystal display system (LCD) and associated 4-pin touch screen input device. A preferred LCD device is the 51553 manufactured by Optrex and the preferred touch screen device is the TSG-51 manufactured by Apollo Displays. LCD display information, configuration commands, and bitmaps images can be loaded through the software calculation engine via a parallel memory interface to emulate a graphical user interface. A touch screen input device is connected to a controller circuit to decode soft key presses at areas over the graphical object. Preferably, the key presses are registered, filtered, decoded, and processed by the controller and then transferred to the software calculation engine via an interrupt driven asynchronous serial communication channel of the I/O interface. A preferred LCD controller is the UR7HCTS manufactured by Semtech.</p>
    <p num="p-0175">The timing capture-control circuit provides controlling means for the stimuli interface and portions of the sensor interface. The stimuli interface is preferably comprised of a five channel 16-bit timer PWM module with programmable interrupt control which provides 250 ηs timing resolution to automatically modulate the circuits' drivers through variable duty cycle control.</p>
    <p num="p-0176">In the preferred embodiment, the A/D conversion circuit receives the output from the accelerometer circuit and consists of a two channel 10-bit analog-to-digital converter used determine the rotational angle of roll and pitch in the accelerometer deviates from its horizontal plane orientation. This information is communicated to the signal processor via the radio link.</p>
    <p num="p-0177">In the preferred embodiment, the radio link circuit is comprised of a wireless bi-directional communication interface (with a receiver and transmitter shown generally at <b>20</b> and <b>30</b>) to (1) receive a synchronization signal for control of the transponders interoperability, (2) to transfer acquired local sensor data, including, but not limited to, accelerometer, heart rate, battery, user I/O status, to processor unit and (3) to provide means to configure its local database from command of processor unit. The preferred wireless communication link is based upon the AT86RF211, a highly integrated, low-power FSK transceiver optimized for license-free ISM band operations from 400 MHz to 950 MHz. and manufactured by Atmel. Its key features are described above.</p>
    <p num="p-0178">In the preferred embodiment, the switch I/O circuit uses a SPST push button switch for user input to control the system's operational states, start and stop program execution, and function as feedback input to the program. A preferred device is the KSS231 SPST pushbutton switch manufactured by ITT Industries.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4337049">US4337049</a></td><td class="patent-data-table-td patent-date-value">Jan 9, 1981</td><td class="patent-data-table-td patent-date-value">Jun 29, 1982</td><td class="patent-data-table-td ">Connelly Edward M</td><td class="patent-data-table-td ">Method and system for automated training of manual skills</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4375674">US4375674</a></td><td class="patent-data-table-td patent-date-value">Oct 17, 1980</td><td class="patent-data-table-td patent-date-value">Mar 1, 1983</td><td class="patent-data-table-td ">The United States Of America As Represented By The Administrator Of The National Aeronautics And Space Administration</td><td class="patent-data-table-td ">Kinesimetric method and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4627620">US4627620</a></td><td class="patent-data-table-td patent-date-value">Dec 26, 1984</td><td class="patent-data-table-td patent-date-value">Dec 9, 1986</td><td class="patent-data-table-td ">Yang John P</td><td class="patent-data-table-td ">Electronic athlete trainer for improving skills in reflex, speed and accuracy</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4631676">US4631676</a></td><td class="patent-data-table-td patent-date-value">May 25, 1983</td><td class="patent-data-table-td patent-date-value">Dec 23, 1986</td><td class="patent-data-table-td ">Hospital For Joint Diseases Or</td><td class="patent-data-table-td ">Computerized video gait and motion analysis system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4645458">US4645458</a></td><td class="patent-data-table-td patent-date-value">Apr 15, 1985</td><td class="patent-data-table-td patent-date-value">Feb 24, 1987</td><td class="patent-data-table-td ">Harald Phillip</td><td class="patent-data-table-td ">Athletic evaluation and training apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4695953">US4695953</a></td><td class="patent-data-table-td patent-date-value">Apr 14, 1986</td><td class="patent-data-table-td patent-date-value">Sep 22, 1987</td><td class="patent-data-table-td ">Blair Preston E</td><td class="patent-data-table-td ">TV animation interactively controlled by the viewer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4702475">US4702475</a></td><td class="patent-data-table-td patent-date-value">Jul 25, 1986</td><td class="patent-data-table-td patent-date-value">Oct 27, 1987</td><td class="patent-data-table-td ">Innovating Training Products, Inc.</td><td class="patent-data-table-td ">Sports technique and reaction training system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4751642">US4751642</a></td><td class="patent-data-table-td patent-date-value">Aug 29, 1986</td><td class="patent-data-table-td patent-date-value">Jun 14, 1988</td><td class="patent-data-table-td ">Silva John M</td><td class="patent-data-table-td ">Interactive sports simulation system with physiological sensing and psychological conditioning</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4787051">US4787051</a></td><td class="patent-data-table-td patent-date-value">May 16, 1986</td><td class="patent-data-table-td patent-date-value">Nov 22, 1988</td><td class="patent-data-table-td ">Tektronix, Inc.</td><td class="patent-data-table-td ">Inertial mouse system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4817950">US4817950</a></td><td class="patent-data-table-td patent-date-value">May 8, 1987</td><td class="patent-data-table-td patent-date-value">Apr 4, 1989</td><td class="patent-data-table-td ">Goo Paul E</td><td class="patent-data-table-td ">Video game control unit and attitude sensor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4912638">US4912638</a></td><td class="patent-data-table-td patent-date-value">Dec 23, 1988</td><td class="patent-data-table-td patent-date-value">Mar 27, 1990</td><td class="patent-data-table-td ">Pratt Jr G Andrew</td><td class="patent-data-table-td ">Biofeedback lifting monitor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4925189">US4925189</a></td><td class="patent-data-table-td patent-date-value">Jan 13, 1989</td><td class="patent-data-table-td patent-date-value">May 15, 1990</td><td class="patent-data-table-td ">Braeunig Thomas F</td><td class="patent-data-table-td ">Body-mounted video game exercise device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4988981">US4988981</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 1989</td><td class="patent-data-table-td patent-date-value">Jan 29, 1991</td><td class="patent-data-table-td ">Vpl Research, Inc.</td><td class="patent-data-table-td ">Computer data entry and manipulation apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5148154">US5148154</a></td><td class="patent-data-table-td patent-date-value">Dec 4, 1990</td><td class="patent-data-table-td patent-date-value">Sep 15, 1992</td><td class="patent-data-table-td ">Sony Corporation Of America</td><td class="patent-data-table-td ">Multi-dimensional user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5184295">US5184295</a></td><td class="patent-data-table-td patent-date-value">Oct 16, 1989</td><td class="patent-data-table-td patent-date-value">Feb 2, 1993</td><td class="patent-data-table-td ">Mann Ralph V</td><td class="patent-data-table-td ">System and method for teaching physical skills</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5214615">US5214615</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 1991</td><td class="patent-data-table-td patent-date-value">May 25, 1993</td><td class="patent-data-table-td ">Will Bauer</td><td class="patent-data-table-td ">Three-dimensional displacement of a body with computer interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5227985">US5227985</a></td><td class="patent-data-table-td patent-date-value">Aug 19, 1991</td><td class="patent-data-table-td patent-date-value">Jul 13, 1993</td><td class="patent-data-table-td ">University Of Maryland</td><td class="patent-data-table-td ">Computer vision system for position monitoring in three dimensions using non-coplanar light sources attached to a monitored object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5229756">US5229756</a></td><td class="patent-data-table-td patent-date-value">May 14, 1992</td><td class="patent-data-table-td patent-date-value">Jul 20, 1993</td><td class="patent-data-table-td ">Yamaha Corporation</td><td class="patent-data-table-td ">Image control apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5239463">US5239463</a></td><td class="patent-data-table-td patent-date-value">Dec 9, 1991</td><td class="patent-data-table-td patent-date-value">Aug 24, 1993</td><td class="patent-data-table-td ">Blair Preston E</td><td class="patent-data-table-td ">Method and apparatus for player interaction with animated characters and objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5255211">US5255211</a></td><td class="patent-data-table-td patent-date-value">Feb 22, 1990</td><td class="patent-data-table-td patent-date-value">Oct 19, 1993</td><td class="patent-data-table-td ">Redmond Productions, Inc.</td><td class="patent-data-table-td ">Methods and apparatus for generating and processing synthetic and absolute real time environments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5288078">US5288078</a></td><td class="patent-data-table-td patent-date-value">Jul 16, 1992</td><td class="patent-data-table-td patent-date-value">Feb 22, 1994</td><td class="patent-data-table-td ">David G. Capper</td><td class="patent-data-table-td ">Control interface apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5296871">US5296871</a></td><td class="patent-data-table-td patent-date-value">Jul 27, 1992</td><td class="patent-data-table-td patent-date-value">Mar 22, 1994</td><td class="patent-data-table-td ">Paley W Bradford</td><td class="patent-data-table-td ">Three-dimensional mouse with tactile feedback</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5320538">US5320538</a></td><td class="patent-data-table-td patent-date-value">Sep 23, 1992</td><td class="patent-data-table-td patent-date-value">Jun 14, 1994</td><td class="patent-data-table-td ">Hughes Training, Inc.</td><td class="patent-data-table-td ">Interactive aircraft training system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5347306">US5347306</a></td><td class="patent-data-table-td patent-date-value">Dec 17, 1993</td><td class="patent-data-table-td patent-date-value">Sep 13, 1994</td><td class="patent-data-table-td ">Mitsubishi Electric Research Laboratories, Inc.</td><td class="patent-data-table-td ">Animated electronic meeting place</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5372365">US5372365</a></td><td class="patent-data-table-td patent-date-value">Nov 12, 1992</td><td class="patent-data-table-td patent-date-value">Dec 13, 1994</td><td class="patent-data-table-td ">Sportsense, Inc.</td><td class="patent-data-table-td ">Methods and apparatus for sports training</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5375610">US5375610</a></td><td class="patent-data-table-td patent-date-value">Apr 28, 1992</td><td class="patent-data-table-td patent-date-value">Dec 27, 1994</td><td class="patent-data-table-td ">University Of New Hampshire</td><td class="patent-data-table-td ">Apparatus for the functional assessment of human activity</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5385519">US5385519</a></td><td class="patent-data-table-td patent-date-value">Apr 19, 1994</td><td class="patent-data-table-td patent-date-value">Jan 31, 1995</td><td class="patent-data-table-td ">Hsu; Chi-Hsueh</td><td class="patent-data-table-td ">Running machine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5405152">US5405152</a></td><td class="patent-data-table-td patent-date-value">Jun 8, 1993</td><td class="patent-data-table-td patent-date-value">Apr 11, 1995</td><td class="patent-data-table-td ">The Walt Disney Company</td><td class="patent-data-table-td ">Method and apparatus for an interactive video game with physical feedback</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5421590">US5421590</a></td><td class="patent-data-table-td patent-date-value">Jul 23, 1993</td><td class="patent-data-table-td patent-date-value">Jun 6, 1995</td><td class="patent-data-table-td ">Commodore Electronics Limited</td><td class="patent-data-table-td ">For use with an electronic game device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5423554">US5423554</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 1993</td><td class="patent-data-table-td patent-date-value">Jun 13, 1995</td><td class="patent-data-table-td ">Metamedia Ventures, Inc.</td><td class="patent-data-table-td ">Virtual reality game method and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5429140">US5429140</a></td><td class="patent-data-table-td patent-date-value">Jun 4, 1993</td><td class="patent-data-table-td patent-date-value">Jul 4, 1995</td><td class="patent-data-table-td ">Greenleaf Medical Systems, Inc.</td><td class="patent-data-table-td ">Integrated virtual reality rehabilitation system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5466200">US5466200</a></td><td class="patent-data-table-td patent-date-value">Feb 1, 1994</td><td class="patent-data-table-td patent-date-value">Nov 14, 1995</td><td class="patent-data-table-td ">Cybergear, Inc.</td><td class="patent-data-table-td ">Interactive exercise apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5469740">US5469740</a></td><td class="patent-data-table-td patent-date-value">Dec 2, 1992</td><td class="patent-data-table-td patent-date-value">Nov 28, 1995</td><td class="patent-data-table-td ">Impulse Technology, Inc.</td><td class="patent-data-table-td ">Interactive video testing and training system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5474083">US5474083</a></td><td class="patent-data-table-td patent-date-value">Nov 15, 1993</td><td class="patent-data-table-td patent-date-value">Dec 12, 1995</td><td class="patent-data-table-td ">Empi, Inc.</td><td class="patent-data-table-td ">Lifting monitoring and exercise training system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5485402">US5485402</a></td><td class="patent-data-table-td patent-date-value">Mar 21, 1994</td><td class="patent-data-table-td patent-date-value">Jan 16, 1996</td><td class="patent-data-table-td ">Prosthetics Research Study</td><td class="patent-data-table-td ">Gait activity monitor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5495576">US5495576</a></td><td class="patent-data-table-td patent-date-value">Jan 11, 1993</td><td class="patent-data-table-td patent-date-value">Feb 27, 1996</td><td class="patent-data-table-td ">Ritchey; Kurtis J.</td><td class="patent-data-table-td ">Panoramic image based virtual reality/telepresence audio-visual system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5506605">US5506605</a></td><td class="patent-data-table-td patent-date-value">Jan 26, 1994</td><td class="patent-data-table-td patent-date-value">Apr 9, 1996</td><td class="patent-data-table-td ">Paley; W. Bradford</td><td class="patent-data-table-td ">Three-dimensional mouse with tactile feedback</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5516105">US5516105</a></td><td class="patent-data-table-td patent-date-value">Oct 6, 1994</td><td class="patent-data-table-td patent-date-value">May 14, 1996</td><td class="patent-data-table-td ">Exergame, Inc.</td><td class="patent-data-table-td ">For providing input signals to a video game controller</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5524637">US5524637</a></td><td class="patent-data-table-td patent-date-value">Jun 29, 1994</td><td class="patent-data-table-td patent-date-value">Jun 11, 1996</td><td class="patent-data-table-td ">Erickson; Jon W.</td><td class="patent-data-table-td ">Interactive system for measuring physiological exertion</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5554033">US5554033</a></td><td class="patent-data-table-td patent-date-value">Jul 1, 1994</td><td class="patent-data-table-td patent-date-value">Sep 10, 1996</td><td class="patent-data-table-td ">Massachusetts Institute Of Technology</td><td class="patent-data-table-td ">System for human trajectory learning in virtual environments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5574479">US5574479</a></td><td class="patent-data-table-td patent-date-value">Jan 7, 1994</td><td class="patent-data-table-td patent-date-value">Nov 12, 1996</td><td class="patent-data-table-td ">Selectech, Ltd.</td><td class="patent-data-table-td ">Optical system for determining the roll orientation of a remote unit relative to a base unit</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5577981">US5577981</a></td><td class="patent-data-table-td patent-date-value">Aug 4, 1995</td><td class="patent-data-table-td patent-date-value">Nov 26, 1996</td><td class="patent-data-table-td ">Jarvik; Robert</td><td class="patent-data-table-td ">Virtual reality exercise machine and computer controlled video system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5580249">US5580249</a></td><td class="patent-data-table-td patent-date-value">Feb 14, 1994</td><td class="patent-data-table-td patent-date-value">Dec 3, 1996</td><td class="patent-data-table-td ">Sarcos Group</td><td class="patent-data-table-td ">Apparatus for simulating mobility of a human</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5584700">US5584700</a></td><td class="patent-data-table-td patent-date-value">Dec 19, 1994</td><td class="patent-data-table-td patent-date-value">Dec 17, 1996</td><td class="patent-data-table-td ">Advanced Technology And Research Corporation</td><td class="patent-data-table-td ">Virtual-reality based flycycle exercise machine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5587937">US5587937</a></td><td class="patent-data-table-td patent-date-value">Apr 25, 1995</td><td class="patent-data-table-td patent-date-value">Dec 24, 1996</td><td class="patent-data-table-td ">Massachusetts Institute Of Technology</td><td class="patent-data-table-td ">Force reflecting haptic interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5591104">US5591104</a></td><td class="patent-data-table-td patent-date-value">Jan 27, 1993</td><td class="patent-data-table-td patent-date-value">Jan 7, 1997</td><td class="patent-data-table-td ">Life Fitness</td><td class="patent-data-table-td ">Physical exercise video system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5597309">US5597309</a></td><td class="patent-data-table-td patent-date-value">Mar 28, 1994</td><td class="patent-data-table-td patent-date-value">Jan 28, 1997</td><td class="patent-data-table-td ">Riess; Thomas</td><td class="patent-data-table-td ">Method and apparatus for treatment of gait problems associated with parkinson&#39;s disease</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5602569">US5602569</a></td><td class="patent-data-table-td patent-date-value">Apr 24, 1995</td><td class="patent-data-table-td patent-date-value">Feb 11, 1997</td><td class="patent-data-table-td ">Nintendo Co., Ltd.</td><td class="patent-data-table-td ">Controller for image processing apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5605505">US5605505</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 24, 1995</td><td class="patent-data-table-td patent-date-value">Feb 25, 1997</td><td class="patent-data-table-td ">Lg Electronics Co., Ltd.</td><td class="patent-data-table-td ">Two-player game playing apparatus using wireless remote controllers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5616078">US5616078</a></td><td class="patent-data-table-td patent-date-value">Dec 27, 1994</td><td class="patent-data-table-td patent-date-value">Apr 1, 1997</td><td class="patent-data-table-td ">Konami Co., Ltd.</td><td class="patent-data-table-td ">Motion-controlled video entertainment system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5638300">US5638300</a></td><td class="patent-data-table-td patent-date-value">Dec 5, 1994</td><td class="patent-data-table-td patent-date-value">Jun 10, 1997</td><td class="patent-data-table-td ">Johnson; Lee E.</td><td class="patent-data-table-td ">Golf swing analysis system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5641288">US5641288</a></td><td class="patent-data-table-td patent-date-value">Jan 11, 1996</td><td class="patent-data-table-td patent-date-value">Jun 24, 1997</td><td class="patent-data-table-td ">Zaenglein, Jr.; William G.</td><td class="patent-data-table-td ">Shooting simulating process and training device using a virtual reality display screen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5645077">US5645077</a></td><td class="patent-data-table-td patent-date-value">Jun 16, 1994</td><td class="patent-data-table-td patent-date-value">Jul 8, 1997</td><td class="patent-data-table-td ">Massachusetts Institute Of Technology</td><td class="patent-data-table-td ">Inertial orientation tracker apparatus having automatic drift compensation for tracking human head and other similarly sized body</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5656904">US5656904</a></td><td class="patent-data-table-td patent-date-value">Sep 28, 1995</td><td class="patent-data-table-td patent-date-value">Aug 12, 1997</td><td class="patent-data-table-td ">Lander; Ralph</td><td class="patent-data-table-td ">Movement monitoring and control apparatus for body members</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5659691">US5659691</a></td><td class="patent-data-table-td patent-date-value">Sep 23, 1993</td><td class="patent-data-table-td patent-date-value">Aug 19, 1997</td><td class="patent-data-table-td ">Virtual Universe Corporation</td><td class="patent-data-table-td ">Virtual reality network with selective distribution and updating of data to reduce bandwidth requirements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5679004">US5679004</a></td><td class="patent-data-table-td patent-date-value">Dec 7, 1995</td><td class="patent-data-table-td patent-date-value">Oct 21, 1997</td><td class="patent-data-table-td ">Movit, Inc.</td><td class="patent-data-table-td ">Myoelectric feedback system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5702323">US5702323</a></td><td class="patent-data-table-td patent-date-value">Jul 26, 1995</td><td class="patent-data-table-td patent-date-value">Dec 30, 1997</td><td class="patent-data-table-td ">Poulton; Craig K.</td><td class="patent-data-table-td ">Electronic exercise enhancer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5703623">US5703623</a></td><td class="patent-data-table-td patent-date-value">Jan 24, 1996</td><td class="patent-data-table-td patent-date-value">Dec 30, 1997</td><td class="patent-data-table-td ">Hall; Malcolm G.</td><td class="patent-data-table-td ">Smart orientation sensing circuit for remote control</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5704837">US5704837</a></td><td class="patent-data-table-td patent-date-value">Mar 25, 1994</td><td class="patent-data-table-td patent-date-value">Jan 6, 1998</td><td class="patent-data-table-td ">Namco Ltd.</td><td class="patent-data-table-td ">Video game steering system causing translation, rotation and curvilinear motion on the object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5711304">US5711304</a></td><td class="patent-data-table-td patent-date-value">Jul 29, 1996</td><td class="patent-data-table-td patent-date-value">Jan 27, 1998</td><td class="patent-data-table-td ">Dower; Gordon Ewbank</td><td class="patent-data-table-td ">Signal processing apparatus and method for adding additional chest leads to the 12-lead electrocardiogram without additional electrodes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5715834">US5715834</a></td><td class="patent-data-table-td patent-date-value">May 16, 1995</td><td class="patent-data-table-td patent-date-value">Feb 10, 1998</td><td class="patent-data-table-td ">Scuola Superiore Di Studi Universitari &amp; Di Perfezionamento S. Anna</td><td class="patent-data-table-td ">Device for monitoring the configuration of a distal physiological unit for use, in particular, as an advanced interface for machine and computers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5720619">US5720619</a></td><td class="patent-data-table-td patent-date-value">Apr 24, 1995</td><td class="patent-data-table-td patent-date-value">Feb 24, 1998</td><td class="patent-data-table-td ">Fisslinger; Johannes</td><td class="patent-data-table-td ">Interactive computer assisted multi-media biofeedback system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5741182">US5741182</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 17, 1994</td><td class="patent-data-table-td patent-date-value">Apr 21, 1998</td><td class="patent-data-table-td ">Sports Sciences, Inc.</td><td class="patent-data-table-td ">For providing/responding to electromagnetic radiation or other energy</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5759044">US5759044</a></td><td class="patent-data-table-td patent-date-value">Jul 6, 1995</td><td class="patent-data-table-td patent-date-value">Jun 2, 1998</td><td class="patent-data-table-td ">Redmond Productions</td><td class="patent-data-table-td ">Methods and apparatus for generating and processing synthetic and absolute real time environments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5785630">US5785630</a></td><td class="patent-data-table-td patent-date-value">Nov 6, 1996</td><td class="patent-data-table-td patent-date-value">Jul 28, 1998</td><td class="patent-data-table-td ">Tectrix Fitness Equipment, Inc.</td><td class="patent-data-table-td ">Interactive exercise apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5785631">US5785631</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 1994</td><td class="patent-data-table-td patent-date-value">Jul 28, 1998</td><td class="patent-data-table-td ">W.A.Y.S.S. Inc.</td><td class="patent-data-table-td ">Exercise device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5790076">US5790076</a></td><td class="patent-data-table-td patent-date-value">Aug 28, 1996</td><td class="patent-data-table-td patent-date-value">Aug 4, 1998</td><td class="patent-data-table-td ">Sypniewski; Jozef</td><td class="patent-data-table-td ">Tracking sensor specially for computer applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5790124">US5790124</a></td><td class="patent-data-table-td patent-date-value">Nov 20, 1995</td><td class="patent-data-table-td patent-date-value">Aug 4, 1998</td><td class="patent-data-table-td ">Silicon Graphics, Inc.</td><td class="patent-data-table-td ">System and method for allowing a performer to control and interact with an on-stage display device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5792031">US5792031</a></td><td class="patent-data-table-td patent-date-value">Jul 24, 1997</td><td class="patent-data-table-td patent-date-value">Aug 11, 1998</td><td class="patent-data-table-td ">Alton; Michael J.</td><td class="patent-data-table-td ">Human activity simulator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5796354">US5796354</a></td><td class="patent-data-table-td patent-date-value">Feb 7, 1997</td><td class="patent-data-table-td patent-date-value">Aug 18, 1998</td><td class="patent-data-table-td ">Reality Quest Corp.</td><td class="patent-data-table-td ">Hand-attachable controller with direction sensing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5812257">US5812257</a></td><td class="patent-data-table-td patent-date-value">Oct 6, 1993</td><td class="patent-data-table-td patent-date-value">Sep 22, 1998</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Absolute position tracker</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5830065">US5830065</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 14, 1996</td><td class="patent-data-table-td patent-date-value">Nov 3, 1998</td><td class="patent-data-table-td ">Sitrick; David H.</td><td class="patent-data-table-td ">User image integration into audiovisual presentation system and methodology</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5835077">US5835077</a></td><td class="patent-data-table-td patent-date-value">Mar 15, 1996</td><td class="patent-data-table-td patent-date-value">Nov 10, 1998</td><td class="patent-data-table-td ">Remec, Inc.,</td><td class="patent-data-table-td ">For inputting information to a computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5838816">US5838816</a></td><td class="patent-data-table-td patent-date-value">Feb 7, 1996</td><td class="patent-data-table-td patent-date-value">Nov 17, 1998</td><td class="patent-data-table-td ">Hughes Electronics</td><td class="patent-data-table-td ">Pattern recognition system providing automated techniques for training classifiers for non stationary elements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5846086">US5846086</a></td><td class="patent-data-table-td patent-date-value">Feb 13, 1996</td><td class="patent-data-table-td patent-date-value">Dec 8, 1998</td><td class="patent-data-table-td ">Massachusetts Institute Of Technology</td><td class="patent-data-table-td ">System for human trajectory learning in virtual environments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5850201">US5850201</a></td><td class="patent-data-table-td patent-date-value">Oct 6, 1994</td><td class="patent-data-table-td patent-date-value">Dec 15, 1998</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Low cost virtual reality system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5872438">US5872438</a></td><td class="patent-data-table-td patent-date-value">Jul 1, 1997</td><td class="patent-data-table-td patent-date-value">Feb 16, 1999</td><td class="patent-data-table-td ">Cybernet Systems Corporation</td><td class="patent-data-table-td ">Whole-body kinesthetic display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5875257">US5875257</a></td><td class="patent-data-table-td patent-date-value">Mar 7, 1997</td><td class="patent-data-table-td patent-date-value">Feb 23, 1999</td><td class="patent-data-table-td ">Massachusetts Institute Of Technology</td><td class="patent-data-table-td ">Apparatus for controlling continuous behavior through hand and arm gestures</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5888172">US5888172</a></td><td class="patent-data-table-td patent-date-value">Jul 1, 1997</td><td class="patent-data-table-td patent-date-value">Mar 30, 1999</td><td class="patent-data-table-td ">Brunswick Corporation</td><td class="patent-data-table-td ">Physical exercise video system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5890995">US5890995</a></td><td class="patent-data-table-td patent-date-value">Feb 4, 1998</td><td class="patent-data-table-td patent-date-value">Apr 6, 1999</td><td class="patent-data-table-td ">Tectrix Fitness Equipment, Inc.</td><td class="patent-data-table-td ">Interactive exercise apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5897437">US5897437</a></td><td class="patent-data-table-td patent-date-value">Oct 8, 1996</td><td class="patent-data-table-td patent-date-value">Apr 27, 1999</td><td class="patent-data-table-td ">Nintendo Co., Ltd.</td><td class="patent-data-table-td ">Controller pack</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5913727">US5913727</a></td><td class="patent-data-table-td patent-date-value">Jun 13, 1997</td><td class="patent-data-table-td patent-date-value">Jun 22, 1999</td><td class="patent-data-table-td ">Ahdoot; Ned</td><td class="patent-data-table-td ">Interactive movement and contact simulation game</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5919149">US5919149</a></td><td class="patent-data-table-td patent-date-value">Mar 14, 1997</td><td class="patent-data-table-td patent-date-value">Jul 6, 1999</td><td class="patent-data-table-td ">Allum; John H.</td><td class="patent-data-table-td ">Monitoring system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5929782">US5929782</a></td><td class="patent-data-table-td patent-date-value">Feb 15, 1995</td><td class="patent-data-table-td patent-date-value">Jul 27, 1999</td><td class="patent-data-table-td ">Stark; John G.</td><td class="patent-data-table-td ">Communication system for an instrumented orthopedic restraining device and method therefor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5963891">US5963891</a></td><td class="patent-data-table-td patent-date-value">Apr 24, 1997</td><td class="patent-data-table-td patent-date-value">Oct 5, 1999</td><td class="patent-data-table-td ">Modern Cartoons, Ltd.</td><td class="patent-data-table-td ">System for tracking body movements in a virtual reality system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5986644">US5986644</a></td><td class="patent-data-table-td patent-date-value">Jul 30, 1997</td><td class="patent-data-table-td patent-date-value">Nov 16, 1999</td><td class="patent-data-table-td ">Selectech, Ltd.</td><td class="patent-data-table-td ">Remote control system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5989157">US5989157</a></td><td class="patent-data-table-td patent-date-value">Jul 11, 1997</td><td class="patent-data-table-td patent-date-value">Nov 23, 1999</td><td class="patent-data-table-td ">Walton; Charles A.</td><td class="patent-data-table-td ">Exercising system with electronic inertial game playing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6004243">US6004243</a></td><td class="patent-data-table-td patent-date-value">Oct 4, 1996</td><td class="patent-data-table-td patent-date-value">Dec 21, 1999</td><td class="patent-data-table-td ">Ewert; Bruce</td><td class="patent-data-table-td ">Dynamic real time exercise video apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6013007">US6013007</a></td><td class="patent-data-table-td patent-date-value">Mar 26, 1998</td><td class="patent-data-table-td patent-date-value">Jan 11, 2000</td><td class="patent-data-table-td ">Liquid Spark, Llc</td><td class="patent-data-table-td ">Athlete&#39;s GPS-based performance monitor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6028593">US6028593</a></td><td class="patent-data-table-td patent-date-value">Jun 14, 1996</td><td class="patent-data-table-td patent-date-value">Feb 22, 2000</td><td class="patent-data-table-td ">Immersion Corporation</td><td class="patent-data-table-td ">Method and apparatus for providing simulated physical interactions within computer generated environments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6043873">US6043873</a></td><td class="patent-data-table-td patent-date-value">Jan 10, 1997</td><td class="patent-data-table-td patent-date-value">Mar 28, 2000</td><td class="patent-data-table-td ">Advanced Optical Technologies, Llc</td><td class="patent-data-table-td ">Position tracking system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6050822">US6050822</a></td><td class="patent-data-table-td patent-date-value">Oct 1, 1997</td><td class="patent-data-table-td patent-date-value">Apr 18, 2000</td><td class="patent-data-table-td ">The United States Of America As Represented By The Secretary Of The Army</td><td class="patent-data-table-td ">Electromagnetic locomotion platform for translation and total immersion of humans into virtual environments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6050963">US6050963</a></td><td class="patent-data-table-td patent-date-value">Jun 18, 1998</td><td class="patent-data-table-td patent-date-value">Apr 18, 2000</td><td class="patent-data-table-td ">Innovative Sports Training, Inc.</td><td class="patent-data-table-td ">System for analyzing the motion of lifting an object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6054951">US6054951</a></td><td class="patent-data-table-td patent-date-value">Jun 2, 1997</td><td class="patent-data-table-td patent-date-value">Apr 25, 2000</td><td class="patent-data-table-td ">Sypniewski; Jozef</td><td class="patent-data-table-td ">Multi-dimensional tracking sensor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6059576">US6059576</a></td><td class="patent-data-table-td patent-date-value">Nov 21, 1997</td><td class="patent-data-table-td patent-date-value">May 9, 2000</td><td class="patent-data-table-td ">Brann; Theodore L.</td><td class="patent-data-table-td ">Training and safety device, system and method to aid in proper movement during physical activity</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6066075">US6066075</a></td><td class="patent-data-table-td patent-date-value">Dec 29, 1997</td><td class="patent-data-table-td patent-date-value">May 23, 2000</td><td class="patent-data-table-td ">Poulton; Craig K.</td><td class="patent-data-table-td ">Direct feedback controller for user interaction</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6073489">US6073489</a></td><td class="patent-data-table-td patent-date-value">Mar 3, 1998</td><td class="patent-data-table-td patent-date-value">Jun 13, 2000</td><td class="patent-data-table-td ">French; Barry J.</td><td class="patent-data-table-td ">Testing and training system for assessing the ability of a player to complete a task</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6077201">US6077201</a></td><td class="patent-data-table-td patent-date-value">Jun 12, 1998</td><td class="patent-data-table-td patent-date-value">Jun 20, 2000</td><td class="patent-data-table-td ">Cheng; Chau-Yang</td><td class="patent-data-table-td ">Exercise bicycle</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6088091">US6088091</a></td><td class="patent-data-table-td patent-date-value">May 12, 1998</td><td class="patent-data-table-td patent-date-value">Jul 11, 2000</td><td class="patent-data-table-td ">Advanced Optical Technologies, Llc</td><td class="patent-data-table-td ">Position tracking system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6098458">US6098458</a></td><td class="patent-data-table-td patent-date-value">Nov 6, 1995</td><td class="patent-data-table-td patent-date-value">Aug 8, 2000</td><td class="patent-data-table-td ">Impulse Technology, Ltd.</td><td class="patent-data-table-td ">Testing and training system for assessing movement and agility skills without a confining field</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6100896">US6100896</a></td><td class="patent-data-table-td patent-date-value">Mar 24, 1997</td><td class="patent-data-table-td patent-date-value">Aug 8, 2000</td><td class="patent-data-table-td ">Mitsubishi Electric Information Technology Center America, Inc.</td><td class="patent-data-table-td ">System for designing graphical multi-participant environments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6119516">US6119516</a></td><td class="patent-data-table-td patent-date-value">May 22, 1998</td><td class="patent-data-table-td patent-date-value">Sep 19, 2000</td><td class="patent-data-table-td ">Advantedge Systems, Inc.</td><td class="patent-data-table-td ">Biofeedback system for monitoring the motion of body joint</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6238289">US6238289</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 10, 2000</td><td class="patent-data-table-td patent-date-value">May 29, 2001</td><td class="patent-data-table-td ">Eleven Engineering Inc.</td><td class="patent-data-table-td ">Radio frequency game controller</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6554707">US6554707</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 24, 1999</td><td class="patent-data-table-td patent-date-value">Apr 29, 2003</td><td class="patent-data-table-td ">Nokia Corporation</td><td class="patent-data-table-td ">Interactive voice, wireless game system using predictive command input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6585596">US6585596</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 8, 2000</td><td class="patent-data-table-td patent-date-value">Jul 1, 2003</td><td class="patent-data-table-td ">Arista Enterprises Inc.</td><td class="patent-data-table-td ">Wireless game control units</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="MEMS+enable+smart+golf+clubs"'>MEMS enable smart golf clubs</a>", Small Times, Jan. 6, 2005, 2 pages.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Achenbach, James, "<a href='http://scholar.google.com/scholar?q="Golfs+New+Measuring+Stick"'>Golfs New Measuring Stick</a>", Golfweek, Jun. 11, 2005, www.golfweek.com.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Allard, P., et al, Three-Dimensional Analysis of Human Movement, Human Kinetics (1995) 3, 8-14.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Analog Devices Data Sheet, "<a href='http://scholar.google.com/scholar?q="MicroConverter%C2%AE%2C+Multichannel+12-Bit+ADC+with+Embedded+Flash+MCU%2C+ADuC812"'>MicroConverter®, Multichannel 12-Bit ADC with Embedded Flash MCU, ADuC812</a>" (2003) (http://www.analog.com/static/imported-files/data-sheets/ADUC812.pdf) 60 pages.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Analog Devices Data Sheet, "<a href='http://scholar.google.com/scholar?q="MicroConverter%C2%AE%2C+Multichannel+12-Bit+ADC+with+Embedded+Flash+MCU%2C+ADuC812"'>MicroConverter®, Multichannel 12-Bit ADC with Embedded Flash MCU, ADuC812</a>" (2003) (http://www.analog.com/static/imported-files/data—sheets/ADUC812.pdf) 60 pages.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bachman, E.R., Duman, I., Usta, U.Y., McGhee, R.B., Yun, X.P., Zyda, M.J.,"<a href='http://scholar.google.com/scholar?q="Orientation+Tracking+for+Humans+and+Robots+Using+Inertial+Sensors%2C"'>Orientation Tracking for Humans and Robots Using Inertial Sensors,</a>" 1999 IEEE International Symposium on Computational Intelligence in Robotics and Automation (CIRA99) at 187-94 (1999).</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ballagas, Rafael; Ringel, Meredith; Stone Maureen; and Borchers, Jan, "<a href='http://scholar.google.com/scholar?q="iStuff%3A+A+Physical+User+Interface+Toolkit+For+Ubiquitous+Computer+Environments%2C"'>iStuff: A Physical User Interface Toolkit For Ubiquitous Computer Environments,</a>" Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, vol. 5, No. 1, at 537-44 (ACM) (Apr. 5-10, 2003) (Gilbert Cockton, Panu Korhonen, eds.).</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Benbsat, Ari Y., Paradiso, Joseph A., "<a href='http://scholar.google.com/scholar?q="An+Inertial+Measurement+Framework+for+Gesture+Recognition+and+Applications%2C"'>An Inertial Measurement Framework for Gesture Recognition and Applications,</a>" Gesture and Sign Language in Human-Computer Interaction , vol. 2298/2002 at 77-90 (Springer-Verlag Berlin Heibelberg 2002); International Gesture Workshop, GW 2001, London, UK, 2001 Proceedings, LNAI 2298, at 9-20, I. Wachsmuth and T. Sowa (eds.).</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Brownell, Richard, Review of Peripheral-GameCube-G3 Wireless Controller, GAF, Jul. 17, 2003, 2 pages.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Brownstein, B., et al, Functional Movement in Orthopedic and Sports Physical Therapy, Churchill Livingstone (1997), 15.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Brugger, W., et al, Computer-aided tracking of body motions using a c.c.d.-image sensor, Med. Biol. Eng. &amp; Comput, (Mar. 1978), 207-210.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Buchanan, Levi, "<a href='http://scholar.google.com/scholar?q="Happy+Birthday%2C+Rumble+Pak"'>Happy Birthday, Rumble Pak</a>", IGN.com, Apr. 3, 2008, 2 pages.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Codamotion: The science of real-time motion capture and analysis, webpages from http://www.charndyn.com/index.html, (Apr. 17, 2004) 1.</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Codella, C., et al, Interactive Simulation in a Multi-Person Virtual World ACM (May 3-7, 1992), 329-334.</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">CyberGlove product, Immersion Corporation, 1990, http://www.cyberglovesystem.com.</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">DeLoura, M., et al, Game Programming Gems, Charles River Media, (2000) 200-204.</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Europe is Bursting with Virtual Reality Ideas, But Developers Are Critically Strapped for Cash, webpages from https://www/lexis.com/research/retrieve?-m=66d17057c1b77f197aledb9f5fadb87d&amp;-browseType=Text, (Jan. 1993) 1-2.</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Europe is Bursting with Virtual Reality Ideas, But Developers Are Critically Strapped for Cash, webpages from https://www/lexis.com/research/retrieve?—m=66d17057c1b77f197aledb9f5fadb87d&amp;—browseType=Text, (Jan. 1993) 1-2.</td></tr><tr><td class="patent-data-table-td ">19</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Eyestone, Dick, "<a href='http://scholar.google.com/scholar?q="SmartSwing+Company%3A+Letter+from+the+CEO"'>SmartSwing Company: Letter from the CEO</a>", 3 pages, 2004, retrieved May 19, 2009, http://web.archive.org/web/20040810101314/www.smartsinggolf.com.</td></tr><tr><td class="patent-data-table-td ">20</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Fielder, Lauren, "<a href='http://scholar.google.com/scholar?q="E3+2001%3A+Nintendo+unleashes+GameCube+software%2C+a+new+Miyamoto+game%2C+and+more"'>E3 2001: Nintendo unleashes GameCube software, a new Miyamoto game, and more</a>", GameSpot, May 16, 2001, 2 pages, http://www.gamespot.com/downloads/2761390.</td></tr><tr><td class="patent-data-table-td ">21</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">FrontSide Field Test, "<a href='http://scholar.google.com/scholar?q="Smart+Bomber+Get+This"'>Smart Bomber Get This</a>", Golf Magazine, Jun. 2005.</td></tr><tr><td class="patent-data-table-td ">22</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Furniss, Maureen, "<a href='http://scholar.google.com/scholar?q="Motion+Capture%2C"'>Motion Capture,</a>" MoCap MIT (Dec. 1999) 12 pages.</td></tr><tr><td class="patent-data-table-td ">23</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Gamecubicle, Jim-New Contributor, Nintendo Wavebird Control, http://www.gamecubicle.com/news-nintendo—gamecube—wavebird—contoller.htm, May 14, 2002.</td></tr><tr><td class="patent-data-table-td ">24</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Greenleaf, W.J., DataGlove, DataSuit, and virtual reality Advanced technology for people with disabilities, Proceedings of the Seventh Annual Conference ‘Technology and Persons with Disabilities,’ (Mar. 1992) 211-214.</td></tr><tr><td class="patent-data-table-td ">25</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Greenleaf, W.J., DataGlove, DataSuit, and virtual reality Advanced technology for people with disabilities, Proceedings of the Seventh Annual Conference '<a href='http://scholar.google.com/scholar?q="Technology+and+Persons+with+Disabilities%2C"'>Technology and Persons with Disabilities,</a>' (Mar. 1992) 211-214.</td></tr><tr><td class="patent-data-table-td ">26</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Gyration, Inc., GyroRemote GP240-001 Professional Series, copyrighted 2003, www.gyration.com.</td></tr><tr><td class="patent-data-table-td ">27</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Hinckley Synchronous Gestures Device, Oct. 12-15, 2003, Microsoft Research, USA.</td></tr><tr><td class="patent-data-table-td ">28</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Hinckley, Ken et al, "<a href='http://scholar.google.com/scholar?q="Sensing+Techniques+for+Mobile+Interaction%2C"'>Sensing Techniques for Mobile Interaction,</a>" Proceedings of the 13th Annual ACM Symposium on User Interface Software and Technology (San Diego, Cal.), ACM UIST 2000 &amp; Technology, CHI Letters 2 (2), at 91-100 (2000).</td></tr><tr><td class="patent-data-table-td ">29</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Hinckley, Ken, "<a href='http://scholar.google.com/scholar?q="Haptic+Issues+for+Virtual+Manipulation-A+Dissertation+Presented+to+the+Faculty+of+the+School+of+Engineering+and+Applied+Science+at+the+University+of+Virginia"'>Haptic Issues for Virtual Manipulation-A Dissertation Presented to the Faculty of the School of Engineering and Applied Science at the University of Virginia</a>" (Dec. 1996).</td></tr><tr><td class="patent-data-table-td ">30</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Hinckley, Ken, "<a href='http://scholar.google.com/scholar?q="Synchronous+Gestures+for+Multiple+Persons+and+Computer%2C"'>Synchronous Gestures for Multiple Persons and Computer,</a>" Chi Letter, vol. 5, Issue 2, pp. 149-158, ACM, 2003.</td></tr><tr><td class="patent-data-table-td ">31</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Hinckley, Ken, "<a href='http://scholar.google.com/scholar?q="Haptic+Issues+for+Virtual+Manipulation%E2%80%94A+Dissertation+Presented+to+the+Faculty+of+the+School+of+Engineering+and+Applied+Science+at+the+University+of+Virginia"'>Haptic Issues for Virtual Manipulation—A Dissertation Presented to the Faculty of the School of Engineering and Applied Science at the University of Virginia</a>" (Dec. 1996).</td></tr><tr><td class="patent-data-table-td ">32</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Hinkley et al, Proceedings for the working conference on Advanced visual interfaces, Portal: the guide to computing literature, 1 page, 2004.</td></tr><tr><td class="patent-data-table-td ">33</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Holden, Maureen K., et al Use of Virtual Environments in Motor Learning and Rehabilitation Department of Brain and Cognitive Sciences, Handbook of Virtual Environments: Design, Implementation, and Applications chap 49, pp. 999-1026, Stanney (ed), Lawrence Erlbaum Associates 2002.</td></tr><tr><td class="patent-data-table-td ">34</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Immersion CyberGlove product, Immersion Corporation, 1990, http://www.cyberglovesystem.com.</td></tr><tr><td class="patent-data-table-td ">35</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">InterSense, "<a href='http://scholar.google.com/scholar?q="InterSense+Inc.-The+New+Standard+in+Motion+Tracking"'>InterSense Inc.-The New Standard in Motion Tracking</a>" www.isense.com Dec. 18, 2003.</td></tr><tr><td class="patent-data-table-td ">36</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">InterSense, "<a href='http://scholar.google.com/scholar?q="InterSense+Inc.-The+New+Standard+in+Motion+Tracking"'>InterSense Inc.-The New Standard in Motion Tracking</a>" www.isense.com Mar. 27, 2004.</td></tr><tr><td class="patent-data-table-td ">37</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">InterSense, "<a href='http://scholar.google.com/scholar?q="InterSense+Motion+Trackers"'>InterSense Motion Trackers</a>" www.isense.com Mar. 12, 1998.</td></tr><tr><td class="patent-data-table-td ">38</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">InterSense, "<a href='http://scholar.google.com/scholar?q="IS-900+Precision+Motion+Trackers"'>IS-900 Precision Motion Trackers</a>" www.isense.com May 16, 2003.</td></tr><tr><td class="patent-data-table-td ">39</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">InterSense, "<a href='http://scholar.google.com/scholar?q="IS-900+Precision+Motion+Trackers"'>IS-900 Precision Motion Trackers</a>" www.isense.com Sep. 10, 2002.</td></tr><tr><td class="patent-data-table-td ">40</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">InterSense, "<a href='http://scholar.google.com/scholar?q="Technical+Overview+IS-900+Motion+Tracking+System"'>Technical Overview IS-900 Motion Tracking System</a>" www.isense.com, Apr. 2004.</td></tr><tr><td class="patent-data-table-td ">41</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">InterSense, "<a href='http://scholar.google.com/scholar?q="InterSense+Inc.%E2%80%94The+New+Standard+in+Motion+Tracking"'>InterSense Inc.—The New Standard in Motion Tracking</a>" www.isense.com Dec. 18, 2003.</td></tr><tr><td class="patent-data-table-td ">42</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">InterSense, "<a href='http://scholar.google.com/scholar?q="InterSense+Inc.%E2%80%94The+New+Standard+in+Motion+Tracking"'>InterSense Inc.—The New Standard in Motion Tracking</a>" www.isense.com Mar. 27, 2004.</td></tr><tr><td class="patent-data-table-td ">43</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">IREX, Virtual Reality Technologies, webpages from http://www.irexonline.com/how-it-works.htm, (Apr. 19, 2004) 1-2.</td></tr><tr><td class="patent-data-table-td ">44</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">IREX, Virtual Reality Technologies, webpages from http://www.irexonline.com/how—it—works.htm, (Apr. 19, 2004) 1-2.</td></tr><tr><td class="patent-data-table-td ">45</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">IS-900 product, InterSense, Inc. 1999.</td></tr><tr><td class="patent-data-table-td ">46</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kaplan, US S.I.R. H1383, Dec. 6, 1994.</td></tr><tr><td class="patent-data-table-td ">47</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kasvand, T., et al, Computers and the Kinesiology of Gait, Comput. Biol. Med. Pergamon Press (1976) vol. 6 111-120.</td></tr><tr><td class="patent-data-table-td ">48</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kenmochi, A., et al, A network virtual reality skiing system-system overview and skiing movement estimation, Symbiosis of Human and Artifact, (Jul. 1995) 423-428.</td></tr><tr><td class="patent-data-table-td ">49</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kraus, A., Matrices for Engineers, Hemisphere Publishing Corp. (1987) 118-120, 124-126.</td></tr><tr><td class="patent-data-table-td ">50</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kunz, Andreas and Burri, Adrian "<a href='http://scholar.google.com/scholar?q="Design+and+Construction+of+a+New+Haptic+Interface%2C"'>Design and Construction of a New Haptic Interface,</a>" ASME 2000 Design Engineering Technical Conferences and Computers and Information in Engineering Conference, Baltimore, Maryland, Sep. 10-13, 2000; Proceedings of DETC '00.</td></tr><tr><td class="patent-data-table-td ">51</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Lengyel, E., Mathematics for 3D Game Programming &amp; Computer Graphics, Charles River Media (2004) 76-78, 467-468.</td></tr><tr><td class="patent-data-table-td ">52</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Logitech 3D Mouse &amp; Head Tracker Technical Manual, Logitech Inc., Fremont CA, 1992, 88 pages.</td></tr><tr><td class="patent-data-table-td ">53</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Logitech WingMan Cordless Rumblepad, Logitech, Press release Sep. 2, 2001, 2 pages.</td></tr><tr><td class="patent-data-table-td ">54</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">MacLean, Karen, "<a href='http://scholar.google.com/scholar?q="Designing+with+Haptic+Feedback"'>Designing with Haptic Feedback</a>", Proceedings of IEEE Robotics and Automation (ICRA '2000), at 783-88 (Apr. 22-28 (2000)).</td></tr><tr><td class="patent-data-table-td ">55</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Marrin, Teresa and Paradiso, Joseph, "<a href='http://scholar.google.com/scholar?q="The+Digital+Baton%3A+a+Versatile+Performance+Instrument"'>The Digital Baton: a Versatile Performance Instrument</a>" (1997).</td></tr><tr><td class="patent-data-table-td ">56</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Marrin, Teresa, "<a href='http://scholar.google.com/scholar?q="Toward+an+Understanding+of+Musical+Gesture%3A+Mapping+Expressive+Intention+with+the+Digital+Baton"'>Toward an Understanding of Musical Gesture: Mapping Expressive Intention with the Digital Baton</a>" (1996).</td></tr><tr><td class="patent-data-table-td ">57</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Marti, Gaetan et al, "<a href='http://scholar.google.com/scholar?q="Biopsy+navigator%3A+a+smart+haptic+interface+for+interventional+radiological+gestures%2C"'>Biopsy navigator: a smart haptic interface for interventional radiological gestures,</a>" Swiss Federal Institute of Technology (EPFL), CARS 2003 (Computer Assisted Radiology and Surgery), Proceedings of the 17th Int'l Congress &amp; Exhibition, International Congress Series, vol. 1256 at 788-93 (Jun. 2003).</td></tr><tr><td class="patent-data-table-td ">58</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Medved, V., Towards a virtual reality-assisted movement diagnostics-an outline, Robotica (Jan.-Feb. 1994) vol. 12, 55-57.</td></tr><tr><td class="patent-data-table-td ">59</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Merians, Alma S. et al "<a href='http://scholar.google.com/scholar?q="Virtual+Reality-Augmented+Rehabilitation+for+Patients+Following+Stroke%2C"'>Virtual Reality-Augmented Rehabilitation for Patients Following Stroke,</a>" Physical Therapy, vol. 82, No. 9 (Sep. 2002).</td></tr><tr><td class="patent-data-table-td ">60</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Merians, Alma S. et al "<a href='http://scholar.google.com/scholar?q="Virtual+Reality%E2%80%94Augmented+Rehabilitation+for+Patients+Following+Stroke%2C"'>Virtual Reality—Augmented Rehabilitation for Patients Following Stroke,</a>" Physical Therapy, vol. 82, No. 9 (Sep. 2002).</td></tr><tr><td class="patent-data-table-td ">61</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mulder, A., Human movement tracking technology, School of Kinesiology, Simon Fraser University (Jul. 1994) 1-14.</td></tr><tr><td class="patent-data-table-td ">62</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mulder, Axel, "<a href='http://scholar.google.com/scholar?q="Human+movement+tracking+technology%2C"'>Human movement tracking technology,</a>" Hand Centered Studies of Human Movement Project, Technical Report 94-1, Simon Fraser University (Jul. 1994), 16 pages.</td></tr><tr><td class="patent-data-table-td ">63</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo Game Boy Advance Wireless Adapter, Nintendo World Report, Sep. 25, 2003, 2 pages.</td></tr><tr><td class="patent-data-table-td ">64</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo Game Boy Advance, Nintendo, Sep. 26, 2003, Wikipedia Article http://en.wikipedia.org/wiki/Game-Boy-Advance.</td></tr><tr><td class="patent-data-table-td ">65</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo Game Boy Advance, Nintendo, Sep. 26, 2003, Wikipedia Article http://en.wikipedia.org/wiki/Game—Boy—Advance.</td></tr><tr><td class="patent-data-table-td ">66</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo Game Boy Color Game Catridge with Built-In Rumble, Nintendo, Jun. 28, 1999, Wikipedia Article, 2pages http://en.wikipedia.org/wiki/Rumble-Pak.</td></tr><tr><td class="patent-data-table-td ">67</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo Game Boy Color Game Catridge with Built-In Rumble, Nintendo, Jun. 28, 1999, Wikipedia Article, 2pages http://en.wikipedia.org/wiki/Rumble—Pak.</td></tr><tr><td class="patent-data-table-td ">68</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo Game Boy Color Nintendo, Nov. 18, 1998, Wikipedia Article http://en.wikipedia.org/wiki/Game-Boy-Color.</td></tr><tr><td class="patent-data-table-td ">69</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo Game Boy Color Nintendo, Nov. 18, 1998, Wikipedia Article http://en.wikipedia.org/wiki/Game—Boy—Color.</td></tr><tr><td class="patent-data-table-td ">70</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo Game Boy, Consumer Information and Precautions Booklet, Nintendo,Jul. 31, 1989.</td></tr><tr><td class="patent-data-table-td ">71</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo Game Boy, Nintendo,Jul. 31, 1989, Wikipedia Article, http://en.wikipedia.org/wiki/Game-Boy.</td></tr><tr><td class="patent-data-table-td ">72</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo Game Boy, Nintendo,Jul. 31, 1989, Wikipedia Article, http://en.wikipedia.org/wiki/Game—Boy.</td></tr><tr><td class="patent-data-table-td ">73</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo GameCube Standard Controller, Nintendo, Nov. 18, 2001, Wikipedia Article, http://en.wikipedia.org/wiki/Nintendo-GameCube-Controller.</td></tr><tr><td class="patent-data-table-td ">74</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo GameCube Standard Controller, Nintendo, Nov. 18, 2001, Wikipedia Article, http://en.wikipedia.org/wiki/Nintendo—GameCube—Controller.</td></tr><tr><td class="patent-data-table-td ">75</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo GameCube System, Nintendo, Nov. 18, 2001, Wikipedia Article, http://en.wikipedia.org/wiki/Nintendo-GameCube.</td></tr><tr><td class="patent-data-table-td ">76</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo GameCube System, Nintendo, Nov. 18, 2001, Wikipedia Article, http://en.wikipedia.org/wiki/Nintendo—GameCube.</td></tr><tr><td class="patent-data-table-td ">77</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo N64 Rumble Pack, Nintendo, Jul. 1997, Wikipedia Article, 12 pages, http://en.wikipedia.org/wiki/Rumble-Pak.</td></tr><tr><td class="patent-data-table-td ">78</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo N64 Rumble Pack, Nintendo, Jul. 1997, Wikipedia Article, 12 pages, http://en.wikipedia.org/wiki/Rumble—Pak.</td></tr><tr><td class="patent-data-table-td ">79</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo Wavebird Controller, Nintendo, Jun. 2002 Wikipedia Article, http://en.wikipedia.org/wiki/WaveBird.</td></tr><tr><td class="patent-data-table-td ">80</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo Wavebird Controller, Operation of the WaveBird Controller Nintendo, Jun. 2002, 2 pages.</td></tr><tr><td class="patent-data-table-td ">81</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo"<a href='http://scholar.google.com/scholar?q="Kirby+Tilt+%27n%27+Tumble"'>Kirby Tilt 'n' Tumble</a>" video game Instruction booklet, Nintendo, Apr. 11, 2001, 18 pages.</td></tr><tr><td class="patent-data-table-td ">82</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo"<a href='http://scholar.google.com/scholar?q="Kirby+Tilt+%27n%27+Tumble"'>Kirby Tilt 'n' Tumble</a>" video game, Nintendo, Apr. 11, 2001, Wikipedia Article, http:en.wikipedia.org/wiki/Kirby-Tilt-%27%-Tumble.</td></tr><tr><td class="patent-data-table-td ">83</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo, Nintendo Entertainment System Booth 2002.</td></tr><tr><td class="patent-data-table-td ">84</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo, Nintendo Entertainment System Consumer Information and Precautions Booklet , Nintendo of America, Inc. 1992.</td></tr><tr><td class="patent-data-table-td ">85</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo, Nintendo Entertainment System Instruction Manual, Nintendo of America, Inc. 1992.</td></tr><tr><td class="patent-data-table-td ">86</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo, Nintendo Entertainment System Layout May 9, 2002.</td></tr><tr><td class="patent-data-table-td ">87</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo, Nintendo Feature: History of Pokemon Part 2, Official Nintendo Magazine, May 17, 2009, http://www.officialnintendomagazine.co.uk/article.php?id=8576.</td></tr><tr><td class="patent-data-table-td ">88</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo"<a href='http://scholar.google.com/scholar?q="Kirby+Tilt+%E2%80%98n%E2%80%99+Tumble"'>Kirby Tilt ‘n’ Tumble</a>" video game Instruction booklet, Nintendo, Apr. 11, 2001, 18 pages.</td></tr><tr><td class="patent-data-table-td ">89</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo"<a href='http://scholar.google.com/scholar?q="Kirby+Tilt+%E2%80%98n%E2%80%99+Tumble"'>Kirby Tilt ‘n’ Tumble</a>" video game, Nintendo, Apr. 11, 2001, Wikipedia Article, http:en.wikipedia.org/wiki/Kirby—Tilt—%27%—Tumble.</td></tr><tr><td class="patent-data-table-td ">90</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nintendo"<a href='http://scholar.google.com/scholar?q="Kirby+Tilt+%E2%80%98n%E2%80%99+Tumble"'>Kirby Tilt ‘n’ Tumble</a>" video game, Nintendo, Apr. 11, 2001.</td></tr><tr><td class="patent-data-table-td ">91</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Paradiso, Joseph A., "<a href='http://scholar.google.com/scholar?q="The+Brain+Opera+Technology%3A+New+Instruments+and+Gestural+Sensors+for+Musical+Interaction+and+Performance%22+%28Nov.+1998%29+%28%22Brain+Opera+Article"'>The Brain Opera Technology: New Instruments and Gestural Sensors for Musical Interaction and Performance" (Nov. 1998) ("Brain Opera Article</a>").</td></tr><tr><td class="patent-data-table-td ">92</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Pelican Accessories G3 Wireless Controller, Pelican Accessories, Sep. 20, 2002.</td></tr><tr><td class="patent-data-table-td ">93</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Perry, Simon, "<a href='http://scholar.google.com/scholar?q="Nintendo+to+Launch+Wireless+Game+Boy+Adaptor%2C"'>Nintendo to Launch Wireless Game Boy Adaptor,</a>" Digital Lifestyles, Sep. 26, 2003 http://digital-lifestyles.info/2003/09/26/nintendo-to-launch-wireless-game-boy-adaptor/.</td></tr><tr><td class="patent-data-table-td ">94</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Philips, LPC2104/2105/2106 Single-chip 32 bit microcontrollers; 128 kB ISP/IAP Flash with 64 kB/32 kB/16 kB RAM Product Data, Rev. 05 Dec. 22, 2004.</td></tr><tr><td class="patent-data-table-td ">95</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Pokemon Pinball Game, 1999, Wikipedia Article, http://en.wikipedia.org/wiki/Pok?C3?A9mon—Pinball.</td></tr><tr><td class="patent-data-table-td ">96</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Polhemus, Liberty: The Forerunner in Electromagnetic Tracking Technology, www.polhemus.com, (May 2003) 1-2.</td></tr><tr><td class="patent-data-table-td ">97</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Polhemus, Patriot: The Fast and Affordable Digital Tracker, www.polhemus.com, (Feb. 2004) 1-2.</td></tr><tr><td class="patent-data-table-td ">98</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Polhemus,"<a href='http://scholar.google.com/scholar?q="Fastrak%2C+The+Fast+and+Easy+Digital+Tracker"'>Fastrak, The Fast and Easy Digital Tracker</a>" copyrighted 2001, Coldiester, Vermont, 2 pages.</td></tr><tr><td class="patent-data-table-td ">99</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">PowerGlove and Nintendo product photo, Mattel, 1989.</td></tr><tr><td class="patent-data-table-td ">100</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">PowerGlove product photo, Mattel, 1989.</td></tr><tr><td class="patent-data-table-td ">101</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">PowerGlove product Program Guide, Mattel, 1989.</td></tr><tr><td class="patent-data-table-td ">102</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">PowerGlove product, Instructions, Mattel, 1989.</td></tr><tr><td class="patent-data-table-td ">103</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">PowerGlove product, Mattel, 1989 Wikipedia Article.</td></tr><tr><td class="patent-data-table-td ">104</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Reality built for two: a virtual reality tool, Symposium on Interactive 3D Graphics, ACM Press webpages from http://portal.acm.org/citation.cfm?id=91385.91409&amp;dl+ACM&amp;type=series&amp;i (Jun. 10, 2004) 1-4.</td></tr><tr><td class="patent-data-table-td ">105</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Regan, Mary Beth, "<a href='http://scholar.google.com/scholar?q="Smart+Golf+Clubs"'>Smart Golf Clubs</a>", baltimoresun.com, Jun. 17, 2005.</td></tr><tr><td class="patent-data-table-td ">106</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ruby, D., Biomechanics-how computers extend athletic performance to the body's far limits, Popular Science (Jan. 1982) 58-60.</td></tr><tr><td class="patent-data-table-td ">107</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Sandweiss, J., et al, Biofeedback and Sports Science, Plenum Press New York (1985) 1-201.</td></tr><tr><td class="patent-data-table-td ">108</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Satterfield, Shane, E3 2002: Nintendo announces new GameCube games, GameSpot, May 21, 2002, http://wwwgamespot.com/gamecube/action.rollorama/new.html?sid=2866974&amp;com—act-convert&amp;om—clk=nesfeatures&amp;tag=newsfeatures%Btitle%3B.</td></tr><tr><td class="patent-data-table-td ">109</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Scarborough, E.L., Enhancement of Audio Localization Cue Synthesis by Adding Environmental and Visual Cues, Air Force Inst. Of Tech., Wright-Patterson AFB, OH. School of Engineering (Dec. 1992) 1-4.</td></tr><tr><td class="patent-data-table-td ">110</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Smartswing, "<a href='http://scholar.google.com/scholar?q="Register+to+be+Notified+When+SmartSwing+Products+are+Available+for+Purchase"'>Register to be Notified When SmartSwing Products are Available for Purchase</a>", 3 pages, May 2004, retrieved May 19, 2009, http://web.archive.org/web/20040426182437/www.smartswinggolf.com/...</td></tr><tr><td class="patent-data-table-td ">111</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Smartswing, "<a href='http://scholar.google.com/scholar?q="SmartSwing%3A+Intelligent+Golf+Clubs+that+Build+a+Better+Swing"'>SmartSwing: Intelligent Golf Clubs that Build a Better Swing</a>", 2 pages, 2004, retrieved May 19, 2009, http://web.archive.org/web/20040728221951/http://www.smartswinggolf...</td></tr><tr><td class="patent-data-table-td ">112</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">SmartSwing, SmartSwing Inc.,Apr. 2004, Austin, Texas.</td></tr><tr><td class="patent-data-table-td ">113</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Smartswing, The SmartSwing Learning System Overview, 3 pages, 2004, retrieved May 19, 2009, http://web.archive.org/web/20040810142134/http://www.smartswinggolf.com/t...</td></tr><tr><td class="patent-data-table-td ">114</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Smartswing, The SmartSwing Learning System: How it works, 3 pages, 2004, retrieved May 19, 2009, http://web.archive.org/web/20040403213108/http://www.smartswinggolf.com/t...</td></tr><tr><td class="patent-data-table-td ">115</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Smartswing, The SmartSwing Product, 3 pages, 2004, retrieved May 19, 2009, http://web.archive.org/web/200400403204628/http://www.smartswinggolf.com/t...</td></tr><tr><td class="patent-data-table-td ">116</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Smartswing, The SmartSwing Product: Technical Information, 1 page, 2004, retrieved May 19, 2009, http://web.archive.org/web/200400403205906/http://www.smartswinggolf.com/...</td></tr><tr><td class="patent-data-table-td ">117</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Smith, J., et al, Virtual Batting Cage and Human Model, Virtual Human http://www.cs.berkeley.edu/ rcdavis/classes/'cs294/, (Jun. 17, 2004)1-5.</td></tr><tr><td class="patent-data-table-td ">118</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Success Story Profile: Innovative Sports Training, Motion Monitor, (2002) 1-2.</td></tr><tr><td class="patent-data-table-td ">119</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Sulic, Ivan, Logitech Wingman Cordless Rumblepad Review, IGN Entertainment Games, Jan. 14, 2002, 3 pages.</td></tr><tr><td class="patent-data-table-td ">120</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Swisher, Kara, "<a href='http://scholar.google.com/scholar?q="How+Science+Can+Improve+Your+Golf+Game"'>How Science Can Improve Your Golf Game</a>", The Wall Street Journal, Apr. 18, 2005.</td></tr><tr><td class="patent-data-table-td ">121</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Training Aid, SmartSwing, PGA Magazine, p. 46, Apr. 2005, www.pgamagazine.com.</td></tr><tr><td class="patent-data-table-td ">122</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Verplaetse, C., "<a href='http://scholar.google.com/scholar?q="Inertial+Proprioceptive+Devices%3A+Self-Motion-Sensing+Toys+and+Tools%2C"'>Inertial Proprioceptive Devices: Self-Motion-Sensing Toys and Tools,</a>" IBM Systems Journal, vol. 35 Nos. 3 &amp; 4, at 639-50 (IBM Corp.) (1996).</td></tr><tr><td class="patent-data-table-td ">123</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Villoria, Gerald, Hands on Roll-O-Rama Game Cube, Game Spot, May 29, 2002, http://www.gamespot.com/gamecube/action/rollorama/news.html?sid=2868421&amp;com—act=convert&amp;om—clk=newsfeatures&amp;tag=newsfeatures;title;1&amp;m.</td></tr><tr><td class="patent-data-table-td ">124</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">VTI, Mindflux-VTi CyberTouch, 1996, http://www.mindflux.com.au/products/vti/cybertouch.html.</td></tr><tr><td class="patent-data-table-td ">125</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Wiley, M., "<a href='http://scholar.google.com/scholar?q="Nintendo+Wavebird+Review"'>Nintendo Wavebird Review</a>" US, Jun. 11, 2002, 21 pages.</td></tr><tr><td class="patent-data-table-td ">126</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Williams II, Robert L. et al, "<a href='http://scholar.google.com/scholar?q="Implementation+and+Evaluation+of+A+Haptic+Playback+System%2C"'>Implementation and Evaluation of A Haptic Playback System,</a>" vol. 3, No. 3, Haptics-e, 2004, http://www.haptics-e.org.</td></tr><tr><td class="patent-data-table-td ">127</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Williams II, Robert L. et al, "<a href='http://scholar.google.com/scholar?q="The+Virtual+Haptic+Back+Project%2C"'>The Virtual Haptic Back Project,</a>" Presented at the Image 2003 Conference, Scottsdale, Arizona, 14-18 Jul. 2003.</td></tr><tr><td class="patent-data-table-td ">128</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Wilson et al, "<a href='http://scholar.google.com/scholar?q="Gesture+Recognition+Using+the+XWand%2C"'>Gesture Recognition Using the XWand,</a>" Assistive Intelligent Environments Group, Robotics Institute, Carnegie Mellon University, Report CMU-RI-TR-04-57 (Apr. 2004).</td></tr><tr><td class="patent-data-table-td ">129</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Wilson et al, "<a href='http://scholar.google.com/scholar?q="XWand%3A+UI+for+Intelligent+Spaces%2C"'>XWand: UI for Intelligent Spaces,</a>" Microsoft Research, inProceedings of the SIGCHI Conference on Human Factors in Computing Systems (Ft. Lauderdale, Florida, USA, Apr. 5-10, 2003; CHI '03, ACM, New York, NY, at 545-52 (2003).</td></tr><tr><td class="patent-data-table-td ">130</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Wilson, Andy, "<a href='http://scholar.google.com/scholar?q="XWand%3A+UI+for+Intelligent+Environments"'>XWand: UI for Intelligent Environments</a>", 5 pages retrieved Jan. 11, 2009, http://research.microsoft.com/en-us/um/people/awilson/wand/default.htm.</td></tr><tr><td class="patent-data-table-td ">131</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Wormell, D. and Foxlin, E., "<a href='http://scholar.google.com/scholar?q="Advancements+in+3D+Interactive+Devices+for+Virtual+Environments%2C"'>Advancements in 3D Interactive Devices for Virtual Environments,</a>" EGVE, Proceedings of the Workshop on Virtual Environments 2003, vol. 39 at 47-56 (ACM) (2003) (The Eurographics Association).</td></tr><tr><td class="patent-data-table-td ">132</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Yang, Ungyeon and Kim, Gerard Jounghyun, "<a href='http://scholar.google.com/scholar?q="Implementation+and+Evaluation+of+%27Just+Follow+Me%27%3A+An+Ilmmersive%2C+VR-Based%2C+Motion-Training+System%2C"'>Implementation and Evaluation of 'Just Follow Me': An Ilmmersive, VR-Based, Motion-Training System,</a>" Presence: Teleoperators and Virtual Environments vol. 11 No. 3, at 304-23 (MIT Press) (Jun. 2002).</td></tr><tr><td class="patent-data-table-td ">133</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Yang, Ungyeon and Kim, Gerard Jounghyun, "<a href='http://scholar.google.com/scholar?q="Implementation+and+Evaluation+of+%E2%80%98Just+Follow+Me%E2%80%99%3A+An+Ilmmersive%2C+VR-Based%2C+Motion-Training+System%2C"'>Implementation and Evaluation of ‘Just Follow Me’: An Ilmmersive, VR-Based, Motion-Training System,</a>" Presence: Teleoperators and Virtual Environments vol. 11 No. 3, at 304-23 (MIT Press) (Jun. 2002).</td></tr><tr><td class="patent-data-table-td ">134</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Zetu, D., et al, Extended range tracking for remote virtual reality-aided facility management, Department of Mechanical Engineering The University of Illinois at Chicago, http://alpha.me.uic.edu/dan/NsfPaper/nsf2.html, (Apr. 19, 2004)1-9.</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8159354">US8159354</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 28, 2011</td><td class="patent-data-table-td patent-date-value">Apr 17, 2012</td><td class="patent-data-table-td ">Motiva Llc</td><td class="patent-data-table-td ">Human movement measurement system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8380561">US8380561</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 30, 2008</td><td class="patent-data-table-td patent-date-value">Feb 19, 2013</td><td class="patent-data-table-td ">Immersion Corporation</td><td class="patent-data-table-td ">Method and apparatus for scoring haptic devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8427325">US8427325</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 23, 2012</td><td class="patent-data-table-td patent-date-value">Apr 23, 2013</td><td class="patent-data-table-td ">Motiva Llc</td><td class="patent-data-table-td ">Human movement measurement system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8451117">US8451117</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 23, 2009</td><td class="patent-data-table-td patent-date-value">May 28, 2013</td><td class="patent-data-table-td ">Empire Technology Development Llc</td><td class="patent-data-table-td ">Content processing system capable of event detection and content management</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8565483">US8565483</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 31, 2011</td><td class="patent-data-table-td patent-date-value">Oct 22, 2013</td><td class="patent-data-table-td ">Seiko Epson Corporation</td><td class="patent-data-table-td ">Motion analyzing apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090153365">US20090153365</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 18, 2005</td><td class="patent-data-table-td patent-date-value">Jun 18, 2009</td><td class="patent-data-table-td ">Fabio Salsedo</td><td class="patent-data-table-td ">Portable haptic interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110148628">US20110148628</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 23, 2009</td><td class="patent-data-table-td patent-date-value">Jun 23, 2011</td><td class="patent-data-table-td ">Empire Technology Development Llc</td><td class="patent-data-table-td ">Content processing system capable of event detection and content management</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120128203">US20120128203</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 31, 2011</td><td class="patent-data-table-td patent-date-value">May 24, 2012</td><td class="patent-data-table-td ">Seiko Epson Corporation</td><td class="patent-data-table-td ">Motion analyzing apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120142436">US20120142436</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 1, 2011</td><td class="patent-data-table-td patent-date-value">Jun 7, 2012</td><td class="patent-data-table-td ">Konami Digital Entertainment Co., Ltd.</td><td class="patent-data-table-td ">Game device, control method for a game device, and non-transitory information storage medium</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120178534">US20120178534</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 23, 2012</td><td class="patent-data-table-td patent-date-value">Jul 12, 2012</td><td class="patent-data-table-td ">Motiva Llc</td><td class="patent-data-table-td ">Human movement measurement system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120220233">US20120220233</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 27, 2012</td><td class="patent-data-table-td patent-date-value">Aug 30, 2012</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Ranging with body motion capture</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc340/defs340.htm&usg=AFQjCNGk_NCDWkt8oMijCQ2jvfqday0GbA#C340S573100">340/573.1</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc463/defs463.htm&usg=AFQjCNHbYC-xHRx7sVIdiPe-iWWWtNPv0g#C463S036000">463/36</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc340/defs340.htm&usg=AFQjCNGk_NCDWkt8oMijCQ2jvfqday0GbA#C340S005610">340/5.61</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc345/defs345.htm&usg=AFQjCNF0b52M2HqQQp5rThx3mQ75nwjbGg#C345S161000">345/161</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc340/defs340.htm&usg=AFQjCNGk_NCDWkt8oMijCQ2jvfqday0GbA#C340S013200">340/13.2</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G08B0023000000">G08B23/00</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A61B2560/06">A61B2560/06</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A61B2560/0443">A61B2560/0443</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B2230/62">A63B2230/62</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G08B21/0288">G08B21/0288</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B2220/836">A63B2220/836</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B2225/12">A63B2225/12</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A61B5/1122">A61B5/1122</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B2225/54">A63B2225/54</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B2220/803">A63B2220/803</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A61B5/024">A61B5/024</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B2220/40">A63B2220/40</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B2024/0012">A63B2024/0012</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B2071/063">A63B2071/063</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G08B21/0261">G08B21/0261</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B24/0003">A63B24/0003</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B2220/806">A63B2220/806</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B71/06">A63B71/06</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A61B5/6828">A61B5/6828</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A61B5/6824">A61B5/6824</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B2024/0096">A63B2024/0096</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B2225/50">A63B2225/50</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A61B5/486">A61B5/486</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63B24/0087">A63B24/0087</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G08B21/22">G08B21/22</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A61B5/1127">A61B5/1127</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A61B2562/0219">A61B2562/0219</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A61B5/1124">A61B5/1124</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=K_feBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A63F13/06">A63F13/06</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G08B21/22</span>, <span class="nested-value">A61B5/68B2L</span>, <span class="nested-value">A61B5/11W2</span>, <span class="nested-value">A61B5/11U</span>, <span class="nested-value">A61B5/48S</span>, <span class="nested-value">G08B21/02A18</span>, <span class="nested-value">G08B21/02A27</span>, <span class="nested-value">A63B71/06</span>, <span class="nested-value">A63B24/00R</span>, <span class="nested-value">A63B24/00A</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Dec 17, 2013</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIM 19 IS CONFIRMED.CLAIMS 1-4, 6-17, 20-22, 24-27, 30-40, 42, 43 AND 49 ARECANCELLED.CLAIMS 44, 46 AND 48 ARE DETERMINED TO BE PATENTABLE AS AMENDED.CLAIMS 45, 47 AND 50, DEPENDENT ON AN AMENDED CLAIM, ARE DETERMINED TO BE PATENTABLE.CLAIMS 5, 18, 23, 28, 29, AND 41 WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20120119</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 16, 2009</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">MOTIVA LLC, OHIO</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:FERGUSON, KEVIN;GRONACHAN, DONALD;REEL/FRAME:022261/0663</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20081028</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_4ff636b3d23669b7103f3b3a3a18b4cd.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U2Zazz-MB0cj3Ust9dr7fJ3hiFzOQ\u0026id=K_feBgABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U2a1J3Vd8xk6ERsRkyVX6SEJn5mtg\u0026id=K_feBgABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U1hQgDbvy9F9PImV_H7qxJ5tNutTg","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Human_movement_measurement_system.pdf?id=K_feBgABERAJ\u0026output=pdf\u0026sig=ACfU3U1BLXRH0bbxEpxy9QQ7Tkxytdwb6g"},"sample_url":"http://www.google.com/patents/reader?id=K_feBgABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>