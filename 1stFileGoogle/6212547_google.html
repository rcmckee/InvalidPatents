<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6212547 - UTP based video and data conferencing - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="UTP based video and data conferencing"><meta name="DC.contributor" content="Lester F. Ludwig" scheme="inventor"><meta name="DC.contributor" content="J. Chris Lauwers" scheme="inventor"><meta name="DC.contributor" content="Collaboration Properties, Inc." scheme="assignee"><meta name="DC.date" content="1998-5-5" scheme="dateSubmitted"><meta name="DC.description" content="A multimedia collaboration system that integrates separate real-time and asynchronous networks—the former real-tine audio and video, and the latter for control signals and textual, graphical and other data—in a manner that is interoperable across different computer and network operating system platforms and which closely approximates the experience of face-to-face collaboration, while liberating the participants from the limitations of time and distance. These capabilities are achieved by exploiting a variety of hardware, software and networking technologies in a manner that preserves the quality and integrity of audio/video/data and other multimedia information, even after wide area transmission, and at a significantly reduced networking cost as compared to what would be required by presently known approaches. The system architecture is readily scalable to the largest enterprise network enviroments. It accommodates differing levels of collaborative capabilities available to individual users and permits high-quality audio and video capabilities to be readily superimposed onto existing personal computers and workstations and their interconnecting LANs and WANs. In a particular preferred embodiment, a plurality of geographically dispersed multimedia LANs are interconnected by a WAN. The demands made on the WAN are significantly reduced by employing multi-hopping techniques, including dynamically avoiding the unnecessary decompression of data at intermediate hops, and exploiting video mosaicing, cut-and-paste and audio mixing technologies so that significantly fewer wide area transmission paths are required while maintaining the high quality of the transmitted audio/video."><meta name="DC.date" content="2001-4-3" scheme="issued"><meta name="DC.relation" content="US:3723653" scheme="references"><meta name="DC.relation" content="US:3873771" scheme="references"><meta name="DC.relation" content="US:3974337" scheme="references"><meta name="DC.relation" content="US:4005265" scheme="references"><meta name="DC.relation" content="US:4054908" scheme="references"><meta name="DC.relation" content="US:4210927" scheme="references"><meta name="DC.relation" content="US:4441180" scheme="references"><meta name="DC.relation" content="US:4451705" scheme="references"><meta name="DC.relation" content="US:4475193" scheme="references"><meta name="DC.relation" content="US:4516156" scheme="references"><meta name="DC.relation" content="US:4529839" scheme="references"><meta name="DC.relation" content="US:4529840" scheme="references"><meta name="DC.relation" content="US:4531024" scheme="references"><meta name="DC.relation" content="US:4574374" scheme="references"><meta name="DC.relation" content="US:4645872" scheme="references"><meta name="DC.relation" content="US:4650929" scheme="references"><meta name="DC.relation" content="US:4653090" scheme="references"><meta name="DC.relation" content="US:4686698" scheme="references"><meta name="DC.relation" content="US:4710917" scheme="references"><meta name="DC.relation" content="US:4716585" scheme="references"><meta name="DC.relation" content="US:4796293" scheme="references"><meta name="DC.relation" content="US:4817018" scheme="references"><meta name="DC.relation" content="US:4837798" scheme="references"><meta name="DC.relation" content="US:4847829" scheme="references"><meta name="DC.relation" content="US:4888795" scheme="references"><meta name="DC.relation" content="US:4922523" scheme="references"><meta name="DC.relation" content="US:4931872" scheme="references"><meta name="DC.relation" content="US:4953159" scheme="references"><meta name="DC.relation" content="US:4961211" scheme="references"><meta name="DC.relation" content="US:4965819" scheme="references"><meta name="DC.relation" content="US:4977520" scheme="references"><meta name="DC.relation" content="US:4987492" scheme="references"><meta name="DC.relation" content="US:4995071" scheme="references"><meta name="DC.relation" content="US:4998243" scheme="references"><meta name="DC.relation" content="US:5003532" scheme="references"><meta name="DC.relation" content="US:5010399" scheme="references"><meta name="DC.relation" content="US:5014267" scheme="references"><meta name="DC.relation" content="US:5027400" scheme="references"><meta name="DC.relation" content="US:5042062" scheme="references"><meta name="DC.relation" content="US:5056136" scheme="references"><meta name="DC.relation" content="US:5072442" scheme="references"><meta name="DC.relation" content="US:5073926" scheme="references"><meta name="DC.relation" content="US:5099510" scheme="references"><meta name="DC.relation" content="US:5130399" scheme="references"><meta name="DC.relation" content="US:5130793" scheme="references"><meta name="DC.relation" content="US:5130801" scheme="references"><meta name="DC.relation" content="US:5155761" scheme="references"><meta name="DC.relation" content="US:5157491" scheme="references"><meta name="DC.relation" content="US:5170427" scheme="references"><meta name="DC.relation" content="US:5195086" scheme="references"><meta name="DC.relation" content="US:5195087" scheme="references"><meta name="DC.relation" content="US:5200989" scheme="references"><meta name="DC.relation" content="US:5202957" scheme="references"><meta name="DC.relation" content="US:5218627" scheme="references"><meta name="DC.relation" content="US:5224094" scheme="references"><meta name="DC.relation" content="US:5231492" scheme="references"><meta name="DC.relation" content="US:5239466" scheme="references"><meta name="DC.relation" content="US:5253362" scheme="references"><meta name="DC.relation" content="US:5260941" scheme="references"><meta name="DC.relation" content="US:5283637" scheme="references"><meta name="DC.relation" content="US:5303343" scheme="references"><meta name="DC.relation" content="US:5315633" scheme="references"><meta name="DC.relation" content="US:5319795" scheme="references"><meta name="DC.relation" content="US:5333133" scheme="references"><meta name="DC.relation" content="US:5333299" scheme="references"><meta name="DC.relation" content="US:5335321" scheme="references"><meta name="DC.relation" content="US:5345258" scheme="references"><meta name="DC.relation" content="US:5351276" scheme="references"><meta name="DC.relation" content="US:5353398" scheme="references"><meta name="DC.relation" content="US:5363507" scheme="references"><meta name="DC.relation" content="US:5365265" scheme="references"><meta name="DC.relation" content="US:5367629" scheme="references"><meta name="DC.relation" content="US:5373549" scheme="references"><meta name="DC.relation" content="US:5374952" scheme="references"><meta name="DC.relation" content="US:5375068" scheme="references"><meta name="DC.relation" content="US:5379374" scheme="references"><meta name="DC.relation" content="US:5382972" scheme="references"><meta name="DC.relation" content="US:5384598" scheme="references"><meta name="DC.relation" content="US:5384772" scheme="references"><meta name="DC.relation" content="US:5392223" scheme="references"><meta name="DC.relation" content="US:5392346" scheme="references"><meta name="DC.relation" content="US:5404435" scheme="references"><meta name="DC.relation" content="US:5408526" scheme="references"><meta name="DC.relation" content="US:5408662" scheme="references"><meta name="DC.relation" content="US:5422883" scheme="references"><meta name="DC.relation" content="US:5432525" scheme="references"><meta name="DC.relation" content="US:5444476" scheme="references"><meta name="DC.relation" content="US:5471318" scheme="references"><meta name="DC.relation" content="US:5473679" scheme="references"><meta name="DC.relation" content="US:5475421" scheme="references"><meta name="DC.relation" content="US:5485504" scheme="references"><meta name="DC.relation" content="US:5491695" scheme="references"><meta name="DC.relation" content="US:5506954" scheme="references"><meta name="DC.relation" content="US:5515491" scheme="references"><meta name="DC.relation" content="US:5526024" scheme="references"><meta name="DC.relation" content="US:5550966" scheme="references"><meta name="DC.relation" content="US:5553222" scheme="references"><meta name="DC.relation" content="US:5561736" scheme="references"><meta name="DC.relation" content="US:5565910" scheme="references"><meta name="DC.relation" content="US:5581702" scheme="references"><meta name="citation_reference" content="ACM Press, Conference on Organizational Computing Systems, SIGOIS Bulletin, vol. 12, No. 2-3, Nov. 5-8, 1991."><meta name="citation_reference" content="Ahuja et al., &quot;Coordination and Control of Multimedia Conferencing,&quot; IEEE Communication Magazine, 30(5): 38-42, May 1992."><meta name="citation_reference" content="Ahuja et al., &quot;Networking Requirements of the Rapport Multimedia Conferencing System,&quot; INFOCOM &#39;88, IEEE, pp. 746-751, 1988."><meta name="citation_reference" content="Bellcore News, &quot;IMAL Makes Media Merging Magic,&quot; 5(20), Nov. 9, 1988."><meta name="citation_reference" content="Biswas et al., &quot;Distributed Scheduling of Meetings: A Case Study in Prototyping Distributed Application,&quot; System Integration, 1992 2nd International Conference."><meta name="citation_reference" content="Cohen et al., &quot;Audio Windows for Binaural Telecommunication,&quot; EIC, Tokyo (Oct. 1991)."><meta name="citation_reference" content="Cohen et al., &quot;Audio Windows: User Interfaces for Manipulating Virtual Acoustic Environments,&quot; pp. 479-480."><meta name="citation_reference" content="Cohen et al., &quot;Design and Control of Shared Conferencing Environments for Audio Telecommunication,&quot; Proceedings of the Second Int&#39;l Symposium on Measurement and Control Robotics (ISMCR &#39;92), Tsukuba Science City, Japan, (Nov. 15-19, 1992), pp. 405-412."><meta name="citation_reference" content="Cohen et al., &quot;Exocentric Control of Audio Imaging in Biaural Telecommunication,&quot; IEICE Trans. Fundamentals, vol. E75-A, No. 2, (Feb. 1992)."><meta name="citation_reference" content="Cohen et al., &quot;Multidimensional Audio Window Management,&quot; Int&#39;l Journal of Man-Machine Studie, vol. 34:319-336 (1991)."><meta name="citation_reference" content="Cohen et al., &quot;Multidimensional Audio Windows: Conferences, Concerts and Cocktails,&quot; Human Factors Society Meeting, SF, CA, pp. 1-15, Jun. 12, 1991."><meta name="citation_reference" content="Crawford et al., &quot;Videomatic Switching: System and Services, Digital Communications,&quot; Int. Zurich Seminar, 1988."><meta name="citation_reference" content="Ensor et al., &quot;The Rapport Multimedia Conferencing System-Software Overview,&quot; Computer Workstation Conference, IEEE, pp. 52-58, 1988."><meta name="citation_reference" content="Ensor et al., “The Rapport Multimedia Conferencing System—Software Overview,” Computer Workstation Conference, IEEE, pp. 52-58, 1988."><meta name="citation_reference" content="Frontiers in computer communications technology, Sigcom &#39;87 Workshop (Aug. 11-13, 1987)."><meta name="citation_reference" content="Gopal et al., &quot;Directories of Networks with Causally Connected Users,&quot; IEEE, pp. 1060-1064, 1988."><meta name="citation_reference" content="Horn et al., &quot;An ISDN Multimedia Conference Bridge,&quot; TENCON &#39;90-1990 IEEE Region 10 Conference on Computer and Communication, pp. 853-856, 1990."><meta name="citation_reference" content="Horn et al., “An ISDN Multimedia Conference Bridge,” TENCON &#39;90—1990 IEEE Region 10 Conference on Computer and Communication, pp. 853-856, 1990."><meta name="citation_reference" content="Kamel, &quot;An Integrated Approach to Share Synchronous Groupware Workspaces,&quot; IEEE 1993."><meta name="citation_reference" content="Kendall et al., &quot;Simulating the Cue of Spatial Hearing in Natural Environments,&quot; Northwestern University, Evanston, IL 60201."><meta name="citation_reference" content="Klein, Telekommunikation von Angesichtzu Angesicht 2323 Telcom Report 9 (1986) Sep./Okt., No. 5, Erlangen, W. Germany."><meta name="citation_reference" content="Kobayashi et al., &quot;Development and Trial Operation of Video Teleconference System,&quot; IEEE Globecom, pp. 2060-2063, 1999."><meta name="citation_reference" content="Lake et al. &quot;A network environment for studying multimedia network architecture and control,&quot; (1989 Globecom)."><meta name="citation_reference" content="Lantz et al., Collaboration Technology Research at Olivetti Research California, Aug. 1989."><meta name="citation_reference" content="Lantz, An Experiment in Integrated Multimedia Conferencing, Department of Computer Science, Stanford University, Stanford , CA 94305, Dec. 1986."><meta name="citation_reference" content="Lauwers et al., Collaboration Awareness in Support of Collaboration Transparency: Requirements for the Next Generation of Shared Windows Systems, (Olivetti Research California) Version of Apr. 1989."><meta name="citation_reference" content="Lauwers et al., Replicated Architectures for Shared Window Systems: A Critique, (Olivetti Research California) Version of Apr. 19990."><meta name="citation_reference" content="Leung et al., Optimum Connection Paths for a Class of Videoconferences, Department of Information Engineering, the Chinese University of Honk Kong, Shatin, Hong Kong."><meta name="citation_reference" content="Maeno et al., Distributed Desktop Conferencing System (Mermaid) Based on Group Communication Architecture, The Transactions of the Institute of Electronics, Information and Comm. Engineers E74 (1991) Sep., No. 9, Tokyo, JP."><meta name="citation_reference" content="Martens, &quot;Principal Components Analysis and Resynthesis of Spectral Cues to Perceived Direction,&quot; Proceedings of the 1987 Int&#39;l Computer Music Conference, Northwestern University, Evanston, IL 60201."><meta name="citation_reference" content="Masaki et al., &quot;A Desktop Teleconferencing Terminal Based on B-ISDN: PMTC,&quot; NTT Review, 4(4):81-85, 1992."><meta name="citation_reference" content="Ng et al., Systems Integration &#39;90, (Apr. 23-26, 1990)."><meta name="citation_reference" content="Nunokawa et al., &quot;Teleconferencing Using Stereo Voices and Electronic OHP,&quot; IEEE, 1988."><meta name="citation_reference" content="Ohmori et al., &quot;Distributed Cooperative Control for Sharing Applications Based on Multiparty and Multimedia Desktop Conferencing System,&quot; IEEE, 1992."><meta name="citation_reference" content="Pate, &quot;Trends in Multimedia Applications and the Network Models to Support Them,&quot; Globecom&#39;s 90: 1990."><meta name="citation_reference" content="Perkins, &quot;Spider: An investigation in collaborative technologies and their effects on network performance&quot;."><meta name="citation_reference" content="Ramanathan et al., Optimal Communication Architectures for Multimedia Conferencing in Distributed Systems, Multimedia Laboratory Dept. of Computer Science and Engineering, University of San Diego, La Jolla, CA."><meta name="citation_reference" content="Rangan et al., &quot;A Window-Based Editor for Digital Video and Audio ,&quot; System Sciences, 1992 Hawaii Int&#39;l Conference (1992)."><meta name="citation_reference" content="Rangan et al., &quot;Software architecture for integration of video services in the etherphone system,&quot; IEEE J. on Selected Areas in Comm., 9(9):1395-1404, Dec. &#39;91."><meta name="citation_reference" content="Sakata et al., &quot;Development and Evaluation of an In-House Multimedia Desktop Conference System,&quot; NEC Research &amp; Development, No. 98, pp. 107-117, Jul. 1990."><meta name="citation_reference" content="Sakata, &quot;B-ISDN Multimedia Workstation Architecture,&quot; IEEE, 1993."><meta name="citation_reference" content="Statement by Attorney for Applicants-Describing Product Development."><meta name="citation_reference" content="Stefik et al., &quot;Beyond the Chalkboard: Computer Support for Collaboration and Problem Solving,&quot; Communications of the ACM, vol. 30, No. 1, Jan. 1987."><meta name="citation_reference" content="The American Users Forum (Niu-Forum) Aug. 6-9, 1990."><meta name="citation_reference" content="Unix 4th Berkeley Release 1991 man pages for &quot;login,&#39; &quot;htmp,&#39; &quot;talk,&#39; and &quot;who.&#39; Online Internet: http://www.de.freebds.org."><meta name="citation_reference" content="Unix 4th Berkeley Release 1991 man pages for ‘login,’ ‘htmp,’ ‘talk,’ and ‘who.’ Online Internet: http://www.de.freebds.org."><meta name="citation_reference" content="Vin et al., Hierarchical Conferencing Architectures for Inter-Group Multimedia Collaboration, Multimedia Laboratory Department of Computer Science and Engineering University of California at San Diego, La Jolla."><meta name="citation_reference" content="Vin et al., Multimedia Conferencing in the Etherphone Environment, Computer Magazine, vol. 24, Issue 10, pp. 69-79, 1991."><meta name="citation_reference" content="Watabe et al., &quot;A Distributed Multiparty Desktop Conferencing System and Its Architecture,&quot; IEEE, 1991."><meta name="citation_reference" content="Watabe et al., &quot;Distributed Desktop Conferencing System with Multiuser Multimedia Interface,&quot; IEEE, 1991."><meta name="citation_reference" content="Weiss, Desk Top Video Conferencing-An Important Feature of Future Visual, Siemens AG-Munich-West Germany."><meta name="citation_reference" content="Weiss, Desk Top Video Conferencing—An Important Feature of Future Visual, Siemens AG—Munich—West Germany."><meta name="citation_reference" content="Zellweger et al., &quot;An Overview of the Etherphone System and Its Applications,&quot; Computer Workstations Conference, 1988."><meta name="citation_patent_number" content="US:6212547"><meta name="citation_patent_application_number" content="US:09/072,542"><link rel="canonical" href="http://www.google.com/patents/US6212547"/><meta property="og:url" content="http://www.google.com/patents/US6212547"/><meta name="title" content="Patent US6212547 - UTP based video and data conferencing"/><meta name="description" content="A multimedia collaboration system that integrates separate real-time and asynchronous networks—the former real-tine audio and video, and the latter for control signals and textual, graphical and other data—in a manner that is interoperable across different computer and network operating system platforms and which closely approximates the experience of face-to-face collaboration, while liberating the participants from the limitations of time and distance. These capabilities are achieved by exploiting a variety of hardware, software and networking technologies in a manner that preserves the quality and integrity of audio/video/data and other multimedia information, even after wide area transmission, and at a significantly reduced networking cost as compared to what would be required by presently known approaches. The system architecture is readily scalable to the largest enterprise network enviroments. It accommodates differing levels of collaborative capabilities available to individual users and permits high-quality audio and video capabilities to be readily superimposed onto existing personal computers and workstations and their interconnecting LANs and WANs. In a particular preferred embodiment, a plurality of geographically dispersed multimedia LANs are interconnected by a WAN. The demands made on the WAN are significantly reduced by employing multi-hopping techniques, including dynamically avoiding the unnecessary decompression of data at intermediate hops, and exploiting video mosaicing, cut-and-paste and audio mixing technologies so that significantly fewer wide area transmission paths are required while maintaining the high quality of the transmitted audio/video."/><meta property="og:title" content="Patent US6212547 - UTP based video and data conferencing"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("eY3tU-a5Go_AggT10oDIAg"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("GRC"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("eY3tU-a5Go_AggT10oDIAg"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("GRC"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6212547?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6212547"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=f0hUBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6212547&amp;usg=AFQjCNEFGFRthQgC0ieaNcYEDkCB54E6rw" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6212547.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6212547.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6212547" style="display:none"><span itemprop="description">A multimedia collaboration system that integrates separate real-time and asynchronous networks—the former real-tine audio and video, and the latter for control signals and textual, graphical and other data—in a manner that is interoperable across different computer and network operating system platforms...</span><span itemprop="url">http://www.google.com/patents/US6212547?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6212547 - UTP based video and data conferencing</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6212547 - UTP based video and data conferencing" title="Patent US6212547 - UTP based video and data conferencing"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6212547 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/072,542</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Apr 3, 2001</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">May 5, 1998</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Oct 1, 1993</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CA2173204A1">CA2173204A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/CA2173204C">CA2173204C</a>, </span><span class="patent-bibdata-value"><a href="/patents/CA2173209A1">CA2173209A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/CA2173209C">CA2173209C</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69426456D1">DE69426456D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69426456T2">DE69426456T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69428725D1">DE69428725D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69428725T2">DE69428725T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69429684D1">DE69429684D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69429684T2">DE69429684T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69430272D1">DE69430272D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69430272T2">DE69430272T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69431525D1">DE69431525D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69431525T2">DE69431525T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69431536D1">DE69431536D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69431536T2">DE69431536T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69432803D1">DE69432803D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69432803T2">DE69432803T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69433042D1">DE69433042D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69433042T2">DE69433042T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69434762D1">DE69434762D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69434762T2">DE69434762T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69435132D1">DE69435132D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0721725A1">EP0721725A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0721725B1">EP0721725B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0721726A1">EP0721726A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0721726B1">EP0721726B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0898424A2">EP0898424A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0898424A3">EP0898424A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0898424B1">EP0898424B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0899952A2">EP0899952A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0899952A3">EP0899952A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0899952B1">EP0899952B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0899953A2">EP0899953A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0899953A3">EP0899953A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0899953B1">EP0899953B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0899954A2">EP0899954A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0899954A3">EP0899954A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0899954B1">EP0899954B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0912055A2">EP0912055A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0912055A3">EP0912055A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0912055B1">EP0912055B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0912056A2">EP0912056A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0912056A3">EP0912056A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0912056B1">EP0912056B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0955765A1">EP0955765A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1307038A2">EP1307038A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1307038A3">EP1307038A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1307038B1">EP1307038B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1705913A1">EP1705913A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1705913B1">EP1705913B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5617539">US5617539</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5689641">US5689641</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5758079">US5758079</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5802294">US5802294</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5854893">US5854893</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5867654">US5867654</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5884039">US5884039</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5896500">US5896500</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5915091">US5915091</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5978835">US5978835</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6237025">US6237025</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6343314">US6343314</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6351762">US6351762</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6426769">US6426769</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6437818">US6437818</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6583806">US6583806</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20020154210">US20020154210</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1995010157A1">WO1995010157A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1995010158A2">WO1995010158A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1995010158A3">WO1995010158A3</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">072542, </span><span class="patent-bibdata-value">09072542, </span><span class="patent-bibdata-value">US 6212547 B1, </span><span class="patent-bibdata-value">US 6212547B1, </span><span class="patent-bibdata-value">US-B1-6212547, </span><span class="patent-bibdata-value">US6212547 B1, </span><span class="patent-bibdata-value">US6212547B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Lester+F.+Ludwig%22">Lester F. Ludwig</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22J.+Chris+Lauwers%22">J. Chris Lauwers</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Collaboration+Properties,+Inc.%22">Collaboration Properties, Inc.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6212547.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6212547.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6212547.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (100),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (53),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (69),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (90),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (11)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6212547&usg=AFQjCNHn5Z6TAQdvrCStgPEIqyzjMdvU3w">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6212547&usg=AFQjCNG8eF_s4zyAjXpLV9rZnJXgg0sz-A">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6212547B1%26KC%3DB1%26FT%3DD&usg=AFQjCNEC4_GMF2NKSX5OgYA4Te2v2k3zmw">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54750358" lang="EN" load-source="patent-office">UTP based video and data conferencing</invention-title></span><br><span class="patent-number">US 6212547 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA72536697" lang="EN" load-source="patent-office"> <div class="abstract">A multimedia collaboration system that integrates separate real-time and asynchronous networks—the former real-tine audio and video, and the latter for control signals and textual, graphical and other data—in a manner that is interoperable across different computer and network operating system platforms and which closely approximates the experience of face-to-face collaboration, while liberating the participants from the limitations of time and distance. These capabilities are achieved by exploiting a variety of hardware, software and networking technologies in a manner that preserves the quality and integrity of audio/video/data and other multimedia information, even after wide area transmission, and at a significantly reduced networking cost as compared to what would be required by presently known approaches. The system architecture is readily scalable to the largest enterprise network enviroments. It accommodates differing levels of collaborative capabilities available to individual users and permits high-quality audio and video capabilities to be readily superimposed onto existing personal computers and workstations and their interconnecting LANs and WANs. In a particular preferred embodiment, a plurality of geographically dispersed multimedia LANs are interconnected by a WAN. The demands made on the WAN are significantly reduced by employing multi-hopping techniques, including dynamically avoiding the unnecessary decompression of data at intermediate hops, and exploiting video mosaicing, cut-and-paste and audio mixing technologies so that significantly fewer wide area transmission paths are required while maintaining the high quality of the transmitted audio/video.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(35)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00018.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00018.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00019.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00019.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00020.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00020.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00021.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00021.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00022.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00022.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00023.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00023.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00024.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00024.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00025.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00025.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00026.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00026.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00027.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00027.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00028.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00028.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00029.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00029.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00030.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00030.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00031.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00031.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00032.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00032.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00033.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00033.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6212547B1/US06212547-20010403-D00034.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6212547B1/US06212547-20010403-D00034.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(45)</span></span></div><div class="patent-text"><div mxw-id="PCLM28616602" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is: </claim-statement> <div class="claim"> <div num="1" id="US-6212547-B1-CLM-00001" class="claim">
      <div class="claim-text">1. A videoconferencing system comprising:</div>
      <div class="claim-text">(a) at least one video signal source; </div>
      <div class="claim-text">(b) at least one video display device; </div>
      <div class="claim-text">(c) at least one unshielded twisted pair of wires </div>
      <div class="claim-text">(i) defining a UTP video communication path, </div>
      <div class="claim-text">(ii) arranged for transport of video signals, </div>
      <div class="claim-text">(1) originating at one of the signal sources, </div>
      <div class="claim-text">(2) to at least one of the display devices; </div>
      <div class="claim-text">(d) at least one processor </div>
      <div class="claim-text">(i) capable of providing data conferencing signals; and </div>
      <div class="claim-text">(e) at least one data communication path </div>
      <div class="claim-text">(i) arranged for transmission of </div>
      <div class="claim-text">(1) data conferencing signals, </div>
      <div class="claim-text">(2) to one of the display devices, </div>
      <div class="claim-text">wherein the system is configured to</div>
      <div class="claim-text">(ii) reproduce video images, </div>
      <div class="claim-text">(1) based on the transported video signals, </div>
      <div class="claim-text">(2) on one of the display devices, and </div>
      <div class="claim-text">(iii) display information, </div>
      <div class="claim-text">(1) based on the carried data conferencing signals, </div>
      <div class="claim-text">(2) on one of the display devices. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" id="US-6212547-B1-CLM-00002" class="claim">
      <div class="claim-text">2. The system of claim <b>1</b>, wherein the data communication path is defined by</div>
      <div class="claim-text">(a) at least one unshielded twisted pair of wires. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" id="US-6212547-B1-CLM-00003" class="claim">
      <div class="claim-text">3. The system of claim <b>2</b>, wherein:</div>
      <div class="claim-text">(a) images </div>
      <div class="claim-text">(i) based on the video signals </div>
      <div class="claim-text">(ii) can be reproduced </div>
      <div class="claim-text">(iii) in a first window </div>
      <div class="claim-text">(iv) on one of the display devices, and </div>
      <div class="claim-text">(b) information </div>
      <div class="claim-text">(i) based on the data conference signals </div>
      <div class="claim-text">(ii) can be displayed </div>
      <div class="claim-text">(iii) in a second window </div>
      <div class="claim-text">(iv) on the display device. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" id="US-6212547-B1-CLM-00004" class="claim">
      <div class="claim-text">4. The system of claim <b>2</b>, further comprising:</div>
      <div class="claim-text">(a) a control communication path, </div>
      <div class="claim-text">wherein, the system is configured</div>
      <div class="claim-text">(i) to respond to control signals </div>
      <div class="claim-text">(1) transmitted over the control communication path </div>
      <div class="claim-text">(ii) to control the display of the video images. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" id="US-6212547-B1-CLM-00005" class="claim">
      <div class="claim-text">5. The system of claim <b>4</b>, wherein</div>
      <div class="claim-text">(a) at least one unshielded twisted pair of wires defines both </div>
      <div class="claim-text">(i) the data path, and p<b>2</b> (ii) the control communication path. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" id="US-6212547-B1-CLM-00006" class="claim">
      <div class="claim-text">6. The system of claim <b>4</b>, wherein</div>
      <div class="claim-text">(a) at least two video display devices </div>
      <div class="claim-text">(i) each have an associated processor </div>
      <div class="claim-text">(ii) to each define a workstation, and </div>
      <div class="claim-text">wherein the system is configured</div>
      <div class="claim-text">(i) to control the reproduction of video images and spoken audio </div>
      <div class="claim-text">(1) of a first user of a workstation </div>
      <div class="claim-text">(2) at the workstation of a second workstation user. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" id="US-6212547-B1-CLM-00007" class="claim">
      <div class="claim-text">7. The system of claim <b>6</b>, wherein</div>
      <div class="claim-text">(a) the information </div>
      <div class="claim-text">(i) based on the data conferencing signals </div>
      <div class="claim-text">(b) can be displayed </div>
      <div class="claim-text">(i) interactively. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" id="US-6212547-B1-CLM-00008" class="claim">
      <div class="claim-text">8. The system of claim <b>7</b>, wherein the system is configured to</div>
      <div class="claim-text">(a) reproduce the video images, </div>
      <div class="claim-text">(i) at TV quality. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" id="US-6212547-B1-CLM-00009" class="claim">
      <div class="claim-text">9. The system of claim <b>8</b>, wherein</div>
      <div class="claim-text">(a) the video signals are transported in analog. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" id="US-6212547-B1-CLM-00010" class="claim">
      <div class="claim-text">10. The system of claim <b>6</b>, wherein the system is configured</div>
      <div class="claim-text">(a) to combine video images </div>
      <div class="claim-text">(i) of at least a first and a second user </div>
      <div class="claim-text">(ii) into a mosaic image, and </div>
      <div class="claim-text">(b) to reproduce the mosaic image </div>
      <div class="claim-text">(i) on one of the video display devices. </div>
    </div>
    </div> <div class="claim"> <div num="11" id="US-6212547-B1-CLM-00011" class="claim">
      <div class="claim-text">11. A teleconferencing system, for conducting a teleconference among a plurality of users, comprising:</div>
      <div class="claim-text">(a) a plurality of workstations </div>
      <div class="claim-text">(i) each associated with at least one user, </div>
      <div class="claim-text">(ii) each workstation including </div>
      <div class="claim-text">(1) a video display device, and </div>
      <div class="claim-text">(2) associated audio reproduction capabilities; </div>
      <div class="claim-text">(b) audio and video (AV) capture capabilities configured to capture </div>
      <div class="claim-text">(i) video images and </div>
      <div class="claim-text">(ii) spoken audio </div>
      <div class="claim-text">(iii) of a workstation user; and </div>
      <div class="claim-text">(c) at least one unshielded twisted pair of wires defining </div>
      <div class="claim-text">(i) a UTP data path </div>
      <div class="claim-text">(1) along which data can be shared </div>
      <div class="claim-text">(2) among the workstations, and </div>
      <div class="claim-text">(ii) a UTP AV path, </div>
      <div class="claim-text">(1) along which AV signals </div>
      <div class="claim-text">a. representing user video images and audio </div>
      <div class="claim-text">b. can be transported among the workstations, </div>
      <div class="claim-text">wherein the system is configured to</div>
      <div class="claim-text">(i) interactively display images </div>
      <div class="claim-text">(1) based on the shared data, </div>
      <div class="claim-text">(2) on at least two of the video display devices, and </div>
      <div class="claim-text">(ii) reproduce video images and spoken audio </div>
      <div class="claim-text">(1) based on the AV signals, </div>
      <div class="claim-text">(2) on at least one of the video display devices. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" id="US-6212547-B1-CLM-00012" class="claim">
      <div class="claim-text">12. The system of claim <b>11</b>, wherein:</div>
      <div class="claim-text">(a) images </div>
      <div class="claim-text">(i) based on the video signals </div>
      <div class="claim-text">(ii) can be reproduced </div>
      <div class="claim-text">(iii) in a first window on one of the display devices, and </div>
      <div class="claim-text">(b) information </div>
      <div class="claim-text">(i) based on the data conferencing signals </div>
      <div class="claim-text">(ii) can be displayed </div>
      <div class="claim-text">(iii) in a second window on the display device. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" id="US-6212547-B1-CLM-00013" class="claim">
      <div class="claim-text">13. The system of claim <b>11</b>, wherein</div>
      <div class="claim-text">(a) the reproduction </div>
      <div class="claim-text">(i) of the video images and audio </div>
      <div class="claim-text">(ii) is enabled by control signals transmitted </div>
      <div class="claim-text">(1) over the UTP data path. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" id="US-6212547-B1-CLM-00014" class="claim">
      <div class="claim-text">14. The system of claim <b>13</b>, wherein</div>
      <div class="claim-text">(a) wherein the UTP data and UTP AV paths are separate. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" id="US-6212547-B1-CLM-00015" class="claim">
      <div class="claim-text">15. The system of claim <b>14</b>, wherein</div>
      <div class="claim-text">(a) the video signals are transported in analog. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" id="US-6212547-B1-CLM-00016" class="claim">
      <div class="claim-text">16. The system of claim <b>14</b>, wherein the system is configured to</div>
      <div class="claim-text">(a) reproduce the video images, </div>
      <div class="claim-text">(i) at TV quality. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" id="US-6212547-B1-CLM-00017" class="claim">
      <div class="claim-text">17. The system of claim <b>15</b>, wherein the system is configured</div>
      <div class="claim-text">(a) to combine video images </div>
      <div class="claim-text">(i) of at least a first and a second user </div>
      <div class="claim-text">(ii) into a mosaic image, and </div>
      <div class="claim-text">(b) to reproduce the mosaic image </div>
      <div class="claim-text">(i) on a video display device. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" id="US-6212547-B1-CLM-00018" class="claim">
      <div class="claim-text">18. The system of claim <b>16</b>, wherein</div>
      <div class="claim-text">(a) the video images are reproduced in color. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" id="US-6212547-B1-CLM-00019" class="claim">
      <div class="claim-text">19. The system of claim <b>11</b>, wherein the system is configured:</div>
      <div class="claim-text">(a) to allow a first user </div>
      <div class="claim-text">(i) to use a first graphical user interface </div>
      <div class="claim-text">(ii) to select a user </div>
      <div class="claim-text">(iii) from a plurality of users; and </div>
      <div class="claim-text">(b) to allow the first user </div>
      <div class="claim-text">(i) to use a second graphical user interface </div>
      <div class="claim-text">(ii) to select a collaboration type </div>
      <div class="claim-text">(iii) from a group of collaboration types; and </div>
      <div class="claim-text">(c) to respond </div>
      <div class="claim-text">(i) by establishing communication </div>
      <div class="claim-text">(ii) of the selected collaboration type </div>
      <div class="claim-text">(iii) from the first user </div>
      <div class="claim-text">(iv) to the selected user. </div>
    </div>
    </div> <div class="claim"> <div num="20" id="US-6212547-B1-CLM-00020" class="claim">
      <div class="claim-text">20. A method of conducting a videoconference,</div>
      <div class="claim-text">over at least one unshielded twisted pair of wires defining a UTP video communication path, </div>
      <div class="claim-text">using a system including </div>
      <div class="claim-text">at least one video signal source, </div>
      <div class="claim-text">at least one video display device, </div>
      <div class="claim-text">at least one processor, and </div>
      <div class="claim-text">at least one data communication path, </div>
      <div class="claim-text">the method comprising the steps of:</div>
      <div class="claim-text">(a) generating video signals </div>
      <div class="claim-text">(i) at one of the video signal sources; </div>
      <div class="claim-text">(b) transporting the generated video signals, </div>
      <div class="claim-text">(i) over one of the UTP video communication paths, </div>
      <div class="claim-text">(ii) to at least one of the display devices; </div>
      <div class="claim-text">(c) reproducing video images, </div>
      <div class="claim-text">(i) based on the transported video signals, </div>
      <div class="claim-text">(ii) on the display device; </div>
      <div class="claim-text">(d) producing data conferencing signals </div>
      <div class="claim-text">(i) at the processor; </div>
      <div class="claim-text">(e) transporting the data conferencing signals </div>
      <div class="claim-text">(i) over one of the data communication paths, </div>
      <div class="claim-text">(ii) to at least one of the display devices; and </div>
      <div class="claim-text">(f) displaying information, </div>
      <div class="claim-text">(i) based on the transmitted data conferencing signals, </div>
      <div class="claim-text">(ii) on the display device. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" id="US-6212547-B1-CLM-00021" class="claim">
      <div class="claim-text">21. The method of claim <b>20</b>, comprising the steps of:</div>
      <div class="claim-text">(a) reproducing the images </div>
      <div class="claim-text">(i) based on the video signals </div>
      <div class="claim-text">(ii) in a first window </div>
      <div class="claim-text">(iii) on a display device, and </div>
      <div class="claim-text">(b) the information </div>
      <div class="claim-text">(i) based on the data conference signals </div>
      <div class="claim-text">(ii) in a second window </div>
      <div class="claim-text">(iii) on this display device. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="22" id="US-6212547-B1-CLM-00022" class="claim">
      <div class="claim-text">22. The method of claim <b>21</b>, further comprising the steps of:</div>
      <div class="claim-text">(a) producing control signals; </div>
      <div class="claim-text">(b) transmitting the control signals </div>
      <div class="claim-text">(i) over at least one control communication path, </div>
      <div class="claim-text">(c) controlling the display of the video images </div>
      <div class="claim-text">(i) in response to the transmitted control signals. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="23" id="US-6212547-B1-CLM-00023" class="claim">
      <div class="claim-text">23. The method of claim <b>22</b>, wherein</div>
      <div class="claim-text">(a) at least one unshielded twisted pair of wires defines </div>
      <div class="claim-text">(i) the data path, and </div>
      <div class="claim-text">(ii) the control communication path. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="24" id="US-6212547-B1-CLM-00024" class="claim">
      <div class="claim-text">24. The method of claim <b>20</b>, wherein</div>
      <div class="claim-text">(a) at least two video display devices </div>
      <div class="claim-text">(i) each have an associated processor </div>
      <div class="claim-text">(ii) to each define a workstation, and </div>
      <div class="claim-text">wherein the method includes the step of</div>
      <div class="claim-text">(a) controlling the reproduction of video images and spoken audio </div>
      <div class="claim-text">(i) of a first user of a workstation </div>
      <div class="claim-text">(ii) at the workstation of a second workstation user. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" id="US-6212547-B1-CLM-00025" class="claim">
      <div class="claim-text">25. The method of claim <b>24</b>, wherein</div>
      <div class="claim-text">(a) the information </div>
      <div class="claim-text">(i) based on the data conferencing signals </div>
      <div class="claim-text">(ii) can be displayed interactively on at least two display devices. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="26" id="US-6212547-B1-CLM-00026" class="claim">
      <div class="claim-text">26. The method of claim <b>25</b>, further comprising the step of:</div>
      <div class="claim-text">(a) reproducing the video images, </div>
      <div class="claim-text">(i) at TV quality. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="27" id="US-6212547-B1-CLM-00027" class="claim">
      <div class="claim-text">27. The method of claim <b>26</b>, wherein</div>
      <div class="claim-text">(a) the video signals are transported in analog. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="28" id="US-6212547-B1-CLM-00028" class="claim">
      <div class="claim-text">28. The method of claim <b>25</b>, further comprising the steps of</div>
      <div class="claim-text">(a) combining video images </div>
      <div class="claim-text">(i) of at least a first and a second user </div>
      <div class="claim-text">(ii) into a mosaic image, and </div>
      <div class="claim-text">(b) reproducing the mosaic image </div>
      <div class="claim-text">(i) on a video display device. </div>
    </div>
    </div> <div class="claim"> <div num="29" id="US-6212547-B1-CLM-00029" class="claim">
      <div class="claim-text">29. A method for conducting a teleconference,</div>
      <div class="claim-text">among a plurality of users, </div>
      <div class="claim-text">over at least one unshielded twisted pair of wires defining both a UTP data path and a UTP AV path, and </div>
      <div class="claim-text">using a system including </div>
      <div class="claim-text">a plurality of workstations, each associated with at least one user and having a video display device, </div>
      <div class="claim-text">the method comprising the steps of: </div>
      <div class="claim-text">(a) capturing </div>
      <div class="claim-text">(i) video images and </div>
      <div class="claim-text">(ii) spoken audio </div>
      <div class="claim-text">(iii) of a workstation user; </div>
      <div class="claim-text">(b) representing </div>
      <div class="claim-text">(i) the captured user video images and audio </div>
      <div class="claim-text">(ii) as AV signals </div>
      <div class="claim-text">(c) transporting the AV signals </div>
      <div class="claim-text">(i) over the AV path </div>
      <div class="claim-text">(ii) among the workstations; </div>
      <div class="claim-text">(d) reproducing video images and spoken audio </div>
      <div class="claim-text">(i) based on the AV signals, </div>
      <div class="claim-text">(ii) on t least one of the video display devices; </div>
      <div class="claim-text">(e) transmitting data </div>
      <div class="claim-text">(i) over the data path </div>
      <div class="claim-text">(ii) among the workstations; and </div>
      <div class="claim-text">(f) interactively displaying images </div>
      <div class="claim-text">(i) based on the shared data, </div>
      <div class="claim-text">(ii) on at least two of the video display devices. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="30" id="US-6212547-B1-CLM-00030" class="claim">
      <div class="claim-text">30. The method of claim <b>29</b>, further comprising the steps of:</div>
      <div class="claim-text">(a) displaying images </div>
      <div class="claim-text">(i) based on the video signals </div>
      <div class="claim-text">(ii) in a first window on one of the display devices; and </div>
      <div class="claim-text">(b) displaying information </div>
      <div class="claim-text">(i) based on the data conferencing signals </div>
      <div class="claim-text">(ii) in a second window on the display device. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="31" id="US-6212547-B1-CLM-00031" class="claim">
      <div class="claim-text">31. The method of claim <b>29</b>, further comprising the step of:</div>
      <div class="claim-text">(a) using control signals </div>
      <div class="claim-text">(i) transmitted on the data path </div>
      <div class="claim-text">(ii) to control the reproduction </div>
      <div class="claim-text">(1) of the video images and audio. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="32" id="US-6212547-B1-CLM-00032" class="claim">
      <div class="claim-text">32. The method of claim <b>29</b>, further comprising the step of:</div>
      <div class="claim-text">(a) reproducing the video images </div>
      <div class="claim-text">(i) at TV quality. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="33" id="US-6212547-B1-CLM-00033" class="claim">
      <div class="claim-text">33. The method of claim <b>32</b>, wherein</div>
      <div class="claim-text">(a) the video signals are transported in analog. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="34" id="US-6212547-B1-CLM-00034" class="claim">
      <div class="claim-text">34. The method of claim <b>32</b>, wherein</div>
      <div class="claim-text">(a) the video images are reproduced in color. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="35" id="US-6212547-B1-CLM-00035" class="claim">
      <div class="claim-text">35. The method of claim <b>32</b>, further comprising the steps of:</div>
      <div class="claim-text">(a) combining video images </div>
      <div class="claim-text">(i) of at least a first and a second user </div>
      <div class="claim-text">(ii) into a mosaic image, and </div>
      <div class="claim-text">(b) reproducing the mosaic image </div>
      <div class="claim-text">(i) on one of the video display devices. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="36" id="US-6212547-B1-CLM-00036" class="claim">
      <div class="claim-text">36. The method of claim <b>32</b>, further comprising the steps of:</div>
      <div class="claim-text">(a) allowing a first user </div>
      <div class="claim-text">(i) to use a first graphical user interface </div>
      <div class="claim-text">(ii) to select a user </div>
      <div class="claim-text">(iii) from a plurality of users; and </div>
      <div class="claim-text">(b) allowing the first user </div>
      <div class="claim-text">(i) to use a second graphical user interface </div>
      <div class="claim-text">(ii) to select a collaboration type </div>
      <div class="claim-text">(iii) from a group of collaboration types; and </div>
      <div class="claim-text">(c) establishing communication </div>
      <div class="claim-text">(i) of the selected collaboration type </div>
      <div class="claim-text">(ii) from the first user </div>
      <div class="claim-text">(iii) to the selected user. </div>
    </div>
    </div> <div class="claim"> <div num="37" id="US-6212547-B1-CLM-00037" class="claim">
      <div class="claim-text">37. A videoconferencing system</div>
      <div class="claim-text">for operation, </div>
      <div class="claim-text">with an infrastructure including </div>
      <div class="claim-text">at least one video signal source and </div>
      <div class="claim-text">at least one video display device, </div>
      <div class="claim-text">at least one data communication path </div>
      <div class="claim-text">arranged for carrying the data conferencing signals, to one of the display devices, and </div>
      <div class="claim-text">over at least one unshielded twisted pair of wires defining a video communication path, </div>
      <div class="claim-text">arranged for transport of video signals, </div>
      <div class="claim-text">originating at one of the signal sources, </div>
      <div class="claim-text">to at least one of the display devices, </div>
      <div class="claim-text">the system comprising</div>
      <div class="claim-text">(a) at least one processor </div>
      <div class="claim-text">(i) capable of </div>
      <div class="claim-text">(1) providing data conferencing signals, and </div>
      <div class="claim-text">(2) communicating with the data communication path </div>
      <div class="claim-text">wherein the system is configured to</div>
      <div class="claim-text">(i) reproduce video images, </div>
      <div class="claim-text">(1) based on the transported video signals, </div>
      <div class="claim-text">(2) on the display device, and </div>
      <div class="claim-text">(ii) display information, </div>
      <div class="claim-text">(1) based on the carried data conferencing signals, </div>
      <div class="claim-text">(2) on the display device. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="38" id="US-6212547-B1-CLM-00038" class="claim">
      <div class="claim-text">38. The system of claim <b>37</b>, wherein the system is further configured to</div>
      <div class="claim-text">(a) operate with </div>
      <div class="claim-text">(i) data communication path defined by </div>
      <div class="claim-text">(ii) at least one unshielded twisted pair of wires. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="39" id="US-6212547-B1-CLM-00039" class="claim">
      <div class="claim-text">39. The system of claim <b>38</b>, wherein:</div>
      <div class="claim-text">(a) images </div>
      <div class="claim-text">(i) based on the video signals </div>
      <div class="claim-text">(ii) can be reproduced </div>
      <div class="claim-text">(iii) in a first window </div>
      <div class="claim-text">(iv) on a display device, and </div>
      <div class="claim-text">(b) information </div>
      <div class="claim-text">(i) based on the data conference signals </div>
      <div class="claim-text">(ii) can be displayed </div>
      <div class="claim-text">(iii) in a second window </div>
      <div class="claim-text">(iv) on this display device. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="40" id="US-6212547-B1-CLM-00040" class="claim">
      <div class="claim-text">40. The system of claim <b>38</b>, wherein the infrastructure further includes:</div>
      <div class="claim-text">a control communication path, and </div>
      <div class="claim-text">wherein, the system is further configured</div>
      <div class="claim-text">(i) to respond to control signals </div>
      <div class="claim-text">(1) transmitted over the control communication path </div>
      <div class="claim-text">(ii) to control the display of the video images. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="41" id="US-6212547-B1-CLM-00041" class="claim">
      <div class="claim-text">41. The system of claim <b>40</b>, wherein</div>
      <div class="claim-text">(a) at least one unshielded twisted pair of wires defines </div>
      <div class="claim-text">(i) the data path, and </div>
      <div class="claim-text">(ii) the control communication path. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="42" id="US-6212547-B1-CLM-00042" class="claim">
      <div class="claim-text">42. The system of claim <b>40</b>, wherein</div>
      <div class="claim-text">(a) at least two video display devices </div>
      <div class="claim-text">(i) each have an associated processor </div>
      <div class="claim-text">(ii) to each define a workstation, and </div>
      <div class="claim-text">wherein the system is further configured</div>
      <div class="claim-text">(iii) to control the reproduction of video images and spoken audio </div>
      <div class="claim-text">(1) of a first user of a workstation </div>
      <div class="claim-text">(2) at the workstation of a second workstation user. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="43" id="US-6212547-B1-CLM-00043" class="claim">
      <div class="claim-text">43. The system of claim <b>42</b>, wherein</div>
      <div class="claim-text">(a) the information </div>
      <div class="claim-text">(i) based on the data conferencing signals </div>
      <div class="claim-text">(b) can be displayed </div>
      <div class="claim-text">(i) interactively. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="44" id="US-6212547-B1-CLM-00044" class="claim">
      <div class="claim-text">44. The system of claim <b>43</b>, wherein the system is configured to</div>
      <div class="claim-text">(a) reproduce the video images, </div>
      <div class="claim-text">(i) at TV quality. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="45" id="US-6212547-B1-CLM-00045" class="claim">
      <div class="claim-text">45. The system of claim <b>44</b>, wherein</div>
      <div class="claim-text">(a) the video signals can be transported in analog.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES54523319" lang="EN" load-source="patent-office" class="description">
    <heading>RELATED CASES</heading> <p>This application is a continuation of U.S. application Ser. No. 08/660,805 filed Jun. 7, 1996, now U.S. Pat. No. 5,758,079, which is itself a divisional of U.S. application Ser. No. 08/131,523 filed Oct. 1, 1993, now U.S. Pat. No. 5,689,641.</p>
    <heading>BACKGROUND OF THE INVENTION</heading> <p>1. Field of the Invention</p>
    <p>This invention relates to videoconferencing systems and, more particularly, to hardware appropriate for using at a videoconferencing workstation.</p>
    <p>2. Background</p>
    <p>The present invention relates to computer-based systems for enhancing collaboration between and among individuals who are separated by distance and/or time (referred to herein as “distributed collaboration”). Principal among the invention's goals is to replicate in a desktop environment, to the maximum extent possible, the full range, level and intensity of interpersonal communication and information sharing which would occur if all the participants were together in the same room at the same time (referred to herein as “face-to-face collaboration”).</p>
    <p>It is well known to behavioral scientists that interpersonal communication involves a large number of subtle and complex visual cues, referred to by names like “eye contact” and “body language,” which provide additional information over and above the spoken words and explicit gestures. These cues are, for the most part, processed subconsciously by the participants, and often control the course of a meeting.</p>
    <p>In addition to spoken words, demonstrative gestures and behavioral cues, collaboration often involves the sharing of visual information—e.g., printed material such as articles, drawings, photographs, charts and graphs, as well as videotapes and computer-based animations, visualizations and other displays—in such a way that the participants can collectively and interactively examine, discuss, annotate and revise the information. This combination of spoken words, gestures, visual cues and interactive data sharing significantly enhances the effectiveness of collaboration in a variety of contexts, such as “brainstorming” sessions among professionals in a particular field, consultations between one or more experts and one or more clients, sensitive a particular field, consultations between one or more experts and one or more clients, sensitive business or political negotiations, and the like. In distributed collaboration settings, then, where the participants cannot be in the same place at the same time, the beneficial effects of face-to-face collaboration will be realized only to the extent that each of the remotely located participants can be “recreated” at each site.</p>
    <p>To illustrate the difficulties inherent in reproducing the beneficial effects of face-to-face collaboration in a distributed collaboration environment, consider the case of decision-making in the fast-moving commodities trading markets, where many thousands of dollars of profit (or loss) may depend on an expert trader making the right decision within hours, or even minutes, of receiving a request from a distant client. The expert requires immediate access to a wide range of potentially relevant information such as financial data, historical pricing information, current price quotes, newswire services, government policies and programs, economic forecasts, weather reports, etc. Much of this information can be processed by the expert in isolation. However, before making a decision to buy or sell, he or she will frequently need to discuss the information with other experts, who may be geographically dispersed, and with the client. One or more of these other experts may be in a meeting, on another call, or otherwise temporarily unavailable. In this event, the expert must communicate “asynchronously”—to bridge time as well as distance.</p>
    <p>As discussed below, prior art desktop videoconferencing systems provide, at best, only a partial solution to the challenges of distributed collaboration in real time, primarily because of their lack of high-quality video (which is necessary for capturing the visual cues discussed above) and their limited data sharing capabilities. Similarly, telephone answering machines, voice mail, fax machines and conventional electronic mail systems provide incomplete solutions to the problems presented by deferred (asynchronous) collaboration because they are totally incapable of communicating visual cues, gestures, etc. and, like conventional videoconferencing systems, are generally limited in the richness of the data that can be exchanged.</p>
    <p>It has been proposed to extend traditional videoconferencing capabilities from conference centers, where groups of participants must assemble in the same room, to the desktop, where individual participants may remain in their office or home. Such a system is disclosed in U.S. Pat. No. 4,710,917 to Tompkins et al. for Video Conferencing Network issued on Dec. 1, 1987. It has also been proposed to augment such video conferencing systems with limited “video mail” facilities. However, such dedicated videoconferencing systems (and extensions thereof) do not effectively leverage the investment in existing embedded information infrastructures—such as desktop personal computers and workstations, local area network (LAN) and wide area network (WAN) environments, building wiring, etc.—to facilitate interactive sharing of data in the form of text, images, charts, graphs, recorded video, screen displays and the like. That is, they attempt to add computing capabilities to a videoconferencing system, rather than adding multimedia and collaborative capabilities to the user's existing computer system. Thus, while such systems may be useful in limited contexts, they do not provide the capabilities required for maximally effective collaboration, and are not cost-effective.</p>
    <p>Conversely, audio and video capture and processing capabilities have recently been integrated into desktop and portable personal computers and workstations (hereinafter generically referred to as “workstations”). These capabilities have been used primarily in desktop multimedia authoring systems for producing CD-ROM-based works. While such systems are capable of processing, combining, and recording audio, video and data locally (i.e., at the desktop), they do not adequately support networked collaborative environments, principally due to the substantial bandwidth requirements for real-time transmission of high-quality, digitized audio and full-motion video which preclude conventional LANs from supporting more than a few workstations. Thus, although currently available desktop multimedia computers frequently include videoconferencing and other multimedia or collaborative capabilities within their advertised feature set (see, e.g., A. Reinhardt, “Video Conquers the Desktop,” BYTE, September 1993, pp. 64-90), such systems have not yet solved the many problems inherent in any practical implementation of a scalable collaboration system.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>In accordance with the present invention, computer hardware, software and communications technologies are combined in novel ways to produce a multimedia collaboration system that greatly facilitates distributed collaboration, in part by replicating the benefits of face-to-face collaboration. The system tightly integrates a carefully selected set of multimedia and collaborative capabilities, principal among which are desktop teleconferencing and multimedia mail.</p>
    <p>As used herein, desktop teleconferencing includes real-time audio and/or video teleconferencing, as well as data conferencing. Data conferencing, in turn, includes snapshot sharing (sharing of “snapshots” of selected regions of the user's screen), application sharing (shared control of running applications), shared whiteboard (equivalent to sharing a “blank” window), and associated telepointing and annotation capabilities. Teleconferences may be recorded and stored for later playback, including both audio/video and all data interactions.</p>
    <p>While desktop teleconferencing supports real-time interactions, multimedia mail permits the asynchronous exchange of arbitrary multimedia documents, including previously recorded teleconferences. Indeed, it is to be understood that the multimedia capabilities underlying desktop teleconferencing and multimedia mail also greatly facilitate the creation, viewing, and manipulation of high-quality multimedia documents in general, including animations and visualizations that might be developed, for example, in the course of information analysis and modeling. Further, these animations and visualizations may be generated for individual rather than collaborative use, such that the present invention has utility beyond a collaboration context.</p>
    <p>The invention provides for a collaborative multimedia workstation (CMW) system wherein very high-quality audio and video capabilities can be readily superimposed onto an enterprise's existing computing and network infrastructure, including workstations, LANs, WANs, and building wiring.</p>
    <p>In a preferred embodiment, the system architecture employs separate real-time and asynchronous networks—the former for real-time audio and video, and the latter for non-real-time audio and video, text, graphics and other data, as well as control signals. These networks are interoperable across different computers (e.g., Macintosh, Intel-based PCs, and Sun workstations), operating systems (e.g., Apple System 7, DOS/Windows, and UNIX) and network operating systems (e.g., Novell Netware and Sun ONC+). In many cases, both networks can actually share the same cabling and wall jack connector.</p>
    <p>The system architecture also accommodates the situation in which the user's desktop computing and/or communications equipment provides varying levels of media-handling capability. For example, a collaboration session—whether real-time or asynchronous—may include participants whose equipment provides capabilities ranging from audio only (a telephone) or data only (a personal computer with a modem) to a full complement of real-time, high-fidelity audio and full-motion video, and high-speed data network facilities.</p>
    <p>The CMW system architecture is readily scalable to very large enterprise-wide network environments accommodating thousands of users. Further, it is an open architecture that can accommodate appropriate standards. Finally, the CMW system incorporates an intuitive, yet powerful, user interface, making the system easy to learn and use.</p>
    <p>The present invention thus provides a distributed multimedia collaboration environment that achieves the benefits of face-to-face collaboration as nearly as possible, leverages (“snaps on to”) existing computing and network infrastructure to the maximum extent possible, scales to very large networks consisting of thousand of workstations, accommodates emerging standards, and is easy to learn and use. The specific nature of the invention, as well as its objects, features, advantages and uses, will become more readily apparent from the following detailed description and examples, and from the accompanying drawings.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p>FIG. 1 is a diagrammatic representation of a multimedia collaboration system embodiment of the present invention.</p>
    <p>FIGS. 2A and 2B are representations of a computer screen illustrating, to the extent possible in a still image, the full-motion video and related user interface displays which may be generated during operation of a preferred embodiment of the invention.</p>
    <p>FIG. 3 is a block and schematic diagram of a preferred embodiment of a “multimedia local area network” (MLAN) of the present invention.</p>
    <p>FIG. 4 is a block and schematic diagram illustrating how a plurality of geographically dispersed MLANs of the type shown in FIG. 3 can be connected via a wide area network in accordance with the present invention.</p>
    <p>FIG. 5 is a schematic diagram illustrating how collaboration sites at distant locations L<b>1</b>-L<b>8</b> are conventionally interconnected over a wide area network by individually connecting each site to every other site.</p>
    <p>FIG. 6 is a schematic diagram illustrating how collaboration sites at distant locations L<b>1</b>-L<b>8</b> are interconnected over a wide area network in an embodiment of the invention using a multi-hopping approach.</p>
    <p>FIG. 7 is a block diagram illustrating an embodiment of video mosaicing circuitry provided in the MLAN of FIG. <b>3</b>.</p>
    <p>FIGS. 8A, <b>8</b>B and <b>8</b>C illustrate the video window on a typical computer screen which may be generated during operation of the present invention, and which contains only the callee for two-party calls (<b>8</b>A) and a video mosaic of all participants, e.g., for four-party (<b>8</b>B) or eight-party (<b>8</b>C) conference calls.</p>
    <p>FIG. 9 is a block diagram illustrating an embodiment of audio mixing circuitry provided in the MLAN of FIG. <b>3</b>.</p>
    <p>FIG. 10 is a block diagram illustrating video cut-and-paste circuitry provided in the MLAN of FIG. <b>3</b>.</p>
    <p>FIG. 11 is a schematic diagram illustrating typical operation of the video cut-and-paste circuitry in FIG. <b>10</b>.</p>
    <p>FIGS. 12-17 (consisting of FIGS. 12A, <b>12</b>B, <b>13</b>A, <b>13</b>B, <b>14</b>A, <b>14</b>B, <b>15</b>A, <b>15</b>B, <b>16</b>, <b>17</b>A and <b>17</b>B) illustrate various examples of how the present invention provides video mosaicing, video cut-and-pasting, and audio mixing at a plurality of distant sites for transmission over a wide area network in order to provide, at the CMW of each conference participant, video images and audio captured from the other conference participants.</p>
    <p>FIGS. 18A and 18B illustrate two different embodiments of a CMW which may be employed in accordance with the present invention.</p>
    <p>FIG. 19 is a schematic diagram of an embodiment of a CMW add-on box containing integrated audio and video I/O circuitry in accordance with the present invention.</p>
    <p>FIG. 20 illustrates CMW software in accordance with an embodiment of the present invention, integrated with standard multitasking operating system and applications software.</p>
    <p>FIG. 21 illustrates software modules which may be provided for running on the MLAN Server in the MLAN of FIG. 3 for controlling operation of the AV and Data Networks.</p>
    <p>FIG. 22 illustrates an enlarged example of “speed-dial” face icons of certain collaboration participants in a Collaboration Initiator window on a typical CMW screen which may be generated during operation of the present invention.</p>
    <p>FIG. 23 is a diagrammatic representation of the basic operating events occurring in a preferred embodiment of the present invention during initiation of a two-party call.</p>
    <p>FIG. 24 is a block and schematic diagram illustrating how physical connections are established in the MLAN of FIG. 3 for physically connecting first and second workstations for a two-party videoconference call.</p>
    <p>FIG. 25 is a block and schematic diagram illustrating how physical connections are established in MLANs such as illustrated in FIG. 3, for a two-party call between a first CMW located at one site and a second CMW located at a remote site.</p>
    <p>FIGS. 26 and 27 are block and schematic diagrams illustrating how conference bridging is provided in the MLAN of FIG. <b>3</b>.</p>
    <p>FIG. 28 diagrammatically illustrates how a snapshot with annotations may be stored in a plurality of bitmaps during data sharing.</p>
    <p>FIG. 29 is a schematic and diagrammatic illustration of the interaction among multimedia mail (MMM), multimedia call/conference recording (MMCR) and multimedia document management (MMDM) facilities.</p>
    <p>FIG. 30 is a schematic and diagrammatic illustration of the multimedia document architecture employed in an embodiment of the invention.</p>
    <p>FIG. 31A illustrates a centralized Audio/Video Storage Server.</p>
    <p>FIG. 31B is a schematic and diagrammatic illustration of the interactions between the Audio/Video Storage Server and the remainder of the CMW System.</p>
    <p>FIG. 31C illustrates an alternative embodiment of the interactions illustrated in FIG. <b>31</b>B.</p>
    <p>FIG. 31D is a schematic and diagrammatic illustration of the integration of MMM, MMCR and MMDM facilities in an embodiment of the invention.</p>
    <p>FIG. 32 illustrates a generalized hardware implementation of a scalable Audio/Video Storage Server.</p>
    <p>FIG. 33 illustrates a higher throughput version of the server illustrated in FIG. 32, using SCSI-based crosspoint switching to increase the number of possible simultaneous file transfers.</p>
    <p>FIG. 34 illustrates the resulting multimedia collaboration environment achieved by the integration of audio/video/data teleconferencing and MMCR, MMM and MMDM.</p>
    <p>FIGS. 35-42 illustrate a series of CMW screens which may be generated during operation of the present invention for a typical scenario involving a remote expert who takes advantage of many of the features provided by the present invention.</p>
    <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading> <heading>Overall System Architecture</heading> <p>Referring initially to FIG. 1, illustrated therein is an overall diagrammatic view of a multimedia collaboration system in accordance with the present invention. As shown, each of a plurality of “multimedia local area networks” (MLANs) <b>10</b> connects, via lines <b>13</b>, a plurality of CMWs <b>12</b>-<b>1</b> to <b>12</b>-<b>10</b> and provides audio/video/data networking for supporting collaboration among CMW users. WAN <b>15</b> in turn connects multiple MLANs <b>10</b>, and typically includes appropriate combinations of common carrier analog and digital transmission networks. Multiple MLANs <b>10</b> on the same physical premises may be connected via bridges/routes <b>11</b>, as shown, to WANs and one another.</p>
    <p>In accordance with the present invention, the system of FIG. 1 accommodates both “real time” delay- and jitter-sensitive signals (e.g., real-time audio and video teleconferencing) and classical asynchronous data (e.g., data control signals as well as shared textual, graphics and other media) communication among multiple CMWs <b>12</b> regardless of their location. Although only ten CMWs <b>12</b> are illustrated in FIG. 1, it will be understood that many more could be provided. As also indicated in FIG. 1, various other multimedia resources <b>16</b> (e.g., VCRs, laserdiscs, TV feeds, etc.) are connected to MLANs <b>10</b> and are thereby accessible by individual CMWs <b>12</b>.</p>
    <p>CMW <b>12</b> in FIG. 1 may use any of a variety of types of operating systems, such as Apple System 7, UNIX, DOS/Windows and OS/2. The CMWs can also have different types of window systems. Specific embodiments of a CMW <b>12</b> are described hereinafter in connection with FIGS. 18A and 18B. Note that this invention allows for a mix of operating systems and window systems across individual CMWs.</p>
    <p>CMW <b>12</b> provides real-time audio/video/data capabilities along with the usual data processing capabilities provided by its operating system. For example, FIG. 2A illustrates a CMW screen containing live, full-motion video of three conference participants, while FIG. 2B illustrates data and shared annotated by those conferees (lower left window). CMW <b>12</b> provides for bidirectional communication, via lines <b>13</b>, within MLAN <b>10</b>, for audio/video signals as well as data signals. Audio/video signals transmitted from a CMW <b>12</b> typically comprise a high-quality live video image and audio of the CMW operator. These signals are obtained from a video camera and microphone provided at the CMW (via an add-on unit or partially or totally integrated into the CMW), processed, and then made available to low-cost network transmission subsystems.</p>
    <p>Audio/video signals received by a CMW <b>12</b> from MLAN <b>10</b> may typically include: video images of one or more conference participants and associated audio, video and audio from multimedia mail, previously recorded audio/video from previous calls and conferences, and standard broadcast television (e.g., CNN). Received video signals are displayed on the CMW screen or on an adjacent monitor, and the accompanying audio is reproduced by a speaker provided in or near the CMW. In general, the required transducers and signal processing hardware could be integrated into the CMW, or be provided via a CMW add-on unit, as appropriate.</p>
    <p>In the preferred embodiment, it has been found particularly advantageous to provide the above-described video at standard NTSC-quality TV performance (i.e., 30 frames per second at 640×480 pixels per frame and the equivalent of 24 bits of color per pixel) with accompanying high-fidelity audio (typically between 7 and 15 KHz).</p>
    <heading>Multimedia Local Area Network</heading> <p>Referring next to FIG. 3, illustrated therein is a preferred embodiment of MLAN <b>10</b> having ten CMWs (<b>12</b>-<b>1</b>,-<b>12</b>-<b>10</b>), coupled therein via lines <b>13</b> <i>a </i>and <b>13</b> <i>b. </i>MLAN <b>10</b> typically extends over a distance from a few hundred feet to a few miles, and is usually located within a building or a group of proximate buildings.</p>
    <p>Given the current state of networking technologies, it is useful (for the sake of maintaining quality and minimizing costs) to provide separate signal paths for real-time audio/video and classical asynchronous data communications (including digitized audio and video enclosures of multimedia mail messages that are free from real-time delivery constraints). At the moment, analog methods for carrying real-time audio/video are preferred. In the future, digital methods may be used. Eventually, digital audio and video signal paths may be multiplexed with the data signal path as a common digital stream. Another alternative is to multiplex real-time and asynchronous data paths together using analog multiplexing methods. For the purposes of illustration, however, these two signal paths are treated as using physically separate wires. Further, as this embodiment uses analog networking for audio and video, it also physically separates the real-time and asynchronous switching vehicles and, in particular, assumes an analog audio/video switch. In the future, a common switching vehicle (e.g., ATM) could be used.</p>
    <p>The MLAN <b>10</b> thus can be implemented in the preferred embodiment using conventional technology, such as typical Data LAN hubs <b>25</b> and A/V Switching Circuitry <b>30</b> (as used in television studios and other closed-circuit television networks), linked to the CMWs <b>12</b> via appropriate transceivers and unshielded twisted pair (UTP) wiring. Note in FIG. 1 that lines <b>13</b>, which interconnect each CMW <b>12</b> within its respective MLAN <b>10</b>, comprise two sets of lines <b>13</b> <i>a </i>and <b>13</b> <i>b. </i>Lines <b>13</b> <i>a </i>provide bidirectional communication of audio/video within MLAN <b>10</b>, while lines <b>13</b> <i>b </i>provide for the bidirectional communication of data. This separation permits conventional LANs to be used for data communications and a supplemental network to be used for audio/video communications. Although this separation is advantageous in the preferred embodiment, it is again to be understood that audio/video/data networking can also be implemented using a single pair of lines for both audio/video and data communications via a very wide variety of analog and digital multiplexing schemes.</p>
    <p>While lines <b>13</b> <i>a </i>and <b>13</b> <i>b </i>may be implemented in various ways, it is currently preferred to use commonly installed 4-pair UTP telephone wires, wherein one pair is used for incoming video with accompanying audio (mono or stereo) multiplexed in, wherein another pair is used for outgoing multiplexed audio/video, and wherein the remaining two pairs are used for carrying incoming and outgoing data in ways consistent with existing LANs. For example, 10BaseT Ethernet uses RJ-45 pins <b>1</b>, <b>2</b>, <b>4</b>, and <b>6</b>, leaving pins <b>3</b>, <b>5</b>, <b>7</b>, and <b>8</b> available for the two A/V twisted pairs. The resulting system is compatible with standard (AT&amp;T 258A, EIA/TIA 568, 8P8C, 10BaseT, ISDN, 6P6C, etc.) telephone wiring found commonly throughout telephone and LAN cable plants in most office buildings throughout the world. These UTP wires are used in a hierarchy or peer arrangements of star topologies to create MLAN <b>10</b>, described below. Note that the distance range of the data wires often must match that of the video and audio. Various UTP-compatible data LAN networks may be used, such as Ethernet, token ring, FDDI, ATM, etc. For distances longer than the maximum distance specified by the data LAN protocol, data signals can be additionally processed for proper UTP operations.</p>
    <p>As shown in FIG. 3, lines <b>13</b> <i>a </i>from each CMW <b>12</b> are coupled to a conventional Data LAN hub <b>25</b>, which facilitates the communication of data (including control signals) among such CMWs. Lines <b>13</b> <i>b </i>in FIG. 3 are connected to A/V Switching Circuitry <b>30</b>. One or more conference bridges <b>35</b> are coupled to A/V Switching Circuitry <b>30</b> and possibly (if needed) the Data LAN hub <b>25</b>, via lines <b>35</b> <i>b </i>and <b>35</b> <i>a</i>, respectively, for providing multi-party conferencing in a particularly advantageous manner, as will hereinafter be described in detail. A WAN gateway <b>40</b> provides for bidirectional communication between MLAN <b>10</b> and WAN <b>15</b> in FIG. <b>1</b>. For this purpose, Data LAN hub <b>25</b> and A/V Switching Circuitry <b>30</b> are coupled to WAN gateway <b>40</b> via outputs <b>25</b> <i>a </i>and <b>30</b> <i>a, </i>respectively. Other devices connect to the A/V Switching Circuitry <b>30</b> and Data LAN hub <b>25</b> to add additional features (such as multimedia mail, conference recording, etc.) as discussed below.</p>
    <p>Control of A/V Switching Circuitry <b>30</b>, conference bridges <b>35</b> and WAN gateway <b>40</b> in FIG. 3 is provided by MLAN Server <b>60</b> via lines <b>60</b> <i>b</i>, <b>60</b> <i>c</i>, and <b>60</b> <i>d, </i>respectively. In one embodiment, MLAN Server <b>60</b> supports the TCP/IP network protocol suite. Accordingly, software processes on CMWs <b>12</b> communicate with one another and MLAN Server <b>60</b> via MLAN <b>10</b> using these protocols. Other network protocols could also be used, such as IPX. The manner in which software running on MLAN Server <b>60</b> controls the operation of MLAN <b>10</b> will be described in detail hereinafter.</p>
    <p>Note in FIG. 3 that Data LAN hub <b>25</b>, A/V Switching Circuitry <b>30</b> and MLAN Server <b>60</b> also provide respective lines <b>25</b> <i>b</i>, <b>30</b> <i>b</i>, and <b>60</b> <i>e </i>for coupling to additional multimedia resources <b>16</b> (FIG. <b>1</b>), such as multimedia document management, multimedia databases, radio/TV channels, etc. Data LAN hub <b>25</b> (via bridges/routers <b>11</b> in FIG. 1) and A/V Switching Circuitry <b>30</b> additionally provide lines <b>25</b> <i>c </i>and <b>30</b> <i>c </i>for coupling to one or more other MLANs <b>10</b> which may be in the same locality (i.e., not far enough away to require use of WAN technology). Where WANs are required, WAN gateways <b>40</b> are used to provide highest quality compression methods and standards in a shared resource fashion, thus minimizing costs at the workstation for a given WAN quality level, as discussed below.</p>
    <p>The basic operation of the preferred embodiment of the resulting collaboration system shown in FIGS. 1 and 3 will next be considered. Important features of the present invention reside in providing not only multi-party real-time desktop audio/video/data teleconferencing among geographically distributed CMWs, but also in providing from the same desktop audio/video/data/text/graphics mail capabilities, as well as access to other resources, such as databases, audio and video files, overview cameras, standard TV channels, etc. FIG. 2B illustrates a CMW screen showing a multimedia EMAIL mailbox (top left window) containing references to a number of received messages along with a video enclosure (top right window) to the selected message.</p>
    <p>Returning to FIGS. 1 and 3, A/V Switching Circuitry <b>30</b> (whether digital or analog as in the preferred embodiment) provides common audio/video switching for CMWs <b>12</b>, conference bridges <b>35</b>, WAN gateway <b>40</b> and multimedia resources <b>16</b>, as determined by MLAN Server <b>60</b>, which in turn controls conference bridges <b>35</b> and WAN gateway <b>40</b>. Similarly, asynchronous data is communicated within MLAN <b>10</b> utilizing common data communications formats where possible (e.g., for snapshot sharing) so that the system can handle such data in a common manner, regardless of origin, thereby facilitating multimedia mail and data sharing as well as audio/video communications.</p>
    <p>For example, to provide multi-party teleconferencing, an initiating CMW <b>12</b> signals MLAN Server <b>60</b> via Data LAN hub <b>25</b> identifying the desired conference participants. After determining which of these conferees will accept the call, MLAN Server <b>60</b> controls A/V Switching Circuitry <b>30</b> (and CMW software via the data network) to set up the required audio/video and data paths to conferees at the same location as the initiating CMW.</p>
    <p>When one or more conferees are at distant locations, the respective MLAN Servers <b>60</b> of the involved MLANs <b>10</b>, on a peer-to-peer basis, control their respective A/V Switching Circuitry <b>30</b>, conference bridges <b>35</b>, and WAN gateways <b>40</b> to set up appropriate communication paths (via WAN <b>15</b> in FIG. 1) as required for interconnecting the conferees. MLAN Servers <b>60</b> also communicate with one another via data paths so that each MLAN <b>10</b> contains updated information as to the capabilities of all of the system CMWs <b>12</b>, and also the current locations of all parties available for teleconferencing.</p>
    <p>The data conferencing component of the above-described system supports the sharing of visual information at one or more CMWs (as described in greater detail below). This encompasses both “snapshot sharing” (sharing “snapshots” of complete or partial screens, or of one or more selected windows) and “application sharing” (sharing both the control and display of running applications). When transferring images, lossless or slightly lossy image compression can be used to reduce network bandwidth requirements and user-perceived delay while maintaining high image quality.</p>
    <p>In all cases, any participant can point at or annotate the shared data. These associated telepointers and annotations appear on every participant's CMW screen as they are drawn (i.e., effectively in real time). For example, note FIG. 2B which illustrates a typical CMW screen during a multi-party teleconferencing session, wherein the screen contains annotated shared data as well as video images of the conferees. As described in greater detail below, all or portions of the audio/video and data of the teleconference can be recorded at a CMW (or within MLAN <b>10</b>), complete with all the data interactions.</p>
    <p>In the above-described preferred embodiment, audio/video file services can be implemented either at the individual CMWs <b>12</b> or by employing a centralized audio/video storage server. This is one example of the many types of additional servers that can be added to the basic system of MLANs <b>10</b>. A similar approach is used for incorporating other multimedia services, such as commercial TV channels, multimedia mail, multimedia document management, multimedia conference recording, visualization servers, etc. (as described in greater detail below). Certainly, applications that run self-contained on a CMW can be readily added, but the invention extends this capability greatly in the way that MLAN <b>10</b>, storage and other functions are implemented and leveraged.</p>
    <p>In particular, standard signal formats, network interfaces, user interface messages, and call models can allow virtually any multimedia resource to be smoothly integrated into the system. Factors facilitating such smooth integration include: (i) a common mechanism for user access across the network; (ii) a common metaphor (e.g., placing a call) for the user to initiate use of such resource; (iii) the ability for one function (e.g., a multimedia conference or multimedia database) to access and exchange information with another function (e.g., multimedia mail); and (iv) the ability to extend such access of one networked function by another networked function to relatively complex nestings of simpler functions (for example, record a multimedia conference in which a group of users has accessed multimedia mail messages and transferred them to a multimedia database, and then send part of the conference recording just created as a new multimedia mail message, utilizing a multimedia mail editor if necessary).</p>
    <p>A simple example of the smooth integration of functions made possible by the above-described approach is that the GUI and software used for snapshot sharing (described below) can also be used as an input/output interface for multimedia mail and more general forms of multimedia documents. This can be accomplished by structuring the interprocess communication protocols to be uniform across all these applications. More complicated examples—specifically multimedia conference recording, multimedia mail and multimedia document management—will be presented in detail below.</p>
    <heading>Wide Area Network</heading> <p>Next to be described in connection with FIG. 4 is the advantageous manner in which the present invention provides for real-time audio/video/data communication among geographically dispersed MLANs <b>10</b> via WAN <b>15</b> (FIG. <b>1</b>), whereby communication delays, cost and degradation of video quality are significantly minimized from what would otherwise be expected.</p>
    <p>Four MLANs <b>10</b> are illustrated at locations A, B, C and D. CMWs <b>12</b>-<b>1</b> to <b>12</b>-<b>10</b>, A/V Switching Circuitry <b>30</b>, Data LAN hub <b>25</b>, and WAN gateway <b>40</b> at each location correspond to those shown in FIGS. 1 and 3. Each WAN gateway <b>40</b> in FIG. 4 will be seen to comprise a router/codec (R&amp;C) bank <b>42</b> coupled to WAN <b>15</b> via WAN switching multiplexer <b>44</b>. The router is used for data interconnection and the codec is used for audio/video interconnection (for multimedia mail and document transmission, as well as videoconferencing). Codecs from multiple vendors, or supporting various compression algorithms may be employed. In the preferred embodiment, the router and codec are combined with the switching multiplexer to form a single integrated unit.</p>
    <p>Typically, WAN <b>15</b> is comprised of T<b>1</b> or ISDN common-carrier-provided digital links (switched or dedicated), in which case WAN switching multiplexers <b>44</b> are of the appropriate type (T<b>1</b>, ISDN, fractional T<b>1</b>, T<b>3</b>, switched 56 Kbps, etc.). Note that the WAN switching multiplexer <b>44</b> typically creates subchannels whose bandwidth is a multiple of 64 Kbps (i.e., 256 Kbps, 384, 768, etc.) among the T<b>1</b>, T<b>3</b> or ISDN carriers. Inverse multiplexers may be required when using 56 Kbps dedicated or switched services from these carriers.</p>
    <p>In the MLAN <b>10</b> to WAN <b>15</b> direction, router/codec bank <b>42</b> in FIG. 4 provides conventional analog-to-digital conversion and compression of audio/video signals received from A/V Switching Circuitry <b>30</b> for transmission to WAN <b>15</b> via WAN switching multiplexer <b>44</b>, along with transmission and routing of data signals received from Data LAN hub <b>25</b>. In the WAN <b>15</b> to MLAN <b>10</b> direction, each router/codec bank <b>42</b> in FIG. 4 provides digital-to-analog conversion and decompression of audio/video digital signals received from WAN <b>15</b> via WAN switching multiplexer <b>44</b> for transmission to A/V Switching Circuitry <b>30</b>, along with the transmission to Data LAN hub <b>25</b> of data signals received from WAN <b>15</b>.</p>
    <p>The system also provides optimal routes for audio/video signals through the WAN. For example, in FIG. 4, location A can take either a direct route to location D via path <b>47</b>, or a two-hop route through location C via paths <b>48</b> and <b>49</b>. If the direct path <b>47</b> linking location A and location D is unavailable, the multipath route via location C and paths <b>48</b> and <b>49</b> could be used.</p>
    <p>In a more complex network, several multi-hop routes are typically available, in which case the routing system handles the decision making, which for example can be based on network loading considerations. Note the resulting two-level network hierarchy: a MLAN <b>10</b> to MLAN <b>10</b> (i.e., site-to-site) service connecting codecs with one another only at connection endpoints.</p>
    <p>The cost savings made possible by providing the above-described multi-hop capability (with intermediate codec bypassing) are very significant as will become evident by noting the examples of FIGS. 5 and 6. FIG. 5 shows that using the conventional “fully connected mesh” location-to-location approach, thirty-six WAN links are required for interconnecting the nine locations L<b>1</b> to L<b>8</b>. On the other hand, using the above multi-hop capabilities, only nine WAN links are required, as shown in FIG. <b>6</b>. As the number of locations increase, the difference in cost becomes even greater. For example, for 100 locations, the conventional approach would require about 5,000 WAN links, while the multi-hop approach of the present invention would typically require 300 or fewer (possibly considerably fewer) WAN links. Although specific WAN links for the multi-hop approach of the invention would require higher bandwidth to carry the additional traffic, the cost involved is very much smaller as compared to the cost for the very much larger number of WAN links required by the conventional approach.</p>
    <p>At the endpoints of a wide-area call, the WAN switching multiplexer routes audio/video signals directly from the WAN network interface through an available codec to MLAN <b>10</b> and vice versa. At intermediate hops in the network, however, video signals are routed from one network interface on the WAN switching multiplexer to another network interface. Although A/V Switching Circuitry <b>30</b> could be used for this purpose, the preferred embodiment provides switching functionality inside the WAN switching multiplexer. By doing so, it avoids having to route audio/video signals through codecs to the analog switching circuitry, thereby avoiding additional codec delays at the intermediate locations.</p>
    <p>A product capable of performing the basic switching functions described above for WAN switching multiplexer <b>44</b> is available from Teleos Corporation, Eatontown, N.J. (U.S.A.). This product is not known to have been used for providing audio/video multi-hopping and dynamic switching among various WAN links as described above.</p>
    <p>In addition to the above-described multiple-hop approach, the present invention provides a particularly advantageous way of minimizing delay, cost and degradation of video quality in a multi-party video teleconference involving geographically dispersed sites, while still delivering full conference views of all participants. Normally, in order for the CMWs at all sites to be provided with live audio/video of every participant in a teleconference simultaneously, each site has to allocate (in router/codec bank <b>42</b> in FIG. 4) a separate codec for each participant, as well as a like number of WAN trunks (via WAN switching multiplexer <b>44</b> in FIG. <b>4</b>).</p>
    <p>As will next be described, however, the preferred embodiment of the invention advantageously permits each wide area audio/video teleconference to use only one codec at each site, and a minimum number of WAN digital trunks. Basically, the preferred embodiment achieves this most important result by employing “distributed” video mosaicing via a video “cut-and-paste” technology along with distributed audio mixing.</p>
    <heading>Distributed Video Mosaicing</heading> <p>FIG. 7 illustrates a preferred way of providing video mosaicing in the MLAN of FIG. <b>3</b>—i.e., by combining the individual analog video pictures from the individuals participating in a teleconference into a single analog mosaic picture. As shown in FIG. 7, analog video signals <b>112</b>-<b>1</b> to <b>112</b>-n from the participants of a teleconference are applied to video mosaicing circuitry <b>36</b>, which in the preferred embodiment is provided as part of conference bridge <b>35</b> in FIG. <b>3</b>. These analog video inputs <b>112</b>-<b>1</b> to <b>112</b>-n are obtained from the A/V Switching Circuitry <b>30</b> (FIG. 3) and may include video signals from CMWs at one or more distant sites (received via WAN gateway <b>40</b>) as well as from other CMWs at the local site.</p>
    <p>Video mosaicing circuitry, <b>36</b>, represented by block is capable of receiving N individual analog video picture signals (where N is a squared integer, i.e., 4, 9, 16, etc.). Circuitry <b>36</b> first reduces the size of the N input video signals by reducing the resolutions of each by a factor of M (where M is the square root of N (i.e., 2, 3, 4, etc.), and then arranging them in an M-by-M mosaic of N images. The resulting single analog mosaic <b>36</b> <i>a </i>obtained from video mosaicing circuitry <b>36</b> is then transmitted to the individual CMWs for display on the screens thereof.</p>
    <p>As will become evident hereinafter, it may be preferable to send a different mosaic to distant sites, in which case video mosaicing circuitry <b>36</b> would provide an additional mosaic <b>36</b> <i>b </i>for this purpose. A typical displayed mosaic picture (N=4, M=2) showing three participants is illustrated in FIG. 2A. A mosaic containing four participants is shown in FIG. <b>8</b>B. It will be appreciated that, since a mosaic (<b>36</b> <i>a </i>or <b>36</b> <i>b</i>) can be transmitted as a single video picture to an other site, via WAN <b>15</b> (FIGS. <b>1</b> and <b>4</b>), only one codec and digital trunk are required. Of course, if only a single individual video picture is required to be sent from a site, it may be sent directly without being included in a mosaic.</p>
    <p>Note that for large conferences it is possible to employ multiple video mosaics, one for each video window supported by the CMWs (see, e.g., FIG. <b>8</b>C). In very large conferences, it is also possible to display video only from a select focus group whose members are selected by a dynamic “floor control” mechanism. Also note that, with additional mosaic hardware, it is possible to give each CMW its own mosaic. This can be used in small conferences to raise the maximum number of participants (from M<sup>2 </sup>to M<sup>2</sup>+1—i.e., 5, 10, 17, etc.) or to give everyone in a large conference their own “focus group” view.</p>
    <p>Also note that the entire video mosaicing approach described thus far and continued below applies should digital video transmission be used in lieu of analog transmission, particularly since both mosaic and video window implementations use digital formats internally and in current products are transformed to and from analog for external interfacing. In particular, note that mosaicing can be done digitally without decompression with many existing compression schemes. Further, with an all-digital approach, mosaicing can be done as needed directly on the CMW.</p>
    <p>FIG. 9 illustrates audio mixing circuitry <b>38</b>, represented by block for use in conjunction with the video mosaicing circuitry <b>36</b> in FIG. 7, both of which may be part of conference bridges <b>35</b> in FIG. <b>3</b>. As shown in FIG. 9, audio signals <b>114</b>-<b>1</b> to <b>114</b>-n are applied to audio summing circuitry <b>38</b> for combination. These input audio signals <b>114</b>-<b>1</b> to <b>114</b>-n may include audio signals from local participants as well as audio sums from participants at distant sites. Audio mixing circuitry <b>38</b> provides a respective “minus−1” sum output <b>38</b> <i>a</i>-<b>1</b>, <b>38</b> <i>a</i>-<b>2</b>, etc. for each participant. Thus, each participant hears every conference participant's audio except his/her own.</p>
    <p>In the preferred embodiment, sums are decomposed and formed in a distributed fashion, creating partial sums at one site which are completed at other sites by appropriate signal insertion. Accordingly, audio mixing circuitry <b>38</b> is able to provide one or more additional sums, such as indicated by output <b>38</b>, for sending to other sites having conference participants.</p>
    <p>Next to be considered is the manner in which video cut-and-paste techniques are advantageously employed in the preferred embodiment. It will be understood that, since video mosaics and/or individual video pictures may be sent from one or more other sites, the problem arises as to how these situations are handled. Vio cut-and-paste circuitry <b>39</b>, as illustrated in FIG. 10, is provided for this purpose, and may also be incorporated in the conference bridges <b>35</b> in FIG. <b>3</b>.</p>
    <p>Referring to FIG. 10, video cut-and-paste circuitry <b>39</b> eives analog video inputs <b>116</b>, which may be comprised of one or more mosaics or single video pictures received from one or more distant sites and a mosaic or single video picture produced by the local site. It is assumed that the local video mosaicing circuitry <b>36</b> (FIG. 7) and the video cut-and-paste circuitry <b>39</b> have the capability of handling all of the applied individual video pictures, or at least are able to choose which ones are to be displayed based on existing available signals.</p>
    <p>The video cut-and-paste circuitry <b>39</b> digitizes the incoming analog video inputs <b>116</b>, selectively rearranges the digital signals on a region-by-region basis to produce a single digital M-by-M mosaic, having individual pictures in selected regions, and then converts the resulting digital mosaic back to analog form to provide a single analog mosaic picture <b>39</b> <i>a </i>for sending to local participants (and other sites where required) having the individual input video pictures in appropriate regions. This resulting cut-and-paste analog mosaic <b>39</b> <i>a </i>will provide the same type of display as illustrated in FIG. <b>8</b>B. As will become evident hereinafter, it is sometimes beneficial to send different cut-and-paste mosaics to different sites, in which case video cut-and-paste circuitry <b>39</b> will provide additional cut-and-paste mosaics <b>39</b> <i>b</i>-<b>1</b>, <b>39</b> <i>b</i>-<b>2</b>, etc. for this purpose.</p>
    <p>FIG. 11 diagrammatically illustrates an example of how video cut-and-paste circuitry may operate to provide the cut-and-paste analog mosaic <b>39</b> <i>a. </i>As shown in FIG. 11, four digitized individual signals <b>116</b> <i>a, </i> <b>116</b> <i>b, </i> <b>116</b> <i>c </i>and <b>116</b> <i>d </i>derived from the input video signals are “pasted” into selected regions of a digital frame buffer <b>17</b> to form a digital 2×2 mosaic, which is converted into an output analog video mosaic <b>39</b> <i>a </i>or <b>39</b> <i>b </i>in FIG. <b>10</b>. The required audio partial sums may be provided by audio mixing circuitry <b>39</b> in FIG. 9 in the same manner, replacing each cut-and-paste video operation with a partial sum operation.</p>
    <p>Having described in connection with FIGS. 7-11 how video mosaicing, audio mixing, video cut-and-pasting, and distributed audio mixing may be performed, the following description of FIGS. 12-17 will illustrate how these capabilities may advantageously be used in combination in the context of wide-area videoconferencing. For these examples, the teleconference is assumed to have four participants designated as A, B, C and D, in which case 2×2 (quad) mosaics are employed. It is to be understood that greater numbers of participants could be provided. Also, two or more simultaneously occurring teleconferences could also be handled, in which case additional mosaicing, cut-and-paste and audio mixing circuitry would be provided at the various sites along with additional WAN paths. For each example, the “A” figure illustrates the video mosaicing and cut-and-pasting provided, and the corresponding “B” figure (having the same figure number) illustrates the associated audio mixing provided. Note that these figures indicate typical delays that might be encountered for each example (with a single “UNIT” delay ranging from 0-450 milliseonds, depending upon available compression technology).</p>
    <p>FIGS. 12A and 12B illustrate a 2-site example having two participants A and B at Site #<b>1</b> and two participants C and D at Site #<b>2</b>. Note that this example requires mosaicing and cut-and-paste at both sites.</p>
    <p>FIGS. 13A and 13B illustrate another 2-site example, but having three participants A, B and C at Site #<b>1</b> and one participant D at Site #<b>2</b>. Note that this example requires mosaicing at both sites, but cut-and-paste only at Site #<b>2</b>.</p>
    <p>FIGS. 14A and 14B illustrate a 3-site example having participants A and B at Site #<b>1</b>, participant C at Site #<b>2</b>, and participant D at Site #<b>3</b>. At Site #<b>1</b>, the two local videos A and B are put into a mosaic which is sent to both Site #<b>2</b> and Site #<b>3</b>. At Site #<b>2</b> and Site #<b>3</b>, cut-and-paste is used to insert the single video (C or D) at that site into the empty region in the imported A, B, and D or C mosaic, respectively, as shown. Accordingly, mosaicing is required at all three sites, and cut-and-paste is only required for Site #<b>2</b> and Site #<b>3</b>.</p>
    <p>FIGS. 15A and 15B illustrate another <b>3</b>-site example having participant A at Site #<b>1</b>, participant B at Site #<b>2</b>, and participants C and D at Site #<b>3</b>. Note that mosaicing and cut-and-paste are required at all sites. Site #<b>2</b> additionally has the capability to send different cut-and-paste mosaics to Sites #<b>1</b> and Site #<b>3</b>. Further note with respect to FIG. 15B that Site #<b>2</b> creates minus−1 audio mixes for Site #<b>1</b> and Site #<b>2</b>, but only provides a partial audio mix (A&amp;B) for Site #<b>3</b>. These partial mixes are completed at Site #<b>3</b> by mixing in C's signal to complete D's mix (A+B+C) and D's signal to complete C's mix (A+B+D).</p>
    <p>FIG. 16 illustrates a 4-site example employing a star topology, having one participant at each site; that is, participant A is at Site #<b>1</b>, participant B is at Site #<b>2</b>, participant C is at Site #<b>3</b>, and participant D is at Site #<b>4</b>. An audio implementation is not illustrated for this example, since standard minus−1 mixing can be performed at Site #<b>1</b>, and the appropriate sums transmitted to the other sites.</p>
    <p>FIGS. 17A and 17B illustrate a 4-site example that also has only one participant at each site, but uses a line topology rather than a star topology as in the example of FIG. <b>16</b>. Note that this example requires mosaicing and cut-and-paste at all sites. Also note that Site #<b>2</b> and Site #<b>3</b> are each required to transmit two different types of cut-and-paste mosaics.</p>
    <p>The preferred embodiment also provides the capability of allowing a conference participant to select a close-up of a participant displayed on a mosaic. This capability is provided whenever a full individual video picture is available at that user's site. In such case, the A/V Switching Circuitry <b>30</b> (FIG. 3) switches the selected full video picture (whether obtained locally or from another site) to the CMW that requests the close-up.</p>
    <p>Next to be described in connection with FIGS. 18A, <b>18</b>B, <b>19</b> and <b>20</b> are various embodiments of a CMW in accordance with the invention.</p>
    <heading>Collaborative Multimedia Workstation Hardware</heading> <p>One embodiment of a CMW <b>12</b> of the present invention is illustrated in FIG. <b>18</b>A. Currently available personal computers (e.g., an Apple Macintosh or an IBM-compatible PC, desktop or laptop) and workstations (e.g., a Sun SPARCstation) can be adapted to work with the present invention to provide such features as real-time videoconferencing, data conferencing, multimedia mail, etc. In business situations, it can be advantageous to set up a laptop to operate with reduced functionality via cellular telephone links and removable storage media (e.g., CD-ROM, video tape with timecode support, etc.), but take on full capability back in the office via a docking station connected to the MLAN <b>10</b>. This requires a voice and data modem as yet another function server attached to the MLAN.</p>
    <p>The currently available personal computers and workstations serve as a base workstation platform. The addition of certain audio and video I/O devices to the standard components of the base platform <b>100</b> (where standard components include the display monitor <b>200</b>, keyboard <b>300</b> and mouse or tablet (or other pointing device) <b>400</b>), all of which connect with the base platform box through standard peripheral ports <b>101</b>, <b>102</b> and <b>103</b>, enables the CMW to generate and receive real-time audio and video signals. These devices include a video camera <b>500</b> for capturing the user's image, gestures and surroundings (particularly the user's face and upper body), a microphone <b>600</b> for capturing the user's spoken words (and any other sounds generated at the CMW), a speaker <b>700</b> for presenting incoming audio signals (such as the spoken words of another participant to a videoconference or audio annotations to a document), a video input card <b>130</b> in the base platform <b>100</b> for capturing incoming video signals (e.g., the image of another participant to a videoconference, or videomail), and a video display card <b>120</b> for displaying video and graphical output on monitor <b>200</b> (where video is typically displayed in a separate window).</p>
    <p>These peripheral audio and video I/O devices are readily available from a variety of vendors and are just beginning to become standard features in (and often physically integrated into the monitor and/or base platform of) certain personal computers and workstations. See, e.g., the aforementioned BYTE article (“Video Conquers the Desktop”), which describes current models of Apple's Macintosh AV series personal computers and Silicon Graphics' Indy workstations.</p>
    <p>Add-on box <b>800</b> (shown in FIG. <b>18</b>A and illustrated in greater detail in FIG. 19) integrates these audio and video I/O devices with additional functions (such as adaptive echo canceling and signal switching) and interfaces with AV Network <b>901</b>. AV Network <b>901</b> is the part of the MLAN <b>10</b> which carries bidirectional audio and video signals among the CMWs and A/V Switching Circuitry <b>30</b>—e.g., utilizing existing UTP wiring to carry audio and video signals (digital or analog, as in the present embodiment).</p>
    <p>In the present embodiment, the AV network <b>901</b> is separate and distinct from the Data Network <b>902</b> portion of the MLAN <b>10</b>, which carries bidirectional data signals among the CMWs and the Data LAN hub (e.g., an Ethernet network that also utilizes UTP wiring in the present embodiment with a network interface card <b>110</b> in each CMW). Note that each CMW will typically be a node on both the AV and the Data Networks.</p>
    <p>There are several approaches to implementing Add-on box <b>800</b>. In a typical videoconference, video camera <b>500</b> and microphone <b>600</b> capture and transmit outgoing video and audio signals into ports <b>801</b> and <b>802</b>, respectively, of Add-on box <b>800</b>. These signals are transmitted via Audio/Video I/O port <b>805</b> across AV Network <b>901</b>. Incoming video and audio signals (from another videoconference participant) are received across AV network <b>901</b> through Audio/Video I/O port <b>805</b>. The video signals are sent out of V-OUT port <b>803</b> of CMW add-on box <b>800</b> to video input card <b>130</b> of base platform <b>100</b>, where they are displayed (typically in a separate video window) on monitor <b>200</b> utilizing the standard base platform video display card <b>120</b>. The audio signals are sent out of A-OUT port <b>804</b> of CMW add-on box <b>800</b> and played through speaker <b>700</b> while the video signals are displayed on monitor <b>200</b>. The same signal flow occurs for other non-teleconferencing applications of audio and video.</p>
    <p>Add-on box <b>800</b> can be controlled by CMW software (illustrated in FIG. 20) executed by base platform <b>100</b>. Control signals can be communicated between base platform port <b>104</b> and Add-on box Control port <b>806</b> (e.g., an RS-232, Centronics, SCSI or other standard communications port).</p>
    <p>Many other embodiments of the CMW illustrated in FIG. 18A will work in accordance with the present invention. For example, Add-on box <b>800</b> itself can be implemented as an add-in card to the base platform <b>100</b>. Connections to the audio and video I/O devices need not change, though the connection for base platform control can be implemented internally (e.g., via the system bus) rather than through an external RS-232 or SCSI peripheral port. Various additional levels of integration can also be achieved as will be evident to those skilled in the art. For example, microphones, speakers, video cameras and UTP transceivers can be integrated into the base platform <b>100</b> itself, and all media handling technology and communications can be integrated onto a single card.</p>
    <p>A handset/headset jack enables the use of an integrated audio I/O device as an alternate to the separate microphone and speaker. A telephone interface could be integrated into add-on box <b>800</b> as a local implementation of computer-integrated telephony. A “hold” (i.e., audio and video mute) switch and/or a separate audio mute switch could be added to Add-on box <b>800</b> if such an implementation were deemed preferable to a software-based interface.</p>
    <p>The internals of Add-on box <b>800</b> of FIG. 18A are illustrated in FIG. <b>19</b>. Video signals generated at the CMW (e.g., captured by camera <b>500</b> of FIG. 18A) are sent to CMW add-on box <b>800</b> via V-IN port <b>801</b>. They then typically pass unaffected through Loopback/AV Mute circuitry <b>830</b> via video ports <b>833</b> (input) and <b>834</b> (output) and into A/V Transceivers <b>840</b> (via Video In port <b>842</b>) where they are transformed from standard video cable signals to UTP signals and sent out via port <b>845</b> and Audio/Video I/O port <b>805</b> onto AV Network <b>901</b>.</p>
    <p>The Loopback/AV Mute circuitry <b>830</b> can, however, be placed in various modes under software control via Control port <b>806</b> (implemented, for example, as a standard UART). If in loopback mode (e.g., for testing incoming and outgoing signals at the CMW), the video signals would be routed back out V-OUT port <b>803</b> via video port <b>831</b>. If in a mute mode (e.g., muting audio, video or both), video signals might, for example, be disconnected and no video signal would be sent out video port <b>834</b>. Loopback and muting switching functionality is also provided for audio in a similar way. Note that computer control of loopback is very useful for remote testing and diagnostics while manual override of computer control on mute is effective for assured privacy from use of the workstation for electronic spying.</p>
    <p>Video input (e.g., captured by the video camera at the CMW of another videoconference participant) is handled in a similar fashion. It is received along AV Network <b>901</b> through Audio/Video <b>1</b>/<b>0</b> port <b>805</b> and port <b>845</b> of A/V Transceivers <b>840</b>, where it is sent out Video Out port <b>841</b> to video port <b>832</b> of Loopback/AV Mute circuitry <b>830</b>, which typically passes such signals out video port <b>831</b> to V-OUT port <b>803</b> (for receipt by a video input card or other display mechanism, such as LCD display <b>810</b> of CMW Side Mount unit <b>850</b> in FIG. 18B, to be discussed).</p>
    <p>Audio input and output (e.g., for playback through speaker <b>700</b> and capture by microphone <b>600</b> of FIG. 18A) passes through A/V transceivers <b>840</b> (via Audio In port <b>844</b> and Audio Out port <b>843</b>) and Loopback/AV Mute circuitry <b>830</b> (through audio ports <b>837</b>/<b>838</b> and <b>836</b>/<b>835</b>) in a similar manner. The audio input and output ports of Add-on box <b>800</b> interface with standard amplifier and equalization circuitry, as well as an adaptive room echo canceler <b>814</b> to eliminate echo, minimize feedback and provide enhanced audio performance when using a separate microphone and speaker. In particular, use of adaptive room echo cancelers provides high-quality audio interactions in wide area conferences. Because adaptive room echo canceling requires training periods (typically involving an objectionable blast of high-amplitude white noise or tone sequences) for alignment with each acoustic environment, it is preferred that separate echo canceling be dedicated to each workstation rather than sharing a smaller group of echo cancelers across a larger group of workstations.</p>
    <p>Audio inputs passing through audio port <b>835</b> of Loopback/AV Mute circuitry <b>830</b> provide audio signals to a speaker (via standard Echo Canceler circuitry <b>814</b> and A-OUT port <b>804</b>) or to a handset or headset (via I/O ports <b>807</b> and <b>808</b>, respectively, under volume control circuitry <b>815</b> controlled by software through Control port <b>806</b>). In all cases, incoming audio signals pass through power amplifier circuitry <b>812</b> before being sent out of Add-on box <b>800</b> to the appropriate audio-emitting transducer.</p>
    <p>Outgoing audio signals generated at the CMW (e.g., by microphone <b>600</b> of FIG. 18A or the mouthpiece of a handset or headset) enter Add-on box <b>800</b> via A-IN port <b>802</b> (for a microphone) or Handset or Headset I/O ports <b>807</b> and <b>808</b>, respectively. In all cases, outgoing audio signals pass through standard preamplifier (<b>811</b>) and equalization (<b>813</b>) circuitry, whereupon the desired signal is selected by standard “Select” switching circuitry <b>816</b> (under software control through Control port <b>806</b>) and passed to audio port <b>837</b> of Loopback/AV Mute circuitry <b>830</b>.</p>
    <p>It is to be understood that A/V Transceivers <b>840</b> may include muxing/demuxing facilities so as to enable the transmission of audio/video signals on a single pair of wires, e.g., by encoding audio signals digitally in the vertical retrace interval of the analog video signal. Implementation of other audio and video enhancements, such as stereo audio and external audio/video I/O ports (e.g., for recording signals generated at the CMW), are also well within the capabilities of one skilled in the art. If stereo audio is used in teleconferencing (i.e., to create useful spatial metaphors for users), a second echo canceler may be recommended.</p>
    <p>Another embodiment of the CMW of this invention, illustrated in FIG. 18B, utilizes a separate (fully self-contained) “Side Mount” approach which includes its own dedicated video display. This embodiment is advantageous in a variety of situations, such as instances in which additional screen display area is desired (e.g., in a laptop computer or desktop system with a small monitor) or where it is impossible or undesirable to retrofit older, existing or specialized desktop computers for audio/video support. In this embodiment, video camera <b>500</b>, microphone <b>600</b> and speaker <b>700</b> of FIG. 18A are integrated together with the functionality of Add-on box <b>800</b>. Side Mount <b>850</b> eliminates the necessity of external connections to these integrated audio and video I/O devices, and includes an LCD display <b>810</b> for displaying the incoming video signal (which thus eliminates the need for a base platform video input card <b>130</b>).</p>
    <p>Given the proximity of Side Mount device <b>850</b> to the user, and the direct access to audio/video I/O within that device, various additional controls <b>820</b> can be provided at the user's touch (all well within the capabilities of those skilled in the art). Note that, with enough additions, Side Mount unit <b>850</b> can become virtually a standalone device that does not require a separate computer for services using only audio and video. This also provides a way of supplementing a network of full-feature workstations with a few low-cost additional “audio video intercoms” for certain sectors of an enterprise (such as clerical, reception, factory floor, etc.).</p>
    <p>A portable laptop implementation can be made to deliver multimedia mail with video, audio and synchronized annotations via CD-ROM or an add-on videotape unit with separate video, audio and time code tracks (a stereo videotape player can use the second audio channel for time code signals). Videotapes or CD-ROMs can be created in main offices and express mailed, thus avoiding the need for high-bandwidth networking when on the road. Cellular phone links can be used to obtain both voice and data communications (via modems). Modem-based data communications are sufficient to support remote control of mail or presentation playback, annotation, file transfer and fax features. The laptop can then be brought into the office and attached to a docking station where the available MLAN <b>10</b> and additional functions adapted from Add-on box <b>800</b> can be supplied, providing full CMW capability.</p>
    <heading>Collaborative Multimedia Workstation Software</heading> <p>CMW software modules <b>160</b> are illustrated generally in FIG. <b>20</b> and discussed in greater detail below in conjunction with the software running on MLAN Server <b>60</b> of FIG. <b>3</b>. Software <b>160</b> allows the user to initiate and manage (in conjunction with the server software) videoconferencing, data conferencing, multimedia mail and other collaborative sessions with other users across the network.</p>
    <p>Also present on the CMW in this embodiment are standard multitasking operating system/GUI software <b>180</b> (e.g., Apple Macintosh System 7, Microsoft Windows 3.1, or UNIX with the “X Window System” and Motif or other GUI “window manager” software) as well as other applications <b>170</b>, such as word processing and spreadsheet programs. Software modules <b>161</b>-<b>168</b> communicate with operating system/GUI software <b>180</b> and other applications <b>170</b> utilizing standard function calls and interapplication protocols.</p>
    <p>The central component of the Collaborative Multimedia Workstation software is the Collaboration Initiator <b>161</b>. All collaborative functions can be accessed through this module. When the Collaboration Initiator is started, it exchanges initial configuration information with the Audio Video Network Manager (AVNM) <b>60</b> (shown in FIG. 3) through Data Network <b>902</b>. Information is also sent from the Collaboration Initiator to the AVNM indicating the location of the user, the types of services available on that workstation (e.g., videoconferencing, data conferencing, telephony, etc.) and other relevant initialization information.</p>
    <p>The Collaboration Initiator presents a user interface that allows the user to initiate collaborative sessions (both real-time and asynchronous). In the preferred embodiment, session participants can be selected from a graphical rolodex <b>163</b> that contains a scrollable list of user names or from a list of quick-dial buttons <b>162</b>. Quick-dial buttons show the face icons for the users they represent. In the preferred embodiment, the icon representing the user is retrieved by the Collaboration Initiator from the Directory Server <b>66</b> on MLAN Server <b>60</b> when it starts up. Users can dynamically add new quick-dial buttons by dragging the corresponding entries from the graphical rolodex onto the quick-dial panel.</p>
    <p>Once the user elects to initiate a collaborative session, he or she selects one or more desired participants by, for example, clicking on that name to select the desired participant from the system rolodex or a personal rolodex, or by clicking on the quick-dial button for that participant (see, e.g., FIG. <b>2</b>A). In either case, the user then selects the desired session type—e.g., by clicking on a CALL button to initiate a videoconference call, a SHARE button to initiate the sharing of a snapshot image or blank whiteboard, or a MAIL button to send mail. Alternatively, the user can double-click on the rolodex name or a face icon to initiate the default session type—e.g., an audio/video conference call.</p>
    <p>The system also allows sessions to be invoked from the keyboard. It provides a graphical editor to bind combinations of participants and session types to certain hot keys. Pressing this hot key (possibly in conjunction with a modifier key, e.g., &lt;Shift&gt; or &lt;Ctrl&gt;) will cause the Collaboration Initiator to start a session of the specified type with the given participants.</p>
    <p>Once the user selects the desired participant and session type, Collaboration Initiator module <b>161</b> retrieves necessary addressing information from Directory Service <b>66</b> (see FIG. <b>21</b>). In the case of a videoconference call, the Collaboration Initiator (or, in another embodiment, VideoPhone module <b>169</b>) then communicates with the AVNM (as described in greater detail below) to set up the necessary data structures and manage the various states of that call, and to control A/V Switching Circuitry <b>30</b>, which selects the appropriate audio and video signals to be transmitted to/from each participant's CMW. In the case of a data conferencing session, the Collaboration Initiator locates, via the AVNM, the Collaboration Initiator modules at the CMWs of the chosen recipients, and sends a message causing the Collaboration Initiator modules to invoke the Snapshot Sharing modules <b>164</b> at each participant's CMW. Subsequent videoconferencing and data conferencing functionality is discussed in greater detail below in the context of particular usage scenarios.</p>
    <p>As indicated previously, additional collaborative services—such as Mail <b>165</b>, Application Sharing <b>166</b>, Computer-Integrated Telephony <b>167</b> and Computer Integrated Fax <b>168</b>—are also available from the CMW by utilizing Collaboration Initiator module <b>161</b> to initiate the session (i.e., to contact the participants) and to invoke the appropriate application necessary to manage the collaborative session. When initiating asynchronous collaboration (e.g., mail, fax, etc.), the Collaboration Initiator contacts Directory Service <b>66</b> for address information (e.g., EMAIL address, fax number, etc.) for the selected participants and invokes the appropriate collaboration tools with the obtained address information. For real-time sessions, the Collaboration Initiator queries the Service Server module <b>69</b> inside AVNM <b>63</b> for the current location of the specified participants. Using this location information, it communicates (via the AVNM) with the Collaboration Initiators of the other session participants to coordinate session setup. As a result, the various Collaboration Initiators will invoke modules <b>166</b>, <b>167</b> or <b>168</b> (including activating any necessary devices such as the connection between the telephone and the CMW's audio I/O port). Further details on multimedia mail are provided below.</p>
    <heading>Mlan Server Software</heading> <p>FIG. 21 diagrammatically illustrates software <b>62</b> comprised of various modules (as discussed above) provided for running on MLAN Server <b>60</b> (FIG. 3) in the preferred embodiment. It is to be understood that additional software modules could also be provided. It is also to be understood that, although the software illustrated in FIG. 21 offers various significant advantages, as will become evident hereinafter, different forms and arrangements of software may also be employed within the scope of the invention. The software can also be implemented in various sub-parts running as separate processes.</p>
    <p>In one embodiment, clients (e.g., software-controlling workstations, VCRs, laser-disks, multimedia resources, etc.) communicate with the MLAN Server Software Modules <b>62</b> using the TCP/IP network protocols. Generally, the AVNM <b>63</b> cooperates with the Service Server <b>69</b>, Conference Bridge Manager (CBM <b>64</b> in FIG. 21) and the WAN Network Manager (WNM <b>65</b> in FIG. 21) to manage communications within and among both MLANs <b>10</b> and WANs <b>15</b> (FIGS. <b>1</b> and <b>3</b>).</p>
    <p>The AVNM additionally cooperates with Audio/Video Storage Server <b>67</b> and other multimedia services <b>68</b> in FIG. 21 to support various types of collaborative interactions as described herein. CBM <b>64</b> in FIG. 21 operates as a client of the AVNM <b>63</b> to manage conferencing by controlling the operation of conference bridges <b>35</b>. This includes management of the video mosaicing circuitry <b>37</b>, audio mixing circuitry <b>38</b> and cut-and-paste circuitry <b>39</b> preferably incorporated therein. WNM <b>65</b> manages the allocation of paths (codecs and trunks) provided by WAN gateway <b>40</b> for accomplishing the communications to other sites called for by the AVNM.</p>
    <heading>Audio Video Network Manager</heading> <p>The AVNM <b>63</b> manages A/V Switching Circuitry <b>30</b> in FIG. 3 for selectively routing audio/video signals to and from CMWs <b>12</b>, and also to and from WAN gateway <b>40</b>, as called for by clients. Audio/video devices (e.g., CMWs <b>12</b>, conference bridges <b>35</b>, multimedia resources <b>16</b> and WAN gateway <b>40</b> in FIG. 3) connected to A/V Switching Circuitry <b>30</b> in FIG. 3, have physical connections for audio in, audio out, video in and video out. For each device on the network, the AVNM combines these four connections into a port abstraction, wherein each port represents an addressable bidirectional audio/video channel. Each device connected to the network has at least one port. Different ports may share the same physical connections on the switch. For example, a conference bridge may typically have four ports (for 2×2 mosaicing) that share the same video-out connection. Not all devices need both video and audio connections at a port. For example, a TV tuner port needs only incoming audio/video connections.</p>
    <p>In response to client program requests, the AVNM provides connectivity between audio/video devices by connecting their ports. Connecting ports is achieved by switching one port's physical input connections to the other port's physical output connections (for both audio and video) and vice-versa. Client programs can specify which of the 4 physical connections on its ports should be switched. This allows client programs to establish unidirectional calls (e.g., by specifying that only the port's input connections should be switched and not the port's output connections) and audio-only or video-only calls (by specifying audio connections only or video connections only).</p>
    <heading>Service Server</heading> <p>Before client programs can access audio/video resources through the AVNM, they must register the collaborative services they provide with the Service Server <b>69</b>. Examples of these services indicate “video call”, “snapshot sharing”, “conference” and “video file sharing.” These service records are entered into the Service Server's service database. The service database thus keeps track of the location of client programs and the types of collaborative sessions in which they can participate. This allows the Collaboration Initiator to find collaboration participants no matter where they are located. The service database is replicated by all Service Servers: Service Servers communicate with other Service Servers in other MLANs throughout the system to exchange their service records.</p>
    <p>Clients may create a plurality of services, depending on the collaborative capabilities desired. When creating a service, a client can specify the network resources (e.g. ports) that will be used by this service. In particular, service information is used to associate a user with the audio/video ports physically connected to the particular CMW into which the user is logged in. Clients that want to receive requests do so by putting their services in listening mode. If clients want to accept incoming data shares, but want to block incoming video calls, they must create different services.</p>
    <p>A client can create an exclusive service on a set of ports to prevent other clients from creating services on these ports. This is useful, for example, to prevent multiple conference bridges from managing the same set of conference bridge ports.</p>
    <p>Next to be considered is the preferred manner in which the AVNM <b>63</b> (FIG. <b>21</b>), in cooperation with the Service Server <b>69</b>, CBM <b>64</b> and participating CMWs provide for managing A/V Switching Circuitry <b>30</b> and conference bridges <b>35</b> in FIG. 3 during audio/video/data teleconferencing. The participating CMWs may include workstations located at both local and remote sites.</p>
    <heading>Basic Two-Party Videoconferencing</heading> <p>As previously described, a CMW includes a Collaboration Initiator software module <b>161</b>, (see FIG. 20) which is used to establish person-to-person and multiparty calls. The corresponding collaboration initiator window advantageously provides quick-dial face icons of frequently dialed persons, as illustrated, for example, in FIG. 22, which is an enlarged view of typical face icons along with various initiating buttons (described in greater detail below in connection with FIGS. <b>35</b>-<b>42</b>).</p>
    <p>Videoconference calls can be initiated, for example, merely by double-clicking on these icons. When a call is initiated, the CMW typically provides a screen display that includes a live video picture of the remote conference participant, as illustrated for example in FIG. <b>8</b>A. In the preferred embodiment, this display also includes control buttons/menu items that can be used to place the remote participant on hold, to resume a call on hold, to add one or more participants to the call, to initiate data sharing and to hang up the call.</p>
    <p>The basic underlying software-controlled operations occurring for a two-party call are diagrammatically illustrated in FIG. <b>23</b>. After logging to AVNM <b>63</b>, as indicated by (1) in FIG. 23, a caller initiates a call (e.g., by selecting a user from the graphical rolodex and clicking the call button or by double-clicking the face icon of the callee on the quick-dial panel). The caller's Collaboration Initiator responds by identifying the selected user and requesting that user's address from Directory Service <b>66</b>, as indicated by (2) in FIG. <b>23</b>. Directory Service <b>66</b> looks up the callee's address in the directory database, as indicated by (3) in FIG. 23, and then returns it to the caller's Collaboration Initiator, as illustrated by (4) in FIG. <b>23</b>.</p>
    <p>The caller's Collaboration Initiator sends a request to the AVNM to place a video call to the caller with the specified address, as indicated by (5) in FIG. <b>23</b>. The AVNM queries the Service Server to find the service instance of type “video call” whose name corresponds to the callee's address. This service record identifies the location of the callee's Collaboration Initiator as well as the network ports that the callee is connected to. If no service instance is found for the callee, the AVNM notifies the caller that the callee is not logged in. If the callee is local, the AVNM sends a call event to the callee's Collaboration Initiator, as indicated by (6) in FIG. <b>23</b>. If the callee is at a remote site, the AVNM forwards the call request (5) through the WAN gateway <b>40</b> for transmission, via WAN <b>15</b> (FIG. 1) to the Collaboration Initiator of the callee's CMW at the remote site.</p>
    <p>The callee's Collaboration Initiator can respond to the call event in a variety of ways. In the preferred embodiment, a user-selectable sound is generated to announce the incoming call. The Collaboration Initiator can then act in one of two modes. In “Telephone Mode,” the Collaboration Initiator displays an invitation message on the CMW screen that contains the name of the caller and buttons to accept or refuse the call. The Collaboration Initiator will then accept or refuse the call, depending on which button is pressed by the callee. In “Intercom Mode,” the Collaboration Initiator accepts all incoming calls automatically, unless there is already another call active on the callee's CMW, in which case behavior reverts to Telephone Mode.</p>
    <p>The callee's Collaboration Initiator then notifies the AVNM as to whether the call will be accepted or refused. If the call is accepted, (7), the AVNM sets up the necessary communication paths between the caller and the callee required to establish the call. The AVNM then notifies the caller's Collaboration Initiator that the call has been established by sending it an accept event (8). If the caller and callee are at different sites, their AVNMs will coordinate in setting up the communication paths at both sites, as required by the call.</p>
    <p>The AVNM may provide for managing connections among CMWs and other multimedia resources for audio/video/data communications in various ways. The manner employed in the preferred embodiment will next be described.</p>
    <p>As has been described previously, the AVNM manages the switches in the A/V Switching Circuitry <b>30</b> in FIG. 3 to provide port-to-port connections in response to connection requests from clients. The primary data structure used by the AVNM for managing these connections will be referred to as a callhandle, which is comprised of a plurality of bits, including state bits.</p>
    <p>Each port-to-port connection managed by the AVNM comprises two callhandles, one associated with each end of the connection. The callhandle at the client port of the connection permits the client to manage the client's end of the connection. The callhandle mode bits determine the current state of the callhandle and which of a port's four switch connections (video in, video out, audio in, audio out) are involved in a call.</p>
    <p>AVNM clients send call requests to the AVNM whenever they want to initiate a call. As part of a call request, the client specifies the local service in which the call will be involved, the name of the specific port to use for the call, identifying information as to the callee, and the call mode. In response, the AVNM creates a callhandle on the caller's port.</p>
    <p>All callhandles are created in the “idle” state. The AVNM then puts the caller's callhandle in the “active” state. The AVNM next creates a callhandle for the callee and sends it a call event, which places the callee's callhandle in the “ringing” state. When the callee accepts the call, its callhandle is placed in the “active” state, which results in a physical connection between the caller and the callee. Each port can have an arbitrary number of callhandles bound to it, but typically only one of these callhandles can be active at the same time.</p>
    <p>After a call has been set up, AVNM clients can send requests to the AVNM to change the state of the call, which can advantageously be accomplished by controlling the callhandle states. For example, during a call, a call request from another party could arrive. This arrival could be signaled to the user by providing an alert indication in a dialog box on the user's CMW screen. The user could refuse the call by clicking on a refuse button in the dialog box, or by clicking on a “hold” button on the active call window to put the current call on hold and allow the incoming call to be accepted.</p>
    <p>The placing of the currently active call on hold can advantageously be accomplished by changing the caller's callhandle from the active state to a “hold” state, which permits the caller to answer incoming calls or initiate new calls, without releasing the previous call. Since the connection set-up to the callee will be retained, a call on hold can conveniently be resumed by the caller clicking on a resume button on the active call window, which returns the corresponding callhandle back to the active state. Typically, multiple calls can be put on hold in this manner. As an aid in managing calls that are on hold, the CMW advantageously provides a hold list display, identifying these on-hold calls and (optionally) the length of time that each party is on hold. A corresponding face icon could be used to identify each on-hold call. In addition, buttons could be provided in this hold display which would allow the user to send a preprogrammed message to a party on hold. For example, this message could advise the callee when the call will be resumed, or could state that the call is being terminated and will be reinitiated at a later time.</p>
    <p>Reference is now directed to FIG. 24 which diagrammatically illustrates how two-party calls are connected for CMWs WS-<b>1</b> and WS-<b>2</b>, located at the same MLAN <b>10</b>. As shown in FIG. 24, CMWs WS-<b>1</b> and WS-<b>2</b> are coupled to the local A/V Switching Circuitry <b>30</b> via ports <b>81</b> and <b>82</b>, respectively. As previously described, when CMW WS-<b>1</b> calls CMW WS-<b>2</b>, a callhandle is created for each port. If CMW WS-<b>2</b> accepts the call, these two callhandles become active and in response thereto, the AVNM causes the A/V Switching Circuitry <b>30</b> to set up the appropriate connections between ports <b>81</b> and <b>82</b>, as indicated by the dashed line <b>83</b>.</p>
    <p>FIG. 25 diagrammatically illustrates how two-party calls are connected for CMWs WS-<b>1</b> and WS-<b>2</b> when located in different MLANs <b>10</b> <i>a </i>and <b>10</b> <i>b. </i>As illustrated in FIG. 25, CMW WS-<b>1</b> of MLAN <b>10</b> <i>a </i>is connected to a port <b>91</b> a of A/V Switching Circuitry <b>30</b> <i>a </i>of MLAN <b>10</b> <i>a, </i>while CMW WS-<b>2</b> is connected to a port <b>91</b> <i>b </i>of the audio/video switching circuit <b>30</b> <i>b </i>of MLAN <b>10</b> <i>b. </i>It will be assumed that MLANs <b>10</b> <i>a </i>and <b>10</b> <i>b </i>can communicate with each other via ports <b>92</b> <i>a </i>and <b>92</b> <i>b </i>(through respective WAN gateways <b>40</b> <i>a </i>and <b>40</b> <i>b </i>and WAN <b>15</b>). A call between CMWs WS-<b>1</b> and WS-<b>2</b> can then be established by AVNM of MLAN <b>10</b> <i>a </i>in response to the creation of callhandles at ports <b>91</b> <i>a </i>and <b>92</b> <i>a, </i>setting up appropriate connections between these ports as indicated by dashed line <b>93</b> <i>a</i>, and by AVNM of MLAN <b>10</b> <i>b, </i>in response to callhandles created at ports <b>91</b> <i>b </i>and <b>92</b> <i>b, </i>setting up appropriate connections between these ports as indicated by dashed line <b>93</b> <i>b</i>. Appropriate paths <b>94</b> <i>a </i>and <b>94</b> <i>b </i>in WAN gateways <b>40</b> <i>a </i>and <b>40</b> <i>b</i>, respectively, are set up by the WAN network manager <b>65</b> (FIG. 21) in each network.</p>
    <heading>Conference Calls</heading> <p>Next to be described is the specific manner in which the preferred embodiment provides for multi-party conference calls (involving more than two participants). When a multi-party conference call is initiated, the CMW provides a screen that is similar to the screen for two-party calls, which displays a live video picture of the callee's image in a video window. However, for multi-party calls, the screen includes a video mosaic containing a live video picture of each of the conference participants (including the CMW user's own picture), as shown, for example, in FIG. <b>8</b>B. Of course, other embodiments could show only the remote conference participants (and not the local CMW user) in the conference mosaic (or show a mosaic containing both participants in a two-party call). In addition to the controls shown in FIG. 8B, the multi-party conference screen also includes buttons/menu items that can be used to place individual conference participants on hold, to remove individual participants from the conference, to adjourn the entire conference, or to provide a “close-up” image of a single individual (in place of the video mosaic).</p>
    <p>Multi-party conferencing requires all the mechanisms employed for 2-party calls. In addition, it requires the conference bridge manager CBM <b>64</b> (FIG. 21) and the conference bridges <b>36</b> (FIG. <b>3</b>). The CBM acts as a client of the AVNM in managing the operation of the conference bridges <b>36</b>. The CBM also acts as a server to other clients on the network. The CBM makes conferencing services available by creating service records of type “conference” in the AVNM service database and associating these services with the ports on A/V Switching Circuitry <b>30</b> for connection to conference bridges <b>36</b>.</p>
    <p>The preferred embodiment provides two ways for initiating a conference call. The first way is to add one or more parties to an existing two-party call. For this purpose, an ADD button is provided by both the Collaboration Initiator and the Rolodex, as illustrated in FIGS. 2A and 22. To add a new party, a user selects the party to be added (by clicking on the user's rolodex name or face icon as described above) and clicks on the ADD button to invite that new party. Additional parties can be invited in a similar manner. The second way to initiate a conference call is to select the parties in a similar manner and then click on the CALL button (also provided in the Collaboration Initiator and Rolodex windows on the user's CMW screen).</p>
    <p>Another alternative embodiment is to initiate a conference call from the beginning by clicking on a CONFERENCE/MOSAIC icon/button/menu item on the CMW screen. This could initiate a conference call with the call initiator as the sole participant (i.e., causing a conference bridge to be allocated such that the caller's image also appears on his/her own screen in a video mosaic, which will also include images of subsequently added participants). New participants could be invited, for example, by selecting each new party's face icon and then clicking on the ADD button.</p>
    <p>Next to be considered with reference to FIGS. 26 and 27 is the manner in which conference calls are handled in the preferred embodiment. For the purposes of this description it will be assumed that up to four parties may participate in a conference call. Each conference uses four bridge ports <b>136</b>-<b>1</b>, <b>136</b>-<b>2</b>, <b>136</b>-<b>3</b> and <b>136</b>-<b>4</b> provided on A/V Switching Circuitry <b>30</b> <i>a</i>, which are respectively coupled to bidirectional audio/video lines <b>36</b>-<b>1</b>, <b>36</b>-<b>2</b>, <b>36</b>-<b>3</b> and <b>36</b>-<b>4</b> connected to conference bridge <b>36</b>. However, from this description it will be apparent how a conference call may be provided for additional parties, as well as simultaneously occurring conference calls.</p>
    <p>Once the Collaboration Initiator determines that a conference is to be initiated, it queries the AVNM for a conference service. If such a service is available, the Collaboration Initiator requests the associated CBM to allocate a conference bridge. The Collaboration Initiator then places an audio/video call to the CBM to initiate the conference. When the CBM accepts the call, the AVNM couples port <b>101</b> of CMW WS-<b>1</b> to lines <b>36</b>-<b>1</b> of conference bridge <b>36</b> by a connection <b>137</b> produced in response to callhandles created for port <b>101</b> of WS-<b>1</b> and bridge port <b>136</b>-<b>1</b>.</p>
    <p>When the user of WS-<b>1</b> selects the appropriate face icon and clicks the ADD button to invite a new participant to the conference, which will be assumed to be CMW WS-<b>3</b>, the Collaboration Initiator on WS-<b>1</b> sends an add request to the CBM. In response, the CBM calls WS-<b>3</b> via WS-<b>3</b> port <b>103</b>. When CBM initiates the call, the AVNM creates callhandles for WS-<b>3</b> port <b>103</b> and bridge port <b>136</b>-<b>2</b>. When WS-<b>3</b> accepts the call, its callhandle is made “active,” resulting in connection <b>138</b> being provided to connect WS-<b>3</b> and lines <b>136</b>-<b>2</b> of conference bridge <b>36</b>. Assuming CMW WS-<b>1</b> next adds CMW WS-<b>5</b> and then CMW WS-<b>8</b>, callhandles for their respective ports and bridge ports <b>136</b>-<b>3</b> and <b>136</b>-<b>4</b> are created, in turn, as described above for WS-<b>1</b> and WS-<b>3</b>, resulting in connections <b>139</b> and <b>140</b> being provided to connect WS-<b>5</b> and WS-<b>9</b> to conference bridge lines <b>36</b>-<b>3</b> and <b>36</b>-<b>4</b>, respectively. The conferees WS-<b>1</b>, WS-<b>3</b>, WS-<b>5</b> and WS-<b>8</b> are thus coupled to conference bridge lines <b>136</b>-<b>1</b>, <b>136</b>-<b>2</b>, <b>136</b>-<b>3</b> and <b>136</b>-<b>4</b>, respectively as shown in FIG. <b>26</b>.</p>
    <p>It will be understood that the video mosaicing circuitry <b>36</b> and audio mixing circuitry <b>38</b> incorporated in conference bridge <b>36</b> operate as previously described, to form a resulting four-picture mosaic (FIG. 8B) that is sent to all of the conference participants, which in this example are CMWs WS-<b>1</b>, WS-<b>2</b>, WS-<b>5</b> and WS-<b>8</b>. Users may leave a conference by just hanging up, which causes the AVNM to delete the associated callhandles and to send a hangup notification to CBM. When CBM receives the notification, it notifies all other conference participants that the participant has exited. In the preferred embodiment, this results in a blackened portion of that participant's video mosaic image being displayed on the screen of all remaining participants.</p>
    <p>The manner in which the CBM and the conference bridge <b>36</b> operate when conference participants are located at different sites will be evident from the previously described operation of the cut-and-paste circuitry <b>39</b> (FIG. 10) with the video mosaicing circuitry <b>36</b> (FIG. 7) and audio mixing circuitry <b>38</b> (FIG. <b>9</b>). In such case, each incoming single video picture or mosaic from another site is connected to a respective one of the conference bridge lines <b>36</b>-<b>1</b> to <b>36</b>-<b>4</b> via WAN gateway <b>40</b>.</p>
    <p>The situation in which a two-party call is converted to a conference call will next be considered in connection with FIG. <b>27</b> and the previously considered 2-party call illustrated in FIG. <b>24</b>. Converting this 2-party call to a conference requires that this two-party call (such as illustrated between WS-<b>1</b> and WS-<b>2</b> in FIG. 24) be rerouted dynamically so as to be coupled through conference bridge <b>36</b>. When the user of WS-<b>1</b> clicks on the ADD button to add a new party (for example WS-<b>5</b>), the Collaboration Initiator of WS-<b>1</b> sends a redirect request to the AVNM, which cooperates with the CBM to break the two-party connection <b>83</b> in FIG. 24, and then redirect the callhandles created for ports <b>81</b> and <b>83</b> to callhandles created for bridge ports <b>136</b>-<b>1</b> and <b>136</b>-<b>2</b>, respectively.</p>
    <p>As shown in FIG. 27, this results in producing a connection <b>86</b> between WS-<b>1</b> and bridge port <b>136</b>-<b>1</b>, and a connection <b>87</b> between WS-<b>2</b> and bridge port <b>136</b>-<b>2</b>, thereby creating a conference set-up between WS-<b>1</b> and WS-<b>2</b>. Additional conference participants can then be added as described above for the situations described above in which the conference is initiated by the user of WS-<b>1</b> either selecting multiple participants initially or merely selecting a “conference” and then adding subsequent participants.</p>
    <p>Having described the preferred manner in which two-party calls and conference calls are set up in the preferred embodiment, the preferred manner in which data conferencing is provided between CMWs will next be described.</p>
    <heading>Data Conferencing</heading> <p>Data conferencing is implemented in the preferred embodiment by certain Snapshot Sharing software provided at the CMW (see FIG. <b>20</b>). This software permits a “snapshot” of a selected portion of a participant's CMW screen (such as a window) to be displayed on the CMW screens of other selected participants (whether or not those participants are also involved in a videoconference). Any number of snapshots may be shared simultaneously. Once displayed, any participant can then telepoint on or annotate the snapshot, which animated actions and results will appear (virtually simultaneously) on the screens of all other participants. The annotation capabilities provided include lines of several different widths and text of several different sizes. Also, to facilitate participant identification, these annotations may be provided in a different color for each participant. Any annotation may also be erased by any participant. FIG. 2B (lower left window) illustrates a CMW screen having a shared graph on which participants have drawn and typed to call attention to or supplement specific portions of the shared image.</p>
    <p>A participant may initiate data conferencing with selected participants (selected and added as described above for videoconference calls) by clicking on a SHARE button on the screen (available in the Rolodex or Collaboration Initiator windows, shown in FIG. 2A, as are CALL and ADD buttons), followed by selection of the window to be shared. When a participant clicks on his SHARE button, his Collaboration Initiator module <b>161</b> (FIG. 20) queries the AVNM to locate the Collaboration Initiators of the selected participants, resulting in invocation of their respective Snapshot Sharing modules <b>164</b>. The Snapshot Sharing software modules at the CMWs of each of the selected participants query their local operating system <b>180</b> to determine available graphic formats, and then send this information to the initiating Snapshot Sharing module, which determines the format that will produce the most advantageous display quality and performance for each selected participant.</p>
    <p>After the snapshot to be shared is displayed on all CMWs, each participant may telepoint on or annotate the snapshot, which actions and results are displayed on the CMW screens of all participants. This is preferably accomplished by monitoring the actions made at the CMW (e.g., by tracking mouse movements) and sending these “operating system commands” to the CMWs of the other participants, rather than continuously exchanging bitmaps, as would be the case with traditional “remote control” products.</p>
    <p>As illustrated in FIG. 28, the original unchanged snapshot is stored in a first bitmap <b>210</b> <i>a. </i>A second bitmap <b>210</b> <i>b </i>stores the combination of the original snapshot and any annotations. Thus, when desired (e.g., by clicking on a CLEAR button located in each participant's Share window, as illustrated in FIG. <b>2</b>B), the original unchanged snapshot can be restored (i.e., erasing all annotations) using bitmap <b>210</b> <i>a</i>. Selective erasures can be accomplished by copying into (i.e., restoring) the desired erased area of bitmap <b>210</b> <i>b </i>with the corresponding portion from bitmap <b>210</b> <i>a. </i> </p>
    <p>Rather than causing a new Share window to be created whenever a snapshot is shared, it is possible to replace the contents of an existing Share window with a new image. This can be achieved in either of two ways. First, the user can click on the GRAB button and then select a new window whose contents should replace the contents of the existing Share window. Second, the user can click on the REGRAB button to cause a (presumably modified) version of the original source window to replace the contents of the existing Share window. This is particularly useful when one participant desires to share a long document that cannot be displayed on the screen in its entirety. For example, the user might display the first page of a spreadsheet on his screen, use the SHARE button to share that page, discuss and perhaps annotate it, then return to the spreadsheet application to position to the next page, use the REGRAB button to share the new page, and so on. This mechanism represents a simple, effective step toward application sharing.</p>
    <p>Further, instead of sharing a snapshot of data on his current screen, a user may instead choose to share a snapshot that had previously been saved as a file. This is achieved via the LOAD button, which causes a dialog box to appear, prompting the user to select a file. Conversely, via the SAVE button, any snapshot may be saved, with all current annotations.</p>
    <p>The capabilities described above were carefully selected to be particularly effective in environments where the principal goal is to share existing information, rather than to create new information. In particular, user interfaces are designed to make snapshot capture, telepointing and annotation extremely easy to use. Nevertheless, it is also to be understood that, instead of sharing snapshots, a blank “whiteboard” can also be shared (via the WHITEBOARD button provided by the Rolodex, Collaboration Initiator, and active call windows), and that more complex paintbox capabilities could easily be added for application areas that require such capabilities.</p>
    <p>As pointed out previously herein, important features of the present invention reside in the manner in which the capabilities and advantages of multimedia mail (MM), multimedia conference recording (MMCR), and multimedia document management (MMDM) are tightly integrated with audio/video/data teleconferencing to provide a multimedia collaboration system that facilitates an unusually higher level of communication and collaboration between geographically dispersed users than has heretofore been achievable by known prior art systems. FIG. 29 is a schematic and diagrammatic view illustrating how multimedia calls/conferences, MMCR, MMM and MMDM work together to provide the above-described features. In the preferred embodiment, MM Editing Utilities shown supplementing MMM and MMDM may be identical.</p>
    <p>Having already described various embodiments and examples of audio/video/data teleconferencing, next to be considered are various ways of integrating MMCR, MMM and MMDM with audio/video/data teleconferencing in accordance with the invention. For this purpose, basic preferred approaches and features of each will be considered along with preferred associated hardware and software.</p>
    <heading>Multimedia Documents</heading> <p>In one embodiment, the creation, storage, retrieval and editing of multimedia documents serve as the basic element common to MMCR, MMM and MMDM. Accordingly, the preferred embodiment advantageously provides a universal format for multimedia documents. This format defines multimedia documents as a collection of individual components in multiple media combined with an overall structure and timing component that captures the identities, detailed dependencies, references to, and relationships among the various other components. The information provided by this structuring component forms the basis for spatial layout, order of presentation, hyperlinks, temporal synchronization, etc., with respect to the composition of a multimedia document. FIG. 30 shows the structure of such documents as well as their relationship with editing and storage facilities.</p>
    <p>Each of the components of a multimedia document uses its own editors for creating, editing, and viewing. In addition, each component may use dedicated storage facilities. In the preferred embodiment, multimedia documents are advantageously structured for authoring, storage, playback and editing by storing some data under conventional file systems and some data in special-purpose storage servers as will be discussed later. The Conventional File System <b>504</b> can be used to store all non-time-sensitive portions of a multimedia document. In particular, the following are examples of non-time-sensitive data that can be stored in a conventional type of computer file system:</p>
    <p>1. structured and unstructured text</p>
    <p>2. raster images</p>
    <p>3. structured graphics and vector graphics (e.g., PostScript)</p>
    <p>4. references to files in other file systems (video, hi-fidelity audio, etc.) via pointers</p>
    <p>5. restricted forms of executables</p>
    <p>6. structure and timing information for all of the above (spatial layout, order of presentation, hyperlinks, temporal synchronization, etc.)</p>
    <p>Of particular importance in multimedia documents is support for time-sensitive media and media that have synchronization requirements with other media components. Some of these time-sensitive media can be stored on conventional file systems while others may require special-purpose storage facilities.</p>
    <p>Examples of time-sensitive media that can be stored on conventional file systems are small audio files and short or low-quality video clips (e.g. as might be produced using QuickTime or Video for Windows). Other examples include window event lists as supported by the Window-Event Record and Play system <b>512</b> shown in FIG. <b>30</b>. This component allows for storing and replaying a user's interactions with application programs by capturing the requests and events exchanged between the client program and the window system in a time-stamped sequence. After this “record” phase, the resulting information is stored in a conventional file that can later be retrieved and “played” back. During playback the same sequence of window system requests and events reoccurs with the same relative timing as when they were recorded. In prior-art systems, this capability has been used for creating automated demonstrations. In the present invention it can be used, for example, to reproduce annotated snapshots as they occurred at recording.</p>
    <p>As described above in connection with collaborative workstation software, Snapshot Share <b>518</b> shown in FIG. 30 is a utility used in multimedia calls and conferencing for capturing window or screen snapshots, sharing with one or more call or conference participants, and permitting group annotation, telepointing, and re-grabs. Here, this utility is adapted so that its captured images and window events can be recorded by the Window-Event Record and Play system <b>512</b> while being used by only one person. By synchronizing events associated with a video or audio stream to specific frame numbers or time codes, a multimedia call or conference can be recorded and reproduced in its entirety. Similarly, the same functionality is preferably used to create multimedia mail whose authoring steps are virtually identical to participating in a multimedia call or conference (though other forms of MMM are not precluded).</p>
    <p>Some time-sensitive media require dedicated storage servers in order to satisfy real-time requirements. High-quality audio/video segments, for example, require dedicated real-time audio/video storage servers. A preferred embodiment of such a server will be described later. Next to be considered is how the current invention guarantees synchronization between different media components.</p>
    <heading>Media Synchronization</heading> <p>A preferred manner for providing multimedia synchronization in the preferred embodiment will next be considered. Only multimedia documents with real-time material need include synchronization functions and information. Synchronization for such situations may be provided as described below.</p>
    <p>Audio or video segments can exist without being accompanied by the other. If audio and video are recorded simultaneously (“co-recorded”), the preferred embodiment allows the case where their streams are recorded and played back with automatic synchronization—as would result from conventional VCRs, laserdisks, or time-division multiplexed (“interleaved”) audio/video streams. This excludes the need to tightly synchronize (i.e., “lip-sync”) separate audio and video sequences. Rather, reliance is on the co-recording capability of the Real-Time Audio/Video Storage Server <b>502</b> to deliver all closely synchronized audio and video directly at its signal outputs.</p>
    <p>Each recorded video sequence is tagged with time codes (e.g. SMPTE at {fraction (1/30)} second intervals) or video frame numbers. Each recorded audio sequence is tagged with time codes (e.g., SMPTE or MIDI) or, if co-recorded with video, video frame numbers.</p>
    <p>The preferred embodiment also provides synchronization between window events and audio and/or video streams. The following functions are supported:</p>
    <p>1. Media-time-driven Synchronization: synchronization of window events to an audio, video, or audio/video stream, using the real-time media as the timing source.</p>
    <p>2. Machine-time-driven-Synchronization:</p>
    <p>a. synchronization of window events to the system clock</p>
    <p>b. synchronization of the start of an audio, video, or audio/video segment to the system clock</p>
    <p>If no audio or video is involved, machine-time-driven synchronization is used throughout the document. Whenever audio and/or video is playing, media-time-synchronization is used. The system supports transition between machine-time and media-time synchronization whenever an audio/video segment is started or stopped.</p>
    <p>As an example, viewing a multimedia document might proceed as follows:</p>
    <p>Document starts with an annotated share (machine-time-driven synchronization).</p>
    <p>Next, start audio only (a “voice annotation”) as text and graphical annotations on the share continue (audio is timing source for window events).</p>
    <p>Audio ends, but annotations continue (machine-time-driven synchronization).</p>
    <p>Next, start co-recorded audio/video continuing with further annotations on same share (audio is timing source for window events).</p>
    <p>Next, start a new share during the continuing audio/video recording; annotations happen on both shares (audio is timing source for window events).</p>
    <p>Audio/video stops, annotations on both shares continue (machine-time-driven synchronization).</p>
    <p>Document ends.</p>
    <heading>Audio/Video Storage</heading> <p>As described above, the present invention can include many special-purpose servers that provide storage of time-sensitive media (e.g. audio/video streams) and support coordination with other media. This section describes the preferred embodiment for audio/video storage and recording services.</p>
    <p>Although storage and recording services could be provided at each CMW, it is preferable to employ a centralized server <b>502</b> coupled to MLAN <b>10</b>, as illustrated in FIG. 31. A centralized server <b>502</b>, as shown in FIG. 31, provides the following advantages:</p>
    <p>1. The total amount of storage hardware required can be far less (due to better utilization resulting from statistical averaging).</p>
    <p>2. Bulky and expensive compression/decompression hardware can be pooled on the storage servers and shared by multiple clients. As a result, fewer compression/decompression engines of higher performance are required than if each workstation were equipped with its own compression/decompression hardware.</p>
    <p>3. Also, more costly centralized codecs can be used to transfer mail wide area among campuses at far lower costs than attempting to use data WAN technologies.</p>
    <p>4. File system administration (e.g. backups and file system replication, etc.) are far less costly and higher performance.</p>
    <p>The Real-Time Audio/Video Storage Server <b>502</b> shown in FIG. 31A structures and manages the audio/video files recorded and stored on its storage devices. Storage devices may typically include computer-controlled VCRs, as well as rewritable magnetic or optical disks. For example, server <b>502</b> in FIG. 31A includes disks <b>60</b> <i>e </i>for recording and playback. Analog information is transferred between disks <b>60</b> <i>e </i>and the A/V Switching Circuitry <b>30</b> via analog I/O <b>62</b>. Control is provided by control <b>64</b> coupled to Data LAN hub <b>25</b>.</p>
    <p>At a high level, the centralized audio/video storage and playback server <b>502</b> in FIG. 31A performs the following functions:</p>
    <p>File Management</p>
    <p>It provides mechanisms for creating, naming, time-stamping, storing, retrieving, copying, deleting, and playing back some or all portions of an audio/video file.</p>
    <p>File Transfer and Replication</p>
    <p>The audio/video file server supports replication of files on different disks managed by the same file server to facilitate simultaneous access to the same files. Moreover, file transfer facilities are provided to support transmission of audio/video files between itself and other audio/video storage and playback engines. File transfer can also be achieved by using the underlying audio/video network facilities: servers establish a real-time audio/video network connection between themselves so one server can “play back” a file while the second server simultaneously records it.</p>
    <p>Disk Management</p>
    <p>The storage facilities support specific disk allocation, garbage collection and defragmentation facilities. They also support mapping disks with other disks (for replication and staging modes, as appropriate) and mapping disks, via I/O equipment, with the appropriate Video/Audio network port.</p>
    <p>Synchronization support</p>
    <p>Synchronization between audio and video is ensured by the multiplexing scheme used by the storage media, typically by interleaving the audio and video streams in a time-division-multiplexed fashion. Further, if synchronization is required with other stored media (such as window system graphics), then frame numbers, time codes, or other timing events are generated by the storage server. An advantageous way of providing this synchronization in the preferred embodiment is to synchronize record and playback to received frame number or time code events.</p>
    <p>Searching</p>
    <p>To support intra-file searching, at least start, stop, pause, fast forward, reverse, and fast reverse operations are provided. To support inter-file searching, audio/video tagging, or more generalized “go-to” operations and mechanisms, such as frame numbers or time code, are supported at a search-function level.</p>
    <p>Connection Management</p>
    <p>The server handles requests for audio/video network connections from client programs (such as video viewers and editors running on client workstations) for real-time recording and real-time playback of audio/video files.</p>
    <p>Next to be considered is how centralized audio/video storage servers provide for real-time recording and playback of video streams.</p>
    <p>Real-Time Disk Delivery</p>
    <p>To support real-time audio/video recording and playback, the storage server needs to provide a real-time transmission path between the storage medium and the appropriate audio/video network port for each simultaneous client accessing the server. For example, if one user is viewing a video file at the same time several other people are creating and storing new video files on the same disk, multiple simultaneous paths to the storage media are required. Similarly, video mail sent to large distribution groups, video databases, and similar functions may also require simultaneous access to the same video files, again imposing multiple access requirements on the video storage capabilities.</p>
    <p>For storage servers that are based on computer-controlled VCRs or rewritable laserdisks, a real-time transmission path is readily available through the direct analog connection between the disk or tape and the network port. However, because of this single direct connection, each VCR or laserdisk can only be accessed by one client program at the same time (multi-head laserdisks are an exception). Therefore, storage servers based on VCRs and laserdisks are difficult to scale for multiple access usage. In the preferred embodiment, multiple access to the same material is provided by file replication and staging, which greatly increases storage requirements and the need for moving information quickly among storage media units serving different users.</p>
    <p>Video systems based on magnetic disks are more readily scalable for simultaneous use by multiple people. A generalized hardware implementation of such a scalable storage and playback system <b>502</b> is illustrated in FIG. <b>32</b>. Individual I/O cards <b>530</b> supporting digital and analog I/O are linked by intra-chassis digital networking (e.g. buses) for file transfer within chassis <b>532</b> holding some number of these cards. Multiple chassis <b>532</b> are linked by inter-chassis networking. The Digital Video Storage System available from Parallax Graphics is an example of such a system implementation.</p>
    <p>The bandwidth available for the transfer of files among disks is ultimately limited by the bandwidth of these intra-chassis and inter-chassis networking. For systems that use sufficiently powerful video compression schemes, real-time delivery requirements for a small number of users can be met by existing file system software (such as the Unix file system), provided that the block-size of the storage system is optimized for video storage and that sufficient buffering is provided by the operating system software to guarantee continuous flow of the audio/video data.</p>
    <p>Special-purpose software/hardware solutions can be provided to guarantee higher performance under heavier usage or higher bandwidth conditions. For example, a higher throughput version of FIG. 32 is illustrated in FIG. 33, which uses crosspoint switching, such as provided by SCSI Crossbar <b>540</b>, which increases the total bandwidth of the inter-chassis and intra-chassis network, thereby increasing the number of possible simultaneous file transfers.</p>
    <p>Real-Time Network Delivery</p>
    <p>By using the same audio/video format as used for audio/video teleconferencing, the audio/video storage system can leverage the previously described network facilities: the MLANs <b>10</b> can be used to establish a multimedia network connection between client workstations and the audio/video storage servers. Audio/Video editors and viewers running on the client workstation use the same software interfaces as the multimedia teleconferencing system to establish these network connections.</p>
    <p>The resulting architecture is shown in FIG. <b>31</b>B. Client workstations use the existing audio/video network to connect to the storage server's network ports. These network ports are connected to compression/decompression engines that plug into the server bus. These engines compress the audio/video streams that come in over the network and store them on the local disk. Similarly, for playback, the server reads stored video segments from its local disk and routes them through the decompression engines back to client workstations for local display.</p>
    <p>The present invention allows for alternative delivery strategies. For example, some compression algorithms are asymmetric, meaning that decompression requires much less compute power than compression. In some cases, real-time decompression can even be done in software, without requiring any special-purpose decompression hardware. As a result, there is no need to decompress stored audio and video on the storage server and play it back in real-time over the network. Instead, it can be more efficient to transfer an entire audio/video file from the storage server to the client workstation, cache it on the workstation's disk, and play it back locally. These observations lead to a modified architecture as presented in FIG. <b>31</b>C. In this architecture, clients interact with the storage server as follows:</p>
    <p>To record video, clients set up real-time audio/video network connections to the storage server as before (this connection could make use of an analog line).</p>
    <p>In response to a connection request, the storage server allocates a compression module to the new client.</p>
    <p>As soon as the client starts recording, the storage server routes the output from the compression hardware to an audio/video file allocated on its local storage devices.</p>
    <p>For playback, this audio/video file gets transferred over the data network to the client workstation and pre-staged on the workstation's local disk.</p>
    <p>The client uses local decompression software and/or hardware to play back the audio/video on its local audio and video hardware.</p>
    <p>This approach frees up audio/video network ports and compression/decompression engines on the server. As a result, the server is scaled to support a higher number of simultaneous recording sessions, thereby further reducing the cost of the system. Note that such an architecture can be part of a preferred embodiment for reasons other than compression/decompression asymmetry (such as the economics of the technology of the day, existing embedded base in the enterprise, etc.).</p>
    <heading>Multimedia Conference Recording</heading> <p>Multimedia conference recording (MMCR) will next be considered. For full-feature multimedia desktop calls and conferencing (e.g. audio/video calls or conferences with snapshot share), recording (storage) capabilities are preferably provided for audio and video of all parties, and also for all shared windows, including any telepointing and annotations provided during the teleconference. Using the multimedia synchronization facilities described above, these capabilities are provided in a way such that they can be replayed with accurate correspondence in time to the recorded audio and video, such as by synchronizing to frame numbers or time code events.</p>
    <p>A preferred way of capturing audio and video from calls would be to record all calls and conferences as if they were multi-party conferences (even for two-party calls), using video mosaicing, audio mixing and cut-and-pasting, as previously described in connection with FIGS. 7-11. It will be appreciated that MMCR as described will advantageously permit users at their desktop to review real-time collaboration as it previously occurred, including during a later teleconference. The output of a MMCR session is a multimedia document that can be stored, viewed, and edited using the multimedia document facilities described earlier.</p>
    <p>FIG. 31D shows how conference recording relates to the various system components described earlier. The Multimedia Conference Record/Play system <b>522</b> provides the user with the additional GUIs (graphical user interfaces) and other functions required to provide the previously described MMCR functionality.</p>
    <p>The Conference Invoker <b>518</b> shown in FIG. 31D is a utility that coordinates the audio/video calls that must be made to connect the audio/video storage server <b>502</b> with special recording outputs on conference bridge hardware (<b>35</b> in FIG. <b>3</b>). The resulting recording is linked to information identifying the conference, a function also performed by this utility.</p>
    <heading>Multimedia Mail</heading> <p>Now considering multimedia mail (MMM), it will be understood that MMM adds to the above-described MMCR the capability of delivering delayed collaboration, as well as the additional ability to review the information multiple times and, as described hereinafter, to edit, re-send, and archive it. The captured information is preferably a superset of that captured during MMCR, except that no other user is involved and the user is given a chance to review and edit before sending the message.</p>
    <p>The Multimedia Mail system <b>524</b> in FIG. 31D provides the user with the additional GUIs and other functions required to provide the previously described MMM functionality. Multimedia Mail relies on a conventional Email system <b>506</b> shown in FIG. 3 ID for creating, transporting, and browsing messages. However, multimedia document editors and viewers are used for creating and viewing message bodies. Multimedia documents (as described above) consist of time-insensitive components and time-sensitive components. The Conventional Email system <b>506</b> relies on the Conventional File system <b>504</b> and Real-Time Audio/Video Storage Server <b>502</b> for storage support. The time-insensitive components are transported within the Conventional Email system <b>506</b>, while the real-time components may be separately transported through the audio/video network using file transfer utilities associated with the Real-Time Audio/Video Storage Server <b>502</b>.</p>
    <heading>Multimedia Document Management</heading> <p>Multimedia document management (MMDM) provides long-term, high-volume storage for MMCR and MMM. The MMDM system assists in providing the following capabilities to a CMW user:</p>
    <p>1. Multimedia documents can be authored as mail in the MMM system or as call/conference recordings in the MMCR system and then passed on to the MMDM system.</p>
    <p>2. To the degree supported by external compatible multimedia editing and authoring systems, multimedia documents can also be authored by means other than MMM and MMCR.</p>
    <p>3. Multimedia documents stored within the MMDM system can be reviewed and searched.</p>
    <p>4. Multimedia documents stored within the MMDM system can be used as material in the creation of subsequent MMM.</p>
    <p>5. Multimedia documents stored within the MMDM system can be edited to create other multimedia documents.</p>
    <p>The Multimedia Document Management system <b>526</b> in FIG. 31D provides the user with the additional GUIs and other functions required to provide the previously described MMDM functionality. The MMDM includes sophisticated searching and editing capabilities in connection with the MMDM multimedia document such that a user can rapidly access desired selected portions of a stored multimedia document. The Specialized Search system <b>520</b> in FIG. 31D comprises utilities that allow users to do more sophisticated searches across and within multimedia documents. This includes context-based and content-based searches (employing operations such as speech and image recognition, information filters, etc.), time-based searches, and event-based searches (window events, call management events, speech/audio events, etc.).</p>
    <heading>Classes of Collaboration</heading> <p>The resulting multimedia collaboration environment achieved by the above-described integration of audio/video/data teleconferencing, MMCR, MMM and MMDM is illustrated in FIG. <b>34</b>. It will be evident that each user can collaborate with other users in real-time despite separations in space and time. In addition, collaborating users can access information already available within their computing and information systems, including information captured from previous collaborations. Note in FIG. 34 that space and time separations are supported in the following ways:</p>
    <p>1. Same time different place</p>
    <p>Multimedia calls and conferences</p>
    <p>2. Different time, same place</p>
    <p>MMDM access to stored MMCR and MMM information, or use of MMM directly (i.e., copying mail to oneself)</p>
    <p>3. Different time, different place</p>
    <p>MMM</p>
    <p>4. Same time, same place</p>
    <p>Collaborative, face-to-face, multimedia document creation</p>
    <p>By use of the same user interfaces and network functions, the present invention smoothly spans these three venus.</p>
    <heading>Remote Access To Expertise</heading> <p>In order to illustrate how the present invention may be implemented and operated, an exemplary preferred embodiment will be described having features applicable to the aforementioned scenario involving remote access to expertise. It is to be understood that this exemplary embodiment is merely illustrative, and is not to be considered as limiting the scope of the invention, since the invention may be adapted for other applications (such as in engineering and manufacturing) or uses having more or less hardware, software and operating features and combined in various ways.</p>
    <p>Consider the following scenario involving access from remote sites to an in-house corporate “expert” in the trading of financial instruments such as in the securities market:</p>
    <p>The focus of the scenario revolves around the activities of a trader who is a specialist in securities. The setting is the start of his day at his desk in a major financial center (NYC) at a major U.S. investment bank.</p>
    <p>The Expert has been actively watching a particular security over the past week and upon his arrival into the office, he notices it is on the rise. Before going home last night, he previously set up his system to filter overnight news on a particular family of securities and a security within that family. He scans the filtered news and sees a story that may have a long-term impact on this security in question. He believes he needs to act now in order to get a good price on the security. Also, through filtered mail, he sees that his counterpart in London, who has also been watching this security, is interested in getting our Expert's opinion once he arrives at work.</p>
    <p>The Expert issues a multimedia mail message on the security to the head of sales worldwide for use in working with their client base. Also among the recipients is an analyst in the research department and his counterpart in London. The Expert, in preparation for his previously established “on-call” office hours, consults with others within the corporation (using the videoconferencing and other collaborative techniques described above), accesses company records from his CMW, and analyzes such information, employing software-assisted analytic techniques. His office hours are now at hand, so he enters “intercom” mode, which enables incoming calls to appear automatically (without requiring the Expert to “answer his phone” and elect to accept or reject the call).</p>
    <p>The Expert's computer beeps, indicating an incoming call, and the image of a field representative <b>201</b> and his client <b>202</b> who are located at a bank branch somewhere in the U.S. appears in video window <b>203</b> of the Expert's screen (shown in FIG. <b>35</b>). Note that, unless the call is converted to a “conference” call (whether explicitly via a menu selection or implicitly by calling two or more other participants or adding a third participant to a call), the callers will see only each other in the video window and will not see themselves as part of a video mosaic.</p>
    <p>Also illustrated on the Expert's screen in FIG. 35 is the Collaboration Initiator window <b>204</b> from which the Expert can (utilizing Collaboration Initiator software module <b>161</b> shown in FIG. 20) initiate and control various collaborative sessions. For example, the user can initiate with a selected participant a video call (CALL button) or the addition of that selected participant to an existing video call (ADD button), as well as a share session (SHARE button) using a selected window or region on the screen (or a blank region via the WHITEBOARD button for subsequent annotation). The user can also invoke his MAIL software (MAIL button) and prepare outgoing or check incoming Email messages (the presence of which is indicated by a picture of an envelope in the dog's mouth in In Box icon <b>205</b>), as well as check for “I called” messages from other callers (MESSAGES button) left via the LEAVE WORD button in video window <b>203</b>. Video window <b>203</b> also contains buttons from which many of these and certain additional features can be invoked, such as hanging up a video call (HANGUP button), putting a call on hold (HOLD button), resuming a call previously put on hold (RESUME button) or muting the audio portion of a call (MUTE button). In addition, the user can invoke the recording of a conference by the conference RECORD button. Also present on the Expert's screen is a standard desktop window <b>206</b> containing icons from which other programs (whether or not part of this invention) can be launched.</p>
    <p>Returning to the example, the Expert is now engaged in a videoconference with field representative <b>201</b> and his client <b>202</b>. In the course of this videoconference, as illustrated in FIG. 36, the field representative shares with the Expert a graphical image <b>210</b> (pie chart of client portfolio holdings) of his client's portfolio holdings (by clicking on his SHARE button, corresponding to the SHARE button in video window <b>203</b> of the Expert's screen, and selecting that image from his screen, resulting in the shared image appearing in the Share window <b>211</b> of the screen of all participants to the share) and begins to discuss the client's investment dilemma. The field representative also invokes a command to secretly bring up the client profile on the Expert's screen.</p>
    <p>After considering this information, reviewing the shared portfolio and asking clarifying questions, the Expert illustrates his advice by creating (using his own modeling software) and sharing a new graphical image <b>220</b> (FIG. 37) with the field representative and his client. Either party to the share can annotate that image using the drawing tools <b>221</b> (and the TEXT button, which permits typed characters to be displayed) provided within Share window <b>211</b>, or “regrab” a modified version of the original image (by using the REGRAB button), or remove all such annotations (by using the CLEAR button of Share window <b>211</b>), or “grab” a new image to share (by clicking on the GRAB button of Share window <b>211</b> and selecting that new image from the screen). In addition, any participant to a shared session can add a new participant by selecting that participant from the rolodex or quick-dial list (as described above for video calls and for data conferencing) and clicking the ADD button of Share window <b>211</b>. One can also save the shared image (SAVE button), load a previously saved image to be shared (LOAD button), or print an image (PRINT button).</p>
    <p>While discussing the Expert's advice, field representative <b>201</b> makes annotations <b>222</b> to image <b>220</b> in order to illustrate his concerns. While responding to the concerns of field representative <b>201</b>, the Expert hears a beep and receives a visual notice (New Call window <b>223</b>) on his screen (not visible to the field representative and his client), indicating the existence of a new incoming call and identifying the caller. At this point, the Expert can accept the new call (ACCEPT button), refuse the new call (REFUSE button, which will result in a message being displayed on the caller's screen indicating that the Expert is unavailable) or add the new caller to the Expert's existing call (ADD button). In this case, the Expert elects yet another option (not shown)—to defer the call and leave the caller a standard message that the Expert will call back in X minutes (in this case, 1 minute). The Expert then elects also to defer his existing call, telling the field representative and his client that he will call them back in 5 minutes, and then elects to return the initial deferred call.</p>
    <p>It should be noted that the Expert's act of deferring a call results not only in a message being sent to the caller, but also in the caller's name (and perhaps other information associated with the call, such as the time the call was deferred or is to be resumed) being displayed in a list <b>230</b> (see FIG. 38) on the Expert's screen from which the call can be reinitiated. Moreover, the “state” of the call (e.g., the information being shared) is retained so that it can be recreated when the call is reinitiated. Unlike a “hold” (described above), deferring a call actually breaks the logical and physical connections, requiring that the entire call be reinitiated by the Collaboration Initiator and the AVNM as described above.</p>
    <p>Upon returning to the initial deferred call, the Expert engages in a videoconference with caller <b>231</b>, a research analyst who is located <b>10</b> floors up from the Expert with a complex question regarding a particular security. Caller <b>231</b> decides to add London expert <b>232</b> to the videoconference (via the ADD button in Collaboration Initiator window <b>204</b>) to provide additional information regarding the factual history of the security. Upon selecting the ADD button, video window <b>203</b> now displays, as illustrated in FIG. 38, a video mosaic consisting of three smaller images (instead of a single large image displaying only caller <b>231</b>) of the Expert <b>233</b>, caller <b>231</b> and London expert <b>232</b>.</p>
    <p>During this videoconference, an urgent PRIORITY request (New Call window <b>234</b>) is received from the Expert's boss (who is engaged in a three-party videoconference call with two members of the bank's operations department and is attempting to add the Expert to that call to answer a quick question). The Expert puts his three-party videoconference on hold (merely by clicking the HOLD button in video window <b>203</b>) and accepts (via the ACCEPT button of New Call window <b>234</b>) the urgent call from his boss, which results in the Expert being added to the boss' three-party videoconference call.</p>
    <p>As illustrated in FIG. 39, video window <b>203</b> is now replaced with a four-person video mosaic representing a four-party conference call consisting of the Expert <b>233</b>, his boss <b>241</b> and the two members <b>242</b> and <b>243</b> of the bank's operations department. The Expert quickly answers the boss' question and, by clicking on the RESUME button (of video window <b>203</b>) adjacent to the names of the other participants to the call on hold, simultaneously hangs up on the conference call with his boss and resumes his three-party conference call involving the securities issue, as illustrated in video window <b>203</b> of FIG. <b>40</b>.</p>
    <p>While that call was on hold, however, analyst <b>231</b> and London expert <b>232</b> were still engaged in a two-way videoconference (with a blackened portion of the video mosaic on their screens indicating that the Expert was on hold) and had shared and annotated a graphical image <b>250</b> (see annotations <b>251</b> to image <b>250</b> of FIG. 40) illustrating certain financial concerns. Once the Expert resumed the call, analyst <b>231</b> added the Expert to the share session, causing Share window <b>211</b> containing annotated image <b>250</b> to appear on the Expert's screen. Optionally, snapshot sharing could progress while the video was on hold.</p>
    <p>Before concluding his conference regarding the securities, the Expert receives notification of an incoming multimedia mail message—e.g., a beep accompanied by the appearance of an envelope <b>252</b> in the dog's mouth in In Box icon <b>205</b> shown in FIG. <b>40</b>. Once he concludes his call, he quickly scans his incoming multimedia mail message by clicking on In Box icon <b>205</b>, which invokes his mail software, and then selecting the incoming message for a quick scan, as generally illustrated in the top two windows of FIG. <b>2</b>B. He decides it can wait for further review as the sender is an analyst other than the one helping on his security question.</p>
    <p>He then reinitiates (by selecting deferred call indicator <b>230</b>, shown in FIG. 40) his deferred call with field representative <b>201</b> and his client <b>202</b>, as shown in FIG. <b>41</b>. Note that the full state of the call is also recreated, including restoration of previously shared image <b>220</b> with annotations <b>222</b> as they existed when the call was deferred (see FIG. <b>37</b>). Note also in FIG. 41 that, having reviewed his only unread incoming multimedia mail message, In Box icon <b>205</b> no longer shows an envelope in the dog's mouth, indicating that the Expert currently has no unread incoming messages.</p>
    <p>As the Expert continues to provide advice and pricing information to field representative <b>201</b>, he receives notification of three priority calls <b>261</b>-<b>263</b> in short succession. Call <b>261</b> is the Head of Sales for the Chicago office. Working at home, she had instruced her CMW to alert her of all urgent news or messages, and was subsequently alerted to the arrival of the Expert's earlier multimedia mail message. Call <b>262</b> is an urgent international call. Call <b>263</b> is from the Head of Sales in Los Angeles. The Expert quickly winds down and then concludes his call with field representative <b>201</b>.</p>
    <p>The Expert notes from call indicator <b>262</b> that this call is not only an international call (shown in the top portion of the New Call window), but he realizes it is from a laptop user in the field in Central Mexico. The Expert elects to prioritize his calls in the following manner: <b>262</b>, <b>261</b> and <b>263</b>. He therefore quickly answers call <b>261</b> (by clicking on its ACCEPT button) and puts that call on hold while deferring call <b>263</b> in the manner discussed above. He then proceeds to accept the call identified by international call indicator <b>262</b>.</p>
    <p>Note in FIG. 42 deferred call indicator <b>271</b> and the indicator for the call placed on hold (next to the highlighted RESUME button in video window <b>203</b>), as well as the image of caller <b>272</b> from the laptop in the field in Central Mexico. Although Mexican caller <b>272</b> is outdoors and has no direct access to any wired telephone connection, his laptop has two wireless modems permitting dial-up access to two data connections in the nearest field office (through which his calls were routed). The system automatically (based upon the laptop's registered service capabilities) allocated one connection for an analog telephone voice call (using his laptop's built-in microphone and speaker and the Expert's computer-integrated telephony capabilities) to provide audio teleconferencing. The other connection provides control, data conferencing and one-way digital video (i.e., the laptop user cannot see the image of the Expert) from the laptop's built-in camera, albeit at a very slow frame rate (e.g., 3-10 small frames per second) due to the relatively slow dial-up phone connection.</p>
    <p>It is important to note that, despite the limited capabilities of the wireless laptop equipment, the present invention accommodates such capabilities, supplementing an audio telephone connection with limited (i.e., relatively slow) one-way video and data conferencing functionality. As telephony and video compression technologies improve, the present invention will accommodate such improvements automatically. Moreover, even with one participant to a teleconference having limited capabilities, other participants need not be reduced to this “lowest common denominator.” For example, additional participants could be added to the call illustrated in FIG. 42 as described above, and such participants could have full videoconferencing, data conferencing and other collaborative functionality vis-a-vis one another, while having limited functionality only with caller <b>272</b>.</p>
    <p>As his day evolved, the off-site salesperson <b>272</b> in Mexico was notified by his manager through the laptop about a new security and became convinced that his client would have particular interest in this issue. The salesperson therefore decided to contact the Expert as shown in FIG. <b>42</b>. While discussing the security issues, the Expert again shares all captured graphs, charts, etc.</p>
    <p>The salesperson <b>272</b> also needs the Expert's help on another issue. He has hard copy only of a client's portfolio and needs some advice on its composition before he meets with the client tomorrow. He says he will fax it to the Expert for analysis. Upon receiving the fax—on his CMW, via computer-integrated fax—the Expert asks if he should either send the Mexican caller a “QuickTime” movie (a lower quality compressed video standard from Apple Computer) on his laptop tonight or send a higher-quality CD via FedX tomorrow—the notion being that the Expert can produce an actual video presentation with models and annotations in video form. The salesperson can then play it to his client tomorrow afternoon and it will be as if the Expert is in the room. The Mexican caller decides he would prefer the CD.</p>
    <p>Continuing with this scenario, the Expert learns, in the course of his call with remote laptop caller <b>272</b>, that he missed an important issue during his previous quick scan of his incoming multimedia mail message. The Expert is upset that the sender of the message did not utilize the “video highlight” feature to highlight this aspect of the message. This feature permits the composer of the message to define “tags” (e.g., by clicking a TAG button, not shown) during record time which are stored with the message along with a “time stamp,” and which cause a predefined or selectable audio and/or visual indicator to be played/displayed at that precise point in the message during playback.</p>
    <p>Because this issue relates to the caller that the Expert has on hold, the Expert decides to merge the two calls together by adding the call on hold to his existing call. As noted above, both the Expert and the previously held caller will have full video capabilities vis-a-vis one another and will see a three-way mosaic image (with the image of caller <b>272</b> at a slower frame rate), whereas caller <b>272</b> will have access only to the audio portion of this three-way conference call, though he will have data conferencing functionality with both of the other participants.</p>
    <p>The Expert forwards the multimedia mail message to both caller <b>272</b> and the other participant, and all three of them review the video enclosure in greater detail and discuss the concern raised by caller <b>272</b>. They share certain relevant data as described above and realize that they need to ask a quick question of another remote expert. They add that expert to the call (resulting in the addition of a fourth image to the video mosaic, also not shown) for less than a minute while they obtain a quick answer to their question. They then continue their three-way call until the Expert provides his advice and then adjourns the call.</p>
    <p>The Expert composes a new multimedia mail message, recording his image and audio synchronized (as described above) to the screen displays resulting from his simultaneous interaction with his CMW (e.g., running a program that performs certain calculations and displays a graph while the Expert illustrates certain points by telepointing on the screen, during which time his image and spoken words are also captured). He sends this message to a number of salesforce recipients whose identities are determined automatically by an outgoing mail filter that utilizes a database of information on each potential recipient (e.g., selecting only those whose clients have investment policies which allow this type of investment).</p>
    <p>The Expert then receives an audio and visual reminder (not shown) that a particular video feed (e.g., a short segment of a financial cable television show featuring new financial instruments) will be triggered automatically in a few minutes. He uses this time to search his local securities database, which is dynamically updated from financial information feeds (e.g., prepared from a broadcast textual stream of current financial events with indexed headers that automatically applies data filters to select incoming events relating to certain securities). The video feed is then displayed on the Expert's screen and he watches this short video segment.</p>
    <p>After analyzing this extremely up-to-date information, the Expert then reinitiates his previously deferred call, from indicator <b>271</b> shown in FIG. 42, which he knows is from the Head of Sales in Los Angeles, who is seeking to provide his prime clients with securities advice on another securities transaction based upon the most recent available information. The Expert's call is not answered directly, though he receives a short prerecorded video message (left by the caller who had to leave his home for a meeting across town soon after his priority message was deferred) asking that the Expert leave him a multimedia mail reply message with advice for a particular client, and explaining that he will access this message remotely from his laptop as soon as his meeting is concluded. The Expert complies with this request and composes and sends this mail message.</p>
    <p>The Expert then receives an audio and visual reminder on his screen indicating that his office hours will end in two minutes. He switches from “intercom” mode to “telephone” mode so that he will no longer be disturbed without an opportunity to reject incoming calls via the New Call window described above. He then receives and accepts a final call concerning an issue from an electronic meeting several months ago, which was recorded in its entirety.</p>
    <p>The Expert accesses this recorded meeting from his “corporate memory”. He searches the recorded meeting (which appears in a second video window on his screen as would a live meeting, along with standard controls for stop/play/rewind/fast forward/etc.) for an event that will trigger his memory using his fast forward controls, but cannot locate the desired portion of the meeting. He then elects to search the ASCII text log (which was automatically extracted in the background after the meeting had been recorded, using the latest voice recognition techniques), but still cannot locate the desired portion of the meeting. Finally, he applies an information filter to perform a content-oriented (rather than literal) search and finds the portion of the meeting he was seeking. After quickly reviewing this short portion of the previously recorded meeting, the Expert responds to the caller's question, adjourns the call and concludes his office hours.</p>
    <p>It should be noted that the above scenario involves many state-of-the-art desktop tools (e.g., video and information feeds, information filtering and voice recognition) that can be leveraged by our Expert during videoconferencing, data conferencing and other collaborative activities provided by the present invention—because this invention, instead of providing a dedicated videoconferencing system, provides a desktop multimedia collaboration system that integrates into the Expert's existing workstation/LAN/WAN environment.</p>
    <p>It should also be noted that all of the preceding collaborative activities in this scenario took place during a relatively short portion of the expert's day (e.g., less than an hour of cumulative time) while the Expert remained in his office and continued to utilize the tools and information available from his desktop. Prior to this invention, such a scenario would not have been possible because many of these activities could have taken place only with face-to-face collaboration, which in many circumstances is not feasible or economical and which thus may well have resulted in a loss of the associated business opportunities.</p>
    <p>Although the present invention has been described in connection with particular preferred embodiments and examples, it is to be understood that many modifications and variations can be made in hardware, software, operation, uses, protocols and data formats without departing from the scope to which the inventions disclosed herein are entitled. For example, for certain applications, it will be useful to provide some or all of the audio/video signals in digital form. Accordingly, the present invention is to be considered as including all apparatus and methods encompassed by the appended claims.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3723653">US3723653</a></td><td class="patent-data-table-td patent-date-value">Oct 21, 1968</td><td class="patent-data-table-td patent-date-value">Mar 27, 1973</td><td class="patent-data-table-td ">Matsushita Electric Ind Co Ltd</td><td class="patent-data-table-td ">Television telephone system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3873771">US3873771</a></td><td class="patent-data-table-td patent-date-value">Apr 11, 1972</td><td class="patent-data-table-td patent-date-value">Mar 25, 1975</td><td class="patent-data-table-td ">Telescan Communications System</td><td class="patent-data-table-td ">Simultaneous transmission of a video and an audio signal through an ordinary telephone transmission line</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3974337">US3974337</a></td><td class="patent-data-table-td patent-date-value">May 23, 1974</td><td class="patent-data-table-td patent-date-value">Aug 10, 1976</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">FM television telephone system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4005265">US4005265</a></td><td class="patent-data-table-td patent-date-value">Dec 19, 1974</td><td class="patent-data-table-td patent-date-value">Jan 25, 1977</td><td class="patent-data-table-td ">U.S. Philips Corporation</td><td class="patent-data-table-td ">Videophone system synchronizer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4054908">US4054908</a></td><td class="patent-data-table-td patent-date-value">May 24, 1976</td><td class="patent-data-table-td patent-date-value">Oct 18, 1977</td><td class="patent-data-table-td ">Poirier Alain M</td><td class="patent-data-table-td ">Videotelephone conference system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4210927">US4210927</a></td><td class="patent-data-table-td patent-date-value">May 8, 1978</td><td class="patent-data-table-td patent-date-value">Jul 1, 1980</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Method for transmitting a color video signal on a narrow-band transmission line</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4441180">US4441180</a></td><td class="patent-data-table-td patent-date-value">May 30, 1980</td><td class="patent-data-table-td patent-date-value">Apr 3, 1984</td><td class="patent-data-table-td ">Licentia Patent-Verwaltungs-Gmbh</td><td class="patent-data-table-td ">Service integrated communication transmission and interchange system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4451705">US4451705</a></td><td class="patent-data-table-td patent-date-value">May 28, 1982</td><td class="patent-data-table-td patent-date-value">May 29, 1984</td><td class="patent-data-table-td ">Bell Telephone Laboratories, Incorporated</td><td class="patent-data-table-td ">Call completion circuit and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4475193">US4475193</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 1982</td><td class="patent-data-table-td patent-date-value">Oct 2, 1984</td><td class="patent-data-table-td ">Astech, Inc.</td><td class="patent-data-table-td ">Power line carrier multi telephone extension system for full duplex conferencing between telephones</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4516156">US4516156</a></td><td class="patent-data-table-td patent-date-value">Mar 15, 1982</td><td class="patent-data-table-td patent-date-value">May 7, 1985</td><td class="patent-data-table-td ">Satellite Business Systems</td><td class="patent-data-table-td ">Teleconferencing method and system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4529839">US4529839</a></td><td class="patent-data-table-td patent-date-value">Oct 25, 1983</td><td class="patent-data-table-td patent-date-value">Jul 16, 1985</td><td class="patent-data-table-td ">At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Multilocation video conference terminal including an arrangement to reduce disruption in video switching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4529840">US4529840</a></td><td class="patent-data-table-td patent-date-value">Oct 26, 1983</td><td class="patent-data-table-td patent-date-value">Jul 16, 1985</td><td class="patent-data-table-td ">At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Multilocation video conference terminal including controllable conference location reconfiguration</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4531024">US4531024</a></td><td class="patent-data-table-td patent-date-value">Oct 25, 1983</td><td class="patent-data-table-td patent-date-value">Jul 23, 1985</td><td class="patent-data-table-td ">At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Multilocation video conference terminal including video switching contention control</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4574374">US4574374</a></td><td class="patent-data-table-td patent-date-value">Oct 25, 1983</td><td class="patent-data-table-td patent-date-value">Mar 4, 1986</td><td class="patent-data-table-td ">At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Multilocation video conference terminal including rapid video switching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4645872">US4645872</a></td><td class="patent-data-table-td patent-date-value">May 6, 1986</td><td class="patent-data-table-td patent-date-value">Feb 24, 1987</td><td class="patent-data-table-td ">John Hopkins University</td><td class="patent-data-table-td ">Videophone network system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4650929">US4650929</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 1985</td><td class="patent-data-table-td patent-date-value">Mar 17, 1987</td><td class="patent-data-table-td ">Heinrich-Hertz-Institut Fur Nachrichtentechnik Berlin Gmbh</td><td class="patent-data-table-td ">Communication system for videoconferencing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4653090">US4653090</a></td><td class="patent-data-table-td patent-date-value">Dec 16, 1985</td><td class="patent-data-table-td patent-date-value">Mar 24, 1987</td><td class="patent-data-table-td ">American Telephone &amp; Telegraph (At&amp;T)</td><td class="patent-data-table-td ">Terminal for use with a communication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4686698">US4686698</a></td><td class="patent-data-table-td patent-date-value">Apr 8, 1985</td><td class="patent-data-table-td patent-date-value">Aug 11, 1987</td><td class="patent-data-table-td ">Datapoint Corporation</td><td class="patent-data-table-td ">Workstation for interfacing with a video conferencing network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4710917">US4710917</a></td><td class="patent-data-table-td patent-date-value">Apr 8, 1985</td><td class="patent-data-table-td patent-date-value">Dec 1, 1987</td><td class="patent-data-table-td ">Datapoint Corporation</td><td class="patent-data-table-td ">Video conferencing network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4716585">US4716585</a></td><td class="patent-data-table-td patent-date-value">Apr 5, 1985</td><td class="patent-data-table-td patent-date-value">Dec 29, 1987</td><td class="patent-data-table-td ">Datapoint Corporation</td><td class="patent-data-table-td ">Gain switched audio conferencing network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4796293">US4796293</a></td><td class="patent-data-table-td patent-date-value">Dec 18, 1987</td><td class="patent-data-table-td patent-date-value">Jan 3, 1989</td><td class="patent-data-table-td ">Communications Network Enhancement Inc.</td><td class="patent-data-table-td ">Enhanced dedicated teleconferencing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4817018">US4817018</a></td><td class="patent-data-table-td patent-date-value">Jan 29, 1987</td><td class="patent-data-table-td patent-date-value">Mar 28, 1989</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Electronic calendaring method which provides for automatic assignment of alternates in requested events</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4837798">US4837798</a></td><td class="patent-data-table-td patent-date-value">Jun 2, 1986</td><td class="patent-data-table-td patent-date-value">Jun 6, 1989</td><td class="patent-data-table-td ">American Telephone And Telegraph Company</td><td class="patent-data-table-td ">Communication system having unified messaging</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4847829">US4847829</a></td><td class="patent-data-table-td patent-date-value">Nov 25, 1987</td><td class="patent-data-table-td patent-date-value">Jul 11, 1989</td><td class="patent-data-table-td ">Datapoint Corporation</td><td class="patent-data-table-td ">Video conferencing network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4888795">US4888795</a></td><td class="patent-data-table-td patent-date-value">Jun 28, 1988</td><td class="patent-data-table-td patent-date-value">Dec 19, 1989</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Videotelephone apparatus for transmitting high and low resolution video signals over telephone exchange lines</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4922523">US4922523</a></td><td class="patent-data-table-td patent-date-value">Aug 7, 1989</td><td class="patent-data-table-td patent-date-value">May 1, 1990</td><td class="patent-data-table-td ">Hashimoto Corporation</td><td class="patent-data-table-td ">Videotex with telephone call screening system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4931872">US4931872</a></td><td class="patent-data-table-td patent-date-value">Nov 24, 1986</td><td class="patent-data-table-td patent-date-value">Jun 5, 1990</td><td class="patent-data-table-td ">Gte Laboratories Incorporated</td><td class="patent-data-table-td ">Methods of and apparatus for the generation of split-screen video displays</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4953159">US4953159</a></td><td class="patent-data-table-td patent-date-value">Jan 3, 1989</td><td class="patent-data-table-td patent-date-value">Aug 28, 1990</td><td class="patent-data-table-td ">American Telephone And Telegraph Company</td><td class="patent-data-table-td ">Arrangements for processing different categories of messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4961211">US4961211</a></td><td class="patent-data-table-td patent-date-value">Jun 30, 1988</td><td class="patent-data-table-td patent-date-value">Oct 2, 1990</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Television conference system including many television monitors and method for controlling the same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4965819">US4965819</a></td><td class="patent-data-table-td patent-date-value">Sep 22, 1988</td><td class="patent-data-table-td patent-date-value">Oct 23, 1990</td><td class="patent-data-table-td ">Docu-Vision, Inc.</td><td class="patent-data-table-td ">Video conferencing system for courtroom and other applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4977520">US4977520</a></td><td class="patent-data-table-td patent-date-value">Feb 3, 1988</td><td class="patent-data-table-td patent-date-value">Dec 11, 1990</td><td class="patent-data-table-td ">Ibm Corp.</td><td class="patent-data-table-td ">Method to facilitate a reply to electronic meeting invitation in an interactive multi-terminal system employing electronic calendars</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4987492">US4987492</a></td><td class="patent-data-table-td patent-date-value">Sep 28, 1987</td><td class="patent-data-table-td patent-date-value">Jan 22, 1991</td><td class="patent-data-table-td ">Stults Robert A</td><td class="patent-data-table-td ">User interface control for communication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4995071">US4995071</a></td><td class="patent-data-table-td patent-date-value">May 30, 1989</td><td class="patent-data-table-td patent-date-value">Feb 19, 1991</td><td class="patent-data-table-td ">Telenorma Telefonbau Und Normalzeit Gmbh</td><td class="patent-data-table-td ">Video conference installation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4998243">US4998243</a></td><td class="patent-data-table-td patent-date-value">Oct 10, 1989</td><td class="patent-data-table-td patent-date-value">Mar 5, 1991</td><td class="patent-data-table-td ">Racal Data Communications Inc.</td><td class="patent-data-table-td ">ISDN terminal adapter with teleconference provision</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5003532">US5003532</a></td><td class="patent-data-table-td patent-date-value">May 31, 1990</td><td class="patent-data-table-td patent-date-value">Mar 26, 1991</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Multi-point conference system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5010399">US5010399</a></td><td class="patent-data-table-td patent-date-value">Jul 14, 1989</td><td class="patent-data-table-td patent-date-value">Apr 23, 1991</td><td class="patent-data-table-td ">Inline Connection Corporation</td><td class="patent-data-table-td ">Video transmission and control system utilizing internal telephone lines</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5014267">US5014267</a></td><td class="patent-data-table-td patent-date-value">Apr 6, 1989</td><td class="patent-data-table-td patent-date-value">May 7, 1991</td><td class="patent-data-table-td ">Datapoint Corporation</td><td class="patent-data-table-td ">Video conferencing network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5027400">US5027400</a></td><td class="patent-data-table-td patent-date-value">Aug 16, 1989</td><td class="patent-data-table-td patent-date-value">Jun 25, 1991</td><td class="patent-data-table-td ">Hitachi Ltd.</td><td class="patent-data-table-td ">Multimedia bidirectional broadcast system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5042062">US5042062</a></td><td class="patent-data-table-td patent-date-value">Oct 23, 1989</td><td class="patent-data-table-td patent-date-value">Aug 20, 1991</td><td class="patent-data-table-td ">At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Method and apparatus for providing real-time switching of high bandwidth transmission channels</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5056136">US5056136</a></td><td class="patent-data-table-td patent-date-value">Mar 9, 1990</td><td class="patent-data-table-td patent-date-value">Oct 8, 1991</td><td class="patent-data-table-td ">The United States Of America As Represented By The United States Department Of Energy</td><td class="patent-data-table-td ">Secure video communications system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5072442">US5072442</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 1990</td><td class="patent-data-table-td patent-date-value">Dec 10, 1991</td><td class="patent-data-table-td ">Harris Corporation</td><td class="patent-data-table-td ">Multiple clock rate teleconferencing network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5073926">US5073926</a></td><td class="patent-data-table-td patent-date-value">Jan 12, 1990</td><td class="patent-data-table-td patent-date-value">Dec 17, 1991</td><td class="patent-data-table-td ">Asch Corporation</td><td class="patent-data-table-td ">Picture communication apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5099510">US5099510</a></td><td class="patent-data-table-td patent-date-value">Jun 11, 1990</td><td class="patent-data-table-td patent-date-value">Mar 24, 1992</td><td class="patent-data-table-td ">Communications Network Enhancement Inc.</td><td class="patent-data-table-td ">Teleconferencing with bridge partitioning and other features</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5130399">US5130399</a></td><td class="patent-data-table-td patent-date-value">May 20, 1991</td><td class="patent-data-table-td patent-date-value">Jul 14, 1992</td><td class="patent-data-table-td ">Shin-Etsu Chemical Co., Ltd.</td><td class="patent-data-table-td ">Using a basic catalysi and a dehydrating agent for producing polysiloxanes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5130793">US5130793</a></td><td class="patent-data-table-td patent-date-value">Sep 20, 1989</td><td class="patent-data-table-td patent-date-value">Jul 14, 1992</td><td class="patent-data-table-td ">Etat Francais</td><td class="patent-data-table-td ">Reconfigurable multiple-point wired in-house network for simultaneous and/or alternative distribution of several types of signals, notably baseband images, and method for the configuration of a system such as this</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5130801">US5130801</a></td><td class="patent-data-table-td patent-date-value">Aug 23, 1990</td><td class="patent-data-table-td patent-date-value">Jul 14, 1992</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Image superimposing apparatus having limited memory requirement</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5155761">US5155761</a></td><td class="patent-data-table-td patent-date-value">Jan 26, 1990</td><td class="patent-data-table-td patent-date-value">Oct 13, 1992</td><td class="patent-data-table-td ">Intervoice, Inc.</td><td class="patent-data-table-td ">Automatic call back system and method of operation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5157491">US5157491</a></td><td class="patent-data-table-td patent-date-value">Aug 27, 1990</td><td class="patent-data-table-td patent-date-value">Oct 20, 1992</td><td class="patent-data-table-td ">Kassatly L Samuel A</td><td class="patent-data-table-td ">Method and apparatus for video broadcasting and teleconferencing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5170427">US5170427</a></td><td class="patent-data-table-td patent-date-value">Feb 2, 1990</td><td class="patent-data-table-td patent-date-value">Dec 8, 1992</td><td class="patent-data-table-td ">L&#39;etat Francais</td><td class="patent-data-table-td ">Audio and video communications terminal with improved adjustments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5195086">US5195086</a></td><td class="patent-data-table-td patent-date-value">Apr 12, 1990</td><td class="patent-data-table-td patent-date-value">Mar 16, 1993</td><td class="patent-data-table-td ">At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Multiple call control method in a multimedia conferencing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5195087">US5195087</a></td><td class="patent-data-table-td patent-date-value">Jul 7, 1992</td><td class="patent-data-table-td patent-date-value">Mar 16, 1993</td><td class="patent-data-table-td ">At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Telephone system with monitor on hold feature</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5200989">US5200989</a></td><td class="patent-data-table-td patent-date-value">May 23, 1989</td><td class="patent-data-table-td patent-date-value">Apr 6, 1993</td><td class="patent-data-table-td ">Italtel Societa Italiana</td><td class="patent-data-table-td ">Wide band communication system transmitting video and audio signals among a plurality of users</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5202957">US5202957</a></td><td class="patent-data-table-td patent-date-value">Aug 9, 1990</td><td class="patent-data-table-td patent-date-value">Apr 13, 1993</td><td class="patent-data-table-td ">Future Communications</td><td class="patent-data-table-td ">Full motion video telephone system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5218627">US5218627</a></td><td class="patent-data-table-td patent-date-value">Dec 19, 1990</td><td class="patent-data-table-td patent-date-value">Jun 8, 1993</td><td class="patent-data-table-td ">U S West Advanced Technologies</td><td class="patent-data-table-td ">Decentralized video telecommunication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5224094">US5224094</a></td><td class="patent-data-table-td patent-date-value">Feb 11, 1991</td><td class="patent-data-table-td patent-date-value">Jun 29, 1993</td><td class="patent-data-table-td ">Motorola, Inc.</td><td class="patent-data-table-td ">Communication system network that includes full duplex conference calling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5231492">US5231492</a></td><td class="patent-data-table-td patent-date-value">Mar 16, 1990</td><td class="patent-data-table-td patent-date-value">Jul 27, 1993</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Video and audio multiplex transmission system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5239466">US5239466</a></td><td class="patent-data-table-td patent-date-value">Oct 4, 1990</td><td class="patent-data-table-td patent-date-value">Aug 24, 1993</td><td class="patent-data-table-td ">Motorola, Inc.</td><td class="patent-data-table-td ">System for selectively routing and merging independent annotations to a document at remote locations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5253362">US5253362</a></td><td class="patent-data-table-td patent-date-value">Jan 29, 1990</td><td class="patent-data-table-td patent-date-value">Oct 12, 1993</td><td class="patent-data-table-td ">Emtek Health Care Systems, Inc.</td><td class="patent-data-table-td ">Method for storing, retrieving, and indicating a plurality of annotations in a data cell</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5260941">US5260941</a></td><td class="patent-data-table-td patent-date-value">May 29, 1992</td><td class="patent-data-table-td patent-date-value">Nov 9, 1993</td><td class="patent-data-table-td ">Rose Communications, Inc.</td><td class="patent-data-table-td ">Digital radio telephone system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5283637">US5283637</a></td><td class="patent-data-table-td patent-date-value">Aug 20, 1990</td><td class="patent-data-table-td patent-date-value">Feb 1, 1994</td><td class="patent-data-table-td ">Christine Holland Trustee/Goolcharan Trust</td><td class="patent-data-table-td ">Telecommunication system for transmitting full motion video</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5303343">US5303343</a></td><td class="patent-data-table-td patent-date-value">Sep 20, 1990</td><td class="patent-data-table-td patent-date-value">Apr 12, 1994</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Multi-medium store-and-forward exchange apparatus and method of controlling the apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5315633">US5315633</a></td><td class="patent-data-table-td patent-date-value">Dec 20, 1991</td><td class="patent-data-table-td patent-date-value">May 24, 1994</td><td class="patent-data-table-td ">Unisys Corporation</td><td class="patent-data-table-td ">Digital video switch for video teleconferencing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5319795">US5319795</a></td><td class="patent-data-table-td patent-date-value">Feb 27, 1992</td><td class="patent-data-table-td patent-date-value">Jun 7, 1994</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Cellular mobile communications system using frequencies commonly shared by neighboring cells for handoff operations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5333133">US5333133</a></td><td class="patent-data-table-td patent-date-value">Apr 28, 1992</td><td class="patent-data-table-td patent-date-value">Jul 26, 1994</td><td class="patent-data-table-td ">Teloquent Communications Corporation</td><td class="patent-data-table-td ">Call processing control system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5333299">US5333299</a></td><td class="patent-data-table-td patent-date-value">Dec 31, 1991</td><td class="patent-data-table-td patent-date-value">Jul 26, 1994</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Synchronization techniques for multimedia data streams</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5335321">US5335321</a></td><td class="patent-data-table-td patent-date-value">Jun 19, 1992</td><td class="patent-data-table-td patent-date-value">Aug 2, 1994</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Scalable multimedia platform architecture</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5345258">US5345258</a></td><td class="patent-data-table-td patent-date-value">May 4, 1993</td><td class="patent-data-table-td patent-date-value">Sep 6, 1994</td><td class="patent-data-table-td ">Sanyo Electric Co., Inc.</td><td class="patent-data-table-td ">A videophone having an automatic answering capability</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5351276">US5351276</a></td><td class="patent-data-table-td patent-date-value">Feb 10, 1992</td><td class="patent-data-table-td patent-date-value">Sep 27, 1994</td><td class="patent-data-table-td ">Simpact Associates, Inc.</td><td class="patent-data-table-td ">Digital/audio interactive communication network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5353398">US5353398</a></td><td class="patent-data-table-td patent-date-value">Mar 24, 1993</td><td class="patent-data-table-td patent-date-value">Oct 4, 1994</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Group working system having operator discriminating function</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5363507">US5363507</a></td><td class="patent-data-table-td patent-date-value">Aug 12, 1991</td><td class="patent-data-table-td patent-date-value">Nov 8, 1994</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Method and system for storing and retrieving collaboratively processed information by associated identification data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5365265">US5365265</a></td><td class="patent-data-table-td patent-date-value">Jul 15, 1992</td><td class="patent-data-table-td patent-date-value">Nov 15, 1994</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Multipoint teleconference system employing communication channels set in ring configuration</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5367629">US5367629</a></td><td class="patent-data-table-td patent-date-value">Dec 18, 1992</td><td class="patent-data-table-td patent-date-value">Nov 22, 1994</td><td class="patent-data-table-td ">Sharevision Technology, Inc.</td><td class="patent-data-table-td ">Digital video compression system utilizing vector adaptive transform</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5373549">US5373549</a></td><td class="patent-data-table-td patent-date-value">Dec 23, 1992</td><td class="patent-data-table-td patent-date-value">Dec 13, 1994</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Multi-level conference management and notification</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5374952">US5374952</a></td><td class="patent-data-table-td patent-date-value">Feb 18, 1994</td><td class="patent-data-table-td patent-date-value">Dec 20, 1994</td><td class="patent-data-table-td ">Target Technologies, Inc.</td><td class="patent-data-table-td ">Videoconferencing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5375068">US5375068</a></td><td class="patent-data-table-td patent-date-value">Jun 3, 1992</td><td class="patent-data-table-td patent-date-value">Dec 20, 1994</td><td class="patent-data-table-td ">Digital Equipment Corporation</td><td class="patent-data-table-td ">Video teleconferencing for networked workstations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5379374">US5379374</a></td><td class="patent-data-table-td patent-date-value">Nov 21, 1991</td><td class="patent-data-table-td patent-date-value">Jan 3, 1995</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Collaborative information processing system and workstation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5382972">US5382972</a></td><td class="patent-data-table-td patent-date-value">Sep 8, 1992</td><td class="patent-data-table-td patent-date-value">Jan 17, 1995</td><td class="patent-data-table-td ">Kannes; Deno</td><td class="patent-data-table-td ">Video conferencing system for courtroom and other applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5384598">US5384598</a></td><td class="patent-data-table-td patent-date-value">Oct 20, 1992</td><td class="patent-data-table-td patent-date-value">Jan 24, 1995</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and method for frame differencing video compression and decompression with frame rate scalability</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5384772">US5384772</a></td><td class="patent-data-table-td patent-date-value">Sep 1, 1993</td><td class="patent-data-table-td patent-date-value">Jan 24, 1995</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Method and apparatus for audio flow control during teleconferencing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5392223">US5392223</a></td><td class="patent-data-table-td patent-date-value">Jul 29, 1992</td><td class="patent-data-table-td patent-date-value">Feb 21, 1995</td><td class="patent-data-table-td ">International Business Machines Corp.</td><td class="patent-data-table-td ">For coupling workstation units in a system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5392346">US5392346</a></td><td class="patent-data-table-td patent-date-value">May 26, 1994</td><td class="patent-data-table-td patent-date-value">Feb 21, 1995</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Mobile log-in capability featuring fixed physical (terminal-dependent) translations and portable logical (user-dependent) translations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5404435">US5404435</a></td><td class="patent-data-table-td patent-date-value">Feb 14, 1994</td><td class="patent-data-table-td patent-date-value">Apr 4, 1995</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Non-text object storage and retrieval</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5408526">US5408526</a></td><td class="patent-data-table-td patent-date-value">Oct 29, 1992</td><td class="patent-data-table-td patent-date-value">Apr 18, 1995</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Conference calling system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5408662">US5408662</a></td><td class="patent-data-table-td patent-date-value">Jul 16, 1993</td><td class="patent-data-table-td patent-date-value">Apr 18, 1995</td><td class="patent-data-table-td ">Fuji Xerox Co., Ltd.</td><td class="patent-data-table-td ">System for performing a cooperative operation on common data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5422883">US5422883</a></td><td class="patent-data-table-td patent-date-value">Oct 16, 1992</td><td class="patent-data-table-td patent-date-value">Jun 6, 1995</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Call setup and channel allocation for a multi-media network bus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5432525">US5432525</a></td><td class="patent-data-table-td patent-date-value">Jul 24, 1990</td><td class="patent-data-table-td patent-date-value">Jul 11, 1995</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Multimedia telemeeting terminal device, terminal device system and manipulation method thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5444476">US5444476</a></td><td class="patent-data-table-td patent-date-value">Dec 11, 1992</td><td class="patent-data-table-td patent-date-value">Aug 22, 1995</td><td class="patent-data-table-td ">The Regents Of The University Of Michigan</td><td class="patent-data-table-td ">System and method for teleinteraction</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5471318">US5471318</a></td><td class="patent-data-table-td patent-date-value">Apr 22, 1993</td><td class="patent-data-table-td patent-date-value">Nov 28, 1995</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Multimedia communications network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5473679">US5473679</a></td><td class="patent-data-table-td patent-date-value">Dec 9, 1993</td><td class="patent-data-table-td patent-date-value">Dec 5, 1995</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Signaling system for broadband communications networks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5475421">US5475421</a></td><td class="patent-data-table-td patent-date-value">Jul 16, 1992</td><td class="patent-data-table-td patent-date-value">Dec 12, 1995</td><td class="patent-data-table-td ">Digital Equipment Corporation</td><td class="patent-data-table-td ">Video data scaling for video teleconferencing workstations communicating by digital data network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5485504">US5485504</a></td><td class="patent-data-table-td patent-date-value">Dec 30, 1994</td><td class="patent-data-table-td patent-date-value">Jan 16, 1996</td><td class="patent-data-table-td ">Alcatel N.V.</td><td class="patent-data-table-td ">Hand-held radiotelephone with video transmission and display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5491695">US5491695</a></td><td class="patent-data-table-td patent-date-value">Oct 13, 1992</td><td class="patent-data-table-td patent-date-value">Feb 13, 1996</td><td class="patent-data-table-td ">Digital Access Corporation</td><td class="patent-data-table-td ">Means and method of dial up bridging of network for high bandwidth digital communication</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5506954">US5506954</a></td><td class="patent-data-table-td patent-date-value">Nov 24, 1993</td><td class="patent-data-table-td patent-date-value">Apr 9, 1996</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">PC-based conferencing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5515491">US5515491</a></td><td class="patent-data-table-td patent-date-value">Dec 31, 1992</td><td class="patent-data-table-td patent-date-value">May 7, 1996</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and system for managing communications within a collaborative data processing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5526024">US5526024</a></td><td class="patent-data-table-td patent-date-value">Aug 12, 1994</td><td class="patent-data-table-td patent-date-value">Jun 11, 1996</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Apparatus for synchronization and display of plurality of digital video data streams</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5550966">US5550966</a></td><td class="patent-data-table-td patent-date-value">Dec 28, 1994</td><td class="patent-data-table-td patent-date-value">Aug 27, 1996</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Automated presentation capture, storage and playback system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5553222">US5553222</a></td><td class="patent-data-table-td patent-date-value">Dec 19, 1995</td><td class="patent-data-table-td patent-date-value">Sep 3, 1996</td><td class="patent-data-table-td ">Taligent, Inc.</td><td class="patent-data-table-td ">Multimedia synchronization system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5561736">US5561736</a></td><td class="patent-data-table-td patent-date-value">Jun 4, 1993</td><td class="patent-data-table-td patent-date-value">Oct 1, 1996</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Three dimensional speech synthesis</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5565910">US5565910</a></td><td class="patent-data-table-td patent-date-value">Mar 26, 1993</td><td class="patent-data-table-td patent-date-value">Oct 15, 1996</td><td class="patent-data-table-td ">Vionx, Inc.</td><td class="patent-data-table-td ">Data and television network for digital computer workstations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5581702">US5581702</a></td><td class="patent-data-table-td patent-date-value">Dec 20, 1993</td><td class="patent-data-table-td patent-date-value">Dec 3, 1996</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Computer conferencing system for selectively linking and unlinking private page with public page by selectively activating linked mode and non-linked mode for each participant</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">ACM Press, Conference on Organizational Computing Systems, SIGOIS Bulletin, vol. 12, No. 2-3, Nov. 5-8, 1991.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ahuja et al., "<a href='http://scholar.google.com/scholar?q="Coordination+and+Control+of+Multimedia+Conferencing%2C"'>Coordination and Control of Multimedia Conferencing,</a>" IEEE Communication Magazine, 30(5): 38-42, May 1992.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ahuja et al., "<a href='http://scholar.google.com/scholar?q="Networking+Requirements+of+the+Rapport+Multimedia+Conferencing+System%2C"'>Networking Requirements of the Rapport Multimedia Conferencing System,</a>" INFOCOM '88, IEEE, pp. 746-751, 1988.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bellcore News, "<a href='http://scholar.google.com/scholar?q="IMAL+Makes+Media+Merging+Magic%2C"'>IMAL Makes Media Merging Magic,</a>" 5(20), Nov. 9, 1988.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Biswas et al., "<a href='http://scholar.google.com/scholar?q="Distributed+Scheduling+of+Meetings%3A+A+Case+Study+in+Prototyping+Distributed+Application%2C"'>Distributed Scheduling of Meetings: A Case Study in Prototyping Distributed Application,</a>" System Integration, 1992 2nd International Conference.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cohen et al., "<a href='http://scholar.google.com/scholar?q="Audio+Windows+for+Binaural+Telecommunication%2C"'>Audio Windows for Binaural Telecommunication,</a>" EIC, Tokyo (Oct. 1991).</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cohen et al., "<a href='http://scholar.google.com/scholar?q="Audio+Windows%3A+User+Interfaces+for+Manipulating+Virtual+Acoustic+Environments%2C"'>Audio Windows: User Interfaces for Manipulating Virtual Acoustic Environments,</a>" pp. 479-480.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cohen et al., "<a href='http://scholar.google.com/scholar?q="Design+and+Control+of+Shared+Conferencing+Environments+for+Audio+Telecommunication%2C"'>Design and Control of Shared Conferencing Environments for Audio Telecommunication,</a>" Proceedings of the Second Int'l Symposium on Measurement and Control Robotics (ISMCR '92), Tsukuba Science City, Japan, (Nov. 15-19, 1992), pp. 405-412.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cohen et al., "<a href='http://scholar.google.com/scholar?q="Exocentric+Control+of+Audio+Imaging+in+Biaural+Telecommunication%2C"'>Exocentric Control of Audio Imaging in Biaural Telecommunication,</a>" IEICE Trans. Fundamentals, vol. E75-A, No. 2, (Feb. 1992).</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cohen et al., "<a href='http://scholar.google.com/scholar?q="Multidimensional+Audio+Window+Management%2C"'>Multidimensional Audio Window Management,</a>" Int'l Journal of Man-Machine Studie, vol. 34:319-336 (1991).</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cohen et al., "<a href='http://scholar.google.com/scholar?q="Multidimensional+Audio+Windows%3A+Conferences%2C+Concerts+and+Cocktails%2C"'>Multidimensional Audio Windows: Conferences, Concerts and Cocktails,</a>" Human Factors Society Meeting, SF, CA, pp. 1-15, Jun. 12, 1991.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Crawford et al., "<a href='http://scholar.google.com/scholar?q="Videomatic+Switching%3A+System+and+Services%2C+Digital+Communications%2C"'>Videomatic Switching: System and Services, Digital Communications,</a>" Int. Zurich Seminar, 1988.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ensor et al., "<a href='http://scholar.google.com/scholar?q="The+Rapport+Multimedia+Conferencing+System-Software+Overview%2C"'>The Rapport Multimedia Conferencing System-Software Overview,</a>" Computer Workstation Conference, IEEE, pp. 52-58, 1988.</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ensor et al., "<a href='http://scholar.google.com/scholar?q="The+Rapport+Multimedia+Conferencing+System%E2%80%94Software+Overview%2C"'>The Rapport Multimedia Conferencing System—Software Overview,</a>" Computer Workstation Conference, IEEE, pp. 52-58, 1988.</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Frontiers in computer communications technology, Sigcom '87 Workshop (Aug. 11-13, 1987).</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Gopal et al., "<a href='http://scholar.google.com/scholar?q="Directories+of+Networks+with+Causally+Connected+Users%2C"'>Directories of Networks with Causally Connected Users,</a>" IEEE, pp. 1060-1064, 1988.</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Horn et al., "<a href='http://scholar.google.com/scholar?q="An+ISDN+Multimedia+Conference+Bridge%2C"'>An ISDN Multimedia Conference Bridge,</a>" TENCON '90-1990 IEEE Region 10 Conference on Computer and Communication, pp. 853-856, 1990.</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Horn et al., "<a href='http://scholar.google.com/scholar?q="An+ISDN+Multimedia+Conference+Bridge%2C"'>An ISDN Multimedia Conference Bridge,</a>" TENCON '90—1990 IEEE Region 10 Conference on Computer and Communication, pp. 853-856, 1990.</td></tr><tr><td class="patent-data-table-td ">19</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kamel, "<a href='http://scholar.google.com/scholar?q="An+Integrated+Approach+to+Share+Synchronous+Groupware+Workspaces%2C"'>An Integrated Approach to Share Synchronous Groupware Workspaces,</a>" IEEE 1993.</td></tr><tr><td class="patent-data-table-td ">20</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kendall et al., "<a href='http://scholar.google.com/scholar?q="Simulating+the+Cue+of+Spatial+Hearing+in+Natural+Environments%2C"'>Simulating the Cue of Spatial Hearing in Natural Environments,</a>" Northwestern University, Evanston, IL 60201.</td></tr><tr><td class="patent-data-table-td ">21</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Klein, Telekommunikation von Angesichtzu Angesicht 2323 Telcom Report 9 (1986) Sep./Okt., No. 5, Erlangen, W. Germany.</td></tr><tr><td class="patent-data-table-td ">22</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kobayashi et al., "<a href='http://scholar.google.com/scholar?q="Development+and+Trial+Operation+of+Video+Teleconference+System%2C"'>Development and Trial Operation of Video Teleconference System,</a>" IEEE Globecom, pp. 2060-2063, 1999.</td></tr><tr><td class="patent-data-table-td ">23</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Lake et al. "<a href='http://scholar.google.com/scholar?q="A+network+environment+for+studying+multimedia+network+architecture+and+control%2C"'>A network environment for studying multimedia network architecture and control,</a>" (1989 Globecom).</td></tr><tr><td class="patent-data-table-td ">24</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Lantz et al., Collaboration Technology Research at Olivetti Research California, Aug. 1989.</td></tr><tr><td class="patent-data-table-td ">25</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Lantz, An Experiment in Integrated Multimedia Conferencing, Department of Computer Science, Stanford University, Stanford , CA 94305, Dec. 1986.</td></tr><tr><td class="patent-data-table-td ">26</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Lauwers et al., Collaboration Awareness in Support of Collaboration Transparency: Requirements for the Next Generation of Shared Windows Systems, (Olivetti Research California) Version of Apr. 1989.</td></tr><tr><td class="patent-data-table-td ">27</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Lauwers et al., Replicated Architectures for Shared Window Systems: A Critique, (Olivetti Research California) Version of Apr. 19990.</td></tr><tr><td class="patent-data-table-td ">28</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Leung et al., Optimum Connection Paths for a Class of Videoconferences, Department of Information Engineering, the Chinese University of Honk Kong, Shatin, Hong Kong.</td></tr><tr><td class="patent-data-table-td ">29</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Maeno et al., Distributed Desktop Conferencing System (Mermaid) Based on Group Communication Architecture, The Transactions of the Institute of Electronics, Information and Comm. Engineers E74 (1991) Sep., No. 9, Tokyo, JP.</td></tr><tr><td class="patent-data-table-td ">30</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Martens, "<a href='http://scholar.google.com/scholar?q="Principal+Components+Analysis+and+Resynthesis+of+Spectral+Cues+to+Perceived+Direction%2C"'>Principal Components Analysis and Resynthesis of Spectral Cues to Perceived Direction,</a>" Proceedings of the 1987 Int'l Computer Music Conference, Northwestern University, Evanston, IL 60201.</td></tr><tr><td class="patent-data-table-td ">31</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Masaki et al., "<a href='http://scholar.google.com/scholar?q="A+Desktop+Teleconferencing+Terminal+Based+on+B-ISDN%3A+PMTC%2C"'>A Desktop Teleconferencing Terminal Based on B-ISDN: PMTC,</a>" NTT Review, 4(4):81-85, 1992.</td></tr><tr><td class="patent-data-table-td ">32</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ng et al., Systems Integration '90, (Apr. 23-26, 1990).</td></tr><tr><td class="patent-data-table-td ">33</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nunokawa et al., "<a href='http://scholar.google.com/scholar?q="Teleconferencing+Using+Stereo+Voices+and+Electronic+OHP%2C"'>Teleconferencing Using Stereo Voices and Electronic OHP,</a>" IEEE, 1988.</td></tr><tr><td class="patent-data-table-td ">34</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ohmori et al., "<a href='http://scholar.google.com/scholar?q="Distributed+Cooperative+Control+for+Sharing+Applications+Based+on+Multiparty+and+Multimedia+Desktop+Conferencing+System%2C"'>Distributed Cooperative Control for Sharing Applications Based on Multiparty and Multimedia Desktop Conferencing System,</a>" IEEE, 1992.</td></tr><tr><td class="patent-data-table-td ">35</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Pate, "<a href='http://scholar.google.com/scholar?q="Trends+in+Multimedia+Applications+and+the+Network+Models+to+Support+Them%2C"'>Trends in Multimedia Applications and the Network Models to Support Them,</a>" Globecom's 90: 1990.</td></tr><tr><td class="patent-data-table-td ">36</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Perkins, "<a href='http://scholar.google.com/scholar?q="Spider%3A+An+investigation+in+collaborative+technologies+and+their+effects+on+network+performance"'>Spider: An investigation in collaborative technologies and their effects on network performance</a>".</td></tr><tr><td class="patent-data-table-td ">37</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ramanathan et al., Optimal Communication Architectures for Multimedia Conferencing in Distributed Systems, Multimedia Laboratory Dept. of Computer Science and Engineering, University of San Diego, La Jolla, CA.</td></tr><tr><td class="patent-data-table-td ">38</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Rangan et al., "<a href='http://scholar.google.com/scholar?q="A+Window-Based+Editor+for+Digital+Video+and+Audio+%2C"'>A Window-Based Editor for Digital Video and Audio ,</a>" System Sciences, 1992 Hawaii Int'l Conference (1992).</td></tr><tr><td class="patent-data-table-td ">39</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Rangan et al., "<a href='http://scholar.google.com/scholar?q="Software+architecture+for+integration+of+video+services+in+the+etherphone+system%2C"'>Software architecture for integration of video services in the etherphone system,</a>" IEEE J. on Selected Areas in Comm., 9(9):1395-1404, Dec. '91.</td></tr><tr><td class="patent-data-table-td ">40</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Sakata et al., "<a href='http://scholar.google.com/scholar?q="Development+and+Evaluation+of+an+In-House+Multimedia+Desktop+Conference+System%2C"'>Development and Evaluation of an In-House Multimedia Desktop Conference System,</a>" NEC Research &amp; Development, No. 98, pp. 107-117, Jul. 1990.</td></tr><tr><td class="patent-data-table-td ">41</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Sakata, "<a href='http://scholar.google.com/scholar?q="B-ISDN+Multimedia+Workstation+Architecture%2C"'>B-ISDN Multimedia Workstation Architecture,</a>" IEEE, 1993.</td></tr><tr><td class="patent-data-table-td ">42</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Statement by Attorney for Applicants-Describing Product Development.</td></tr><tr><td class="patent-data-table-td ">43</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Stefik et al., "<a href='http://scholar.google.com/scholar?q="Beyond+the+Chalkboard%3A+Computer+Support+for+Collaboration+and+Problem+Solving%2C"'>Beyond the Chalkboard: Computer Support for Collaboration and Problem Solving,</a>" Communications of the ACM, vol. 30, No. 1, Jan. 1987.</td></tr><tr><td class="patent-data-table-td ">44</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">The American Users Forum (Niu-Forum) Aug. 6-9, 1990.</td></tr><tr><td class="patent-data-table-td ">45</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Unix 4th Berkeley Release 1991 man pages for "<a href='http://scholar.google.com/scholar?q="login%2C%27+%22htmp%2C%27+%22talk%2C%27+and+"'>login,' "htmp,' "talk,' and </a>"who.' Online Internet: http://www.de.freebds.org.</td></tr><tr><td class="patent-data-table-td ">46</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Unix 4th Berkeley Release 1991 man pages for ‘login,’ ‘htmp,’ ‘talk,’ and ‘who.’ Online Internet: http://www.de.freebds.org.</td></tr><tr><td class="patent-data-table-td ">47</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Vin et al., Hierarchical Conferencing Architectures for Inter-Group Multimedia Collaboration, Multimedia Laboratory Department of Computer Science and Engineering University of California at San Diego, La Jolla.</td></tr><tr><td class="patent-data-table-td ">48</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Vin et al., Multimedia Conferencing in the Etherphone Environment, Computer Magazine, vol. 24, Issue 10, pp. 69-79, 1991.</td></tr><tr><td class="patent-data-table-td ">49</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Watabe et al., "<a href='http://scholar.google.com/scholar?q="A+Distributed+Multiparty+Desktop+Conferencing+System+and+Its+Architecture%2C"'>A Distributed Multiparty Desktop Conferencing System and Its Architecture,</a>" IEEE, 1991.</td></tr><tr><td class="patent-data-table-td ">50</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Watabe et al., "<a href='http://scholar.google.com/scholar?q="Distributed+Desktop+Conferencing+System+with+Multiuser+Multimedia+Interface%2C"'>Distributed Desktop Conferencing System with Multiuser Multimedia Interface,</a>" IEEE, 1991.</td></tr><tr><td class="patent-data-table-td ">51</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Weiss, Desk Top Video Conferencing-An Important Feature of Future Visual, Siemens AG-Munich-West Germany.</td></tr><tr><td class="patent-data-table-td ">52</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Weiss, Desk Top Video Conferencing—An Important Feature of Future Visual, Siemens AG—Munich—West Germany.</td></tr><tr><td class="patent-data-table-td ">53</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Zellweger et al., "<a href='http://scholar.google.com/scholar?q="An+Overview+of+the+Etherphone+System+and+Its+Applications%2C"'>An Overview of the Etherphone System and Its Applications,</a>" Computer Workstations Conference, 1988.</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6417869">US6417869</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 15, 1999</td><td class="patent-data-table-td patent-date-value">Jul 9, 2002</td><td class="patent-data-table-td ">Citicorp Development Center, Inc.</td><td class="patent-data-table-td ">Method and system of user interface for a computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6600725">US6600725</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 9, 1999</td><td class="patent-data-table-td patent-date-value">Jul 29, 2003</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Apparatus and method for providing multimedia conferencing services with selective information services</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6624827">US6624827</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 4, 2000</td><td class="patent-data-table-td patent-date-value">Sep 23, 2003</td><td class="patent-data-table-td ">Dae-Joon Hwang</td><td class="patent-data-table-td ">Apparatus and method for locking or prohibiting access to designated object displayed on shared electronic whiteboard</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6741271">US6741271</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 27, 2000</td><td class="patent-data-table-td patent-date-value">May 25, 2004</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">Thumbnail address book for linked family of imaging appliances</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6751669">US6751669</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 24, 1997</td><td class="patent-data-table-td patent-date-value">Jun 15, 2004</td><td class="patent-data-table-td ">Avaya Technology Corp.</td><td class="patent-data-table-td ">Multimedia multiparty communication system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6792091">US6792091</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 10, 2002</td><td class="patent-data-table-td patent-date-value">Sep 14, 2004</td><td class="patent-data-table-td ">Marc S. Lemchen</td><td class="patent-data-table-td ">Network-based intercom system and method for simulating a hardware based dedicated intercom system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6937712">US6937712</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 3, 2003</td><td class="patent-data-table-td patent-date-value">Aug 30, 2005</td><td class="patent-data-table-td ">Marc S. Lemchen</td><td class="patent-data-table-td ">Network-based intercom system and method for simulating a hardware based dedicated intercom system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6940826">US6940826</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 30, 1999</td><td class="patent-data-table-td patent-date-value">Sep 6, 2005</td><td class="patent-data-table-td ">Nortel Networks Limited</td><td class="patent-data-table-td ">Apparatus and method for packet-based media communications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6941344">US6941344</a></td><td class="patent-data-table-td patent-date-value">Apr 6, 2001</td><td class="patent-data-table-td patent-date-value">Sep 6, 2005</td><td class="patent-data-table-td ">Andrew J. Prell</td><td class="patent-data-table-td ">Method for managing the simultaneous utilization of diverse real-time collaborative software applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6948131">US6948131</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 8, 2000</td><td class="patent-data-table-td patent-date-value">Sep 20, 2005</td><td class="patent-data-table-td ">Vidiator Enterprises Inc.</td><td class="patent-data-table-td ">Communication system and method including rich media tools</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7058689">US7058689</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 28, 2002</td><td class="patent-data-table-td patent-date-value">Jun 6, 2006</td><td class="patent-data-table-td ">Sprint Communications Company L.P.</td><td class="patent-data-table-td ">Sharing of still images within a video telephony call</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7088871">US7088871</a></td><td class="patent-data-table-td patent-date-value">Jun 3, 2004</td><td class="patent-data-table-td patent-date-value">Aug 8, 2006</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for transmitting data for a shared application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7116284">US7116284</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 30, 2000</td><td class="patent-data-table-td patent-date-value">Oct 3, 2006</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Control apparatus of virtual common space using communication line</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7237006">US7237006</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 19, 2005</td><td class="patent-data-table-td patent-date-value">Jun 26, 2007</td><td class="patent-data-table-td ">Andrew Prell</td><td class="patent-data-table-td ">Method for managing the simultaneous utilization of diverse real-time collaborative software applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7293243">US7293243</a></td><td class="patent-data-table-td patent-date-value">May 22, 2002</td><td class="patent-data-table-td patent-date-value">Nov 6, 2007</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Application sharing viewer presentation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7310338">US7310338</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 14, 2003</td><td class="patent-data-table-td patent-date-value">Dec 18, 2007</td><td class="patent-data-table-td ">Cisco Technology, Inc.</td><td class="patent-data-table-td ">Method and apparatus providing multi-service connections within a data communications device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7353252">US7353252</a></td><td class="patent-data-table-td patent-date-value">Jul 3, 2002</td><td class="patent-data-table-td patent-date-value">Apr 1, 2008</td><td class="patent-data-table-td ">Sigma Design</td><td class="patent-data-table-td ">System for electronic file collaboration among multiple users using peer-to-peer network topology</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7356563">US7356563</a></td><td class="patent-data-table-td patent-date-value">Jun 6, 2002</td><td class="patent-data-table-td patent-date-value">Apr 8, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Methods of annotating a collaborative application display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7404014">US7404014</a></td><td class="patent-data-table-td patent-date-value">Jun 3, 2004</td><td class="patent-data-table-td patent-date-value">Jul 22, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for transmitting and determining the effects of display orders from shared application between a host and shadow computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7414638">US7414638</a></td><td class="patent-data-table-td patent-date-value">Jul 22, 2005</td><td class="patent-data-table-td patent-date-value">Aug 19, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Application sharing user interface improvements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7418476">US7418476</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 8, 2006</td><td class="patent-data-table-td patent-date-value">Aug 26, 2008</td><td class="patent-data-table-td ">Pixion, Inc.</td><td class="patent-data-table-td ">Presenting images in a conference system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7418664">US7418664</a></td><td class="patent-data-table-td patent-date-value">Apr 3, 2002</td><td class="patent-data-table-td patent-date-value">Aug 26, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Application sharing single document sharing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7427983">US7427983</a></td><td class="patent-data-table-td patent-date-value">May 30, 2003</td><td class="patent-data-table-td patent-date-value">Sep 23, 2008</td><td class="patent-data-table-td ">Steelcase Development Corporation</td><td class="patent-data-table-td ">Visual communication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7447369">US7447369</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 7, 2003</td><td class="patent-data-table-td patent-date-value">Nov 4, 2008</td><td class="patent-data-table-td ">Ricoh Co., Ltd.</td><td class="patent-data-table-td ">Communication of compressed digital images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7454760">US7454760</a></td><td class="patent-data-table-td patent-date-value">Apr 22, 2002</td><td class="patent-data-table-td patent-date-value">Nov 18, 2008</td><td class="patent-data-table-td ">Rosebud Lms, Inc.</td><td class="patent-data-table-td ">Method and software for enabling n-way collaborative work over a network of computers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7487457">US7487457</a></td><td class="patent-data-table-td patent-date-value">Jan 30, 2006</td><td class="patent-data-table-td patent-date-value">Feb 3, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Application sharing single document sharing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7530022">US7530022</a></td><td class="patent-data-table-td patent-date-value">Jan 30, 2006</td><td class="patent-data-table-td patent-date-value">May 5, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Application sharing single document sharing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7721223">US7721223</a></td><td class="patent-data-table-td patent-date-value">Apr 11, 2006</td><td class="patent-data-table-td patent-date-value">May 18, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Application sharing user interface improvements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7747956">US7747956</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 21, 2006</td><td class="patent-data-table-td patent-date-value">Jun 29, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Integrated experience of vogue system and method for shared integrated online social interaction</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7761505">US7761505</a></td><td class="patent-data-table-td patent-date-value">Feb 3, 2003</td><td class="patent-data-table-td patent-date-value">Jul 20, 2010</td><td class="patent-data-table-td ">Openpeak Inc.</td><td class="patent-data-table-td ">System, method and computer program product for concurrent performance of video teleconference and delivery of multimedia presentation and archiving of same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7768970">US7768970</a></td><td class="patent-data-table-td patent-date-value">Mar 8, 2006</td><td class="patent-data-table-td patent-date-value">Aug 3, 2010</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Method and apparatus for controlling image data in a wireless terminal with normal video communication mode and image mute mode</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7877692">US7877692</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 14, 2006</td><td class="patent-data-table-td patent-date-value">Jan 25, 2011</td><td class="patent-data-table-td ">Accommodata Corporation</td><td class="patent-data-table-td ">Accessible display system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7895524">US7895524</a></td><td class="patent-data-table-td patent-date-value">Apr 21, 2006</td><td class="patent-data-table-td patent-date-value">Feb 22, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Integrated experience of vogue system and method for shared integrated online social interaction</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7911955">US7911955</a></td><td class="patent-data-table-td patent-date-value">Jan 31, 2007</td><td class="patent-data-table-td patent-date-value">Mar 22, 2011</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">Coordinated media control system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7999840">US7999840</a></td><td class="patent-data-table-td patent-date-value">Jun 13, 2006</td><td class="patent-data-table-td patent-date-value">Aug 16, 2011</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Method for performing video communication service and mobile communication terminal therefor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8015495">US8015495</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 2003</td><td class="patent-data-table-td patent-date-value">Sep 6, 2011</td><td class="patent-data-table-td ">Groupserve It Trust Llc</td><td class="patent-data-table-td ">Centrifugal communication and collaboration method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8082517">US8082517</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 10, 2007</td><td class="patent-data-table-td patent-date-value">Dec 20, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Application sharing viewer presentation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8146002">US8146002</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 8, 2005</td><td class="patent-data-table-td patent-date-value">Mar 27, 2012</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Screen sharing session with selective pop-ups</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8179382">US8179382</a></td><td class="patent-data-table-td patent-date-value">Aug 14, 2008</td><td class="patent-data-table-td patent-date-value">May 15, 2012</td><td class="patent-data-table-td ">Steelcase Development Corporation</td><td class="patent-data-table-td ">Visual communication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8255835">US8255835</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 24, 2004</td><td class="patent-data-table-td patent-date-value">Aug 28, 2012</td><td class="patent-data-table-td ">Research In Motion Limited</td><td class="patent-data-table-td ">Method and system for managing unread electronic messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8286089">US8286089</a></td><td class="patent-data-table-td patent-date-value">Dec 30, 2005</td><td class="patent-data-table-td patent-date-value">Oct 9, 2012</td><td class="patent-data-table-td ">Research In Motion Limited</td><td class="patent-data-table-td ">Representing new messages on a communication device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8291347">US8291347</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 14, 2009</td><td class="patent-data-table-td patent-date-value">Oct 16, 2012</td><td class="patent-data-table-td ">Research In Motion Limited</td><td class="patent-data-table-td ">Method and system for managing unread electronic messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8345835">US8345835</a></td><td class="patent-data-table-td patent-date-value">Sep 26, 2011</td><td class="patent-data-table-td patent-date-value">Jan 1, 2013</td><td class="patent-data-table-td ">Zvi Or-Bach</td><td class="patent-data-table-td ">Systems and methods for visual presentation and selection of IVR menu</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8406388">US8406388</a></td><td class="patent-data-table-td patent-date-value">Jul 18, 2011</td><td class="patent-data-table-td patent-date-value">Mar 26, 2013</td><td class="patent-data-table-td ">Zvi Or-Bach</td><td class="patent-data-table-td ">Systems and methods for visual presentation and selection of IVR menu</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8537989">US8537989</a></td><td class="patent-data-table-td patent-date-value">Feb 8, 2011</td><td class="patent-data-table-td patent-date-value">Sep 17, 2013</td><td class="patent-data-table-td ">Tal Lavian</td><td class="patent-data-table-td ">Device and method for providing enhanced telephony</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8548131">US8548131</a></td><td class="patent-data-table-td patent-date-value">Mar 11, 2011</td><td class="patent-data-table-td patent-date-value">Oct 1, 2013</td><td class="patent-data-table-td ">Tal Lavian</td><td class="patent-data-table-td ">Systems and methods for communicating with an interactive voice response system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8548135">US8548135</a></td><td class="patent-data-table-td patent-date-value">Feb 8, 2011</td><td class="patent-data-table-td patent-date-value">Oct 1, 2013</td><td class="patent-data-table-td ">Tal Lavian</td><td class="patent-data-table-td ">Systems and methods for visual presentation and selection of IVR menu</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8553859">US8553859</a></td><td class="patent-data-table-td patent-date-value">Feb 8, 2011</td><td class="patent-data-table-td patent-date-value">Oct 8, 2013</td><td class="patent-data-table-td ">Tal Lavian</td><td class="patent-data-table-td ">Device and method for providing enhanced telephony</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8572303">US8572303</a></td><td class="patent-data-table-td patent-date-value">May 23, 2011</td><td class="patent-data-table-td patent-date-value">Oct 29, 2013</td><td class="patent-data-table-td ">Tal Lavian</td><td class="patent-data-table-td ">Portable universal communication device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8594280">US8594280</a></td><td class="patent-data-table-td patent-date-value">Feb 3, 2010</td><td class="patent-data-table-td patent-date-value">Nov 26, 2013</td><td class="patent-data-table-td ">Zvi Or-Bach</td><td class="patent-data-table-td ">Systems and methods for visual presentation and selection of IVR menu</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8625756">US8625756</a></td><td class="patent-data-table-td patent-date-value">Feb 8, 2011</td><td class="patent-data-table-td patent-date-value">Jan 7, 2014</td><td class="patent-data-table-td ">Tal Lavian</td><td class="patent-data-table-td ">Systems and methods for visual presentation and selection of IVR menu</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8681200">US8681200</a></td><td class="patent-data-table-td patent-date-value">Oct 20, 2006</td><td class="patent-data-table-td patent-date-value">Mar 25, 2014</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd</td><td class="patent-data-table-td ">Video telephony apparatus and method for mobile terminal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8681951">US8681951</a></td><td class="patent-data-table-td patent-date-value">Oct 18, 2011</td><td class="patent-data-table-td patent-date-value">Mar 25, 2014</td><td class="patent-data-table-td ">Tal Lavian</td><td class="patent-data-table-td ">Systems and methods for visual presentation and selection of IVR menu</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8682972">US8682972</a></td><td class="patent-data-table-td patent-date-value">Aug 10, 2010</td><td class="patent-data-table-td patent-date-value">Mar 25, 2014</td><td class="patent-data-table-td ">Pixion, Inc.</td><td class="patent-data-table-td ">Presenting information in a conference</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8687777">US8687777</a></td><td class="patent-data-table-td patent-date-value">Oct 18, 2011</td><td class="patent-data-table-td patent-date-value">Apr 1, 2014</td><td class="patent-data-table-td ">Tal Lavian</td><td class="patent-data-table-td ">Systems and methods for visual presentation and selection of IVR menu</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8731148">US8731148</a></td><td class="patent-data-table-td patent-date-value">Mar 2, 2012</td><td class="patent-data-table-td patent-date-value">May 20, 2014</td><td class="patent-data-table-td ">Tal Lavian</td><td class="patent-data-table-td ">Systems and methods for visual presentation and selection of IVR menu</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8756513">US8756513</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 23, 2002</td><td class="patent-data-table-td patent-date-value">Jun 17, 2014</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Document viewing mechanism for document sharing environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20050188320">US20050188320</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 24, 2004</td><td class="patent-data-table-td patent-date-value">Aug 25, 2005</td><td class="patent-data-table-td ">Bocking Andrew D.</td><td class="patent-data-table-td ">Method and system for managing unread electronic messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090210823">US20090210823</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 14, 2009</td><td class="patent-data-table-td patent-date-value">Aug 20, 2009</td><td class="patent-data-table-td ">Research In Motion Corporation</td><td class="patent-data-table-td ">Method and system for managing unread electronic messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1271950A1?cl=en">EP1271950A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 16, 2002</td><td class="patent-data-table-td patent-date-value">Jan 2, 2003</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Moving images synchronization system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1304878A1?cl=en">EP1304878A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 11, 2001</td><td class="patent-data-table-td patent-date-value">Apr 23, 2003</td><td class="patent-data-table-td ">Siemens Aktiengesellschaft</td><td class="patent-data-table-td ">Method for transmission of communication data, video conference and video chat system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1732322A2?cl=en">EP1732322A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 7, 2006</td><td class="patent-data-table-td patent-date-value">Dec 13, 2006</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Method and apparatus for controlling image data in a wireless terminal with normal video communication mode and image mute mode</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1773054A2?cl=en">EP1773054A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 25, 2006</td><td class="patent-data-table-td patent-date-value">Apr 11, 2007</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Method for performing video communication service and mobile communication terminal therefor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1784015A2?cl=en">EP1784015A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 3, 2006</td><td class="patent-data-table-td patent-date-value">May 9, 2007</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Video telephony apparatus and method for mobile terminal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2403242A1?cl=en">EP2403242A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 25, 2010</td><td class="patent-data-table-td patent-date-value">Jan 4, 2012</td><td class="patent-data-table-td ">Huawei Device Co., Ltd.</td><td class="patent-data-table-td ">Remote user signals identification method, remote conference processing method, apparatus and system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001077824A1?cl=en">WO2001077824A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 6, 2001</td><td class="patent-data-table-td patent-date-value">Oct 18, 2001</td><td class="patent-data-table-td ">Agora Interactive Inc</td><td class="patent-data-table-td ">Method for managing the simultaneous utilization of diverse real-time collaborative software applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2002033687A2?cl=en">WO2002033687A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 15, 2001</td><td class="patent-data-table-td patent-date-value">Apr 25, 2002</td><td class="patent-data-table-td ">Ira Dvir</td><td class="patent-data-table-td ">Method and system for remote video display through a wireless projector</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2003034730A1?cl=en">WO2003034730A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 10, 2002</td><td class="patent-data-table-td patent-date-value">Apr 24, 2003</td><td class="patent-data-table-td ">Klaus Beuse</td><td class="patent-data-table-td ">Method for transmitting communication data, videoconference system and videochat system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2012120240A1?cl=en">WO2012120240A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 8, 2012</td><td class="patent-data-table-td patent-date-value">Sep 13, 2012</td><td class="patent-data-table-td ">Streamwide</td><td class="patent-data-table-td ">Device and method for the distributed mixing of data streams</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc709/defs709.htm&usg=AFQjCNFBXWYqUOVuuxerz7B8cqt9daJk7Q#C709S204000">709/204</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc370/defs370.htm&usg=AFQjCNEr5EDctcusna2HU7Iww2g4dx3BIw#C370S270000">370/270</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE07084">348/E07.084</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE07082">348/E07.082</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc715/defs715.htm&usg=AFQjCNEsvPOacMHcG92b6Esmd6a_S2NVkg#C715S753000">715/753</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE07081">348/E07.081</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE07083">348/E07.083</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc715/defs715.htm&usg=AFQjCNEsvPOacMHcG92b6Esmd6a_S2NVkg#C715S716000">715/716</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06Q0010000000">G06Q10/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0013000000">G06F13/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0015160000">G06F15/16</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04L0012180000">H04L12/18</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04M0003420000">H04M3/42</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04L0029080000">H04L29/08</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04M0003530000">H04M3/53</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04M0003428000">H04M3/428</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04N0007150000">H04N7/15</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04L0029060000">H04L29/06</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04N0007140000">H04N7/14</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04M0007120000">H04M7/12</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04M0003560000">H04M3/56</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04L0012580000">H04L12/58</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04M0007000000">H04M7/00</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L65/4038">H04L65/4038</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N7/147">H04N7/147</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M3/42323">H04M3/42323</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04Q2213/1324">H04Q2213/1324</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N7/148">H04N7/148</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N7/152">H04N7/152</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M3/564">H04M3/564</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04Q2213/13337">H04Q2213/13337</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L12/58">H04L12/58</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L12/185">H04L12/185</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L12/1831">H04L12/1831</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M3/4285">H04M3/4285</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L29/06">H04L29/06</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M3/567">H04M3/567</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L12/581">H04L12/581</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04Q2213/1326">H04Q2213/1326</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L29/06027">H04L29/06027</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04Q2213/13175">H04Q2213/13175</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M3/562">H04M3/562</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M2203/5027">H04M2203/5027</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L51/04">H04L51/04</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M3/563">H04M3/563</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N7/15">H04N7/15</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L12/1818">H04L12/1818</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L12/1827">H04L12/1827</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M3/5307">H04M3/5307</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04Q2213/1332">H04Q2213/1332</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L12/1813">H04L12/1813</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04Q2213/1305">H04Q2213/1305</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M2201/38">H04M2201/38</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M3/42221">H04M3/42221</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04Q2213/1319">H04Q2213/1319</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04Q2213/13389">H04Q2213/13389</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L12/1822">H04L12/1822</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04Q2213/1322">H04Q2213/1322</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M2203/5018">H04M2203/5018</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M7/12">H04M7/12</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L12/1836">H04L12/1836</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04Q2213/13093">H04Q2213/13093</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M7/006">H04M7/006</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L67/14">H04L67/14</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L67/36">H04L67/36</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06Q10/10">G06Q10/10</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=f0hUBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L65/80">H04L65/80</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">H04L12/18D3</span>, <span class="nested-value">G06Q10/10</span>, <span class="nested-value">H04M3/56G</span>, <span class="nested-value">H04L12/58B</span>, <span class="nested-value">H04L12/18D1</span>, <span class="nested-value">H04L12/18D2</span>, <span class="nested-value">H04L12/18D4</span>, <span class="nested-value">H04L51/04</span>, <span class="nested-value">H04M3/56D</span>, <span class="nested-value">H04L29/06C2</span>, <span class="nested-value">H04N7/14A3</span>, <span class="nested-value">H04L29/06</span>, <span class="nested-value">H04N7/14A4</span>, <span class="nested-value">H04M3/53M</span>, <span class="nested-value">H04N7/15</span>, <span class="nested-value">H04L29/08N35</span>, <span class="nested-value">H04L12/18D</span>, <span class="nested-value">H04N7/15M</span>, <span class="nested-value">H04M3/56M</span>, <span class="nested-value">H04L12/58</span>, <span class="nested-value">H04L29/08N13</span>, <span class="nested-value">H04L29/06M8</span>, <span class="nested-value">H04L29/06M4C2</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Sep 5, 2012</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 5, 2011</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 11-19, 29-35 AND 36 IS CONFIRMED. CLAIMS 1-10, 20-28 AND 37-45 ARE CANCELLED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 10, 2010</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100616</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:INTELLECTUAL VENTURES FUND 61 LLC;REEL/FRAME:025339/0981</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">PRAGMATUS AV LLC, VIRGINIA</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 12, 2010</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">AVISTAR COMMUNICATIONS CORPORATION,CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE BY SECURED PARTY;ASSIGNOR:BALDWIN ENTERPRISES, INC.;US-ASSIGNMENT DATABASE UPDATED:20100216;REEL/FRAME:23928/118</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090115</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">INTELLECTUAL VENTURES FUND 61 LLC,NEVADA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:AVISTAR COMMUNICATIONS CORPORATION;US-ASSIGNMENT DATABASEUPDATED:20100216;REEL/FRAME:23928/222</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20091217</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:AVISTAR COMMUNICATIONS CORPORATION;REEL/FRAME:23928/222</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE BY SECURED PARTY;ASSIGNOR:BALDWIN ENTERPRISES, INC.;REEL/FRAME:23928/118</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE BY SECURED PARTY;ASSIGNOR:BALDWIN ENTERPRISES, INC.;REEL/FRAME:023928/0118</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:AVISTAR COMMUNICATIONS CORPORATION;REEL/FRAME:023928/0222</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">INTELLECTUAL VENTURES FUND 61 LLC, NEVADA</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">AVISTAR COMMUNICATIONS CORPORATION, CALIFORNIA</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 30, 2009</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">AVISTAR COMMUNICATIONS CORPORATION, CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE OF SECURITY INTEREST IN INTELLECTUAL PROPERTY;ASSIGNOR:BALDWIN ENTERPRISES, INC., AS COLLATERAL AGENT;REEL/FRAME:023708/0861</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20091229</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">AVISTAR COMMUNICATIONS CORPORATION,CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE OF SECURITY INTEREST IN INTELLECTUAL PROPERTY;ASSIGNOR:BALDWIN ENTERPRISES, INC., AS COLLATERAL AGENT;REEL/FRAME:23708/861</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 22, 2008</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 22, 2008</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080225</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 7, 2008</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">BALDWIN ENTERPRISES, INC., AS COLLATERAL AGENT, UT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:AVISTAR COMMUNICATIONS CORPORATION;REEL/FRAME:020325/0091</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080104</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">BALDWIN ENTERPRISES, INC., AS COLLATERAL AGENT,UTA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:AVISTAR COMMUNICATIONS CORPORATION;REEL/FRAME:20325/91</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 2, 2007</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">AVISTAR COMMUNICATIONS CORPORATION, CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">MERGER;ASSIGNOR:COLLABORATION PROPERTIES, INC.;REEL/FRAME:019910/0032</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20071001</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">AVISTAR COMMUNICATIONS CORPORATION,CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">MERGER;ASSIGNOR:COLLABORATION PROPERTIES, INC.;REEL/FRAME:19910/32</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 8, 2004</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 5, 1998</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COLLABORATION PROPERTIES, INC., NEVADA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LUDWIG, LESTER F.;LAUWERS, J. CHRIS;REEL/FRAME:009167/0020</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19980501</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U3MjpmwUnFZKQksNAkJg30G8BELXg\u0026id=f0hUBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U1fSOK0p417MivoBzoaPcr3E3YTGA\u0026id=f0hUBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U3Ojpa7Y3qAkp4g6CyVX28B7YAaIA","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/UTP_based_video_and_data_conferencing.pdf?id=f0hUBAABERAJ\u0026output=pdf\u0026sig=ACfU3U3T1Rhu9xxNqQ7ns-Ja3VEfD2mZtw"},"sample_url":"http://www.google.com/patents/reader?id=f0hUBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>