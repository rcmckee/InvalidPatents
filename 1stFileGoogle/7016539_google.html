<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7016539 - Method for fast, robust, multi-dimensional pattern recognition - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Method for fast, robust, multi-dimensional pattern recognition"><meta name="DC.contributor" content="William M. Silver" scheme="inventor"><meta name="DC.contributor" content="E. John McGarry" scheme="inventor"><meta name="DC.contributor" content="Matthew L. Hill" scheme="inventor"><meta name="DC.contributor" content="Nigel Foster" scheme="inventor"><meta name="DC.contributor" content="Sanjay Nichani" scheme="inventor"><meta name="DC.contributor" content="Willard P. Foster" scheme="inventor"><meta name="DC.contributor" content="Adam Wagman" scheme="inventor"><meta name="DC.contributor" content="Cognex Corporation" scheme="assignee"><meta name="DC.date" content="1998-7-13" scheme="dateSubmitted"><meta name="DC.description" content="Disclosed is a method for determining the absence or presence of one or more instances of a predetermined pattern in an image, and for determining the location of each found instance within a multidimensional space. A model represents the pattern to be found, the model including a plurality of probes. Each probe represents a relative position at which a test is performed in an image at a given pose, each such test contributing evidence that the pattern exists at the pose. The method further includes a comparison of the model with a run-time image at each of a plurality of poses. A match score is computed at each pose to provide a match score surface. Then, the match score is compared with an accept threshold, and used to provide the location any instances of the pattern in the image."><meta name="DC.date" content="2006-3-21" scheme="issued"><meta name="DC.relation" content="US:3069654" scheme="references"><meta name="DC.relation" content="US:3936800" scheme="references"><meta name="DC.relation" content="US:4200861" scheme="references"><meta name="DC.relation" content="US:4441206" scheme="references"><meta name="DC.relation" content="US:4567610" scheme="references"><meta name="DC.relation" content="US:4637055" scheme="references"><meta name="DC.relation" content="US:4651341" scheme="references"><meta name="DC.relation" content="US:4672676" scheme="references"><meta name="DC.relation" content="US:4736437" scheme="references"><meta name="DC.relation" content="US:4783829" scheme="references"><meta name="DC.relation" content="US:4799175" scheme="references"><meta name="DC.relation" content="US:4823394" scheme="references"><meta name="DC.relation" content="US:4843631" scheme="references"><meta name="DC.relation" content="US:4955062" scheme="references"><meta name="DC.relation" content="US:5046109" scheme="references"><meta name="DC.relation" content="US:5048094" scheme="references"><meta name="DC.relation" content="US:5168530" scheme="references"><meta name="DC.relation" content="US:5220621" scheme="references"><meta name="DC.relation" content="US:5253306" scheme="references"><meta name="DC.relation" content="US:5313532" scheme="references"><meta name="DC.relation" content="US:5347595" scheme="references"><meta name="DC.relation" content="US:5384711" scheme="references"><meta name="DC.relation" content="US:5471541" scheme="references"><meta name="DC.relation" content="US:5481712" scheme="references"><meta name="DC.relation" content="US:5495537" scheme="references"><meta name="DC.relation" content="US:5537669" scheme="references"><meta name="DC.relation" content="US:5568563" scheme="references"><meta name="DC.relation" content="US:5586058" scheme="references"><meta name="DC.relation" content="US:5602937" scheme="references"><meta name="DC.relation" content="US:5657403" scheme="references"><meta name="DC.relation" content="US:5717785" scheme="references"><meta name="DC.relation" content="US:5850466" scheme="references"><meta name="DC.relation" content="US:6324299" scheme="references"><meta name="DC.relation" content="US:6466923" scheme="references"><meta name="DC.relation" content="US:6658145" scheme="references"><meta name="DC.relation" content="US:6856698" scheme="references"><meta name="citation_reference" content="Ballard, D.H., &quot;Generalizing the Hough Transform to Detect Arbitrary Shapes,&quot; Pattern Recognition, 1981, pp. 111-122, vol. 13, No. 2, Pergamon Press Ltd., UK."><meta name="citation_reference" content="Ballard, et al., &quot;Section 4.2 Searching Near and Approximate Location,&quot; and &quot;Section 4.3 The Hough Method for Curve Detection,&quot; Computer Vision, 1982, pp. 121-131, Prentice-Hall, Inc., Englewood Cliffs, NJ, USA."><meta name="citation_reference" content="Brown, Lisa Gottesfeld, &quot;A Survey of Image Registration Techniques,&quot; ACM Computing Surveys, Dec. 1992, pp. 325-376, vol. 24, No. 4, Association for Computing Machinery, USA."><meta name="citation_reference" content="Caelli, et al., &quot;Fast Edge-Only Matching Techniques for Robot Pattern Recognition,&quot; Computer Vision, Graphics, and Image Processing 39, 1987, pp. 131-143, Academic Press, Inc."><meta name="citation_reference" content="Caelli, et al., &quot;On the Minimum Number of Templates Required for Shift, Rotation and Size Invariant Pattern Recognition,&quot; Pattern Recognition, 1988, pp. 205-216, vol. 21, No. 3, Pergamon Press plc."><meta name="citation_reference" content="Cognex Corporation, &quot;Apex Model Object,&quot; acuWin version 1.5, Mar. 31, 1997, pp. 1-17, Natick, MA, USA."><meta name="citation_reference" content="Cognex Corporation, &quot;Apex Search Object Library Functions,&quot; Natick, MA, USA, 1998 but public before the above-referenced filing date."><meta name="citation_reference" content="Cognex Corporation, &quot;Apex Search Object,&quot; acuWin version 1.5, Mar. 31, 1997, pp. 1-35, Natick, MA, USA."><meta name="citation_reference" content="Cognex Corporation, &quot;Chapter 1 Searching,&quot; Cognex 3000/4000/5000 Programmable Vision Engines, Vision Tools, 1996, pp. 1-68, Revision 7.4 590-1036, Natick, MA, USA."><meta name="citation_reference" content="Cognex Corporation, &quot;Chapter 14 Golden Template Comparison,&quot; Cognex 3000/4000/5000 Programmable Vision Engines, Vision Tools, 1996, pp. 569-595, Revision 7.4 590-1036, Natick, MA, USA."><meta name="citation_reference" content="Cognex Corporation, &quot;Chapter 2 Searching,&quot; Cognex 2000/3000/4000 Vision Tools, 1992, pp. 2-1 to 2-62, Revision 5.2 P/N 590-0103, Natick, MA, USA."><meta name="citation_reference" content="Cognex Corporation, &quot;Description of Sobel Search,&quot; Natick, MA, USA, 1998 but public before the above-referenced filing date."><meta name="citation_reference" content="Crouzil, et al., &quot;A New Correlation Criterion Based on Gradient Fields Similarity,&quot; Proceedings of the 13&lt;SUP&gt;th &lt;/SUP&gt;International Conference on Pattern Recognition vol. I Track A: Computer Vision, Aug. 25-29, 1996, pp. 632-636, IEEE Computer Society Press, Los Alamitos, CA, USA."><meta name="citation_reference" content="Grimson et al., &quot;On the Sensitivity of the Hough Transform for Object Recognition,&quot; IEEE Transactions on Pattern Analysis and Machine Intelligence, Mar. 1990, pp. 255-274, vol. 12. No. 3."><meta name="citation_reference" content="Hsieh et al., &quot;Image Registration Using a New Edge-Based Approach,&quot; Computer Vision and Image Understanding, Aug. 1997, pp. 112-130, vol. 67, No. 2, Academic Press."><meta name="citation_reference" content="Joseph. &quot;Fast Optimal Pose Estimation for Matching in Two Dimensions.&quot; 5&lt;SUP&gt;th &lt;/SUP&gt;Int. Conf. on Image Processing and Its Applications, Jul. 4, 1995, pp. 355-359."><meta name="citation_reference" content="Rosenfeld et al., &quot;Coarse-Fine Template Matching,&quot; IEEE Transactions on Systems, Man, and Cybernetics, Feb. 1997, pp. 104-107, USA."><meta name="citation_reference" content="Tian et al., &quot;Algorithms for Subpixel Registration,&quot; Computer Vision, Graphics, and Image Processing 35, 1986, pp. 220-233, Academic Press, Inc."><meta name="citation_patent_number" content="US:7016539"><meta name="citation_patent_application_number" content="US:09/114,335"><link rel="canonical" href="http://www.google.com/patents/US7016539"/><meta property="og:url" content="http://www.google.com/patents/US7016539"/><meta name="title" content="Patent US7016539 - Method for fast, robust, multi-dimensional pattern recognition"/><meta name="description" content="Disclosed is a method for determining the absence or presence of one or more instances of a predetermined pattern in an image, and for determining the location of each found instance within a multidimensional space. A model represents the pattern to be found, the model including a plurality of probes. Each probe represents a relative position at which a test is performed in an image at a given pose, each such test contributing evidence that the pattern exists at the pose. The method further includes a comparison of the model with a run-time image at each of a plurality of poses. A match score is computed at each pose to provide a match score surface. Then, the match score is compared with an accept threshold, and used to provide the location any instances of the pattern in the image."/><meta property="og:title" content="Patent US7016539 - Method for fast, robust, multi-dimensional pattern recognition"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("CpbtU5DSBrSzsASWk4C4DA"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("ITA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("CpbtU5DSBrSzsASWk4C4DA"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("ITA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7016539?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7016539"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=_y5yBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7016539&amp;usg=AFQjCNHh-_Mz08ntnWwZX8WOF2NSTDCt0A" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7016539.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7016539.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7016539" style="display:none"><span itemprop="description">Disclosed is a method for determining the absence or presence of one or more instances of a predetermined pattern in an image, and for determining the location of each found instance within a multidimensional space. A model represents the pattern to be found, the model including a plurality of probes....</span><span itemprop="url">http://www.google.com/patents/US7016539?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7016539 - Method for fast, robust, multi-dimensional pattern recognition</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7016539 - Method for fast, robust, multi-dimensional pattern recognition" title="Patent US7016539 - Method for fast, robust, multi-dimensional pattern recognition"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7016539 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/114,335</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Mar 21, 2006</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Jul 13, 1998</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Jul 13, 1998</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US8229222">US8229222</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8244041">US8244041</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8249362">US8249362</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8254695">US8254695</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8265395">US8265395</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8270748">US8270748</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8295613">US8295613</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8320675">US8320675</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8331673">US8331673</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8335380">US8335380</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8363942">US8363942</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8363956">US8363956</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8363972">US8363972</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20130142421">US20130142421</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">09114335, </span><span class="patent-bibdata-value">114335, </span><span class="patent-bibdata-value">US 7016539 B1, </span><span class="patent-bibdata-value">US 7016539B1, </span><span class="patent-bibdata-value">US-B1-7016539, </span><span class="patent-bibdata-value">US7016539 B1, </span><span class="patent-bibdata-value">US7016539B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22William+M.+Silver%22">William M. Silver</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22E.+John+McGarry%22">E. John McGarry</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Matthew+L.+Hill%22">Matthew L. Hill</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Nigel+Foster%22">Nigel Foster</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Sanjay+Nichani%22">Sanjay Nichani</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Willard+P.+Foster%22">Willard P. Foster</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Adam+Wagman%22">Adam Wagman</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Cognex+Corporation%22">Cognex Corporation</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7016539.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7016539.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7016539.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (36),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (18),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (33),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (8),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (6)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=_y5yBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7016539&usg=AFQjCNGI8klJ4ssltP_pW2mWPAwgSZrz3A">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=_y5yBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7016539&usg=AFQjCNEJKTQ9vWTbDlSTxGHC29F9AnTN_A">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=_y5yBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7016539B1%26KC%3DB1%26FT%3DD&usg=AFQjCNE-hssQRj_8CZJhl67rviBJLgp7KA">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55546916" lang="EN" load-source="patent-office">Method for fast, robust, multi-dimensional pattern recognition</invention-title></span><br><span class="patent-number">US 7016539 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA50946170" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">Disclosed is a method for determining the absence or presence of one or more instances of a predetermined pattern in an image, and for determining the location of each found instance within a multidimensional space. A model represents the pattern to be found, the model including a plurality of probes. Each probe represents a relative position at which a test is performed in an image at a given pose, each such test contributing evidence that the pattern exists at the pose. The method further includes a comparison of the model with a run-time image at each of a plurality of poses. A match score is computed at each pose to provide a match score surface. Then, the match score is compared with an accept threshold, and used to provide the location any instances of the pattern in the image.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(35)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00018.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00018.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00019.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00019.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00020.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00020.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00021.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00021.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00022.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00022.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00023.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00023.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00024.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00024.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00025.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00025.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00026.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00026.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00027.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00027.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00028.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00028.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00029.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00029.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00030.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00030.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00031.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00031.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00032.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00032.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00033.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00033.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7016539B1/US07016539-20060321-D00034.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7016539B1/US07016539-20060321-D00034.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(35)</span></span></div><div class="patent-text"><div mxw-id="PCLM8979627" lang="EN" load-source="patent-office" class="claims">
  <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
    <div class="claim-text">1. A method for determining the presence or absence of at least one instance of a predetermined pattern in a run-time image, and for determining the multidimensional location (pose) of each present instance, the method comprising:
<div class="claim-text">providing a model that represents the pattern to be found, the model including a plurality of probes, each probe representing a relative position at which at least one test is performed in an image at a given pose, each such test contributing evidence that the pattern exists at the pose;</div>
<div class="claim-text">providing the run-time image;</div>
<div class="claim-text">comparing the model with the run-time image at each of a plurality of poses;</div>
<div class="claim-text">computing a match score at each pose to provide a match score surface;</div>
<div class="claim-text">locating local maxima in the match score surface;</div>
<div class="claim-text">comparing the magnitude of each local maxima with an accept threshold; and</div>
<div class="claim-text">returning the location of each local maxima with magnitude that exceeds the accept threshold so as to provide the location any instances of the pattern in the image.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
    <div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the model is created from a training image.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
    <div class="claim-text">3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the model is synthesized from a geometric description.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
    <div class="claim-text">4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each probe provides a measurement of gradient direction.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
    <div class="claim-text">5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each probe provides a measurement of both gradient direction and magnitude.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
    <div class="claim-text">6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each probes represent different tests at different steps of the method.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
    <div class="claim-text">7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a probe is characterized by a position, a direction, and a weight.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
    <div class="claim-text">8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein probe position is a point in a pattern coordinate system at which, aft transforming to a image coordinate system using a given pose, a measurement and test is to be made.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
    <div class="claim-text">9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein probe direction is the expected gradient direction in pattern coordinates at the indicated position, which also must be transformed to image coordinates prior to use.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
    <div class="claim-text">10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein probe weight gives the relative importance of the probe in determining the presence and location of the pattern.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
    <div class="claim-text">11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein probe weights can be positive or negative, a negative weight indicating that a test showing similar gradient direction and sufficient gradient magnitude counts as evidence against the existence of the pattern at the specified pose.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
    <div class="claim-text">12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the model includes a plurality of probes placed at selected points along boundaries represented by the corresponding pattern.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
    <div class="claim-text">13. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the probes are uniformly spaced along segments of the boundaries characterized by a small curvature.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
    <div class="claim-text">14. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the spacing between the probes is chosen so that a predetermined number of probes is used.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
    <div class="claim-text">15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein fewer probes can be used to prevent the spacing from being set below some predetermined minimum value, and more probes can be used to prevent the spacing from being set above some predetermined maximum value.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
    <div class="claim-text">16. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the predetermined number of probes is 64.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
    <div class="claim-text">17. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein model granularity is selectable over a wide range down to the limit imposed by an sensor that provides the image to be searched.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
    <div class="claim-text">18. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein providing a model includes automatically choosing a suitable granularity.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
    <div class="claim-text">19. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein providing a model includes using at least two granularities are used, so that the speed advantages of the coarsest granularity and the accuracy advantages of the finest granularity can be obtained.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
    <div class="claim-text">20. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein The model includes a separate set of probes for each granularity.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00021" num="00021" class="claim">
    <div class="claim-text">21. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the horizontal and vertical components of gradient are measured using a Sobel kernel.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00022" num="00022" class="claim">
    <div class="claim-text">22. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein a CORDIC algorithm is used to compute gradient magnitude and direction.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00023" num="00023" class="claim">
    <div class="claim-text">23. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein providing the run-time image includes:
<div class="claim-text">applying Sobel kernels to a filtered, sub-sampled acquired image so as to provide a gradient magnitude image and a gradient direction image that together provide image gradient information at uniformly spaced points.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00024" num="00024" class="claim">
    <div class="claim-text">24. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein computing a match score includes computing gradient direction.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00025" num="00025" class="claim">
    <div class="claim-text">25. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, further including computing gradient magnitude.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00026" num="00026" class="claim">
    <div class="claim-text">26. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, further including computing probe weight.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00027" num="00027" class="claim">
    <div class="claim-text">27. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein computing a match score includes:
<div class="claim-text">assigning a rating of 0 to probe positions having a gradient direction error below a first predetermined value;</div>
<div class="claim-text">assigning a rating of 1 to probe positions having gradient direction error above a second predetermined value; and</div>
<div class="claim-text">assigning a rating between 1 and 0 to probe positions having a gradient direction error that falls between the said first and second predetermined values.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00028" num="00028" class="claim">
    <div class="claim-text">28. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein computing a match score includes:
<div class="claim-text">computing a weighted sum of probe ratings; and</div>
<div class="claim-text">dividing the weighted sum by the total weight of all probes to provide the match score.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00029" num="00029" class="claim">
    <div class="claim-text">29. The method of clam <b>28</b>, wherein all probe weights are 1.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00030" num="00030" class="claim">
    <div class="claim-text">30. The method of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein an expected value of the weighted sum of the probe ratings on random gradient directions is subtracted from an actual weighted sum, with the total weight adjusted accordingly, so that a perfect match still gets a score of 1.0 but an expected value of the score on random noise is 0.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00031" num="00031" class="claim">
    <div class="claim-text">31. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein computing a match score includes:
<div class="claim-text">computing a direction rating factor for each probe;</div>
<div class="claim-text">assigning a probe rating to each probe that is the product of the direction rating factor and a gradient magnitude under the probe; and</div>
<div class="claim-text">computing a weighted sum of the probe ratings to provide the match score.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00032" num="00032" class="claim">
    <div class="claim-text">32. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein computing a match score includes:
<div class="claim-text">computing a direction rating factor for each probe;</div>
<div class="claim-text">computing a magnitude rating factor that is 1.0 for gradient magnitudes above a certain first value, 0 for magnitudes below a certain second value, and proportionally between 0 and 1.0 for values between said first and second values;</div>
<div class="claim-text">assigning to each probe a rating that is the product of the direction rating factor and the magnitude rating factor; and</div>
<div class="claim-text">providing a match score that is a weighted sum of the probe ratings divided by the total weight of all the probes.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00033" num="00033" class="claim">
    <div class="claim-text">33. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein returning the location of each local maxima with magnitude that exceeds the accept threshold includes:
<div class="claim-text">refining the found positions of the local maxima by evaluating a small, dense set of poses surrounding each coarse peak.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00034" num="00034" class="claim">
    <div class="claim-text">34. The method of <claim-ref idref="CLM-00033">claim 33</claim-ref>, further including:
<div class="claim-text">interpolating among the dense set of poses to provide an interpolated position of the local maximum so as to provide a potential instance of the pattern in the run-time image.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00035" num="00035" class="claim">
    <div class="claim-text">35. The method of <claim-ref idref="CLM-00034">claim 34</claim-ref>, further including:
<div class="claim-text">evaluating a match function to determine whether an instance of the pattern is actually present in the run-time image at said pose by comparing the value of the match function to an accept threshold. </div>
</div>
  </div>
</div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES16002171" lang="EN" load-source="patent-office" class="description">
<heading>BACKGROUND OF THE INVENTION</heading> <p num="p-0002">Digital images are formed by many devices and used for many practical purposes. Devices include TV cameras operating on visible or infrared light, line-scan sensors, flying spot scanners, electron microscopes, X-ray devices including CT scanners, magnetic resonance imagers, and other devices known to those skilled in the art. Practical applications are found in industrial automation, medical diagnosis, satellite imaging for a variety of military, civilian, and scientific purposes, photographic processing, surveillance and traffic monitoring, document processing, and many others.</p>
  <p num="p-0003">To serve these applications the images formed by the various devices are analyzed by digital devices to extract appropriate information. One form of analysis that is of considerable practical importance is determining the position, orientation, and size of patterns in an image that correspond to objects in the field of view of the imaging device. Pattern location methods are of particular importance in industrial automation, where they are used to guide robots and other automation equipment in semiconductor manufacturing, electronics assembly, pharmaceuticals, food processing, consumer goods manufacturing, and many others.</p>
  <p num="p-0004">Another form of digital image analysis of practical importance is identifying differences between an image of an object and a stored pattern that represents the ideal appearance of the object. Methods for identifying these differences are generally referred to as pattern inspection methods, and are used in industrial automation for assembly, packaging, quality control, and many other purposes.</p>
  <p num="p-0005">One early, widely-used method for pattern location and inspection is known as blob analysis. In this method, the pixels of a digital image are classified as object or background by some means, typically by comparing pixel gray-levels to a threshold. Pixels classified as object are grouped into blobs using the rule that two object pixels are part of the same blob if they are neighbors; this is known as connectivity analysis. For each such blob one determines properties such as area, perimeter, center of mass, principal moments of inertia, and principal axes of inertia. The position, orientation, and size of a blob is taken to be its center of mass, angle of first principal axis of inertia, and area, respectively. These and the other blob properties can be compared against a known ideal for proposes of inspection.</p>
  <p num="p-0006">Blob analysis is relatively inexpensive to compute, allowing for fast operation on inexpensive hardware. It is reasonably accurate under ideal conditions, and well-suited to objects whose orientation and size are subject to change. One limitation is that accuracy can be severely degraded if some of the object is missing or occluded, or if unexpected extra features are present.</p>
  <p num="p-0007">Another limitation is that the values available for inspection purposes represent coarse features of the object, and cannot be used to detect fine variations. The most severe limitation, however, is that except under limited and well-controlled conditions there is in general no reliable method for classifying pixels as object or background. These limitations forced developers to seek other methods for pattern location and inspection.</p>
  <p num="p-0008">Another method that achieved early widespread use is binary template matching. In this method a training image is used that contains an example of the pattern to be located. The subset of the training image containing the example is thresholded to produce a binary pattern and then stored in a memory. At run-time, images are presented that contain the object to be found. The stored pattern is compared with like-sized subsets of the run-time image at all or selected positions, and the position that best matches the stored pattern is considered the position of the object. Degree of match at a given position of the pattern is simply the fraction of pattern pixels that match their corresponding image pixel.</p>
  <p num="p-0009">Binary template matching does not depend on classifying image pixels as object or background, and so it can be applied to a much wider variety of problems than blob analysis. It also is much better able to tolerate missing or extra pattern features without severe loss of accuracy, and it is able to detect finer differences between the pattern and the object. One limitation, however, is that a binarization threshold is needed, which can be difficult to choose reliably in practice, particularly under conditions of poor signal-to-noise ratio or when illumination intensity or object contrast is subject to variation. Accuracy is typically limited to about one whole pixel due to the substantial loss of information associated with thresholding. Even more serious, however, is that binary template matching cannot measure object orientation and size. Furthermore, accuracy degrades rapidly with small variations in orientation and/or size, and if larger variations are expected the method cannot be used at all.</p>
  <p num="p-0010">A significant improvement over binary template matching came with the advent of relatively inexpensive methods for the use of gray-level normalized correlation for pattern location and inspection. These methods are similar to binary template matching, except that no threshold is used so that the full range of image gray-levels are considered, and the degree of match becomes the correlation coefficient between the stored pattern and the image subset at a given position.</p>
  <p num="p-0011">Since no binarization threshold is needed, and given the fundamental noise immunity of correlation, performance is not significantly compromised under conditions of poor signal-to-noise ratio or when illumination intensity or object contrast is subject to variation. Furthermore, since there is no loss of information due to thresholding, position accuracy down to about  pixel is practical using well-known interpolation methods. The situation regarding orientation and size, however, is not much improved.</p>
  <p num="p-0012">Another limitation of correlation methods is that in many applications object shading can vary locally and non-linearly across an object, resulting in poor correlation with the stored pattern and therefore failure to locate it. For example, in semiconductor fabrication the process step known as chemical mechanical planarization (CMP) results in radical, non-linear changes in pattern shading, which makes alignment using correlation impossible. As another example, in almost any application involving 3-dimensional objects, such as robot pick-and-place applications, shading will vary as a result of variations in angles of illumination incidence and reflection, and from shadows and mutual illumination. The effects are more severe for objects that exhibit significant specular reflection, particularly metals and plastics.</p>
  <p num="p-0013">More recently, improvements to gray-level correlation have been developed that allow it to be used in applications where significant variation in orientation and/or size is expected. In these methods, the stored pattern is rotated and/or scaled by digital image re-sampling methods before being matched against the image. By matching over a range of angles, sizes, and x-y positions, one can locate an object in the corresponding multidimensional space. Note that such methods would not work well with binary template matching, due to the much more severe pixel quantization errors associated with binary images.</p>
  <p num="p-0014">One problem with these methods is the severe computational cost, both of digital re-sampling and of searching a space with more than 2 dimensions. To manage this cost, the search methods break up the problem into two or more phases. The earliest phase uses a coarse, subsampled version of the pattern to cover the entire search space quickly and identify possible object locations. Subsequent phases use finer versions of the pattern to refine the positions determined at earlier phases, and eliminate positions that the finer resolution reveals are not well correlated with the pattern. Note that variations of these coarse-fine methods have also been used with binary template matching and the original two-dimensional correlation, but are even more important with the higher-dimensional search space.</p>
  <p num="p-0015">Even with these techniques, however, the computational cost is still high, and the problems associated with non-linear variation in shading remain.</p>
  <p num="p-0016">Another pattern location method in common use is known as the Generalized Hough Transform (GHT). This method traces its origins to U.S. Pat. No. 3,069,654 [Hough, P.V.C., 1962], which described a method for locating parameterized curves such as lines or conic sections. Subsequently the method was generalized to be able to locate essentially arbitrary patterns. As with the above template matching and correlation methods, the method is based on a trained pattern. Instead of using gray levels directly, however, the GHT method identifies points along object boundaries using well-known methods of edge detection. A large array of accumulators, called Hough space, is constructed, with one such accumulator for each position in the multidimensional space to be searched. Each edge point in the image corresponds to a surface of possible pattern positions in Hough space. For each such edge point, the accumulators along the corresponding surface are incremented. After all image edge points have been processed, the accumulator with the highest count is considered to be the multidimensional location of the pattern.</p>
  <p num="p-0017">The general performance characteristics of GHT are very similar to correlation. Computational cost rises very rapidly with number of dimensions, and although coarse-fine methods have been developed to improve performance, practical applications beyond 2 dimensions are almost nonexistent.</p>
  <p num="p-0018">The edge detection step of GHT generally reduces problems due to non-linear variations in object contrast, but introduces new problems. Use of edge detectors generally increases susceptibility to noise and defocus. For many objects the edges are not sharply defined enough for the edge detection step to yield reliable results. Furthermore, edge detection fundamentally requires a binarization step, where pixels are classified as edge or not edge, usually by a combination of thresholding and peak detection. Binarization, no matter what method is used, is always subject to uncertainty and misclassification, and will contribute failure modes to any method that requires it.</p>
  <p num="p-0019">Terminology</p>
  <p num="p-0020">The following terminology is used throughout the specification:</p>
  <p num="p-0021">ObjectAny physical or simulated object, or portion thereof, having characteristics that can be measured by an image forming device or simulated by a data processing device.</p>
  <p num="p-0022">ImageA 2-dimensional function whose values correspond to physical characteristics of an object, such as brightness (radiant energy, reflected or otherwise), color, temperature, height above a reference plane, etc., and measured by any image-forming device, or whose values correspond to simulated characteristics of an object, and generated by any data processing device.</p>
  <p num="p-0023">BrightnessThe physical or simulated quantity represented by the values of an image, regardless of source.</p>
  <p num="p-0024">GranularityA selectable size (in units of distance) below which spatial variations in image brightness are increasingly attenuated, and below which therefore image features increasingly cannot be resolved. Granularity can be thought of as being related to resolution.</p>
  <p num="p-0025">BoundaryAn imaginary contour, open-ended or closed, straight or curved, smooth or sharp, along which a discontinuity of image brightness occurs at a specified granularity, the direction of said discontinuity being normal to the boundary at each point.</p>
  <p num="p-0026">GradientA vector at a given point in an image giving the direction and magnitude of greatest change in brightness at a specified granularity at said point.</p>
  <p num="p-0027">PatternA specific geometric arrangement of contours lying in a bounded subset of the plane of the contours, said contours representing the boundaries of an idealized image of an object to be located and/or inspected.</p>
  <p num="p-0028">ModelA set of data encoding characteristics of a pattern to be found for use by a pattern finding method.</p>
  <p num="p-0029">TrainingThe act of creating a model from an image of an example object or from a geometric description of an object or a pattern.</p>
  <p num="p-0030">PoseA mapping from pattern to image coordinates and representing a specific transformation and superposition of a pattern onto an image.</p>
  <heading>SUMMARY OF THE INVENTION</heading> <p num="p-0031">In one aspect the invention is a general-purpose method for determining the absence or presence of one or more instances of a predetermined pattern in an image, and determining the location of each found instance. The process of locating patterns occurs within a multidimensional space that can include, but is not limited to, x-y position (also called translation), orientation, and size. In another aspect the invention is a method for identifying differences between a predetermined pattern and a matching image subset. The process of identifying differences is called inspection.</p>
  <p num="p-0032">To avoid ambiguity we will call the location of a pattern in a multidimensional space its pose. More precisely, a pose is a coordinate transform that maps points in a pattern to corresponding points in an image. In a preferred embodiment, a pose is a general 6 degree of freedom linear coordinate transform. The 6 degrees of freedom can be represented by the 4 elements of a 22 matrix, plus the 2 elements of a vector corresponding to the 2 translation degrees of freedom. Alternatively and equivalently, the 4 non-translation degrees of freedom can be represented in other ways, such as orientation, size, aspect ratio, and shear, or x-size, y-size, x-axis-angle, and y-axis-angle.</p>
  <p num="p-0033">The results produced by the invention can be used directly, or can be further refined by multidimensional localization methods such as described in U.S. Pat. No. 6,658,145 entitled Fast High-Accuracy Multi-Dimensional Pattern Inspection.</p>
  <p num="p-0034">The invention uses a model that represents the pattern to be found. The model can be created from a training image or synthesized from a geometric description. The invention is a template matching method-the model is compared to an image at each of an appropriate set of poses, a match score is computed at each pose, and those poses that correspond to a local maximum in match score, and whose match scores are above a suitable accept threshold, are considered instances of the pattern in the image.</p>
  <p num="p-0035">According to the invention, a model includes a set of data elements called probes. Each probe represents a relative position at which certain measurements and tests are to be made in an image at a given pose, each such test contributing evidence that the pattern exists at said pose. In one embodiment of the invention, each probe represents a measurement and test of gradient direction. In another embodiment, each probe represents a measurement and test of both gradient direction and magnitude. In a preferred embodiment, the probes represent different tests at different steps of the method. The gradient magnitude or direction to be tested by a probe is referred to as the gradient magnitude or direction under the probe.</p>
  <p num="p-0036">In a preferred embodiment, a probe is defined by its position, direction, and weight. Each of these quantities are conceptually real numbers, although of course in any actual embodiment they would be represented as floating or fixed point approximations. Probe position is a point in a pattern coordinate system at which, after transforming to a image coordinate system using a given pose, a measurement and test is to be made. Probe direction is the expected gradient direction in pattern coordinates at the indicated position, which also must be transformed to image coordinates prior to use. Probe weight gives the relative importance of the probe in determining the presence and location of the pattern.</p>
  <p num="p-0037">In a preferred embodiment, probe weights can be positive or negative. A negative weight indicates that a test showing similar gradient direction and sufficient gradient magnitude should count as evidence against the existence of the pattern at the specified pose.</p>
  <p num="p-0038">Most points in an image contain little useful information about pattern position. Uniform regions, for example, contain no information about position, since brightness is locally independent of position. Generally the second or higher derivative of brightness must be non-zero in some direction for there to be useful information, and it has long been recognized in the art that the best information occurs along boundaries. Thus examining an image at every point for the purpose of pattern location is unnecessary as well as wasteful of memory and processing time.</p>
  <p num="p-0039">In a preferred embodiment, a model includes a small set of probes placed at selected points along the boundaries represented by the corresponding pattern. The probes are uniformly spaced along segments of the boundaries characterized by a small curvature. The spacing between the probes is chosen so that a predetermined number of probes is used, except that fewer probes can be used to prevent the spacing from being set below some predetermined minimum value, and more probes can be used to prevent the spacing from being set above some predetermined maximum value. In a preferred embodiment, the said predetermined number of probes is 64.</p>
  <p num="p-0040">The boundaries that appear in a given image are not absolute but depend on the granularity at which the image is interpreted. Consider for example a newspaper photograph. Over some range of very fine granularities, one perceives nothing but a pattern of dots of various sizes and separations. Over some range of coarser granularity, the dots cannot be resolved and one may perceive human facial features such as eyes, noses, and mouths. At even coarser granularity, one may perceive only human heads.</p>
  <p num="p-0041">For an image sensor producing a digital image, granularity is limited by pixel size and sharpness of focus. Granularity may be increased above this limit (i.e. made coarser) by suitable image processing operations, and thus effectively controlled over a wide range. In a pattern locating system, choice of granularity affects speed, accuracy, and reliability. When suitable methods are used, pattern locating speed can be made to increase rapidly as granularity increases, which can be crucial for high speed applications where the pattern's pose can vary in more than 2 degrees of freedom. Pattern location accuracy, however, decreases as granularity increases. Pattern locating reliability, the ability to correctly identify patterns when they exist and to avoid misidentifying image subsets that are not instances of the pattern, may fall off if the granularity is too coarse to resolve key pattern features, and may fall off if the granularity is so fine that details are resolved that are inconsistent from instance to instance, such as surface texture or other random microstructure.</p>
  <p num="p-0042">In a preferred embodiment of the invention, granularity is selectable over a wide range down to the limit imposed by the image sensor. In another preferred embodiment, a suitable granularity is automatically chosen during model training. In another preferred embodiment, at least two granularities are used, so that the speed advantages of the coarsest granularity and the accuracy advantages of the finest granularity can be obtained. In the preferred embodiment wherein at least two granularities are used, the model includes a separate set of probes for each granularity.</p>
  <p num="p-0043">Granularity can be increased above the image sensor limit by a low-pass filtering operation, optionally followed by a sub-sampling operation. Methods for low-pass filtering and subsampling of digital images are well known in the art. Until recently, however, inexpensive, high speed methods that could be tuned over a wide range with no significant loss in performance were not available. In a preferred embodiment, the invention makes use of a constant-time second-order approximately parabolic filter, as described in U.S. Pat. No. 6,4570,032, entitled Efficient, Flexible Digital Filtering, followed by a non-integer sub-sampling step wherein brightness values spaced g pixels apart, horizontally and vertically, are linearly interpolated between the filtered pixel values for some value of g chosen at training time.</p>
  <p num="p-0044">Methods for estimating image gradient magnitude and direction are well known in the art, but most methods in common use are either too slow or of insufficient accuracy to be suitable for the practice of the invention. For example, most commercially available gradient estimation methods can only resolve direction to within 45, and only provide a crude estimate of magnitude. One notable exception is described in U.S. Pat. No. 5,657,403, herein incorporated by reference, although at the time of that patent specialized hardware was required for high speed operation. Recent advances in computer architecture and performance have made high speed, accurate gradient estimation practical on inexpensive hardware. In a preferred embodiment, the invention uses the well-known Sobel kernels to estimate the horizontal and vertical components of gradient, and the well-known CORDIC algorithm, as described, for example, in U.S. Pat. No. 6,408,109, entitled Apparatus and Method for Detecting and Sub-Pixel Location of Edges in a Digital Image, herein incorporated by reference, for example, to compute gradient magnitude and direction. The Sobel kernels are applied either to the input image or to a filtered, sub-sampled image, as described above, so that the result is a gradient magnitude image and a gradient direction image that together provide image gradient information at uniformly spaced points, which mayor may not correspond to the pixels of the input image, and at a selectable granularity. In a preferred embodiment, the gradient magnitude and direction images are stored in a random access memory of a computer or other data processing device, in such a manner that the address difference between pixels in the horizontal direction is a first constant, and the address difference between pixels in the vertical direction is a second constant.</p>
  <p num="p-0045">The method of the invention, which tests gradient direction at each of a small set (e.g. 64) of positions, offers many advantages over prior art methods of template matching. Since neither probe position nor direction are restricted to a discrete pixel grid, and since probes represent purely geometric information and not image brightness, they can be translated, rotated, and scaled much faster than digital image re-sampling methods and with less pixel grid quantization error. Furthermore, since probes are spaced along contours where a maximum amount of position information occurs, a small set of probes can be used so that processing time can be minimized.</p>
  <p num="p-0046">Gradient direction is a much more reliable basis for pattern location than image brightness. Brightness may vary as a result of object surface reflectance, intensity of illumination, angles of illumination incidence and reflection, mutual illumination and shadows, sensor gain, and other factors. Gradient direction at boundaries is generally unaffected by these factors as long as the overall shape of the object is reasonably consistent. Furthermore, each individual test of gradient direction provides direct evidence of the presence or absence of a pattern at a given pose, and this evidence has absolute meaning that is independent of the conditions under which a given image was obtained. For example, one generally can conclude that a direction error of 3 degrees is a good match, and of 30 degrees is a poor match, without knowing anything about the pattern to be located or any of the above listed factors affecting the individual brightness values in any given image. By contrast, a test of image brightness is meaningless in itselfwhether 3 brightness units of difference or 30 units of difference is good or bad can only be assessed in relation to the statistics of a large set of brightness values.</p>
  <p num="p-0047">The method of the invention also offers many advantages over prior art methods based on the Hough transform. The high quality of the information provided by tests of gradient direction allows fewer points to be processed, resulting in higher speed. Sets of probes can be rotated and scaled more quickly and accurately than the edge point sets used by GHT methods. Hough methods including the GHT tend to be adversely affected by small variations in pattern shape, or edge position quantization error, where a shift in edge position by even one pixel will cause an undesirable spreading of the peak in Hough space. By contrast, gradient direction is generally consistent within a couple of pixels of a boundary, so that the effects of small variations in shape or quantization errors are generally insignificant. Loss of sharp focus can degrade the edge detection step required for Hough methods, whereas defocus has no effect on gradient direction. Hough transform methods, and all methods based on edge detection, fundamentally require a binarization step, where pixels are classified as edge or not edge, and all such methods are subject to uncertainty and misclassification. The use of gradient direction by the invention requires no binarization or other classification to be applied.</p>
  <p num="p-0048">A variety of match functions based on gradient direction, and optionally gradient magnitude and probe weight, can be used within the scope of the invention. In a first match function, probe positions having gradient direction errors below a first predetermined value are given a rating of 1.0, above a second predetermined value are given a rating of 0, and errors that fall between the said first and second values are given a rating proportionally between 0 and 1.0. The weighted sum of probe ratings, divided by the total weight of all probes, is the match score. With said first match function all probe weights are positive, since a negative weight probe cannot be considered to provide evidence against a pose unless the gradient magnitude is tested and found to be sufficiently strong. The first match function results in the highest possible speed, for two primary reasons. First, gradient magnitude is not used, which reduces both the calculations needed and the number of accesses to memory wherein gradient magnitude information would be stored. Second, due to the general consistency of gradient direction surrounding a boundary, the first match function tends to produce broad peaks in match score, which allows a relatively sparse set of poses to be evaluated.</p>
  <p num="p-0049">In a first variation on said first match function probe weights are not used (i.e. probe weights are effectively all 1.0), which further increases the speed of operation. In a second variation on said first match function, the expected value of the weighted sum of the probe ratings on random gradient directions is subtracted from the actual weighted sum, with the total weight adjusted accordingly, so that a perfect match still gets a score of 1.0 but the expected value of the score on random noise is 0.</p>
  <p num="p-0050">In a second match function, a direction rating factor is computed for each probe that is the same as the probe rating used by the first match function, and probes receive a rating that is the product of the direction rating factor and the gradient magnitude under the probe. The match score is the weighted sum of the probe ratings. With the second match function, probe weights can be positive or negative. The second match function produces sharper peaks in match score than the first, since gradient magnitude is at a maximum at a boundary and falls off sharply on either side. As a result pattern position can be determined more accurately with the second match function, but at a cost of lower speed since more calculations and memory accesses are needed and since a denser set of poses must be evaluated. Unlike the first match function which produces a score between 0 and 1.0, the second match function's score is essentially open-ended and dependent on boundary contrast. Thus while the score can be used to compare a pose to a neighboring pose to determine a peak in match score, it cannot in general be used to compare a pose to a distant pose or to provide a value that can be used reliably to judge whether or not an instance of the pattern is present in the image at a given pose.</p>
  <p num="p-0051">In a third match function, a direction rating factor is computed for each probe identical to that of the second match function, and a magnitude rating factor is computed that is 1.0 for gradient magnitudes above a certain first value, 0 for magnitudes below a certain second value, and proportionally between 0 and 1.0 for values between said first and second values. Each probe receives a rating that is the product of the direction rating factor and the magnitude rating factor, and the match score is the weighted sum of the probe ratings divided by the total weight of all the probes. Probe weights can be positive or negative. In a preferred embodiment, the said first value is computed based on image characteristics at any pose for which the third match function is to be evaluated, so the third match function takes the longest to compute. Furthermore, peaks in match score are generally less sharp than for the second match function, so position is less accurate. The primary advantage of the third match function is that it produces a score that falls between 0 and 1.0 that can be used for comparison and to judge whether or not an instance of the pattern is present in the image at a given pose, and that said score takes into account gradient magnitude and allows negative probe weights.</p>
  <p num="p-0052">In the aspect of the invention where inspection is to be performed, the score produced by the third match function is used to provide an overall measure of the quality of a specific instance of the pattern found in an image, and the individual probe ratings computed during evaluation of the third match function are used to provide more detailed information about differences between the found instance and the pattern.</p>
  <p num="p-0053">In a preferred embodiment, the first match function, and using both the first and second variation, is used during a coarse scan step during which the entire multidimensional search space is evaluated with a relatively sparse set of poses. Poses that are coarse peaks, specifically those at which the first match score is a local maximum and above a predetermined accept threshold, are refined during a fine scan step that evaluates a small, dense set of poses surrounding each coarse peak. The fine scan step uses the second match function to achieve a precise position and to consider the evidence of negative weight probes. An interpolation between the pose resulting in the highest value of the second match function and its neighbors is considered the location of one potential instance of the pattern in the image. A scoring step evaluates the third match function at this final, interpolated pose to judge whether or not an instance of the pattern is actually present in the image at said pose by comparing the value of the third match function to an accept threshold.</p>
  <p num="p-0054">In any specific embodiment of the invention the search space is defined by certain degrees of freedom that include the two translation degrees of freedom and some number, possibly zero, of non-translation degrees of freedom such as orientation and size. Many methods can be devised within the scope of the invention to generate the set of poses to be evaluated for purposes of pattern location. In a preferred embodiment, any specific pose is the result of specifying values for each degree of freedom. The set of poses to be evaluated during the coarse scan step is the result of generating all combinations of selected values for each degree of freedom. For this preferred embodiment, two distinct methods are used in combination to generate the set of poses, one for translation and one for non-translation degrees of freedom.</p>
  <p num="p-0055">According to this preferred embodiment of the invention, for each combination of values of the non-translation degrees of freedom the probe positions and directions are transformed according to the said combination of values from pattern coordinates to an image coordinate system associated with the gradient magnitude and direction images. The resulting positions, which are relative positions since the translation degrees of freedom have not yet been included, are rounded to relative integer pixel coordinates and, using the horizontal and vertical address difference constants, converted to a single integer offset value that gives the relative position of the probe in either the gradient magnitude or direction image at poses corresponding to the said combination of non-translation degrees of freedom. The result is a new set of data elements called compiled probes that include relative image offset, transformed expected gradient direction, and weight.</p>
  <p num="p-0056">According to this preferred embodiment of the invention, during the coarse step the compiled probes are used to evaluate the first match function at a set of translations corresponding to some regular tessellation. For any such translation that is a local maximum in first match score, and where said score is above a suitable accept threshold, a set of data called a result is allocated and added to a list of results. A translation is interpolated between the local maximum and its neighbors and stored in the newly-allocated result, along with an interpolated score. According to this preferred embodiment of the invention, a hexagonal tessellation is used along with methods for determining a local maximum and for interpolation on such a tessellation. In a less preferred variation on this embodiment, a conventional square tessellation is used, including well-known methods for determining the presence of a local maximum and for interpolating between said maximum and its neighbors.</p>
  <p num="p-0057">According to this preferred embodiment of the invention, each non-translation degree of freedom is defined and described by a set of data and functions called a generalized-DOF. Each generalized-DOF includes a single real-valued (or floating point approximation) parameter that specifies its value, for example an orientation degree of freedom would have an angle parameter and a size degree of freedom would have a scale factor parameter. Each generalized-DOF includes a function that maps the parameter value to a corresponding 2-dimensional coordinate transform. For example, for an orientation generalized-DOF this function might include the matrix 
<maths id="MATH-US-00001" num="00001"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mrow> <mi>cos</mi> <mo></mo> <mrow> <mo>(</mo> <mi>x</mi> <mo>)</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>-</mo> <mrow> <mi>sin</mi> <mo></mo> <mrow> <mo>(</mo> <mi>x</mi> <mo>)</mo> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>sin</mi> <mo></mo> <mrow> <mo>(</mo> <mi>x</mi> <mo>)</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mi>cos</mi> <mo></mo> <mrow> <mo>(</mo> <mi>x</mi> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>1</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br>
where x is the parameter. Each generalized-DOF includes a low and high limit value that specifies the range of parameter values within which the pattern should be located for the degree of freedom, and which are set based on the requirements of the application to which the invention is being used. Each generalized-DOF includes a value that if non-zero specifies the period of cyclic degrees of freedom such as orientation (e.g., 360 if the parameter is in degrees). Each generalized-DOF includes a maximum step size value that specifies the maximum allowable separation between parameter values used to generate poses for the degree of freedom for the coarse scan step. In the preferred embodiment, the maximum step size for each generalized-DOF is determined from an analysis of the magnitude of motion of the probes in the expected gradient direction as the generalized-DOF parameter is varied. In a less preferred embodiment, the maximum step size is a fixed, predetermined value.
</p>
  <p num="p-0058">According to this preferred embodiment of the invention, for a given generalized-DOF if the difference between the high and low limit values, which is referred to as the parameter range, is not greater than the maximum step size, then the generalized-DOF parameter is not varied during the coarse step, but is instead set to the halfway point between the limits. If the parameter range is greater than the maximum step size, then an actual step size is computed such that the actual step size is not greater than the maximum and the range is an integer multiple of the actual step size. For the given generalized-DOF, the set of parameter values generated for the coarse scan step range from one-half of the actual step size below the low limit to one-half of the actual step size above the high limit, in increments of the actual step size. It can be seen that a minimum of three distinct parameter values are generated in this case.</p>
  <p num="p-0059">The invention uses a set of nested loops during the coarse scan step to generate all combinations of parameter values of the generalized-DOFs in use, where each such loop corresponds to one generalized-DOF. Each loop steps the parameter value of the corresponding generalized-DOF over the range and using the actual step size as described above, generating a coordinate transform corresponding to each parameter value. In the innermost loop, the coordinate transforms corresponding to the current parameter values of all of the generalized-DOFs are composed to produce a single overall transform specifying all of the non-translation degrees of freedom as needed for the translation degrees of freedom as described above. Data specifying the values of the generalized-DOF parameters are added to the results lists produced during scanning of the translation degrees of freedom. At the end of each of the nested loops, the results lists are scanned to identify sets of results that correspond to the same instance of the pattern in the image at a consecutive sequence of parameter values of the generalized-DOF corresponding to the given nested loop. For each such set found, all but the peak result (the one with the highest score) are deleted, and the parameter value and score are interpolated between the peak result and its neighbors. All of the remaining (i.e. peak) results are concatenated to produce a single master results list representing the results of the coarse scan step.</p>
  <p num="p-0060">During the fine scan step the pose of each result produced by the coarse scan step is refined one or more times. For each such refinement, all of the translation and non-translation degrees of freedom are analyzed and updated, using a revised actual step size for the generalized-DOFs that is one-half that of the previous refinement step. For the first refinement step, the revised actual step size for the generalized-DOFs is one-half that of the coarse scan step. As for the coarse scan step, two distinct methods are used in combination to generate a set of poses, one for translation and one for non-translation degrees of freedom.</p>
  <p num="p-0061">For the translation degrees of freedom, compiled probes are generated as before using the composed coordinate transform that specifies the value all of the non-translation degrees of freedom. The second match function is evaluated at every pixel offset within a small, approximately circular region centered on the translation stored in the result being refined. A new translation is stored that is interpolated between the maximum value of the second match function and its neighbors.</p>
  <p num="p-0062">The invention uses a set of nested loops during the fine scan step to generate certain combinations of parameter values of the generalized-DOFs in use, where each such loop corresponds to one generalized-DOF. For each such loop three parameter values are chosen to startthe current value stored in the result, a value lower by the current revised actual step size, and a value higher by the current revised actual step size. If the lower parameter value results in a second match score that is higher than the match scores resulting from the other two parameter values, further lower parameter values are generated in steps of the current revised actual step size, until either a parameter value is found that does not result in a higher second match score than the other parameter values, or the lower limit for this generalized-DOF is reached. If the lower limit is reached, the final parameter value is stored in the result for this generalized-DOF. If instead a peak second match value was found, the parameter value is interpolated between said peak and its neighbors and stored in the result. Similar steps are followed if at the start of the nested loop the higher parameter value results in a second match score that is higher than the match scores resulting from the other two parameter values.</p>
  <p num="p-0063">Sometimes two or more results produced by the coarse scan step are duplicates that correspond to the same instance of the pattern in the image, differing only slightly in pose ill a preferred embodiment of the invention, when results are found that overlap by more than some predetermined amount in each degree of freedom, the result with the highest score is kept and the other duplicates are deleted.</p>
  <p num="p-0064">The steps associated with the generalized-DOFs for the coarse and fine scan steps can be realized by means of a computer program. Following conventional practice the nested loops described above can be coded directly based on the attributes of a predetermined set of generalized-DOFs. Such a conventional method of coding, however, results in duplication for each generalized-DOF of substantially similar code for both the coarse and fine scan steps, as well as other steps such as the determining of the maximum and actual step sizes, since the steps required for each generalized-DOF are basically the same with only minor differences in how the parameter value is used to produce a coordinate transform and whether or not the generalized-DOF is cyclic. Duplication with minor modifications of a significant amount of complex code for each generalized-DOF results in a computer program where debugging, modification, and maintenance are difficult and error-prone. Adding or removing specific generalized-DOFs, or changing the nesting order, would be particularly difficult and error-prone and could only be done at compile time.</p>
  <p num="p-0065">One aspect of the invention is a solution to the problem of coding a method for scanning non-translation degrees of freedom for the purpose of pattern locating. The coding method requires no duplication of code, and allows non-translation degrees of freedom to be added and removed, and to have the nesting order changed, at run time. The invention is based on the so-called object oriented programming methodology that has recently become generally available in the form of programming languages such as C++ and Java, although the invention could be practiced using many conventional languages such as C and assembly language by simulating the appropriate object oriented features.</p>
  <p num="p-0066">In a preferred embodiment, each generalized-DOF is represented by a C++ class that is derived from an abstract base class that specifies a fixed interface to any generalized-DOF and implements all functionality that is common to all generalized-DOFs. Any functionality that is specific to a generalized-DOF, including the function that maps the parameter value to a coordinate transform, is specified as a virtual function in the base class and overridden in each derived class for the specific generalized-DOF. Data that is specific to a generalized-DOF, such as the parameter limits, maximum and actual step sizes, and cyclic period value, can be held in data members of the base class. Data specifying fixed attributes of the generalized-DOF, such as the cyclic period value, can be initialized by the constructor of the derived class.</p>
  <p num="p-0067">According to this preferred embodiment, a list of generalized-DOFs (instances of the derived classes) is used to specify the non-translation degrees of freedom to be scanned and the nesting order. The list can be constructed from available generalized-DOF classes at runtime. The nested loops of the coarse scan step are implemented by a single non-virtual member function of the base class, which is given a list of generalized-DOFs as one argument. The coarse scan function processes the generalized-DOF at the head of the list by generating the appropriate sequence of parameter values as described above, generating the corresponding coordinate transform, and then calling itself recursively on the remainder of the list of generalized-DOFs to implement loops nested within the current one. When the coarse scan function is finally passed a null list of generalized-DOFs, it calls another function to do the translation degrees of freedom. The nested loops of the fine scan step are handled similarly by another non-virtual member function of the base class.</p>
  <p num="p-0068">The pattern location method of the invention is truly general purpose, because it has the following characteristics:
</p> <ul> <li id="ul0001-0001" num="0000"> <ul> <li id="ul0002-0001" num="0068">Essentially arbitrary patterns can be trained by example;</li> <li id="ul0002-0002" num="0069">Variations in pattern location, orientation, and size can be tolerated and measured;</li> <li id="ul0002-0003" num="0070">Pattern defects such as missing or unexpected features can be tolerated with no significant loss in accuracy;</li> <li id="ul0002-0004" num="0071">Non-linear variations in shading can be tolerated;</li> <li id="ul0002-0005" num="0072">Real-world imaging conditions such as defocus, video noise, and illumination variation can be tolerated; and</li> <li id="ul0002-0006" num="0073">Price/performance ratio is low enough for widespread use.</li> </ul> </li> </ul> <p num="p-0069">Prior to the present invention, there were no practical, truly general-purpose pattern location methods available.</p>
<description-of-drawings> <heading>BRIEF DESCRIPTION OF THE DRAWING</heading> <p num="p-0070">The invention will be more fully understood from the following detailed description, in conjunction with the accompanying figures, wherein:</p>
    <p num="p-0071"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a high-level block diagram of one embodiment of the invention;</p>
    <p num="p-0072"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a is block diagram of the steps for granularity control, gradient estimation, and boundary detection;</p>
    <p num="p-0073"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a flow chart of a preferred embodiment of the model training step of <figref idrefs="DRAWINGS">FIG. 1</figref>;</p>
    <p num="p-0074"> <figref idrefs="DRAWINGS">FIG. 4</figref> is an illustration of an example of a portion of a boundary point list for a small subset of a training image;</p>
    <p num="p-0075"> <figref idrefs="DRAWINGS">FIG. 5</figref> <i>a</i>, <b>5</b> <i>b</i>, and <b>5</b> <i>c </i>show an illustration of portion of <figref idrefs="DRAWINGS">FIG. 4</figref>, showing the details of the connect step of the model training step of <figref idrefs="DRAWINGS">FIG. 3</figref>;</p>
    <p num="p-0076"> <figref idrefs="DRAWINGS">FIG. 6</figref> is an illustration of the portion of <figref idrefs="DRAWINGS">FIG. 4</figref>, showing the details of the segment step of <figref idrefs="DRAWINGS">FIG. 3</figref>;</p>
    <p num="p-0077"> <figref idrefs="DRAWINGS">FIG. 7</figref> is an illustration of a set of probes according to the invention to be included in a model resulting from the training step of <figref idrefs="DRAWINGS">FIG. 1</figref>;</p>
    <p num="p-0078"> <figref idrefs="DRAWINGS">FIG. 8</figref> is an illustration of a synthetic model according to the invention for locating or inspecting a rounded rectangle pattern;</p>
    <p num="p-0079"> <figref idrefs="DRAWINGS">FIG. 9</figref> is an illustration of various typographic and symbolic convention used in the specification;</p>
    <p num="p-0080"> <figref idrefs="DRAWINGS">FIG. 10</figref> is an illustration of a data set that represents a model of <figref idrefs="DRAWINGS">FIG. 1</figref>;</p>
    <p num="p-0081"> <figref idrefs="DRAWINGS">FIG. 11</figref> <i>a </i>is an illustration of a data set that represents a probe of <figref idrefs="DRAWINGS">FIG. 10</figref>;</p>
    <p num="p-0082"> <figref idrefs="DRAWINGS">FIG. 11</figref> <i>b </i>is an illustration of a data set that represents a compiled probe generated from the probe of <figref idrefs="DRAWINGS">FIG. 11</figref> <i>a; </i> </p>
    <p num="p-0083"> <figref idrefs="DRAWINGS">FIGS. 12</figref> <i>a </i>and <b>12</b> <i>b </i>are flow charts illustrating the steps for converting a list of probe objects into a list of compiled-probe objects;</p>
    <p num="p-0084"> <figref idrefs="DRAWINGS">FIGS. 13</figref> <i>a </i>and <b>13</b> <i>b </i>are illustrations of direction rating factor functions;</p>
    <p num="p-0085"> <figref idrefs="DRAWINGS">FIG. 13</figref> <i>c </i>is an illustration of a magnitude rating factor function;</p>
    <p num="p-0086"> <figref idrefs="DRAWINGS">FIG. 14</figref> is an illustration of a data set that represents a generalized-DOF;</p>
    <p num="p-0087"> <figref idrefs="DRAWINGS">FIG. 15</figref> is a table that details specific generalized-DOFs of <figref idrefs="DRAWINGS">FIG. 14</figref> that can be used with the invention;</p>
    <p num="p-0088"> <figref idrefs="DRAWINGS">FIG. 16</figref> is an illustration of the details of a list of generalized-DOFs used in a preferred embodiment of the invention;</p>
    <p num="p-0089"> <figref idrefs="DRAWINGS">FIG. 17</figref> is an illustration of a data set that represents a result corresponding to an instance of a pattern in a run-time image;</p>
    <p num="p-0090"> <figref idrefs="DRAWINGS">FIG. 18</figref> <i>a</i>, <b>18</b> <i>b </i>and <b>18</b> <i>c </i>show an illustration of how position overlap is calculated for a pair of results to determine if they might be neighbors or duplicates;</p>
    <p num="p-0091"> <figref idrefs="DRAWINGS">FIG. 19</figref> is a top-level flow chart of a preferred embodiment of run-time;</p>
    <p num="p-0092"> <figref idrefs="DRAWINGS">FIG. 20</figref> is a flow chart of setting generalized-DOF elements;</p>
    <p num="p-0093"> <figref idrefs="DRAWINGS">FIG. 21</figref> <i>a</i>, <b>21</b> <i>b </i>and <b>21</b> <i>c </i>show a flow chart of a function that scans all of the generalized-DOFs on an input list, and returns a list of results describing poses representing possible instance of a pattern in a run-time image;</p>
    <p num="p-0094"> <figref idrefs="DRAWINGS">FIG. 22</figref> <i>a </i>and <b>22</b> <i>b </i>show a flow chart of a function used to scan the translation degrees of freedom;</p>
    <p num="p-0095"> <figref idrefs="DRAWINGS">FIG. 23</figref> is an illustration of four different preferred coarse scan patterns;</p>
    <p num="p-0096"> <figref idrefs="DRAWINGS">FIG. 24</figref> <i>a </i>shows peak detection rules used by a preferred embodiment of the invention;</p>
    <p num="p-0097"> <figref idrefs="DRAWINGS">FIG. 24</figref> <i>b </i>shows symbols to be used for interpolation on hexagonal scan patterns;</p>
    <p num="p-0098"> <figref idrefs="DRAWINGS">FIG. 25</figref> is a flow chart illustrating the fine scan step of <figref idrefs="DRAWINGS">FIG. 19</figref>;</p>
    <p num="p-0099"> <figref idrefs="DRAWINGS">FIG. 26</figref> is a flow chart illustrating the procedure for performing fine scans;</p>
    <p num="p-0100"> <figref idrefs="DRAWINGS">FIG. 27</figref> is a flow chart illustrating the procedure of <figref idrefs="DRAWINGS">FIG. 26</figref> for performing fine scans in the x-y degrees of freedom;</p>
    <p num="p-0101"> <figref idrefs="DRAWINGS">FIG. 28</figref> is an illustration of a fine x-y scan pattern;</p>
    <p num="p-0102"> <figref idrefs="DRAWINGS">FIG. 29</figref> is a flow chart of the hill climbing step of <figref idrefs="DRAWINGS">FIG. 26</figref>;</p>
    <p num="p-0103"> <figref idrefs="DRAWINGS">FIG. 30</figref> is a flow chart of the plus direction step of <figref idrefs="DRAWINGS">FIG. 29</figref>; and</p>
    <p num="p-0104"> <figref idrefs="DRAWINGS">FIG. 31</figref> is a flow chart illustrating how model granularity is selected based on ratings Q<sub>g</sub>.</p>
  </description-of-drawings> <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading> <p num="p-0105"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a high-level block diagram of one embodiment of the invention. A training image <b>100</b> containing an example of a pattern <b>105</b> to be located and/or inspected is presented. A training step <b>110</b> analyzes the training image and produces a model <b>120</b> for subsequent use. At least one runtime image <b>130</b> is presented, each such image containing zero or more instances of patterns <b>135</b> similar in shape to the training pattern <b>105</b>.</p>
  <p num="p-0106">For each run-time image a run-time step <b>140</b> analyzes the image <b>130</b>, using the model <b>120</b>, and the list of generalized-DOFs <b>150</b>. As a result of the analysis, the run-time step produces a list <b>160</b> of zero or more results, each result corresponding to an instance of the trained pattern <b>105</b> in image <b>130</b> and containing a pose that maps pattern points to corresponding image points, and individual probe rating information for inspection purposes.</p>
  <p num="p-0107"> <figref idrefs="DRAWINGS">FIG. 2</figref> shows a preferred embodiment of image processing steps used by the invention during training step <b>110</b> and run-time step <b>140</b> for granularity control, gradient estimation, and boundary detection. These steps process a source image <b>200</b>, which can be either a training image or a run-time image.</p>
  <p num="p-0108">A low-pass filter <b>210</b> and image sub-sampler <b>220</b> are used to control granularity by attenuating fine detail, such as noise or texture, in the source image that for a variety of reasons we wish to ignore. Methods for low-pass filtering and sub-sampling of digital images are well known in the art. In a preferred embodiment, a constant-time second-order approximately parabolic filter is used, as described in detail in U.S. Pat. No. 6,457,032, entitled Efficient, Flexible Digital Filtering. The parabolic filter <b>210</b> has a single parameter s, not shown in the figure, that controls the response of the filter in both dimensions. In a preferred embodiment, a sub-sampler is used that can produce output pixels corresponding to non-integer positions of the input image by means of the well-known method of bilinear interpolation. The sub-sampler <b>220</b> is controlled by a single parameter g corresponding to the distance in pixels in both dimensions between the points of the input image to which the pixels of the output image should correspond. For example if g is 2.5 then output pixels correspond to input points (0,0), (2.5,0), (5,0), (0,2.5), (2.5,2.5), (2.5,5), (5,0), (5,2.5), (5,5), etc. Input points with non-integer coordinates are interpolated between the four surrounding pixels using bilinear interpolation.</p>
  <p num="p-0109">For a preferred embodiment, the filter parameter s is tied to the sub-sample parameter g by the following formula:
<br> <i>s</i>=round(2.15(<i>g</i>1))(2)</p>
  <p num="p-0110">Thus there is a single independent parameter g for control of the filter <b>210</b> and sub-sampler <b>220</b>, which together constitute the steps used for granularity control, and therefore granularity is defined to be the value g, which is in units of source image pixels. Note that the minimum value of g is 1 pixel, which setting effectively disables the filter (since s is 0) and sub-sampler (since the spacing of output pixels is the same as input pixels), and which corresponds to the granularity limit imposed by the sensor.</p>
  <p num="p-0111">The filtered, sub-sampled image is processed by a gradient estimation step <b>230</b> to produce an estimate of the x (horizontal) and y (vertical) components of image gradient at each pixel. A Cartesian-to-polar conversion step <b>240</b> converts the x and y components of gradient to magnitude and direction. Methods for gradient estimation and Cartesian-to-polar conversion are well-known in the art. In a preferred embodiment, the methods described in U.S. Pat. No. 6,408,109, entitled Apparatus and Method for Detecting and Sub-Pixel Location of Edges in a Digital Image, herein incorporated by reference, are used. In a preferred embodiment, the source image <b>200</b> has 8 bits of gray-scale per pixel. The low-pass filter <b>210</b> produces a 16-bit image, taking advantage of the inherent noise-reduction properties of a low-pass filter. The gradient estimation step <b>230</b> uses the well-known Sobel kernels and operates on either a 16-bit filtered image, if the parameter s is set greater than 0 so as to enable the filter, or an 8-bit unfiltered image if the parameter s is set to 0 so as to disable the filter. The x and y components of gradient are always calculated to 16 bits to avoid loss of precision, and the gradient magnitude and direction are calculated to at least 6 bits using the well-known CORDIC algorithm.</p>
  <p num="p-0112">The output of Cartesian-to-polar conversion step <b>240</b> is a gradient magnitude image <b>242</b> and a gradient direction image <b>244</b>. These images are suitable for use by the run-time step <b>140</b>, but further processing is required to identify boundary points for the training step <b>110</b>. Methods for identifying points along image boundaries are well-known in the art. Any such method can be used for practice of the invention, whether based on gradient estimation or other techniques. In a preferred embodiment shown in <figref idrefs="DRAWINGS">FIG. 2</figref>, the methods described in detail in U.S. Pat. No. 6,408,109, entitled Apparatus and Method for Detecting and Sub-Pixel Location of Edges in a Digital Image, herein incorporated by reference, are used. A peak detection step <b>250</b> identifies points where the gradient magnitude exceeds a noise threshold and is a local maximum along a 1-dimensional profile that lies in approximately the gradient direction, and produces a list of the grid coordinates (row and column number), gradient magnitude, and gradient direction for each such point. A sub-pixel interpolation step <b>260</b> interpolates the position of maximum gradient magnitude along said 1-dimensional profile to determine real-valued (to some precision) coordinates (x<sub>i</sub>, y<sub>i</sub>) of the point. The result is a boundary point list <b>270</b> of points that lie along boundaries in the image, which includes the coordinates, direction, and magnitude of each point.</p>
  <p num="p-0113"> <figref idrefs="DRAWINGS">FIG. 3</figref> shows a flow chart of a preferred embodiment of the model training step <b>110</b>. A training image <b>100</b> containing an example of a pattern to be located and/or inspected is analyzed by a series of steps resulting in a model <b>120</b> containing a list of probes. Additional results of the training step are that a granularity value g is determined that is used both during the training step <b>110</b> and run-time step <b>140</b>. Also, the pattern contrast is determined and stored in the model.</p>
  <p num="p-0114">In the preferred embodiment shown in <figref idrefs="DRAWINGS">FIG. 3</figref>, a step <b>300</b> selects an appropriate value for the granularity control g and stores the value in the model <b>120</b>. Generally lower values of g result in higher accuracy and higher values of g result in higher speed, so the tradeoff is application dependent. For any given pattern, there are limits beyond which the granularity may be too large (coarse) to resolve key pattern features or too small (fine) to attenuate inconsistent detail such as surface texture. In a preferred embodiment a user can enter a suitable granularity manually by viewing a display of boundary point list <b>270</b> superimposed on training image <b>100</b>. In another preferred embodiment, the value g is set automatically based on an analysis of the training image <b>100</b> as described below.</p>
  <p num="p-0115">As shown in <figref idrefs="DRAWINGS">FIG. 3</figref> a boundary point detection step <b>310</b> processes the training image <b>100</b> to produce a list of points <b>270</b> that lie along boundaries in the training image. This step is shown in detail in <figref idrefs="DRAWINGS">FIG. 2</figref> and described above. A connect step <b>320</b> connects boundary points to neighboring boundary points that have consistent directions, using rules further described below, to form chains by associating with each boundary point links to left and right neighbors along the boundaries, if any. A chain step <b>330</b> scans the connected boundary points to identify and catalog discrete chains. For each chain, the starting and ending points, length, total gradient magnitude, and whether the chain is open or closed is determined and stored.</p>
  <p num="p-0116">A filter step <b>340</b> deletes weak chains. A variety of criteria can be used to identify weak chains. In a preferred embodiment, chains whose total gradient magnitude or average gradient magnitude are below some specified parameter are considered weak.</p>
  <p num="p-0117">A segment step <b>350</b> divides chains into zones of low curvature called segments, separated by zones of high curvature called corners. Each boundary point is marked as a part of a segment or corner. Curvature can be determined by a variety of methods; in a preferred embodiment, a boundary point is considered a corner if its direction differs from that of either neighbor by more than 22.5 degrees.</p>
  <p num="p-0118">A probe spacing step <b>360</b> analyzes the segments found by step <b>350</b> and determines a probe spacing that would result in all of the segments being covered by a predetermined target number of probes that are distributed evenly along the segments. The probe spacing is not allowed to fall beyond certain predetermined limits. In a preferred embodiment, the target number of probes is 64, the lower limit of probe spacing is 0.5 pixels, and the upper limit of probe spacing is 4.0 pixels.</p>
  <p num="p-0119">A probe generation step <b>370</b> creates probes evenly spaced along the segments found by step <b>350</b>, and stores them in the model <b>120</b>.</p>
  <p num="p-0120">A contrast step <b>380</b> determines the contrast of the pattern <b>105</b> in the training image <b>100</b> by using the run-time step <b>140</b> to obtain a result corresponding to the pattern <b>105</b>, and extracting the contrast value from said result. Contrast is defined below.</p>
  <p num="p-0121"> <figref idrefs="DRAWINGS">FIG. 4</figref> shows an example of a portion of a boundary point list <b>270</b> for a small subset of a training image <b>100</b> as might be produced by step <b>310</b>. The boundary points are shown superimposed on a grid <b>410</b>, which is a portion of the pixel grid of the image input to gradient estimation step <b>230</b> from which the boundary points were extracted. In a preferred embodiment using the steps of <figref idrefs="DRAWINGS">FIG. 2</figref>, as further described in U.S. Pat. No. 6,408,109, entitled Apparatus and Method for Detecting and Sub-Pixel Location of Edges in a Digital Image, herein incorporated by reference, no more than one boundary point will fall within any given grid element, and there will be no gaps in the boundary due to grid quantization effects. For example, the boundary point <b>400</b> falls within grid element <b>420</b>, shaded gray in <figref idrefs="DRAWINGS">FIG. 4</figref>. The boundary point <b>400</b> has gradient direction and magnitude shown by vector <b>440</b>. Also shown is a small straight-line section of pattern boundary <b>460</b> corresponding to the example boundary point <b>400</b> and normal to the gradient direction <b>440</b>. This section of boundary is shown primarily to aid in understanding the figure. Its orientation and position along the gradient direction are significant, but its length is essentially arbitrary.</p>
  <p num="p-0122"> <figref idrefs="DRAWINGS">FIG. 5</figref> shows details of the connect step <b>320</b> of the training step <b>110</b>. <figref idrefs="DRAWINGS">FIG. 5</figref> <i>a </i>shows the same grid <b>410</b>, the same example boundary points including example <b>400</b> with gradient <b>440</b>, and the same example grid element <b>420</b>, shaded light gray, as was shown in <figref idrefs="DRAWINGS">FIG. 4</figref>.</p>
  <p num="p-0123">For every boundary point, the grid <b>410</b> is examined to identify neighboring grid elements that contain boundary points to which the boundary point should be connected. For the example boundary point <b>400</b> in grid element <b>420</b>, the neighboring grid elements <b>500</b> are shown, shaded medium gray. The neighboring grid elements <b>500</b> are examined in two steps of four neighboring grid elements each, each step in a particular order, determined by the gradient direction <b>440</b> of the boundary point <b>400</b> corresponding to grid element <b>420</b>.</p>
  <p num="p-0124">In one step a left neighbor grid element <b>510</b> is identified, and a left link <b>515</b> is associated with the boundary point <b>400</b> identifying the boundary point <b>517</b> contained by grid element <b>510</b> as its left neighbor. In the other step a right neighbor grid element <b>520</b> is identified, and a right link <b>525</b> is associated with the boundary point <b>400</b> identifying the boundary point <b>527</b> contained by grid element <b>520</b> as its right neighbor. If a given neighbor cannot be found, a null link is associated. Note that left and right are defined arbitrarily but consistently by an imaginary observer looking along the gradient direction.</p>
  <p num="p-0125"> <figref idrefs="DRAWINGS">FIG. 5</figref> <i>b </i>shows the order in which neighboring grid elements are examined for a boundary point whose gradient direction falls between arrows <b>540</b> and <b>542</b>, corresponding to a boundary tangent that falls between dotted lines <b>544</b> and <b>546</b>. The sequence for identifying the left neighbor is +1, +2, +3, and +4. The first neighbor in said sequence that contains a boundary point, if any, is the left neighbor. Similarly, the sequence for identifying the right neighbor is 1, 2, 3, and 4.</p>
  <p num="p-0126"> <figref idrefs="DRAWINGS">FIG. 5</figref> <i>c </i>shows another example, where the gradient direction falls between arrows <b>560</b> and <b>562</b>, corresponding to a boundary tangent that falls between dotted lines <b>564</b> and <b>566</b>. The sequences of neighbors are as shown. The sequences for all other gradient directions are simply rotations of the two cases of <figref idrefs="DRAWINGS">FIGS. 5</figref> <i>b </i>and <b>5</b> <i>c. </i> </p>
  <p num="p-0127">Note that the sequences given in <figref idrefs="DRAWINGS">FIGS. 5</figref> <i>b </i>and <b>5</b> <i>c </i>show a preference for orthogonal neighbors over diagonal neighbors, even when diagonal neighbors are closer to the direction of the boundary tangent. This preference insures that the chains will properly follow a stair-step pattern for boundaries not aligned with the grid axes. Clearly this preference is somewhat dependent on the specific details of how the boundary point detection step <b>310</b> chooses points along the boundary.</p>
  <p num="p-0128">Once left and right links have been associated with all boundary points (some of said links may be null), a consistency check is performed. Specifically, the right neighbor of a boundary point's left neighbor should be the boundary point itself, and the left neighbor of a boundary point's right neighbor should also be the boundary point itself. If any links are found for which these conditions do not hold, those links are broken by replacing them with a null link. At the end of the connect step <b>320</b>, only consistent links remain.</p>
  <p num="p-0129">Many alternate methods can be used to establish boundaries within the spirit of the invention.</p>
  <p num="p-0130"> <figref idrefs="DRAWINGS">FIG. 6</figref> shows details of the segment step <b>350</b>, the probe spacing step <b>360</b>, and the probe generation step <b>370</b>. In the figure, boundary points <b>600</b>, drawn as diamonds within shaded grid elements, have been marked by segment step <b>350</b> as belonging to corners, and boundary points <b>620</b>, <b>622</b>, <b>624</b>, <b>626</b>, and <b>640</b>, drawn as circles within unshaded grid elements, have been marked as belonging to segments. The boundary points <b>620</b>, <b>622</b>, <b>624</b>, and <b>626</b> belong to one segment to be explained in more detail, and the boundary points <b>640</b> belong to another segment and are shown for illustration and not further discussed. All of the boundary points of <figref idrefs="DRAWINGS">FIG. 6</figref> are connected by left and right links as shown, for example <b>660</b>, and form a portion of one chain.</p>
  <p num="p-0131">Segment step <b>350</b> also determines an arc position for each boundary point along a segment, starting with 0 at the left end of the segment and increasing to the right by an amount equal to the distance between the boundary points along the segment. For example, boundary point <b>620</b> is at the left end of a segment and is at arc position 0.00 as shown. The right neighbor <b>622</b> of boundary point <b>620</b> is 1.10 pixels distant and is therefore at arc position 1.10. Similarly the right neighbor <b>624</b> of boundary point <b>622</b> is 1.15 pixels distant and is therefore at arc position 2.25. Finally, the right-most boundary point <b>626</b> along this segment is at arc position 3.20. The total arc length of a segment is defined to be the arc position of the right-most boundary point, in this example 3.20 pixels. Note that the distance between boundary points along a chain can be substantially larger or smaller than 1 pixel, particularly along diagonal boundaries where the boundary points tend to follow a stair-step pattern. By approximating true arc distance along the chain as described above, instead of considering the boundary points to be evenly spaced, grid quantization effects are substantially reduced.</p>
  <p num="p-0132">Probe spacing step <b>360</b> determines a spacing value that would result in a predetermined target number of probes evenly distributed among the segments. The number of probes n that can be fit along a segment of arc length l at a probe spacing of s is given by the formula: 
<maths id="MATH-US-00002" num="00002"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mi>n</mi> <mo>=</mo> <mrow> <mrow> <mi>floor</mi> <mo></mo> <mrow> <mo>(</mo> <mfrac> <mi>l</mi> <mi>s</mi> </mfrac> <mo>)</mo> </mrow> </mrow> <mo>+</mo> <mn>1</mn> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>3</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
  <p num="p-0133">In the example of <figref idrefs="DRAWINGS">FIG. 6</figref> where the arc length is 3.2, one probe can fit if the spacing is greater than 3.2, two probes can fit if the spacing is greater than 1.6 but not greater than 3.2, and so on. Fewer than the target number of probes will be used to keep the spacing from falling below a predetermined lower limit, and more than the target number of probes will be used to keep the spacing from exceeding a predetermined upper limit.</p>
  <p num="p-0134">Once a probe spacing value has been chosen by probe spacing step <b>360</b>, the set of probes to be included in the model <b>120</b> are generated by probe generation step <b>370</b>. In the example of <figref idrefs="DRAWINGS">FIG. 6</figref>, a probe spacing of <b>2</b>.<b>5</b> has been chosen, and applying equation 3 shows that two probes will fit along the example segment. The two probes are centered along the segment, so that the first probe location <b>680</b> is at arc position 0.35, and the second probe location <b>685</b> is at arc position 2.85. The position and direction of each probe are interpolated between the surrounding boundary points. In the example of <figref idrefs="DRAWINGS">FIG. 6</figref>, the first probe at arc position 0.35 is placed along the line segment connecting boundary points <b>620</b> and <b>622</b> and at a distance of 0.35 pixels from boundary point <b>620</b>. The gradient direction of the first probe is proportionally 0.35/1.10 between the directions of boundary points <b>620</b> and <b>622</b>. Similarly, the second probe at arc position 2.85 is placed along the line segment connecting boundary points <b>624</b> and <b>626</b> and at a distance of 0.60 pixels from boundary point <b>624</b>. The gradient direction of the second probe is proportionally 0.60/0.95 between the directions of boundary points <b>624</b> and <b>626</b>.</p>
  <p num="p-0135">Probe positions and directions may be represented in any convenient coordinate system, which is referred to in this specification as pattern coordinates.</p>
  <p num="p-0136"> <figref idrefs="DRAWINGS">FIG. 7</figref> shows the set of probes to be included in model <b>120</b>, resulting from training step <b>110</b> shown in <figref idrefs="DRAWINGS">FIG. 1</figref> and detailed in <figref idrefs="DRAWINGS">FIG. 3</figref>, for the pattern <b>105</b> of <figref idrefs="DRAWINGS">FIG. 1</figref>. The pattern <b>105</b> consists of 2 boundaries, the outside circular boundary <b>700</b> and the inside cross-shaped boundary <b>720</b>. The boundary <b>700</b> results in one closed chain of boundary points containing one segment and no corners, since the curvature of the boundary at each point is sufficiently low 32 probes, for example 710, are distributed evenly along boundary <b>700</b>. The boundary <b>720</b> results in one closed chain of boundary points containing 12 segments and 12 corners 32 probes, for example 730, are distributed evenly among the 12 segments of boundary <b>720</b>. All probes in <figref idrefs="DRAWINGS">FIG. 7</figref> have weight 1.0.</p>
  <p num="p-0137">In some applications it is desirable to create a model <b>120</b> based on a geometric description of a known shape, rather than by example from a training image <b>100</b>. This is referred to as synthetic training, and can be performed by a human designer or suitable software working from a geometric description such as CAD data. One advantage of synthetic training is that the model can be made essentially perfect, whereas any specific example of an object appearing in a training image may have defects and since the process of extracting boundary information, even from a high quality example, is subject to noise, grid quantization effects, and other undesirable artifacts. Another advantage of synthetic training by human designers is that the model designer can use application-specific knowledge and judgment to design the probes, including use of variable weights and negative weights, to achieve higher detection reliability.</p>
  <p num="p-0138"> <figref idrefs="DRAWINGS">FIG. 8</figref> shows a human-designed model for locating or inspecting a rounded rectangle pattern using the invention. An ideal rounded rectangle <b>800</b> to be synthetically trained has boundary <b>805</b>. 36 probes are placed along the boundary <b>805</b> as shown. 12 probes including examples <b>812</b> and <b>816</b> are placed along the top of the boundary <b>805</b>, and 12 probes including examples <b>810</b> and <b>814</b> are placed along the bottom of the boundary <b>805</b>. Each of these 24 probes placed along the top and bottom have weight 1.0, shown by the relative lengths of the probes. 20 of these 24 probes, including examples <b>810</b> and <b>812</b>, point straight up or down, and the other 4, including examples <b>814</b> and <b>816</b>, are rotated slightly due to the expected rounding of the corners.</p>
  <p num="p-0139">6 probes including examples <b>822</b> and <b>826</b> are placed along the left of the boundary <b>805</b>, and 6 probes including examples <b>820</b> and <b>824</b> are placed along the right of the boundary <b>805</b>. Each of these 12 probes placed along the left and right have weight 2.0, shown by the relative lengths of the probes. 8 of these 12 probes, including examples <b>820</b> and <b>822</b>, point straight left or right, and the other 4, including examples <b>824</b> and <b>826</b>, are rotated slightly due to the expected rounding of the corners.</p>
  <p num="p-0140">The use of weight 1.0 for the top and bottom, and weight 2.0 for the left and right, makes the total influence of each edge the same. The same effect could be achieved by adding 6 more probes each to the left and right sides, packing them together more densely, but this would result in 33% more probes to be processed for what may be little or no improvement in the quality of information extracted from the image. The use of variable positive weights allows the designer the flexibility to make that tradeoff.</p>
  <p num="p-0141">It can be seen that any sufficiently long straight boundary in the run-time image will match almost perfectly any of the four sides of the model, even though such a straight boundary is not part of a rectangle. Similarly, any sufficiently long right-angle boundary in the run-time image will match almost perfectly fully one-half of the model, even though such a boundary is not part of a rectangle. This matching between the model and image features that are not part of the pattern to be located, which is substantial in this case due to the geometric simplicity of the pattern, significantly reduces the ability of the model to discriminate between true instances of the pattern and other image features. The problem is made more severe if the application calls for the model to be used over a wide range of angles. The problem can be mitigated by the use of negative weight probes.</p>
  <p num="p-0142">In <figref idrefs="DRAWINGS">FIG. 8</figref> imaginary lines <b>830</b>, <b>832</b>, <b>834</b>, and <b>836</b> are drawn extending from the edges of the rounded rectangle <b>800</b>. Probes with weights of 2.0, drawn with dashed lines and open arrow heads, and including examples <b>840</b>, <b>842</b>, <b>844</b>, and <b>846</b>, are placed along the imaginary lines as shown. Example negative weight probes <b>840</b>, <b>842</b>, <b>844</b>, and <b>846</b> are placed along imaginary line <b>830</b>, <b>832</b>, <b>834</b>, and <b>836</b> respectively. With this arrangement, any match function used by the invention that considers negative weight probes, such as the second and third match functions described in the summary section, will score approximately 0 against a sufficiently long straight boundary, and approximately 0.25 instead of 0.5 against a sufficiently long right-angle boundary.</p>
  <p num="p-0143">Generalizing this example, one can see that a suitable arrangement of positive probes surrounded by negative probes along a line can be used to discriminate line segments from lines and rays. Furthermore, a suitable arrangement of positive probes bounded by negative probes on one side, along a line, can be used to discriminate rays from lines.</p>
  <p num="p-0144">In the following paragraphs and associated figures describing in detail run-time step <b>140</b>, certain typographic and symbolic conventions are used that are set forth in <figref idrefs="DRAWINGS">FIG. 9</figref>. Descriptions of named sets of data are presented in tabular format <b>900</b>. A named set of data is analogous to a record, structure, or class as used by many conventional programming languages familiar to those skilled in the art. Such a set consists of a collection of named elements of various types. One or more instances of any given set may be used in a specific embodiment of the invention, as appropriate. A specific instance of a named set is called an object, in accord with the conventions of object-oriented programming, which usage can be distinguished by context from physical or simulated objects to be located by the invention. In the example table <b>900</b>, the name of the set is given in box <b>905</b> as shown, and each element of the set occupies a row of the table <b>900</b>. In each row the name of the element is given in column <b>910</b>, the type of the element in column <b>915</b>, and a descriptive summary of the element in column <b>920</b>. Element names generally follow the conventions of common high-level programming languages for lexical program elements commonly referred to as identifiers. When an element's type is given as a real number, or as containing real numbers, the element is assumed to take on non-integral values and be represented in any specific embodiment by a fixed or floating point number of suitable but unspecified precision. When an element's type is given as a function of certain argument types and producing a certain value type, the element is assumed to be a function pointer or virtual member function as defined by common programming languages such as C and C++. The reader should understand that these descriptions of implementation are intended to be descriptive and not limiting, and that many suitable alternative implementations can be found within the spirit of the invention.</p>
  <p num="p-0145">When flowcharts are used to express a sequence of steps, certain conventions are followed. A rectangle, for example <b>930</b>, indicates an action be performed. A diamond, for example <b>940</b>, indicates a decision to be made and will have two labeled arrows to be followed depending on the outcome of the decision.</p>
  <p num="p-0146">A right-pointing pentagon, for example <b>950</b>, indicates a that a sequence of loop steps are to be executed for each of a sequence of values of some variable. The loop steps are found by following an arrow, e.g. <b>952</b>, extending from the point of the pentagon, and when the loop steps have been executed for the entire sequence of values, flow is to continue by following an arrow, e.g. <b>954</b>, extending from the bottom of the pentagon.</p>
  <p num="p-0147">An oval, for example <b>960</b>, indicates a termination of a sequence of loop steps or a termination of a procedure. A down-pointing pentagon, for example <b>970</b>, indicates a connection to a like-named pentagon on another figure.</p>
  <p num="p-0148">At times a description in a flow chart or other text can be made more clearly and concisely in a form similar to that used in conventional programming languages. In these cases a pseudo-code will be used, which while not following precisely the syntax of any specific language will nevertheless be easily understood by anyone skilled in the art. Pseudo-code is always written in the bold, fixed-spaced font illustrated in rectangle <b>930</b>. Pseudo-code is generally used for variables, references to elements of objects, simple arithmetic expressions, and procedure calls. Often the syntactic detail of programming languages obscures rather than reveals the workings of the invention, and in these cases English descriptions will be used instead of pseudo-code, for example as shown in diamond <b>940</b>. In such cases implementation details can be filled in by those skilled in the art. In still other cases, particularly for vector and matrix operations, greatest clarity can be achieved using standard mathematical notation, which can represent compactly many arithmetic operations; the use of detailed pseudo-code or English would only obscure the intent.</p>
  <p num="p-0149"> <figref idrefs="DRAWINGS">FIG. 10</figref> shows a data set that represents a model <b>120</b>. Element probes <b>1000</b> is a list of probes created by training step <b>370</b>. Element granularity <b>1010</b> is the granularity value g chosen during training step <b>300</b> and used during training step <b>310</b> to obtain the boundary points. Element contrast <b>1020</b> is the pattern contrast measured during training step <b>380</b>.</p>
  <p num="p-0150"> <figref idrefs="DRAWINGS">FIG. 11</figref> <i>a </i>shows a data set <b>1190</b> that represents a probe. Element position <b>1100</b> is a 2-vector that specifies the position of the probe in pattern coordinates. Element direction <b>1110</b> specifies the gradient direction expected by the probe, again relative to the probe coordinate system. In a preferred embodiment, a binary angle is used to represent direction to simplify angle arithmetic using well-known methods. Element weight <b>1120</b> specifies the weight assigned to the probe, which can be positive or negative. In a preferred embodiment, zero weight probes are not used.</p>
  <p num="p-0151"> <figref idrefs="DRAWINGS">FIG. 11</figref> <i>b </i>shows a data set <b>1195</b> that represents a compiled probe. A list of compiled probes is generated from the probe list <b>1000</b> stored in model <b>120</b> based on a specific combination of generalized-DOF parameters that specify an overall coordinate transform representing the non-translation degrees of freedom. Element offset <b>1130</b> is the pixel address offset of the probe in the gradient magnitude image <b>242</b> and the gradient direction image <b>244</b> (i.e. the same offset applies to both images). Element direction <b>1140</b> is the expected gradient direction, mapped to image coordinates. Element weight <b>1150</b> is the probe weight, copied from the corresponding probe object and in some embodiments converted from a real to a scaled integer to take advantage of integer multiply hardware.</p>
  <p num="p-0152"> <figref idrefs="DRAWINGS">FIGS. 12</figref> <i>a </i>and <b>12</b> <i>b </i>gives details of a function compileProbes <b>1200</b> that converts the list of probe objects <b>1000</b> stored in model <b>120</b> to a list of compiled-probe objects <b>1195</b>. Starting on <figref idrefs="DRAWINGS">FIG. 12</figref> <i>a</i>, function compileProbes <b>1200</b> takes map input <b>1202</b> and probeMER input <b>1204</b> as shown, and returns a list of compiled-probe objects. Note that probeMER input <b>1204</b> is a reference to a rectangle to be set by compileProbes, and so may be considered an output or side-effect, but is listed as an input following the usual programming convention that a function can return only one value or object.</p>
  <p num="p-0153">Step <b>1205</b> and step <b>1210</b> perform initialization as shown. Loop step <b>1215</b> specifies a sequence of steps for each probe object in the list of probe objects <b>1000</b> stored in model <b>120</b>. Step <b>1220</b> sets up a new compiled-probe object. Step <b>1225</b> copies the weight <b>1150</b> from the corresponding probe object, and in some embodiments may convert it to a format more suited to efficient processing on the available hardware. Continuing to <figref idrefs="DRAWINGS">FIG. 12</figref> <i>b</i>, in step <b>1230</b>, some definitions are made for the subsequent math. In step <b>1235</b>, the probe position <b>1100</b> is mapped to image coordinates using the input coordinate transform <b>1202</b> and rounded to an integer image pixel offset in x (e.g., horizontal) and y (e.g., vertical), from which is computed an offset <b>1130</b> based on the address difference of pixels in the gradient magnitude image <b>242</b> and the gradient direction image <b>244</b>. The use of a single offset value <b>1130</b>, instead of the (x, y) pair, allows higher speed access to gradient magnitude or direction as the set of compiled probes are translated around the images.</p>
  <p num="p-0154">In step <b>1240</b> the expected gradient direction <b>1110</b> is mapped to an image coordinate relative value <b>1140</b>. The formula for the mapped direction <b>1140</b> effectively does the following, reading the vector and matrix operations <b>1242</b> right to left:
</p> <ul> <li id="ul0003-0001" num="0000"> <ul> <li id="ul0004-0001" num="0160">Construct a unit vector <b>1270</b> in the gradient direction, with respect to pattern coordinates, by computing the cosine and sine of the angle.</li> <li id="ul0004-0002" num="0161">Rotate the unit vector 90 <b>1272</b> to get a direction along the boundary that contains the probe.</li> <li id="ul0004-0003" num="0162">Map the rotated unit vector to image coordinates <b>1274</b> to get a boundary direction in image coordinates.</li> <li id="ul0004-0004" num="0163">Rotate the mapped rotated unit vector 90 <b>1276</b> to get a direction normal to the boundary in image coordinates.</li> <li id="ul0004-0005" num="0164">If the determinant of the transform matrix is negative, the transform changes the left-handedness or right-handedness of the coordinate system, so rotate the vector 180 <b>1278</b> because the 90 of the previous step should have been +90.</li> <li id="ul0004-0006" num="0165">Compute the angle of the resulting vector <b>1280</b> using the well-known version of the arctangent function of two arguments whose result is in the range 0 to 360.</li> </ul> </li> </ul> <p num="p-0155">Note that in computing mapped expected gradient direction <b>1140</b> the boundary direction is mapped instead of the gradient direction. This is necessary to handle the general case where the transform matrix C is not orthonormal. If C is orthonormal, i.e. if C<sub>11</sub>=C<sub>22 </sub>and C<sub>12</sub>=C<sub>21</sub>, then step <b>1240</b> can be replaced with a step that simply adds the constant arctan(C<sub>21</sub>/C<sub>11</sub>) to the probe direction <b>1110</b>.</p>
  <p num="p-0156">Note as shown in step <b>1240</b> that these calculations can be simplified considerably <b>1290</b>. In a preferred embodiment the arctangent function is computed using the well-known CORDIC method.</p>
  <p num="p-0157">Step <b>1245</b> keeps track of the minimum enclosing rectangle in integer pixels of the mapped probes, for subsequent use by the invention to detect duplicate results by determining the extent to which pairs of results overlap. Note that the size and shape of the minimum enclosing rectangle will vary depending on the settings of the generalized-DOF parameters, and so must be recomputed for each such setting.</p>
  <p num="p-0158">Step <b>1250</b> marks the end of the loop steps; control flows back to step <b>1215</b> on <figref idrefs="DRAWINGS">FIG. 12</figref> <i>a </i>to continue with the next probe. If there are no more probes, control flows to step <b>1255</b>, which returns the list of compiled-probe objects to the caller of the compileProbes function <b>1200</b>.</p>
  <p num="p-0159"> <figref idrefs="DRAWINGS">FIG. 13</figref> gives details for the match functions used by the invention. <figref idrefs="DRAWINGS">FIGS. 13</figref> <i>a </i>and <b>13</b> <i>b </i>show examples of direction rating factor functions. A direction rating factor is value between 0 and 1 that indicates degree of match between a probe's expected gradient direction <b>1140</b> and the actual gradient direction found in a gradient direction image <b>244</b> under the probe. A direction rating factor function produces a direction rating factor as a function of direction error, defined as an angle measured from the expected gradient direction to the actual gradient direction. Any of a variety of direction rating factor functions could in principal be used to practice the invention.</p>
  <p num="p-0160">There are two general types of direction rating factor functions, called consider polarity and ignore polarity functions. The difference between the two types is in how they handle direction errors at and around 180, which corresponds to a gradient direction opposite from what was expected, implying that the boundary is in the expected orientation but the dark-to-light transition of image brightness across the boundary is opposite in polarity from expected. The consider polarity functions return 0 at and around 180, so that polarity reversals do not match the pattern, while the ignore polarity functions rreturn 1 at and around 180, so that polarity reversals do match the pattern. Choice between consider polarity and ignore polarity is application dependent, and so in a preferred embodiment, the user can select either type.</p>
  <p num="p-0161">In a preferred embodiment the consider polarity direction rating factor function of <figref idrefs="DRAWINGS">FIG. 13</figref> <i>a </i>is used. The function is at 1 from 0 <b>1300</b> to 11.25 <b>1302</b>, then falls in a straight line to 0 at 22.5 <b>1304</b>, remains at 0 until 337.5 <b>1306</b>, rises in a straight line to 1 at 348.75 <b>1308</b>, and remains at <b>1</b> until 360 <b>1310</b> (which is the same as the 0 point <b>1300</b>). In a preferred embodiment the corresponding ignore polarity direction rating factor function of <figref idrefs="DRAWINGS">FIG. 13</figref> <i>b </i>is used. The points <b>1320</b>, <b>1322</b>, <b>1324</b>, <b>1326</b>, <b>1328</b>, and <b>1330</b> correspond exactly to the points <b>1300</b>, <b>1302</b>, <b>1304</b>, <b>1306</b>, <b>1308</b>, and <b>1310</b>, respectively, of <figref idrefs="DRAWINGS">FIG. 13</figref> <i>a</i>. The points <b>1332</b>, <b>1334</b>, <b>1336</b>, and <b>1338</b> correspond to points <b>1328</b>, <b>1322</b>, <b>1324</b>, and <b>1326</b>, respectively, but shifted 180. Note that the points <b>1320</b> and <b>1330</b> have no corresponding points shifted 180, since these points are an artifact of the decision to start the drawing at 0.</p>
  <p num="p-0162"> <figref idrefs="DRAWINGS">FIG. 13</figref> <i>c </i>shows an example of a magnitude rating factor function. A magnitude rating factor is a value between 0 and 1 that indicates a degree of confidence that a particular pixel position lies along a boundary and therefore that a probe test made at said position would result in reliable evidence for, in the case of positive weight probes, or against, in the case of negative weight probes, the existence of an instance of the trained pattern <b>105</b> at the pose under test. A magnitude rating factor function produces a magnitude rating factor as a function of gradient magnitude. Any of a variety of magnitude rating factor functions could in principal be used to practice the invention.</p>
  <p num="p-0163">In a preferred embodiment the magnitude rating factor function of <figref idrefs="DRAWINGS">FIG. 13</figref> <i>c </i>is used. The function is 0 at magnitude 0 <b>1350</b>, rises in a straight line to 1 at a point <b>1352</b> corresponding to a certain target magnitude further described below, and continues at 1 until the maximum magnitude <b>1354</b>, which in the illustrated embodiment is <b>255</b>.</p>
  <p num="p-0164">The goal in the design of the example magnitude rating factor function of <figref idrefs="DRAWINGS">FIG. 13</figref> <i>c </i>is primarily to distinguish between noise and true boundaries. The intention is that an embodiment of the invention using this magnitude rating factor function be sensitive to the shape of boundaries but not overly sensitive to the contrast and sharpness of the boundaries. A separate target magnitude point <b>1352</b> is computed for each distinct pose (placement of probes), since global decisions about what is noise and what is signal are notoriously unreliable. If we consider only the set B of positive weight probes with a high direction rating factor, it is reasonable to assume that a majority of probes in B lie along a true boundary and not noise. The median gradient magnitude m<sub>median </sub>under the probes in B is a good guess as to a representative gradient magnitude value corresponding to the boundary. In a preferred embodiment we choose target point <b>1352</b> to have the value 0.7. m<sub>median</sub>.</p>
  <p num="p-0165">In the following let:
</p> <ul> <li id="ul0005-0001" num="0000"> <ul> <li id="ul0006-0001" num="0177">P<sub>i </sub>be the offset <b>1130</b> of the i<sup>th </sup>compiled probe <b>1195</b>;</li> <li id="ul0006-0002" num="0178">d<sub>i </sub>be the direction <b>1140</b> of the i<sup>th </sup>compiled probe <b>1195</b>;</li> <li id="ul0006-0003" num="0179">W<sub>i </sub>be the weight <b>1150</b> of the i<sup>th </sup>compiled probe <b>1195</b>;</li> <li id="ul0006-0004" num="0180">M(a) be the gradient magnitude at offset a in gradient magnitude image <b>242</b>;</li> <li id="ul0006-0005" num="0181">D(a) be the gradient direction at offset a in gradient magnitude image <b>244</b>;</li> <li id="ul0006-0006" num="0182">R<sub>dir</sub>( ) be a direction rating factor function, for example the one in <figref idrefs="DRAWINGS">FIG. 13</figref> <i>a </i>or <figref idrefs="DRAWINGS">FIG. 13</figref> <i>b</i>; and</li> <li id="ul0006-0007" num="0183">R<sub>mag</sub>( ) be a magnitude rating factor function, for example the one in <figref idrefs="DRAWINGS">FIG. 13</figref> <i>c. </i> </li> </ul> </li> </ul> <p num="p-0166">With these definitions, it can be seen that for a set of compiled probes placed at offset a in gradient magnitude image <b>242</b> or gradient direction image <b>244</b>,
</p> <ul> <li id="ul0007-0001" num="0000"> <ul> <li id="ul0008-0001" num="0185">M(a+p<sub>i</sub>) is the gradient magnitude under compiled probe i</li> <li id="ul0008-0002" num="0186">D(a+p<sub>i</sub>) is the gradient direction under compiled probe i</li> <li id="ul0008-0003" num="0187">D(a+p<sub>i</sub>)d<sub>i </sub>is the direction error at compiled probe i</li> </ul> </li> </ul> <p num="p-0167">In the following equations, a term of the form x=y or x&gt;y is 1 if the expression is true and 0 otherwise, following the conventions of the C programming language. This is not standard algebraic notation, but tends to simplify and clarify the formulas.</p>
  <p num="p-0168">To avoid having to set a threshold to decide whether or not a probe is a member of B, a probe's direction rating factor is used to specify a weighted membership value. For a specific set of compiled probes (i.e. corresponding to specific settings of the generalized-DOF parameters) a weighted histogram H<sub>a</sub>(m) of gradient magnitude for complied probes placed at image offset a is computed as follows: 
<maths id="MATH-US-00003" num="00003"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <msub> <mi>H</mi> <mi>a</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mi>m</mi> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo></mo> <mrow> <mrow> <mi>max</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo>,</mo> <mn>0</mn> </mrow> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <mi>M</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>a</mi> <mo>+</mo> <msub> <mi>p</mi> <mi>i</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mi>m</mi> </mrow> <mo>)</mo> </mrow> <mo></mo> <mrow> <msub> <mi>R</mi> <mi>dir</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mo></mo> <mrow> <mrow> <mi>D</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>a</mi> <mo>+</mo> <msub> <mi>p</mi> <mi>i</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> <msub> <mi>d</mi> <mi>i</mi> </msub> </mrow> <mo></mo> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mrow> <mn>4</mn> <mo></mo> <mi>a</mi> </mrow> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
  <p num="p-0169">Equation 4a states that each bin m of the histogram H is the sum of direction rating factors, weighted by probe weight w<sub>i</sub>, for all positive weight probes where the gradient magnitude under the probe is m. From histogram H can be computed the median gradient magnitude by finding the value M<sub>median </sub>such that: 
<maths id="MATH-US-00004" num="00004"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <munderover> <mo></mo> <mrow> <mi>i</mi> <mo>=</mo> <mn>0</mn> </mrow> <msub> <mi>m</mi> <mi>median</mi> </msub> </munderover> <mo></mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo></mo> <mrow> <msub> <mi>H</mi> <mi>a</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mi>i</mi> <mo>)</mo> </mrow> </mrow> </mrow> <mo>=</mo> <mrow> <munderover> <mo></mo> <mrow> <mi>i</mi> <mo>=</mo> <msub> <mi>m</mi> <mi>median</mi> </msub> </mrow> <mn>255</mn> </munderover> <mo></mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo></mo> <mrow> <msub> <mi>H</mi> <mi>a</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mi>i</mi> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mrow> <mn>4</mn> <mo></mo> <mi>b</mi> </mrow> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
  <p num="p-0170">The first match function S<sub>1a</sub>, used in the coarse scan step <b>1925</b> of <figref idrefs="DRAWINGS">FIG. 19</figref>, can now be defined as follows: 
<maths id="MATH-US-00005" num="00005"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <msub> <mi>S</mi> <mrow> <mn>1</mn> <mo></mo> <mi>a</mi> </mrow> </msub> <mo></mo> <mrow> <mo>(</mo> <mi>a</mi> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mfrac> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo></mo> <mrow> <mrow> <mi>max</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo>,</mo> <mn>0</mn> </mrow> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <msub> <mi>R</mi> <mi>dir</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mo></mo> <mrow> <mrow> <mi>D</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>a</mi> <mo>+</mo> <msub> <mi>p</mi> <mi>i</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> <msub> <mi>d</mi> <mi>i</mi> </msub> </mrow> <mo></mo> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo></mo> <mrow> <mi>max</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo>,</mo> <mn>0</mn> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mfrac> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mrow> <mn>5</mn> <mo></mo> <mi>a</mi> </mrow> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
  <p num="p-0171">This gives the first match score at any offset a of the compiled probes. As can be seen from equation 5a, only positive weight probes are used. The first variation S<sub>1b</sub>, which, to achieve higher execution speed doesn't use probe weight (except to select positive weight probes) is: 
<maths id="MATH-US-00006" num="00006"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <msub> <mi>S</mi> <mrow> <mn>1</mn> <mo></mo> <mi>b</mi> </mrow> </msub> <mo></mo> <mrow> <mo>(</mo> <mi>a</mi> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mfrac> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <mrow> <mo>(</mo> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo>&gt;</mo> <mn>0</mn> </mrow> <mo>)</mo> </mrow> <mo></mo> <mrow> <msub> <mi>R</mi> <mi>dir</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mo></mo> <mrow> <mrow> <mi>D</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>a</mi> <mo>+</mo> <msub> <mi>p</mi> <mi>i</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> <msub> <mi>d</mi> <mi>i</mi> </msub> </mrow> <mo></mo> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo>&gt;</mo> <mn>0</mn> </mrow> <mo>)</mo> </mrow> </mrow> </mfrac> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mrow> <mn>5</mn> <mo></mo> <mi>b</mi> </mrow> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
  <p num="p-0172">The first match function using the first and second variations S<sub>1</sub>, which subtracts the expected value of the direction rating factor on random noise, and is used in a preferred embodiment, is: 
<maths id="MATH-US-00007" num="00007"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <msub> <mi>S</mi> <mn>1</mn> </msub> <mo></mo> <mrow> <mo>(</mo> <mi>a</mi> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mfrac> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <mrow> <mo>(</mo> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo>&gt;</mo> <mn>0</mn> </mrow> <mo>)</mo> </mrow> <mo></mo> <mrow> <mo>[</mo> <mrow> <mrow> <msub> <mi>R</mi> <mi>dir</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mo></mo> <mrow> <mrow> <mi>D</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>a</mi> <mo>+</mo> <msub> <mi>p</mi> <mi>i</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> <msub> <mi>d</mi> <mi>i</mi> </msub> </mrow> <mo></mo> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> <mi>N</mi> </mrow> <mo>]</mo> </mrow> </mrow> </mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mn>1</mn> <mo>-</mo> <mi>N</mi> </mrow> <mo>)</mo> </mrow> <mo></mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo>&gt;</mo> <mn>0</mn> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mfrac> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mrow> <mn>5</mn> <mo></mo> <mi>c</mi> </mrow> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p> <ul> <li id="ul0009-0001" num="0000"> <ul> <li id="ul0010-0001" num="0194">where noise term N is given by: 
<maths id="MATH-US-00008" num="00008"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mi>N</mi> <mo>=</mo> <mrow> <mfrac> <mn>1</mn> <mn>360</mn> </mfrac> <mo></mo> <mrow> <msubsup> <mo></mo> <mn>0</mn> <mn>360</mn> </msubsup> <mo></mo> <mrow> <mrow> <msub> <mi>R</mi> <mi>dir</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mi></mi> <mo>)</mo> </mrow> </mrow> <mo></mo> <mstyle> <mspace width="0.2em" height="0.2ex"> </mspace> </mstyle> <mo></mo> <mrow> <mo></mo> <mi></mi> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>6</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </li> </ul> </li> </ul> <p num="p-0173">Using the noise term in the first match function is important because, due to the higher computational cost, gradient magnitude is not used to filter out noise. Note that the computation of S<sub>1 </sub>as specified by equation 5c can be arranged so that the use of N adds no per-probe cost. For the preferred consider polarity direction rating factor function of <figref idrefs="DRAWINGS">FIG. 13</figref> <i>a</i>, N= 3/32. For the preferred ignore polarity direction rating factor function of <figref idrefs="DRAWINGS">FIG. 13</figref> <i>b</i>, N= 3/16.</p>
  <p num="p-0174">The second match function S<sub>2 </sub>used in the fine scan step <b>1940</b> of <figref idrefs="DRAWINGS">FIG. 19</figref> is: 
<maths id="MATH-US-00009" num="00009"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <msub> <mi>S</mi> <mn>2</mn> </msub> <mo></mo> <mrow> <mo>(</mo> <mi>a</mi> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <mi>M</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>a</mi> <mo>+</mo> <msub> <mi>p</mi> <mi>i</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <msub> <mi>R</mi> <mi>dir</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mo></mo> <mrow> <mrow> <mi>D</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>a</mi> <mo>+</mo> <msub> <mi>p</mi> <mi>i</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> <msub> <mi>d</mi> <mi>i</mi> </msub> </mrow> <mo></mo> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>7</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
  <p num="p-0175">The third match function S<sub>3 </sub>used in the scoring steps <b>1930</b> <b>1945</b> <b>1930</b> and <b>1945</b> is: 
<maths id="MATH-US-00010" num="00010"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <msub> <mi>S</mi> <mn>3</mn> </msub> <mo></mo> <mrow> <mo>(</mo> <mi>a</mi> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mfrac> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <msub> <mi>R</mi> <mi>mag</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>M</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>a</mi> <mo>+</mo> <msub> <mi>p</mi> <mi>i</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <msub> <mi>R</mi> <mi>dir</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mo></mo> <mrow> <mrow> <mi>D</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>a</mi> <mo>+</mo> <msub> <mi>p</mi> <mi>i</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> <msub> <mi>d</mi> <mi>i</mi> </msub> </mrow> <mo></mo> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <msub> <mi>w</mi> <mi>i</mi> </msub> </mrow> </mfrac> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>8</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
  <p num="p-0176"> <figref idrefs="DRAWINGS">FIG. 14</figref> shows a data set <b>1490</b>, that represents a generalized-DOF. In a preferred embodiment using the C++ programming language, <figref idrefs="DRAWINGS">FIG. 14</figref> describes an abstract base class that specifies the interface for any generalized-DOF. Specific generalized-DOFs, corresponding in a preferred embodiment to concrete derived classes, will be described below. In <figref idrefs="DRAWINGS">FIG. 14</figref>, elements low <b>1400</b> and high <b>1405</b> specify the range of parameter values to be searched, as appropriate for the application. If low=high, the parameter value is fixed for this generalized-DOFno searching is done, but the fixed parameter value contributes to all poses considered by run-time step <b>140</b> and returned in list of results <b>160</b>. In a preferred embodiment the invention requires lowhigh except for cyclic generalized-DOFs.</p>
  <p num="p-0177">Element maxStepSize <b>1410</b> specifies the maximum allowable increment in parameter value for the coarse scan step <b>1925</b> or fine scan step <b>1940</b>. In a preferred embodiment maxStepSize <b>1410</b> is chosen automatically for each generalized-DOF based on the geometry of the probes, as described below. Element maxStepSize <b>1410</b> should be set so that the pattern <b>105</b> will match sufficiently well against an instance in the run-time image <b>130</b> even if the pose is off by up to one-half maxStepSize <b>1410</b> in every generalized-DOF that does not have a fixed parameter value.</p>
  <p num="p-0178">Element dupRange <b>1415</b> specifies a range of parameter values within which distinct results may be considered duplicates. Two results are duplicates if they overlap sufficiently in position (i.e. the translation degrees of freedom) and if for each generalized-DOF their respective parameter values are within the dupRange <b>1415</b> of that generalized-DOF.</p>
  <p num="p-0179">Element start <b>1420</b> specifies the actual start of the range of parameter values to be searched, which extends half stepSize <b>1435</b> beyond the requested range given by low <b>1400</b> and high <b>1405</b> so that interpolation can be performed up to the limit of the requested range.</p>
  <p num="p-0180">Element numCoarseSteps <b>1430</b> gives the number of steps in stepSize <b>1435</b> increments to be used during coarse scan step <b>1925</b>. Element stepSize <b>1435</b> is derived from maxStepSize <b>1410</b> and the requested range from low <b>1400</b> to high <b>1405</b> such that stepSize <b>1435</b> is not greater than maxStepSize <b>1410</b> and there are an integral number of steps to cover a range that extends one-half step beyond low <b>1400</b> and high <b>1405</b>. Note that if a generalized-DOF is cyclic and the range covers the entire cycle, then 2 fewer steps are needed because the ends of the range are coincident.</p>
  <p num="p-0181">Element cycle <b>1440</b> specifies one cycle for cyclic generalized-DOFs (e.g., 360), or 0 for non-cyclic generalized-DOFs. Adding or subtracting cycle <b>1440</b> to any parameter value has no effect on the pose. The element cycle <b>1440</b> allows cyclic and non-cyclic generalized-DOFs, which are much more similar than different, to share a large body of code with only minor special case handling in several places.</p>
  <p num="p-0182">Element mapper <b>1445</b> is a function that converts a parameter value to an equivalent coordinate transform. In a preferred embodiment, mapper <b>1445</b> is a virtual function. Element mapper <b>1445</b> is the key to the generalized-DOF method, because the resulting coordinate transforms can be composed to produce a pose regardless of the type, number, and order of generalized-DOFs used by the invention in any given embodiment. In a preferred embodiment the coordinate transform produced by mapper <b>1445</b> includes a translation vector, but it is always 0.</p>
  <p num="p-0183">Elements stepSizeMatrix <b>1450</b> and stepSizeFactor <b>1455</b> are used to compute maxStepSize <b>1410</b> based on the geometry of the probes and the nature of the generalized-DOF, as further described below. Element scaleFactor <b>1460</b>, a virtual function in a preferred embodiment, computes the factor by which the pattern is scaled by this generalized-DOF (changed in size) at the middle of the search range between low <b>1400</b> and high <b>1405</b>. This is used as a rough estimate of the change in scale from the training image <b>100</b> to the run-time image <b>130</b>, so that certain parameters, such as granularity <b>1010</b>, can be adjusted.</p>
  <p num="p-0184"> <figref idrefs="DRAWINGS">FIG. 15</figref> is a table that details specific generalized-DOFs that can be used with the invention. Many other variations not shown can be devised based on the teachings disclosed herein. Each row of the table describes a specific generalized-DOF, while the columns generally specify values for specific elements.</p>
  <p num="p-0185">Column <b>1500</b> describes the parameter used by the generalized-DOF. In a preferred embodiment, rotational generalized-DOFs, e.g. <b>1540</b> and <b>1545</b>, use an angle parameter in degrees. Radians are not used because one cycle (i.e. 2) cannot be represented exactly in any practical device. Size-related generalized-DOFs use either a scale factor parameter, e.g. <b>1570</b>, <b>1575</b>, and <b>1580</b>, or a logarithmic scale factor parameter, e.g. <b>1550</b>, <b>1555</b>, and <b>1560</b>. Aspect ratio generalized-DOFs use either a ratio parameter, e.g. <b>1585</b>, or a logarithmic ratio parameter, e.g. <b>1565</b>.</p>
  <p num="p-0186">Element cycle <b>1440</b> is set to 360 for rotational generalized-DOFs, e.g. <b>1540</b> and <b>1545</b>, and 0 otherwise. Element stepSizeFactor <b>1455</b> is set to 180/ to convert radians to degrees for rotational generalized-DOFs, e.g. <b>1540</b> and <b>1545</b>, and 1 otherwise. Function element scaleFactor <b>1460</b> returns the scale factor at the geometric midpoint of the search range for uniform size-related generalized-DOFs, e.g. <b>1550</b> and <b>1570</b>, the square root of the scale factor at the geometric midpoint of the search range for non-uniform size-related generalized-DOFs, e.g. <b>1555</b>, <b>1560</b>, <b>1575</b>, and <b>1580</b>, and 1 for non-size related generalized-DOFs. For non-uniform size-related generalized-DOFs, the square root-is a reasonable estimate of the overall effect when size is varying in one dimension but not the other.</p>
  <p num="p-0187">For function element mapper <b>1445</b>, only the 22 matrix component of the coordinate transform is shown; the vector translation component is 0. Element stepSizeMatrix <b>1450</b> is determined by taking the derivative 
<maths id="MATH-US-00011" num="00011"> <math overflow="scroll"> <mfrac> <mrow> <mo></mo> <mrow> <mi>mapper</mi> <mo></mo> <mrow> <mo>(</mo> <mi>x</mi> <mo>)</mo> </mrow> </mrow> </mrow> <mrow> <mo></mo> <mi>x</mi> </mrow> </mfrac> </math> </maths> <br>
and evaluating the resulting matrix at parameter value x such that mapper(x) is the identity transform.
</p>
  <p num="p-0188"> <figref idrefs="DRAWINGS">FIG. 16</figref> shows details of the list of generalized-DOFs <b>150</b> used in a preferred embodiment of the invention. As described in the summary section, the list <b>150</b> specifies nested loops for the coarse scan step <b>1925</b> and fine scan step <b>1940</b>. The list <b>150</b> specifies both the nesting order for scanning the search space and the order in which the coordinate transforms produced by the mapper functions <b>1445</b> are composed to get the overall pose for the non-translation degrees of freedom. For the preferred embodiment shown in <figref idrefs="DRAWINGS">FIG. 16</figref>, a log y size generalized-DOF <b>1560</b> is the first element of list <b>150</b>, is the outermost loop in the scanning sequence, and its transform is applied first in mapping from pattern to image coordinates. Next is log x size generalized-DOF <b>1555</b>, followed by log size <b>1550</b>, and finally rotation <b>1540</b>, which is the innermost loop in the scanning sequence, and its transform is applied last in mapping from pattern to image coordinates. Other orders are possible, and are chosen to suit the particular application.</p>
  <p num="p-0189">There is a redundant degree of freedom among list <b>150</b> elements log y size <b>1560</b>, log x size <b>1555</b>, and log size <b>1550</b>. These three generalized-DOFs cover only a two degree of freedom search space, which any of the three possible pairs are sufficient to cover. The use of these three, however, gives the user of the invention much greater flexibility in specifying the search space than if only two non-redundant generalized-DOFs were used. Specifically, the user has 7 sensible choices-holding all three fixed, allowing anyone to vary, and allowing any pair to vary.</p>
  <p num="p-0190"> <figref idrefs="DRAWINGS">FIG. 17</figref> shows a data set that represents a result corresponding to an instance of a pattern <b>105</b> in a run-time image <b>130</b>. A list of results <b>160</b> is the primary output of the invention. Element position <b>1700</b> specifies the position in image coordinates of the origin of the pattern coordinate system at the match pose, i.e. the pose corresponding to the instance of pattern <b>105</b> in run-time image <b>130</b> represented by the result. Element probeMER <b>1710</b> specifies the minimum enclosing rectangle in image coordinates of the probes at the match pose, and is used to determine whether or not two results overlap sufficiently in position to be considered possible duplicates.</p>
  <p num="p-0191">Element score <b>1720</b> is the match score, which is refined and updated as run-time step <b>140</b> progresses. During coarse scan step <b>1925</b> it is set to a value interpolated among a set of values of first match function S<sub>1</sub>(a) of Equation 5c, evaluated at a corresponding set of offsets a in gradient direction image <b>244</b> that includes a local maximum of S<sub>1 </sub>and its neighbors. Subsequently during the coarse scan step <b>1925</b>, element score <b>1720</b> is refined by interpolating among neighboring results in each non-fixed generalized-DOF. During fine scan step <b>1940</b> it is set to second match function S<sub>2 </sub>of Equation 7. During steps <b>1930</b> and <b>1945</b>, score <b>1720</b> is set to the value of third match function S<sub>3 </sub>of Equation 8.</p>
  <p num="p-0192">Element contrast <b>1730</b> is set to the median gradient magnitude value M<sub>median</sub>, as defined by equations <b>4</b> <i>a </i>and <b>4</b> <i>b</i>, and computed as part of determining target gradient magnitude point <b>1352</b> needed for third match function S<sub>3</sub>.</p>
  <p num="p-0193">Element DOFParameters <b>1740</b> is a list of generalized-DOF parameters corresponding to generalized-DOF list <b>150</b> and specifying the non-translation degrees of freedom of the pose represented by the result. Element DOFIndices <b>1750</b> is a list of step indices corresponding to generalized-DOF list <b>150</b>. A step index for a generalized-DOF is an integer between 0 and numCoarseSteps1 that indicates a specific step during the coarse scan step <b>1925</b> for said generalized-DOF. Element DOFIndices <b>1750</b> is used to identify results that are neighbors along a generalized-DOF, as further described below.</p>
  <p num="p-0194"> <figref idrefs="DRAWINGS">FIG. 18</figref> shows how position overlap is calculated for a pair of results to determine if they might be neighbors or duplicates. Overlap is a value between 0 and 1 inclusive that indicates degree of position overlap (i.e. overlap in the translation degrees of freedom) between two results.</p>
  <p num="p-0195">In <figref idrefs="DRAWINGS">FIG. 18</figref> <i>a</i>, rectangle <b>1800</b> is the element probeMER <b>1710</b> of a first result, with center point <b>1802</b> at distance w<sub>1 </sub> <b>1804</b> from the left and right edge and distance h<sub>1 </sub> <b>1806</b> from the top and bottom edge. Similarly, rectangle <b>1810</b> is the element probeMER <b>1710</b> of a second result, with center point <b>1812</b> at distance w<sub>2 </sub> <b>1814</b> from the left and right edge and distance h<sub>2 </sub> <b>1816</b> from the top and bottom edge.</p>
  <p num="p-0196">Since in general the minimum enclosing rectangles are of different shapes, the relative positions d<sub>x </sub> <b>1818</b> and d<sub>y </sub> <b>1808</b> of the center points <b>1802</b> and <b>1812</b> as shown in <figref idrefs="DRAWINGS">FIG. 18</figref> <i>b </i>are used instead of area of intersection to determine overlap. The formula for overlap is the product of an x overlap term and ay overlap term, as follows: 
<maths id="MATH-US-00012" num="00012"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mo>[</mo> <mrow> <mi>max</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <mn>1</mn> <mo>-</mo> <mfrac> <msub> <mi>d</mi> <mi>x</mi> </msub> <mrow> <msub> <mi>w</mi> <mn>1</mn> </msub> <mo>+</mo> <msub> <mi>w</mi> <mn>2</mn> </msub> </mrow> </mfrac> </mrow> <mo>,</mo> <mn>0</mn> </mrow> <mo>)</mo> </mrow> </mrow> <mo>]</mo> </mrow> <mo></mo> <mrow> <mo>[</mo> <mrow> <mi>max</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <mn>1</mn> <mo>-</mo> <mfrac> <msub> <mi>d</mi> <mi>y</mi> </msub> <mrow> <msub> <mi>h</mi> <mn>1</mn> </msub> <mo>+</mo> <msub> <mi>h</mi> <mn>2</mn> </msub> </mrow> </mfrac> </mrow> <mo>,</mo> <mn>0</mn> </mrow> <mo>)</mo> </mrow> </mrow> <mo>]</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>9</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
  <p num="p-0197"> <figref idrefs="DRAWINGS">FIG. 18</figref> <i>c </i>shows examples of overlap 1.0 <b>1820</b>, overlap 0.5 <b>1825</b> and <b>1835</b>, overlap 0.25 <b>1840</b>, and overlap 0.0 <b>1830</b> and <b>1845</b>.</p>
  <p num="p-0198"> <figref idrefs="DRAWINGS">FIG. 19</figref> is a top-level flow chart of a preferred embodiment of run-time step <b>140</b>. Step <b>1900</b> sets pseudo-code identifiers model and dofList to model <b>120</b> and list of generalized-DOFs <b>150</b>, respectively, for reference by subsequent pseudo-code.</p>
  <p num="p-0199">Step <b>1905</b> determines the nominal scale factor s<sub>nominal </sub>of the set of ranges of parameters of all the generalized-DOFs. This is a crude estimate of the typical scale factor of the non-translation poses generated by list of generalized-DOFs <b>150</b>, and is most useful when the range of scale factors is small and significantly different from 1.0, i.e. the patterns are expected to be significantly larger or smaller in run-time image <b>130</b> than training image <b>100</b>. The value s<sub>nominal </sub>is the product, over generalized-DOF elements <b>1490</b> of dofList <b>150</b>, of all values scaleFactor <b>1460</b> of <figref idrefs="DRAWINGS">FIG. 14</figref>.</p>
  <p num="p-0200">Step <b>1910</b> computes run-time granularity g<sub>run </sub>as the product of training granularity model.granularity <b>1010</b> and s<sub>nominal </sub>but not less than 1.0. Step <b>1915</b> processes run-time image <b>130</b> to obtain gradient magnitude image <b>242</b> and gradient direction image <b>244</b>, following the steps of <figref idrefs="DRAWINGS">FIG. 2</figref>, and using run-time granularity g<sub>run</sub>.</p>
  <p num="p-0201">Step <b>1920</b> determines, for each generalized-DOF element <b>1490</b> of dofList <b>150</b>, settings for maxStepSize <b>1410</b>, start <b>1420</b>, numCoarseSteps <b>1430</b>, and stepSize <b>1435</b>.</p>
  <p num="p-0202">In a preferred embodiment, maxStepSize <b>1410</b> is computed from the geometry of the probes and the number of non-fixed generalized-DOFs. In the following, let:
</p> <ul> <li id="ul0011-0001" num="0000"> <ul> <li id="ul0012-0001" num="0225">p<sub>i </sub>be the position vector <b>1100</b> of the i<sup>th </sup>probe <b>1190</b>;</li> <li id="ul0012-0002" num="0226">(x<sub>i</sub>, y<sub>i</sub>) be the components of the position vector <b>1100</b> of the i<sup>th </sup>probe <b>1190</b>;</li> <li id="ul0012-0003" num="0227"><sub>i </sub>be the direction <b>1110</b> of the i<sup>th </sup>probe <b>1190</b>;</li> <li id="ul0012-0004" num="0228">U<sub>i </sub>be a unit vector in direction <b>1110</b> of the i<sup>th </sup>probe <b>1190</b>;</li> <li id="ul0012-0005" num="0229">W<sub>i </sub>be the weight <b>1120</b> of the i<sup>th </sup>probe <b>1190</b>;</li> <li id="ul0012-0006" num="0230">n be the number of generalized-DOFs on dofList <b>150</b> that are not fixed, i.e. where low <b>1400</b> does not equal high <b>1405</b>;</li> <li id="ul0012-0007" num="0231">M be a stepSizeMatrix <b>1450</b> of a generalized-DOF <b>1490</b>; and</li> <li id="ul0012-0008" num="0232">f be a stepSizeFactor <b>1455</b> of a generalized-DOF <b>1490</b>.</li> </ul> </li> </ul> <p num="p-0203">Define the center of projection c=(c<sub>x</sub>, c<sub>y</sub>) of a list of probes <b>1000</b> as the point that minimizes the sum of squared distance between lines, passing through the probes positions normal to the gradient direction, and said point. The center of projection is similar to center of mass of a set of points, except that center of projection considers the probes to provide information in only one degree of freedom, the gradient direction, instead of 2 degrees of freedom as for a normal point. Center of projection can be computed as follows:</p>
  <p num="p-0204"> <br>r<sub>i</sub>=x<sub>i </sub>cos(<sub>i</sub>)+y<sub>i </sub>sin(<sub>i</sub>)<maths id="MATH-US-00013" num="00013"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <msub> <mi>c</mi> <mi>x</mi> </msub> <mo>=</mo> <mfrac> <mtable> <mtr> <mtd> <mrow> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msub> <mi>r</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <mi>cos</mi> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msup> <mi>sin</mi> <mn>2</mn> </msup> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> <mo>-</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msub> <mi>r</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <mi>sin</mi> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <mrow> <mi>cos</mi> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <mi>sin</mi> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> </mtr> </mtable> <mrow> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <mrow> <msup> <mi>cos</mi> <mn>2</mn> </msup> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msup> <mi>sin</mi> <mn>2</mn> </msup> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> <mo>-</mo> <msup> <mrow> <mo>(</mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <mrow> <mi>cos</mi> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <mi>sin</mi> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> <mo>)</mo> </mrow> <mn>2</mn> </msup> </mrow> </mfrac> </mrow> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <msub> <mi>c</mi> <mi>y</mi> </msub> <mo>=</mo> <mfrac> <mtable> <mtr> <mtd> <mrow> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msub> <mi>r</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <mi>sin</mi> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msup> <mi>cos</mi> <mn>2</mn> </msup> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> <mo>-</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msub> <mi>r</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <mi>cos</mi> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <mrow> <mi>cos</mi> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <mi>sin</mi> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> </mtr> </mtable> <mrow> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <mrow> <msup> <mi>cos</mi> <mn>2</mn> </msup> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msup> <mi>sin</mi> <mn>2</mn> </msup> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> <mo>-</mo> <msup> <mrow> <mo>(</mo> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <mrow> <mi>cos</mi> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <mi>sin</mi> <mo></mo> <mrow> <mo>(</mo> <msub> <mi></mi> <mi>i</mi> </msub> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> <mo>)</mo> </mrow> <mn>2</mn> </msup> </mrow> </mfrac> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>10</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
  <p num="p-0205">For each generalized-DOF element <b>1490</b> of dofList <b>150</b>, maxStepSize <b>1410</b> is computed as follows: 
<maths id="MATH-US-00014" num="00014"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mi>b</mi> <mo>=</mo> <msqrt> <mfrac> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <mrow> <msub> <mi>w</mi> <mi>i</mi> </msub> <mo></mo> <msup> <mrow> <mo>[</mo> <mrow> <msub> <mi>u</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>M</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>p</mi> <mi>i</mi> </msub> <mo>-</mo> <mi>c</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> <mo>]</mo> </mrow> <mn>2</mn> </msup> </mrow> </mrow> <mrow> <munder> <mo></mo> <mi>i</mi> </munder> <mo></mo> <msub> <mi>w</mi> <mi>i</mi> </msub> </mrow> </mfrac> </msqrt> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>11</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>maxStepSize</mi> <mo>=</mo> <mfrac> <mrow> <mi>f</mi> <mo></mo> <mrow> <mi>min</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mfrac> <mn>1.5</mn> <mi>b</mi> </mfrac> <mo>,</mo> <mn>0.2</mn> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <msqrt> <mi>n</mi> </msqrt> </mfrac> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>12</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
  <p num="p-0206">Equation 11 computes a baseline b in units of distance that is a measure of the sensitivity of the probes to motion induced by the generalized-DOFs parameter. For example, a circular boundary would have probes pointing radially that would be very sensitive to size changes but insensitive to rotation. In the equation, probe position vectors p <b>1100</b>, relative to center of projection c, are adjusted depending on the specific generalized-DOF by matrix M <b>1450</b>, and then the dot product with unit vector u in the probe direction <b>1110</b> is taken. In the example of a circular boundary, for a size generalized-DOF, e.g. <b>1550</b> or <b>1570</b>, M is such that the said dot product gives the radius of the circle, and baseline b also becomes the radius of the circle. For a rotation generalized-DOF such as <b>1540</b>, M is such that the said dot product gives 0. In equation 12, the bigger the baseline b, the more sensitive is the pattern to changes in the parameter of the generalized-DOF, and so the smaller the step size should be. The constants 1.5 and 0.2 are used in a preferred embodiment, although other constants can be used to obtain similar performance in other applications. The step size is further reduced by the square root of the number of generalized-DOFs that can vary, since if the pose can be off in more degrees of freedom simultaneously then the step sizes must be smaller.</p>
  <p num="p-0207">Once maxStepSize <b>1410</b> is set, elements start <b>1420</b>, numCoarseSteps <b>1430</b>, and stepSize <b>1435</b> are set as shown in <figref idrefs="DRAWINGS">FIG. 20</figref> and described below.</p>
  <p num="p-0208">Step <b>1925</b> does the coarse scan of the entire search space, producing a preliminary list of results <b>160</b> for further processing. Note that the second argument I is the identity transform. Coarse scan step <b>1925</b> is described in more detail below.</p>
  <p num="p-0209">Step <b>1930</b> evaluates the third match function S<sub>3 </sub>(equation 8) for each element of results <b>160</b>, at the pose determined by coarse scan step <b>1925</b>. The purpose of step <b>1930</b> is to qualify each result as being sufficiently high in score to be worth running the fine scan step <b>1940</b>. Step <b>1930</b> is reasonably fast since only one pose is evaluated for each result.</p>
  <p num="p-0210">Step <b>1935</b> discards both weak results and duplicate results. In a preferred embodiment, a weak result is one whose score <b>1720</b> is below some fraction of a global accept threshold chosen to be suitable for the application. For step <b>1935</b>, said fraction is 0.75only results substantially weaker than the accept threshold are discarded, since the fine scan step <b>1940</b> might improve the score. In a preferred embodiment a pair of results are considered duplicates if their overlap value, as described in <figref idrefs="DRAWINGS">FIG. 18</figref>, is at least 0.8, and if their lists of generalized-DOF parameters DOFParameters <b>1740</b> agree to within dupRange <b>1415</b> for all generalized-DOFs on doflist <b>150</b>. For all duplicate pairs in results <b>160</b>, the member of the pair with the lower score <b>1720</b> is discarded.</p>
  <p num="p-0211">Step <b>1940</b> does the fine scan on each remaining result in results <b>160</b>, as further described below. Step <b>1940</b> establishes the final position <b>1700</b>, probeMER <b>1710</b>, and DOFParameters <b>1740</b> for each result <b>1790</b>.</p>
  <p num="p-0212">Step <b>1945</b> evaluates the third match function S<sub>3 </sub>(equation 8) for each element of results <b>160</b>, at the pose determined by fine scan step <b>1940</b>. Step <b>1945</b> establishes the final score <b>1720</b> and contrast <b>1730</b> for each result <b>1790</b>. Step <b>1945</b> also stores individual probe ratings, which are the product of magnitude rating factor R<sub>mag </sub>and direction rating factor R<sub>dir </sub>from equation 8, in result element <b>1760</b>.</p>
  <p num="p-0213">Step <b>1950</b> repeats step <b>1935</b> to discard weak and duplicate results. For step <b>1950</b>, however, weak results are defined to be those whose score <b>1720</b> is below 0.9 of the global accept threshold.</p>
  <p num="p-0214"> <figref idrefs="DRAWINGS">FIG. 20</figref> provides more details on the setting of generalized-DOF elements start <b>1420</b>, numCoarseSteps <b>1430</b>, and stepSize <b>1435</b> in step <b>1920</b>. Step <b>2000</b> determines if the requested range of parameter values between low <b>1400</b> and high <b>1405</b> is sufficiently large to require coarse scanning. If the range from low <b>1400</b> to high <b>1405</b> is not greater than maxStepSize <b>1410</b>, then no coarse scanning is required and so numCoarseSteps <b>1430</b> is set to 1 in step <b>2005</b>, and start <b>1420</b> is set to the midpoint of the range in step <b>2010</b>.</p>
  <p num="p-0215">If coarse scanning is required, numCoarseSteps <b>1430</b> is set in step <b>2020</b> to be the range divided by maxStepSize <b>1410</b>, but rounded up to the nearest integer. Note that this is not yet the final value for numCoarseSteps, because boundary conditions must be considered. The actual step size stepSize <b>1435</b> is then set in step <b>2025</b> to be the range divided by numCoarseSteps. The result of steps <b>2020</b> and <b>2025</b> is that stepSize <b>1435</b> is the smallest value that can cover the range in the same integral number of steps as can be done by maxStepSize <b>1410</b>.</p>
  <p num="p-0216">Step <b>2030</b> tests to see if the generalized-DOF is cyclic and if the requested range covers the full cycle. If so, start <b>1420</b> is set to low <b>1400</b> in step <b>2040</b>in this case it doesn't really matter where the scan starts. The value of numCoarseSteps <b>1430</b> computed in step <b>2020</b> is correct, because the range has no end points. If the requested range is not a full cycle (including non-cyclic generalized-DOFs), start <b>1420</b> is set one-half step below low <b>1400</b> in step <b>2050</b>, and numCoarseSteps <b>1430</b> is increased by two in step <b>2060</b> to cover the end points.</p>
  <p num="p-0217"> <figref idrefs="DRAWINGS">FIG. 21</figref> <i>a </i>is a flow chart of a function coarseScanDOF <b>2100</b> that scans all of the generalizedDOFs on input list doflist <b>2102</b>, and returns a list of results <b>1790</b> describing poses representing possible instances of pattern <b>105</b> in run-time image <b>130</b>. Function coarseScanDOF <b>2100</b> is a recursive function-it operates on an outermost generalized-DOF which is the first element of input list <b>2102</b>, and calls itself to operate on the inner generalized-DOFs represented by the rest of the list. At each level of the recursion a partial pose is constructed by composing a current mapper <b>1445</b> transform with the input map <b>2104</b> constructed by recursion levels representing outer generalized-DOFs, and passing said partial pose along to recursion levels representing inner generalized-DOFs. At the outermost level in step <b>1925</b> the identity transform is provided for input map <b>2104</b>. At the innermost level, when step <b>2110</b> determines that input dofList <b>2102</b> is null, the non-translation portion of the pose is complete and the procedure coarseScanXY <b>2200</b> of <figref idrefs="DRAWINGS">FIG. 22</figref> <i>a </i>is called in step <b>2112</b> to scan the translation degrees of freedom.</p>
  <p num="p-0218">If input doflist <b>2102</b> is not null, step <b>2114</b> extracts the first element representing the current generalized-DOF, and the rest of the list representing inner generalized-DOFs. An empty results list is allocated in step <b>2116</b>. Loop step <b>2118</b> executes search loop <b>2130</b> for a sequence of values of a step index. Each iteration of loop search <b>2130</b> scans one parameter value of the current generalized-DOF. When the scanning is complete, loop step <b>2120</b> executes peak loop <b>2160</b> for every result found by scan loop <b>2118</b>. Each iteration of peak loop <b>2160</b> determines whether a result is a peaka local maximum in the current generalized-DOFand if so, interpolates it, otherwise marks it for deletion. Step <b>2122</b> actually deletes all results marked for deletion, and finally step <b>2124</b> returns the list of remaining results.</p>
  <p num="p-0219"> <figref idrefs="DRAWINGS">FIG. 21</figref> <i>b </i>is a flow chart of search loop <b>2130</b>. Step <b>2140</b> computes the parameter value corresponding to the current step index. Step <b>2142</b> is the recursive call to coarseScanDOF <b>2100</b> that scans the inner generalized-DOFs. Note the second argument, which composes the current mapper transform with map input <b>2104</b>. Loop steps <b>2144</b>, <b>2146</b>, <b>2148</b>, and <b>2150</b> add the current step index and parameter value to the beginning of elements DOFIndices <b>1750</b> and DOFParameters <b>1740</b> in every result <b>1790</b> returned by recursive step <b>2142</b>. Finally, in step <b>2151</b> the list of results returned by recursive step <b>2142</b> is added to the end of results and in step <b>2154</b> the search loop continues to the next index at step <b>2118</b> of <figref idrefs="DRAWINGS">FIG. 21</figref> <i>a. </i> </p>
  <p num="p-0220"> <figref idrefs="DRAWINGS">FIG. 21</figref> <i>c </i>is a flow chart of peak loop <b>2160</b>, which operates on an specific result r. Steps <b>2170</b> and <b>2172</b> search results for a previous and next neighbor of r, respectively. A neighbor of r is a result whose step index for the current generalized-DOF, which is the first element of DOFIndices <b>1750</b>, differs by exactly 1 from that of r, whose step indices for all inner generalized-DOFs, which are the second and subsequent elements if any of DOFIndices <b>1750</b>, differs by no more than 1 from that of r, and whose overlap value with r (equation 9) is at least 0.8. For cyclic generalized-DOFs where the scan range covers the full cycle, step index differences are considered modulo numCoarseSteps <b>1430</b>. A previous neighbor is a neighbor where the current step index difference is 1, and a next neighbor is a neighbor where the current step index difference is +1.</p>
  <p num="p-0221">Step <b>2174</b> determines if result r is a peak (local maximum in score) compared to its neighbors. Note that if a previous or next neighbor was not found, its score is assumed to be 0. If not, r is marked for deletion in step <b>2176</b>. If so, the parameter value is interpolated in step <b>2178</b>, and, if both a previous and next neighbor were found, the score is interpolated in step <b>2180</b>, and then the loop is continued in step <b>2182</b>.</p>
  <p num="p-0222">The following 3-point parabolic interpolation functions are used: 
<maths id="MATH-US-00015" num="00015"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mrow> <mi>InterpPos</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>l</mi> <mo>,</mo> <mi>c</mi> <mo>,</mo> <mi>r</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mfrac> <mrow> <mi>r</mi> <mo>-</mo> <mi>l</mi> </mrow> <mrow> <mrow> <mn>4</mn> <mo></mo> <mi>c</mi> </mrow> <mo>-</mo> <mrow> <mn>2</mn> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>l</mi> <mo>+</mo> <mi>r</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mfrac> </mrow> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mrow> <mi>InterpPos</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>l</mi> <mo>,</mo> <mi>c</mi> <mo>,</mo> <mi>r</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mrow> <mi>c</mi> <mo>+</mo> <mfrac> <msup> <mrow> <mo>(</mo> <mrow> <mi>r</mi> <mo>-</mo> <mi>l</mi> </mrow> <mo>)</mo> </mrow> <mn>2</mn> </msup> <mrow> <mrow> <mn>16</mn> <mo></mo> <mi>c</mi> </mrow> <mo>-</mo> <mrow> <mn>8</mn> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>l</mi> <mo>+</mo> <mi>r</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mfrac> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>13</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
  <p num="p-0223">The interpolated parameter value is obtained by adding:
<br>stepSize*InterpPos(prevResult.score, r.score, nextResult.score to the current value. The interpolated score is:<br>InterpScore(prevResult.score, r.score, nextResult.score)</p>
  <p num="p-0224"> <figref idrefs="DRAWINGS">FIG. 22</figref> <i>a </i>is a flow chart of a function coarseScanXY <b>2200</b> used by coarseScanDOF <b>2100</b> to scan the translation degrees of freedom. In takes one input map <b>2202</b> that specifies the non-translation portion of the pose, and returns a list of results <b>1790</b> representing potential instances of pattern <b>105</b> in run-time image <b>130</b> at poses corresponding to map <b>2202</b>.</p>
  <p num="p-0225">Step <b>2210</b> allocates an empty list of results. Step <b>2212</b> compiles list of probes <b>1000</b> using function compileProbes <b>1200</b> resulting in a list of compiled probes <b>1195</b> and a minimum enclosing rectangle of the probes. Step <b>2214</b> evaluates first match function S<sub>1</sub>(a) at a subset of possible offsets a in gradient magnitude image <b>244</b>, as selected by a scan pattern described below. Only image offsets a such that all of the compiled probes, each placed at its offset <b>1130</b> relative to a, contained in said image <b>244</b>, are evaluated by step <b>2214</b>.</p>
  <p num="p-0226">Loop step <b>2216</b> iterates over the scores evaluated in step <b>2214</b>, and step <b>2218</b> examines the scores and looks for local maxima above a noise threshold. In a preferred embodiment, the noise threshold is set to 0.66 of the global accept threshold. Detection of local maxima is described below. When a local maximum above the noise threshold is found, new result loop <b>2240</b> is executed; otherwise, control flows to step <b>2220</b> and then back to <b>2216</b>. When all the scores have been examined by step <b>2218</b>, control passes to step <b>2222</b> which returns any results found.</p>
  <p num="p-0227"> <figref idrefs="DRAWINGS">FIG. 22</figref> <i>b </i>is a flow chart of new result loop <b>2240</b>, which is executed whenever a local maximum in score above the noise threshold is found by step <b>2218</b>. Step <b>2242</b> allocates a new result <b>1790</b>. Step <b>2244</b> initializes the values by setting position <b>1700</b> to an interpolated position of the maximum score, score <b>1720</b> to an interpolated score, probeMER <b>1710</b> to the minimum enclosing rectangle computed by compileProbes <b>1200</b>, offset by the interpolated position of the maximum score, and lists DOFParameters <b>1740</b> and DOFIndices <b>1750</b> to empty lists.</p>
  <p num="p-0228">Step <b>2246</b> searches the results found so far for a duplicate of the new result. In a preferred embodiment, a duplicate is a result with overlap value (equation 9) of at least 0.8. Steps <b>2248</b> and <b>2250</b> select among three cases. If no duplicate was found, step <b>2252</b> adds the new result to the list. If a duplicate was found with a score lower than the new result, step <b>2254</b> replaces the duplicate with the new result. If a duplicate was found with a score not lower than the new result, step <b>2256</b> discards the new result. Finally, step <b>2258</b> transfers control back to step <b>2216</b> to continue looking for local maxima.</p>
  <p num="p-0229"> <figref idrefs="DRAWINGS">FIG. 23</figref> shows coarse x-y scan patterns used by step <b>2214</b> in a preferred embodiment. In each example the dots indicate relative positions to be evaluated. Traditionally template matching systems have evaluated a match score at every position, or in a square pattern of sub-sampled positions, and the same may be done in a less preferred embodiment of the invention. The patterns shown in <figref idrefs="DRAWINGS">FIG. 23</figref>, called hexagonal scan patterns due to the shape of the neighborhoods, e.g. <b>2304</b>, <b>2314</b>, <b>2324</b>, and <b>2334</b>, are both more efficient and more flexible than a square or any other pattern method. With hexagonal patterns it is possible to evaluate a fraction of possible positions, i.e.  for example <b>2300</b>,  for example <b>2310</b>,  for example <b>2320</b>, and 1/9 for example <b>2330</b>, that is not restricted to reciprocals of perfect squares, as for square subsampling patterns, and not significantly anisotropic, as for rectangular sub-sampling patterns. For a given fraction of positions, the worst case distance from any point in the plane to the nearest evaluated point is less for the hexagonal pattern than for any other pattern. Since the grid itself is square it is only possible to approximate a hexagonal pattern, but the worst case distance is still very close to optimum. In a preferred embodiment, the  pattern <b>2310</b> is used.</p>
  <p num="p-0230"> <figref idrefs="DRAWINGS">FIG. 23</figref> shows example evaluated points <b>2302</b>, <b>2312</b>, <b>2322</b>, and <b>2332</b>, and corresponding neighborhoods <b>2304</b>, <b>2314</b>, <b>2324</b>, and <b>2334</b> for use by peak detection step <b>2218</b> and interpolation step <b>2244</b>.</p>
  <p num="p-0231"> <figref idrefs="DRAWINGS">FIG. 24</figref> <i>a </i>shows peak detection rules used by a preferred embodiment for step <b>2218</b>. Evaluated point <b>2400</b>, with neighborhood <b>2405</b> corresponding schematically to any hexagonal neighborhood, e.g., <b>2304</b>, <b>2314</b>, <b>2324</b>, and <b>2334</b>, is considered a local maximum if its score is greater than or equal to the scores of neighbors <b>2410</b>, <b>2415</b>, and <b>2420</b>, and greater than the scores of neighbors <b>2425</b>, <b>2430</b>, and <b>2435</b>.</p>
  <p num="p-0232"> <figref idrefs="DRAWINGS">FIG. 24</figref> <i>b </i>gives symbols to be used for interpolation on hexagonal scan patterns, as is used in step <b>2244</b>. Evaluated score at local maximum z <b>2440</b>, with neighborhood <b>2445</b> corresponding schematically to any hexagonal neighborhood, e.g., <b>2304</b>, <b>2314</b>, <b>2324</b>, and <b>2334</b>, has neighboring scores in a first grid direction x<sub>p </sub> <b>2450</b> and X<sub>n </sub> <b>2465</b>, in a second grid direction up <b>2455</b> and U<sub>n </sub> <b>2470</b>, and a third grid direction v<sub>p </sub> <b>2460</b> and v<sub>n </sub> <b>2475</b>. For each grid direction a three-point parabolic interpolation is computed as follows:
<br>r<sub>x</sub>=InterpPos(x<sub>n</sub>, z, x<sub>p</sub>)<br>r<sub>u</sub>=InterpPos(u<sub>n</sub>, z, u<sub>p</sub>)<br>r<sub>v</sub>=InterpPos(v<sub>n</sub>, z, v<sub>p</sub>)(14)</p>
  <p num="p-0233">Construct lines <b>2480</b>, <b>2482</b>, and <b>2484</b> normal to grid directions x, u, and v respectively, and at a distance r<sub>x</sub>, r<sub>u</sub>, and r<sub>y</sub>, respectively from local maximum <b>2440</b> in the direction of x<sub>p </sub> <b>2450</b>, u<sub>p </sub> <b>2455</b>, and v<sub>p </sub> <b>2460</b> respectively. The interpolated position <b>2490</b> is the point that minimizes the sum squared distance between lines <b>2480</b>, <b>2482</b>, and <b>2484</b> and point <b>2490</b>. In the example, r<sub>x </sub>and r<sub>u </sub>are negative and r<sub>v </sub>is positive. The offset (x, y) of interpolated point <b>2490</b> from the local maximum point <b>2440</b> is given by: 
<maths id="MATH-US-00016" num="00016"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mrow> <mi></mi> <mo></mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo></mo> <mi>x</mi> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi></mi> <mo></mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo></mo> <mi>y</mi> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo>=</mo> <mrow> <mi>J</mi> <mo></mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>r</mi> <mi>x</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>r</mi> <mi>u</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>r</mi> <mi>v</mi> </msub> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>15</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p> <ul> <li id="ul0013-0001" num="0000"> <ul> <li id="ul0014-0001" num="0264">where 23 matrix J is:</li> </ul> </li> </ul> <p num="p-0234">
    <tables id="TABLE-US-00001" num="00001"> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup align="left" colsep="0" rowsep="0" cols="3"> <colspec colname="offset" colwidth="35pt" align="left"> </colspec> <colspec colname="1" colwidth="35pt" align="center"> </colspec> <colspec colname="2" colwidth="147pt" align="center"> </colspec> <thead> <tr class="description-tr"> <td class="description-td"> </td>
              <td namest="offset" nameend="2" align="center" rowsep="1" class="description-td" colspan="3"> </td>
            </tr> <tr class="description-tr"> <td class="description-td"> </td>
              <td class="description-td">Pattern</td>
              <td class="description-td">J</td>
            </tr> <tr class="description-tr"> <td class="description-td"> </td>
              <td namest="offset" nameend="2" align="center" rowsep="1" class="description-td" colspan="3"> </td>
            </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td class="description-td"> </td>
              <td class="description-td"> 2300</td>
              <td class="description-td">
                <maths id="MATH-US-00017" num="00017"> <math overflow="scroll"> <mrow> <mfrac> <mn>1</mn> <mn>2</mn> </mfrac> <mo></mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mn>2</mn> </mtd> <mtd> <mn>1</mn> </mtd> <mtd> <mrow> <mo>-</mo> <mn>1</mn> </mrow> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>1</mn> </mtd> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </math> </maths> </td>
            </tr> <tr class="description-tr"> <td class="description-td"> </td>
              <td class="description-td"> </td>
            </tr> <tr class="description-tr"> <td class="description-td"> </td>
              <td class="description-td"> 2310</td>
              <td class="description-td">
                <maths id="MATH-US-00018" num="00018"> <math overflow="scroll"> <mrow> <mfrac> <mn>1</mn> <mn>35</mn> </mfrac> <mo></mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mn>50</mn> </mtd> <mtd> <mn>25</mn> </mtd> <mtd> <mrow> <mo>-</mo> <mn>25</mn> </mrow> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>42</mn> </mtd> <mtd> <mn>42</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </math> </maths> </td>
            </tr> <tr class="description-tr"> <td class="description-td"> </td>
              <td class="description-td"> </td>
            </tr> <tr class="description-tr"> <td class="description-td"> </td>
              <td class="description-td"> 2320</td>
              <td class="description-td">
                <maths id="MATH-US-00019" num="00019"> <math overflow="scroll"> <mrow> <mfrac> <mn>1</mn> <mn>22</mn> </mfrac> <mo></mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mn>39</mn> </mtd> <mtd> <mn>24</mn> </mtd> <mtd> <mrow> <mo>-</mo> <mn>15</mn> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mo>-</mo> <mn>3</mn> </mrow> </mtd> <mtd> <mn>32</mn> </mtd> <mtd> <mn>35</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </math> </maths> </td>
            </tr> <tr class="description-tr"> <td class="description-td"> </td>
              <td class="description-td"> </td>
            </tr> <tr class="description-tr"> <td class="description-td"> </td>
              <td class="description-td"> 1/9 2330</td>
              <td class="description-td">
                <maths id="MATH-US-00020" num="00020"> <math overflow="scroll"> <mrow> <mfrac> <mn>1</mn> <mn>96</mn> </mfrac> <mo></mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mn>207</mn> </mtd> <mtd> <mn>117</mn> </mtd> <mtd> <mrow> <mo>-</mo> <mn>90</mn> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mo>-</mo> <mn>21</mn> </mrow> </mtd> <mtd> <mn>169</mn> </mtd> <mtd> <mn>190</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </math> </maths> </td>
            </tr> <tr class="description-tr"> <td class="description-td"> </td>
              <td namest="offset" nameend="2" align="center" rowsep="1" class="description-td" colspan="3"> </td>
            </tr> </tbody> </tgroup> </table> </tables> </p>
  <p num="p-0235">The interpolated score is:
<br>max(InterpScore (x<sub>n</sub>, z, x<sub>p</sub>), InterpScore (u<sub>n</sub>, z, u<sub>p</sub>), InterpScore (v<sub>n</sub>, z, v<sub>p</sub>)).</p>
  <p num="p-0236">In a less preferred embodiment using a hexagonal scan pattern, interpolation is accomplished by fitting an elliptical paraboloid to scores <b>2440</b>, <b>2450</b>, <b>2455</b>, <b>2460</b>, <b>2465</b>, <b>2470</b>, <b>2475</b>, and defining the position and height of the extremum of said elliptical paraboloid to be the interpolated position and score.</p>
  <p num="p-0237"> <figref idrefs="DRAWINGS">FIG. 25</figref> is a top level flow chart of fine scan step <b>1940</b>. Loop step <b>2500</b> does some number of iterations of refinement, where each iteration refines all of the results and the step size is halved each time. In a preferred embodiment, two iterations are done. Loop steps <b>2510</b> and <b>2520</b> halve the step size for each generalized-DOF on dofList <b>150</b>. Loop steps <b>2530</b> and <b>2540</b> call procedure fineScanDOF <b>2600</b> for each result. Note that the second argument I to fineScanDOF <b>2600</b> is the identity transform.</p>
  <p num="p-0238"> <figref idrefs="DRAWINGS">FIG. 26</figref> is a flow chart of procedure fineScanDOF <b>2600</b>, with inputs doflist <b>2602</b>, map <b>2604</b>, and result <b>2606</b>. Function fineScanDOF <b>2600</b> is a recursive functionit operates on an outermost generalized-DOF which is the first element of input list <b>2602</b>, and calls itself to operate on the inner generalized-DOFs represented by the rest of the list. At each level of the recursion a partial pose is constructed by composing a current mapper <b>1445</b> transform with the input map <b>2604</b> constructed by recursion levels representing outer generalized-DOFs, and passing said partial pose along to recursion levels representing inner generalized-DOFs. At the outermost level in step <b>2540</b> the identity transform is provided for input map <b>2604</b>. At the innermost level, when step <b>2610</b> determines that input dofList <b>2602</b> is null, the non-translation portion of the pose is complete and control flows to fine scan x-y step <b>2700</b> to scan the translation degrees of freedom.</p>
  <p num="p-0239">If input doflist <b>2602</b> is not null, step <b>2620</b> extracts the first element representing the current generalized-DOF, and the rest of the list representing inner generalized-DOFs. Step <b>2630</b> fetches the current value of the parameter corresponding to the current generalized-DOF from list DOFParameters <b>1740</b> in the result <b>2606</b> being refined. Step <b>2640</b> calls fineScanDOF <b>2600</b> recursively to scan the inner generalized-DOFs for poses corresponding to the current parameter setting of the current generalized-DOF. Step <b>2650</b> tests to determine if the current generalizedDOF is fixed, and if so step <b>2660</b> returns, otherwise control flows to hill climb step <b>2900</b>.</p>
  <p num="p-0240"> <figref idrefs="DRAWINGS">FIG. 27</figref> is a flow chart of the translation portion of the fine scan step <b>1940</b>. Step <b>2710</b> makes a list of compiled probes based on the non-translation degrees of freedom specified by input map <b>2604</b>. Step <b>2720</b> evaluates second match function S<sub>2 </sub>at a set of offsets surrounding the current position <b>1700</b> of result, determined by fine scan pattern <b>2800</b>. Step <b>2730</b> stores the highest match score found in step <b>2720</b> in element score <b>1720</b> of result. Step <b>2740</b> sets position <b>1700</b> of result to the position of the highest match score, interpolated between north-south neighbors (y) (v) neighbors, and east-west (x) neighbors, using InterpPos (equation 13). Step <b>2750</b> returns from procedure fineScanDOF <b>2600</b>.</p>
  <p num="p-0241"> <figref idrefs="DRAWINGS">FIG. 28</figref> shows the fine scan pattern <b>2800</b> used in a preferred embodiment 32 offsets are evaluated surrounding the current best position <b>1700</b>.</p>
  <p num="p-0242"> <figref idrefs="DRAWINGS">FIG. 29</figref> is a flow chart of hill climb step <b>2900</b>, used for non-fixed generalized-DOFs. In step <b>2910</b> two temporary results rp (plus direction) and rn (minus direction) are allocated and initialized to input result <b>2606</b>. Step <b>2920</b> evaluates poses at plus and minus one step size from the current parameter value. Step <b>2930</b> tests to see if the score improves in either the plus or minus direction. If not, control flows to interpolate step <b>2950</b>. If so, step <b>2940</b> tests to see which direction is better, plus or minus. If the plus direction is better (has higher score), control flows to plus direction step <b>3000</b>. If the minus direction is better, control flows to minus direction step <b>2990</b>. Once plus direction step <b>3000</b> or minus direction step <b>2990</b> finishes, step <b>2950</b> interpolates the parameter value by adding
<br>stepSize*InterpPos(rn.score, result. score, rp.score) to the appropriate parameter value in list DOFParameters <b>1740</b> of result <b>1790</b>.</p>
  <p num="p-0243"> <figref idrefs="DRAWINGS">FIG. 30</figref> is a flow chart of plus direction step <b>3000</b>. Step <b>3010</b> tests to see if stepping in the plus direction would exceed the high limit <b>1405</b> of the search range. If so, control flows to ending step <b>3060</b>. If not, step <b>3020</b> steps the generalized-DOF parameter in the plus direction. Step <b>3030</b> shifts the temporary results over by one step, and step <b>3040</b> evaluates a new set of poses in the plus direction. Step <b>3050</b> tests to see if the score at the new parameter value is greater than the previous best score, and if so control flows back to step <b>3010</b> to continue scanning in the plus direction. If not, scanning terminates at step <b>3060</b>, and control flows back to step <b>2950</b>.</p>
  <p num="p-0244">Minus direction step <b>2990</b> is identical in form to plus direction step <b>3000</b>, with obvious modifications to scan in the other direction. Details are not shown.</p>
  <p num="p-0245">In a preferred embodiment, granularity <b>1010</b> is chosen automatically during training step <b>300</b> based on an analysis of pattern <b>105</b>. The portion of training image <b>100</b> corresponding to pattern <b>105</b> is processed to extract boundary points <b>270</b> at a set of granularities according to <figref idrefs="DRAWINGS">FIG. 2</figref>, and each resulting boundary point list <b>270</b> is processed according to <figref idrefs="DRAWINGS">FIGS. 4</figref>, <b>5</b>, and <b>6</b>, corresponding to steps <b>320</b>, <b>330</b>, <b>340</b>, and <b>350</b>, to produce, for each boundary point, a left and right neighbor, to produce chains, to discard weak chains, and to determine the total arc length of each chain.</p>
  <p num="p-0246">In the following, for any given boundary point list <b>270</b> and associated chains at some granularity g, let:
</p> <ul> <li id="ul0015-0001" num="0000"> <ul> <li id="ul0016-0001" num="0278">P<sub>r </sub>be the position vector of the r<sup>th </sup>boundary point.</li> <li id="ul0016-0002" num="0279">x<sub>r</sub>, x<sub>r </sub>be the components of p<sub>r</sub>.</li> <li id="ul0016-0003" num="0280">d<sub>r </sub>be the gradient direction of the r<sup>th </sup>boundary point.</li> <li id="ul0016-0004" num="0281">M<sub>r </sub>be the gradient magnitude of the r<sup>th </sup>boundary point.</li> <li id="ul0016-0005" num="0282">u<sub>r </sub>be the unit vector in the gradient direction d<sub>r</sub>.</li> <li id="ul0016-0006" num="0283">a be the area of pattern <b>105</b> in pixels.</li> <li id="ul0016-0007" num="0284">I<sub>r </sub>be the arc length of the chain, which is the sum of the arc lengths of all the chain segments, containing the r<sup>th </sup>boundary point.</li> </ul> </li> </ul> <p num="p-0247">An estimate of a suitable granularity g<sub>est </sub>is made using the formula: 
<maths id="MATH-US-00021" num="00021"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <msub> <mi>g</mi> <mi>est</mi> </msub> <mo>=</mo> <msqrt> <mfrac> <msqrt> <mi>a</mi> </msqrt> <mn>8</mn> </mfrac> </msqrt> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>16</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
  <p num="p-0248">A set of integer granularities in the range 1 to g<sub>max</sub>, inclusive, is analyzed, where
<br>g<sub>max</sub>=floor({overscore (2)}g<sub>est</sub>)(17)</p>
  <p num="p-0249">For each granularity g in the above range, an overall rating Q<sub>g </sub>is computed. The formulas for the rating Q<sub>g </sub>have the following goals:
</p> <ul> <li id="ul0017-0001" num="0000"> <ul> <li id="ul0018-0001" num="0288">To prefer granularities closer to g<sub>est</sub>.</li> <li id="ul0018-0002" num="0289">To prefer that boundary points be spread out, covering more area.</li> <li id="ul0018-0003" num="0290">To prefer longer chains.</li> <li id="ul0018-0004" num="0291">To prefer smaller curvature along the chains.</li> <li id="ul0018-0005" num="0292">To prefer stronger gradient magnitudes.</li> <li id="ul0018-0006" num="0293">To prefer that boundary points not be near other parallel boundaries.</li> <li id="ul0018-0007" num="0294">To normalize the rating so that ratings at different granularities can be compared.</li> </ul> </li> </ul> <p num="p-0250">Define a curvature rating function of neighboring boundary points r and j as follows: 
<maths id="MATH-US-00022" num="00022"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mi>C</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>r</mi> <mo>,</mo> <mi>j</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mrow> <mn>1</mn> <mo>-</mo> <mrow> <mi>min</mi> <mo></mo> <mrow> <mo>[</mo> <mfrac> <mrow> <mi>max</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mrow> <mo></mo> <mrow> <msub> <mi>d</mi> <mi>r</mi> </msub> <mo>-</mo> <msub> <mi>d</mi> <mi>j</mi> </msub> </mrow> <mo></mo> </mrow> <mn>360</mn> </msub> <mo>-</mo> <mrow> <mn>16.875</mn> <mo></mo> <mi></mi> </mrow> </mrow> <mo>,</mo> <mn>0</mn> </mrow> <mo>)</mo> </mrow> </mrow> <mrow> <mn>11.25</mn> <mo></mo> <mi></mi> </mrow> </mfrac> <mo>]</mo> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>18</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p>
  <p num="p-0251">In this formula, the absolute difference of the two gradient directions is taken module 360, so that the result is positive and in the range 0180. The curvature rating is 1 for direction differences less than 16.875, 0 for direction differences above 28.125, and proportionally between 0 and 1 for differences between 16.875 and 28.125.</p>
  <p num="p-0252">Define a parallel magnitude value e<sub>r </sub>whose purpose is to estimate the gradient magnitude of boundaries close to and parallel to the r<sup>th </sup>boundary point. Let G<sub>r </sub>be a set of boundary points found approximately along a line passing through boundary point r and in gradient direction d<sub>r</sub>. Define 
<maths id="MATH-US-00023" num="00023"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <msub> <mi>e</mi> <mi>r</mi> </msub> <mo>=</mo> <mrow> <munder> <mo></mo> <mrow> <mi>j</mi> <mo></mo> <msub> <mi>G</mi> <mi>r</mi> </msub> </mrow> </munder> <mo></mo> <mstyle> <mspace width="0.3em" height="0.3ex"> </mspace> </mstyle> <mo></mo> <mrow> <msub> <mi>M</mi> <mi>j</mi> </msub> <mo></mo> <mrow> <mi>P</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>r</mi> <mo>,</mo> <mi>j</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo></mo> <mrow> <mi>D</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>r</mi> <mo>,</mo> <mi>j</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>19</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p> <ul> <li id="ul0019-0001" num="0000"> <ul> <li id="ul0020-0001" num="0298">so that e<sub>r </sub>is the sum over all boundary points j in G<sub>r </sub>of the product of gradient magnitude M<sub>j</sub>, a parallel rating P(r,j) and a distance rating D(r,j), where 
<maths id="MATH-US-00024" num="00024"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mi>P</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>r</mi> <mo>,</mo> <mi>j</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mrow> <mn>1</mn> <mo>-</mo> <mrow> <mi>min</mi> <mo></mo> <mrow> <mo>[</mo> <mrow> <mfrac> <mrow> <mi>max</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mrow> <mo></mo> <mrow> <msub> <mi>d</mi> <mi>r</mi> </msub> <mo>-</mo> <msub> <mi>d</mi> <mi>j</mi> </msub> </mrow> <mo></mo> </mrow> <mn>180</mn> </msub> <mo>-</mo> <msup> <mn>11.25</mn> <mi></mi> </msup> </mrow> <mo>,</mo> <mn>0</mn> </mrow> <mo>)</mo> </mrow> </mrow> <msup> <mn>11.25</mn> <mi></mi> </msup> </mfrac> <mo>,</mo> <mn>1</mn> </mrow> <mo>]</mo> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>20</mn> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mi>D</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>r</mi> <mo>,</mo> <mi>j</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mrow> <mn>1</mn> <mo>-</mo> <mrow> <mi>min</mi> <mo></mo> <mrow> <mo>[</mo> <mrow> <mfrac> <mrow> <mi>max</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <mrow> <mo></mo> <mrow> <msub> <mi>u</mi> <mi>r</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>p</mi> <mi>r</mi> </msub> <mo>-</mo> <msub> <mi>p</mi> <mi>j</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo></mo> </mrow> <mo>-</mo> <mn>1.0</mn> </mrow> <mo>,</mo> <mn>0</mn> </mrow> <mo>)</mo> </mrow> </mrow> <mn>4.0</mn> </mfrac> <mo>,</mo> <mn>1</mn> </mrow> <mo>]</mo> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>21</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </li> </ul> </li> </ul> <p num="p-0253">Parallel rating P(r,j) is similar to curvature rating C(r,j), except that the absolute difference of gradient direction is taken module 180, and ranges from 0 to 90.</p>
  <p num="p-0254">Distance rating D(r,j) is based on the distance between boundary points r and j in gradient direction d<sub>r</sub>. This is the effect of the dot product shown. Distances smaller than 1 pixel get a rating of 1.0, greater than 5 pixels get a rating of 0, and proportionally in between.</p>
  <p num="p-0255">Define a weight W<sub>r </sub>for the r<sup>th </sup>boundary point.
<br> <i>W</i> <sub>r</sub> <i>=l</i> <sub>r</sub> <sup>0.25</sup> <i>C</i>(<i>r</i>, left)C(<i>r</i>, right)max(<i>m</i> <sub>r</sub> <i>e</i> <sub>r</sub>,0)</p> <ul> <li id="ul0021-0001" num="0000"> <ul> <li id="ul0022-0001" num="0302">where left and right identify the left and right neighbors of the r<sup>th </sup>boundary point along the chain, respectively. The weight W<sub>r </sub>is the gradient magnitude m<sub>r</sub>, but discounted for near by parallel magnitudes e<sub>r</sub>, further discounted for excessive left or right curvature ratings, and enhanced based on chain length according to a power law. In a preferred embodiment the power is 0.25.</li> </ul> </li> </ul> <p num="p-0256">Now define the overall rating 
<maths id="MATH-US-00025" num="00025"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <msub> <mi>Q</mi> <mi>g</mi> </msub> <mo>=</mo> <mrow> <msup> <mi>g</mi> <mn>1.625</mn> </msup> <mo></mo> <msup> <mi>e</mi> <mrow> <mo>-</mo> <mrow> <mo></mo> <mrow> <mi>log</mi> <mo></mo> <mrow> <mo>(</mo> <mfrac> <mi>g</mi> <msub> <mi>g</mi> <mi>est</mi> </msub> </mfrac> <mo>)</mo> </mrow> </mrow> <mo></mo> </mrow> </mrow> </msup> <mo></mo> <mi>I</mi> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>22</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </p> <ul> <li id="ul0023-0001" num="0000"> <ul> <li id="ul0024-0001" num="0304">where I is boundary point moment of inertia: 
<maths id="MATH-US-00026" num="00026"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mi>I</mi> <mo>=</mo> <msqrt> <mrow> <mrow> <munder> <mo></mo> <mi>r</mi> </munder> <mo></mo> <mrow> <msub> <mi>w</mi> <mi>r</mi> </msub> <mo></mo> <msubsup> <mi>x</mi> <mi>r</mi> <mn>2</mn> </msubsup> </mrow> </mrow> <mo>+</mo> <mrow> <munder> <mo></mo> <mi>r</mi> </munder> <mo></mo> <mrow> <msub> <mi>w</mi> <mi>r</mi> </msub> <mo></mo> <msubsup> <mi>y</mi> <mi>r</mi> <mn>2</mn> </msubsup> </mrow> </mrow> <mo>-</mo> <mfrac> <mrow> <msup> <mrow> <mo>(</mo> <mrow> <munder> <mo></mo> <mi>r</mi> </munder> <mo></mo> <mrow> <msub> <mi>w</mi> <mi>r</mi> </msub> <mo></mo> <msub> <mi>x</mi> <mi>r</mi> </msub> </mrow> </mrow> <mo>)</mo> </mrow> <mn>2</mn> </msup> <mo></mo> <msup> <mrow> <mo>(</mo> <mrow> <munder> <mo></mo> <mi>r</mi> </munder> <mo></mo> <mrow> <msub> <mi>w</mi> <mi>r</mi> </msub> <mo></mo> <msub> <mi>y</mi> <mi>r</mi> </msub> </mrow> </mrow> <mo>)</mo> </mrow> <mn>2</mn> </msup> </mrow> <mrow> <mo></mo> <msub> <mi>w</mi> <mi>r</mi> </msub> </mrow> </mfrac> </mrow> </msqrt> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>23</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> </li> </ul> </li> </ul> <p num="p-0257">The moment of inertia factor takes into account the spread of the boundary points and their weights, which in turn take into account chain length, curvature, gradient magnitude, and parallel boundaries. The factor 
<maths id="MATH-US-00027" num="00027"> <math overflow="scroll"> <mtable> <mtr> <mtd> <msup> <mi></mi> <mrow> <mo>-</mo> <mrow> <mo></mo> <mrow> <mi>log</mi> <mo></mo> <mrow> <mo>(</mo> <mfrac> <mi>g</mi> <msub> <mi>g</mi> <mi>est</mi> </msub> </mfrac> <mo>)</mo> </mrow> </mrow> <mo></mo> </mrow> </mrow> </msup> </mtd> <mtd> <mrow> <mo>(</mo> <mn>24</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br>
discounts (or attenuates) the moment of inertia based on the ratio of g to the estimated granularity g<sub>est</sub>. The factor
<br>g<sup>1.625</sup>(25)<br>
compensates for the fact that all distances scale by g so that number of boundary points scales by g, moment of inertia by g<sup>2</sup>, and arc length l<sub>0.25 </sub>scales by g<sup>0.25</sup>, for total scale by g<sup>3.25</sup>. The square root in the formula for I makes the scale factor g<sup>1.625</sup>.
</p>
  <p num="p-0258"> <figref idrefs="DRAWINGS">FIG. 31</figref> is a flow chart showing how model granularity <b>1010</b> is selected based on ratings Q<sub>g</sub>. In step <b>3100</b> a granularity g<sub>best </sub>and rating Q<sub>best </sub>are initialized. Loop step <b>3105</b> scans all integer values of granularity n in the range 1 to g<sub>max</sub>, inclusive. For each loop interaction, variables q and g are initialized in step <b>3110</b>. The loop looks for maxima of Q, interpolates both rating and granularity at the maxima, and then chooses the interpolated granularity at the maximum with the highest interpolated rating.</p>
  <p num="p-0259">Steps <b>3115</b>, <b>3120</b>, and <b>3125</b> handle the case where n is 1, the smallest granularity considered. Step <b>3120</b> tests to see if n=1 is a maximum, and if so step <b>3125</b> is the interpolation for this case.</p>
  <p num="p-0260">Steps <b>3130</b>, <b>3135</b>, and <b>3140</b> handle the case where n is the largest granularity considered. Step <b>3135</b> tests for a maximum, and step <b>3140</b> is the interpolation.</p>
  <p num="p-0261">Steps <b>3143</b> and <b>3145</b> handle the case where n is neither the smallest nor largest granularity considered, and so a 3-point interpolation can be done. Step <b>3143</b> tests for a maximum. The formulas shown for step <b>3145</b> implement a 3-point parabolic interpolation similar to InterpPos and InterpScore, except that the domain of the parabola is log granularity instead of the usual linear scale.</p>
  <p num="p-0262">Steps <b>3150</b> and <b>3155</b> replace Q<sub>best </sub>and g<sub>best </sub>with q and g if a better rating q has been found, and step <b>3160</b> continues the loop at step <b>3105</b>.</p>
  <p num="p-0263">When the loop is finished, step <b>3170</b> sets model granularity <b>1010</b> to the interpolated granularity g<sub>best </sub>at the maximum with highest rating Q<sub>best</sub>.</p>
  <p num="p-0264">Other modifications and implementations will occur to those skilled in the art without departing from the spirit and the scope of the invention as claimed. Accordingly, the above description is not intended to limit the invention except as indicated in the following claims.</p>
</div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3069654">US3069654</a></td><td class="patent-data-table-td patent-date-value">Mar 25, 1960</td><td class="patent-data-table-td patent-date-value">Dec 18, 1962</td><td class="patent-data-table-td ">Hough Paul V C</td><td class="patent-data-table-td ">Method and means for recognizing complex patterns</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3936800">US3936800</a></td><td class="patent-data-table-td patent-date-value">Mar 27, 1974</td><td class="patent-data-table-td patent-date-value">Feb 3, 1976</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Pattern recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4200861">US4200861</a></td><td class="patent-data-table-td patent-date-value">Sep 1, 1978</td><td class="patent-data-table-td patent-date-value">Apr 29, 1980</td><td class="patent-data-table-td ">View Engineering, Inc.</td><td class="patent-data-table-td ">Pattern recognition apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4441206">US4441206</a></td><td class="patent-data-table-td patent-date-value">Dec 14, 1981</td><td class="patent-data-table-td patent-date-value">Apr 3, 1984</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Pattern detecting apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4567610">US4567610</a></td><td class="patent-data-table-td patent-date-value">Jul 22, 1982</td><td class="patent-data-table-td patent-date-value">Jan 28, 1986</td><td class="patent-data-table-td ">Wayland Research Inc.</td><td class="patent-data-table-td ">Method of and apparatus for pattern recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4637055">US4637055</a></td><td class="patent-data-table-td patent-date-value">May 25, 1984</td><td class="patent-data-table-td patent-date-value">Jan 13, 1987</td><td class="patent-data-table-td ">Pa Consulting Services Limited</td><td class="patent-data-table-td ">Adaptive pattern recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4651341">US4651341</a></td><td class="patent-data-table-td patent-date-value">Sep 13, 1983</td><td class="patent-data-table-td patent-date-value">Mar 17, 1987</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Pattern recognition apparatus and a pattern recognition method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4672676">US4672676</a></td><td class="patent-data-table-td patent-date-value">Dec 20, 1984</td><td class="patent-data-table-td patent-date-value">Jun 9, 1987</td><td class="patent-data-table-td ">International Business Machines Corp.</td><td class="patent-data-table-td ">Method and apparatus for automatically aligning an object with respect to a reference pattern</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4736437">US4736437</a></td><td class="patent-data-table-td patent-date-value">Apr 21, 1987</td><td class="patent-data-table-td patent-date-value">Apr 5, 1988</td><td class="patent-data-table-td ">View Engineering, Inc.</td><td class="patent-data-table-td ">High speed pattern recognizer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4783829">US4783829</a></td><td class="patent-data-table-td patent-date-value">Feb 22, 1984</td><td class="patent-data-table-td patent-date-value">Nov 8, 1988</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Pattern recognition apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4799175">US4799175</a></td><td class="patent-data-table-td patent-date-value">May 10, 1985</td><td class="patent-data-table-td patent-date-value">Jan 17, 1989</td><td class="patent-data-table-td ">Dainippon Screen Mfg., Co.</td><td class="patent-data-table-td ">System for inspecting pattern defects of printed wiring boards</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4823394">US4823394</a></td><td class="patent-data-table-td patent-date-value">Apr 24, 1986</td><td class="patent-data-table-td patent-date-value">Apr 18, 1989</td><td class="patent-data-table-td ">Kulicke &amp; Soffa Industries, Inc.</td><td class="patent-data-table-td ">Pattern recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4843631">US4843631</a></td><td class="patent-data-table-td patent-date-value">Dec 22, 1986</td><td class="patent-data-table-td patent-date-value">Jun 27, 1989</td><td class="patent-data-table-td ">Dietmar Steinpichler</td><td class="patent-data-table-td ">Pattern recognition process</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4955062">US4955062</a></td><td class="patent-data-table-td patent-date-value">Jul 18, 1989</td><td class="patent-data-table-td patent-date-value">Sep 4, 1990</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Pattern detecting method and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5046109">US5046109</a></td><td class="patent-data-table-td patent-date-value">Feb 1, 1990</td><td class="patent-data-table-td patent-date-value">Sep 3, 1991</td><td class="patent-data-table-td ">Nikon Corporation</td><td class="patent-data-table-td ">Pattern inspection apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5048094">US5048094</a></td><td class="patent-data-table-td patent-date-value">Oct 24, 1989</td><td class="patent-data-table-td patent-date-value">Sep 10, 1991</td><td class="patent-data-table-td ">Nippon Seiko Kabushiki Kaisha</td><td class="patent-data-table-td ">Method and apparatus for checking pattern</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5168530">US5168530</a></td><td class="patent-data-table-td patent-date-value">Aug 29, 1988</td><td class="patent-data-table-td patent-date-value">Dec 1, 1992</td><td class="patent-data-table-td ">Raytheon Company</td><td class="patent-data-table-td ">Confirmed boundary pattern matching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5220621">US5220621</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 31, 1991</td><td class="patent-data-table-td patent-date-value">Jun 15, 1993</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Character recognition system using the generalized hough transformation and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5253306">US5253306</a></td><td class="patent-data-table-td patent-date-value">Dec 31, 1990</td><td class="patent-data-table-td patent-date-value">Oct 12, 1993</td><td class="patent-data-table-td ">Futec Inc.</td><td class="patent-data-table-td ">Used for detecting a defect</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5313532">US5313532</a></td><td class="patent-data-table-td patent-date-value">Oct 28, 1991</td><td class="patent-data-table-td patent-date-value">May 17, 1994</td><td class="patent-data-table-td ">Massachusetts Institute Of Technology</td><td class="patent-data-table-td ">Recognition of patterns in images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5347595">US5347595</a></td><td class="patent-data-table-td patent-date-value">Aug 23, 1991</td><td class="patent-data-table-td patent-date-value">Sep 13, 1994</td><td class="patent-data-table-td ">Palantir Corporation (Calera Recognition Systems)</td><td class="patent-data-table-td ">Preprocessing means for use in a pattern classification system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5384711">US5384711</a></td><td class="patent-data-table-td patent-date-value">Nov 12, 1991</td><td class="patent-data-table-td patent-date-value">Jan 24, 1995</td><td class="patent-data-table-td ">Dainippon Screen Mfg. Co., Ltd.</td><td class="patent-data-table-td ">Method of and apparatus for inspecting pattern on printed board</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5471541">US5471541</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 16, 1993</td><td class="patent-data-table-td patent-date-value">Nov 28, 1995</td><td class="patent-data-table-td ">National Research Council Of Canada</td><td class="patent-data-table-td ">System for determining the pose of an object which utilizes range profiles and synethic profiles derived from a model</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5481712">US5481712</a></td><td class="patent-data-table-td patent-date-value">Apr 6, 1993</td><td class="patent-data-table-td patent-date-value">Jan 2, 1996</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and apparatus for interactively generating a computer program for machine vision analysis of an object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5495537">US5495537</a></td><td class="patent-data-table-td patent-date-value">Jun 1, 1994</td><td class="patent-data-table-td patent-date-value">Feb 27, 1996</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Methods and apparatus for machine vision template matching of images predominantly having generally diagonal and elongate features</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5537669">US5537669</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 1993</td><td class="patent-data-table-td patent-date-value">Jul 16, 1996</td><td class="patent-data-table-td ">Kla Instruments Corporation</td><td class="patent-data-table-td ">Inspection method and apparatus for the inspection of either random or repeating patterns</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5568563">US5568563</a></td><td class="patent-data-table-td patent-date-value">May 3, 1994</td><td class="patent-data-table-td patent-date-value">Oct 22, 1996</td><td class="patent-data-table-td ">Mitsubishi Denki Kabushiki Kaisha</td><td class="patent-data-table-td ">Method and apparatus of pattern recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5586058">US5586058</a></td><td class="patent-data-table-td patent-date-value">Apr 21, 1992</td><td class="patent-data-table-td patent-date-value">Dec 17, 1996</td><td class="patent-data-table-td ">Orbot Instruments Ltd.</td><td class="patent-data-table-td ">Apparatus and method for inspection of a patterned object by comparison thereof to a reference</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5602937">US5602937</a></td><td class="patent-data-table-td patent-date-value">Jun 1, 1994</td><td class="patent-data-table-td patent-date-value">Feb 11, 1997</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Methods and apparatus for machine vision high accuracy searching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5657403">US5657403</a></td><td class="patent-data-table-td patent-date-value">Jun 1, 1992</td><td class="patent-data-table-td patent-date-value">Aug 12, 1997</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Digital image processing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5717785">US5717785</a></td><td class="patent-data-table-td patent-date-value">May 9, 1994</td><td class="patent-data-table-td patent-date-value">Feb 10, 1998</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and apparatus for locating patterns in an optical image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5850466">US5850466</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 22, 1995</td><td class="patent-data-table-td patent-date-value">Dec 15, 1998</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Golden template comparison for rotated and/or scaled images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6324299">US6324299</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 3, 1998</td><td class="patent-data-table-td patent-date-value">Nov 27, 2001</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Object image search using sub-models</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6466923">US6466923</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 29, 1998</td><td class="patent-data-table-td patent-date-value">Oct 15, 2002</td><td class="patent-data-table-td ">Chroma Graphics, Inc.</td><td class="patent-data-table-td ">Method and apparatus for biomathematical pattern recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6658145">US6658145</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 22, 2000</td><td class="patent-data-table-td patent-date-value">Dec 2, 2003</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Fast high-accuracy multi-dimensional pattern inspection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6856698">US6856698</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 18, 2002</td><td class="patent-data-table-td patent-date-value">Feb 15, 2005</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Fast high-accuracy multi-dimensional pattern localization</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ballard, D.H., "<a href='http://scholar.google.com/scholar?q="Generalizing+the+Hough+Transform+to+Detect+Arbitrary+Shapes%2C"'>Generalizing the Hough Transform to Detect Arbitrary Shapes,</a>" Pattern Recognition, 1981, pp. 111-122, vol. 13, No. 2, Pergamon Press Ltd., UK.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ballard, et al., "<a href='http://scholar.google.com/scholar?q="Section+4.2+Searching+Near+and+Approximate+Location%2C%22+and+%22Section+4.3+The+Hough+Method+for+Curve+Detection%2C"'>Section 4.2 Searching Near and Approximate Location," and "Section 4.3 The Hough Method for Curve Detection,</a>" Computer Vision, 1982, pp. 121-131, Prentice-Hall, Inc., Englewood Cliffs, NJ, USA.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Brown, Lisa Gottesfeld, "<a href='http://scholar.google.com/scholar?q="A+Survey+of+Image+Registration+Techniques%2C"'>A Survey of Image Registration Techniques,</a>" ACM Computing Surveys, Dec. 1992, pp. 325-376, vol. 24, No. 4, Association for Computing Machinery, USA.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Caelli, et al., "<a href='http://scholar.google.com/scholar?q="Fast+Edge-Only+Matching+Techniques+for+Robot+Pattern+Recognition%2C"'>Fast Edge-Only Matching Techniques for Robot Pattern Recognition,</a>" Computer Vision, Graphics, and Image Processing 39, 1987, pp. 131-143, Academic Press, Inc.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Caelli, et al., "<a href='http://scholar.google.com/scholar?q="On+the+Minimum+Number+of+Templates+Required+for+Shift%2C+Rotation+and+Size+Invariant+Pattern+Recognition%2C"'>On the Minimum Number of Templates Required for Shift, Rotation and Size Invariant Pattern Recognition,</a>" Pattern Recognition, 1988, pp. 205-216, vol. 21, No. 3, Pergamon Press plc.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, "<a href='http://scholar.google.com/scholar?q="Apex+Model+Object%2C"'>Apex Model Object,</a>" acuWin version 1.5, Mar. 31, 1997, pp. 1-17, Natick, MA, USA.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, "<a href='http://scholar.google.com/scholar?q="Apex+Search+Object+Library+Functions%2C"'>Apex Search Object Library Functions,</a>" Natick, MA, USA, 1998 but public before the above-referenced filing date.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, "<a href='http://scholar.google.com/scholar?q="Apex+Search+Object%2C"'>Apex Search Object,</a>" acuWin version 1.5, Mar. 31, 1997, pp. 1-35, Natick, MA, USA.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, "<a href='http://scholar.google.com/scholar?q="Chapter+1+Searching%2C"'>Chapter 1 Searching,</a>" Cognex 3000/4000/5000 Programmable Vision Engines, Vision Tools, 1996, pp. 1-68, Revision 7.4 590-1036, Natick, MA, USA.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, "<a href='http://scholar.google.com/scholar?q="Chapter+14+Golden+Template+Comparison%2C"'>Chapter 14 Golden Template Comparison,</a>" Cognex 3000/4000/5000 Programmable Vision Engines, Vision Tools, 1996, pp. 569-595, Revision 7.4 590-1036, Natick, MA, USA.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, "<a href='http://scholar.google.com/scholar?q="Chapter+2+Searching%2C"'>Chapter 2 Searching,</a>" Cognex 2000/3000/4000 Vision Tools, 1992, pp. 2-1 to 2-62, Revision 5.2 P/N 590-0103, Natick, MA, USA.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, "<a href='http://scholar.google.com/scholar?q="Description+of+Sobel+Search%2C"'>Description of Sobel Search,</a>" Natick, MA, USA, 1998 but public before the above-referenced filing date.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Crouzil, et al., "<a href='http://scholar.google.com/scholar?q="A+New+Correlation+Criterion+Based+on+Gradient+Fields+Similarity%2C"'>A New Correlation Criterion Based on Gradient Fields Similarity,</a>" Proceedings of the 13&lt;SUP&gt;th &lt;/SUP&gt;International Conference on Pattern Recognition vol. I Track A: Computer Vision, Aug. 25-29, 1996, pp. 632-636, IEEE Computer Society Press, Los Alamitos, CA, USA.</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Grimson et al., "<a href='http://scholar.google.com/scholar?q="On+the+Sensitivity+of+the+Hough+Transform+for+Object+Recognition%2C"'>On the Sensitivity of the Hough Transform for Object Recognition,</a>" IEEE Transactions on Pattern Analysis and Machine Intelligence, Mar. 1990, pp. 255-274, vol. 12. No. 3.</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Hsieh et al., "<a href='http://scholar.google.com/scholar?q="Image+Registration+Using+a+New+Edge-Based+Approach%2C"'>Image Registration Using a New Edge-Based Approach,</a>" Computer Vision and Image Understanding, Aug. 1997, pp. 112-130, vol. 67, No. 2, Academic Press.</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Joseph. "<a href='http://scholar.google.com/scholar?q="Fast+Optimal+Pose+Estimation+for+Matching+in+Two+Dimensions."'>Fast Optimal Pose Estimation for Matching in Two Dimensions.</a>" 5&lt;SUP&gt;th &lt;/SUP&gt;Int. Conf. on Image Processing and Its Applications, Jul. 4, 1995, pp. 355-359.</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Rosenfeld et al., "<a href='http://scholar.google.com/scholar?q="Coarse-Fine+Template+Matching%2C"'>Coarse-Fine Template Matching,</a>" IEEE Transactions on Systems, Man, and Cybernetics, Feb. 1997, pp. 104-107, USA.</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Tian et al., "<a href='http://scholar.google.com/scholar?q="Algorithms+for+Subpixel+Registration%2C"'>Algorithms for Subpixel Registration,</a>" Computer Vision, Graphics, and Image Processing 35, 1986, pp. 220-233, Academic Press, Inc.</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7349567">US7349567</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 5, 2004</td><td class="patent-data-table-td patent-date-value">Mar 25, 2008</td><td class="patent-data-table-td ">Electro Scientific Industries, Inc.</td><td class="patent-data-table-td ">Method and apparatus for determining angular pose of an object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7379623">US7379623</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 26, 2004</td><td class="patent-data-table-td patent-date-value">May 27, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method to quickly warp a 2-D image using only integer math</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7397970">US7397970</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 7, 2004</td><td class="patent-data-table-td patent-date-value">Jul 8, 2008</td><td class="patent-data-table-td ">Lockheed Martin Corporation</td><td class="patent-data-table-td ">Automatic scene correlation and identification</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7474354">US7474354</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 30, 2003</td><td class="patent-data-table-td patent-date-value">Jan 6, 2009</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Image angle detection device and scan line interpolation device having the same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7483559">US7483559</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 13, 2004</td><td class="patent-data-table-td patent-date-value">Jan 27, 2009</td><td class="patent-data-table-td ">Synopsys, Inc.</td><td class="patent-data-table-td ">Method and apparatus for deblurring mask images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7529392">US7529392</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 20, 2003</td><td class="patent-data-table-td patent-date-value">May 5, 2009</td><td class="patent-data-table-td ">Koninklijke Philips Electronics N.V.</td><td class="patent-data-table-td ">Image processing system and medical examination apparatus for correlating features in medical images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7765517">US7765517</a></td><td class="patent-data-table-td patent-date-value">Oct 24, 2007</td><td class="patent-data-table-td patent-date-value">Jul 27, 2010</td><td class="patent-data-table-td ">Semiconductor Insights Inc.</td><td class="patent-data-table-td ">Method and apparatus for removing dummy features from a data structure</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7773769">US7773769</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 28, 2007</td><td class="patent-data-table-td patent-date-value">Aug 10, 2010</td><td class="patent-data-table-td ">University College Cardiff Consultants Limited</td><td class="patent-data-table-td ">Method of and apparatus for detecting degradation of visual performance</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7817871">US7817871</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 18, 2004</td><td class="patent-data-table-td patent-date-value">Oct 19, 2010</td><td class="patent-data-table-td ">Adobe Systems Incorporated</td><td class="patent-data-table-td ">Scaling of raster images without blurring of edges</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7857752">US7857752</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 26, 2007</td><td class="patent-data-table-td patent-date-value">Dec 28, 2010</td><td class="patent-data-table-td ">Olympus Corporation</td><td class="patent-data-table-td ">Medical image processing apparatus and medical image processing method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7886258">US7886258</a></td><td class="patent-data-table-td patent-date-value">Jun 15, 2010</td><td class="patent-data-table-td patent-date-value">Feb 8, 2011</td><td class="patent-data-table-td ">Semiconductor Insights, Inc.</td><td class="patent-data-table-td ">Method and apparatus for removing dummy features from a data structure</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8026951">US8026951</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 2, 2007</td><td class="patent-data-table-td patent-date-value">Sep 27, 2011</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Image processing device and method, recording medium, and program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8063350">US8063350</a></td><td class="patent-data-table-td patent-date-value">Jul 31, 2008</td><td class="patent-data-table-td patent-date-value">Nov 22, 2011</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Circuits and methods allowing for pixel array exposure pattern control</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8103127">US8103127</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 3, 2007</td><td class="patent-data-table-td patent-date-value">Jan 24, 2012</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Image processing method and image processing apparatus of calculating position and orientation of target objects located in image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8145656">US8145656</a></td><td class="patent-data-table-td patent-date-value">Feb 4, 2007</td><td class="patent-data-table-td patent-date-value">Mar 27, 2012</td><td class="patent-data-table-td ">Mobixell Networks Ltd.</td><td class="patent-data-table-td ">Matching of modified visual and audio media</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8181876">US8181876</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 27, 2008</td><td class="patent-data-table-td patent-date-value">May 22, 2012</td><td class="patent-data-table-td ">Datalogic ADC, Inc.</td><td class="patent-data-table-td ">Methods and systems for forming images of moving optical codes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8219940">US8219940</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 6, 2005</td><td class="patent-data-table-td patent-date-value">Jul 10, 2012</td><td class="patent-data-table-td ">Semiconductor Insights Inc.</td><td class="patent-data-table-td ">Method and apparatus for removing dummy features from a data structure</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8233670">US8233670</a></td><td class="patent-data-table-td patent-date-value">Dec 31, 2007</td><td class="patent-data-table-td patent-date-value">Jul 31, 2012</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">System and method for traffic sign recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8238639">US8238639</a></td><td class="patent-data-table-td patent-date-value">Apr 9, 2008</td><td class="patent-data-table-td patent-date-value">Aug 7, 2012</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and system for dynamic feature detection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8411929">US8411929</a></td><td class="patent-data-table-td patent-date-value">Aug 1, 2012</td><td class="patent-data-table-td patent-date-value">Apr 2, 2013</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and system for dynamic feature detection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8422759">US8422759</a></td><td class="patent-data-table-td patent-date-value">Feb 17, 2011</td><td class="patent-data-table-td patent-date-value">Apr 16, 2013</td><td class="patent-data-table-td ">Omron Corporation</td><td class="patent-data-table-td ">Image processing method and image processing device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8457390">US8457390</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 10, 2008</td><td class="patent-data-table-td patent-date-value">Jun 4, 2013</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and apparatus for training a probe model based machine vision system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8467596">US8467596</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 30, 2011</td><td class="patent-data-table-td patent-date-value">Jun 18, 2013</td><td class="patent-data-table-td ">Seiko Epson Corporation</td><td class="patent-data-table-td ">Method and apparatus for object pose estimation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8570393">US8570393</a></td><td class="patent-data-table-td patent-date-value">May 17, 2010</td><td class="patent-data-table-td patent-date-value">Oct 29, 2013</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">System and method for processing image data relative to a focus of attention within the overall image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8705851">US8705851</a></td><td class="patent-data-table-td patent-date-value">Jan 3, 2013</td><td class="patent-data-table-td patent-date-value">Apr 22, 2014</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and apparatus for training a probe model based machine vision system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20070011628">US20070011628</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 6, 2005</td><td class="patent-data-table-td patent-date-value">Jan 11, 2007</td><td class="patent-data-table-td ">Semiconductor Insights Inc.</td><td class="patent-data-table-td ">Method and apparatus for removing dummy features from a data structure</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090040237">US20090040237</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 10, 2008</td><td class="patent-data-table-td patent-date-value">Feb 12, 2009</td><td class="patent-data-table-td ">Alcatel Lucent</td><td class="patent-data-table-td ">Method for tracking moving entities</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100004857">US20100004857</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 2, 2008</td><td class="patent-data-table-td patent-date-value">Jan 7, 2010</td><td class="patent-data-table-td ">Palm, Inc.</td><td class="patent-data-table-td ">User defined names for displaying monitored location</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120082385">US20120082385</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 2010</td><td class="patent-data-table-td patent-date-value">Apr 5, 2012</td><td class="patent-data-table-td ">Sharp Laboratories Of America, Inc.</td><td class="patent-data-table-td ">Edge based template matching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130142421">US20130142421</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 19, 2012</td><td class="patent-data-table-td patent-date-value">Jun 6, 2013</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method for Fast, Robust, Multi-Dimensional Pattern Recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE102012223047A1?cl=en">DE102012223047A1</a></td><td class="patent-data-table-td patent-date-value">Dec 13, 2012</td><td class="patent-data-table-td patent-date-value">Jul 11, 2013</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Mehrteil-Korrespondierer fr mehrere Kameras</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2191440A1?cl=en">EP2191440A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 27, 2008</td><td class="patent-data-table-td patent-date-value">Jun 2, 2010</td><td class="patent-data-table-td ">Riverain Medical Group, LLC</td><td class="patent-data-table-td ">Object segmentation using dynamic programming</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2012092132A2?cl=en">WO2012092132A2</a></td><td class="patent-data-table-td patent-date-value">Dec 22, 2011</td><td class="patent-data-table-td patent-date-value">Jul 5, 2012</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Determining the uniqueness of a model for machine vision</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=_y5yBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S216000">382/216</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=_y5yBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S291000">382/291</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=_y5yBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009620000">G06K9/62</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=_y5yBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/481">G06K9/481</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=_y5yBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/6204">G06K9/6204</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=_y5yBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/6201">G06K9/6201</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06K9/62A1A1</span>, <span class="nested-value">G06K9/48A</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Sep 5, 2013</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 5, 2010</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIMS 1, 6, 8-11 AND 19 ARE DETERMINED TO BE PATENTABLE AS AMENDED. CLAIMS 2-5, 12, 17, 18, 20, 21, 24, 25 AND 33-35, DEPENDENT ON AN AMENDED CLAIM, ARE DETERMINED TO BE PATENTABLE. CLAIMS 7, 13-16, 22, 23 AND 26-32 WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 22, 2009</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 22, 2009</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 11, 2009</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090622</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 5, 2003</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COGNEX TECHNOLOGY AND INVESTMENT CORPORATION, CALI</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SILVER, WILLIAM;MCGARRY, E. JOHN;HILL, MATTHEW;AND OTHERS;REEL/FRAME:014019/0472;SIGNING DATES FROM 20020424 TO 20030408</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U2P2XagP0sbzVKgu72vr1MKGjLvWQ\u0026id=_y5yBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U2UH7ExwlYfMP8JpU9ayIZ-mGADcA\u0026id=_y5yBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U3FCCjNmS2DYvZjKlQwVRSm8UDyIw","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Method_for_fast_robust_multi_dimensional.pdf?id=_y5yBAABERAJ\u0026output=pdf\u0026sig=ACfU3U1n_atGd2cQpH-D-ngVWpf0CWqVFg"},"sample_url":"http://www.google.com/patents/reader?id=_y5yBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>