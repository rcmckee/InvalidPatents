<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US5713740 - System and method for converting written text into a graphical image for ... - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="System and method for converting written text into a graphical image for improved comprehension by the learning disabled"><meta name="DC.contributor" content="R. David Middlebrook" scheme="inventor"><meta name="DC.contributor" content="Middlebrook; R. David" scheme="assignee"><meta name="DC.date" content="1996-6-3" scheme="dateSubmitted"><meta name="DC.description" content="A system and method which enables individuals to rapidly and accurately obtain information about the contents of a written text without having to read the words of the text. Written text, which is comprised of graphic markings that represent linguistic sound, forms a graphic image. This graphic image can be mapped in various ways to illustrate and provide insight about the structure and content of the text with regard to one or more selected features. Individuals can perceive the graphic information contained in the text using parafoveal and peripheral vision, thereby enabling those individuals to process the graphic information using visual/spacial cognitive abilities. The present invention system and method therefore enables a person to understand a large amount of information about the body of written text without reading the words comprising the body of written text. The image of the text is then mapped in various ways to illustrate, without words, the various features of the text that provide insight into the contents of the text."><meta name="DC.date" content="1998-2-3" scheme="issued"><meta name="DC.relation" content="US:4661074" scheme="references"><meta name="DC.relation" content="US:4907971" scheme="references"><meta name="DC.relation" content="US:5057020" scheme="references"><meta name="DC.relation" content="US:5306153" scheme="references"><meta name="DC.relation" content="US:5336093" scheme="references"><meta name="citation_patent_number" content="US:5713740"><meta name="citation_patent_application_number" content="US:08/655,699"><link rel="canonical" href="http://www.google.com/patents/US5713740"/><meta property="og:url" content="http://www.google.com/patents/US5713740"/><meta name="title" content="Patent US5713740 - System and method for converting written text into a graphical image for improved comprehension by the learning disabled"/><meta name="description" content="A system and method which enables individuals to rapidly and accurately obtain information about the contents of a written text without having to read the words of the text. Written text, which is comprised of graphic markings that represent linguistic sound, forms a graphic image. This graphic image can be mapped in various ways to illustrate and provide insight about the structure and content of the text with regard to one or more selected features. Individuals can perceive the graphic information contained in the text using parafoveal and peripheral vision, thereby enabling those individuals to process the graphic information using visual/spacial cognitive abilities. The present invention system and method therefore enables a person to understand a large amount of information about the body of written text without reading the words comprising the body of written text. The image of the text is then mapped in various ways to illustrate, without words, the various features of the text that provide insight into the contents of the text."/><meta property="og:title" content="Patent US5713740 - System and method for converting written text into a graphical image for improved comprehension by the learning disabled"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("sI_tU_7HJbfcsATk2IHAAg"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("USA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("sI_tU_7HJbfcsATk2IHAAg"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("USA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us5713740?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US5713740"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=0SZFBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS5713740&amp;usg=AFQjCNFh82e48vElBTl0TGsYDp6k3DGSig" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US5713740.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US5713740.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US5713740" style="display:none"><span itemprop="description">A system and method which enables individuals to rapidly and accurately obtain information about the contents of a written text without having to read the words of the text. Written text, which is comprised of graphic markings that represent linguistic sound, forms a graphic image. This graphic image...</span><span itemprop="url">http://www.google.com/patents/US5713740?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US5713740 - System and method for converting written text into a graphical image for improved comprehension by the learning disabled</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US5713740 - System and method for converting written text into a graphical image for improved comprehension by the learning disabled" title="Patent US5713740 - System and method for converting written text into a graphical image for improved comprehension by the learning disabled"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US5713740 A</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 08/655,699</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Feb 3, 1998</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Jun 3, 1996</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Jan 18, 1994</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US5556282">US5556282</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1995019616A1">WO1995019616A1</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">08655699, </span><span class="patent-bibdata-value">655699, </span><span class="patent-bibdata-value">US 5713740 A, </span><span class="patent-bibdata-value">US 5713740A, </span><span class="patent-bibdata-value">US-A-5713740, </span><span class="patent-bibdata-value">US5713740 A, </span><span class="patent-bibdata-value">US5713740A</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22R.+David+Middlebrook%22">R. David Middlebrook</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Middlebrook%3B+R.+David%22">Middlebrook; R. David</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US5713740.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5713740.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5713740.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (5),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (9),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (13),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (7)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=0SZFBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/5713740&usg=AFQjCNF9HvdVJXkD-0jquBvzUCv4ciBeNg">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=0SZFBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D5713740&usg=AFQjCNFC-qLSVbJJs612QV6glvjUhVrqNA">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=0SZFBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D5713740A%26KC%3DA%26FT%3DD&usg=AFQjCNFj1WI8zWoKAXBkcDe3hxKr45n5JA">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54254500" lang="EN" load-source="patent-office">System and method for converting written text into a graphical image for improved comprehension by the learning disabled</invention-title></span><br><span class="patent-number">US 5713740 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA37724684" lang="EN" load-source="patent-office"> <div class="abstract">A system and method which enables individuals to rapidly and accurately obtain information about the contents of a written text without having to read the words of the text. Written text, which is comprised of graphic markings that represent linguistic sound, forms a graphic image. This graphic image can be mapped in various ways to illustrate and provide insight about the structure and content of the text with regard to one or more selected features. Individuals can perceive the graphic information contained in the text using parafoveal and peripheral vision, thereby enabling those individuals to process the graphic information using visual/spacial cognitive abilities. The present invention system and method therefore enables a person to understand a large amount of information about the body of written text without reading the words comprising the body of written text. The image of the text is then mapped in various ways to illustrate, without words, the various features of the text that provide insight into the contents of the text.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(8)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5713740-1.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5713740-1.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5713740-2.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5713740-2.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5713740-3.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5713740-3.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5713740-4.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5713740-4.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5713740-5.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5713740-5.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5713740-6.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5713740-6.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5713740-7.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5713740-7.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5713740-8.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5713740-8.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(20)</span></span></div><div class="patent-text"><div mxw-id="PCLM5184755" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is:</claim-statement> <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1. A method of producing a representation of text to enable a person to obtain some comprehension of said text without reading all of said text, comprising the steps of:<div class="claim-text">identifying at least one feature contained within at least a portion of said text;</div> <div class="claim-text">creating at least one representation of said portion of said text, wherein said representation of said portion of said text does not include any readable words but does include a graphical indication that indicates the presence of said at least one feature at at least one location within said at least one representation.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2. The method according to claim 1, wherein said step of identifying at least one feature includes identifying the most frequently occurring noun within said portion of said text.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3. The method according to claim 1, wherein said step of identifying at least one feature includes identifying the most frequently occurring nouns within said portion of said text.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4. The method according to claim 3, wherein said step of creating at least one representation includes creating multiple representations of said portion of said text wherein each of said representations includes a graphical indication that indicates the presence of one of said frequently occurring nouns at at least one location therein.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5. The method according to claim 4, further including the step of creating a composite representation from each of said representations, wherein said composite representation illustrates areas within said portion of said text common to at least two of said representations.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6. The method according to claim 3, wherein said step of creating at least one representation includes creating a representation that indicates locations within said portion of said text that contain more than one frequently occurring nouns.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7. The method according to claim 1, wherein said step of identifying at least one feature includes the substeps of:<div class="claim-text">analyzing all feature instances within said portion of said text;</div> <div class="claim-text">eliminating from consideration features having only a single instance;</div> <div class="claim-text">eliminating from consideration features that are not possibly nouns, thereby resulting in a group of eligible features that are possibly nouns and occur more than once in said portion of said text.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8. The method according to claim 7, further including the substeps of:<div class="claim-text">categorizing said eligible feature into groups sharing substantially the same meaning as a dominant word within each group;</div> <div class="claim-text">considering all words within one of said groups to be the same as the dominant word for that group;</div> <div class="claim-text">identifying the most frequently occurring dominant words within said portion of said text.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9. The method according to claim 8, wherein said step of creating at least one representation includes creating a representation for each of said most frequently occurring dominant words.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" class="claim">
      <div class="claim-text">10. The method according to claim 9, further including the step of creating a composite representation from each representation for each of said most frequently occurring dominant words, wherein said composite representation illustrates areas within said portion of said text common to at least two of said dominant words.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" class="claim">
      <div class="claim-text">11. The method according to claim 1, wherein said step of creating at least one representation of said portion of said text includes the substeps of:<div class="claim-text">reducing said portion of said text to an illegible size;</div> <div class="claim-text">displaying the reduced text in an uninterrupted format on a common backing.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" class="claim">
      <div class="claim-text">12. The method according to claim 1, further including the step of displaying said at least one representation in a three dimensional format.</div>
    </div>
    </div> <div class="claim"> <div num="13" class="claim">
      <div class="claim-text">13. A method of producing a representation of text contained within a document to enable a person to obtain some comprehension of said text without reading said text, comprising the steps of:<div class="claim-text">creating an image of said text wherein individual words of said text are not discernable within said image;</div> <div class="claim-text">identifying at least one textual feature contained within said text;</div> <div class="claim-text">illustrating on said image the areas of said text that contain said at least one textual feature.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" class="claim">
      <div class="claim-text">14. The method according to claim 13, wherein said step of creating an image includes displaying all of said text as a single image.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" class="claim">
      <div class="claim-text">15. The method according to claim 13, wherein said step of identifying at least one textual feature includes identifying words in said text having generally the same definition.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" class="claim">
      <div class="claim-text">16. The method according to claim 13, wherein said step of identifying at least one textual feature includes identifying multiple textual features.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" class="claim">
      <div class="claim-text">17. The method according to claim 16, wherein said step of illustrating on said image the areas of said text that contain said at least one textual feature includes illustrating the areas of said text that contain at least two of said textual features.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" class="claim">
      <div class="claim-text">18. The method according to claim 13, wherein said step of identifying at least one textual feature includes the substeps of:<div class="claim-text">analyzing all feature instances within said text;</div> <div class="claim-text">eliminating from consideration features having only a single instance;</div> <div class="claim-text">eliminating from consideration features that are not possibly nouns, thereby resulting in a group of eligible words that are possibly nouns and occur more than once in said text.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" class="claim">
      <div class="claim-text">19. The method according to claim 18, further including the substeps of:<div class="claim-text">categorizing said eligible words into groups sharing substantially the same meaning as a dominant word within each group;</div> <div class="claim-text">considering all words within one of said groups to be the same as the dominant word for that group;</div> <div class="claim-text">identifying the most frequently occurring dominant words within said text.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" class="claim">
      <div class="claim-text">20. The method according to claim 19, wherein said step of illustrating on said image the areas of said text that contain said at least one textual feature includes illustrating the areas of said text that contain at least two of said most frequently occurring dominant words.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES67073815" lang="EN" load-source="patent-office" class="description">
    <heading>RELATED APPLICATIONS</heading> <p>This application is a continuation-in-part of U.S. patent application Ser. No. 08/184,493, filed Jan. 18, 1994, U.S. Pat. No. 5,556,282, by the inventor herein and entitled METHOD FOR THE GEOGRAPHICAL PROCESSING OF GRAPHIC LANGUAGE TEXTS.</p>
    <heading>BACKGROUND OF THE INVENTION</heading> <p>1. Field of the Invention</p>
    <p>The present invention relates to systems and methods relates to systems and methods which enable individuals to quickly and accurately obtain information about the contents of a written text without having to read the words of the text.</p>
    <p>2. Prior Art Statement</p>
    <p>Reading and writing require the extensive use of a number of genetically inherited abilities. Some of the most important of these abilities are foveal vision, auditory processing and sequential processing.</p>
    <p>Traditional reading techniques regard text as sound. The standard approach to reading calls for text to be perceived by means of foveal vision, decoded as auditory information, structured as auditory sequences and comprehended as auditorally-based abstract concepts. Since it is these very abilities that are often deficient in a learning disabled person, it can be seen why it is so difficult for a learning disabled person to efficiently read written text. In attempting to teach the learning disabled, educators have often tried correct the deficits and deficiencies of an individual. Since such deficits and deficiencies are genetic in nature, such attempts often find only limited success. In contrast, the present invention focuses upon the abilities of the learning disabled person that are not deficient. As will be explained, the present invention does not attempt to change the abilities of the individual, rather it changes the way written text is presented so that learning disabled people who have normal abilities in the areas of spatial processing and/or parafoveal vision can use these abilities to better comprehend text.</p>
    <p>It is therefore an objective of the present invention to provide a system and method to enable learning disabled individuals to use their visual/spatial abilities and/or parafoveal/peripheral vision instead of, or in addition to, their auditory abilities and/or foveal vision for comprehending written text.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>A system and method which enables individuals to rapidly and accurately obtain information about the contents of a written text without having to read the words of the text. Written text, which is comprised of graphic markings that represent linguistic sound, forms a graphic image. This graphic image can be mapped in various ways to illustrate and provide insight about the structure and content of the text with regard to one or more selected features. Individuals can perceive the graphic information contained in the text using parafoveal and peripheral vision, thereby enabling those individuals to process the graphic information using visual/spacial cognitive abilities. The present invention system and method therefore enables a person to understand a large amount of information about the body of written text without reading the words comprising the body of written text. The image of the text is then mapped in various ways to illustrate, without words, the various features of the text that provide insight into the contents of the text.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p>For a better understanding of the present invention, reference is made to the following description of an exemplary embodiment thereof, considered in conjunction with the accompanying drawings, in which:</p>
    <p>FIG. 1 is a schematic view of one preferred embodiment of the present invention;</p>
    <p>FIG. 2 shows a first base typographic map of an exemplary textscape produced by the embodiment of FIG. 1;</p>
    <p>FIG. 3 shows a second base typographic map of an exemplary textscape produced by the embodiment of FIG. 1;</p>
    <p>FIG. 4 shows a detail typographic map of an exemplary textscape produced by the embodiment of FIG. 1;</p>
    <p>FIG. 5 shows a section-specific detail typographic map 60 of the fourth section of the detail typographic map shown in FIG. 4;</p>
    <p>FIG. 6 shows a chorochromatic map of the fourth section of the detail typographic map shown in FIG. 4;</p>
    <p>FIG. 7 shows a spread map for the second most prevalent noun contained within the fourth section of the detail typographic map shown in FIG. 4;</p>
    <p>FIG. 8 shows a spread map for the first most prevalent noun contained within the fourth section of the detail typographic map shown in FIG. 4;</p>
    <p>FIG. 9 shows a spread map for the third most prevalent noun contained within the fourth section of the detail typographic map shown in FIG. 4;</p>
    <p>FIG. 10 shows a spread map for the fourth most prevalent noun contained within the fourth section of the detail typographic map shown in FIG. 4;</p>
    <p>FIG. 11 shows a composite spread map of the fourth section of the detail typographic map shown in FIG. 4, wherein the composite spread map shows the area of intersection should the maps of FIG. 7, FIG. 8, FIG. 9, and FIG. 10 be combined; and</p>
    <p>FIG. 12 shows a three-dimensional representation of the composite spread map shown in FIG. 11.</p>
    <heading>DETAILED DESCRIPTION OF THE INVENTION</heading> <p>The present invention system and method enables individuals to capitalize on the graphic information inherent in written language texts in order to improve reading speed and comprehension. Written text, which is comprised of graphic markings that represent linguistic sound, forms a graphic image. This graphic image can be mapped in various ways to illustrate and provide insight about the structure and content of the text with regard to one or more selected features. Individuals can perceive the graphic information contained in the text using parafoveal and peripheral vision, thereby enabling those individuals to process the graphic information using visual/spacial cognitive abilities. The present invention system and method therefore enables a person to understand a large amount of information about the body of written text without reading the words comprising the body of written text.</p>
    <p>The present invention system and method changes a body of written text into a graphic image that does not contain readable words, or contains very few readable words. By converting a body of text into an image, the body of text can be analyzed by using a person's visual/spatial abilities and parafoveal/peripheral vision. Since the visual/spatial abilities and parafoveal/peripheral vision of a learning disabled person are often normal, the present invention system and method enables a disabled person to understand a large amount of information about the body of written text without reading the words comprising the body of written text.</p>
    <p>Referring to FIG. 1, an overview of one preferred embodiment of the present invention system 10 is shown. The purpose of the system 10 is to convert the text from a written body of text 12 into a graphic image that does not contain words that are individually decipherable. For the purposes of this disclosure, any graphic representation of written text is referred to as a "textscape". A textscape may contain recognizable words, or it may contain space and/or graphic markings in place of one or more words. In a textscape which contains words the words may or may not be individually decipherable. In the shown embodiment, a body of text 12 is scanned into a computer 14 using a scanner 16. The body of text 12 can be any written material, such as a book, newspaper, magazine, musical score or the like. Once a desired section of text is read into the computer 14, the computer 14 converts the written text into a corresponding textscape 20. The textscape 20 can then be either viewed on the display 15 of the computer 14 or printed onto paper by a printer 18 coupled to the computer 14.</p>
    <p>It will be understood that the production of a textscape 20, from a body of text, using a scanner 16 and a computer 14 is merely exemplary and other means can be employed. For example, text need not be scanned into the computer 14, but rather can be read from a disc or electronically sent to the computer via a modem or other network connection. Furthermore, the use of the computer 14, scanner 16 and printer 18 can be eliminated by manually producing the textscape 20, as indicated in FIG. 1 by line 22. Manually producing the textscape 20 may include copying the text and placing the graphic images on a single display to create the textscape 20.</p>
    <p>The textscape 20, being a graphic representation of a section of the body of text 12, is essentially a map of the selected section of the body of text 12. As will be explained, different maps of the textscape 20 are created to graphically depict different things about the text 12. By reading one or more mappings of a particular textscape, a learning disabled person may learn a great deal about the organization and context of the text without reading the text, in the same way that a person can learn about a city by viewing different types of maps for the same city. For example, by reading a topographic map, road map, bus route map, tourism map, tax map and political precinct map for a city, a person can learn about the size of a city, the important parts of the city, the business sections and residential sections of the city and the like without ever having to read text describing the city. As will be explained, by viewing different maps of a common textscape 20, a learning disabled person is able to tell what a given body of text is about, what parts of the text address different topics, how large the body of text is, and a great deal of other information about the body of text 12. The information obtained about the body of text 12 is obtained without having to read the words contained within the body of text 12.</p>
    <p>Referring to FIG. 2, there is shown a first base typographic map 22 of a textscape 20. In the example shown, the textscape 20 was created from a forty one paragraph article about labor wages in the housing and manufacturing industries. The textscape 20 is a single image that contains the entire article. If the textscape 20 is to be printed on paper, it is preferably made as a single sheet of paper or scroll of paper that can be scanned all at once by the eyes. If the textscape 20 is viewed on an electronic display, the textscape 20 is formatted so that it can be rapidly scrolled across the screen from one end of the textscape 20 to the other, either with a side-to-side motion or with a up-and-down motion.</p>
    <p>To create the base typographic map 22 of the textscape 20, shown in FIG. 1, the text of the article is reduced in size until the entire text can be viewed as a continuous whole within the confines of the display area. This may require that the text be reduced to a size where the majority of the text cannot be read by the eye. The entire reduced sized article is then displayed as a single display. The base typographic map 22 provides its viewer with basic information about the length and structure of the text, as well as the flow of information contained within the text. For example, the length of the text can be seen from the base typographic map 22. Furthermore, it can be seen that the text includes two illustrations 24, 26 and forty one paragraphs 28. From the base typographic map 22, it can also be seen that two sections 32, 33 of a paragraph 28 near the center of the article were set by the typographer with a more narrow column width than the other columns. The narrow sections 32, 33 may be assumed to be a quotation since it is a common typographic convention to indent block quotations within a body of text.</p>
    <p>Six small block areas 34 are apparent when viewing the base typographic map 22. The six small block areas 34 are graphical representations of six large alphabetic characters used within the body of text. Typographers commonly use oversized alphabetic characters to mark points of transition within a text. In the base typographic map 22, the six small block areas 34 are identical in size and color. As such, a person viewing the six small block areas 34 can assume that the alphabetic characters that the six small block areas 34 represent are of the same typeface style and point size. The six small block areas 34 indicate that the text is divided into six separate sections, wherein the forty one paragraphs 28 comprising the full text are unequally divided among the six sections.</p>
    <p>The base typographic map 22 in FIG. 2 does not contain readable alphabetic characters. Consequently, the use of foveal vision is not necessary to analyze the textscape 20. Furthermore, the base typographic map 22 contains no words to be converted into sound information, which means that with regard to reading, auditory processing ability is of no use. The base typographic map 22, however, may be easily read using parafoveal/peripheral vision and visual/spatial processing.</p>
    <p>Referring to FIG. 3, a second base typographic map 40 is shown for the textscape 20. As with the base typographic map 22 in FIG. 2, the second base typographic map 40 shows the two illustrations 24, 26 contained within the article. The sections 32, 33 of the block quotation are shown as is the title block 31. The six small block areas 34 are also shown, wherein each of the block areas 34 represents a single alphabetic character that marks the beginning of a new section of text. The second base typographic map 40 of FIG. 2, however, does not show the individual paragraphs contained within the text. Rather, the text is divided into six sections 41, 42, 43, 44, 45, 46 wherein each section begins with one of the six small block areas 34. Each of the six sections 41, 42, 43, 44, 45, 46 is blocked so that a person viewing the second base typographic map 40 can see the size of each of the six sections 41, 42, 43, 44, 45, 46, but cannot tell how many paragraphs are contained within each of the sections 41, 42, 43, 44, 45, 46.</p>
    <p>Referring to FIG. 4, a detail typographic map 50 is shown for the textscape 20. To create the detail typographic map 50, a target text feature is selected. A target text feature can be a word, punctuation symbol, space or indentation. In the present example, the target feature is a base word. The process of selection for the target base word will later be explained. For purposes of illustration, the term "house" was selected as well as its related terms "houses" and "housing". The location of these related words are then graphically depicted on the detail typographic map 50. The term "house" is assigned the graphic symbol of an oval 52. The term "houses" is assigned the graphic symbol of a rectangle 54. Lastly, the term "housing" is assigned the graphic symbol of a triangle 56. The graphic symbols of the oval 52, rectangle 54 and triangle 56 appear in the detail typographic map 50 at the positions where the terms "house", "houses" and "housing", respectively, would have appeared in the text.</p>
    <p>The most obvious information provided by the detail typographic map 50 is the location of terms "house", "houses" and "housing" in the textscape 20. All three terms are located within the fourth section 44 of the mapped text of the textscape 20. None of the other text sections 41, 42, 43, 45, 46 contain any references to "house", "houses" or "housing". On the basis of this graphic information, a person viewing the detail typographic map 50 can logically infer that the topic of house/houses/housing is discussed in the fourth section 44 of the mapped text and is probably not discussed anywhere else.</p>
    <p>Another piece of information provide by the detail typographic map 50 concerns the distinction between the topic "houses", the topic "house" and the topic "housing". In the shown embodiment, the terms "house", "houses" and "housing" are each symbolized differently. Because the distinction between these three terms is noted on the detail typographic map 50, it can be clearly seen that there are six instances of "housing", two instances of "houses" and one instance of "house". Moreover, these instances do not appear to be clustered or segregated in any way which would suggest that "houses" is specific to one location, "housing" to another and "house" to yet another. On the basis of this information, one might reasonably infer that the sense of house/houses/housing in this context is most closely related to the general topic "housing" (which appears six times) and not to the general topics "houses" (which appears two times) or "house" (which appears only one time).</p>
    <p>The detail typographic map 50 does not, however, provide information regarding the relative importance of the topic "housing". The fact that "housing" is discussed is clear. The fact that it is an important topic, or even the main topic, within the context of the fourth section 44 of the mapped text is not clear. The most that can be said is that the detail typographic map 50 shows that the words house/houses/housing do appear in the fourth section 44 of the mapped text and that they are concentrated in, and thus probably specific to, the fourth section 44 of the mapped text.</p>
    <p>Referring to FIG. 5, a section-specific detail typographic map 60 is shown, wherein the section specific detail typographic map 60 shows only the fourth section 44 of the detail typographic map 50 shown in FIG. 4. By isolating the section of the mapped text containing certain key words, a viewer can better understand the overall importance of those key words in the context of the section, as well as in the context of the entire text. In FIG. 4, the paragraphs within each of the six sections of text were not shown. In FIG. 5, the paragraphs contained within the fourth section 44 of the mapped text are reintroduced. As can be seen, the section specific detail typographic map 60 shows that the fourth section 44 of the mapped text contains seven paragraphs 61, 62, 63, 64, 65, 66, 67. The oval 52 representing the term "house", the rectangles 54 representing the term "houses" and the triangles 56 representing the term "housing" are shown in the context of the seven paragraphs 61, 62, 63, 64, 65, 66, 67. As such, from the section specific detail typographic map 60, it can be seen that only three of the paragraphs contain one or more occurrences of "house", "houses" or "housing". No information is provided about the paragraphs that do not contain a reference to "house", "houses" or "housing".</p>
    <p>Referring to FIG. 6, a chorochromatic map 70 is shown of the fourth section 44 of the originally mapped text. In the chorochromatic map 70, the paragraphs 64, 65, 66 that contain a reference to the target terms, herein "house", "houses" or "housing", are hatched so as to become highly noticeable. As a result, a person viewing the chorochromatic map 70 can instantly tell which of the paragraphs contain the target terms. In the shown embodiment, the presence of the target term "house" or "houses" in a paragraph is indicated by the use of oblique hatching, such as is shown in paragraph 64 and paragraph 66. The presence of the target term "housing" in a paragraph is indicated by the use of reverse oblique hatching, as is shown in paragraph 65. As such, it will be understood that different hatchings can be used to indicate the presence or absence of different target terms in any of the paragraphs mapped.</p>
    <p>Paragraphs that are not hatched do not contain occurrences of the target terms. With regard to the empty locations, it is important to consider the possibility that the information not mapped, i.e. the information which exists in the empty areas is more important than the nine instances of house/houses/housing already mapped. The problem is to find a way to get at and sort through the information in the empty areas without having to use foveal vision and auditory and sequential processing, i.e. without having to read the words of the text. One way of accomplishing this is to treat the text as statistical spatial data and to employ statistical techniques to sort, classify and select for mapping only those textual features which are likely to yield useful and relevant information about the text.</p>
    <p>The detail typographic maps shown in FIGS. 4 and 5 and the chorochromatic map shown in FIG. 6 only convey to a viewer the location of certain target terms within the larger spatial context. No distinction is made as to the significance or insignificance of a target term.</p>
    <p>By using statistical textmapping, the significance of a target term can be better interpreted. The primary factors used in statistical textmapping are frequency and location of target terms. Secondary factors include the graphic attributes, phonetic attributes, meaning and usage of the target terms.</p>
    <p>The primary factor of frequency refers to the number of times a target term appears in a body of text. Frequency may be expressed in terms of a number, a percentage or a ratio. The primary factor of location refers to the physical location of a target term in a body of text. The location is definable as either a point or a zone. A point is where the target term actually appears. A zone is defined as any area larger than a point, such as the area occupied by a textual feature itself or the area occupied by a phrase, a clause, a sentence, a paragraph, a section, a chapter, an entire text or a collection of texts.</p>
    <p>Information about frequency and location may be combined to produce information regarding a selected textual feature's distribution. For example, information on frequency and location can answer the question: "Does the textual feature appear more or less evenly throughout the text?". More specifically, such information can answer the questions: "Does the feature appear in one area of the text?" and "Can the influence of the textual feature be said to extend over a particular zone?". Information about frequency and location may also be combined with information about graphic attributes, phonetic attributes, meaning and/or usage. The result of which is a more refined representation of the specificity, distribution and spread of selected textual features within a text. Graphic attributes, as used herein, refer to the visual attributes of a textscape and how text is mapped on the textscape. Relevance is given to the size and shape of mapped textual features, the presence or absence of color, brightness, intensity, density, boldness, as well as recognizable gradiations, blendings and patterns. Phonetic attributes, as used herein, refer to all sound-based attributes useful for distinguishing homographs.</p>
    <p>FIGS. 7, 8, 9, 10 and 11 show different spread maps for the fourth section 44 of the originally mapped text. Referring to FIG. 7, the target terms "house", "houses" and "housing" are treated as being equivalent. The basis of the spread map 80 in FIG. 7 is to determine whether a target term appears in two or more sequentially contiguous paragraphs. The end limits of the spread are determined by the first and last instances of the target term in question. In the spread map 80 shown in FIG. 7, the spread for the target terms house/houses/housing is shown by oblique hatching. The spread begins at the bottom of the fourth paragraph 64, extends across all of the fourth paragraph 64 and ends close to the bottom of the sixth paragraph 66.</p>
    <p>Until this point, how the target terms "house", "houses" and "housing" were selected has not been explained. To analyze the information contained within a body of text, statistical techniques are used to analyze all of the text features contained within the original body of text. Each text feature is analyzed on the basis of its frequency, location, graphic attributes, phonetic attributes, meaning and/or usage. For example, in the case of the exemplary body of text 12 shown in FIG. 1, there are 4,425 different words and a total of 7,549 word instances. Of the 4,425 words, 3,720 (roughly 84%) of them occur only one time in the text. As such, 705 words occur more than once. The most frequently occurring word is "the" which occurs 387 times. The word "of" appears 190 times, "a" appears 114 times, and "and" appears 90 times. A similar listing of punctuation marks shows that there are, for example, 242 commas, 145 periods and 16 question marks. Such features are often very useful in determining text criteria such as whether or not a word is a verb or noun. However, for the sake of clarity, such words were eliminated from consideration in this example, as were all likely verbs, adjectives, adverbs, prepositions, conjunctions and articles. in this case, only possible nouns were considered.</p>
    <p>In order to select the textual features to be mapped, a list of all possible nouns appearing in the text is then compiled, and the frequency of each noun is noted. The noun list is then ranked in descending order on the basis of frequency. Then each word is classified on the basis of its possible usage. The term "possible usage" is used here because it is often difficult to determine, out of context, the usage of a particular word. For example, the word "houses" means one thing when used as a noun and another thing when used as a verb.</p>
    <p>After the most commonly occurring relevant words are found within a body of text, these words are then categorized so that related words can be identified. Where two or more terms are determined to be related, the term having the highest ranking on the basis of frequency may be declared the dominant word. All related terms may therein considered to be the same as the dominant word. Among the graphic attributes considered in categorizing terms are similarities in graphic structure, for example, "house", "houses" and "housing" all have in common the graphic root "hous". The usage and meaning of the words is also considered, wherein words with the same meaning are categorized as being the same. For instance, the term "house" may or may not mean the same thing as "residence", "domicile" and/or "home". A determination as to whether terms are related is done using statistical probabilities considering the occurrence of the terms and the spread of the terms in the overall context of the body of text.</p>
    <p>In the embodiment of the present invention shown, a search of the initial text was conducted to find the dominant form of the four most frequent possible nouns occurring in the fourth section 44 of the text. Only the fourth section is being analyzed because it has been previously determined that the body of text in question is divided into six sections which may or may not be related by topic. As such, a conservative approach would be to analyze each of the six sections separately as well as the entire text as a whole.</p>
    <p>The fourth section 44 of the mapped text includes twelve occurrences of the most prevalent noun, nine occurrences of the second most prevalent noun, seven occurrences of the third most prevalent noun and five occurrences of the fourth most prevalent noun. FIG. 7, as is known, shows the spread map 80 of the terms house/houses/housing. The terms house/houses/housing represent the second most prevalent noun contained within the fourth section 44 of the mapped text.</p>
    <p>FIG. 8, FIG. 9 and FIG. 10 show spread maps for the first, third and fourth most prevalent nouns, respectively, found in the fourth section 44 of the mapped text. Since the first, third and fourth most prevalent nouns are not known to the viewer, such spread maps are blind mappings and do not provide any information about the identity of the nouns, other than their spread. In FIG. 8, the spread map 90 shows the spread area of the most prevalent noun with a reverse oblique hatching. In FIG. 9, the spread map 92 shows the spread area of the third most prevalent noun with a horizontal hatching. In FIG. 10, the spread map 94 shows the spread area of the fourth most prevalent noun with a vertical hatching.</p>
    <p>Referring to FIG. 11, a composite spread map 100 is shown for the fourth section 44 of the mapped text that represents the combination of spread maps shown in FIGS. 7, 8, 9 and 10. In FIG. 11, the block areas 101, 102, 103, 104, 105 with widely spaced oblique hatching show areas in the fourth section 44 of the mapped text where two of the spread maps from FIGS. 7, 8, 9 and 10 intersect. As such, the block areas 101, 102, 103, 104, 105 contain references to two of the four most prevalent nouns. Block areas 106, 107 with reversed hatching show areas in the fourth section 44 of the mapped text where three of the spread maps from FIGS. 7, 8, 9 and 10 intersect. As such, the block areas 106, 107 contain references to three of the four prevalent nouns. No section of the fourth section 44 of the mapped text contains references to all four of the four most prevalent nouns.</p>
    <p>From viewing the spread map 100 in FIG. 11, it can be inferred that the hatched block areas are the most likely to provide a reader with a quick and accurate overview of the subject discussed in the section. The fact that the information is in the form of a map suggests that the overview may be comprehended quickly. It is reasonable to expect that if one's goal is to gain an overview of the fourth section 44, one will save a considerable amount of auditory-based reading time by going first to the hatched block areas. The fact that the overview is likely to be accurate is a matter of probability based upon the multivariate statistical analysis. The results of this analysis show the following. Block areas 106, 107 represent the only locations in the section in which three of the previous section maps intersect. Block areas 101, 102, 103, 104, 105 are all similar in that they represent the intersection of two previous section maps. Block areas. 101 and 105, however, by virtue of their locations within the first and last paragraphs of the section, are more likely to be useful with regard to establishing a general understanding of the subject discussed in the section than are the other block areas.</p>
    <p>By viewing the section maps shown in FIGS. 7, 8, 9, 10 and 11, a viewer can determine what parts of the fourth section 44 of the mapped text are most likely to provide useful information. However, the viewer cannot determine the identity of the four most prevalent nouns used to create the section maps of FIGS. 7, 8, 9, 10 and 11. By visually comparing FIGS. 7, 8, 9, 10 and 11, it can be seen that all the hatched sections 101, 102, 103, 104, 105, 106, 107 in FIG. 11 are also contained in FIG. 9. FIG. 9 shows the section map for the third most prevalent noun. As such, it can be determined that all of the hatched sections 101, 102, 103, 104, 105, 106, 107 in FIG. 11 contain references to the third most prevalent noun in conjunction with the first, second and/or fourth most prevalent nouns. Since the third most prevalent noun is the only word common to all the intersections illustrated by FIG. 11, it can be ascertained that the third most prevalent noun somehow represents the general subject matter of the fourth section 44 of the mapped text. This information is ascertainable despite the fact that the first and second most prevalent nouns occur more often than the third most prevalent noun. From FIGS. 9, 10 and 11, it can be seen that the fourth most prevalent noun appears with the third most prevalent noun in the first and last paragraphs of the fourth section 44 of the mapped text. As such, it can be determined that the fourth most prevalent noun is nearly as important as the third most prevalent noun.</p>
    <p>After viewing the text as a graphic image or textscape, the person viewing the various maps of the textscape can be shown or told the identity of the most prevalent nouns. In the shown embodiment, the most prevalent noun is "manufacturing". The second most prevalent noun, as mentioned earlier, is "housing". The third most prevalent noun is "wages" and the fourth most prevalent noun is "labor". Having previewed the text as graphic information, an individual may begin reading the words of the text as sound information with the intention of testing the inferences made by using the textmapping process. The most important inference is that the main topic of the section is labor wages in the housing and manufacturing industries.</p>
    <p>Statistical textmapping is a powerful tool for decoding and comprehending written texts. It may be used for a variety of tasks including both reading and writing. For example, it may be used to map a text for concepts, arguments, information and the like or to reveal the structure, organization and flow of information and concepts within a text.</p>
    <p>By comparing different maps of the same text and looking for patterns and intersections, the reader may, without actually reading the words of the text as sound information, determine which locations are most likely to yield the main point of the text. The reader can also determine which locations are most likely to contain specific details or more developed arguments, and which locations are likely to contain minor details, information of secondary importance or digressions from the main point. By viewing a composite map, such as that shown by FIG. 11, a reader can see that heavily intersected locations yield the most general information. Locations which contain very few or zero intersections are less likely to contain general information. They are, however, more likely to contain specific kinds of information, such as details, arguments, documentation and supporting quotations.</p>
    <p>Ultimately, each individual reader and writer is responsible for deciding what is and what is not relevant to any given text. Relevance is critical to comprehension. Comprehension requires a conceptual leap across the gap between literal understanding and interpretive understanding. Textmapping provides a set of tools which can be used to help individuals read texts as visual spatial information, but textmapping cannot change the fact that true comprehension of phonetically-based written language texts ultimately depends upon an auditory-based reading technique. This does not mean, however, that textmapping cannot be used to supplement auditory-based reading.</p>
    <p>FIGS. 2-11 all show two dimensional maps. However, maps that depict a three-dimensional representation of information can also be used. Referring to FIG. 12, a three-dimensional representation is shown for the composite spread map 100 previously shown in FIG. 11. The difference is that in FIG. 12, the textscape is treated as a three-dimensional surface, while in FIG. 11, the textscape is treated as a two-dimensional plane. The three-dimensional blocks 111, 112, 113, 114, 115, 116, 117 in FIG. 12 correspond to the two-dimensional block areas 101, 102, 103, 104, 105, 106, 107 in FIG. 11. It is a matter of personal perspective as to which of the two mappings, FIG. 11 or FIG. 12, more clearly describes the difference between those locations which contain three word intersections, two word intersections or zero word intersections.</p>
    <p>As the above description demonstrates, there are many ways to map a text. Once a text is mapped in two or more different ways, the different mappings may, in turn, be used in a variety of ways to achieve a number of different results. Information from two or more maps may be combined in whole or in part into a new textmap. Two or more maps can be compared by sequential display, by juxtaposition, by superimposition or by animation (very rapid sequential display).</p>
    <p>Textmapping may be practiced, in its simplest form, as a pencil and paper process directly upon a text. For example, one can photocopy the pages from a text, tape the photocopied pages together into a scrolled textscape and map it. One can also print text from a computer on an overhead projector to project the image of the text onto a wall or screen.</p>
    <p>Textmapping may also be practiced on text displayed on a computer screen. Existing computer graphics software, including pen technology for computers, may be used in much the same way as pencils, pens and colored markers are used to map a text displayed on paper. In addition, elements of existing software programs, such as the search functions employed in most word processing software packages, can be used to quickly locate selected features in a text. Existing statistical software packages may be used to perform the statistical analysis which serves as the basis for statistical textmapping.</p>
    <p>It will be understood that a person skilled in the art can make many variations to the exemplary embodiment of the present invention as described above. All such variations and modifications are intended to be included in the scope of the present invention as defined by the appended claims.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4661074">US4661074</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 9, 1985</td><td class="patent-data-table-td patent-date-value">Apr 28, 1987</td><td class="patent-data-table-td ">Walker Susan M</td><td class="patent-data-table-td ">Apparatus for and method of teaching reading and spelling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4907971">US4907971</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 26, 1988</td><td class="patent-data-table-td patent-date-value">Mar 13, 1990</td><td class="patent-data-table-td ">Tucker Ruth L</td><td class="patent-data-table-td ">System for analyzing the syntactical structure of a sentence</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5057020">US5057020</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 15, 1986</td><td class="patent-data-table-td patent-date-value">Oct 15, 1991</td><td class="patent-data-table-td ">Cytanovich Kathryn F</td><td class="patent-data-table-td ">Reading enabler</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5306153">US5306153</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 10, 1993</td><td class="patent-data-table-td patent-date-value">Apr 26, 1994</td><td class="patent-data-table-td ">Foster Margaret J</td><td class="patent-data-table-td ">Educational device for developing vocabulary and spelling skills</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5336093">US5336093</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 22, 1993</td><td class="patent-data-table-td patent-date-value">Aug 9, 1994</td><td class="patent-data-table-td ">Cox Carla H</td><td class="patent-data-table-td ">Reading instructions method for disabled readers</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6160554">US6160554</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 19, 1998</td><td class="patent-data-table-td patent-date-value">Dec 12, 2000</td><td class="patent-data-table-td ">Hewlett Packard Company</td><td class="patent-data-table-td ">Computer file content preview window</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6289361">US6289361</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 4, 1998</td><td class="patent-data-table-td patent-date-value">Sep 11, 2001</td><td class="patent-data-table-td ">Sharp Kabushiki Kaisha</td><td class="patent-data-table-td ">Document display apparatus for displaying a plurality of multimedia documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7036075">US7036075</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 28, 2001</td><td class="patent-data-table-td patent-date-value">Apr 25, 2006</td><td class="patent-data-table-td ">Walker Randall C</td><td class="patent-data-table-td ">Reading product fabrication methodology</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7192283">US7192283</a></td><td class="patent-data-table-td patent-date-value">Apr 14, 2003</td><td class="patent-data-table-td patent-date-value">Mar 20, 2007</td><td class="patent-data-table-td ">Paley W Bradford</td><td class="patent-data-table-td ">System and method for visual analysis of word frequency and distribution in a text</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7287984">US7287984</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 14, 2003</td><td class="patent-data-table-td patent-date-value">Oct 30, 2007</td><td class="patent-data-table-td ">Techenable, Inc.</td><td class="patent-data-table-td ">System and method for providing a visual language for non-reading sighted persons</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7347694">US7347694</a></td><td class="patent-data-table-td patent-date-value">Jan 16, 2003</td><td class="patent-data-table-td patent-date-value">Mar 25, 2008</td><td class="patent-data-table-td ">Oculearn, Llc</td><td class="patent-data-table-td ">Method and apparatus for screening aspects of vision development and visual processing related to cognitive development and learning on the internet</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7792785">US7792785</a></td><td class="patent-data-table-td patent-date-value">Nov 1, 2007</td><td class="patent-data-table-td patent-date-value">Sep 7, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Translating text into visual imagery content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8672682">US8672682</a></td><td class="patent-data-table-td patent-date-value">Oct 20, 2011</td><td class="patent-data-table-td patent-date-value">Mar 18, 2014</td><td class="patent-data-table-td ">Howard A. Engelsen</td><td class="patent-data-table-td ">Conversion of alphabetic words into a plurality of independent spellings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2003063111A1?cl=en">WO2003063111A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 16, 2003</td><td class="patent-data-table-td patent-date-value">Jul 31, 2003</td><td class="patent-data-table-td ">Ronald M Berger</td><td class="patent-data-table-td ">Visual screnning related to learning over the internet</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=0SZFBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc434/defs434.htm&usg=AFQjCNHESPMrMtLQqidjD1IIYvF5AT4S8A#C434S178000">434/178</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=0SZFBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc434/defs434.htm&usg=AFQjCNHESPMrMtLQqidjD1IIYvF5AT4S8A#C434S167000">434/167</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=0SZFBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc434/defs434.htm&usg=AFQjCNHESPMrMtLQqidjD1IIYvF5AT4S8A#C434S156000">434/156</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=0SZFBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc434/defs434.htm&usg=AFQjCNHESPMrMtLQqidjD1IIYvF5AT4S8A#C434S159000">434/159</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=0SZFBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0017000000">G06T17/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=0SZFBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G09B0005060000">G09B5/06</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=0SZFBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G09B0017000000">G09B17/00</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=0SZFBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06T17/00">G06T17/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=0SZFBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G09B5/065">G09B5/065</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=0SZFBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G09B17/00">G09B17/00</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G09B5/06C</span>, <span class="nested-value">G06T17/00</span>, <span class="nested-value">G09B17/00</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Apr 5, 2011</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIMS 1-6 AND 11-17 ARE CANCELLED. CLAIMS 7-10 AND 18-20 WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 15, 2010</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100402</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 28, 2010</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">11</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 28, 2010</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 7, 2009</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 28, 2005</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 1, 2001</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U2iL6nDvBjvzqgV1g8CCURHkWsP-w\u0026id=0SZFBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U2jaeuKeeihybHd7dFNW2ANoHpSXQ\u0026id=0SZFBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U34joMmt6Cw_jVGtQi4J4X_bW-7JA","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/System_and_method_for_converting_written.pdf?id=0SZFBAABERAJ\u0026output=pdf\u0026sig=ACfU3U1yxF2D8Knm_uaAxBaxZDKozpvy5w"},"sample_url":"http://www.google.com/patents/reader?id=0SZFBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>