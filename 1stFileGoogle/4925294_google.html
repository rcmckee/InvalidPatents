<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US4925294 - Method to convert two dimensional motion pictures for three-dimensional systems - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Method to convert two dimensional motion pictures for three-dimensional systems"><meta name="DC.contributor" content="David M. Geshwind" scheme="inventor"><meta name="DC.contributor" content="Anthony H. Handal" scheme="inventor"><meta name="DC.contributor" content="Geshwind David M" scheme="assignee"><meta name="DC.contributor" content="Handal Anthony H" scheme="assignee"><meta name="DC.date" content="1986-12-17" scheme="dateSubmitted"><meta name="DC.description" content="The present invention relates to the computer-assisted processing of standard two-dimensional motion pictures to generate processed image sequences which exhibit some three-dimensional depth effects when viewed under appropriate conditions."><meta name="DC.date" content="1990-5-15" scheme="issued"><meta name="DC.relation" content="US:3772465" scheme="references"><meta name="DC.relation" content="US:3824336" scheme="references"><meta name="DC.relation" content="US:4606625" scheme="references"><meta name="DC.relation" content="US:4809065" scheme="references"><meta name="citation_patent_number" content="US:4925294"><meta name="citation_patent_application_number" content="US:07/227,403"><link rel="canonical" href="http://www.google.com/patents/US4925294"/><meta property="og:url" content="http://www.google.com/patents/US4925294"/><meta name="title" content="Patent US4925294 - Method to convert two dimensional motion pictures for three-dimensional systems"/><meta name="description" content="The present invention relates to the computer-assisted processing of standard two-dimensional motion pictures to generate processed image sequences which exhibit some three-dimensional depth effects when viewed under appropriate conditions."/><meta property="og:title" content="Patent US4925294 - Method to convert two dimensional motion pictures for three-dimensional systems"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("D03sU5XNDMexyATimICIBw"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("CAN"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("D03sU5XNDMexyATimICIBw"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("CAN"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us4925294?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US4925294"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=PTctBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS4925294&amp;usg=AFQjCNHVTHa0AH7P3FdVLhKqsgEJlCE9aw" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US4925294.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US4925294.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US4925294" style="display:none"><span itemprop="description">The present invention relates to the computer-assisted processing of standard two-dimensional motion pictures to generate processed image sequences which exhibit some three-dimensional depth effects when viewed under appropriate conditions....</span><span itemprop="url">http://www.google.com/patents/US4925294?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US4925294 - Method to convert two dimensional motion pictures for three-dimensional systems</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US4925294 - Method to convert two dimensional motion pictures for three-dimensional systems" title="Patent US4925294 - Method to convert two dimensional motion pictures for three-dimensional systems"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US4925294 A</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 07/227,403</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">May 15, 1990</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Dec 17, 1986</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Dec 17, 1986</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/EP0442865A1">EP0442865A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0442865A4">EP0442865A4</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1988004804A1">WO1988004804A1</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">07227403, </span><span class="patent-bibdata-value">227403, </span><span class="patent-bibdata-value">US 4925294 A, </span><span class="patent-bibdata-value">US 4925294A, </span><span class="patent-bibdata-value">US-A-4925294, </span><span class="patent-bibdata-value">US4925294 A, </span><span class="patent-bibdata-value">US4925294A</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22David+M.+Geshwind%22">David M. Geshwind</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Anthony+H.+Handal%22">Anthony H. Handal</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Geshwind+David+M%22">Geshwind David M</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Handal+Anthony+H%22">Handal Anthony H</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US4925294.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US4925294.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US4925294.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (4),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (91),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (54),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (12)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/4925294&usg=AFQjCNGpnUsj5e8tL5kxFN8fG2hry_KOPw">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D4925294&usg=AFQjCNEHt_pR9HpsOZqP5jd2Th8Yu0tY7g">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D4925294A%26KC%3DA%26FT%3DD&usg=AFQjCNHStDLRMiFF9ENRTd9MRdBvF3q9Fg">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT53457097" lang="EN" load-source="patent-office">Method to convert two dimensional motion pictures for three-dimensional systems</invention-title></span><br><span class="patent-number">US 4925294 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA36935560" lang="EN" load-source="patent-office"> <div class="abstract">The present invention relates to the computer-assisted processing of standard two-dimensional motion pictures to generate processed image sequences which exhibit some three-dimensional depth effects when viewed under appropriate conditions.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(2)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US4925294-1.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US4925294-1.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US4925294-2.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US4925294-2.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(44)</span></span></div><div class="patent-text"><div mxw-id="PCLM4313659" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>Now that the invention has been described, what is claimed as new and desired to be secured by Letters Patent is:</claim-statement> <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1. A method of converting a two-dimensional image frame into a three-dimensional image frame consisting of the steps of:<div class="claim-text">a. inputing a frame of a two-dimensional image into a computer;</div> <div class="claim-text">b. specifying at least two individual image elements in the two-dimensional image;</div> <div class="claim-text">c. separating the two-dimensional image into said image elements;</div> <div class="claim-text">d. specifying three-dimensional information for at least one of said image elements;</div> <div class="claim-text">e. processing at least one of said image elements to incorporate said three-dimensional information and create at least one processed image element;</div> <div class="claim-text">f. generating at least one processed image frame comprising at least one of said processed image elements.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2. A method as in claim 1 wherein said step f results in the generation of a left and right pair of processed image frames.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3. A method as in claim 2 comprising the additional step of:<div class="claim-text">g. combining said left and right image pair into a single processed image.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4. A method as in claim 2 comprising the additional step of:<div class="claim-text">g. encoding said left and right processed image pair for viewing, by coloring each of said pair different colors.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5. A method as in claim 2 comprising the additional step of:<div class="claim-text">g. encoding said left and right processed image pair for viewing, by passing each of said pair through mutually perpendicularly polarized filters.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6. A method as in claim 2 comprising the additional step of:<div class="claim-text">g. encoding said left and right processed image pair for viewing, by incorporating said image pair into a video signal suitable for displaying each of said pair alternately on a video display.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7. As method as in claim 1, wherein said step f results in a processed image frame such that, when viewed through glasses with one dark and one light lens, 3-dimensional effects are perceived.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8. A method as in claim 1 comprising the additional step of:<div class="claim-text">g. recording said processed image frame.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9. A method as in claim 7 comprising the additional step of:<div class="claim-text">g. recording said processed image frame.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" class="claim">
      <div class="claim-text">10. A method as in claim 1 wherein said steps are applied to successive frames in a motion picture sequence.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" class="claim">
      <div class="claim-text">11. A method as in claim 2 wherein said steps are applied to successive frames in a motion picture sequence.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" class="claim">
      <div class="claim-text">12. A product produced by the method described in claim 7.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" class="claim">
      <div class="claim-text">13. A product produced by the method described in claim 10.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" class="claim">
      <div class="claim-text">14. A product produced by the method described in claim 11.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" class="claim">
      <div class="claim-text">15. A product produced by the method described in claim 1.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" class="claim">
      <div class="claim-text">16. A method as in claim 1 wherein at least one of said processed image elements produced in step e is a shadow element.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" class="claim">
      <div class="claim-text">17. A method as in claim 1 wherein at least one of said image elements in step e is processed to include additional two-dimensional image information not contained in the original unprocessed two-dimensional image.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" class="claim">
      <div class="claim-text">18. A method as in claim 17 wherein said additional two-dimensional image information is derived from another image.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" class="claim">
      <div class="claim-text">19. A method as in claim 1 wherein said processed image elements in step f are combined with at least one additional 3-D image element not derived from the source image to create said processed image frame.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" class="claim">
      <div class="claim-text">20. A method as in claim 19 wherein said additional 3-D image element is derived from a 3-D photograph.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" class="claim">
      <div class="claim-text">21. A method as in claim 19 wherein said additional 3-D image element is derived from a computer generated 3-D image.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="22" class="claim">
      <div class="claim-text">22. A method as in claim 1 wherein said three-dimensional information for at least one of said image elements in step d is specified only at certain points and is interpolatively derived for other points on said image element.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="23" class="claim">
      <div class="claim-text">23. A method as in claim 10 wherein said three-dimensional information for at least one of said image elements in step d is specified only for certain frames and is temporally interpolated for frames between said certain frames.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="24" class="claim">
      <div class="claim-text">24. A method as in claim 10 wherein said specification of at least one of said image elements in step b is specified only for certain frames and is temporally interpolated for frames between said certain frames.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" class="claim">
      <div class="claim-text">25. A method as in claim 1 wherein random noise is added to the three-dimensional information specified in step d.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="26" class="claim">
      <div class="claim-text">26. A method as in claim 1 wherein at least some of said three-dimensional information specified in step d is derived from the measurement of at least one aspect of an image element.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="27" class="claim">
      <div class="claim-text">27. A method as in claim 26 wherein said aspect pertains to geometry.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="28" class="claim">
      <div class="claim-text">28. A method as in claim 26 wherein said aspect pertains to illumination.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="29" class="claim">
      <div class="claim-text">29. A method as in claim 10 wherein at least some of said three-dimensional information specified in step d is derived from the measurement of the change of at least one aspect of an image element in successive frames.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="30" class="claim">
      <div class="claim-text">30. A method as in claim 29 wherein said aspect pertains to geometry.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="31" class="claim">
      <div class="claim-text">31. A method as in claim 29 wherein said aspect pertains to illumination.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="32" class="claim">
      <div class="claim-text">32. A method as in claim 1 wherein said two-dimensional image frame is a black and white image frame, said image elements are black and white image elements, and said processing includes the process of adding color to at least one of said black and white image elements.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="33" class="claim">
      <div class="claim-text">33. A method as in claim 32 wherein said steps are applied to successive frames in a motion picture sequence.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="34" class="claim">
      <div class="claim-text">34. A product produced by the method of claim 33.</div>
    </div>
    </div> <div class="claim"> <div num="35" class="claim">
      <div class="claim-text">35. An apparatus for converting a two-dimensional image frame into a three-dimensional image frame comprising, in combination:<div class="claim-text">a. a means for scanning said two-dimensional image frame into a computer;</div> <div class="claim-text">b. a means for specifying individual image elements in said frame;</div> <div class="claim-text">c. a means for separating said frame into said individual elements;</div> <div class="claim-text">d. a means for specifying three-dimensional information for said individual image elements;</div> <div class="claim-text">e. a means for processing said individual image elements to create processed image elements;</div> <div class="claim-text">f. a means for creating said three-dimensional image frame comprising at least one of said processed image elements;</div> <div class="claim-text">g. a means for outputing said three-dimensional image frame.</div> </div>
    </div>
    </div> <div class="claim"> <div num="36" class="claim">
      <div class="claim-text">36. An apparatus for converting a two-dimensional image sequence into a three-dimensional image sequence and producing a three-dimensional image recording comprising, in combination:<div class="claim-text">a. a means for scanning said sequence into a computer;</div> <div class="claim-text">b. a means for specifying individual image elements in said sequence;</div> <div class="claim-text">c. a means for separating said sequence into said individual image elements;</div> <div class="claim-text">d. a means for specifying three-dimensional information for said individual image elements;</div> <div class="claim-text">e. a means for processing said individual image elements to create processed image elements;</div> <div class="claim-text">f. a means for combining said processed image elements into a processed image sequence;</div> <div class="claim-text">g. a means for outputing said three-dimensional image sequence;</div> <div class="claim-text">h. a means for recording said three-dimensional image sequence.</div> </div>
    </div>
    </div> <div class="claim"> <div num="37" class="claim">
      <div class="claim-text">37. A method of converting a two-dimensional image frame into a three-dimensional image frame consisting of the steps of:<div class="claim-text">a. inputing frames of a two-dimensional image sequence into a computer;</div> <div class="claim-text">b. specifying at least two individual image elements in the two-dimensional image sequence;</div> <div class="claim-text">c. separating the two-dimensional images into said image elements;</div> <div class="claim-text">d. specifying three-dimensional information for at least one of said image elements;</div> <div class="claim-text">e. processing at least one of said image elements to incorporate said three-dimensional information and create at least one processed image elements;</div> <div class="claim-text">f. generating a sequence of processed image frames comprising at least one of said processed image elements said generation to be of such a nature so as to exhibit three-dimensional depth characteristics when viewed using a complementary display system.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="38" class="claim">
      <div class="claim-text">38. A method as in claim 37 comprising the additional step:<div class="claim-text">g. transmission of said three-dimensional image sequence.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="39" class="claim">
      <div class="claim-text">39. A method as in claim 33 comprising the additional step:<div class="claim-text">g. recording of said three-dimensional image sequence.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="40" class="claim">
      <div class="claim-text">40. A method as in claim 37 wherein said two-dimensional image sequence is a black and white image sequence, said image elements are black and white image elements, and said processing includes the process of adding color to at least one of said black and white image elements.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="41" class="claim">
      <div class="claim-text">41. A product produced by the method of claim 40.</div>
    </div>
    </div> <div class="claim"> <div num="42" class="claim">
      <div class="claim-text">42. A method of converting a two-dimensional image frame into a three-dimensional image frame consisting of the steps of:<div class="claim-text">a. inputing frames of a two-dimensional image sequence into a computer, each of said frames consisting of at least two individual image elements;</div> <div class="claim-text">b. specifying three-dimensional information for at least one of said image elements;</div> <div class="claim-text">c. processing at least one of said image elements to incorporate said three-dimensional information and create at least one processed image element;</div> <div class="claim-text">d. generating a sequence of processed image frames comprising at least one of said processed image elements.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="43" class="claim">
      <div class="claim-text">43. A method as in claim 42 wherein said individual image elements in step a are derived from sub-components of an animated film.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="44" class="claim">
      <div class="claim-text">44. A product produced by the method described in claim 42.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES65977218" lang="EN" load-source="patent-office" class="description">
    <heading>TECHNICAL FIELD</heading> <p>The invention relates to a method for converting existing film or videotape motion pictures to a form that can be used with three-dimensional systems for broadcast or exhibition.</p>
    <heading>BACKGROUND ART</heading> <p>With the advent of stereophonic sound, various techniques were developed to convert or `re-process` existing monophonic programs for stereophonic broadcast or recording systems. These included modifying the equalization phase or tonal qualities of separate copies of the monophonic program for the left and right channels. While true stereophonic or binaural effects may not have been achieved, the effects were much improved over feeding the identical monophonic signal to both channels.</p>
    <p>Similarly, with the almost universal use of color production, exhibition and broadcast systems for motion pictures and television, systems have been developed to convert existing monochrome or black and white materials to color programs. Such a systems is described in applicant Geshwind's Pat. No. 4,606,625, issued Aug. 19, 1986. The results of these colorized products, while not always identical to true color motion pictures, are more suitable than black and white for color systems.</p>
    <p>There have been a number of systems for exhibition or display of left- and right-eye pairs of binocular motion pictures. Early systems required two completely redundant projection or display systems; e.g. two film projectors or CRT television displays, each routed to one eye via mirrors. Other systems require either complicated and expensive projection or display systems, or expensive `glasses` to deliver two separate images. For example:</p>
    <p>red- and green-tinted monochrome images are both projected or displayed to be viewed through glasses with left and right lenses tinted either red or green;</p>
    <p>two full-color images are projected through mutually perpendicular polarized filters and viewed through glasses with lenses that are also polarized in the same manner;</p>
    <p>left and right images are displayed on alternate odd and even fields (or frames) of a standard (or high scan rate) television CRT and are viewed through `glasses` with shutters (either rotating blades or flickering LCDs, for example) that alternate the view of left and right eyes in synchrony with the odd or even fields of the CRT.</p>
    <p>Of the above systems, the second is not at all usable with standard home television receivers, the third requires very expensive `glasses` and may flicker with standard home receivers, and the first produces only strangely tinted monochrome images. Further, none of the systems may be broadcast over standard television for unimpeded viewing without special glasses.</p>
    <p>Thus, until now, compatible (i.e., viewable as two-dimensional, without glasses) home reception of b 3-D images was not possible. However, a new system, which takes advantage of differential processing of left- and right-eye images in the human perceptual system, delivers a composite image on a standard home television receiver that can be viewed as a normal 2-D picture without glasses. Very inexpensive glasses, with one light and one dark lens, accentuate the differential processing of the image, as viewed by each eye, to produce a 3-D depth effect.</p>
    <p>Practical, inexpensive, compatible (with standard TV) 3-D television may now become widespread. In addition to materials specifically produced for the new system (or other 3-D systems) conversion of standard 2-D programs to 3-D format would provide additional product to broadcast using the new compatible system (or for other 3-D projection systems).</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>Applicant Handal's previous application, No. 479,679, filed Mar. 28, 1983 and now abandoned, relates a process and apparatus for deriving left- and right-eye pairs of binocular images, from certain types of two dimensional film materials. In particular, the materials must consist of separate foreground and background elements, such as cartoon animation cells and background art. By taking into account the parallax between scenes as viewed by the left and right eyes, two images are prepared where foreground elements are shifted with respect to background elements by an amount that indicates their depth in the third dimension. Two-dimensional motion pictures that consist of a series of single composite images could not be converted to three-dimensional format, by this technique, without first being separated into various background and foreground elements.</p>
    <p>Once committed to two-dimensional motion pictures, the separation and depth information for various scene elements, in the third dimension, are lost. Thus, the separation of two-dimensional image sequences into individual image elements and the generation of three-dimensional depth information for each such image element are not simple or trivial tasks, and are the further subject of the instant invention.</p>
    <p>In accordance with the invention, standard two-dimensional motion picture film or videotape may be converted or processed, for use with three-dimensional exhibition or transmission systems, so as to exhibit at least some three-dimensional or depth characteristics. Separation of a single 2-D image stream into diverse elements is accomplished by a computer assisted, human operated system. (If working from discrete 2-D film sub-components, such as animation elements, the separation step may be omitted.) Depth information is assigned to various elements by a combination of human decisions and/or computer analyses and resulting images of three-dimensional format are produced under computer control.</p>
    <heading>BRIEF DESCRIPTION OF DRAWINGS</heading> <p>A method for carrying out the invention is described in the accompanying drawings in which:</p>
    <p>FIG. 1 is a diagram illustrating the relationship of a 2-D source film image, left and right image pairs, and a composite 3-D image frame.</p>
    <p>FIG. 2 is a schematic diagram of a system for carrying out an implementation of the present invention.</p>
    <heading>DETAILED DESCRIPTION</heading> <p>An immense amount of standard, 2-D motion picture material exists in the form of film and videotape. In addition, certain materials exist in the form of discrete 2-D image sub-components, such as animated cell and background paintings. As the use of 3-D exhibition and broadcast systems becomes more widespread, the conversion of existing 2-D programs to a format that will exhibit at least some 3-D or depth effects, when used with 3-D systems, is desired.</p>
    <p>Extracting individual image elements or 3-D depth information from a 2-D film frame, or synthesizing 3-D information for those elements, entirely by computer equipped with artificial intelligence, is not now practical. Therefore, the embodiment of the invention as described herein employs a high degree of human interaction with the computer. However, as artificial intelligence progresses, a predominantly or completely automated system may become practical and is within the intended scope of the invention.</p>
    <p>FIG. 1 shows a frame 20 of a standard 2-D film of video motion picture, consisting of a cross-hatched background plane 21, a large white circle 22 in the mid-ground, and a small black square 23 in the foreground. It is a 2-D representation of original 3-D scene 10 comprising elements 11, 12 and 13, which is not  available for direct 3-D photography. After human identification of individual image elements and depth assignment, the computer generates, from frame 10, a pair of coordinated left 30 and right 40 images with backgrounds 31 and 41, circles 32 and 42, and squares 33 and 43 respectively. Note that the relative positions of the square and circle are different in the left and right images; this situation is similar to the parallax that might result between left- and right-eye views if one were to have viewed the original scene 10 directly. Three-dimensional format frame 50 is generated by encoding the information in the left 30 and right 40 image pair, in a manner consistent with any one of a number of existing 3-D systems. The specific operation of these various 3-D systems is not the subject of the instant invention.</p>
    <p>Alternately, the steps of generating and encoding 3-D information may be combined such that 3-D format frame 50 may be processed directly from 2-D frame 20 without generating left 30 and right 40 image pairs. In either case, 3-D format frame 50 when viewed by human 60 through 3-D glasses 70 is perceived as 3-D scene 80, containing elements 81, 82 and 83, which has at least some of the 3-D characteristics of original 3-D scene 10.</p>
    <p>Various systems for the encoding, display, projection, recording, transmission or viewing of 3-D images exist, and new systems may be developed. Specifically, various techniques for specifying, encoding and viewing 3-D information may now, or come to, exist, which do not make use of parallax offset and/or left and right image pairs and/or viewing glasses, or which embody new techniques or changes and improvements to current systems. Further, such systems may integrate information from more than one 2-D source frame 20 into a single resultant 3-D frame 50. The specifics of operation of such systems is not the subject of the instant invention, however, preparation of 2-D program material for such systems is.</p>
    <p>The offsets shown for elements 31, 32 and 33 in left frame 30 and elements 41, 42 and 43 in right frame 40 are meant to be illustrative and do not necessarily follow the correct rules for image parallax. In fact, depending upon where viewer attention is meant to be centered, different rules may apply. For example, one technique is to give no parallax offset to far background elements and to give progressively more parallax offset to objects as they get closer. Alternately, attention may be centered in the mid-ground with no parallax offset to mid-range objects, some parallax offset to close-range objects and reverse parallax offset to far-range objects. The particular placement of objects and attention point in the 3-D scene is as much an art as a science and is critical to the enjoyment of 3-D programs and, in any event, is not meant to be the subject of this invention.</p>
    <p>FIG. 2 shows a schematic of a system to implement the instant invention. A 2-D film or video image 10 is input to a video monitor 20 and to the scanning portion 41 of frame buffer 40. Video monitor 20 is capable of displaying either the 2-D image being input 10 or the output from display portion 42 of frame buffer 40.</p>
    <p>Frame buffer 40 consists of an image scanner 41 which can convert the input image into digital form to be stored in a portion of frame buffer memory section 44, a display section 42 which creates a video image from the contents of a portion of memory section 44, and a computer interface section 43 which allows the computer CPU 50 to read from and write to the memory section 44.</p>
    <p>Graphic input tablet 30 and stylus 35 allow the human operator to input position information to the computer 50 which can indicate the outline of individual image elements, choice of a specific image element or part of an image element, depth specification, choice of one of a number of functions offered on a `menu`, or other information. An image cursor can be displayed on monitor 20 by frame buffer 40 to visually indicate the location or status of the input from the tablet 30 and pen 35. Text and numeric information can also be input by the operator on keyboard 55.</p>
    <p>Computer CPU 50 is equipped with software which allows it to interpret the commands and input from the human operator, and to process the digitized 2-D information input from 2-D frame 10 into digital 3-D frame information, based on said human commands and input. Said digital 3-D frame information is then output by output interface 60 (which may be similar to frame buffer 40 or of some other design) to a videotape recorder 70 or to a film recorder 75, capable of recording 3-D format frames.</p>
    <p>The system as described above operates as follows. A frame of the 2-D program is displayed on the video monitor for viewing by the human operator. With the tablet and stylus the operator outlines various 2-D image areas to indicate to the computer the boundaries of various image elements to be separately processed. (For materials, such as animation components, that already exist as separate elements, the previous stage of the process may be skipped.) Depth position information, in the third dimension, is determined for each image element, by a combination of operator input and computer analysis. Left and right image pairs or a 3-D composite image is processed by the computer, from the 2-D input image, based on the computer software and operator instructions. Depending upon the particular 3-D system to be used, left- and right-image pairs may or may not be the final stage or an intermediate stage or bypassed entirely. Further, for some 3-D systems, information from more than one 2-D source frame may be combined into one 3-D frame (or frame pair). In any event, the final 3-D information is then collected on a videotape or film recorder. The process is then repeated for additional 2-D frames.</p>
    <p>Each image element may be given a uniform depth designation which may cause the perception of `cardboard cut-out` characters. Alternately, different portions of a single image element may be given different depth designations with the computer interpolating depth coordinates over the entire element. For example, an image element positioned diagonally in a frame may have its right edge designated to be closer than its left edge. Alternately, one feature of an image element, say a person's nose in a close-up, might be designated as being closer than the rest of the image element. In such manner, the computer would be instructed to interpolate and process depth information over the entire image element. Such processing of the image element, in accordance with varying depth information, may result in the stretching, skewing or other distortion of the image element.</p>
    <p>Depth interpolation may also be carried out over time, between frames. For example, an image element might be designated to be close in one frame and to be far away in a later frame. The depth of the image element may be interpolated for the frames between the two designated frames. Further, the position of the image element in the two dimensions of the film frame, and the steps of the outline separating the image element from the rest of the source image, may also be interpolated over time, between frames Linear and non-linear interpolation techniques are well known and may be readily applied.</p>
    <p>It is also possible to add random noise to the depth information to eliminate the appearance of flat objects moving in space and to help achieve greater realism.</p>
    <p>For a particular image element, depth position may be derived by the computer, alone or with operator input, by measuring the size of the image element in a frame. For example, once a person were outlined, his overall height in the 2-D film frame might be extracted by the computer as an indication of depth distance. using knowledge, or making assumptions, about the object's real size, and the characteristics of the camera and lens, the distance of the object from the camera may be calculated by applying well known principles of perspective and optics. Alternately, if overall size cannot be easily extracted by the computer, key points might be indicated by the operator for the computer to measure. Comparing the change of size of an image element in several frames will provide information about the movement of the object in the third dimension.</p>
    <p>It should be noted that horizontal parallax offsets are, by far, more obvious, due to the fact that our eyes are separated in the horizontal direction. However, vertical offset differences between left- and right-eye views may also be appropriate.</p>
    <p>As various image elements are separated and assigned depth values, a situation develops where diverse objects exist in a `three-dimensional space` within the computer. It should be noted that, in order to display a realistic representation of the entire scene, forwardmost objects must obscure all or part of rearmost objects with which they overlap (except in the case where a forwardmost object were transparent). When generating left- and right-eye views, the pattern of overlap of image elements, and thus the pattern of obscuring of image elements will, in general, be different. Further, for image elements that have been assigned non-uniform depth values (e.g., image elements that are not flat or not in a plane perpendicular to the third dimension) the intersection between these image elements, for the purpose of one obscuring the other, may be non-trivial to calculate. However, there are well known techniques, that have been developed for computer image synthesis, that allow for the sorting, intersection and display of diverse, overlapping 3-D image elements.</p>
    <p>As image elements are separated from background scenes or each other, holes may develop. The computer software will compensate for this by filling in missing information in one particular 2-D frame with the equivalent part of the image element from earlier or later frames whenever possible. Alternately, material to `plug holes` may be created by the operator or by the computer from adjacent areas or may be newly synthesized. Further, as two image elements are separated from each other, the forwardmost of the two may be moved closer to the viewer; hence, it may be appropriate to make it larger. Thus, the increased size may cover the gaps in the rearmost element of the two.</p>
    <p>As part of the processing to be performed on the 2-D source image, individual image elements or composite images, additional effects may be programmed into the computer to heighten the sense of depth. For example, shadows cast by one image element on another element or the background may be calculated and inserted; far distant landscape elements may be made hazy or bluish to indicate remoteness; different image elements may be blurred or sharpened to simulate depth-of-field; or, the distance between close objects may be exaggerated to emphasize their separation. There are, of course, other technical or artistic techniques that can be used to indicate depth in an image and which may also be incorporated into the image processing programs and would therefore be part of the invention as described herein. Therefore, the above examples are illustrative and should not be construed as limiting the scope of the invention. Alternately, depth information may be intentionally distorted for effect or for artistic purposes.</p>
    <p>Improvements may be made to the final product by including new image elements that were not part of the original 2-D source image. These could include 2-D image elements that are then assigned depth values, 3-D image elements created by 3-D photography and then entered into the computer as left- and right-image pairs, for example, or synthetic 3-D computer generated graphics. In particular, since computer generated image elements can be created with depth information, they can be easily integrated into the overall 3-D scene with vivid effect. For example, a 3-D laser blast could be created by computer image synthesis such that it would in turn obscure and be obscured by other image elements in an appropriate manner and might even be created so as to appear to continue beyond the front of the screen into `viewer space`.</p>
    <p>Animated film components usually have relatively few background paintings, which are kept separate from the animated characters in the foreground. For these, or for live 2-D filmed scenes, once the foreground elements have been separated from the background, the flat 2-D backgrounds may be replaced by 3-D backgrounds. The 3-D backgrounds might consist of computer generated graphics, in which case depth information for the various elements of the background would be available at the time of the background creation. Alternatively, 3-D backgrounds might be created using 3-D photography, in which case depth information for the background elements may be derived, by the computer, from the comparison of the left- and right-image pairs of the 3-D background photographs and applying known image processing and pattern recognition techniques. Alternately, depth information to create 3-D backgrounds may be specified otherwise by operator input and/or computer processing. Once depth information is available for the various background elements, it may compared to depth information for the other image elements with which it is to be combined in each frame. Thus, the 3-D background may itself hve some depth and, in effect, be a `set` within which other image elements may be positioned in front of, behind or intersecting various background elements for added realism.</p>
    <p>It should be noted that, intended purpose and software algorithms aside, the design and operation of the 3-D conversion system described herein has many similarities with the black and white footage colorization system described in applicant Geshwind's Pat. No. 4,606,625. They are both computer-aided systems which  display standard film images to an operator, allow the operator to input information separating various image elements within the frame, allow the operator to specify attributes (color in one case, depth in the other) for the image elements, and cause the computer to process new image frames from the original, based on the operator input. Thus, the processing of 2-D black and white footage to add both color and depth information would be much more efficient than implementing each process separately.</p>
    <p>The scope of the instant invention is the conversion or processing of 2-D program material for use with any 3-D exhibition or distribution system now in use or later developed, but not the specific method of operation of any particular 3-D system.</p>
    <p>It will thus be seen that the objects set forth above, among those made apparent from the proceeding description, are efficiently attained and certain changes may be made in carrying out the above method and in the construction set forth. Accordingly, it is intended that all matter contained in the above description or shown in the accompanying figures shall be interpreted as illustrative and not in a limiting sense.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3772465">US3772465</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 9, 1971</td><td class="patent-data-table-td patent-date-value">Nov 13, 1973</td><td class="patent-data-table-td ">Ass Of Motion Picture Televisi</td><td class="patent-data-table-td ">Image modification of motion pictures</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3824336">US3824336</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 17, 1972</td><td class="patent-data-table-td patent-date-value">Jul 16, 1974</td><td class="patent-data-table-td ">Teletronics Int Inc</td><td class="patent-data-table-td ">Editing system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4606625">US4606625</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 9, 1983</td><td class="patent-data-table-td patent-date-value">Aug 19, 1986</td><td class="patent-data-table-td ">Geshwind David M</td><td class="patent-data-table-td ">Method for colorizing black and white footage</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4809065">US4809065</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 1, 1986</td><td class="patent-data-table-td patent-date-value">Feb 28, 1989</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Interactive system and related method for displaying data to produce a three-dimensional image of an object</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5751397">US5751397</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 6, 1995</td><td class="patent-data-table-td patent-date-value">May 12, 1998</td><td class="patent-data-table-td ">Osgood; Alan G.</td><td class="patent-data-table-td ">Color motion depth effect system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6031564">US6031564</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 7, 1997</td><td class="patent-data-table-td patent-date-value">Feb 29, 2000</td><td class="patent-data-table-td ">Reveo, Inc.</td><td class="patent-data-table-td ">Method and apparatus for monoscopic to stereoscopic image conversion</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6108005">US6108005</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 4, 1997</td><td class="patent-data-table-td patent-date-value">Aug 22, 2000</td><td class="patent-data-table-td ">Space Corporation</td><td class="patent-data-table-td ">Method for producing a synthesized stereoscopic image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6215516">US6215516</a></td><td class="patent-data-table-td patent-date-value">Dec 17, 1999</td><td class="patent-data-table-td patent-date-value">Apr 10, 2001</td><td class="patent-data-table-td ">Reveo, Inc.</td><td class="patent-data-table-td ">Method and apparatus for monoscopic to stereoscopic image conversion</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6384859">US6384859</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 26, 1996</td><td class="patent-data-table-td patent-date-value">May 7, 2002</td><td class="patent-data-table-td ">Sanyo Electric Co., Ltd.</td><td class="patent-data-table-td ">Methods for creating an image for a three-dimensional display, for calculating depth information and for image processing using the depth information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6466205">US6466205</a></td><td class="patent-data-table-td patent-date-value">Nov 19, 1998</td><td class="patent-data-table-td patent-date-value">Oct 15, 2002</td><td class="patent-data-table-td ">Push Entertainment, Inc.</td><td class="patent-data-table-td ">System and method for creating 3D models from 2D sequential image data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6477267">US6477267</a></td><td class="patent-data-table-td patent-date-value">Jun 22, 1998</td><td class="patent-data-table-td patent-date-value">Nov 5, 2002</td><td class="patent-data-table-td ">Dynamic Digital Depth Research Pty Ltd.</td><td class="patent-data-table-td ">Image conversion and encoding techniques</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6496598">US6496598</a></td><td class="patent-data-table-td patent-date-value">Mar 1, 2000</td><td class="patent-data-table-td patent-date-value">Dec 17, 2002</td><td class="patent-data-table-td ">Dynamic Digital Depth Research Pty. Ltd.</td><td class="patent-data-table-td ">Image processing method and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6590573">US6590573</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 25, 1992</td><td class="patent-data-table-td patent-date-value">Jul 8, 2003</td><td class="patent-data-table-td ">David Michael Geshwind</td><td class="patent-data-table-td ">Interactive computer system for creating three-dimensional image information and for converting two-dimensional image information for three-dimensional display systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6760021">US6760021</a></td><td class="patent-data-table-td patent-date-value">Jul 13, 2000</td><td class="patent-data-table-td patent-date-value">Jul 6, 2004</td><td class="patent-data-table-td ">Orasee Corp.</td><td class="patent-data-table-td ">Multi-dimensional image system for digital image input and output</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6965379">US6965379</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 8, 2001</td><td class="patent-data-table-td patent-date-value">Nov 15, 2005</td><td class="patent-data-table-td ">Koninklijke Philips Electronics N.V.</td><td class="patent-data-table-td ">N-view synthesis from monocular video of certain broadcast and stored mass media content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7035451">US7035451</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 9, 2001</td><td class="patent-data-table-td patent-date-value">Apr 25, 2006</td><td class="patent-data-table-td ">Dynamic Digital Depth Research Pty Ltd.</td><td class="patent-data-table-td ">Image conversion and encoding techniques</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7054478">US7054478</a></td><td class="patent-data-table-td patent-date-value">Aug 18, 2003</td><td class="patent-data-table-td patent-date-value">May 30, 2006</td><td class="patent-data-table-td ">Dynamic Digital Depth Research Pty Ltd</td><td class="patent-data-table-td ">Image conversion and encoding techniques</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7102633">US7102633</a></td><td class="patent-data-table-td patent-date-value">May 15, 2002</td><td class="patent-data-table-td patent-date-value">Sep 5, 2006</td><td class="patent-data-table-td ">In-Three, Inc.</td><td class="patent-data-table-td ">Method for conforming objects to a common depth perspective for converting two-dimensional images into three-dimensional images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7116323">US7116323</a></td><td class="patent-data-table-td patent-date-value">Dec 10, 2002</td><td class="patent-data-table-td patent-date-value">Oct 3, 2006</td><td class="patent-data-table-td ">In-Three, Inc.</td><td class="patent-data-table-td ">Method of hidden surface reconstruction for creating accurate three-dimensional images converted from two-dimensional images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7116324">US7116324</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 2003</td><td class="patent-data-table-td patent-date-value">Oct 3, 2006</td><td class="patent-data-table-td ">In-Three, Inc.</td><td class="patent-data-table-td ">Method for minimizing visual artifacts converting two-dimensional motion pictures into three-dimensional motion pictures</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7180536">US7180536</a></td><td class="patent-data-table-td patent-date-value">May 26, 2003</td><td class="patent-data-table-td patent-date-value">Feb 20, 2007</td><td class="patent-data-table-td ">Raphael-Armament Development Authority Ltd.</td><td class="patent-data-table-td ">Method for producing stereoscopic images from monoscopic images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7254264">US7254264</a></td><td class="patent-data-table-td patent-date-value">Apr 1, 2001</td><td class="patent-data-table-td patent-date-value">Aug 7, 2007</td><td class="patent-data-table-td ">Newsight Corporation</td><td class="patent-data-table-td ">Method and device for generating 3D images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7254265">US7254265</a></td><td class="patent-data-table-td patent-date-value">Jan 8, 2003</td><td class="patent-data-table-td patent-date-value">Aug 7, 2007</td><td class="patent-data-table-td ">Newsight Corporation</td><td class="patent-data-table-td ">Methods and systems for 2D/3D image conversion and optimization</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7321374">US7321374</a></td><td class="patent-data-table-td patent-date-value">May 29, 2003</td><td class="patent-data-table-td patent-date-value">Jan 22, 2008</td><td class="patent-data-table-td ">Newsight Corporation</td><td class="patent-data-table-td ">Method and device for the generation of 3-D images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7419617">US7419617</a></td><td class="patent-data-table-td patent-date-value">Oct 20, 2005</td><td class="patent-data-table-td patent-date-value">Sep 2, 2008</td><td class="patent-data-table-td ">Basf Aktiengesellschaft</td><td class="patent-data-table-td ">Coolant comprising azole derivatives for cooling systems in fuel-cell drives</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7419618">US7419618</a></td><td class="patent-data-table-td patent-date-value">Jun 8, 2006</td><td class="patent-data-table-td patent-date-value">Sep 2, 2008</td><td class="patent-data-table-td ">Basf Aktiengesellschaft</td><td class="patent-data-table-td ">Aqueous alkylene glycol solution containing a five membered nitrogen-containing an optionally fused heterocyclic compound optionally containing a sulfur atom (benzotriazole, tolutriazole and/or hydrogenated tolutriazole) and an orthosilicate; initial electrical conductivity of at most 50 mu S/cm</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7542034">US7542034</a></td><td class="patent-data-table-td patent-date-value">Sep 23, 2004</td><td class="patent-data-table-td patent-date-value">Jun 2, 2009</td><td class="patent-data-table-td ">Conversion Works, Inc.</td><td class="patent-data-table-td ">System and method for processing video images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7551770">US7551770</a></td><td class="patent-data-table-td patent-date-value">Aug 18, 2003</td><td class="patent-data-table-td patent-date-value">Jun 23, 2009</td><td class="patent-data-table-td ">Dynamic Digital Depth Research Pty Ltd</td><td class="patent-data-table-td ">Image conversion and encoding techniques for displaying stereoscopic 3D images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7666096">US7666096</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 19, 2006</td><td class="patent-data-table-td patent-date-value">Feb 23, 2010</td><td class="patent-data-table-td ">Tdvision Corporation S.A. De C.V.</td><td class="patent-data-table-td ">Method for generating the left and right perspectives in a 3D videogame</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7822265">US7822265</a></td><td class="patent-data-table-td patent-date-value">Apr 8, 2005</td><td class="patent-data-table-td patent-date-value">Oct 26, 2010</td><td class="patent-data-table-td ">Koninklijke Philips Electronics N.V.</td><td class="patent-data-table-td ">Ghost artifact reduction for rendering 2.5D graphics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7856055">US7856055</a></td><td class="patent-data-table-td patent-date-value">Mar 13, 2003</td><td class="patent-data-table-td patent-date-value">Dec 21, 2010</td><td class="patent-data-table-td ">Imax Corporation</td><td class="patent-data-table-td ">Systems and methods for digitally re-mastering or otherwise modifying motion pictures or other image sequences data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7894633">US7894633</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 5, 2000</td><td class="patent-data-table-td patent-date-value">Feb 22, 2011</td><td class="patent-data-table-td ">Dynamic Digital Depth Research Pty Ltd</td><td class="patent-data-table-td ">Image conversion and encoding techniques</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7907793">US7907793</a></td><td class="patent-data-table-td patent-date-value">Aug 17, 2009</td><td class="patent-data-table-td patent-date-value">Mar 15, 2011</td><td class="patent-data-table-td ">Legend Films Inc.</td><td class="patent-data-table-td ">Image sequence depth enhancement system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7999844">US7999844</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 12, 2002</td><td class="patent-data-table-td patent-date-value">Aug 16, 2011</td><td class="patent-data-table-td ">Dynamic Digital Depth Research Pty Ltd.</td><td class="patent-data-table-td ">Image conversion and encoding techniques</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8073247">US8073247</a></td><td class="patent-data-table-td patent-date-value">Nov 1, 2010</td><td class="patent-data-table-td patent-date-value">Dec 6, 2011</td><td class="patent-data-table-td ">Legend3D, Inc.</td><td class="patent-data-table-td ">Minimal artifact image sequence depth enhancement system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8078006">US8078006</a></td><td class="patent-data-table-td patent-date-value">Nov 1, 2010</td><td class="patent-data-table-td patent-date-value">Dec 13, 2011</td><td class="patent-data-table-td ">Legend3D, Inc.</td><td class="patent-data-table-td ">Minimal artifact image sequence depth enhancement system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8160390">US8160390</a></td><td class="patent-data-table-td patent-date-value">Nov 1, 2010</td><td class="patent-data-table-td patent-date-value">Apr 17, 2012</td><td class="patent-data-table-td ">Legend3D, Inc.</td><td class="patent-data-table-td ">Minimal artifact image sequence depth enhancement system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8206218">US8206218</a></td><td class="patent-data-table-td patent-date-value">Feb 22, 2010</td><td class="patent-data-table-td patent-date-value">Jun 26, 2012</td><td class="patent-data-table-td ">Tdvision Corporation S.A. De C.V.</td><td class="patent-data-table-td ">3D videogame system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8213711">US8213711</a></td><td class="patent-data-table-td patent-date-value">Jul 23, 2009</td><td class="patent-data-table-td patent-date-value">Jul 3, 2012</td><td class="patent-data-table-td ">Her Majesty The Queen In Right Of Canada As Represented By The Minister Of Industry, Through The Communications Research Centre Canada</td><td class="patent-data-table-td ">Method and graphical user interface for modifying depth maps</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8217931">US8217931</a></td><td class="patent-data-table-td patent-date-value">Mar 25, 2011</td><td class="patent-data-table-td patent-date-value">Jul 10, 2012</td><td class="patent-data-table-td ">Conversion Works, Inc.</td><td class="patent-data-table-td ">System and method for processing video images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8243123">US8243123</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 2, 2006</td><td class="patent-data-table-td patent-date-value">Aug 14, 2012</td><td class="patent-data-table-td ">Geshwind David M</td><td class="patent-data-table-td ">Three-dimensional camera adjunct</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8249299">US8249299</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 17, 2009</td><td class="patent-data-table-td patent-date-value">Aug 21, 2012</td><td class="patent-data-table-td ">Adobe Systems Incorporated</td><td class="patent-data-table-td ">Systems and methods of tracking objects in video</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8274530">US8274530</a></td><td class="patent-data-table-td patent-date-value">Mar 11, 2008</td><td class="patent-data-table-td patent-date-value">Sep 25, 2012</td><td class="patent-data-table-td ">Conversion Works, Inc.</td><td class="patent-data-table-td ">Systems and methods for filling occluded information for 2-D to 3-D conversion</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8326088">US8326088</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 26, 2009</td><td class="patent-data-table-td patent-date-value">Dec 4, 2012</td><td class="patent-data-table-td ">The United States Of America As Represented By The Secretary Of The Air Force</td><td class="patent-data-table-td ">Dynamic image registration</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8340466">US8340466</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 31, 2005</td><td class="patent-data-table-td patent-date-value">Dec 25, 2012</td><td class="patent-data-table-td ">Heraeus Kulzer Gmbh</td><td class="patent-data-table-td ">Arrangement for the imaging of surface structures of three-dimensional objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8384763">US8384763</a></td><td class="patent-data-table-td patent-date-value">Jul 25, 2006</td><td class="patent-data-table-td patent-date-value">Feb 26, 2013</td><td class="patent-data-table-td ">Her Majesty the Queen in right of Canada as represented by the Minster of Industry, Through the Communications Research Centre Canada</td><td class="patent-data-table-td ">Generating a depth map from a two-dimensional source image for stereoscopic and multiview imaging</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8385684">US8385684</a></td><td class="patent-data-table-td patent-date-value">Feb 17, 2011</td><td class="patent-data-table-td patent-date-value">Feb 26, 2013</td><td class="patent-data-table-td ">Legend3D, Inc.</td><td class="patent-data-table-td ">System and method for minimal iteration workflow for image sequence depth enhancement</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8396328">US8396328</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 2010</td><td class="patent-data-table-td patent-date-value">Mar 12, 2013</td><td class="patent-data-table-td ">Legend3D, Inc.</td><td class="patent-data-table-td ">Minimal artifact image sequence depth enhancement system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8401336">US8401336</a></td><td class="patent-data-table-td patent-date-value">Dec 22, 2010</td><td class="patent-data-table-td patent-date-value">Mar 19, 2013</td><td class="patent-data-table-td ">Legend3D, Inc.</td><td class="patent-data-table-td ">System and method for rapid image sequence depth enhancement with augmented computer-generated elements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8411931">US8411931</a></td><td class="patent-data-table-td patent-date-value">Jun 25, 2007</td><td class="patent-data-table-td patent-date-value">Apr 2, 2013</td><td class="patent-data-table-td ">Imax Corporation</td><td class="patent-data-table-td ">Methods and systems for converting 2D motion pictures for stereoscopic 3D exhibition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8508550">US8508550</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 9, 2008</td><td class="patent-data-table-td patent-date-value">Aug 13, 2013</td><td class="patent-data-table-td ">Pixar</td><td class="patent-data-table-td ">Selective rendering of objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8620029">US8620029</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 23, 2012</td><td class="patent-data-table-td patent-date-value">Dec 31, 2013</td><td class="patent-data-table-td ">Adobe Systems Incorporated</td><td class="patent-data-table-td ">Systems and methods of tracking objects in video</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8624964">US8624964</a></td><td class="patent-data-table-td patent-date-value">Nov 27, 2006</td><td class="patent-data-table-td patent-date-value">Jan 7, 2014</td><td class="patent-data-table-td ">Koninklijke Philips N.V.</td><td class="patent-data-table-td ">Depth dependent filtering of image signal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8655052">US8655052</a></td><td class="patent-data-table-td patent-date-value">Jan 26, 2007</td><td class="patent-data-table-td patent-date-value">Feb 18, 2014</td><td class="patent-data-table-td ">Intellectual Discovery Co., Ltd.</td><td class="patent-data-table-td ">Methodology for 3D scene reconstruction from 2D image sequences</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8687123">US8687123</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 12, 2008</td><td class="patent-data-table-td patent-date-value">Apr 1, 2014</td><td class="patent-data-table-td ">Entropic Communications, Inc.</td><td class="patent-data-table-td ">Video signal processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8730232">US8730232</a></td><td class="patent-data-table-td patent-date-value">Feb 1, 2011</td><td class="patent-data-table-td patent-date-value">May 20, 2014</td><td class="patent-data-table-td ">Legend3D, Inc.</td><td class="patent-data-table-td ">Director-style based 2D to 3D movie conversion system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090304302">US20090304302</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 31, 2005</td><td class="patent-data-table-td patent-date-value">Dec 10, 2009</td><td class="patent-data-table-td ">Bernd Kordass</td><td class="patent-data-table-td ">Arrangement for the imaging of surface structures of three-dimensional objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100265402">US20100265402</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 12, 2008</td><td class="patent-data-table-td patent-date-value">Oct 21, 2010</td><td class="patent-data-table-td ">Koninklijke Philips Electronics N.V.</td><td class="patent-data-table-td ">Video signal processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110069064">US20110069064</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 28, 2008</td><td class="patent-data-table-td patent-date-value">Mar 24, 2011</td><td class="patent-data-table-td ">Thomson Licensing</td><td class="patent-data-table-td ">System and method for depth extraction of images with forward and backward depth prediction</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110286661">US20110286661</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 18, 2011</td><td class="patent-data-table-td patent-date-value">Nov 24, 2011</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Method and apparatus for temporally interpolating three-dimensional depth image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120013605">US20120013605</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 21, 2011</td><td class="patent-data-table-td patent-date-value">Jan 19, 2012</td><td class="patent-data-table-td ">Lg Electronics Inc.</td><td class="patent-data-table-td ">Mobile terminal and controlling method thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120086705">US20120086705</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 8, 2010</td><td class="patent-data-table-td patent-date-value">Apr 12, 2012</td><td class="patent-data-table-td ">City University Of Hong Kong</td><td class="patent-data-table-td ">Methods for creating and displaying two and three dimensional images on a digital canvas</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120197428">US20120197428</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 28, 2011</td><td class="patent-data-table-td patent-date-value">Aug 2, 2012</td><td class="patent-data-table-td ">Scott Weaver</td><td class="patent-data-table-td ">Method For Making a Piata</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120264515">US20120264515</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 21, 2012</td><td class="patent-data-table-td patent-date-value">Oct 18, 2012</td><td class="patent-data-table-td ">Tdvision Corporation S.A. De C.V.</td><td class="patent-data-table-td ">3d videogame system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120288155">US20120288155</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 23, 2012</td><td class="patent-data-table-td patent-date-value">Nov 15, 2012</td><td class="patent-data-table-td ">Adobe Systems Incorporated</td><td class="patent-data-table-td ">Systems and methods of tracking objects in video</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/USRE39342">USRE39342</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 22, 2002</td><td class="patent-data-table-td patent-date-value">Oct 17, 2006</td><td class="patent-data-table-td ">For3D, Inc.</td><td class="patent-data-table-td ">Method for producing a synthesized stereoscopic image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN1942902B?cl=en">CN1942902B</a></td><td class="patent-data-table-td patent-date-value">Apr 8, 2005</td><td class="patent-data-table-td patent-date-value">May 12, 2010</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ghost artifact reduction for rendering 2.5D graphics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101053000B?cl=en">CN101053000B</a></td><td class="patent-data-table-td patent-date-value">Sep 7, 2005</td><td class="patent-data-table-td patent-date-value">Jan 5, 2011</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">System and method for processing video images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101061519B?cl=en">CN101061519B</a></td><td class="patent-data-table-td patent-date-value">Nov 8, 2005</td><td class="patent-data-table-td patent-date-value">May 18, 2011</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Rendering images based on image segmentation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101322418B?cl=en">CN101322418B</a></td><td class="patent-data-table-td patent-date-value">Nov 27, 2006</td><td class="patent-data-table-td patent-date-value">Sep 1, 2010</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Depth dependent filtering of image signal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101632102B?cl=en">CN101632102B</a></td><td class="patent-data-table-td patent-date-value">Jan 2, 2008</td><td class="patent-data-table-td patent-date-value">Mar 7, 2012</td><td class="patent-data-table-td ">3D</td><td class="patent-data-table-td ">Method and apparatus for generating stereoscopic image from two-dimensional image by using mesh map</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0746949A1?cl=en">EP0746949A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 30, 1994</td><td class="patent-data-table-td patent-date-value">Dec 11, 1996</td><td class="patent-data-table-td ">Medi-Vision Technologies, Inc.</td><td class="patent-data-table-td ">Synthesized stereoscopic imaging system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0868818A1?cl=en">EP0868818A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 20, 1996</td><td class="patent-data-table-td patent-date-value">Oct 7, 1998</td><td class="patent-data-table-td ">Xenotech Research Pty. Ltd.</td><td class="patent-data-table-td ">Image conversion and encoding techniques</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1044432A1?cl=en">EP1044432A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 3, 1998</td><td class="patent-data-table-td patent-date-value">Oct 18, 2000</td><td class="patent-data-table-td ">Dynamic Digital Depth Research Pty. Ltd.</td><td class="patent-data-table-td ">Improved image conversion and encoding techniques</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1187494A2?cl=en">EP1187494A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 20, 1996</td><td class="patent-data-table-td patent-date-value">Mar 13, 2002</td><td class="patent-data-table-td ">Dynamic Digital Depth Research Pty. Ltd.</td><td class="patent-data-table-td ">Image conversion and encoding techniques</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1549084A1?cl=en">EP1549084A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 30, 2003</td><td class="patent-data-table-td patent-date-value">Jun 29, 2005</td><td class="patent-data-table-td ">Sharp Corporation</td><td class="patent-data-table-td ">Device capable of easily creating and editing a content which can be viewed in three-dimensional way</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1587035A1?cl=en">EP1587035A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 14, 2004</td><td class="patent-data-table-td patent-date-value">Oct 19, 2005</td><td class="patent-data-table-td ">Philips Electronics N.V.</td><td class="patent-data-table-td ">Ghost artifact reduction for rendering 2.5D graphics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2102822A1?cl=en">EP2102822A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 2, 2008</td><td class="patent-data-table-td patent-date-value">Sep 23, 2009</td><td class="patent-data-table-td ">Real Image Corp.</td><td class="patent-data-table-td ">Method and apparatus for generating stereoscopic image from two-dimensional image by using mesh map</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2157803A1?cl=en">EP2157803A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 19, 2007</td><td class="patent-data-table-td patent-date-value">Feb 24, 2010</td><td class="patent-data-table-td ">Thomson Licensing</td><td class="patent-data-table-td ">System and method for combining text with three-dimensional content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2337326A1?cl=en">EP2337326A1</a></td><td class="patent-data-table-td patent-date-value">Dec 15, 2009</td><td class="patent-data-table-td patent-date-value">Jun 22, 2011</td><td class="patent-data-table-td ">Deutsche Telekom AG</td><td class="patent-data-table-td ">Method and device for highlighting selected objects in image and video messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2337327A1?cl=en">EP2337327A1</a></td><td class="patent-data-table-td patent-date-value">Dec 15, 2009</td><td class="patent-data-table-td patent-date-value">Jun 22, 2011</td><td class="patent-data-table-td ">Deutsche Telekom AG</td><td class="patent-data-table-td ">Method and device for highlighting selected objects in image and video messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO1999001988A1?cl=en">WO1999001988A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 6, 1997</td><td class="patent-data-table-td patent-date-value">Jan 14, 1999</td><td class="patent-data-table-td ">Ericsson Ge Mobile Inc</td><td class="patent-data-table-td ">Three-dimensional imaging and display system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO1999003068A1?cl=en">WO1999003068A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 10, 1998</td><td class="patent-data-table-td patent-date-value">Jan 21, 1999</td><td class="patent-data-table-td ">Reveo Inc</td><td class="patent-data-table-td ">Method and apparatus for monoscopic to stereoscopic image conversion</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO1999012127A1?cl=en">WO1999012127A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 2, 1998</td><td class="patent-data-table-td patent-date-value">Mar 11, 1999</td><td class="patent-data-table-td ">Xenotech Res Pty Ltd</td><td class="patent-data-table-td ">Image processing method and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2000019265A1?cl=en">WO2000019265A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 1, 1999</td><td class="patent-data-table-td patent-date-value">Apr 6, 2000</td><td class="patent-data-table-td ">Thomas Riegel</td><td class="patent-data-table-td ">Arrangement and method for stereoscopic representation of an object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2000030039A1?cl=en">WO2000030039A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 18, 1999</td><td class="patent-data-table-td patent-date-value">May 25, 2000</td><td class="patent-data-table-td ">Push Entertainment Inc</td><td class="patent-data-table-td ">System and method for creating 3d models from 2d sequential image data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001076258A2?cl=en">WO2001076258A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 1, 2001</td><td class="patent-data-table-td patent-date-value">Oct 11, 2001</td><td class="patent-data-table-td ">Naske Rolf Dieter</td><td class="patent-data-table-td ">Generation of a sequence of stereoscopic images from a sequence of 2d images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2002027667A1?cl=en">WO2002027667A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 14, 2001</td><td class="patent-data-table-td patent-date-value">Apr 4, 2002</td><td class="patent-data-table-td ">Orasee Corp</td><td class="patent-data-table-td ">Method for automated two-dimensional and three-dimensional conversion</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2003105491A1?cl=en">WO2003105491A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 26, 2003</td><td class="patent-data-table-td patent-date-value">Dec 18, 2003</td><td class="patent-data-table-td ">Rafael Armament Dev Authority</td><td class="patent-data-table-td ">Method for producing stereoscopic images from monoscopic images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2005101324A1?cl=en">WO2005101324A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 8, 2005</td><td class="patent-data-table-td patent-date-value">Oct 27, 2005</td><td class="patent-data-table-td ">Robert-Paul M Berretty</td><td class="patent-data-table-td ">Ghost artifact reduction for rendering 2.5d graphics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2006004932A2?cl=en">WO2006004932A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 29, 2005</td><td class="patent-data-table-td patent-date-value">Jan 12, 2006</td><td class="patent-data-table-td ">In Three Inc</td><td class="patent-data-table-td ">Method for creating artifact free three-dimensional images converted from two-dimensional images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2006036469A2?cl=en">WO2006036469A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 7, 2005</td><td class="patent-data-table-td patent-date-value">Apr 6, 2006</td><td class="patent-data-table-td ">Conversion Works Inc</td><td class="patent-data-table-td ">System and method for processing video images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2007063477A2?cl=en">WO2007063477A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 27, 2006</td><td class="patent-data-table-td patent-date-value">Jun 7, 2007</td><td class="patent-data-table-td ">Koninkl Philips Electronics Nv</td><td class="patent-data-table-td ">Depth dependent filtering of image signal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2008115222A1?cl=en">WO2008115222A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 19, 2007</td><td class="patent-data-table-td patent-date-value">Sep 25, 2008</td><td class="patent-data-table-td ">Thomson Licensing</td><td class="patent-data-table-td ">System and method for combining text with three-dimensional content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2011034680A2?cl=en">WO2011034680A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 17, 2010</td><td class="patent-data-table-td patent-date-value">Mar 24, 2011</td><td class="patent-data-table-td ">Microvision, Inc.</td><td class="patent-data-table-td ">Three-dimensional display using an invisible wavelength light source</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc352/defs352.htm&usg=AFQjCNEbehx0-LFeJk8XaIOFM0RGGxUk2g#C352S057000">352/57</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc355/defs355.htm&usg=AFQjCNGzafFuG96UYXP219TiGmzTCfDa_A#C355S040000">355/40</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13062">348/E13.062</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13058">348/E13.058</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13033">348/E13.033</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13059">348/E13.059</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13039">348/E13.039</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13061">348/E13.061</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13040">348/E13.04</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc352/defs352.htm&usg=AFQjCNEbehx0-LFeJk8XaIOFM0RGGxUk2g#C352S086000">352/86</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13023">348/E13.023</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13037">348/E13.037</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13020">348/E13.02</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13038">348/E13.038</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13071">348/E13.071</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13008">348/E13.008</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13072">348/E13.072</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13065">348/E13.065</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13044">348/E13.044</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13019">348/E13.019</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE13067">348/E13.067</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0019000000">G06T19/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0003153000">G06F3/153</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04N0013000000">H04N13/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04L0029060000">H04L29/06</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L69/16">H04L69/16</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0037">H04N13/0037</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0289">H04N13/0289</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0221">H04N13/0221</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0438">H04N13/0438</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N2013/0081">H04N2013/0081</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/026">H04N13/026</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N2013/0092">H04N2013/0092</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0422">H04N13/0422</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N19/00769">H04N19/00769</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0048">H04N13/0048</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0055">H04N13/0055</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0431">H04N13/0431</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0459">H04N13/0459</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0456">H04N13/0456</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0436">H04N13/0436</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0434">H04N13/0434</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0011">H04N13/0011</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0257">H04N13/0257</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06T19/00">G06T19/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0497">H04N13/0497</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0018">H04N13/0018</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=PTctBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N13/0059">H04N13/0059</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">H04N13/02C</span>, <span class="nested-value">H04N13/02A1M</span>, <span class="nested-value">G06T19/00</span>, <span class="nested-value">H04N19/00P5</span>, <span class="nested-value">H04N13/00P1B</span>, <span class="nested-value">H04L29/06J</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Nov 6, 2012</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIM 25 IS CONFIRMED.CLAIMS 1-24 AND 26-44 ARE CANCELLED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 28, 2007</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">GESHWIND, DAVID M., NEW YORK</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GESHWIND, FRANK B.;REEL/FRAME:019754/0069</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19900701</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">GESHWIND, FRANK B., NEW YORK</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:AVELLONE, ANNE C.;REEL/FRAME:019754/0148</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19900415</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">THREE-DIMENSIONAL MEDIA GROUP, LTD., NEW YORK</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:GESHWIND, DAVID M.;HANDAL, ANTHONY H.;REEL/FRAME:019754/0151</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20050128</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 19, 2005</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20050609</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 15, 2002</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 15, 2002</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">11</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 4, 2001</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 17, 1999</td><td class="patent-data-table-td ">PRDP</td><td class="patent-data-table-td ">Patent reinstated due to the acceptance of a late maintenance fee</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19990507</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 6, 1998</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 6, 1998</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 16, 1994</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 16, 1994</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 10, 1994</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U10WpKLp7pudG_r7nwhTpP7atxzAw\u0026id=PTctBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U1nnehtGuzuEzi8sj7SvD0rbJmY1A\u0026id=PTctBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U0tbX15VmKhFNcfIwEgoT6U4NiXOQ","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Method_to_convert_two_dimensional_motion.pdf?id=PTctBAABERAJ\u0026output=pdf\u0026sig=ACfU3U3LPsBReJA-Je2H_ss72es6GRLBfA"},"sample_url":"http://www.google.com/patents/reader?id=PTctBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>