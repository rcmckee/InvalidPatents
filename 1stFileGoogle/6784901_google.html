<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6784901 - Method, system and computer program product for the delivery of a chat ... - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Method, system and computer program product for the delivery of a chat message in a 3D multi-user environment"><meta name="DC.contributor" content="William D. Harvey" scheme="inventor"><meta name="DC.contributor" content="Jason G. McHugh" scheme="inventor"><meta name="DC.contributor" content="Fernando J. Paiz" scheme="inventor"><meta name="DC.contributor" content="Jeffrey J. Ventrella" scheme="inventor"><meta name="DC.contributor" content="There" scheme="assignee"><meta name="DC.date" content="2000-8-31" scheme="dateSubmitted"><meta name="DC.description" content="A chat system, method and computer program product for delivering a message between a sender and a recipient in a three-dimensional (3D) multi-user environment, wherein the 3D multi-user environment maintains respective digital representations of the sender and the recipient, uses a recipient interface to receive a message, map the message to a texture to generate a textured message, and render the textured message in the 3D multi-user environment so as to permit the recipient to visually ascertain the location of the digital representation of the sender in the 3D world. Received messages are mantained as two-dimensional elements on a recipient viewport."><meta name="DC.date" content="2004-8-31" scheme="issued"><meta name="DC.relation" content="US:5021976" scheme="references"><meta name="DC.relation" content="US:5423554" scheme="references"><meta name="DC.relation" content="US:5515078" scheme="references"><meta name="DC.relation" content="US:5559995" scheme="references"><meta name="DC.relation" content="US:5587936" scheme="references"><meta name="DC.relation" content="US:5588104" scheme="references"><meta name="DC.relation" content="US:5588139" scheme="references"><meta name="DC.relation" content="US:5633993" scheme="references"><meta name="DC.relation" content="US:5644993" scheme="references"><meta name="DC.relation" content="US:5679075" scheme="references"><meta name="DC.relation" content="US:5684943" scheme="references"><meta name="DC.relation" content="US:5696892" scheme="references"><meta name="DC.relation" content="US:5704837" scheme="references"><meta name="DC.relation" content="US:5732232" scheme="references"><meta name="DC.relation" content="US:5736982" scheme="references"><meta name="DC.relation" content="US:5755620" scheme="references"><meta name="DC.relation" content="US:5784570" scheme="references"><meta name="DC.relation" content="US:5793382" scheme="references"><meta name="DC.relation" content="US:5815156" scheme="references"><meta name="DC.relation" content="US:5827120" scheme="references"><meta name="DC.relation" content="US:5846134" scheme="references"><meta name="DC.relation" content="US:5852672" scheme="references"><meta name="DC.relation" content="US:5874956" scheme="references"><meta name="DC.relation" content="US:5880731" scheme="references"><meta name="DC.relation" content="US:5884029" scheme="references"><meta name="DC.relation" content="US:5903395" scheme="references"><meta name="DC.relation" content="US:5903396" scheme="references"><meta name="DC.relation" content="US:5907328" scheme="references"><meta name="DC.relation" content="US:5913727" scheme="references"><meta name="DC.relation" content="US:5919045" scheme="references"><meta name="DC.relation" content="US:5926179" scheme="references"><meta name="DC.relation" content="US:5926400" scheme="references"><meta name="DC.relation" content="US:5947823" scheme="references"><meta name="DC.relation" content="US:5951015" scheme="references"><meta name="DC.relation" content="US:5956038" scheme="references"><meta name="DC.relation" content="US:5966129" scheme="references"><meta name="DC.relation" content="US:5966130" scheme="references"><meta name="DC.relation" content="US:5980256" scheme="references"><meta name="DC.relation" content="US:5982372" scheme="references"><meta name="DC.relation" content="US:5982390" scheme="references"><meta name="DC.relation" content="US:5983003" scheme="references"><meta name="DC.relation" content="US:5999641" scheme="references"><meta name="DC.relation" content="US:5999944" scheme="references"><meta name="DC.relation" content="US:6001065" scheme="references"><meta name="DC.relation" content="US:6009458" scheme="references"><meta name="DC.relation" content="US:6009460" scheme="references"><meta name="DC.relation" content="US:6011558" scheme="references"><meta name="DC.relation" content="US:6014136" scheme="references"><meta name="DC.relation" content="US:6014142" scheme="references"><meta name="DC.relation" content="US:6020885" scheme="references"><meta name="DC.relation" content="US:6020891" scheme="references"><meta name="DC.relation" content="US:6023270" scheme="references"><meta name="DC.relation" content="US:6024643" scheme="references"><meta name="DC.relation" content="US:6028584" scheme="references"><meta name="DC.relation" content="US:6031549" scheme="references"><meta name="DC.relation" content="US:6034692" scheme="references"><meta name="DC.relation" content="US:6040841" scheme="references"><meta name="DC.relation" content="US:6040842" scheme="references"><meta name="DC.relation" content="US:6052114" scheme="references"><meta name="DC.relation" content="US:6054991" scheme="references"><meta name="DC.relation" content="US:6055563" scheme="references"><meta name="DC.relation" content="US:6057810" scheme="references"><meta name="DC.relation" content="US:6057828" scheme="references"><meta name="DC.relation" content="US:6057856" scheme="references"><meta name="DC.relation" content="US:6072466" scheme="references"><meta name="DC.relation" content="US:6072478" scheme="references"><meta name="DC.relation" content="US:6073115" scheme="references"><meta name="DC.relation" content="US:6434604" scheme="references"><meta name="citation_reference" content="Brewer, Stephen et al., &quot;A New World of Animated Discussions: 3D Chat-See why online chats&#39; new visual trappings are the talk of the town,&quot; Home PC, CMP Media, 1997, No. 403, p. 88."><meta name="citation_reference" content="D&#39;Amico, Marie, &quot;Who wnats to live in cyberspace? Virtual world developers bet we all do (virtual worlds are not useful nor valuable),&quot; Digital Media, Seybold Publications, Inc., vol. 5, No. 2, p. 9 (4), Jul. 3, 1995."><meta name="citation_reference" content="Feiner, Steven K., &quot;Research in 3D user interface design at Columbia University,&quot; Feiner@cs,columbia,edu."><meta name="citation_reference" content="Gabriel Knight 3 (Sierra Online&#39;s adventure game) (Software Review) (Evaluation), Computer Gaming World, Golden Empire Publications, Inc., p. 62 (1), Jun. 1999."><meta name="citation_reference" content="Gordon Brockhouse, &quot;3D Explodes!-New 3D video accelerators and sound cards can add mind-blowing realism to your games,&quot; Home PC, CMP Media, No. 503, p. 64, 1998."><meta name="citation_reference" content="Gordon Brockhouse, “3D Explodes!—New 3D video accelerators and sound cards can add mind-blowing realism to your games,” Home PC, CMP Media, No. 503, p. 64, 1998."><meta name="citation_reference" content="Greenhalgh, C., et al., &quot;Creating a live broadcast from a virtual environment,&quot; Proceedings of SIGGRAPH 99: 26&lt;th &gt;Internatioanl Conference on Computer Graphics and Interactive Techniques, IEEE, Aug. 8-13, 1999."><meta name="citation_reference" content="Greenhalgh, C., et al., “Creating a live broadcast from a virtual environment,” Proceedings of SIGGRAPH 99: 26th Internatioanl Conference on Computer Graphics and Interactive Techniques, IEEE, Aug. 8-13, 1999."><meta name="citation_reference" content="Heid, Jim, &quot;Face-to-face online,&quot; Macworld, Macworld Communications Inc., vol. 14, No. 1, p. 146, Jan. 1997."><meta name="citation_reference" content="Iworld-ZDNet Offers Virtual Reality on the Web, Newsbytes Inc., Nov. 3, 1995."><meta name="citation_reference" content="James Jung-Hoon Seo, &quot;Active Worlds&quot;, Mar. 13, 2000, &lt;http://acg.media.mit.edu/people/jseo/courses/mas963/assign5-activeworlds.html&gt; (visited Mar. 11, 2003)."><meta name="citation_reference" content="Joch, Alan, &quot;Enterprises now donning 3-D glasses: 3-D interfaces move to handle corporate e-biz (Technology Information),&quot; PC Week, Ziff-Davis Publishing Company, p. 59, Nov. 8, 1999."><meta name="citation_reference" content="Kaplan, F., et al., &quot;Growing virtual communities in 3D meeting spaces,&quot; Virtual Worlds First International Conference, VW &#39;98 Proceedings, IEEE, Jul. 1-3, 1998, (Abstract)."><meta name="citation_reference" content="Krista Ostertag, &quot;3D tools for virtual worlds,&quot; Varbusiness, CMP Media, No. 1203, p. 24, 1996."><meta name="citation_reference" content="Kyoya, T., et al., &quot;A 3D game with force feedback in a virtual space-virtual Tower of Hanoi,&quot; Journal of the Institute of Television Engineers of Japan, IEEE, vol. 49, No. 10, pp. 1347-1352, Oct. 1995, (Abstract)."><meta name="citation_reference" content="Leibs, B., et al., &quot;The Perceptive Workbench: Toward spontaneous and natural interaction in semi-immersive virtual environments,&quot; Proceedings IEEE Virtual Reality 2000, IEEE, pp. 13-20, Mar. 18-22, 2000."><meta name="citation_reference" content="Mohageg, Mike, et al., &quot;A user interface for accessing 3D content on the World Wide Web.&quot;"><meta name="citation_reference" content="New Dimension for 3D chat, Home PC, CMP Media, No. 405, p. 27, 1997."><meta name="citation_reference" content="Patent application No. 09/653,276; Specification and Figures 1-14 from Method, System and Computer Program Product for Providing a Two-Dimensional Interface to a Three-Dimensional World, Harvey, William et al."><meta name="citation_reference" content="The Client side of Virtual Worlds (Immersive Systems, Knowledge Adventure Worlds, Apple&#39;s QuickTime VR desktop video software, Virtual Games&#39; Stim-Slum computer game), Release 1.0, Edventure Holdings Inc., vol. 94, No. 6, p. 12 (7), Jun. 27, 1994."><meta name="citation_reference" content="Vacca, John R., &quot;3D worlds on the Web,&quot; Computer Graphics World, Pennwell Publishing Company, vol. 19, No. 5, p. 43 (6), May 1996."><meta name="citation_reference" content="Vallina, Joe, &quot;This &quot;3D&quot; Hunting Game Takes 1 Step Forward, 10 Steps Back,&quot; Computer Gaming World, Golden Empire Publications, Inc., No. 172, p. 299 (1), Nov. 1998."><meta name="citation_reference" content="Virtools to Launch 3D Games Development Tool, Computergram International, Computerwire, Inc., No. 126, Jun. 25, 1998."><meta name="citation_reference" content="Visual Interface, Inc., &quot;The Future of 3D Imaging.&quot;"><meta name="citation_patent_number" content="US:6784901"><meta name="citation_patent_application_number" content="US:09/653,352"><link rel="canonical" href="http://www.google.com/patents/US6784901"/><meta property="og:url" content="http://www.google.com/patents/US6784901"/><meta name="title" content="Patent US6784901 - Method, system and computer program product for the delivery of a chat message in a 3D multi-user environment"/><meta name="description" content="A chat system, method and computer program product for delivering a message between a sender and a recipient in a three-dimensional (3D) multi-user environment, wherein the 3D multi-user environment maintains respective digital representations of the sender and the recipient, uses a recipient interface to receive a message, map the message to a texture to generate a textured message, and render the textured message in the 3D multi-user environment so as to permit the recipient to visually ascertain the location of the digital representation of the sender in the 3D world. Received messages are mantained as two-dimensional elements on a recipient viewport."/><meta property="og:title" content="Patent US6784901 - Method, system and computer program product for the delivery of a chat message in a 3D multi-user environment"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("najtU6CGJsqysASB_YC4BA"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("NOR"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("najtU6CGJsqysASB_YC4BA"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("NOR"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6784901?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6784901"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=OlNoBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6784901&amp;usg=AFQjCNEoo8dtSekoNX674cKDEoCI3UmP5g" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6784901.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6784901.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6784901" style="display:none"><span itemprop="description">A chat system, method and computer program product for delivering a message between a sender and a recipient in a three-dimensional (3D) multi-user environment, wherein the 3D multi-user environment maintains respective digital representations of the sender and the recipient, uses a recipient interface...</span><span itemprop="url">http://www.google.com/patents/US6784901?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6784901 - Method, system and computer program product for the delivery of a chat message in a 3D multi-user environment</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6784901 - Method, system and computer program product for the delivery of a chat message in a 3D multi-user environment" title="Patent US6784901 - Method, system and computer program product for the delivery of a chat message in a 3D multi-user environment"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6784901 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/653,352</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Aug 31, 2004</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Aug 31, 2000</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">May 9, 2000</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">09653352, </span><span class="patent-bibdata-value">653352, </span><span class="patent-bibdata-value">US 6784901 B1, </span><span class="patent-bibdata-value">US 6784901B1, </span><span class="patent-bibdata-value">US-B1-6784901, </span><span class="patent-bibdata-value">US6784901 B1, </span><span class="patent-bibdata-value">US6784901B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22William+D.+Harvey%22">William D. Harvey</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Jason+G.+McHugh%22">Jason G. McHugh</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Fernando+J.+Paiz%22">Fernando J. Paiz</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Jeffrey+J.+Ventrella%22">Jeffrey J. Ventrella</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22There%22">There</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6784901.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6784901.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6784901.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (68),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (24),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (134),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (25),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (8)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6784901&usg=AFQjCNFTbHaOpNQYty9Jc5VeNJ7A6KaUrw">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6784901&usg=AFQjCNHNvNwH_nHOlKhARbM6cwRjPVVlyQ">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6784901B1%26KC%3DB1%26FT%3DD&usg=AFQjCNEALlfuy2XeQz3yRjl2LQLAlO8FKg">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55313285" lang="EN" load-source="patent-office">Method, system and computer program product for the delivery of a chat message in a 3D multi-user environment</invention-title></span><br><span class="patent-number">US 6784901 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA50715133" lang="EN" load-source="patent-office"> <div class="abstract">A chat system, method and computer program product for delivering a message between a sender and a recipient in a three-dimensional (3D) multi-user environment, wherein the 3D multi-user environment maintains respective digital representations of the sender and the recipient, uses a recipient interface to receive a message, map the message to a texture to generate a textured message, and render the textured message in the 3D multi-user environment so as to permit the recipient to visually ascertain the location of the digital representation of the sender in the 3D world. Received messages are mantained as two-dimensional elements on a recipient viewport.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(23)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00018.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00018.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00019.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00019.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00020.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00020.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00021.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00021.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6784901B1/US06784901-20040831-D00022.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6784901B1/US06784901-20040831-D00022.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(87)</span></span></div><div class="patent-text"><div mxw-id="PCLM8726293" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is: </claim-statement> <div class="claim"> <div num="1" id="US-6784901-B1-CLM-00001" class="claim">
      <div class="claim-text">1. A system for delivering a message between a sender and a recipient in a three-dimensional multi-user environment, wherein said three-dimensional multi-user environment maintains respective digital representations of the sender and the recipient, comprising:</div>
      <div class="claim-text">a sender interface; and </div>
      <div class="claim-text">a recipient interface, including a recipient viewport; </div>
      <div class="claim-text">wherein said recipient interface receives the message from said sender interface, maps the message to a texture to generate a textured message, and renders said textured message at locations along a path in the three-dimensional multi-user environment, whereby the recipient can visually ascertain at least portions of the path of the textured message through said recipient viewport. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" id="US-6784901-B1-CLM-00002" class="claim">
      <div class="claim-text">2. The system of <claim-ref idref="US-6784901-B1-CLM-00001">claim 1</claim-ref>, wherein said sender and recipient interfaces comprise software modules and the message is sent from said sender interface to said recipient interface over the Internet.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" id="US-6784901-B1-CLM-00003" class="claim">
      <div class="claim-text">3. The system of <claim-ref idref="US-6784901-B1-CLM-00001">claim 1</claim-ref>, wherein said recipient interface receives appearance information and uses said appearance information to generate said textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" id="US-6784901-B1-CLM-00004" class="claim">
      <div class="claim-text">4. The system of <claim-ref idref="US-6784901-B1-CLM-00003">claim 3</claim-ref>, wherein said appearance information is sent from said sender interface to said recipient interface.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" id="US-6784901-B1-CLM-00005" class="claim">
      <div class="claim-text">5. The system of <claim-ref idref="US-6784901-B1-CLM-00003">claim 3</claim-ref>, wherein said appearance information is indicative of the identity of the sender.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" id="US-6784901-B1-CLM-00006" class="claim">
      <div class="claim-text">6. The system of <claim-ref idref="US-6784901-B1-CLM-00003">claim 3</claim-ref>, wherein said appearance information includes color information.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" id="US-6784901-B1-CLM-00007" class="claim">
      <div class="claim-text">7. The system of <claim-ref idref="US-6784901-B1-CLM-00003">claim 3</claim-ref>, wherein said appearance information includes font information.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" id="US-6784901-B1-CLM-00008" class="claim">
      <div class="claim-text">8. The system of <claim-ref idref="US-6784901-B1-CLM-00003">claim 3</claim-ref>, wherein said appearance information includes point size information.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" id="US-6784901-B1-CLM-00009" class="claim">
      <div class="claim-text">9. The system of <claim-ref idref="US-6784901-B1-CLM-00003">claim 3</claim-ref>, wherein said appearance information is indicative of the content of the message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" id="US-6784901-B1-CLM-00010" class="claim">
      <div class="claim-text">10. The system of <claim-ref idref="US-6784901-B1-CLM-00003">claim 3</claim-ref>, wherein said appearance information is indicative of the number of recipients of the message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" id="US-6784901-B1-CLM-00011" class="claim">
      <div class="claim-text">11. The system of <claim-ref idref="US-6784901-B1-CLM-00001">claim 1</claim-ref>, wherein said recipient interface generates a textured message that is a different color than the colors used to define the three-dimensional multi-user environment along the path of the textured message, thereby ensuring the legibility of the textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" id="US-6784901-B1-CLM-00012" class="claim">
      <div class="claim-text">12. The system of <claim-ref idref="US-6784901-B1-CLM-00001">claim 1</claim-ref>, wherein said recipient interface obtains a sound associated with said textured message, and plays said sound contemporaneously with the rendering of said textured message in the three-dimensional multi-user environment.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" id="US-6784901-B1-CLM-00013" class="claim">
      <div class="claim-text">13. The system of <claim-ref idref="US-6784901-B1-CLM-00012">claim 12</claim-ref>, wherein said recipient interface receives audio information and uses said audio information to obtain said sound associated with the textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" id="US-6784901-B1-CLM-00014" class="claim">
      <div class="claim-text">14. The system of <claim-ref idref="US-6784901-B1-CLM-00013">claim 13</claim-ref>, wherein said audio information is sent from said sender interface to said recipient interface.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" id="US-6784901-B1-CLM-00015" class="claim">
      <div class="claim-text">15. The system of <claim-ref idref="US-6784901-B1-CLM-00013">claim 13</claim-ref>, wherein said audio information is indicative of the identity of the sender.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" id="US-6784901-B1-CLM-00016" class="claim">
      <div class="claim-text">16. The system of <claim-ref idref="US-6784901-B1-CLM-00012">claim 12</claim-ref>, wherein said sound associated with said textured message is indicative of the content of the message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" id="US-6784901-B1-CLM-00017" class="claim">
      <div class="claim-text">17. The system of <claim-ref idref="US-6784901-B1-CLM-00012">claim 12</claim-ref>, wherein said sound associated with said textured message is indicative of the number of recipients of the message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" id="US-6784901-B1-CLM-00018" class="claim">
      <div class="claim-text">18. The system of <claim-ref idref="US-6784901-B1-CLM-00012">claim 12</claim-ref>, wherein said sound associated with said textured message is indicative of the location of the digital representation of the sender in the three-dimensional multi-user environment.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" id="US-6784901-B1-CLM-00019" class="claim">
      <div class="claim-text">19. The system of <claim-ref idref="US-6784901-B1-CLM-00001">claim 1</claim-ref>, wherein said recipient interface generates an animation texture associated with said textured message, and renders said animation texture at locations along a path in the three-dimensional multi-user environment, whereby the recipient can visually ascertain at least portions of the path of the animation texture through said recipient viewport.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" id="US-6784901-B1-CLM-00020" class="claim">
      <div class="claim-text">20. The system of <claim-ref idref="US-6784901-B1-CLM-00019">claim 19</claim-ref>, wherein said recipient interface receives appearance information and uses said appearance information to generate said animation texture.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" id="US-6784901-B1-CLM-00021" class="claim">
      <div class="claim-text">21. The system of <claim-ref idref="US-6784901-B1-CLM-00020">claim 20</claim-ref>, wherein said appearance information is sent from said sender interface to said recipient interface.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="22" id="US-6784901-B1-CLM-00022" class="claim">
      <div class="claim-text">22. The system of <claim-ref idref="US-6784901-B1-CLM-00020">claim 20</claim-ref>, wherein said appearance information is indicative of the identity of the sender.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="23" id="US-6784901-B1-CLM-00023" class="claim">
      <div class="claim-text">23. The system of <claim-ref idref="US-6784901-B1-CLM-00020">claim 20</claim-ref>, wherein said appearance information is indicative of the content of the message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="24" id="US-6784901-B1-CLM-00024" class="claim">
      <div class="claim-text">24. The system of <claim-ref idref="US-6784901-B1-CLM-00001">claim 1</claim-ref>, wherein said recipient interface displays said textured message as a two-dimensional clement on said recipient viewport after rendering said textured message in the three-dimensional multi-user environment.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" id="US-6784901-B1-CLM-00025" class="claim">
      <div class="claim-text">25. The system of <claim-ref idref="US-6784901-B1-CLM-00024">claim 24</claim-ref>, wherein said recipient interface displays said two-dimensional element on said recipient viewport for a predetermined time period.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="26" id="US-6784901-B1-CLM-00026" class="claim">
      <div class="claim-text">26. The system of <claim-ref idref="US-6784901-B1-CLM-00024">claim 24</claim-ref>, wherein said recipient interface displays said textured message as a two-dimensional element on the bottom of said recipient viewport, and vertically displaces said two-dimensional element to accomodate other textured messages at the bottom of said recipient viewport.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="27" id="US-6784901-B1-CLM-00027" class="claim">
      <div class="claim-text">27. The system of <claim-ref idref="US-6784901-B1-CLM-00024">claim 24</claim-ref>, wherein said recipient interface displays said two-dimensional element in horizontal alignment w with the appearance of the digital representation of the sender in said recipient viewport.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="28" id="US-6784901-B1-CLM-00028" class="claim">
      <div class="claim-text">28. The system of <claim-ref idref="US-6784901-B1-CLM-00024">claim 24</claim-ref>, wherein said recipient interface displays said two-dimensional element flush right on said recipient viewport when the digital representation of the sender is to the right of the scope of said recipient viewport and flush left on said recipient viewport when the digital representation of the sender is to the left of the scope of said recipient viewport.</div>
    </div>
    </div> <div class="claim"> <div num="29" id="US-6784901-B1-CLM-00029" class="claim">
      <div class="claim-text">29. A system for delivering a message between a sender and a recipient in a three-dimensional multi-user environment, wherein said three-dimensional multi-user environment maintains respective digital representations of the sender and the recipient, comprising:</div>
      <div class="claim-text">a sender interface; </div>
      <div class="claim-text">a recipient interface, including a recipient viewport; </div>
      <div class="claim-text">wherein said recipient interface receives the message from said sender interface, maps the message to a texture to generate a textured message, selects a first location in the three-dimensional multi-user environment closer to the digital representation of the sender than to the digital representation of the recipient, selects a second location in the three-dimensional multi-user environment closer to the digital representation of the recipient than to the digital representation of the sender, and periodically renders said textured message in the three-dimensional multi-user environment along a path beginning at said first location and ending at said second location, so as to permit the recipient to visually ascertain the location of the digital representation of the sender through said recipient viewport. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="30" id="US-6784901-B1-CLM-00030" class="claim">
      <div class="claim-text">30. The system of <claim-ref idref="US-6784901-B1-CLM-00029">claim 29</claim-ref>, wherein said recipient interface displays said textured message as a two-dimensional element on said recipient viewport after rendering said textured message at said second location in the three-dimensional multi-user environment.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="31" id="US-6784901-B1-CLM-00031" class="claim">
      <div class="claim-text">31. The system of <claim-ref idref="US-6784901-B1-CLM-00029">claim 29</claim-ref>, wherein said recipient interface receives path information and uses said path information to define the shape of said path between said first location and said second location.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="32" id="US-6784901-B1-CLM-00032" class="claim">
      <div class="claim-text">32. The system of <claim-ref idref="US-6784901-B1-CLM-00031">claim 31</claim-ref>, wherein said path information is sent from said sender interface to said recipient interface.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="33" id="US-6784901-B1-CLM-00033" class="claim">
      <div class="claim-text">33. The system of <claim-ref idref="US-6784901-B1-CLM-00031">claim 31</claim-ref>, wherein said path information is indicative of the identity of the sender.</div>
    </div>
    </div> <div class="claim"> <div num="34" id="US-6784901-B1-CLM-00034" class="claim">
      <div class="claim-text">34. A system for delivering a message between a sender and a recipient in a three-dimensional multi-user environment, wherein said three-dimensional multi-user environment maintains respective digital representations of the sender and the recipient, comprising:</div>
      <div class="claim-text">a sender interface; </div>
      <div class="claim-text">a recipient interface, including a recipient viewport; </div>
      <div class="claim-text">wherein said recipient interface receives the message from said sender interface, maps the message to a texture to generate a textured message, periodically determines the location of the digital representation of the sender and the location of the digital representation of the recipient, and periodically renders said textured message in the three-dimensional multi-user environment along a path between said location of the digital representation of the sender and said location of the digital representation of the recipient, so as to permit the recipient to visually ascertain said location of the digital representation of the sender through the recipient viewport. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="35" id="US-6784901-B1-CLM-00035" class="claim">
      <div class="claim-text">35. The system of <claim-ref idref="US-6784901-B1-CLM-00034">claim 34</claim-ref>, wherein said recipient interface displays said textured message as a two-dimensional element on the recipient viewport after periodically rendering said textured message along a path between said location of the digital representation of the sender and said location of the digital representation of the recipient.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="36" id="US-6784901-B1-CLM-00036" class="claim">
      <div class="claim-text">36. The system of <claim-ref idref="US-6784901-B1-CLM-00034">claim 34</claim-ref> wherein said recipient interface receives path information, and uses said path information to define the shape of said path between said location of the digital representation of the sender and said location of the digital representation of the recipient.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="37" id="US-6784901-B1-CLM-00037" class="claim">
      <div class="claim-text">37. The system of <claim-ref idref="US-6784901-B1-CLM-00036">claim 36</claim-ref> wherein said path information is sent from said sender interface to said recipient interface.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="38" id="US-6784901-B1-CLM-00038" class="claim">
      <div class="claim-text">38. The system of <claim-ref idref="US-6784901-B1-CLM-00036">claim 36</claim-ref>, wherein said path information is indicative of the identity of the sender.</div>
    </div>
    </div> <div class="claim"> <div num="39" id="US-6784901-B1-CLM-00039" class="claim">
      <div class="claim-text">39. A chat system for delivering a message between a sender and a recipient in a three-dimensional multi-user environment, wherein said three-dimensional multi-user environment maintains respective digital representations of the sender and the recipient, comprising:</div>
      <div class="claim-text">a recipient interface; </div>
      <div class="claim-text">wherein said recipient interface receives a message, maps the message to a texture to generate a textured message, and renders said textured message in the three-dimensional multi-user environment so as to permit the recipient to visually ascertain the location of the digital representation of the sender. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="40" id="US-6784901-B1-CLM-00040" class="claim">
      <div class="claim-text">40. The chat system of <claim-ref idref="US-6784901-B1-CLM-00039">claim 39</claim-ref>, wherein said recipient interface renders said textured message in the three-dimensional multi-user environment closer to the digital representation of the sender than to the digital representation of the recipient, and subsequently renders said textured message in the three-dimensional multi-user environment closer to the digital representation of the recipient than to the digital representation of the sender.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="41" id="US-6784901-B1-CLM-00041" class="claim">
      <div class="claim-text">41. The chat system of <claim-ref idref="US-6784901-B1-CLM-00040">claim 40</claim-ref>, wherein said recipient interface comprises a software module and the message is sent to said recipient interface over the Internet.</div>
    </div>
    </div> <div class="claim"> <div num="42" id="US-6784901-B1-CLM-00042" class="claim">
      <div class="claim-text">42. A method for receiving a message sent from a sender to a recipient in a three-dimensional multi-user environment wherein the recipient has a viewport into the three-dimensional multi-user environment and the sender and recipient are each digitally represented therein, comprising the steps of:</div>
      <div class="claim-text">receiving the message; </div>
      <div class="claim-text">mapping the message to a texture to generate a textured message; and </div>
      <div class="claim-text">rendering said textured message at locations along a path in the three-dimensional multi-user environment, whereby the recipient can visually ascertain at least portions of the path of said textured message through the recipient viewport. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="43" id="US-6784901-B1-CLM-00043" class="claim">
      <div class="claim-text">43. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, wherein the message is sent from the sender to the recipient over the Internet.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="44" id="US-6784901-B1-CLM-00044" class="claim">
      <div class="claim-text">44. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of receiving appearance information, and using said appearance information to generate said textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="45" id="US-6784901-B1-CLM-00045" class="claim">
      <div class="claim-text">45. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of receiving appearance information from the sender, and using said appearance information to generate said textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="46" id="US-6784901-B1-CLM-00046" class="claim">
      <div class="claim-text">46. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of receiving appearance information indicative of the identity of the sender, and using said appearance information to generate said textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="47" id="US-6784901-B1-CLM-00047" class="claim">
      <div class="claim-text">47. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of receiving appearance information that includes color information, and using said appearance information to generate said textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="48" id="US-6784901-B1-CLM-00048" class="claim">
      <div class="claim-text">48. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of receiving appearance information that includes font information, and using said appearance information to generate said textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="49" id="US-6784901-B1-CLM-00049" class="claim">
      <div class="claim-text">49. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref> further comprising the steps of receiving appearance information that includes point size information, and using said appearance information to obtain said textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="50" id="US-6784901-B1-CLM-00050" class="claim">
      <div class="claim-text">50. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of receiving appearance information indicative of the content of the message, and using said appearance information to generate said textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="51" id="US-6784901-B1-CLM-00051" class="claim">
      <div class="claim-text">51. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of receiving appearance information indicative of the number of recipients of the message, and using said appearance information to generate said textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="52" id="US-6784901-B1-CLM-00052" class="claim">
      <div class="claim-text">52. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, wherein said step of mapping the message to a texture to generate a textured message further comprises generating a textured message that is a different color than the colors used to define the three-dimensional multi-user environment along the path of said textured message, thereby ensuring the legibility of the textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="53" id="US-6784901-B1-CLM-00053" class="claim">
      <div class="claim-text">53. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">obtaining a sound associated with said textured message; and </div>
      <div class="claim-text">playing said sound contemporaneously with the rendering of said textured message in the three-dimensional multi-user environment. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="54" id="US-6784901-B1-CLM-00054" class="claim">
      <div class="claim-text">54. The method of <claim-ref idref="US-6784901-B1-CLM-00053">claim 53</claim-ref>, further comprising the steps of receiving audio information and using said audio information to obtain said sound associated with said textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="55" id="US-6784901-B1-CLM-00055" class="claim">
      <div class="claim-text">55. The method of <claim-ref idref="US-6784901-B1-CLM-00053">claim 53</claim-ref>, further comprising the steps of receiving audio information from the sender and using said audio information to obtain said sound associated with said textured message.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="56" id="US-6784901-B1-CLM-00056" class="claim">
      <div class="claim-text">56. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">obtaining a sound associated with said textured message; and </div>
      <div class="claim-text">playing said sound contemporaneously with the rendering of said textured message in the three-dimensional multi-user environment; </div>
      <div class="claim-text">wherein said sound is indicative of the identity of the sender. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="57" id="US-6784901-B1-CLM-00057" class="claim">
      <div class="claim-text">57. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">obtaining a sound associated with said textured message; and </div>
      <div class="claim-text">playing said sound contemporaneously with the rendering of said textured message in the three-dimensional multi-user environment; </div>
      <div class="claim-text">wherein said sound is indicative of the content of the message. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="58" id="US-6784901-B1-CLM-00058" class="claim">
      <div class="claim-text">58. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">obtaining a sound associated with said textured message; and </div>
      <div class="claim-text">playing said sound contemporaneously with the rendering of said textured message in the three-dimensional multi-user environment; </div>
      <div class="claim-text">wherein said sound is indicative of the number of recipients of the message. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="59" id="US-6784901-B1-CLM-00059" class="claim">
      <div class="claim-text">59. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">obtaining a sound associated with said textured message; and </div>
      <div class="claim-text">playing said sound contemporaneously with the rendering of said textured message in the three-dimensional multi-user environment; </div>
      <div class="claim-text">wherein said sound associated with said textured message is indicative of the location of the digital representation of the sender in the three-dimensional multi-user environment. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="60" id="US-6784901-B1-CLM-00060" class="claim">
      <div class="claim-text">60. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">generating an animation texture associated with said textured message; and </div>
      <div class="claim-text">rendering said animation texture at locations along a path in the three-dimensional multi-user environment, whereby the recipient can visually ascertain at least portions of the path of said animation texture through the recipient viewport. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="61" id="US-6784901-B1-CLM-00061" class="claim">
      <div class="claim-text">61. The method of <claim-ref idref="US-6784901-B1-CLM-00060">claim 60</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">receiving appearance information; and </div>
      <div class="claim-text">using said appearance information to generate said animation texture. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="62" id="US-6784901-B1-CLM-00062" class="claim">
      <div class="claim-text">62. The method of <claim-ref idref="US-6784901-B1-CLM-00060">claim 60</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">receiving appearance information from the sender; and </div>
      <div class="claim-text">using said appearance information to generate said animation texture. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="63" id="US-6784901-B1-CLM-00063" class="claim">
      <div class="claim-text">63. The method of <claim-ref idref="US-6784901-B1-CLM-00060">claim 60</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">receiving appearance information indicative of the identity of the sender; and </div>
      <div class="claim-text">using said appearance information to generate said animation texture. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="64" id="US-6784901-B1-CLM-00064" class="claim">
      <div class="claim-text">64. The method of <claim-ref idref="US-6784901-B1-CLM-00060">claim 60</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">receiving appearance information indicative of the content of the message; and </div>
      <div class="claim-text">using said appearance information to generate said animation texture. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="65" id="US-6784901-B1-CLM-00065" class="claim">
      <div class="claim-text">65. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the step of displaying said textured message as a two-dimensional element on the recipient viewport after rendering said textured message in the three-dimensional multi-user environment.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="66" id="US-6784901-B1-CLM-00066" class="claim">
      <div class="claim-text">66. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the step of displaying said textured message as a two-dimensional element on the recipient viewport for a predetermined time period after rendering said textured message at said textured message in the three-dimensional multi-user environment.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="67" id="US-6784901-B1-CLM-00067" class="claim">
      <div class="claim-text">67. The method of <claim-ref idref="US-6784901-B1-CLM-00042">claim 42</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">displaying said textured as a two-dimensional element on the bottom of said recipient viewport after rendering said textured message in the three-dimensional multi-user environment; and </div>
      <div class="claim-text">vertically displacing said two-dimensional element to accomodate other textured messages at the bottom of said recipient viewport. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="68" id="US-6784901-B1-CLM-00068" class="claim">
      <div class="claim-text">68. The method of <claim-ref idref="US-6784901-B1-CLM-00065">claim 65</claim-ref>, wherein said two-dimensional element is displayed in horizontal alignment with the appearance of the digital representation of the sender in the recipient viewport.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="69" id="US-6784901-B1-CLM-00069" class="claim">
      <div class="claim-text">69. The method of <claim-ref idref="US-6784901-B1-CLM-00065">claim 65</claim-ref>, wherein said two-dimensional element is displayed flush right on the recipient viewport when the digital representation of the sender is to the right of the scope of the recipient viewport and flush left on the recipient viewport when the digital representation of the sender is to the left of the scope of the recipient viewport.</div>
    </div>
    </div> <div class="claim"> <div num="70" id="US-6784901-B1-CLM-00070" class="claim">
      <div class="claim-text">70. A method for delivering a message from at sender to a recipient in a three-dimensional multi-user environment wherein the recipient has a viewport into the three-dimensional multi-user environment and the sender and the recipient are each digitally represented therein, comprising the steps of receiving the message;</div>
      <div class="claim-text">mapping the message to a texture to generate a textured message; </div>
      <div class="claim-text">selecting a first location in the three-dimensional multi-user environment closer to the digital representation of the sender than to the digital representation of the recipient; </div>
      <div class="claim-text">selecting a second location in the three-dimensional multi-user environment closer to the digital representation of the recipient than to the digital representation of the sender; and </div>
      <div class="claim-text">periodically rendering said textured message in the three-dimensional multi-user environment along a path beginning at the first location and ending at the second location, so as to permit the recipient to visually ascertain the location of the digital representation of the sender through the recipient viewport. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="71" id="US-6784901-B1-CLM-00071" class="claim">
      <div class="claim-text">71. The method of<claim-ref idref="US-6784901-B1-CLM-00070">claim 70</claim-ref>, further comprising the step of displaying said textured message as a two-dimensional element on the recipient viewport after rendering said textured message at said second location in the three-dimensional multi-user environment.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="72" id="US-6784901-B1-CLM-00072" class="claim">
      <div class="claim-text">72. The method of <claim-ref idref="US-6784901-B1-CLM-00070">claim 70</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">receiving path information; and </div>
      <div class="claim-text">using said path information to define the shape of said path between said first location and said second location. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="73" id="US-6784901-B1-CLM-00073" class="claim">
      <div class="claim-text">73. The method of <claim-ref idref="US-6784901-B1-CLM-00070">claim 70</claim-ref>, further compromising the steps of:</div>
      <div class="claim-text">receiving path information from the sender; and </div>
      <div class="claim-text">using said path information to define the shape of said path between said first location and said second location. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="74" id="US-6784901-B1-CLM-00074" class="claim">
      <div class="claim-text">74. The method of <claim-ref idref="US-6784901-B1-CLM-00070">claim 70</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">receiving path information indicative of the identity of the sender; and </div>
      <div class="claim-text">using said path information to define the shape of said path between said first location and said second location. </div>
    </div>
    </div> <div class="claim"> <div num="75" id="US-6784901-B1-CLM-00075" class="claim">
      <div class="claim-text">75. A method for delivering a message from a sender to a recipient in a three-dimensional multi-user environment wherein the recipient has a viewport into the three-dimensional multi-user environment and the sender and the recipient are each digitally represented therein, comprising the steps of:</div>
      <div class="claim-text">receiving the message; </div>
      <div class="claim-text">mapping the message to a texture to generate a textured message; </div>
      <div class="claim-text">periodically determining the location of the digital representation of the sender and the location of the digital representation of the recipient; </div>
      <div class="claim-text">periodically rendering said textured message in the three-dimensional multi-user environment along a path between said location of the digital representation of the sender and said location of the digital representation of the recipient, so as to permit the recipient to visually ascertain said location of the digital representation of the sender through the recipient viewport. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="76" id="US-6784901-B1-CLM-00076" class="claim">
      <div class="claim-text">76. The method of <claim-ref idref="US-6784901-B1-CLM-00075">claim 75</claim-ref>, further comprising the step of displaying said textured message as a two-dimensional element on the recipient viewport after periodically rendering said textured message along a path between said location of the digital representation of the sender and said location of the digital representation of the recipient.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="77" id="US-6784901-B1-CLM-00077" class="claim">
      <div class="claim-text">77. The method of <claim-ref idref="US-6784901-B1-CLM-00075">claim 75</claim-ref>, wherein said step of periodically rendering said textured message in the three-dimensional multi-user environment along a path between said location of the digital representation of the sender and said location of the recipient further comprises the steps of:</div>
      <div class="claim-text">selecting a starting point based on the location of the digital representation of the sender; </div>
      <div class="claim-text">selecting an ending point based on the location of the digital representation of the recipient; </div>
      <div class="claim-text">selecting a preliminary rendering point on a vector that extends from said starting point to said ending point; </div>
      <div class="claim-text">calculating an offset according to an offset function based at least in part on the position of said preliminary rendering point along said vector; </div>
      <div class="claim-text">determine a final rendering point by adding said offset to said preliminary rendering point; and </div>
      <div class="claim-text">rendering said textured message at said final rendering point; </div>
      <div class="claim-text">wherein said offset function is equal to zero at said starting point and said ending point, thereby ensuring smooth rendering of said textured message. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="78" id="US-6784901-B1-CLM-00078" class="claim">
      <div class="claim-text">78. The method of <claim-ref idref="US-6784901-B1-CLM-00077">claim 77</claim-ref>, wherein said offset function is a sinusoidal function.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="79" id="US-6784901-B1-CLM-00079" class="claim">
      <div class="claim-text">79. The method of <claim-ref idref="US-6784901-B1-CLM-00077">claim 77</claim-ref>, further comprising the step of:</div>
      <div class="claim-text">receiving path information, wherein said path information includes said offset function. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="80" id="US-6784901-B1-CLM-00080" class="claim">
      <div class="claim-text">80. The method of <claim-ref idref="US-6784901-B1-CLM-00077">claim 77</claim-ref>, further comprising the step of:</div>
      <div class="claim-text">receiving path information from the sender, wherein said path information includes said offset function. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="81" id="US-6784901-B1-CLM-00081" class="claim">
      <div class="claim-text">81. The method of <claim-ref idref="US-6784901-B1-CLM-00077">claim 77</claim-ref>, further comprising the step of:</div>
      <div class="claim-text">receiving path information from the sender, wherein said path information includes said offset function and is indicative of the identity of the sender. </div>
    </div>
    </div> <div class="claim"> <div num="82" id="US-6784901-B1-CLM-00082" class="claim">
      <div class="claim-text">82. A method for delivering a message from a sender to a recipient in a three-dimensional multi-user environment wherein the recipient has a viewport into the three-dimensional multi-user environment and the sender and the recipient are each digitally represented therein, comprising the steps of:</div>
      <div class="claim-text">periodically checking a transition state; </div>
      <div class="claim-text">if said transition state is “just arrived”, mapping the message to a texture to generate a textured message and changing said transition state to “3D transition”; </div>
      <div class="claim-text">if said transition state is “3D transition”, rendering said textured message at locations along a path in the three-dimensional multi-user environment and changing said transition state to “2D transition”; </div>
      <div class="claim-text">if said transition state is “2D transition”, displaying a 2D transition animation and changing said transition state to “in list”, and </div>
      <div class="claim-text">if said transition state is “in list”, displaying said textured message as a two-dimensional element on the recipient viewport. </div>
    </div>
    </div> <div class="claim"> <div num="83" id="US-6784901-B1-CLM-00083" class="claim">
      <div class="claim-text">83. A method for delivering a message from a sender to a recipient in a three-dimensional multi-user environment wherein the sender and recipient are each digitally represented therein, comprising the steps of:</div>
      <div class="claim-text">mapping the message to a texture to generate a textured message; </div>
      <div class="claim-text">rendering said textured message in the three-dimensional multi-user environment so as to permit the recipient to visually ascertain the location of the digital representation of the sender. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="84" id="US-6784901-B1-CLM-00084" class="claim">
      <div class="claim-text">84. The method of <claim-ref idref="US-6784901-B1-CLM-00083">claim 83</claim-ref>, wherein the rendering step includes rendering said textured message in the three-dimensional multi-user environment closer to the digital representation of the sender than to the digital representation of the recipient, and subsequently rendering said textured message in the three-dimensional multi-user environment closer to the digital representation of the recipient than to the digital representation of the sender.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="85" id="US-6784901-B1-CLM-00085" class="claim">
      <div class="claim-text">85. The method of <claim-ref idref="US-6784901-B1-CLM-00084">claim 84</claim-ref>, wherein said message is sent to the recipient over the Internet.</div>
    </div>
    </div> <div class="claim"> <div num="86" id="US-6784901-B1-CLM-00086" class="claim">
      <div class="claim-text">86. A computer program product comprising a computer useable medium having computer program logic for enabling at least one processor in a computer system to provide a recipient interface to a three-dimensional multi-user environment that includes a recipient viewport and to receive a message from a sender, wherein said three-dimensional multi-user environment maintains a digital representation of the sender, said computer program logic comprising:</div>
      <div class="claim-text">means for enabling at least one processor to receive a message; </div>
      <div class="claim-text">means for enabling at least one processor to map said message to a texture to generate a textured message; </div>
      <div class="claim-text">means for enabling at least one processor to render said textured message in the three-dimensional multi-user environment so as to indicate the location of the digital representation of the sender on the recipient viewport. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="87" id="US-6784901-B1-CLM-00087" class="claim">
      <div class="claim-text">87. The computer program product of <claim-ref idref="US-6784901-B1-CLM-00086">claim 86</claim-ref>, further comprising means for enabling at least one processor to display said textured message as a two-dimensional element on the recipient viewport after rendering said textured message in the three-dimensional multi-user environment.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES54290462" lang="EN" load-source="patent-office" class="description">
    <heading>CROSS-REFERENCE TO RELATED APPLICATIONS</heading> <p>This application claims priority to provisional application entitled “User Interface for a 3D World,” U.S. patent application Ser. No. 60/202,920, filed on May 9, 2000 (incorporated by reference in its entirety herein).</p>
    <heading>BACKGROUND OF THE INVENTION</heading> <p>1. Field of the Invention</p>
    <p>The present invention relates to a system, method and computer program product for delivering electronic messages. More specifically, the present invention relates to a method, system and computer program for delivering electronic “chat” messages.</p>
    <p>2. Related Art</p>
    <p>Computers have gained widespread acceptance as a means of communication between individuals. The real-time exchange of messages via a computer, or computer “chat,” is a particularly popular method of on-line communication, as it permits the dynamic exchange of messages between remote computer users that is much like a real conversation. For this reason, chat is a particularly effective communication tool for social interaction.</p>
    <p>In a typical chat session, when a participant types a line of text into his computer and then presses the Enter key, that participant's words appear in a two-dimensional (2D) window on the computer screens of other participants, who can respond in kind. Two-dimensional chat systems of this type are well known and are commonly implemented through the use of specialized Internet utility programs or via client-server systems such as Internet Relay Chat (IRC).</p>
    <p>The use of chat programs in combination with three-dimensional (3D) multi-user environments is also known in the art. In a 3D multi-user environment, computer users interact with digital representations of each other, sometimes called “Avatars,” and objects in a commonly-shared virtual 3D space. 3D multi-user environments are often implemented in client-server systems, in which the server maintains a virtual 3D space and the client provides the user interface to that space. Chat is particularly useful in combination with social 3D multi-user environments that permit users to meet and converse with each other in 3D space. In addition, chat is useful with 3D on-line multiplayer games, in which many players interact, either cooperatively or competitively, and must communicate with each in order to respond to real-time events occurring in the 3D game world. In 3D multi-user environments the users typically view the 3D world through a window, or 3D viewport, displayed on their computer screen. Conventional systems that combine chat with a 3D multi-user environment typically display chat messages in a 2D window that is adjacent to, or overlays, the 3D viewport. As such, these systems suffer from a number of disadvantages. For example, by allotting screen space to the 2D chat window, these systems necessarily provide smaller or obscured viewports into the virtual 3D space. As a result, users may have a more difficult time perceiving and responding to events that occur in the 3D world.</p>
    <p>Also, these systems require the user to manage it least two interfaces in order to interact with other users, which can be difficult and may cause the user to miss events that occur in the 3D world or lose track of conversations that appear in the 2D window, or both. Because the user is required to deal with two distinct interfaces (2D and 3D) that have very different appearances and do not mesh seamlessly, the user will generally have a less immersive and less intuitive experience.</p>
    <p>Conventional systems that use both a 2D and 3D window are particularly disadvantageous in that they make it difficult to locate the digital representation of the sender of any given chat message in the 3D world, as there is no direct connection between the sender's chat message appearing in the 2D window and the sender's Avatar in the 3D world. Some of these systems attempt to establish such a connection by, for example, displaying name tags adjacent to the Avatars in the 3D world and also placing the sender's name adjacent to the sender's chat message in the 2D window. By looking at the name adjacent to the chat message in the 2D window and then scanning the 3D window for an Avatar with a matching name tag, the recipient of the message can attempt to locate the digital representation of the sender in the 3D world. However, because the recipient is required to scan at least two interfaces to locate the sender's Avatar, this solution is less than ideal and can cause the recipient to miss events that occur in the 3D environment as well as lose track of messages appearing in the 2D window. Also, this solution simply does not work when the sender's Avatar is behind the recipient or other wise out side of the view of the recipient's 3D viewport. Similar correlation schemes involving color coding or some other association between the 2D chat message and the Avatars in the 3D viewport suffer from the same disadvantages.</p>
    <p>These systems are also disadvantageous in that they do not indicate in 3D space the direction from which a chat message originated. As a result, the recipient of the chat message is not given an indication of the location of the digital representation of the sender in the 3D world.</p>
    <p>These systems are also disadvantageous in that chat messages are limited to two-dimensional text, and do not include any graphic, audio or animation component. Thus, senders of chat messages in conventional systems are unable to customize their chat messages with a graphic, audio or animation component, thereby generating a unique “signature” for the sender that enhances the ability of the recipient to identify the sender of the message.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>The present invention is directed to a chat system for delivering a message between a sender and a recipient in a three-dimensional (3D) multi-user environment, wherein the 3D multi-user environment maintains respective digital representations of the sender and the recipient. Embodiments of the invention include a recipient interface that receives a message, maps the message to a texture to generate a textured message, and renders the textured message in the 3D multi-user environment so as to permit the recipient to visually ascertain the location of the digital representation of the sender.</p>
    <p>Alternate embodiments of the invention include a sender interface and a recipient interface that includes a recipient viewport. The recipient interface receives a message from the sender interface, maps the message to a texture to generate a textured message, periodically determines the location of the digital representation of the sender and the location of the digital representation of the recipient, and periodically renders the textured message in the three-dimensional multi-user environment along a path between the location of the digital representation of the sender and the location of the digital representation of the recipient, so as to permit the recipient to visually ascertain the location of the digital representation of the sender through the recipient viewport.</p>
    <p>In an embodiment of the invention, the sender interface and recipient interface each comprise software modules and the messages are sent over the Internet.</p>
    <p>The invention is advantageous in that it permits the delivery of a chat message in a 3D multi-user environment without requiring the use of both a 3D viewport and a separate 2D window. Accordingly, the invention allows screen space to be conserved for the 3D viewport, which provides the user with a better view into the 3D world and thereby generates a more immersive and intuitive experience for the user. In on-line multiplayer games that involve interaction with objects or other players in a 3D world, it is critical to provide as large a viewport as possible so that players can effectively perceive and respond to game events.</p>
    <p>The invention is also advantageous in that it permits the delivery of a chat message in a 3D multi-user environment via one integrated viewport, rather than via two or more windows. Because the invention permits the recipient of a chat message to view the chat message and interact with the 3D world via the same interface, the recipient is better able to avoid missing events in the 3D world and/or losing track of chat messages.</p>
    <p>Another benefit of the invention is that it permits the recipient of a chat message in a 3D multi-user environment to locate the digital representation of the sender of the chat message in the 3D world, even when the digital representation of the sender is outside the scope of the recipient's 3D viewport.</p>
    <p>Yet another benefit of the invention is that it permits the recipient of a chat message in a 3D multi-user environment to determine the direction from which a chat message originated. Accordingly, the recipient of a chat message is given an indication of the location of the digital representation of the sender in the 3D world.</p>
    <p>A further advantage of the invention is that it supports chat messages with a graphic, audio or animation component customizable by the sender. The sender is thereby permitted to customize a chat message to generate a unique “signature” that enhances the recipient's ability to identify the sender of the message. The graphic, audio or animation component can also be utilized to indicate the content of the chat message or the intended number of recipients of the chat message.</p>
    <p>A further benefit of the invention is that it manages received messages on a 3D viewport in a manner that ensures legibility of received messages and indicates the location of the digital representation of the sender in the 3D world.</p>
    <p>Additional features and advantages of the invention will be set forth in the description that follows, and in part will be apparent from the description, or may be learned by practice of the invention. The objectives and other advantages of the invention will be realized and attained by the system and method particularly pointed out in the written description and claims hereof as well as the appended drawings.</p>
    <heading>BRIEF DESCRIPTION OF THE FIGURES</heading> <p>The accompanying drawings are included to provide a further understanding of the invention and are incorporated in and constitute part of this specification. The drawings illustrate the preferred embodiment of the invention and together with the description serve to explain the principles of the invention.</p>
    <p>FIG. 1 is a block diagram of a distributed chat interface system according to one embodiment of the present invention.</p>
    <p>FIG. 2 is a high-level block diagram of an example local chat system according to one embodiment of the present invention.</p>
    <p>FIG. 3 is a block diagram that illustrates the inputs and outputs of the chat local display according to one embodiment of the present invention.</p>
    <p>FIG. 4 is a message flow diagram of the components of the distributed chat interface system according to an example implementation of the present invention.</p>
    <p>FIG. 5 is an exemplary chat message wad object according to an embodiment of the present invention.</p>
    <p>FIG. 6 is an exemplary drawing parameters object of the chat message wad according to an embodiment of the present invention.</p>
    <p>FIG. 7 is a flowchart that describes the high level routine for local chat according to an embodiment of the present invention.</p>
    <p>FIG. 8 is a flowchart further describing the step of creating chat wad in the flowchart of FIG. 7 according to an embodiment of the present invention.</p>
    <p>FIG. 9 is a flowchart further describing the step of determining the drawing parameters of the chat message in the flowchart of FIG. 8 according to an embodiment of the present invention.</p>
    <p>FIG. 10 is a flowchart further describing the step of scrolling up previously displayed chat messages in the flowchart of FIG. 7 according to an embodiment of the present invention.</p>
    <p>FIG. 11 is an exemplary screen layout illustrating the default locations and functionality of user interface objects available to the user according to an embodiment of the present invention.</p>
    <p>In FIGS. 12, <b>13</b> and <b>14</b> are exemplary screen shots illustrating local chat according to an embodiment of the present invention.</p>
    <p>FIGS. 15A-15B is a flowchart showing a method for delivering a chat message in a 3D multi-user environment in accordance with the present</p>
    <p>FIGS. 16A, <b>16</b>B, <b>16</b>C, <b>16</b>D, <b>16</b>E, <b>16</b>F, <b>16</b>G, <b>16</b>H and <b>16</b>I show example textured messages and animation textures according to embodiments of the invention.</p>
    <p>FIGS. 17A, <b>17</b>B and <b>17</b>C depict example rendering paths for the 3D transition of a textured message according to embodiments of the invention.</p>
    <p>FIGS. 18A, <b>18</b>B, <b>18</b>C, <b>18</b>D, <b>18</b>E and <b>18</b>F depict the delivery of a chat message from a sender to a recipient according to an embodiment of the invention.</p>
    <p>FIGS. 19A, <b>19</b>B, <b>19</b>C, <b>19</b>D, <b>19</b>E and <b>19</b>F depict the delivery of a chat message from a sender to a recipient according to an embodiment of the invention.</p>
    <p>FIGS. 20A, <b>20</b>B, <b>20</b>C, <b>20</b>D, <b>20</b>E and <b>20</b>F show the management of textured messages as two-dimensional elements on a recipient viewport in an embodiment of the invention.</p>
    <p>FIGS. 21A, <b>21</b>B, <b>21</b>C, <b>21</b>D, <b>21</b>E, and <b>21</b>F depict the delivery of a chat message with an additional animation component according to an embodiment of the invention.</p>
    <p>The present invention will now be described with reference to the accompanying drawings. In the drawings, like reference numbers indicate identical or functionally similar elements. Additionally, the left most digit(s) of a reference number identifies the drawing in which the reference number first appears. Although several preferred embodiments of the present invention are particularly shown and described below, one skilled in the art will appreciate that various changes in forms and details may be made without departing from the spirit and scope of the invention as defined in the appended claims.</p>
    <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading> <heading>A. Overview of the Invention</heading> <p>The present invention is directed to a chat system, method and computer program product for delivering a message between a sender and a recipient in a three-dimensional multi-user environment, in which the three-dimensional multi-user environment maintains a digital representations of the sender and the recipient. The chat system includes a recipient interface. According to the present invention, the recipient interface receives a message, maps the message to a texture to generate a textured message, and renders the textured message in the three-dimensional multi-use environment in a manner that permits the recipient to visually ascertain the location of the digital representation of the sender in the 3D world. The invention will now be further described in reference to FIGS. 1-21.</p>
    <heading>B. System Architecture Overview</heading> <p>FIG. 1 is a block diagram representing an example operating environment of the distributed chat inter face system of the present invention. It should be understood that the example operating environment in FIG. 1 is shown for illustrative purposes only and does not limit the invention. Other implementations of the operating environment described herein will be apparent to persons skilled in the relevant art(s) based on the teachings contained herein, and the invention is directed to such other implementations.</p>
    <p>Referring to FIG. 1, an Avatar manager <b>102</b>, a host server <b>104</b>, a network <b>106</b>, a local chat system <b>108</b>, a user interface (UI) <b>110</b> and a chat local display <b>112</b> is shown. Avatar manager <b>102</b> and host server <b>104</b> are considered server side components of the present invention. Local chat system <b>108</b>, user interface (UI) <b>110</b> and chat local display <b>112</b> are considered client side components of the present invention.</p>
    <p>Only one Avatar manager <b>102</b>, host server <b>104</b>, local chat system <b>108</b>, user interface (UI) <b>110</b> and chat local display <b>112</b> is shown for clarity. In general, any number of these components can be included in the distributed chat interface system of the present invention. Each of these components is briefly described next.</p>
    <p>Avatar manager <b>102</b> is a server side component that handles multiple Avatars. In fact, Avatar manager <b>102</b> is responsible for storing all chat messages that are exchanged via the distributed chat interface system. This allows for the persistent storage of chat messages. Persistent storage of chat messages by Avatar manager <b>102</b> allows users to access Avatar manager <b>102</b> days later to view what was said during a particular chat session in which they participated.</p>
    <p>The other server side component is host server <b>104</b>. Host server <b>104</b> receives the chat messages from the client side components via network <b>106</b>. Host server <b>104</b> then provides these chat messages to Avatar manager <b>102</b> for persistent storage. Host server <b>104</b>, among other things, is responsible for hosting chat (and/or game playing) and communication (including control and data flow) between a user's interface and the chat (and/or game play) environment. In addition, host server <b>104</b> demultiplexes chat messages and requests for data that arrive from the client side components and from other Avatar managers.</p>
    <p>Network <b>106</b> connects the server side and client side components of the present invention. Network <b>106</b> can be any type of computer network or combination of networks including, but not limited to circuit switched and/or packet switched networks. In one example, network <b>106</b> includes the Internet. Any conventional communication protocols can be used to support communication between the components of the distributed chat interface system. For example, a TCP/IP suite can be used to establish links and transport data. A World Wide Web-based application layer and browser (and Web server) can also be used to further facilitate communication between the components shown in FIG. <b>1</b>. However, these examples are illustrative. The present invention is not intended to be limited to a specific communication protocol or application, and other proprietary or non-proprietary network communication protocols and applications can be used.</p>
    <p>Local chat system <b>108</b> is a client side component and is responsible for managing a user's chat file. A user's chat file is a file that contains chat messages sent both from and to the user during the particular session. These chat messages are eventually stored by Avatar manager <b>102</b> for persistent storage and later retrieval by the user. Local chat system <b>108</b> is also responsible for receiving chat messages and dispatching them to the correct user (or Avatar), managing system messages, and so forth. The various components that make up local chat system <b>108</b>, and the interaction between them, will be described in detail below with reference to FIG. <b>2</b>.</p>
    <p>UI <b>110</b> is also a client side component and is responsible for accepting and sending user input (or interface) events to local chat system <b>108</b> and chat local display <b>112</b>. Examples of user input events include, but are not limited to, relevant button clicks and keyboard events when the user types in a chat message.</p>
    <p>Chat local display <b>112</b> maintains and tracks a list of chat message wads. Each chat message wad contains one chat message, along with state and structures required to animate and display the chat message in the 3D world. The chat message includes the message sent by the user, the time, recipient animation attributes, appearance attributes, and so forth. The chat message wad of the present invention is described in more detail below with reference to FIGS. 5 and 6. Internally, chat local display <b>112</b> includes a data structure that grows as number of active chat message wads grow. Chat local display <b>112</b> also includes logic to maintain and track the list of chat message wads. Chat local display <b>112</b> is described in more detail below with reference to FIGS. 3 and 11.</p>
    <p>Prior to describing the various components that make up local chat system <b>108</b> (and the distributed chat interface system in general), and the interaction between them, the user interface objects (or elements) of the present invention will be described next with reference to FIG. <b>11</b>.</p>
    <heading>C. User Interface Objects of the Present Invention</heading> <p>The present invention provides these user interface objects to provide cues to guide the user from the 3D world to the 2D world, and vice-versa. Therefore, most of the components that make up the distributed chat interface system perform functions or tasks that create and/or manage the user interface objects. The exemplary screen layout of FIG. 11 illustrates the default locations and functionality of user interface objects available to the user according to one embodiment of the present invention. These objects include, but are not limited to, a 3D viewport <b>1100</b>, one or more Avatar name tags <b>1102</b>, a local chat handle <b>1104</b>, one or more local chat message wads <b>1106</b>, a local chat area <b>1107</b>, a chat entry area <b>1108</b> (which includes a chat mode button <b>1110</b>, a specific recipient list <b>1109</b> and a chat text entry area <b>1112</b> ), a dashboard <b>1114</b>, a items window <b>1116</b> and an exemplary tool tip <b>1118</b>. Each of these user interface objects are described next.</p>
    <p>In FIG. 11, the exemplary screen layout includes 3D viewport <b>1100</b>. In one embodiment of the present invention, 3D viewport <b>1100</b> is the entire screen. Using the entire screen for 3D viewport <b>1100</b> is advantageous in that it provides as much screen space as possible to the 3D viewport, so as to allow the user a better view into the 3D world and thereby generate a more immersive and intuitive experience for the user. In on-line multiplayer games that involve interaction with objects or other players in a 3D world, it is critical to provide as large a viewport as possible so that players can effectively perceive and respond to game events. In addition, 2D user interface objects are overlaid on 3D viewport <b>1100</b> over the view into the 3D world to help create one experience for the user. A camera is a logical object that follows the user's Avatar and provides the perspective on the rendered scene. What is rendered in the 3D viewport <b>1100</b> is determined by the camera's position, orientation and field of view. The windshield of the present invention is a plane projected in front of a user's camera. Objects “stuck to” the windshield appear to be hovering in 3D space.</p>
    <p>A considerable portion of the windshield space (i.e., 3D viewport <b>1100</b>) is occupied by the user interface objects related to local chat. Local chat refers to text chat with other Avatars who are within a given radius (e.g., 25 M in a virtual game world distance) from a user's Avatar. Unlike other types of instant messaging, the present invention provides a physical world where proximity to others allows users to share physical, as well as social, experiences other than chatting.</p>
    <p>Local chat handle <b>1104</b> allows users to set the height of local chat area <b>1107</b>. Thus, the height of local chat area <b>1107</b> varies. Local chat handle <b>1104</b> can also be used to get additional chat options (such as chat logs) by right clicking on local chat handle <b>1104</b>. Local chat handle <b>1104</b> is displayed by default, but may be removed by the user as an option.</p>
    <p>One or more local chat message wads <b>1106</b> contain chat message objects from Avatars and appear in the local chat area <b>1107</b> between the local chat handle <b>1104</b> and the bottom of 3D viewport <b>1100</b>. The chat message will center itself horizontally on the screen so that it is lined up with the Avatar that sent the message. The chat message continues to horizontally align itself as the Avatar moves in the 3D world. In order to be less obtrusive, older message objects become more translucent as they go higher up local chat area <b>1107</b> (i.e., as they move up the screen). However, at any time no two chat message wads <b>1106</b> overlap. This is due to the fact that message objects in the chat message wads <b>1106</b> occupy different rows in local chat area <b>1107</b>. The present invention also allows the user to recreate the flow of the conversation at any time by following the message objects on the screen from the oldest message (at the top of local chat area <b>1107</b>) to the newest message (at the bottom of local chat area <b>1107</b>).</p>
    <p>Also there is a 3D animation or message transition from the sender Avatar to local chat area <b>1107</b> when a message object is sent. This animation helps to further link the chat message wad <b>1106</b> with the sender in the 3D world. Thus, every new message object is added to local chat area <b>1107</b> with a 3D animation. This transition can be customized by the sender (e.g., path shape, trails, speed, etc.) and helps to cue the user as to the location of the Avatar communicating with him or her.</p>
    <p>The name of the Avatar is not needed in the message object because of the way the user interface of the present invention is designed. Chat message wads <b>1106</b> are linked to their author both spatially and aesthetically. The customized stylistic choices on the part of the sender create a “voice” for their text chat which will soon become familiar to the users they frequently encounter in the present invention. This allows the present invention to save screen real estate by omitting the sender's name from the standard display of chat message wads <b>1106</b>. At any time, a user may mouse over a message object and be reminded who sent it with a pop-up tool tip.</p>
    <p>As mentioned above, the number of local chat message wads <b>1106</b> increases as more chat messages are received from an Avatar. Local chat message wad <b>1106</b> is made up of multiple objects, including: the actual chat (i.e., text) message, texture, some drawing parameters about the texture, the author's name of the message, etc. The objects of chat message wad <b>1106</b> will be described in detail below with reference to FIGS. 5 and 6.</p>
    <p>Local chat area <b>1107</b> is the area on 3D viewport <b>1100</b> between local chat handle <b>1104</b> and chat entry area <b>1108</b>. Local chat area <b>1107</b> is the area when one or more local chat message wads <b>1106</b> are displayed.</p>
    <p>Chat entry area <b>1108</b> is comprised of chat mode button <b>1110</b> and chat text entry area <b>1112</b>. Chat mode button <b>1110</b> allows the user to cycle through various modes of communication. The default mode is “speak” mode. This sends messages to all users in local chat area <b>1107</b>. “Whisper” mode allows a user to send a private message to a single other user or Avatar in local chat area <b>1107</b>. When in whisper mode, a list box with the names of local Avatars and the members of a user's buddy list appears to select a recipient. This entry area is also configurable from the outside, to allow it to be configured with a recipient when the user double clicks on an Avatar figure or Avatar name tag <b>1102</b>. The final chat mode is “shout” mode which reaches all users currently known by the user's server.</p>
    <p>Chat text entry area <b>1112</b> is a translucent text edit box that the user types in his or her message. Chat text entry area <b>1112</b> also indicates to which Avatar the user is “speaking,” “whispering” or “shouting”.</p>
    <p>One important challenge faced in presenting the local chat is tying the messages to the Avatars sending them in the 3D world. It is important for There users to be able to make visual contact and know that they have done so. However, because the Avatars may be obscured, in a crowd or behind the user's camera, it is sometimes hard just to identify an Avatar's position in the world. To address that problem, the present invention provides Avatar name tags <b>1102</b>.</p>
    <p>Avatar name tags <b>1102</b> are located at the very top of the screen directly above the Avatars' heads. Name tags <b>1102</b> serve both to identify Avatars in the local area and to identify the chat associated from that Avatar by aligning the name tag horizontally on the screen with the Avatar and all of the messages the Avatar sent. Name tags <b>1102</b> track an Avatar's motion in the 3-D world and appear far right or far left for Avatars not in view. An arrow below Avatar name tag <b>1102</b> serves both to remind the user of the other Avatar's position and provides an easy place for the user to click to bring a name tag to the foreground in the event that two name tags overlap. Avatar name tags <b>1102</b> are displayed by default, but could be removed by the user as an option. In an embodiment of the present invention, an Avatar's name tag <b>1102</b> automatically pops up to the front of other objects displayed on 3D viewport <b>1100</b> when a new message object arrives for that Avatar. Avatar name tags <b>1102</b> are ordered (on the z plane from the camera's perspective) based on distance from the user's Avatar to the Avatar for the name tag.</p>
    <p>In an embodiment of the present invention, Avatar name tags <b>1102</b> indicate to the user which Avatars are in “hearing” range of the user's Avatar. This is done by displaying ears (or another indicator) on the name tag of all Avatars within “hearing range” of the user's Avatar. Once an Avatar is not within “hearing range”, then the ears on the Avatar's name tag disappears. This indicates to the user which Avatars the user can and cannot contact in local chat of the present invention. Because of network delays, it is possible that at times the user may think a particular Avatar is within “hearing range”, and at the same time that same Avatar may not believe it is within “hearing range” of the user. Therefore, the calculation of which Avatars are within the “hearing range” is done by the client side components (i.e., locally).</p>
    <p>Dashboard <b>1114</b> is the user's jumping off point for all other (non local chat related) user interface objects. It includes icons for commonly used actions toolbars and windows. Dashboard <b>1114</b> is essentially a toolbar. The idea is to use a very small portion of the screen to allow the user access to functionality they might need. This functionality must be broken down into several main categories, which then receive icons on the dashboard. Depending on the importance of a given function, it may map directly to one of these icons (such as enabling voice chat) or may be nested in a configuration page that is brought up by the icon (such as customization options). Stylistically, the dashboard should also tie closely with chat entry area <b>1108</b> since they share the bottom of the screen and are the only two objects that are always visible.</p>
    <p>Items window <b>1116</b> is a floating window that displays the user's items including, but not limited to, any weapons, money, and so forth, that the user has to interact with the present invention. Items window <b>1116</b> be moved, closed or minimized. In addition, items window <b>1116</b> has a title bar, border and/or background color. Items window <b>1116</b> may be translucent so as to not completely obscure the action in the 3D world.</p>
    <p>Exemplary tool tip <b>1118</b> floats over the mouse, and in this case displays the name of the Avatar. Almost everything in the present invention can provide tool tip text to a tool tip manager. The tool tip manager is responsible for determining who should display a tool tip and render it for the user. The tool tip manager of the present invention makes up a part of the basic user interface element (or object) interaction provided to users. Other parts of the basic user interface object interaction includes a drag and drop manager and a pull down menu manager. All three are described in more detail with reference to FIG. 4 below. Next, the various components that make up local chat system <b>108</b>, and the interaction between them, will be described with reference to FIG. <b>2</b>.</p>
    <heading>D. Local Chat System of the Present Invention</heading> <p>In FIG. 2, local chat system <b>108</b> includes an Avatar session manager <b>202</b>, a chat log <b>204</b>, a chat sender <b>206</b> and a local Avatar tracker <b>212</b>. As stated above, local chat system <b>108</b> is a client side component and therefore focuses on the current or active chat session that involves the user. Chat log <b>204</b> stores all of the messages sent to the user. These messages are sent via network <b>106</b> from host server <b>104</b> and Avatar manager <b>102</b> (i.e., the server side components of the present invention). As stated above, Avatar manager <b>102</b> handles multiple Avatars and stores all messages sent to the Avatars. Therefore, any messages sent to the user (or the user's Avatar) is also stored by Avatar manager <b>102</b>. Chat log <b>204</b> polls Avatar massager <b>1102</b> to determine if any new messages have been sent to the user. How this polling may be accomplished is described in detail with reference to FIG. 4 below. If chat log <b>204</b> receives a nest message for the user it then dispatches the message to chat local display <b>112</b>.</p>
    <p>Chat sender <b>206</b> manages all messages sent from the user to other Avatars. These messages are sent via network <b>106</b> to host server <b>104</b> and Avatar manager <b>102</b>. How this is accomplished by the present invention will be further explained with reference to FIG. 4 below.</p>
    <p>Finally, local Avatar tracker <b>212</b> manages Avatar name tags <b>1102</b> (FIG. <b>11</b>). Local Avatar tracker <b>212</b> is constantly polling to find the current positions of Avatars in 3D viewport <b>1100</b> so to reposition Avatar name tags <b>1102</b> accordingly. As stated above, in an embodiment of the present invention, Avatar name tags <b>1102</b> are located at the very top of the screen directly above the Avatars' heads. Name tags <b>1102</b> serve both to identify Avatars in the local area and to identify the chat associated from that Avatar by aligning Avatar name tags <b>1102</b> and chat wads on the same horizontal plane. Name tags <b>1102</b> track an Avatar's motion in the 3-D world and appear far right or far left for Avatars not in view. Chat local display <b>112</b> is described next with reference to FIG. <b>3</b>.</p>
    <heading>E. Chat Local Display of the Present Invention</heading> <p>In FIG. 3, chat local display <b>112</b> receives inputs from various components of local chat system <b>108</b> (including 3D positions of potential Avatars <b>304</b>, current camera position <b>306</b> and a text chat message <b>308</b>). Chat local display <b>112</b> may also receive a user interface event <b>302</b> from UI <b>110</b>. Based on the type of user interface event <b>302</b>, chat local display <b>112</b> informs either a drag and drop manager <b>310</b>, a tool tip manager <b>312</b> or a pull down menu manager <b>314</b> to manage user interface event <b>302</b>. Chat local display <b>112</b> uses inputs <b>304</b>, <b>306</b> and <b>308</b> to produce the message graphic in 3D and its 3D position, as indicated by output <b>316</b>.</p>
    <p>Inputs <b>304</b>, <b>306</b> and <b>308</b> will be described first. Input <b>304</b> consists of the 3D positions of potential Avatars that chat local display <b>112</b> must be aware of in order to determine the 3D position of Output <b>316</b>. Input <b>304</b> is sent to chat local display <b>112</b> by local Avatar tracker <b>212</b>.</p>
    <p>Input <b>306</b> consists of the camera position of the user, among other things. The camera position is indicated by three values including: position, orientation and the field of view (fov). What is rendered in the 3D viewport <b>1100</b> is determined by the camera's position, orientation and field of view. Input <b>306</b> is also sent to chat local display <b>112</b> by local Avatar tracker <b>212</b>.</p>
    <p>Input <b>308</b> is the text chat message that one Avatar has sent to another. Input <b>308</b> is sent to chat local display <b>112</b> by chat log <b>204</b>.</p>
    <p>As described above, chat local display <b>112</b> maintains and tracks a list of chat message wads. Each chat message wad contains one chat message, along with state and structures required to animate and display the chat message in the 3D world. The chat message includes the message sent by the user, the time, recipient animation attributes, appearance attributes, and so forth. Chat local display <b>112</b> takes input <b>308</b> (i.e., text chat message) and creates a new chat message wad. Also described above, is how chat local display <b>112</b> handles the 3D animation or message transition from the sender Avatar to local chat area <b>1107</b> when a message object is sent. This animation helps to further link the chat message wad <b>1106</b> with the sender in the 3D world. Every new message object is added to local chat area <b>1107</b> with a 3D animation. The transition helps to cue the user as to the location of the Avatar communicating with him or her. This transition can be customized by the sender (e.g., path shape, trails, speed, etc.) and are stored as objects in the chat message wad <b>1106</b>. The objects of chat message wad <b>1106</b> are described next with reference to FIGS. 5 and 6.</p>
    <p>FIG. 5 illustrates that exemplar) chat wad objects <b>502</b> include a chat message object <b>504</b>, an author name object <b>506</b>, an author key object <b>508</b>, a time object <b>510</b> indicating when the chat message was received, a channel object <b>512</b> indicating on which channel the chat message sent, and a drawing parameters object <b>514</b>. As illustrated in FIG. 6, drawing parameters object <b>514</b> is further broken into a texture object <b>602</b>, a font object <b>604</b>, a color object <b>606</b>, a point size object <b>608</b>, a transition state object <b>610</b>, a transition sound object <b>612</b>, a transition animation object <b>614</b> and a chat mode object <b>616</b>. How chat local display <b>112</b> uses chat wad objects <b>502</b> to accomplish the transition (or animation) is described in detail below in Section I with references to FIGS. 15-21.</p>
    <heading>F. User Interaction with the Present Invention</heading> <p>User interface event <b>302</b> may be the result of user interaction with one or more user interface objects described above with reference to FIG. <b>11</b>. Once chat local display <b>112</b> receives a user interface event <b>302</b> from UI <b>110</b>, chat local display <b>112</b> determines whether to send the event to either drag and drop manager <b>310</b>, tool tip manager <b>312</b> or pull down menu manager <b>314</b> to carry out event <b>302</b>. User interaction with the present invention is next described in more detail.</p>
    <p>The following table illustrates a partial list of user interaction with the user interface objects of the present invention, along with each interaction's triggered response, as they relate to the mouse interface of the present invention.</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup align="left" colsep="0" rowsep="0" cols="2"> <colspec colname="1" colwidth="105pt" align="left"> </colspec> <colspec colname="2" colwidth="112pt" align="left"> </colspec> <thead> <tr class="description-tr"> <td namest="1" nameend="2" align="center" rowsep="1" class="description-td" colspan="2"> </td>
              </tr> <tr class="description-tr"> <td class="description-td">USER ACTION</td>
                <td class="description-td">TRIGGERED RESPONSE</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="2" align="center" rowsep="1" class="description-td" colspan="2"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td class="description-td">Mouse hovers over an object.</td>
                <td class="description-td">A floating tool tip is brought up near</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">the cursor.</td>
              </tr> <tr class="description-tr"> <td class="description-td">Left Click</td>
                <td class="description-td">Can be used to select an object.</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">menu item or pull up a tool tip.</td>
              </tr> <tr class="description-tr"> <td class="description-td">Right click</td>
                <td class="description-td">A context sensitive drop down menu</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">of object interactions is displayed</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">near the cursor.</td>
              </tr> <tr class="description-tr"> <td class="description-td">Double (left) click</td>
                <td class="description-td">Executes default object action</td>
              </tr> <tr class="description-tr"> <td class="description-td">Drag and Drop (with left mouse</td>
                <td class="description-td">Used to move objects around the</td>
              </tr> <tr class="description-tr"> <td class="description-td">button)</td>
                <td class="description-td">world and onto other objects,</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">combine objects, equip objects use</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">one object on another object (e.g.,</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">key on door), etc.</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="2" align="center" rowsep="1" class="description-td" colspan="2"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>The tool tip and right-click functions are key parts of a user's interaction with the present invention. Most user interface objects in FIG. 11 support these two functions. These functions aid a user in his or her interaction with the present invention. In fact, users can identify an element or object by merely mousing over it and determining what they can do to it with the right-click menu. Furthermore, this right click menu also clearly identifies the default interaction by rendering the option in a different style. In this way an advanced user will know that the next time they encounter the object they can just double click to perform that action. Note that the list of object interactions, and the default may be different depending on the current context. For example, a carriable item may have a default action to pick it up when on the ground, but default to using or examining it once in the user's inventory.</p>
    <p>To facilitate the user's dealings with items in their possession, objects in item window <b>1116</b> (FIG. 11) support all the same methods of interactions as they do in the 3D world. The present invention makes no distinction between objects currently represented in 2D or 3D.</p>
    <p>Drag and drop manager <b>310</b> supports the drag and drop function of the present invention. Dragging and dropping items is supported to perform actions that move an object from one place to another. A prime example is picking up and dropping objects in the 3D world. Picking up an item is possible by dragging it from the 3D world onto either the user's Avatar or item window <b>1116</b>. When an object from the 3D world is dragged, an icon of the object is used to represent it. This same icon is used to represent the object in all 2D interfaces (such as in item window <b>1116</b> and trade windows). The drag and drop function may also be used to initiate interactions between different objects. In order to make it clear if dropping a dragged item in a given place will produce a response, the dragged icon may change to indicate if something is willing to accept the dragged object. In the case where more than one interaction is possible, a menu of possible interactions between objects could be brought up when the object is dropped. How the user can interact with other Avatars is described next.</p>
    <p>Avatars in 3D viewport <b>1100</b> can be interacted with in many of the same ways as other objects in 3D viewport <b>1100</b>. However, because user to user interactions are a primary focus of the present invention, it also supports additional features. One important feature that is provided by the present invention is the atomic exchange of virtual possessions through a trade window. The trade window can be brought up by dragging an item from the user's item window <b>1116</b> onto another Avatar or directly onto the trade window. Additional interactions are available for users to customize their communication to other users (or Avatars) by adding them to their buddy or ignore lists. These options can be accessed through drop down menus (via pull down menu manager <b>314</b>). This is available by right-clicking other Avatars or their name tags. Also, private messages can be “whispered” to other Avatars by double-clicking or through the right-click menu. Communication between components of the distributed chat interface system is described next with reference to FIG. <b>4</b>.</p>
    <heading>G. Communication Between Components of the Present Invention</heading> <p>FIG. 4 is a message flow diagram illustrating communication between some of the components of the distributed chat interface system to facilitate chat according to an example implementation of the present invention. The components shown in FIG. 4 include Avatar manager <b>102</b>, chat log <b>204</b>, chat sender <b>206</b>, UI <b>110</b>, chat local display <b>112</b>, a chat message <b>402</b> and a chat instant messaging window <b>404</b>. As introduced above with reference to FIG. 2, chat log <b>204</b> stores all of the messages sent to the user. These messages are sent via network <b>106</b> from host server <b>104</b> and Avatar manager <b>102</b> (i.e., the server side components of the present invention). As stated above, Avatar manager <b>102</b> handles multiple Avatars and stores (on persistent storage) all messages sent to the Avatars. Therefore, any messages sent to the user (or the user s Avatar) is also stored by Avatar manager <b>102</b>.</p>
    <p>Chat log <b>204</b> does a polling on Avatar manager <b>102</b> to determine if any new messages have been sent to the user. In one embodiment of the present invention, network <b>106</b> is the Internet. Therefore, chat log <b>204</b> must communicate with Avatar manager <b>102</b> via the Internet. In an embodiment of the present invention, chat log <b>204</b> does the polling on Avatar manager <b>102</b> by sending a request using the Hyper-Text Transfer Protocol (HTTP). HTTP is well known in the relevant art.</p>
    <p>Once Avatar manager <b>102</b> receives the HTTP request from chat log <b>204</b>, Avatar manger <b>102</b> sends a Extensible Markup Language (“XML”) document to chat log <b>204</b>. The XML document is implemented as a tree data structure. XML is a presentation markup language and is used as a data description language. Tree data structures and XML are also well known in the relevant art. Once chat log <b>204</b> receives the XML document from Avatar manager <b>102</b>, chat log <b>204</b> traverses the tree data structure and makes a decision as to what types of actions need to be taken. For example, if chat log <b>204</b> determines that a new chat message has been sent to the user, then chat log <b>204</b> sends the chat message to chat local display <b>112</b> via UI <b>110</b> (as illustrated in FIG. 3 with chat message <b>308</b>).</p>
    <p>Chat sender <b>206</b> routes all messages sent from the user to other Avatars. As shown in FIG. 4, the user enters chat message <b>402</b> into UI <b>110</b>. UI <b>110</b> forwards chat message <b>402</b> to chat sender <b>206</b>. Chat sender <b>206</b> forwards chat message <b>402</b> to all relevant Avatar managers <b>102</b> via network <b>106</b> and host server <b>104</b>. When network <b>106</b> is the Internet, chat sender <b>206</b> sends the chat message via a HTTP post. Exemplary screen shots in FIGS. 12-14 illustrate this sequence of events. In FIG. 13, the user has entered the chat message “Hey Hello There! How are you doing???” into chat text entry area <b>1112</b>. In FIG. 12, the 3D chat message is flying through the chat text entry area <b>1112</b>. In FIG. 14, chat local display <b>112</b> has already completed the transition and has displayed the chat message in local chat area <b>107</b> under the appropriate Avatar. As stated above, local chat refers to text chat with other Avatars who are within a given radius (e.g., 25 M) from a user's Avatar. FIG. 7 is a flowchart that describes the high level routine for local chat of the present invention.</p>
    <heading>H. Routine for Local Chat of the Present Invention</heading> <p>In FIG. 7, flowchart <b>700</b> starts at step <b>702</b>. In step <b>702</b>, chat local display <b>112</b> creates the necessary chat wad objects <b>502</b> (FIG. 5) for the new chat message. Control then passes to step <b>704</b>.</p>
    <p>In step <b>704</b>, chat local display requests the author/Avatar's 3D position from local Avatar tracker <b>212</b> (FIG. <b>2</b>). The author/Avatar is the author of the new chat message. Control then passes to step <b>706</b>.</p>
    <p>In step <b>706</b>, chat local display creates and renders the 3D to 2D transition of the chat message. Control then passes to step <b>708</b>.</p>
    <p>In step <b>708</b>, chat local display <b>112</b> scrolls up the chat messages that have previously been displayed. Control then passes to step <b>710</b>.</p>
    <p>In step <b>710</b>, chat local display <b>112</b> displays the chat message at the appropriate location in local chat area <b>1107</b>. Flowchart <b>700</b> ends at this point.</p>
    <p>FIG. 8 is a flowchart further describing the step of creating chat wad objects <b>502</b> in FIG. <b>7</b>. In FIG. 8, control starts at step <b>802</b>. In step <b>802</b>, chat local display <b>112</b> receives a new chat message. The new chat message is stored as chat message object <b>504</b> in chat wad object <b>502</b>. Control then passes to step <b>804</b>.</p>
    <p>In step <b>804</b>, chat local display <b>112</b> determines the author's name who created the chat message. Once determined, the author's name is stored in author name object <b>506</b> in chat wad object <b>502</b>. Control then passes to step <b>806</b>.</p>
    <p>In step <b>806</b>, chat local display <b>112</b> determines the author's key (i.e., unique identifier) who created the chat message. Once determined, the author's key is stored in author key object <b>508</b> in chat wad object <b>502</b>. Control then passes to step <b>808</b>. In step <b>808</b>, chat local display <b>112</b> determines the time the chat message was received. Once determined, the time received is stored in time object <b>510</b> in chat wad object <b>502</b>. Control then passes to step <b>810</b>.</p>
    <p>In step <b>810</b>, chat local display <b>112</b> determines the channel the chat message was sent. Once determined, the channel is stored in channel object <b>512</b> in chat wad object <b>502</b>. Control then passes to step <b>812</b>.</p>
    <p>In step <b>812</b>, chat local display <b>112</b> determines the drawing parameters of the chat message. Once determined, the drawing parameters are stored in drawing parameters object <b>514</b> in chat wad object <b>502</b>. The flowchart in FIG. 8 ends at this point.</p>
    <p>FIG. 9 is a flowchart further describing the step of determining the drawing parameters of the chat message in FIG. <b>8</b>. In FIG. 9, control starts at step <b>904</b>. In step <b>904</b>, chat local display <b>112</b> determines the font in which the chat message is to be displayed. Once determined, the font is stored in font object <b>604</b> (FIG. 6) in chat wad object <b>502</b>. Control then passes to step <b>906</b>.</p>
    <p>In step <b>906</b>, chat local display <b>112</b> determines the color in which the chat message is to be displayed. Once determined, the color is stored in color object <b>606</b> in chat wad object <b>502</b>. Control then passes to step <b>908</b>.</p>
    <p>In step <b>908</b>, chat local display <b>112</b> determines the point size of the font in which the chat message is to be displayed. Once determined, the point size is stored in point size object <b>608</b> in chat wad object <b>502</b>. Control then passes to step <b>909</b>.</p>
    <p>In step <b>909</b>, chat local display <b>112</b> creates the texture in which the chat message is to be displayed. Once created, the texture is stored in texture object <b>602</b> in chat wad object <b>502</b>. Control then passes to step <b>910</b>.</p>
    <p>In step <b>910</b>, chat local display <b>112</b> determines the transition state of the chat message. Once determined, the transition state is stored in transition state object <b>610</b> in chat wad object <b>502</b>. Control then passes to step <b>912</b>.</p>
    <p>In step <b>912</b>, chat local display <b>112</b> determines the transition sound of the chat message. Once determined, the transition sound is stored in transition sound object <b>612</b> in chat wad object <b>502</b>. Control then passes to step <b>914</b>.</p>
    <p>In step <b>914</b>, chat local display <b>112</b> determines the transition animation of the chat message. Once determined, the transition animation is stored in transition animation object <b>614</b> in chat wad object <b>502</b>. Control then passes to step <b>916</b>.</p>
    <p>In step <b>916</b>, chat local display <b>112</b> determines the chat mode of the chat message. Once determined, the chat mode is stored in chat mode object <b>616</b> in chat wad object <b>502</b>. The flowchart in FIG. 9 ends at this point.</p>
    <p>FIG. 10 is a flowchart further describing the step of scrolling up previously displayed chat messages in FIG. <b>7</b>. In FIG. 10, control starts at step <b>1002</b>.</p>
    <p>In step <b>1002</b>, chat local display <b>112</b> determines the oldest previously displayed chat message still remaining in local chat area <b>1107</b> (FIG. <b>11</b>). Control then passes to step <b>1004</b>.</p>
    <p>In step <b>1004</b>, chat local display <b>112</b> determines the new position for the oldest previously displayed chat message in step <b>1002</b>. Control then passes to step <b>1005</b>.</p>
    <p>In step <b>1005</b>, chat local display <b>112</b> determines whether the new position is beyond chat handle <b>1104</b> (FIG. <b>11</b>). Recall that local chat area <b>1107</b> ends at chat handle <b>1104</b>. If the new position is beyond chat handle <b>1104</b>, then control passes to step <b>1010</b>. Alternatively, control passes to step <b>1006</b>.</p>
    <p>In step <b>1006</b>, chat local display <b>112</b> determines the translucency of the chat message based on its new position. Recall that the present invention displays a message more translucent as it gets older. Control then passes to step <b>1008</b>.</p>
    <p>In step <b>1008</b>, chat local display <b>112</b> re-displays the chat message in local chat area <b>1107</b>. Control then passes to step <b>1010</b>.</p>
    <p>In step <b>1010</b>, chat local display <b>112</b> determines if there are more previously displayed chat messages that need to be scrolled up. If the answer is positive, then control passes back to step <b>1002</b> and chat local display <b>112</b> re-executes steps <b>1002</b>-<b>1008</b> for the remaining chat message. Alternatively, the flowchart in FIG. 10 ends.</p>
    <heading>3D and 2D Chat Message Transitions According to the Present Invention</heading> <p>FIGS. 15A-15B depict a flowchart <b>1502</b> of a method for delivering a chat message in accordance with the present invention. The invention, however, is not limited to this functional description. Rather, it will be apparent to persons skilled in the art from the teachings herein that other functional flows are within the scope and spirit of the present invention.</p>
    <p>The steps of the flowchart <b>1502</b> are executed by a recipient interface. As discussed above in regard to FIGS. 1-2, in an embodiment, the recipient interface comprises a GUI <b>110</b>, a chat local display <b>112</b> and a local chat system <b>108</b>, which itself is further comprised of an Avatar session manager <b>202</b>, a chat log <b>204</b>, a chat sender <b>206</b> and a local Avatar tracker <b>212</b>. Accordingly, the steps of flowchart <b>1502</b> will be discussed in reference to these elements. However, one skilled in the art will appreciate that the steps of the flowchart <b>1502</b> may be executed by a recipient interface comprised of any suitable hardware or software, or combination of both hardware and software for performing the listed steps.</p>
    <p>The delivery of the chat message starts at step <b>1504</b>, after the local chat system <b>108</b> has received the chat message <b>308</b>. The chat local display <b>112</b> incorporates the chat message <b>308</b> into a chat wad object <b>502</b>. As discussed above in regard to FIG. 5, the chat wad object <b>502</b> includes the chat message <b>308</b>, author name <b>506</b>, the author key <b>508</b>, the time the chat message was sent <b>510</b>, the channel on which the chat message was sent <b>512</b>, and the drawing parameters <b>514</b>. Furthermore, as discussed above in regard to FIG. 6, the drawing parameters <b>514</b> further includes the texture <b>602</b>, the font <b>604</b>, the color <b>606</b>, the point size <b>608</b>, the transition state <b>610</b>, the transition sound <b>612</b>, the transition animation <b>614</b>, and the chat mode <b>616</b>. However, reference to chat wad object <b>502</b> is not limiting and other data structures, including other forms of chat wad objects, can be used as will be appreciated by one skilled in the relevant art.</p>
    <p>At step <b>1506</b>, after the chat local display creates the chat wad object <b>502</b>, the chat local display <b>112</b> checks the transition state <b>610</b> of the drawing parameters <b>514</b> of the chat wad object <b>502</b>.</p>
    <p>As shown in steps <b>1508</b> and <b>1510</b>, if the transition state <b>610</b> is “just arrived,” then the chat local display <b>112</b> generates a textured message by mapping the chat message <b>308</b> to a texture. FIGS. 16A-16F show example textured messages for the chat message “Hi there.”</p>
    <p>In embodiments, the chat local display <b>112</b> uses information in the drawing parameters <b>514</b> to generate the textured message. For example, the chat local display <b>112</b> can select a texture for the textured message based on the texture <b>602</b> parameter in the drawing parameters <b>514</b>. Similarly, the chat local display <b>112</b> can select a font, color or point size for the textured message based on the font <b>604</b>, color <b>606</b> and point size <b>608</b> parameters in the drawing parameters <b>515</b>, respectively.</p>
    <p>In an embodiment, the texture <b>602</b>, font <b>604</b>, color <b>606</b>, and point size <b>608</b> drawing parameters are each selectable by the sender of the chat message via the sender's GUI, thereby allowing the sender to personally customize his or her textured messages. By customizing his or her textured messages in this manner, the sender can in essence create a recognizable “signature” for the textured message that assists the recipient in identifying its author. In an alternative embodiment, the texture <b>602</b>, font <b>604</b>, color <b>606</b>, and point size <b>608</b> drawing parameters can be selected by the sender to indicate the content of the chat message <b>308</b>. For example, if the chat message <b>308</b> is an urgent message, the sender of the chat message can select a bold font with a very large point size to lend a particular emphasis to the textured message.</p>
    <p>Alternatively, the texture <b>602</b>, font <b>604</b>, color <b>606</b>, and point size <b>608</b> drawing parameters can each be selected by the chat local display <b>112</b> itself to indicate the number of intended recipients of the chat message <b>308</b>. In other words, the texture <b>602</b>, font <b>604</b>, color <b>606</b> and point size <b>608</b> drawing parameters can each be selected by the chat local display <b>112</b> as a function of the chat mode <b>616</b>. For example, if the chat message <b>308</b> is directed to only a single recipient (i.e. the chat mode <b>616</b> is “whisper”), this may be indicated by generating a textured message with a predetermined texture, font, color or point size that uniquely identifies a whispered chat message.</p>
    <p>In another embodiment, the chat local display <b>112</b> selects a texture <b>602</b> and/or color <b>606</b> for the textured message that ensures legibility of the textured message when it is rendered in the 3D world. For example, the chat local display <b>112</b> can select a texture <b>602</b> for the textured message that appears as an overlay of the original chat message <b>308</b> (i.e., the outline of the message is solid, but the message itself is transparent), and a color <b>606</b> for the textured message that is different than the color of the portion of the 3D world in which the textured message will be rendered, thereby ensuring that the textured message will be legible to the recipient when it is rendered in 3D space. An example of this is shown in FIG. 19C, where the textured message corresponding to the chat message “Hi there” is rendered using a white overlay against a shaded background. This example is not limiting, and one skilled in the art will appreciate that other textures and/or colors can be selected to ensure legibility of the textured message.</p>
    <p>Returning to step <b>1510</b> of the flowchart <b>1502</b> of FIG. 15, when the transition state <b>610</b> is “just arrived,” the chat local display <b>112</b> also determines a start point and end point for the 3D transition of the textured message in the 3D world. The start point can be any point closer to the digital representation of the sender than to the digital representation of the recipient, and the end point can be any point closer to the digital representation of the recipient than to the digital representation of the sender. In embodiments, the start point is equal to the location of the digital representation of the sender and the end point is equal to the location of the digital representation of the recipient. As discussed above, the location of the digital representation of the sender and recipient is tracked by the local Avatar tracker <b>212</b> within the local chat system <b>108</b>. These locations can be communicated to the chat local display <b>112</b> for the purpose of calculating the start and end point of the 3D transition in step <b>1510</b>.</p>
    <p>After the chat local display <b>112</b> has determined the start and end points for the 3D transition of the textured message in the 3D world, it changes the transition state <b>610</b> from “just arrived” to “3D transition,” and sets a counter to the number of rendering steps N<b>1</b> required to display the 3D transition of the textured message, as shown in step <b>1512</b>. The chat local display <b>112</b> then returns to step <b>1506</b> and again checks the transition state <b>610</b>.</p>
    <p>As shown in steps <b>1514</b> and <b>1516</b>, if the transition state <b>610</b> is equal to “3D transition,” then the chat local display <b>112</b> calculates a rendering position in the 3D world at which to render the textured message and renders the textured message at that location. The number of rendering steps N<b>1</b> is then reduced by one and, at step <b>1518</b>, if N<b>1</b> is not equal to zero (i.e., there are more 3D transition rendering steps remaining), then the functional flow loops back through steps <b>1506</b>, <b>1508</b> and <b>1514</b> and the rendering of the textured message in the 3D world continues at step <b>1516</b> until the number of steps has been reduced to zero.</p>
    <p>In embodiments, the rendering position of the textured message is calculated in step <b>1516</b> to progress along a path beginning at the start point determined in the earlier step <b>1510</b> and terminating at the end point determined in the same earlier step. As viewed through the recipient's 3D viewport, this periodic rendering of the textured message creates the appearance that the textured message is “flying” or traveling through 3D space from the sender's Avatar toward the recipient. This effect not only notifies the recipient that he or she is receiving a chat message, but also indicates to the recipient the location of the digital representation of the sender in 3D space.</p>
    <p>The number of steps N<b>1</b> in the 3D transition and the rate at which the repeated rendering of the textured message occurs in step <b>1516</b> determines the frame rate at which the textured message will appeal to be moving toward the recipient's 3D viewport. Generally speaking, the higher the frame rate, the smoother the transition of the textured message as viewed through the recipient's 3D viewport. In embodiments, the frame rate is 30 frames per second or higher, although the invention is not so limited, and the invention can be practiced at frame rates of less than 30 frames per second.</p>
    <p>In embodiments, the rendering position progresses along a path between the start point and the end point defined by a vector (i.e., a straight line in 3D space). This is shown, for example, in FIG. 17A, where the start point <b>1702</b> is the location of the sender's Avatar <b>1706</b>, the end point <b>1704</b> is the location of the recipient's Avatar <b>1708</b>, and the line <b>1706</b> is the vector connecting them. FIG. 17A is intended to indicate a top-down view of the 3D world. As can be seen in FIG. 17A, the location of the recipient's Avatar <b>1708</b> is also the location of the recipient viewport <b>1710</b>, and the dashed lines emanating from the recipient viewport <b>1710</b> indicate the field of view of the recipient viewport <b>1710</b> in the 3D world. The end point <b>1704</b> is also located on the recipient viewport <b>1710</b>. The vector <b>1706</b> indicates the path along which the textured message is repeatedly rendered in step <b>1516</b>. In embodiments, rendering positions are calculated by dividing the vector <b>1706</b> into N<b>1</b>−1 equal segments between the start point and the end point, where N<b>1</b> is the number of rendering steps determined in step <b>1512</b>. The rendering positions progress sequentially from the start point <b>1702</b> to the next nearest point between segments, terminating at the end point <b>1704</b>. As viewed through the recipient viewport <b>1710</b>, this periodic rendering of the textured message along the vector <b>1706</b> creates the appearance that the textured message is traveling along a straight line through 3D space to the recipient viewport <b>1710</b> from the sender's Avatar <b>1702</b>.</p>
    <p>In an alternate embodiment, the rendering position progresses along a path between the start and end points that is defined by a vector as discussed above plus some offset value determined by an offset function. This is depicted, for example, in FIG. 17B, where the start point <b>1702</b> and the end point <b>1704</b> are the same as in FIG. <b>17</b>A. Here, however, the path along which the textured message is repeatedly rendered is the curving path <b>1714</b>. This curving effect is achieved by dividing the vector <b>1706</b> into N<b>1</b>−1 segments between the start point <b>1702</b> and end point <b>1704</b> as discussed above in regard to FIG. <b>17</b>A. The rendering position is calculated by choosing a point on the vector <b>1706</b> beginning with the start point <b>1702</b>, progressing to the nearest point between segments and terminating at the end point <b>1704</b>. For each point that is chosen, an offset value is calculated as a function of the distance of the point along vector <b>1706</b>. This offset value is added to the position of the chosen point to determine the rendering position. For example, in FIG. 17B, during the 3D transition, the point <b>1712</b> on the vector <b>1706</b> is chosen. An offset value <b>1716</b> is calculated as a function of the distance of the point <b>1712</b> along the vector <b>1706</b>, and is added to the location of the point <b>1712</b> to calculate the rendering position <b>1714</b>. The offset value <b>1716</b> is represented in FIG. 17B as a line between the point <b>1712</b> and the resulting rendering position <b>1714</b>.</p>
    <p>In an embodiment where the start point is the location of the sender's Avatar and the end point is the location of the recipient's Avatar, the offset function discussed above can be selected so that it is always equal to zero at the start point and end point, which ensures that the rendering of the textured message during the 3D transition will always begin at the location of the sender's Avatar and terminate at the recipient's Avatar. This creates a smoother 3D transition from the point of view of the recipient's viewport. For example, the offset function can be a sinusoidal function, where the value of the sinusoidal function is equal to zero at the start point and is also equal to zero at the end point. This is shown, for example, in FIG. 17B, where the resulting offset is the sinusoidal curve <b>1718</b> with a zero offset value at the start point <b>1702</b> and the end point <b>1704</b>. This example is not limiting, however, and one skilled in the art will appreciate that any offset function may be utilized that is equal to zero at the start and end points.</p>
    <p>In an alternative embodiment, the rendering path is a combination of a vector and a vector plus some offset value, calculated according to the methods discussed above. This is represented, for example, in FIG. 17C, where a portion of the rendering path <b>1722</b> is defined by the vector between the start point <b>1702</b> and the point <b>1704</b> plus some offset value and another portion of the rendering path <b>1724</b> is defined by only the vector between the point <b>1720</b> and the end point <b>1704</b>.</p>
    <p>In embodiments, the chat local display <b>112</b> uses information in the drawing parameters <b>514</b> to determine the shape of the rendering path. For example, although it is not shown in FIG. 6, the drawing parameters <b>514</b> could include an offset function used by the chat local display <b>112</b> to determine the shape of the rendering path as described in regard to FIGS. 17B and 17C above.</p>
    <p>In other embodiments, the shape of the rendering path is selectable by the sender of the chat message via the sender's GUI, thereby allowing the sender to personally customize the delivery of his or her textured messages. By customizing his or her textured messages in this manner, the sender can create a recognizable “signature” for the delivery of his or her textured message that assists the recipient in identifying its author. For example, in an embodiment, the sender can select the offset function that will be used by the chat local display <b>112</b> to determine the shape of the rendering path as described in regard to FIGS. 17B and 17C above.</p>
    <p>In embodiments, the chat local display <b>112</b> can account for the possibility that the desired start points and end points may change during the 3D transition of the textured message. This may occur, for example, if the start point is equal to the location of the sender's Avatar and the end point is equal to the location of the recipient's Avatar and the sender's Avatar or the recipient's Avatar moves during the 3D transition of the textured message. In order to account for this possibility, the chat local display <b>112</b> can determine a new start and end point as well as recalculate a new rendering path between them before each execution of the rendering step <b>1516</b>. Because the chat local display <b>112</b> keeps track of how many rendering steps have occurred, the chat local display <b>112</b> can also then calculate how far along the new path the next rendering position should be. This will ensure that, even though the start and end points may change, the textured message will continue to be rendered progressively along a path between them.</p>
    <p>In other embodiments, the chat local display <b>112</b> can also account for the possibility that the desired start point may be outside of the field of view of the recipient's 3D viewport. This may occur, for example, if the start point is equal to the location of the digital representation of the sender and the digital representation of the sender is outside the field of view of the recipient's 3D viewport (e.g., the sender's Avatar is behind the field of view of the recipient's 3D viewport). In this situation, the chat local display <b>112</b> can render the textured message from the start point to the end point along a curved path that curves outward into the field of vision of the recipient's 3D viewport and back to arrive at the viewport itself. By rendering the textured message along this curved path, the chat local display <b>112</b> notifies the recipient that he or she is receiving a chat message, and also indicates to the recipient that the location of the digital representation of the sender is outside of the field of vision of the recipient's 3D viewport.</p>
    <p>Also, in embodiments, if the location of the digital representation of the sender is outside and to the left of the center of the recipient's 3D viewport, then the chat local display <b>112</b> will render the textured message along a curved path that will appear to emanate from the left-hand side the recipient's 3D viewport. Conversely, if the location of the digital representation of the sender is outside and to the right of the center of the recipient's 3D viewport, then the chat local display <b>112</b> will render the textured message along a curved path that will appear to to emanate from the right-hand side of the recipient's 3D viewport. Accordingly, in addition to indicating to the recipient that the location of the digital representation of the sender is outside of the field of vision of the recipient viewport, this will provide the recipient with additional information as to whether the location of the digital representation is to the left or right of the center of the 3D viewport.</p>
    <p>Returning now to step <b>1518</b> of the flowchart <b>1502</b> of FIG. 15, when the number of steps in the 3D transition has reached zero at step <b>1518</b>, the 3D transition is complete. As a result of the final rendering step <b>1516</b>, the textured message appears on the bottom of the recipient's viewport, where it remains until it is displaced by other textured messages as will be discussed in more detail below. As shown in step <b>1520</b>, the chat local display <b>112</b> then changes the transition state <b>610</b> from “3D transition” to “2D transition” and sets the number of steps N<b>2</b> for a subsequent 2D transition. The chat local display <b>112</b> then returns to step <b>1506</b> and again checks the transition state <b>610</b>.</p>
    <p>As shown at steps <b>1522</b> and <b>1524</b>, when the transition state <b>610</b> is equal to “2D transition,” the chat local display <b>112</b> renders a texture to effect a 2D transition and then reduces the number of steps N<b>2</b> by one. At step <b>1526</b>, the chat local display <b>112</b> checks to see if the number of steps in the 2D transition is equal to zero. If the number of steps is not equal to zero then there are 2D transition frames left to render, and the chat local display <b>112</b> loops back through steps <b>1506</b>, <b>1508</b>, <b>1514</b>, and <b>1522</b> to render the texture again at step <b>1524</b>, thereby continuing the 2D transition.</p>
    <p>The 2D transition is essentially an animation effect intended to simulate the collision of the textured message from the 3D world with the recipient's viewport. The texture rendered in step <b>1524</b> can be any suitable texture for achieving this effect, and can be rendered at any suitable position(s) for achieving this effect. For example, the texture rendered in step <b>1524</b> can simulate the appearance of a sheet of water, and can be rendered on a short path leading to the recipient's viewport, thereby generating a “splashing” effect immediately after the textured message has reached the bottom of the recipient's viewport at the end of the 3D transition.</p>
    <p>At step <b>1526</b>, when the number of steps in the 2D transition has reached zero, the 2D transition is complete and the process moves to step <b>1528</b>. At step <b>1528</b>, the chat local display <b>112</b> changes the transition state <b>610</b> from “2D transition” to “in list.” The process then returns to step <b>1506</b>. and the chat local display <b>112</b> again checks the transition state <b>610</b>.</p>
    <p>As discussed above, as a result of the final rendering step <b>1516</b> in the 3D transition, the textured message appears on the bottom of the recipient's viewport, where it is then maintained as essentially a 2D element. In embodiments, the textured message remains in this position until the recipient receives another message that appears as a textured message on the bottom of the recipient's viewport. When this occurs, the chat local display <b>112</b> vertically displaces the original textured message so that it appears above the subsequently received textured message. As each subsequent textured message appears at the bottom of the recipient viewport, the chat local display vertically displaces the original textured message. By vertically displacing older textured messages, the chat local display <b>112</b> ensures that textured messages will not overlap on the bottom of the recipient's viewport and also creates a visible chat “history” on the recipient's viewport that permits the recipient to track the thread of a conversation from older textured messages at the top to newer textured messages below.</p>
    <p>As shown in steps <b>1530</b> and <b>1532</b> of FIG. 15, when the transition state <b>610</b> is equal to “in list,” the chat local display <b>112</b> compares the height of the textured message on the recipient's viewport to a maximum height parameter. This maximum height parameter can be a predetermined height parameter fixed by the chat local display <b>112</b> or can be selected by the recipient via the GUI <b>110</b>. If the height of the textured message does not exceed the maximum height parameter, then the chat local display <b>112</b> loops back through steps <b>1506</b>, <b>1508</b>, <b>1514</b>, <b>1522</b>, and <b>1530</b> to again compare the height of the textured message to the maximum height parameter at step <b>1532</b>. This looping will continue until the textured message is vertically displaced to a height on the recipient's viewport that exceeds the maximum height parameter. When this occurs, the chat local display <b>112</b> no longer displays the textured message on the recipient viewport and, as depicted in step <b>1534</b>, the chat local display <b>112</b> will change the transition state <b>610</b> from “in list” to “scrolled off,” and return to checking the transition state <b>610</b> at step <b>1506</b>.</p>
    <p>The chat local display retains the information in the chat wad object <b>502</b> for a limited period of time after the textured message associated with the chat wad object <b>502</b> has been scrolled of the recipient viewport. This time limit can be a predetermined time period fixed by the chat local display <b>112</b> or can be selected by the recipient via the GUI <b>110</b>. In embodiments, the time limit is in the range of 20-30 seconds; however, any time limit may be used. As shown in steps <b>1536</b> and <b>1538</b>, when the transition state <b>610</b> is equal to “scrolled off,” the chat local display <b>112</b> checks to see if the amount of time that has passed since the textured message was scrolled off the recipient's viewport exceeds the time limit. If it does not, then the chat local display <b>112</b> loops back through steps <b>1506</b>, <b>1508</b>, <b>1514</b>, <b>1522</b>, <b>1530</b>, and <b>1536</b> to again compare the time that has passed to the time limit. Once the time that has passed exceeds the time limit, the chat local display <b>112</b> deletes the information in the chat wad object <b>502</b> and the chat message delivery routine ends.</p>
    <p>FIGS. 18A-18F depict the delivery of a chat message from a sender to a recipient according to an embodiment of a present invention as viewed from the recipient's 3D viewport. As seen in FIG. 18A, the recipient viewport <b>1802</b> provides a view of the 3D world <b>1804</b> which includes Avatars <b>1806</b> and <b>1808</b>. The Avatars <b>1806</b> and <b>1808</b> are the digital representations of users who share the 3D multi-user environment with the recipient.</p>
    <p>As discussed above in regard to FIG. 15, in embodiments, when a user sends a chat message to the recipient, the chat local display <b>112</b> maps the chat message to a texture to generate a textured message and then periodically renders the textured message in 3D space on a path beginning at the location of the user's Avatar and terminating at the location of the recipient's Avatar, which coincides with the location of the recipient's viewport. This 3D transition is shown in FIGS. 18A-18D, where the textured message <b>1810</b> corresponding to the chat message “Hi there” is periodically rendered in the 3D world <b>1804</b> starting at the location of the Avatar <b>1806</b> and terminating at the location of the recipient's Avatar, creating the effect that the textured message <b>1810</b> is traveling through the 3D world <b>1804</b> to land at the bottom of the recipient viewport <b>1802</b>. As noted above, this effect not only notifies the recipient that he or she is receiving a chat message, but also indicates to the recipient the location of the digital representation of the sender <b>1806</b> in the 3D world <b>1804</b>.</p>
    <p>As also discussed in regard to FIG. 15 above, in embodiments, textured messages are maintained as 2D elements at the bottom of the recipient's viewport after the 3D transition until another textured message is received. When this occurs, the chat local display <b>112</b> vertically displaces the original textured message so that it appears above the subsequently received textured message. This is shown, for example, in FIGS. 18D-18F, where the textured message <b>1812</b> corresponding to the chat message “Hello” is periodically rendered in the 3D world <b>1804</b> starting at the location of the Avatar <b>1808</b> and terminating at the location of the recipient's Avatar to land at the bottom of the recipient viewport <b>1802</b>. When the textured message <b>1812</b> arrives at the bottom of the recipient viewport <b>1802</b>, the chat local display <b>112</b> vertically displaces the textured message <b>1810</b>. By vertically displacing the textured message <b>1810</b>, the chat local display <b>112</b> ensures that the textured messages <b>1810</b> and <b>1812</b> do not overlap. This displacement also serves to create a visible chat “history” on recipient viewport <b>1802</b> that permits the recipient to track the thread of a conversation from older textured messages at the top to newer textured messages below.</p>
    <p>In embodiments, the chat local display <b>112</b> renders a textured message gradually more translucent each time that it is vertically displaced on the recipient's viewport, so that older textured messages will not obstruct the recipient's view of the 3D world through the viewport. In further embodiments. the chat local display <b>112</b> causes textured messages to “fade” from the recipient's viewport as the time on which they appear on the recipient's viewport approaches a predetermined time limit.</p>
    <p>FIGS. 19A-19F depicts the delivery of a chat message from a sender to a recipient according to an embodiment of a present invention as viewed from the recipient's 3D viewport, where the digital representation of the sender is outside the field of view of the recipient's 3D viewport. As seen in FIG. 19A, the recipient viewport <b>1902</b> provides a view of the 3D world <b>1904</b> which includes the Avatar <b>1906</b>. The Avatar <b>1906</b> is the digital representation of a user who shares the 3D multi-user environment with the recipient. Not shown in FIG. 19A but described below is another Avatar that is located outside the field of view of the recipient viewport <b>1902</b>.</p>
    <p>As discussed above in regard to FIG. 15, in embodiments where the start point for the 3D transition is the sender's Avatar, and the sender's Avatar is outside the field of vision of the recipient's 3D viewport, the chat local display <b>112</b> canl render the textured message from the start point to the end point along a curved path that curves outward into the field of vision of the recipient's 3D viewport and back to arrive at the viewport itself. By rendering the textured message along this curved path, the chat local display <b>112</b> notifies the recipient that he or she is receiving a chat message, and also indicates to the recipient that the location of the sender's Avatar is currently outside of the field of vision of the recipient's 3D viewport. This is shown, for example, in FIGS. 19A-19F, where the textured message <b>1908</b> corresponding to the chat message “Turn around!” is periodically rendered in the 3D world <b>1904</b> starting at a location outside of the recipient viewport <b>1902</b> in FIG. 19A, curving outward into the field of vision of the recipient viewport <b>1902</b> in FIGS. 19B-19C, and curving back to arrive at the recipient viewport <b>1902</b> in FIGS. 19D-19F. The arrows on the figures indicate the direction of motion of the textured message, and do not appear on the viewport. This rendering method not only notifies the recipient that he or she is receiving a chat message, but also indicates to the recipient that the location of the digital representation of the sender of the textured message <b>1908</b> is outside the field of view of the recipient viewport <b>1902</b>.</p>
    <p>As also discussed above, in embodiments, if the location of the digital representation of the sender is outside and to the left of the center of the recipient's 3D viewport, then the chat local displays <b>112</b> will render the textured message along a curved path that will appear to emanate from the left hand side the recipient's 3D viewport. Conversely, if the location of the digital representation of the sender is outside and to the right of the center of the recipient's 3D viewport, then the chat local display <b>112</b> will render the textured message along a curved path that will appear to emanate from the right hand side of the recipient's 3D viewport. In FIGS. 19A-19F, the textured message is rendered along a curved path that appears to emanate from the left side of the recipient viewport <b>1902</b>, thereby indicating to the recipient that the location of the digital representation of the sender is outside and to the left of center of the recipient viewport <b>1902</b>.</p>
    <p>FIGS. 20A-20F show how textured messages are managed as two-dimensional elements on the recipient's 3D viewport in embodiments of the invention. As seen in FIG. 20A, the recipient viewport <b>2002</b> provides a view of the 3D world <b>2004</b> which includes the Avatars <b>2006</b> and <b>2008</b>. The Avatar <b>2006</b> has sent a chat message to the recipient which now appears as part of a textured message <b>2010</b> on the recipient viewport <b>2002</b>. Similarly, the Avatar <b>2008</b> has sent a chat message to the recipient which now appears as part of a textured message <b>2012</b> on the recipient viewport <b>2002</b>.</p>
    <p>In addition to the functions outlined above, the chat local display <b>112</b> is responsible for managing the textured messages as two-dimensional elements on the recipient's 3D viewport after the 3D and 2D transitions have occurred but before the textured messages are scrolled off. In embodiments, the chat local display <b>112</b> displays the textured messages as 2D elements that are in horizontal alignment with the appearance of the digital representation of the sender when the digital representation of the sender is in the field of vision of the recipient's 3D viewport. This is demonstrated, for example in FIGS. 20A-20E where the textured message <b>2010</b> which reads “I'm moving right” is displayed in horizontal alignment with the Avatar <b>2006</b> representing the sender of that message, even as the Avatar <b>2006</b> moves right across the recipient viewport <b>2002</b>. Similary, FIGS. 20A-20D show that the textured message <b>2012</b> which reads “I'm moving left” is displayed in horizontal alignment with the Avatar <b>2008</b> representing the sender of that message, even as the Avatar <b>2008</b> moves right across the recipient viewport <b>2002</b>. By displaying the textured messages in this fashion, the chat local display <b>112</b> assists the recipient in locating the digital representation of the sender of a given chat message.</p>
    <p>In embodiments, when the digital representation of the sender moves out of the field of view of the recipient's 3D viewport, the chat local display displays the textured message flush right on the recipient's 3D viewport when the digital representation of the sender is to the right of the field of view of the viewport and flush left on the recipient's 3D viewport when the digital representation of the sender is to the left of the field of view of the recipient viewport. This is shown, for example, in FIGS. 20D-20F, where the textured message <b>2012</b> which reads “I'm moving left” is displayed flush left on the recipient viewport <b>2002</b> after the Avatar <b>2008</b> representing the sender of that message has moved left out of the field of view of the recipient viewpoint <b>2002</b>. This is also shown in FIGS. 20E-20F, where the textured message <b>2010</b> which reads “I'm moving right” is displayed flush right on the recipient viewport <b>2002</b> after the Avatar <b>2006</b> representing the sender of that message has moved right out of the field of view of the recipient viewport <b>2002</b>. The displaying of the textured messages in this manner provides a cue to the recipient that assists in locating the digital representation of the sender of a given chat message.</p>
    <p>FIGS. 20A-20F also demonstrate that because the chat local display <b>112</b> vertically displaces the older textured message <b>2010</b> on the recipient viewport <b>2002</b> to accommodate the newer textured message <b>2012</b>, the textured messages <b>2010</b> and <b>2012</b> will not overlap even when the chat local display moves them horizontally on the recipient viewport <b>2002</b>. The management of the textured messages on the recipient viewport in this manner ensures the legibility of the textured messages.</p>
    <p>In embodiments of the invention, the 3D transition of a textured message can be accompanied by an additional animation component. For example, in an embodiment of the inventions the local chat display <b>112</b> can render one or more additional animation textures in the 3D world adjacent to and preferably slightly behind the textured message along the same path upon which the textured message is rendered. The rendering of these additional animation textures creates a “trail” for the textured message that further assists the recipient in identifying the location of the digital representation of the sender.</p>
    <p>FIGS. 21A-21F show an example embodiment in which the 3D transition of a textured message is accompanied by an additional animation component. As shown in FIGS. 20A-20F, the recipient viewport <b>2102</b> provides a view of the 3D world <b>2104</b> which includes an Avatar <b>2106</b>, a textured message <b>2110</b> from a sender represented by the Avatar <b>2106</b>, and additional animation textures <b>2108</b>, <b>2109</b> and <b>2110</b>. The additional animation textures <b>2108</b>, <b>2109</b> and <b>2110</b> are rendered by the local chat display <b>112</b> in the 3D world <b>2104</b> adjacent to and slightly behind the textured message <b>2110</b> as it is rendered along a path from the sender's Avatar <b>2106</b> to the recipient's viewport <b>2102</b>. Because the additional animation textures <b>2108</b>, <b>2109</b> and <b>2110</b> are rendered behind the textured message <b>2110</b> and along the same path as the textured message <b>2110</b>, they, in effect, generate a “trail” of stars behind the textured message <b>2110</b> during the 3D transition. This trail can assist the recipient in identifying the location of the Avatar representing the sender of the textured message <b>2110</b>.</p>
    <p>In embodiments, the chat local display <b>112</b> uses the transition animation parameter <b>614</b> from the drawing parameters <b>514</b> in chat wad object <b>502</b> to generate the additional animation textures. For example, the transition animation <b>614</b> parameter can identify one of a set number of additional animation textures that should accompany the rendering of the textured message during the 3D transition.</p>
    <p>In an embodiment, the transition animation parameter <b>614</b> can be selected by the sender of the chat message via the sender's GUI, thereby allowing the sender to personally customize the additional animation texture accompanying his or her textured messages. By customizing the additional animation texture in this manner, the sender, in essence, can create a recognizable “signature” for the textured message that will assist the recipient in identifying its author. In an alternative embodiment, the transition animation parameter <b>614</b> can be selected by the sender to indicate the content of the chat message <b>308</b>. For example, if the chat message <b>308</b> is a love note, the sender of the chat message <b>308</b> can select an additional animation texture comprised of one or more hearts, an example of which is shown in FIG. 16G, or one or more kisses, an example of which is shown in FIG. 16H, which will be rendered trailing behind the textured message during the 3D transition as discussed above. Or, for example, if the chat message <b>308</b> is a question, the sender of the chat message <b>308</b> can choose an additional animation texture of one or more question marks, an example of which is shown in FIG. 161, which will be rendered trailing behind the textured message during the 3D transition.</p>
    <p>Alternatively, the transition animation parameter <b>614</b> can be selected by the chat local display <b>112</b> itself to indicate the number of intended recipients of the chat message <b>308</b>. In other words, the transition animation parameter <b>614</b> can be selected by the chat local display <b>112</b> as a function of the chat mode <b>616</b>. For example, if the chat message <b>308</b> is directed to only a single recipient (i.e. the chat mode <b>616</b> is “whisper”), this may be indicated by rendering the textured message followed by an accompanying animation texture that uniquely identifies a whispered chat message.</p>
    <p>In embodiments of the invention, the 3D transition of a textured message may also be accompanied by an additional audio component. For example, in an embodiment of the invention, the local chat display <b>112</b> can obtain a sound effect and play the sound effect contemporaneously with the rendering of the textured message during the 3D transition.</p>
    <p>In an embodiment, the local chat display <b>112</b> can play the sound effect using well known 3D audio techniques that will create the impression that the sound effect originates from the location of the sender's Avatar in the 3D world. For example, if the sender's Avatar is to the front and left of the recipient's viewport, then the local chat display <b>112</b> will play the sound effect using 3D audio techniques to create the impression that the sound originates from the front and to the left of the recipient.</p>
    <p>In embodiments, the chat local display <b>112</b> uses the transition sound parameter <b>612</b> from the drawing parameters <b>514</b> in chat wad object <b>502</b> to obtain the sound effect. For example, the transition sound parameter <b>612</b> can identify one of a set number of sound effects that can be played contemporaneously with the rendering of the textured message during the 3D transition.</p>
    <p>In an embodiment, the transition sound parameter <b>612</b> can be selected by the sender of the chat message via the sender's GUI, thereby allowing the sender to personally customize the sound effect accompanying his or her textured messages. By customizing the sound in this manner, the sender, in essence, can create a recognizable “signature” for the textured message that will assist the recipient in identifying its author. In an alternative embodiment, the transition sound parameter <b>612</b> can be selected by the sender to indicate the content of the chat message <b>308</b>. For example, if the chat message <b>308</b> is a message of congratulations, the sender of the chat message <b>308</b> can select the sound effect of applause that will be played contemporaneously with the rendering of the textured message during the 3D transition.</p>
    <p>Alternatively., the transition sound parameter <b>612</b> can be selected by the chat local display <b>112</b> itself to indicate the number of intended recipients of the chat message <b>308</b>. In other words, the transition sound parameter <b>612</b> can be selected by the chat local display <b>112</b> as a function of the chat mode <b>616</b>. For example, if the chat message <b>308</b> is directed to only a single recipient (i.e. the chat mode <b>616</b> is “whisper”), this may be indicated by playing a sound effect that uniquely identifies a whispered chat message contemporaneously with rendering if the textured message during the 3D transition.</p>
    <p>Although the invention has been particularly shown and described with reference to several preferred embodiments thereof, it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined in the appended claims.</p>
    <heading>J. Conclusion</heading> <p>While various embodiments of the present invention have been described above, it should be understood that they have been presented by way of example only, and not limitation. It will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined in the appended claims. Thus, the breadth and scope of the present invention should not be limited by any of the above-described exemplary embodiments, but should be defined only in accordance with the following claims and their equivalents.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5021976">US5021976</a></td><td class="patent-data-table-td patent-date-value">Nov 14, 1988</td><td class="patent-data-table-td patent-date-value">Jun 4, 1991</td><td class="patent-data-table-td ">Microelectronics And Computer Technology Corporation</td><td class="patent-data-table-td ">Method and system for generating dynamic, interactive visual representations of information structures within a computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5423554">US5423554</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 1993</td><td class="patent-data-table-td patent-date-value">Jun 13, 1995</td><td class="patent-data-table-td ">Metamedia Ventures, Inc.</td><td class="patent-data-table-td ">Virtual reality game method and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5515078">US5515078</a></td><td class="patent-data-table-td patent-date-value">Mar 11, 1994</td><td class="patent-data-table-td patent-date-value">May 7, 1996</td><td class="patent-data-table-td ">The Computer Museum, Inc.</td><td class="patent-data-table-td ">Virtual-reality positional input and display system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5559995">US5559995</a></td><td class="patent-data-table-td patent-date-value">Sep 2, 1992</td><td class="patent-data-table-td patent-date-value">Sep 24, 1996</td><td class="patent-data-table-td ">Vpl Research, Inc.</td><td class="patent-data-table-td ">Method and apparatus for creating a wireframe and polygon virtual world</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5587936">US5587936</a></td><td class="patent-data-table-td patent-date-value">Oct 5, 1993</td><td class="patent-data-table-td patent-date-value">Dec 24, 1996</td><td class="patent-data-table-td ">Vpl Research, Inc.</td><td class="patent-data-table-td ">Method and apparatus for creating sounds in a virtual world by simulating sound in specific locations in space and generating sounds as touch feedback</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5588104">US5588104</a></td><td class="patent-data-table-td patent-date-value">Oct 4, 1994</td><td class="patent-data-table-td patent-date-value">Dec 24, 1996</td><td class="patent-data-table-td ">Vpl Research, Inc.</td><td class="patent-data-table-td ">Method and apparatus for creating virtual worlds using a data flow network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5588139">US5588139</a></td><td class="patent-data-table-td patent-date-value">Oct 8, 1993</td><td class="patent-data-table-td patent-date-value">Dec 24, 1996</td><td class="patent-data-table-td ">Vpl Research, Inc.</td><td class="patent-data-table-td ">Method and system for generating objects for a multi-person virtual world using data flow networks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5633993">US5633993</a></td><td class="patent-data-table-td patent-date-value">Feb 10, 1993</td><td class="patent-data-table-td patent-date-value">May 27, 1997</td><td class="patent-data-table-td ">The Walt Disney Company</td><td class="patent-data-table-td ">Method and apparatus for providing a virtual world sound system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5644993">US5644993</a></td><td class="patent-data-table-td patent-date-value">Jan 16, 1996</td><td class="patent-data-table-td patent-date-value">Jul 8, 1997</td><td class="patent-data-table-td ">Balt, Inc.</td><td class="patent-data-table-td ">For attaching shelves to an upright frame</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5679075">US5679075</a></td><td class="patent-data-table-td patent-date-value">Nov 6, 1995</td><td class="patent-data-table-td patent-date-value">Oct 21, 1997</td><td class="patent-data-table-td ">Beanstalk Entertainment Enterprises</td><td class="patent-data-table-td ">Interactive multi-media game system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5684943">US5684943</a></td><td class="patent-data-table-td patent-date-value">Mar 30, 1995</td><td class="patent-data-table-td patent-date-value">Nov 4, 1997</td><td class="patent-data-table-td ">Vpl Research, Inc.</td><td class="patent-data-table-td ">Method and apparatus for creating virtual worlds</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5696892">US5696892</a></td><td class="patent-data-table-td patent-date-value">Jun 7, 1995</td><td class="patent-data-table-td patent-date-value">Dec 9, 1997</td><td class="patent-data-table-td ">The Walt Disney Company</td><td class="patent-data-table-td ">Method and apparatus for providing animation in a three-dimensional computer generated virtual world using a succession of textures derived from temporally related source images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5704837">US5704837</a></td><td class="patent-data-table-td patent-date-value">Mar 25, 1994</td><td class="patent-data-table-td patent-date-value">Jan 6, 1998</td><td class="patent-data-table-td ">Namco Ltd.</td><td class="patent-data-table-td ">Video game steering system causing translation, rotation and curvilinear motion on the object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5732232">US5732232</a></td><td class="patent-data-table-td patent-date-value">Sep 17, 1996</td><td class="patent-data-table-td patent-date-value">Mar 24, 1998</td><td class="patent-data-table-td ">International Business Machines Corp.</td><td class="patent-data-table-td ">Method and apparatus for directing the expression of emotion for a graphical user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5736982">US5736982</a></td><td class="patent-data-table-td patent-date-value">Aug 1, 1995</td><td class="patent-data-table-td patent-date-value">Apr 7, 1998</td><td class="patent-data-table-td ">Nippon Telegraph And Telephone Corporation</td><td class="patent-data-table-td ">Virtual space apparatus with avatars and speech</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5755620">US5755620</a></td><td class="patent-data-table-td patent-date-value">Jan 31, 1996</td><td class="patent-data-table-td patent-date-value">May 26, 1998</td><td class="patent-data-table-td ">Kabushiki Kaisha Sega Enterprises</td><td class="patent-data-table-td ">Game system and data processing method thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5784570">US5784570</a></td><td class="patent-data-table-td patent-date-value">Apr 7, 1995</td><td class="patent-data-table-td patent-date-value">Jul 21, 1998</td><td class="patent-data-table-td ">At&amp;T Corp</td><td class="patent-data-table-td ">Server for applying a recipient filter and compressing the input data stream based upon a set of at least one characteristics in a multiuser interactive virtual environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5793382">US5793382</a></td><td class="patent-data-table-td patent-date-value">Jun 10, 1996</td><td class="patent-data-table-td patent-date-value">Aug 11, 1998</td><td class="patent-data-table-td ">Mitsubishi Electric Information Technology Center America, Inc.</td><td class="patent-data-table-td ">Method for smooth motion in a distributed virtual reality environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5815156">US5815156</a></td><td class="patent-data-table-td patent-date-value">Aug 30, 1995</td><td class="patent-data-table-td patent-date-value">Sep 29, 1998</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Interactive picture providing method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5827120">US5827120</a></td><td class="patent-data-table-td patent-date-value">Jul 8, 1996</td><td class="patent-data-table-td patent-date-value">Oct 27, 1998</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Network game system having communication characters for conversation changing into action characters for action gaming</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5846134">US5846134</a></td><td class="patent-data-table-td patent-date-value">Jul 11, 1996</td><td class="patent-data-table-td patent-date-value">Dec 8, 1998</td><td class="patent-data-table-td ">Latypov; Nurakhmed Nurislamovich</td><td class="patent-data-table-td ">Method and apparatus for immersion of a user into virtual reality</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5852672">US5852672</a></td><td class="patent-data-table-td patent-date-value">Jun 9, 1997</td><td class="patent-data-table-td patent-date-value">Dec 22, 1998</td><td class="patent-data-table-td ">The Regents Of The University Of California</td><td class="patent-data-table-td ">Image system for three dimensional, 360 DEGREE, time sequence surface mapping of moving objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5874956">US5874956</a></td><td class="patent-data-table-td patent-date-value">Nov 12, 1996</td><td class="patent-data-table-td patent-date-value">Feb 23, 1999</td><td class="patent-data-table-td ">Platinum Technology</td><td class="patent-data-table-td ">For use with a programmable digital computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5880731">US5880731</a></td><td class="patent-data-table-td patent-date-value">Dec 14, 1995</td><td class="patent-data-table-td patent-date-value">Mar 9, 1999</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Use of avatars with automatic gesturing and bounded interaction in on-line chat session</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5884029">US5884029</a></td><td class="patent-data-table-td patent-date-value">Nov 14, 1996</td><td class="patent-data-table-td patent-date-value">Mar 16, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">User interaction with intelligent virtual objects, avatars, which interact with other avatars controlled by different users</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5903395">US5903395</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 1994</td><td class="patent-data-table-td patent-date-value">May 11, 1999</td><td class="patent-data-table-td ">I-O Display Systems Llc</td><td class="patent-data-table-td ">Personal visual display system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5903396">US5903396</a></td><td class="patent-data-table-td patent-date-value">Oct 17, 1997</td><td class="patent-data-table-td patent-date-value">May 11, 1999</td><td class="patent-data-table-td ">I/O Display Systems, Llc</td><td class="patent-data-table-td ">Intensified visual display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5907328">US5907328</a></td><td class="patent-data-table-td patent-date-value">Aug 27, 1997</td><td class="patent-data-table-td patent-date-value">May 25, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Automatic and configurable viewpoint switching in a 3D scene</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5913727">US5913727</a></td><td class="patent-data-table-td patent-date-value">Jun 13, 1997</td><td class="patent-data-table-td patent-date-value">Jun 22, 1999</td><td class="patent-data-table-td ">Ahdoot; Ned</td><td class="patent-data-table-td ">Interactive movement and contact simulation game</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5919045">US5919045</a></td><td class="patent-data-table-td patent-date-value">Nov 18, 1996</td><td class="patent-data-table-td patent-date-value">Jul 6, 1999</td><td class="patent-data-table-td ">Mariah Vision3 Entertainment Llc</td><td class="patent-data-table-td ">For receiving input signals indicative of actions of a user</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5926179">US5926179</a></td><td class="patent-data-table-td patent-date-value">Sep 29, 1997</td><td class="patent-data-table-td patent-date-value">Jul 20, 1999</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Three-dimensional virtual reality space display processing apparatus, a three-dimensional virtual reality space display processing method, and an information providing medium</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5926400">US5926400</a></td><td class="patent-data-table-td patent-date-value">Nov 21, 1996</td><td class="patent-data-table-td patent-date-value">Jul 20, 1999</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Apparatus and method for determining the intensity of a sound in a virtual world</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5947823">US5947823</a></td><td class="patent-data-table-td patent-date-value">Feb 15, 1996</td><td class="patent-data-table-td patent-date-value">Sep 7, 1999</td><td class="patent-data-table-td ">Namco Ltd.</td><td class="patent-data-table-td ">Three-dimensional game apparatus and image synthesizing method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5951015">US5951015</a></td><td class="patent-data-table-td patent-date-value">Jun 10, 1998</td><td class="patent-data-table-td patent-date-value">Sep 14, 1999</td><td class="patent-data-table-td ">Eastman Kodak Company</td><td class="patent-data-table-td ">Interactive arcade game apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5956038">US5956038</a></td><td class="patent-data-table-td patent-date-value">Jul 11, 1996</td><td class="patent-data-table-td patent-date-value">Sep 21, 1999</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Three-dimensional virtual reality space sharing method and system, an information recording medium and method, an information transmission medium and method, an information processing method, a client terminal, and a shared server terminal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5966129">US5966129</a></td><td class="patent-data-table-td patent-date-value">Oct 10, 1996</td><td class="patent-data-table-td patent-date-value">Oct 12, 1999</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">System for, and method of displaying an image of an object responsive to an operator&#39;s command</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5966130">US5966130</a></td><td class="patent-data-table-td patent-date-value">Mar 26, 1997</td><td class="patent-data-table-td patent-date-value">Oct 12, 1999</td><td class="patent-data-table-td ">Benman, Jr.; William J.</td><td class="patent-data-table-td ">Integrated virtual networks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5980256">US5980256</a></td><td class="patent-data-table-td patent-date-value">Feb 13, 1996</td><td class="patent-data-table-td patent-date-value">Nov 9, 1999</td><td class="patent-data-table-td ">Carmein; David E. E.</td><td class="patent-data-table-td ">Virtual reality system with enhanced sensory apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5982372">US5982372</a></td><td class="patent-data-table-td patent-date-value">Nov 14, 1996</td><td class="patent-data-table-td patent-date-value">Nov 9, 1999</td><td class="patent-data-table-td ">International Business Machines Corp.</td><td class="patent-data-table-td ">Visual metaphor for shortcut navigation in a virtual world</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5982390">US5982390</a></td><td class="patent-data-table-td patent-date-value">Mar 24, 1997</td><td class="patent-data-table-td patent-date-value">Nov 9, 1999</td><td class="patent-data-table-td ">Stan Stoneking</td><td class="patent-data-table-td ">Controlling personality manifestations by objects in a computer-assisted animation environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5983003">US5983003</a></td><td class="patent-data-table-td patent-date-value">Nov 15, 1996</td><td class="patent-data-table-td patent-date-value">Nov 9, 1999</td><td class="patent-data-table-td ">International Business Machines Corp.</td><td class="patent-data-table-td ">Interactive station indicator and user qualifier for virtual worlds</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5999641">US5999641</a></td><td class="patent-data-table-td patent-date-value">Nov 19, 1996</td><td class="patent-data-table-td patent-date-value">Dec 7, 1999</td><td class="patent-data-table-td ">The Duck Corporation</td><td class="patent-data-table-td ">System for manipulating digitized image objects in three dimensions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5999944">US5999944</a></td><td class="patent-data-table-td patent-date-value">Feb 27, 1998</td><td class="patent-data-table-td patent-date-value">Dec 7, 1999</td><td class="patent-data-table-td ">Oracle Corporation</td><td class="patent-data-table-td ">Method and apparatus for implementing dynamic VRML</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6001065">US6001065</a></td><td class="patent-data-table-td patent-date-value">Mar 21, 1996</td><td class="patent-data-table-td patent-date-value">Dec 14, 1999</td><td class="patent-data-table-td ">Ibva Technologies, Inc.</td><td class="patent-data-table-td ">Method and apparatus for measuring and analyzing physiological signals for active or passive control of physical and virtual spaces and the contents therein</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6009458">US6009458</a></td><td class="patent-data-table-td patent-date-value">May 9, 1996</td><td class="patent-data-table-td patent-date-value">Dec 28, 1999</td><td class="patent-data-table-td ">3Do Company</td><td class="patent-data-table-td ">Networked computer game system with persistent playing objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6009460">US6009460</a></td><td class="patent-data-table-td patent-date-value">Dec 19, 1996</td><td class="patent-data-table-td patent-date-value">Dec 28, 1999</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Virtual reality space sharing system having self modifying avatars (symbols) in accordance to a category that the symbol belongs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6011558">US6011558</a></td><td class="patent-data-table-td patent-date-value">Sep 23, 1997</td><td class="patent-data-table-td patent-date-value">Jan 4, 2000</td><td class="patent-data-table-td ">Industrial Technology Research Institute</td><td class="patent-data-table-td ">Intelligent stitcher for panoramic image-based virtual worlds</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6014136">US6014136</a></td><td class="patent-data-table-td patent-date-value">Mar 4, 1997</td><td class="patent-data-table-td patent-date-value">Jan 11, 2000</td><td class="patent-data-table-td ">Casio Computer Co., Ltd.</td><td class="patent-data-table-td ">Data processing apparatus with communication function</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6014142">US6014142</a></td><td class="patent-data-table-td patent-date-value">Nov 24, 1998</td><td class="patent-data-table-td patent-date-value">Jan 11, 2000</td><td class="patent-data-table-td ">Platinum Technology Ip, Inc.</td><td class="patent-data-table-td ">Apparatus and method for three dimensional manipulation of point of view and object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6020885">US6020885</a></td><td class="patent-data-table-td patent-date-value">Jul 9, 1996</td><td class="patent-data-table-td patent-date-value">Feb 1, 2000</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Three-dimensional virtual reality space sharing method and system using local and global object identification codes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6020891">US6020891</a></td><td class="patent-data-table-td patent-date-value">Jul 29, 1997</td><td class="patent-data-table-td patent-date-value">Feb 1, 2000</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Apparatus for displaying three-dimensional virtual object and method of displaying the same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6023270">US6023270</a></td><td class="patent-data-table-td patent-date-value">Nov 17, 1997</td><td class="patent-data-table-td patent-date-value">Feb 8, 2000</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Delivery of objects in a virtual world using a descriptive container</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6024643">US6024643</a></td><td class="patent-data-table-td patent-date-value">Mar 4, 1997</td><td class="patent-data-table-td patent-date-value">Feb 15, 2000</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Player profile based proxy play</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6028584">US6028584</a></td><td class="patent-data-table-td patent-date-value">Aug 29, 1997</td><td class="patent-data-table-td patent-date-value">Feb 22, 2000</td><td class="patent-data-table-td ">Industrial Technology Research Institute</td><td class="patent-data-table-td ">Real-time player for panoramic imaged-based virtual worlds</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6031549">US6031549</a></td><td class="patent-data-table-td patent-date-value">Jun 11, 1997</td><td class="patent-data-table-td patent-date-value">Feb 29, 2000</td><td class="patent-data-table-td ">Extempo Systems, Inc.</td><td class="patent-data-table-td ">System and method for directed improvisation by computer controlled characters</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6034692">US6034692</a></td><td class="patent-data-table-td patent-date-value">Aug 1, 1997</td><td class="patent-data-table-td patent-date-value">Mar 7, 2000</td><td class="patent-data-table-td ">U.S. Philips Corporation</td><td class="patent-data-table-td ">Virtual environment navigation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6040841">US6040841</a></td><td class="patent-data-table-td patent-date-value">Aug 2, 1996</td><td class="patent-data-table-td patent-date-value">Mar 21, 2000</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for virtual cinematography</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6040842">US6040842</a></td><td class="patent-data-table-td patent-date-value">Apr 29, 1997</td><td class="patent-data-table-td patent-date-value">Mar 21, 2000</td><td class="patent-data-table-td ">U.S. Philips Corporation</td><td class="patent-data-table-td ">Process control with evaluation of stored referential expressions in a multi-agent system adapted for use with virtual actors which are directed by sequentially enabled script agents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6052114">US6052114</a></td><td class="patent-data-table-td patent-date-value">Jun 23, 1998</td><td class="patent-data-table-td patent-date-value">Apr 18, 2000</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Virtual reality body-sensing apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6054991">US6054991</a></td><td class="patent-data-table-td patent-date-value">Jul 29, 1994</td><td class="patent-data-table-td patent-date-value">Apr 25, 2000</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Method of modeling player position and movement in a virtual reality system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6055563">US6055563</a></td><td class="patent-data-table-td patent-date-value">Aug 12, 1997</td><td class="patent-data-table-td patent-date-value">Apr 25, 2000</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Transfer and display of virtual-world data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6057810">US6057810</a></td><td class="patent-data-table-td patent-date-value">Jun 20, 1996</td><td class="patent-data-table-td patent-date-value">May 2, 2000</td><td class="patent-data-table-td ">Immersive Technologies, Inc.</td><td class="patent-data-table-td ">Method and apparatus for orientation sensing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6057828">US6057828</a></td><td class="patent-data-table-td patent-date-value">Jan 16, 1997</td><td class="patent-data-table-td patent-date-value">May 2, 2000</td><td class="patent-data-table-td ">Immersion Corporation</td><td class="patent-data-table-td ">Method and apparatus for providing force sensations in virtual environments in accordance with host software</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6057856">US6057856</a></td><td class="patent-data-table-td patent-date-value">Sep 16, 1997</td><td class="patent-data-table-td patent-date-value">May 2, 2000</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">3D virtual reality multi-user interaction with superimposed positional information display for each user</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6072466">US6072466</a></td><td class="patent-data-table-td patent-date-value">Aug 1, 1997</td><td class="patent-data-table-td patent-date-value">Jun 6, 2000</td><td class="patent-data-table-td ">U.S. Philips Corporation</td><td class="patent-data-table-td ">Virtual environment manipulation device modelling and control</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6072478">US6072478</a></td><td class="patent-data-table-td patent-date-value">Feb 16, 1996</td><td class="patent-data-table-td patent-date-value">Jun 6, 2000</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">System for and method for producing and displaying images which are viewed from various viewpoints in local spaces</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6073115">US6073115</a></td><td class="patent-data-table-td patent-date-value">Oct 7, 1997</td><td class="patent-data-table-td patent-date-value">Jun 6, 2000</td><td class="patent-data-table-td ">Marshall; Paul Steven</td><td class="patent-data-table-td ">Virtual reality generator for displaying abstract information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6434604">US6434604</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 1999</td><td class="patent-data-table-td patent-date-value">Aug 13, 2002</td><td class="patent-data-table-td ">Network Community Creation, Inc.</td><td class="patent-data-table-td ">Chat system allows user to select balloon form and background color for displaying chat statement data</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Brewer, Stephen et al., "<a href='http://scholar.google.com/scholar?q="A+New+World+of+Animated+Discussions%3A+3D+Chat-See+why+online+chats%27+new+visual+trappings+are+the+talk+of+the+town%2C"'>A New World of Animated Discussions: 3D Chat-See why online chats' new visual trappings are the talk of the town,</a>" Home PC, CMP Media, 1997, No. 403, p. 88.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">D'Amico, Marie, "<a href='http://scholar.google.com/scholar?q="Who+wnats+to+live+in+cyberspace%3F+Virtual+world+developers+bet+we+all+do+%28virtual+worlds+are+not+useful+nor+valuable%29%2C"'>Who wnats to live in cyberspace? Virtual world developers bet we all do (virtual worlds are not useful nor valuable),</a>" Digital Media, Seybold Publications, Inc., vol. 5, No. 2, p. 9 (4), Jul. 3, 1995.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Feiner, Steven K., "<a href='http://scholar.google.com/scholar?q="Research+in+3D+user+interface+design+at+Columbia+University%2C"'>Research in 3D user interface design at Columbia University,</a>" Feiner@cs,columbia,edu.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Gabriel Knight 3 (Sierra Online's adventure game) (Software Review) (Evaluation), Computer Gaming World, Golden Empire Publications, Inc., p. 62 (1), Jun. 1999.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Gordon Brockhouse, "<a href='http://scholar.google.com/scholar?q="3D+Explodes%21-New+3D+video+accelerators+and+sound+cards+can+add+mind-blowing+realism+to+your+games%2C"'>3D Explodes!-New 3D video accelerators and sound cards can add mind-blowing realism to your games,</a>" Home PC, CMP Media, No. 503, p. 64, 1998.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Gordon Brockhouse, "<a href='http://scholar.google.com/scholar?q="3D+Explodes%21%E2%80%94New+3D+video+accelerators+and+sound+cards+can+add+mind-blowing+realism+to+your+games%2C"'>3D Explodes!—New 3D video accelerators and sound cards can add mind-blowing realism to your games,</a>" Home PC, CMP Media, No. 503, p. 64, 1998.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Greenhalgh, C., et al., "<a href='http://scholar.google.com/scholar?q="Creating+a+live+broadcast+from+a+virtual+environment%2C"'>Creating a live broadcast from a virtual environment,</a>" Proceedings of SIGGRAPH 99: 26&lt;th &gt;Internatioanl Conference on Computer Graphics and Interactive Techniques, IEEE, Aug. 8-13, 1999.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Greenhalgh, C., et al., "<a href='http://scholar.google.com/scholar?q="Creating+a+live+broadcast+from+a+virtual+environment%2C"'>Creating a live broadcast from a virtual environment,</a>" Proceedings of SIGGRAPH 99: 26th Internatioanl Conference on Computer Graphics and Interactive Techniques, IEEE, Aug. 8-13, 1999.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Heid, Jim, "<a href='http://scholar.google.com/scholar?q="Face-to-face+online%2C"'>Face-to-face online,</a>" Macworld, Macworld Communications Inc., vol. 14, No. 1, p. 146, Jan. 1997.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Iworld-ZDNet Offers Virtual Reality on the Web, Newsbytes Inc., Nov. 3, 1995.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">James Jung-Hoon Seo, "<a href='http://scholar.google.com/scholar?q="Active+Worlds"'>Active Worlds</a>", Mar. 13, 2000, &lt;http://acg.media.mit.edu/people/jseo/courses/mas963/assign5-activeworlds.html&gt; (visited Mar. 11, 2003).</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Joch, Alan, "<a href='http://scholar.google.com/scholar?q="Enterprises+now+donning+3-D+glasses%3A+3-D+interfaces+move+to+handle+corporate+e-biz+%28Technology+Information%29%2C"'>Enterprises now donning 3-D glasses: 3-D interfaces move to handle corporate e-biz (Technology Information),</a>" PC Week, Ziff-Davis Publishing Company, p. 59, Nov. 8, 1999.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kaplan, F., et al., "<a href='http://scholar.google.com/scholar?q="Growing+virtual+communities+in+3D+meeting+spaces%2C"'>Growing virtual communities in 3D meeting spaces,</a>" Virtual Worlds First International Conference, VW '98 Proceedings, IEEE, Jul. 1-3, 1998, (Abstract).</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Krista Ostertag, "<a href='http://scholar.google.com/scholar?q="3D+tools+for+virtual+worlds%2C"'>3D tools for virtual worlds,</a>" Varbusiness, CMP Media, No. 1203, p. 24, 1996.</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kyoya, T., et al., "<a href='http://scholar.google.com/scholar?q="A+3D+game+with+force+feedback+in+a+virtual+space-virtual+Tower+of+Hanoi%2C"'>A 3D game with force feedback in a virtual space-virtual Tower of Hanoi,</a>" Journal of the Institute of Television Engineers of Japan, IEEE, vol. 49, No. 10, pp. 1347-1352, Oct. 1995, (Abstract).</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Leibs, B., et al., "<a href='http://scholar.google.com/scholar?q="The+Perceptive+Workbench%3A+Toward+spontaneous+and+natural+interaction+in+semi-immersive+virtual+environments%2C"'>The Perceptive Workbench: Toward spontaneous and natural interaction in semi-immersive virtual environments,</a>" Proceedings IEEE Virtual Reality 2000, IEEE, pp. 13-20, Mar. 18-22, 2000.</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mohageg, Mike, et al., "<a href='http://scholar.google.com/scholar?q="A+user+interface+for+accessing+3D+content+on+the+World+Wide+Web."'>A user interface for accessing 3D content on the World Wide Web.</a>"</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">New Dimension for 3D chat, Home PC, CMP Media, No. 405, p. 27, 1997.</td></tr><tr><td class="patent-data-table-td ">19</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Patent application No. 09/653,276; Specification and Figures 1-14 from Method, System and Computer Program Product for Providing a Two-Dimensional Interface to a Three-Dimensional World, Harvey, William et al.</td></tr><tr><td class="patent-data-table-td ">20</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">The Client side of Virtual Worlds (Immersive Systems, Knowledge Adventure Worlds, Apple's QuickTime VR desktop video software, Virtual Games' Stim-Slum computer game), Release 1.0, Edventure Holdings Inc., vol. 94, No. 6, p. 12 (7), Jun. 27, 1994.</td></tr><tr><td class="patent-data-table-td ">21</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Vacca, John R., "<a href='http://scholar.google.com/scholar?q="3D+worlds+on+the+Web%2C"'>3D worlds on the Web,</a>" Computer Graphics World, Pennwell Publishing Company, vol. 19, No. 5, p. 43 (6), May 1996.</td></tr><tr><td class="patent-data-table-td ">22</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Vallina, Joe, "<a href='http://scholar.google.com/scholar?q="This+%223D%22+Hunting+Game+Takes+1+Step+Forward%2C+10+Steps+Back%2C"'>This "3D" Hunting Game Takes 1 Step Forward, 10 Steps Back,</a>" Computer Gaming World, Golden Empire Publications, Inc., No. 172, p. 299 (1), Nov. 1998.</td></tr><tr><td class="patent-data-table-td ">23</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Virtools to Launch 3D Games Development Tool, Computergram International, Computerwire, Inc., No. 126, Jun. 25, 1998.</td></tr><tr><td class="patent-data-table-td ">24</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Visual Interface, Inc., "<a href='http://scholar.google.com/scholar?q="The+Future+of+3D+Imaging."'>The Future of 3D Imaging.</a>"</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6933204">US6933204</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 13, 2003</td><td class="patent-data-table-td patent-date-value">Aug 23, 2005</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method for improved alignment of magnetic tunnel junction elements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6961755">US6961755</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 25, 2001</td><td class="patent-data-table-td patent-date-value">Nov 1, 2005</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Information processing apparatus and method, and storage medium</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6963839">US6963839</a></td><td class="patent-data-table-td patent-date-value">Nov 2, 2001</td><td class="patent-data-table-td patent-date-value">Nov 8, 2005</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">System and method of controlling sound in a multi-media communication application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6976082">US6976082</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 2, 2001</td><td class="patent-data-table-td patent-date-value">Dec 13, 2005</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">System and method for receiving multi-media messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6990452">US6990452</a></td><td class="patent-data-table-td patent-date-value">Nov 2, 2001</td><td class="patent-data-table-td patent-date-value">Jan 24, 2006</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Method for sending multi-media messages using emoticons</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7035803">US7035803</a></td><td class="patent-data-table-td patent-date-value">Nov 2, 2001</td><td class="patent-data-table-td patent-date-value">Apr 25, 2006</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Method for sending multi-media messages using customizable background images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7039677">US7039677</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 7, 2002</td><td class="patent-data-table-td patent-date-value">May 2, 2006</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Threaded text-based chat collaboration</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7043530">US7043530</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 30, 2001</td><td class="patent-data-table-td patent-date-value">May 9, 2006</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">System, method and apparatus for communicating via instant messaging</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7091976">US7091976</a></td><td class="patent-data-table-td patent-date-value">Nov 2, 2001</td><td class="patent-data-table-td patent-date-value">Aug 15, 2006</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">System and method of customizing animated entities for use in a multi-media communication application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7124372">US7124372</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 12, 2002</td><td class="patent-data-table-td patent-date-value">Oct 17, 2006</td><td class="patent-data-table-td ">Glen David Brin</td><td class="patent-data-table-td ">Interactive communication between a plurality of users</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7127685">US7127685</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 2002</td><td class="patent-data-table-td patent-date-value">Oct 24, 2006</td><td class="patent-data-table-td ">America Online, Inc.</td><td class="patent-data-table-td ">Instant messaging interface having a tear-off element</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7146343">US7146343</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 11, 2002</td><td class="patent-data-table-td patent-date-value">Dec 5, 2006</td><td class="patent-data-table-td ">J. J. Donahue &amp; Company</td><td class="patent-data-table-td ">Method and apparatus for negotiating a contract over a computer network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7177811">US7177811</a></td><td class="patent-data-table-td patent-date-value">Mar 6, 2006</td><td class="patent-data-table-td patent-date-value">Feb 13, 2007</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Method for sending multi-media messages using customizable background images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7180527">US7180527</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 19, 2003</td><td class="patent-data-table-td patent-date-value">Feb 20, 2007</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Text display terminal device and server</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7185057">US7185057</a></td><td class="patent-data-table-td patent-date-value">Jul 26, 2001</td><td class="patent-data-table-td patent-date-value">Feb 27, 2007</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Individually specifying message output attributes in a messaging system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7203648">US7203648</a></td><td class="patent-data-table-td patent-date-value">Nov 2, 2001</td><td class="patent-data-table-td patent-date-value">Apr 10, 2007</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Method for sending multi-media messages with customized audio</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7203759">US7203759</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 27, 2005</td><td class="patent-data-table-td patent-date-value">Apr 10, 2007</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">System and method for receiving multi-media messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7213206">US7213206</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 9, 2003</td><td class="patent-data-table-td patent-date-value">May 1, 2007</td><td class="patent-data-table-td ">Fogg Brian J</td><td class="patent-data-table-td ">Relationship user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7246151">US7246151</a></td><td class="patent-data-table-td patent-date-value">May 21, 2004</td><td class="patent-data-table-td patent-date-value">Jul 17, 2007</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">System, method and apparatus for communicating via sound messages and personal sound identifiers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7251695">US7251695</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 13, 2001</td><td class="patent-data-table-td patent-date-value">Jul 31, 2007</td><td class="patent-data-table-td ">Aspen Technology, Inc.</td><td class="patent-data-table-td ">Computer network communication method and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7257617">US7257617</a></td><td class="patent-data-table-td patent-date-value">Jul 26, 2001</td><td class="patent-data-table-td patent-date-value">Aug 14, 2007</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Notifying users when messaging sessions are recorded</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7269622">US7269622</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 26, 2001</td><td class="patent-data-table-td patent-date-value">Sep 11, 2007</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Watermarking messaging sessions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7281215">US7281215</a></td><td class="patent-data-table-td patent-date-value">Jul 31, 2002</td><td class="patent-data-table-td patent-date-value">Oct 9, 2007</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">IM conversation counter and indicator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7284207">US7284207</a></td><td class="patent-data-table-td patent-date-value">Sep 6, 2006</td><td class="patent-data-table-td patent-date-value">Oct 16, 2007</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">Instant messaging interface having a tear-off element</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7342587">US7342587</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 2005</td><td class="patent-data-table-td patent-date-value">Mar 11, 2008</td><td class="patent-data-table-td ">Imvu, Inc.</td><td class="patent-data-table-td ">Computer-implemented system and method for home page customization and e-commerce support</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7343561">US7343561</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 19, 2003</td><td class="patent-data-table-td patent-date-value">Mar 11, 2008</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Method and apparatus for message display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7370277">US7370277</a></td><td class="patent-data-table-td patent-date-value">Dec 23, 2002</td><td class="patent-data-table-td patent-date-value">May 6, 2008</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">E-mail interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7401295">US7401295</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 15, 2003</td><td class="patent-data-table-td patent-date-value">Jul 15, 2008</td><td class="patent-data-table-td ">Simulearn, Inc.</td><td class="patent-data-table-td ">Computer-based learning system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7421661">US7421661</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 2002</td><td class="patent-data-table-td patent-date-value">Sep 2, 2008</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">Instant messaging interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7503006">US7503006</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 25, 2003</td><td class="patent-data-table-td patent-date-value">Mar 10, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Visual indication of current voice speaker</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7546536">US7546536</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 26, 2003</td><td class="patent-data-table-td patent-date-value">Jun 9, 2009</td><td class="patent-data-table-td ">Konami Digital Entertainment Co., Ltd.</td><td class="patent-data-table-td ">Communication device, communication method, and computer usable medium</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7565434">US7565434</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 9, 2003</td><td class="patent-data-table-td patent-date-value">Jul 21, 2009</td><td class="patent-data-table-td ">Sprint Spectrum L.P.</td><td class="patent-data-table-td ">Method and system for canceling setup of a packet-based real-time media conference session</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7568014">US7568014</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 17, 2008</td><td class="patent-data-table-td patent-date-value">Jul 28, 2009</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Individually specifying message output attributes in a messaging system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7571213">US7571213</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 26, 2004</td><td class="patent-data-table-td patent-date-value">Aug 4, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Interactive electronic bubble messaging</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7590941">US7590941</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 9, 2003</td><td class="patent-data-table-td patent-date-value">Sep 15, 2009</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">Communication and collaboration system using rich media environments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7647560">US7647560</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 9, 2004</td><td class="patent-data-table-td patent-date-value">Jan 12, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">User interface for multi-sensory emoticons in a communication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7671861">US7671861</a></td><td class="patent-data-table-td patent-date-value">Nov 2, 2001</td><td class="patent-data-table-td patent-date-value">Mar 2, 2010</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">Apparatus and method of customizing animated entities for use in a multi-media communication application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7685237">US7685237</a></td><td class="patent-data-table-td patent-date-value">Oct 19, 2005</td><td class="patent-data-table-td patent-date-value">Mar 23, 2010</td><td class="patent-data-table-td ">Aol Inc.</td><td class="patent-data-table-td ">Multiple personalities in chat communications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7689649">US7689649</a></td><td class="patent-data-table-td patent-date-value">Dec 31, 2002</td><td class="patent-data-table-td patent-date-value">Mar 30, 2010</td><td class="patent-data-table-td ">Aol Inc.</td><td class="patent-data-table-td ">Rendering destination instant messaging personalization items before communicating with destination</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7692658">US7692658</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 17, 2006</td><td class="patent-data-table-td patent-date-value">Apr 6, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Model for layout animations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7725547">US7725547</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 6, 2006</td><td class="patent-data-table-td patent-date-value">May 25, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Informing a user of gestures made by others out of the user&#39;s line of sight</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7769144">US7769144</a></td><td class="patent-data-table-td patent-date-value">Jul 21, 2006</td><td class="patent-data-table-td patent-date-value">Aug 3, 2010</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Method and system for generating and presenting conversation threads having email, voicemail and chat messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7792328">US7792328</a></td><td class="patent-data-table-td patent-date-value">Jan 12, 2007</td><td class="patent-data-table-td patent-date-value">Sep 7, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Warning a vehicle operator of unsafe operation behavior based on a 3D captured image stream</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7801332">US7801332</a></td><td class="patent-data-table-td patent-date-value">Jan 12, 2007</td><td class="patent-data-table-td patent-date-value">Sep 21, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Controlling a system based on user behavioral signals detected from a 3D captured image stream</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7805487">US7805487</a></td><td class="patent-data-table-td patent-date-value">Feb 15, 2006</td><td class="patent-data-table-td patent-date-value">Sep 28, 2010</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">System, method and apparatus for communicating via instant messaging</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7840031">US7840031</a></td><td class="patent-data-table-td patent-date-value">Jan 12, 2007</td><td class="patent-data-table-td patent-date-value">Nov 23, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Tracking a range of body movement based on 3D captured image streams of a user</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7853863">US7853863</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 7, 2002</td><td class="patent-data-table-td patent-date-value">Dec 14, 2010</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Method for expressing emotion in a text message</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7877697">US7877697</a></td><td class="patent-data-table-td patent-date-value">Oct 5, 2007</td><td class="patent-data-table-td patent-date-value">Jan 25, 2011</td><td class="patent-data-table-td ">Aol Inc.</td><td class="patent-data-table-td ">IM conversation counter and indicator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7877706">US7877706</a></td><td class="patent-data-table-td patent-date-value">Jan 12, 2007</td><td class="patent-data-table-td patent-date-value">Jan 25, 2011</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Controlling a document based on user behavioral signals detected from a 3D captured image stream</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7900148">US7900148</a></td><td class="patent-data-table-td patent-date-value">May 5, 2008</td><td class="patent-data-table-td patent-date-value">Mar 1, 2011</td><td class="patent-data-table-td ">Aol Inc.</td><td class="patent-data-table-td ">E-mail interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7908324">US7908324</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 1, 2003</td><td class="patent-data-table-td patent-date-value">Mar 15, 2011</td><td class="patent-data-table-td ">Disney Enterprises, Inc.</td><td class="patent-data-table-td ">Multi-user interactive communication network environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7912793">US7912793</a></td><td class="patent-data-table-td patent-date-value">Jan 13, 2005</td><td class="patent-data-table-td patent-date-value">Mar 22, 2011</td><td class="patent-data-table-td ">Imvu, Inc.</td><td class="patent-data-table-td ">Computer-implemented method and apparatus to allocate revenue from a derived avatar component</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7921013">US7921013</a></td><td class="patent-data-table-td patent-date-value">Aug 30, 2005</td><td class="patent-data-table-td patent-date-value">Apr 5, 2011</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">System and method for sending multi-media messages using emoticons</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7930251">US7930251</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 9, 2007</td><td class="patent-data-table-td patent-date-value">Apr 19, 2011</td><td class="patent-data-table-td ">Sap Ag</td><td class="patent-data-table-td ">Model driven state management of applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7953800">US7953800</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 8, 2004</td><td class="patent-data-table-td patent-date-value">May 31, 2011</td><td class="patent-data-table-td ">Netsuite, Inc.</td><td class="patent-data-table-td ">Integrating a web-based business application with existing client-side electronic mail systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7970840">US7970840</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 2, 2008</td><td class="patent-data-table-td patent-date-value">Jun 28, 2011</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method to continue instant messaging exchange when exiting a virtual world</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7970901">US7970901</a></td><td class="patent-data-table-td patent-date-value">Jun 4, 2009</td><td class="patent-data-table-td patent-date-value">Jun 28, 2011</td><td class="patent-data-table-td ">Netsuite, Inc.</td><td class="patent-data-table-td ">Phased rollout of version upgrades in web-based business information systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7971156">US7971156</a></td><td class="patent-data-table-td patent-date-value">Jan 12, 2007</td><td class="patent-data-table-td patent-date-value">Jun 28, 2011</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Controlling resource access based on user gesturing in a 3D captured image stream of the user</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7979489">US7979489</a></td><td class="patent-data-table-td patent-date-value">Apr 28, 2007</td><td class="patent-data-table-td patent-date-value">Jul 12, 2011</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Notifying users when messaging sessions are recorded</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7979574">US7979574</a></td><td class="patent-data-table-td patent-date-value">Mar 5, 2007</td><td class="patent-data-table-td patent-date-value">Jul 12, 2011</td><td class="patent-data-table-td ">Sony Computer Entertainment America Llc</td><td class="patent-data-table-td ">System and method for routing communications among real and virtual communication devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7990387">US7990387</a></td><td class="patent-data-table-td patent-date-value">Aug 16, 2007</td><td class="patent-data-table-td patent-date-value">Aug 2, 2011</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and apparatus for spawning projected avatars in a virtual universe</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7995064">US7995064</a></td><td class="patent-data-table-td patent-date-value">Aug 26, 2005</td><td class="patent-data-table-td patent-date-value">Aug 9, 2011</td><td class="patent-data-table-td ">Imvu, Inc.</td><td class="patent-data-table-td ">Computer-implemented chat system having dual channel communications and self-defining product structures</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8010903">US8010903</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 28, 2001</td><td class="patent-data-table-td patent-date-value">Aug 30, 2011</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and method for visualizing and navigating dynamic content in a graphical user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8049756">US8049756</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 30, 2007</td><td class="patent-data-table-td patent-date-value">Nov 1, 2011</td><td class="patent-data-table-td ">Brian Mark Shuster</td><td class="patent-data-table-td ">Time-dependent client inactivity indicia in a multi-user animation environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8117551">US8117551</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 18, 2007</td><td class="patent-data-table-td patent-date-value">Feb 14, 2012</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Computer system and method of using presence visualizations of avatars as persistable virtual contact objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8121263">US8121263</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 21, 2006</td><td class="patent-data-table-td patent-date-value">Feb 21, 2012</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Method and system for integrating voicemail and electronic messaging</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8132102">US8132102</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 28, 2008</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">Motorola Mobility, Inc.</td><td class="patent-data-table-td ">Messaging interface systems and methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8140982">US8140982</a></td><td class="patent-data-table-td patent-date-value">Nov 8, 2007</td><td class="patent-data-table-td patent-date-value">Mar 20, 2012</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and system for splitting virtual universes into distinct entities</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8219388">US8219388</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 7, 2006</td><td class="patent-data-table-td patent-date-value">Jul 10, 2012</td><td class="patent-data-table-td ">Konami Digital Entertainment Co., Ltd.</td><td class="patent-data-table-td ">User voice mixing device, virtual space sharing system, computer control method, and information storage medium</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8230033">US8230033</a></td><td class="patent-data-table-td patent-date-value">Apr 14, 2011</td><td class="patent-data-table-td patent-date-value">Jul 24, 2012</td><td class="patent-data-table-td ">Netsuite, Inc.</td><td class="patent-data-table-td ">Message tracking functionality based on thread-recurrent data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8234582">US8234582</a></td><td class="patent-data-table-td patent-date-value">Feb 3, 2009</td><td class="patent-data-table-td patent-date-value">Jul 31, 2012</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">Visualizing object behavior</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8250473">US8250473</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 3, 2009</td><td class="patent-data-table-td patent-date-value">Aug 21, 2012</td><td class="patent-data-table-td ">Amazon Technoloies, Inc.</td><td class="patent-data-table-td ">Visualizing object behavior</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8266517">US8266517</a></td><td class="patent-data-table-td patent-date-value">Jun 2, 2008</td><td class="patent-data-table-td patent-date-value">Sep 11, 2012</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Creation of an interface for constructing conversational policies</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8269834">US8269834</a></td><td class="patent-data-table-td patent-date-value">Jan 12, 2007</td><td class="patent-data-table-td patent-date-value">Sep 18, 2012</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Warning a user about adverse behaviors of others within an environment based on a 3D captured image stream</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8290881">US8290881</a></td><td class="patent-data-table-td patent-date-value">Mar 17, 2011</td><td class="patent-data-table-td patent-date-value">Oct 16, 2012</td><td class="patent-data-table-td ">Imvu, Inc.</td><td class="patent-data-table-td ">Computer-implemented method and apparatus to allocate revenue from a derived digital component</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8295542">US8295542</a></td><td class="patent-data-table-td patent-date-value">Jan 12, 2007</td><td class="patent-data-table-td patent-date-value">Oct 23, 2012</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Adjusting a consumer experience based on a 3D captured image stream of a consumer response</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8334871">US8334871</a></td><td class="patent-data-table-td patent-date-value">Jun 6, 2011</td><td class="patent-data-table-td patent-date-value">Dec 18, 2012</td><td class="patent-data-table-td ">International Business Machine Corporation</td><td class="patent-data-table-td ">Spawning projected avatars in a virtual universe</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8341540">US8341540</a></td><td class="patent-data-table-td patent-date-value">Feb 3, 2009</td><td class="patent-data-table-td patent-date-value">Dec 25, 2012</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">Visualizing object behavior</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8345049">US8345049</a></td><td class="patent-data-table-td patent-date-value">Aug 16, 2007</td><td class="patent-data-table-td patent-date-value">Jan 1, 2013</td><td class="patent-data-table-td ">International Business Machine Corporation</td><td class="patent-data-table-td ">Method and apparatus for predicting avatar movement in a virtual universe</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8397168">US8397168</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 15, 2009</td><td class="patent-data-table-td patent-date-value">Mar 12, 2013</td><td class="patent-data-table-td ">Social Communications Company</td><td class="patent-data-table-td ">Interfacing with a spatial virtual communication environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8407603">US8407603</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 2008</td><td class="patent-data-table-td patent-date-value">Mar 26, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Portable electronic device for instant messaging multiple recipients</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8425322">US8425322</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 5, 2007</td><td class="patent-data-table-td patent-date-value">Apr 23, 2013</td><td class="patent-data-table-td ">Sony Computer Entertainment America Inc.</td><td class="patent-data-table-td ">System and method for communicating with a virtual world</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8429543">US8429543</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 2011</td><td class="patent-data-table-td patent-date-value">Apr 23, 2013</td><td class="patent-data-table-td ">Facebook, Inc.</td><td class="patent-data-table-td ">E-mail interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8458278">US8458278</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 20, 2007</td><td class="patent-data-table-td patent-date-value">Jun 4, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Method and apparatus for displaying information during an instant messaging session</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8484346">US8484346</a></td><td class="patent-data-table-td patent-date-value">Jun 7, 2011</td><td class="patent-data-table-td patent-date-value">Jul 9, 2013</td><td class="patent-data-table-td ">NetSuite Inc.</td><td class="patent-data-table-td ">Simultaneous maintenance of multiple versions of a web-based business information system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8502825">US8502825</a></td><td class="patent-data-table-td patent-date-value">Mar 5, 2007</td><td class="patent-data-table-td patent-date-value">Aug 6, 2013</td><td class="patent-data-table-td ">Sony Computer Entertainment Europe Limited</td><td class="patent-data-table-td ">Avatar email and methods for communicating between real and virtual worlds</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8504926">US8504926</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 16, 2008</td><td class="patent-data-table-td patent-date-value">Aug 6, 2013</td><td class="patent-data-table-td ">Lupus Labs Ug</td><td class="patent-data-table-td ">Model based avatars for virtual presence</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8520809">US8520809</a></td><td class="patent-data-table-td patent-date-value">Jan 13, 2012</td><td class="patent-data-table-td patent-date-value">Aug 27, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Method and system for integrating voicemail and electronic messaging</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8522160">US8522160</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 18, 2010</td><td class="patent-data-table-td patent-date-value">Aug 27, 2013</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Information processing device, contents processing method and program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8539364">US8539364</a></td><td class="patent-data-table-td patent-date-value">Mar 12, 2008</td><td class="patent-data-table-td patent-date-value">Sep 17, 2013</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Attaching external virtual universes to an existing virtual universe</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8554861">US8554861</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 17, 2010</td><td class="patent-data-table-td patent-date-value">Oct 8, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Method and apparatus for displaying information during an instant messaging session</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8564621">US8564621</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 11, 2010</td><td class="patent-data-table-td patent-date-value">Oct 22, 2013</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Replicating changes between corresponding objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8570358">US8570358</a></td><td class="patent-data-table-td patent-date-value">Mar 16, 2010</td><td class="patent-data-table-td patent-date-value">Oct 29, 2013</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Automated wireless three-dimensional (3D) video conferencing via a tunerless television device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8572177">US8572177</a></td><td class="patent-data-table-td patent-date-value">Dec 15, 2010</td><td class="patent-data-table-td patent-date-value">Oct 29, 2013</td><td class="patent-data-table-td ">Xmobb, Inc.</td><td class="patent-data-table-td ">3D social platform for sharing videos and webpages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8577087">US8577087</a></td><td class="patent-data-table-td patent-date-value">Jul 6, 2012</td><td class="patent-data-table-td patent-date-value">Nov 5, 2013</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Adjusting a consumer experience based on a 3D captured image stream of a consumer response</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8577980">US8577980</a></td><td class="patent-data-table-td patent-date-value">May 23, 2012</td><td class="patent-data-table-td patent-date-value">Nov 5, 2013</td><td class="patent-data-table-td ">NetSuite Inc.</td><td class="patent-data-table-td ">Message tracking with thread-recurrent data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8588464">US8588464</a></td><td class="patent-data-table-td patent-date-value">Jan 12, 2007</td><td class="patent-data-table-td patent-date-value">Nov 19, 2013</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Assisting a vision-impaired user with navigation based on a 3D captured image stream</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8589809">US8589809</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 26, 2008</td><td class="patent-data-table-td patent-date-value">Nov 19, 2013</td><td class="patent-data-table-td ">Chevron U.S.A. Inc.</td><td class="patent-data-table-td ">Methods and systems for conducting a meeting in a virtual environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8597123">US8597123</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 19, 2003</td><td class="patent-data-table-td patent-date-value">Dec 3, 2013</td><td class="patent-data-table-td ">Kabushiki Kaisha Square Enix</td><td class="patent-data-table-td ">Method for displaying chat window applied to network game</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8612868">US8612868</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 26, 2008</td><td class="patent-data-table-td patent-date-value">Dec 17, 2013</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Computer method and apparatus for persisting pieces of a virtual world group conversation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8634857">US8634857</a></td><td class="patent-data-table-td patent-date-value">Oct 17, 2011</td><td class="patent-data-table-td patent-date-value">Jan 21, 2014</td><td class="patent-data-table-td ">At&amp;T Mobility Ii Llc</td><td class="patent-data-table-td ">Apparatus and systems for providing location-based services within a wireless network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8650134">US8650134</a></td><td class="patent-data-table-td patent-date-value">Sep 11, 2012</td><td class="patent-data-table-td patent-date-value">Feb 11, 2014</td><td class="patent-data-table-td ">Imvu, Inc.</td><td class="patent-data-table-td ">Computer-implemented hierarchical revenue model to manage revenue allocations among derived product developers in a networked system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8667402">US8667402</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 7, 2011</td><td class="patent-data-table-td patent-date-value">Mar 4, 2014</td><td class="patent-data-table-td ">Onset Vi, L.P.</td><td class="patent-data-table-td ">Visualizing communications within a social setting</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8671349">US8671349</a></td><td class="patent-data-table-td patent-date-value">May 15, 2008</td><td class="patent-data-table-td patent-date-value">Mar 11, 2014</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Virtual universe teleportation suggestion service</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8687046">US8687046</a></td><td class="patent-data-table-td patent-date-value">Mar 16, 2010</td><td class="patent-data-table-td patent-date-value">Apr 1, 2014</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Three-dimensional (3D) video for two-dimensional (2D) video messenger applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8692835">US8692835</a></td><td class="patent-data-table-td patent-date-value">Sep 14, 2012</td><td class="patent-data-table-td patent-date-value">Apr 8, 2014</td><td class="patent-data-table-td ">Activision Publishing, Inc.</td><td class="patent-data-table-td ">Spawning projected avatars in a virtual universe</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8713181">US8713181</a></td><td class="patent-data-table-td patent-date-value">Aug 3, 2007</td><td class="patent-data-table-td patent-date-value">Apr 29, 2014</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method for transferring inventory between virtual universes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8762860">US8762860</a></td><td class="patent-data-table-td patent-date-value">Mar 14, 2011</td><td class="patent-data-table-td patent-date-value">Jun 24, 2014</td><td class="patent-data-table-td ">Disney Enterprises, Inc.</td><td class="patent-data-table-td ">Multi-user interactive communication network environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8788951">US8788951</a></td><td class="patent-data-table-td patent-date-value">Mar 5, 2007</td><td class="patent-data-table-td patent-date-value">Jul 22, 2014</td><td class="patent-data-table-td ">Sony Computer Entertainment America Llc</td><td class="patent-data-table-td ">Avatar customization</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8788952">US8788952</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 3, 2009</td><td class="patent-data-table-td patent-date-value">Jul 22, 2014</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and method for locating missing items in a virtual universe</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20080046511">US20080046511</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 15, 2006</td><td class="patent-data-table-td patent-date-value">Feb 21, 2008</td><td class="patent-data-table-td ">Richard Skrenta</td><td class="patent-data-table-td ">System and Method for Conducting an Electronic Message Forum</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20080215995">US20080215995</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 16, 2008</td><td class="patent-data-table-td patent-date-value">Sep 4, 2008</td><td class="patent-data-table-td ">Heiner Wolf</td><td class="patent-data-table-td ">Model based avatars for virtual presence</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20080229224">US20080229224</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 13, 2008</td><td class="patent-data-table-td patent-date-value">Sep 18, 2008</td><td class="patent-data-table-td ">Sony Computer Entertainment Inc.</td><td class="patent-data-table-td ">User interface in which object is assigned to data file and application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090254842">US20090254842</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 15, 2009</td><td class="patent-data-table-td patent-date-value">Oct 8, 2009</td><td class="patent-data-table-td ">Social Communication Company</td><td class="patent-data-table-td ">Interfacing with a spatial virtual communication environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100077034">US20100077034</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 22, 2008</td><td class="patent-data-table-td patent-date-value">Mar 25, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Modifying environmental chat distance based on avatar population density in an area of a virtual world</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100077318">US20100077318</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 22, 2008</td><td class="patent-data-table-td patent-date-value">Mar 25, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Modifying environmental chat distance based on amount of environmental chat in an area of a virtual world</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100251173">US20100251173</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 18, 2010</td><td class="patent-data-table-td patent-date-value">Sep 30, 2010</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Information processing device, contents processing method and program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110055733">US20110055733</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 3, 2009</td><td class="patent-data-table-td patent-date-value">Mar 3, 2011</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and Method for Locating Missing Items in a Virtual Universe</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110078573">US20110078573</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 20, 2010</td><td class="patent-data-table-td patent-date-value">Mar 31, 2011</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Terminal apparatus, server apparatus, display control method, and program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110149042">US20110149042</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 15, 2010</td><td class="patent-data-table-td patent-date-value">Jun 23, 2011</td><td class="patent-data-table-td ">Electronics And Telecommunications Research Institute</td><td class="patent-data-table-td ">Method and apparatus for generating a stereoscopic image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110161856">US20110161856</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 28, 2009</td><td class="patent-data-table-td patent-date-value">Jun 30, 2011</td><td class="patent-data-table-td ">Nokia Corporation</td><td class="patent-data-table-td ">Directional animation for communications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110225514">US20110225514</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 7, 2011</td><td class="patent-data-table-td patent-date-value">Sep 15, 2011</td><td class="patent-data-table-td ">Oddmobb, Inc.</td><td class="patent-data-table-td ">Visualizing communications within a social setting</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120038667">US20120038667</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 11, 2010</td><td class="patent-data-table-td patent-date-value">Feb 16, 2012</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Replicating Changes Between Corresponding Objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120105457">US20120105457</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 31, 2011</td><td class="patent-data-table-td patent-date-value">May 3, 2012</td><td class="patent-data-table-td ">Brian Mark Shuster</td><td class="patent-data-table-td ">Time-dependent client inactivity indicia in a multi-user animation environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120192088">US20120192088</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 20, 2011</td><td class="patent-data-table-td patent-date-value">Jul 26, 2012</td><td class="patent-data-table-td ">Avaya Inc.</td><td class="patent-data-table-td ">Method and system for physical mapping in a virtual world</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120290949">US20120290949</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 9, 2011</td><td class="patent-data-table-td patent-date-value">Nov 15, 2012</td><td class="patent-data-table-td ">Idle Games</td><td class="patent-data-table-td ">System and method for facilitating user interaction in a virtual space through unintended tangential effects to primary user interactions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130014033">US20130014033</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 8, 2011</td><td class="patent-data-table-td patent-date-value">Jan 10, 2013</td><td class="patent-data-table-td ">WoGo LLC</td><td class="patent-data-table-td ">Systems and methods for facilitating user interaction between multiple virtual environments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130104089">US20130104089</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 20, 2011</td><td class="patent-data-table-td patent-date-value">Apr 25, 2013</td><td class="patent-data-table-td ">Fuji Xerox Co., Ltd.</td><td class="patent-data-table-td ">Gesture-based methods for interacting with instant messaging and event-based communication applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE102004061884A1?cl=en">DE102004061884A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 22, 2004</td><td class="patent-data-table-td patent-date-value">Jul 13, 2006</td><td class="patent-data-table-td ">Combots Product Gmbh &amp; Co. Kg</td><td class="patent-data-table-td ">Messaging type telecommunication between users who are each provided with a virtual representative whose animated interactions are used to transfer chat-type text between the communicating users</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE102004061884B4?cl=en">DE102004061884B4</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 22, 2004</td><td class="patent-data-table-td patent-date-value">Jan 18, 2007</td><td class="patent-data-table-td ">Combots Product Gmbh &amp; Co. Kg</td><td class="patent-data-table-td ">Verfahren und System zur Telekommunikation mit virtuellen Stellvertretern</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2005025115A2?cl=en">WO2005025115A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 7, 2004</td><td class="patent-data-table-td patent-date-value">Mar 17, 2005</td><td class="patent-data-table-td ">Brian J Fogg</td><td class="patent-data-table-td ">Relationship user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2006023738A2?cl=en">WO2006023738A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 16, 2005</td><td class="patent-data-table-td patent-date-value">Mar 2, 2006</td><td class="patent-data-table-td ">Luigi Lira</td><td class="patent-data-table-td ">Overlaid display of messages in the user interface of instant messaging and other digital communication services</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2010071984A1?cl=en">WO2010071984A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 17, 2009</td><td class="patent-data-table-td patent-date-value">Jul 1, 2010</td><td class="patent-data-table-td ">Nortel Networks Limited</td><td class="patent-data-table-td ">Visual indication of audio context in a computer-generated virtual environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2011056473A2?cl=en">WO2011056473A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 22, 2010</td><td class="patent-data-table-td patent-date-value">May 12, 2011</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Three dimensional (3d) video for two-dimensional (2d) video messenger applications</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc715/defs715.htm&usg=AFQjCNEsvPOacMHcG92b6Esmd6a_S2NVkg#C715S757000">715/757</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc715/defs715.htm&usg=AFQjCNEsvPOacMHcG92b6Esmd6a_S2NVkg#C715S758000">715/758</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc709/defs709.htm&usg=AFQjCNFBXWYqUOVuuxerz7B8cqt9daJk7Q#C709S204000">709/204</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc709/defs709.htm&usg=AFQjCNFBXWYqUOVuuxerz7B8cqt9daJk7Q#C709S227000">709/227</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc715/defs715.htm&usg=AFQjCNEsvPOacMHcG92b6Esmd6a_S2NVkg#C715S848000">715/848</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04L0029060000">H04L29/06</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04L0012180000">H04L12/18</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04L0029080000">H04L29/08</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04L0012580000">H04L12/58</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L67/34">H04L67/34</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L67/38">H04L67/38</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L67/02">H04L67/02</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L67/36">H04L67/36</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L69/329">H04L69/329</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L12/5885">H04L12/5885</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L12/581">H04L12/581</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L12/1827">H04L12/1827</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OlNoBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L51/04">H04L51/04</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">H04L51/04</span>, <span class="nested-value">H04L29/08A7</span>, <span class="nested-value">H04L12/58B</span>, <span class="nested-value">H04L29/08N35</span>, <span class="nested-value">H04L29/08N33</span>, <span class="nested-value">H04L29/08N1</span>, <span class="nested-value">H04L12/18D3</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Apr 9, 2014</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20130927</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">LEIDOS, INC., VIRGINIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:SCIENCE APPLICATIONS INTERNATIONAL CORPORATION;REEL/FRAME:032642/0963</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 16, 2012</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 7, 2011</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:FORTERRA SYSTEMS, INC.;REEL/FRAME:027185/0335</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SCIENCE APPLICATIONS INTERNATIONAL CORPORATION, CA</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100128</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 20, 2008</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 6, 2007</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20060919</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 24, 2006</td><td class="patent-data-table-td ">RF</td><td class="patent-data-table-td ">Reissue application filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20060831</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 31, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">FORTERRA SYSTEMS, INC., CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:THERE;REEL/FRAME:018184/0782</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20040819</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 31, 2000</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">THERE, CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:HARVEY, WILLIAM D.;MCHUGH, JASON G.;PAIZ, FERNANDO J.;AND OTHERS;REEL/FRAME:011075/0429</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20000830</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U3gC-oGqT10UX9-2t2EcrSZ8XLfxQ\u0026id=OlNoBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U3BjVdKIatUP9tC-ynLSoHIe8JfRw\u0026id=OlNoBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U2WKKRuEd1MwChPMZVobDUbtD_dew","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Method_system_and_computer_program_produ.pdf?id=OlNoBAABERAJ\u0026output=pdf\u0026sig=ACfU3U0uOT8OGd3qd3914oAIr1OqqQCqXg"},"sample_url":"http://www.google.com/patents/reader?id=OlNoBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>