<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6771808 - System and method for registering patterns transformed in six degrees of ... - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="System and method for registering patterns transformed in six degrees of freedom using machine vision"><meta name="DC.contributor" content="Aaron S. Wallack" scheme="inventor"><meta name="DC.contributor" content="Cognex Corporation" scheme="assignee"><meta name="DC.date" content="2000-12-15" scheme="dateSubmitted"><meta name="DC.description" content="A system and method for utilizing a search tool that registers transformation of a trained pattern by at least four degrees of freedom to register the instance of a pattern in an arbitrary six-degree-of-freedom pose is provided. The search tool is first trained to recognize a plurality of versions of a trained pattern/fiducial that are incrementally transposed to induce differing levels of known aspect and shear. An object having several instances of the trained pattern located at known spacings and orientations therebetween is imaged by the trained search tool, and the located instances of the trained pattern are compared to expected instances of the trained pattern to measure relative six-degree-of-freedom orientation for the underlying object."><meta name="DC.date" content="2004-8-3" scheme="issued"><meta name="DC.relation" content="US:5459636" scheme="references"><meta name="DC.relation" content="US:5621807" scheme="references"><meta name="DC.relation" content="US:5793901" scheme="references"><meta name="DC.relation" content="US:5974365" scheme="references"><meta name="DC.relation" content="US:6137893" scheme="references"><meta name="DC.relation" content="US:6173066" scheme="references"><meta name="DC.relation" content="US:6173070" scheme="references"><meta name="citation_reference" content="&quot;A Tutorial on Visual Servo Control&quot; Seth Hutchinson, Greg Hager and Peter Corke, May 14, 1996; 42 pages."><meta name="citation_reference" content="Alfred M. Bruckstein, Larry O&#39;Gorman and Alon Orlitsky, Design of Shapes for Precise Image Registration, IEEE Transactions on Information Theory, vol., 44, No. 7, Nov. 1998."><meta name="citation_reference" content="David I. Havelock, Geometric Precision in Noise-Fee Digital Images, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. II, No. 10, Oct., 1989."><meta name="citation_reference" content="John W. Hill, Machine Intelligence Research Applied to Industrial Automation, U.S. Department of Commerce, National Technical Information Service, Nov. 1980."><meta name="citation_reference" content="Lawrence O&#39;Gorman, Subpixel Precision of Straight-Edged Shapes for Registration and Measurement, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 18, No. 7, Jul. 1996."><meta name="citation_reference" content="Neal T. Sullivan, Semiconductor Pattern Overlay, Digital Equipment Corp., Advanced Semiconductor Development, Critical Reviews vol. CR52."><meta name="citation_reference" content="W. Makous, Optimal Patterns for Alignment, Applied Optics, vol. 13, No. 3, Mar. 1974."><meta name="citation_patent_number" content="US:6771808"><meta name="citation_patent_application_number" content="US:09/737,836"><link rel="canonical" href="http://www.google.com/patents/US6771808"/><meta property="og:url" content="http://www.google.com/patents/US6771808"/><meta name="title" content="Patent US6771808 - System and method for registering patterns transformed in six degrees of freedom using machine vision"/><meta name="description" content="A system and method for utilizing a search tool that registers transformation of a trained pattern by at least four degrees of freedom to register the instance of a pattern in an arbitrary six-degree-of-freedom pose is provided. The search tool is first trained to recognize a plurality of versions of a trained pattern/fiducial that are incrementally transposed to induce differing levels of known aspect and shear. An object having several instances of the trained pattern located at known spacings and orientations therebetween is imaged by the trained search tool, and the located instances of the trained pattern are compared to expected instances of the trained pattern to measure relative six-degree-of-freedom orientation for the underlying object."/><meta property="og:title" content="Patent US6771808 - System and method for registering patterns transformed in six degrees of freedom using machine vision"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("olbsU6eGFMfKsQSZwYGoCw"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("USA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("olbsU6eGFMfKsQSZwYGoCw"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("USA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6771808?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6771808"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=be5nBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6771808&amp;usg=AFQjCNGbzjee8MsEvD0JlK_CylRz65FAnw" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6771808.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6771808.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6771808" style="display:none"><span itemprop="description">A system and method for utilizing a search tool that registers transformation of a trained pattern by at least four degrees of freedom to register the instance of a pattern in an arbitrary six-degree-of-freedom pose is provided. The search tool is first trained to recognize a plurality of versions of...</span><span itemprop="url">http://www.google.com/patents/US6771808?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6771808 - System and method for registering patterns transformed in six degrees of freedom using machine vision</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6771808 - System and method for registering patterns transformed in six degrees of freedom using machine vision" title="Patent US6771808 - System and method for registering patterns transformed in six degrees of freedom using machine vision"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6771808 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/737,836</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Aug 3, 2004</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Dec 15, 2000</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Dec 15, 2000</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">09737836, </span><span class="patent-bibdata-value">737836, </span><span class="patent-bibdata-value">US 6771808 B1, </span><span class="patent-bibdata-value">US 6771808B1, </span><span class="patent-bibdata-value">US-B1-6771808, </span><span class="patent-bibdata-value">US6771808 B1, </span><span class="patent-bibdata-value">US6771808B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Aaron+S.+Wallack%22">Aaron S. Wallack</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Cognex+Corporation%22">Cognex Corporation</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6771808.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6771808.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6771808.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (7),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (7),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (25),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (13),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (5)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=be5nBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6771808&usg=AFQjCNHlF4LbpTvre-yzfJet8xTLCmVgWQ">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=be5nBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6771808&usg=AFQjCNFZFagkBu58BnfyocGSpkC9djCwvA">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=be5nBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6771808B1%26KC%3DB1%26FT%3DD&usg=AFQjCNH9ZGmWyej-W2lpcpIB4f8F825ltw">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55304646" lang="EN" load-source="patent-office">System and method for registering patterns transformed in six degrees of freedom using machine vision</invention-title></span><br><span class="patent-number">US 6771808 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA50700925" lang="EN" load-source="patent-office"> <div class="abstract">A system and method for utilizing a search tool that registers transformation of a trained pattern by at least four degrees of freedom to register the instance of a pattern in an arbitrary six-degree-of-freedom pose is provided. The search tool is first trained to recognize a plurality of versions of a trained pattern/fiducial that are incrementally transposed to induce differing levels of known aspect and shear. An object having several instances of the trained pattern located at known spacings and orientations therebetween is imaged by the trained search tool, and the located instances of the trained pattern are compared to expected instances of the trained pattern to measure relative six-degree-of-freedom orientation for the underlying object.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(8)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6771808B1/US06771808-20040803-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6771808B1/US06771808-20040803-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6771808B1/US06771808-20040803-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6771808B1/US06771808-20040803-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6771808B1/US06771808-20040803-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6771808B1/US06771808-20040803-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6771808B1/US06771808-20040803-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6771808B1/US06771808-20040803-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(17)</span></span></div><div class="patent-text"><div mxw-id="PCLM8710576" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is: </claim-statement> <div class="claim"> <div num="1" id="US-6771808-B1-CLM-00001" class="claim">
      <div class="claim-text">1. A system for registering an object in six degrees of freedom using a machine vision system comprising:</div>
      <div class="claim-text">a search tool of the machine vision system adapted to recognize a plurality of instances of a trained pattern, the plurality of instances each being transformed to exhibit different amounts of aspect and shear and to provide a plurality of search results corresponding, respectively to the plurality of instances of the trained pattern; and </div>
      <div class="claim-text">a combiner that combines the plurality of search results, wherein found relative positions of the instances of the trained pattern are compared with known relative positions of the instances of the trained pattern so as to provide a location of the object in the six degrees of freedom. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" id="US-6771808-B1-CLM-00002" class="claim">
      <div class="claim-text">2. The system as set forth in <claim-ref idref="US-6771808-B1-CLM-00001">claim 1</claim-ref> wherein the combiner includes a linear transform between the expected relative position of each of the instances of the trained pattern and a normalized measured position of the instances of the trained pattern.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" id="US-6771808-B1-CLM-00003" class="claim">
      <div class="claim-text">3. The system as set forth in <claim-ref idref="US-6771808-B1-CLM-00001">claim 1</claim-ref> wherein the plurality of instances of the trained pattern comprise a plurality of transposed, synthetically generated image data, and the different amounts of aspect and shear are based upon predetermined known increments.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" id="US-6771808-B1-CLM-00004" class="claim">
      <div class="claim-text">4. The system as set forth in <claim-ref idref="US-6771808-B1-CLM-00001">claim 1</claim-ref> wherein the plurality of instances of the trained pattern comprise a plurality of different user-specified values for aspect and shear provided at runtime to the search tool so as to change an orientation of the trained pattern.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" id="US-6771808-B1-CLM-00005" class="claim">
      <div class="claim-text">5. The system as set forth in <claim-ref idref="US-6771808-B1-CLM-00001">claim 1</claim-ref> wherein the plurality of instances of the trained pattern each comprise portions of an overall pattern.</div>
    </div>
    </div> <div class="claim"> <div num="6" id="US-6771808-B1-CLM-00006" class="claim">
      <div class="claim-text">6. A method for registering an object in six degrees of freedom using a machine vision system comprising:</div>
      <div class="claim-text">recognizing, with a search tool of the machine vision system, a plurality of instances of a trained pattern, the plurality of instances each being transformed to exhibit different amounts of aspect and shear; </div>
      <div class="claim-text">providing, from the search tool, a plurality of search results corresponding, respectively, to the plurality of instances of the trained pattern; and </div>
      <div class="claim-text">combining the plurality of search results, including comparing found relative positions of the instances of the trained pattern with known relative positions of the instances of the trained pattern so as to provide a location of the object in the six degrees of freedom. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" id="US-6771808-B1-CLM-00007" class="claim">
      <div class="claim-text">7. The method as set forth in <claim-ref idref="US-6771808-B1-CLM-00006">claim 6</claim-ref> wherein the step of combining includes applying a linear transform between the expected relative position of each of the instances of the trained pattern and a normalized measured position of the instances of the trained pattern.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" id="US-6771808-B1-CLM-00008" class="claim">
      <div class="claim-text">8. The method as set forth in <claim-ref idref="US-6771808-B1-CLM-00006">claim 6</claim-ref> further comprising scoring each of the plurality of search results and selecting a best scoring of the search results for combining by the step of combining.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" id="US-6771808-B1-CLM-00009" class="claim">
      <div class="claim-text">9. The method as set forth in <claim-ref idref="US-6771808-B1-CLM-00006">claim 6</claim-ref> wherein the plurality of instances of the trained pattern comprise a plurality of transposed, synthetically generated image data, and the different amounts of aspect and shear are based upon predetermined known increments.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" id="US-6771808-B1-CLM-00010" class="claim">
      <div class="claim-text">10. The method as set forth in <claim-ref idref="US-6771808-B1-CLM-00006">claim 6</claim-ref> wherein the plurality of instances of the trained pattern comprise a plurality of different user-specified values for aspect and shear provided at runtime to the search tool so as to change an orientation of the trained pattern.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" id="US-6771808-B1-CLM-00011" class="claim">
      <div class="claim-text">11. The method as set forth in <claim-ref idref="US-6771808-B1-CLM-00006">claim 6</claim-ref> wherein the plurality of instances of the trained pattern each comprise portions of an overall pattern.</div>
    </div>
    </div> <div class="claim"> <div num="12" id="US-6771808-B1-CLM-00012" class="claim">
      <div class="claim-text">12. A computer-readable medium including program instructions executed on a computer for registering an object in six degrees of freedom using a machine vision system, the computer-readable medium including program instructions for performing the steps of:</div>
      <div class="claim-text">recognizing, with a search tool of the machine vision system, a plurality of instances of a trained pattern, the plurality of instances each being transformed to exhibit different amounts of aspect and shear; </div>
      <div class="claim-text">providing, from the search tool, a plurality of search results corresponding, respectively, to the plurality of instances of the trained pattern; and </div>
      <div class="claim-text">combining the plurality of search results, including comparing found relative positions of the instances of the trained pattern with known relative positions of the instances of the trained pattern so as to provide a location of the object in the six degrees of freedom. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" id="US-6771808-B1-CLM-00013" class="claim">
      <div class="claim-text">13. The computer-readable medium as set forth in <claim-ref idref="US-6771808-B1-CLM-00012">claim 12</claim-ref> wherein the step of combining includes applying a linear transform between the expected relative position of each of the instances of the trained pattern and a normalized measured position of the instances of the trained pattern.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" id="US-6771808-B1-CLM-00014" class="claim">
      <div class="claim-text">14. The computer-readable medium as set forth in <claim-ref idref="US-6771808-B1-CLM-00012">claim 12</claim-ref> further comprising scoring each of the plurality of search results and selecting best scoring of the search results for combining by the step of combining.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" id="US-6771808-B1-CLM-00015" class="claim">
      <div class="claim-text">15. The computer-readable medium as set forth in <claim-ref idref="US-6771808-B1-CLM-00012">claim 12</claim-ref> wherein the plurality of instances of the trained pattern comprise a plurality of transposed, synthetically generated image data, and the different amounts of aspect and shear are based upon predetermined known increments.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" id="US-6771808-B1-CLM-00016" class="claim">
      <div class="claim-text">16. The computer-readable medium as set forth in <claim-ref idref="US-6771808-B1-CLM-00012">claim 12</claim-ref> wherein the plurality of instances of the trained pattern comprise a plurality of different user-specified values for aspect and shear provided at runtime to the search tool so as to change an orientation of the trained pattern.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" id="US-6771808-B1-CLM-00017" class="claim">
      <div class="claim-text">17. The computer-readable medium as set forth in <claim-ref idref="US-6771808-B1-CLM-00012">claim 12</claim-ref> wherein the plurality of instances of the trained pattern each comprise portions of an overall pattern.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES54274744" lang="EN" load-source="patent-office" class="description">
    <heading>BACKGROUND OF THE INVENTION</heading> <p>1. Field of the Invention</p>
    <p>This invention relates to machine vision systems and more particularly to uses for advanced machine vision search tools that register patterns transformed by multiple degrees of freedom.</p>
    <p>2. Background Information</p>
    <p>The use of advanced machine vision systems and their underlying software is increasingly employed in a variety of manufacturing and quality control processes. Machine vision enables quicker, more accurate and repeatable results to be obtained in the production of both mass-produced and custom products. Basic machine vision systems include one or more cameras (typically having solid-state charge couple device (CCD) is imaging elements) directed at an area of interest, frame grabber/image processing elements that capture and transmit CCD images, a computer and display for running the machine vision software application and manipulating the captured images, and appropriate illumination on the area of interest.</p>
    <p>Many applications of machine vision involve the inspection of components and surfaces for defects that affect quality. Where sufficiently serious defects are noted, a part of the surface is marked as unacceptable/defective. Machine vision has also been employed in varying degrees to assist in manipulating manufacturing engines in the performance of specific tasks. One task using machine vision is visual servoing of robots in which a robot end effector is guided to a target using a machine vision feedback. Other applications also employ machine vision to locate a stationary and/or moving pattern.</p>
    <p>The advent of increasingly faster and higher-performance computers, has enabled the development of machine vision systems that employ powerful search tools. Such search tools enable a previously trained/stored image pattern to be acquired and registered/identified regardless of its viewed position. In particular, existing commercially available search tools can register such patterns transformed by at least three degrees of freedom, including two translational degrees (x and y-axis image plane) and a non-translational degree (rotation and/or scale, for example). One particular implementation of an advanced search tool is the rotation/scale-invariant search (RSIS) tool. This tool registers an image transformed by at least four degrees of freedom including the two translational degrees (x and y-axis image plane) and at least two non-translational degrees (z-axis(scale) and rotation within the x-y plane about an axis perpendicular to the plane). Some tools also register more complex transformations such as aspect ratio (rotation out of the plane whereby size on one axis decreases while size in the transverse axis thereto remains the same). These search tools, therefore, enable a specific pattern within the field of view to be located within a camera field of view to be positively identified and located accurately within the vision system's internal reference system (an x, y, z, rotation coordinate system, for example). The RSIS and other advanced search tools particularly allow for the identification and acquisition of patterns having somewhat arbitrary rotation, scaling (e.g. distancing) and translation with respect to the reference system. In other words, the tool is sufficiently robust to recognize a desired pattern even if it is rotated and larger/smaller/skewed relative to a “model” or trained pattern within the vision system.</p>
    <p>In general, advanced machine vision tools acquire an image of a pattern via a camera and analyze the outline or a particular part of the pattern, such as a predetermined fiducial mark. The processing speed of the underlying computer in which the tool resides is sufficient to enable a very large number of real time calculations to be completed in a short time frame. This particularly enables the search tool to determine the coordinates within an image reference system for each analyzed point in the viewed area, and correlate these through repetition with a desired pattern. The search tool may map the locations of various points in the captured image to stored points in the model image, and determine whether the captured image points fall within an acceptable range of values relative to the model image points. Using various decision algorithms, the tool decides whether the viewed pattern, in a particular rotation and distance (scale) corresponds to the desired search pattern. If so, the tool confirms that the viewed pattern is, in fact, the pattern for which the tool is searching and fixes its position and orientation.</p>
    <p>Machine vision systems having a four-degree-of-freedom, or greater, capability (such as RSIS) are available from a number of commercial vendors including Hexavision® from Adept Technology, Inc. of San Jose, Calif., and the popular Patmax® system from Cognex Corporation of Natick, Mass. Advanced machine vision search tools such as Patmax® also have the ability to take advantage of the previous known position of a search subject or target. This narrows the search area to positions relatively near the last known location. Therefore, searching is relatively faster on the next cycle since a smaller area is searched. In addition, these search tools can tolerate partial occlusion of a pattern and changes in its illumination, adding further to their robustness with respect to less advanced machine vision approaches.</p>
    <p>In general, when a camera views an object, it resolves an imaged pattern on the object into an image plane that, as defined herein, is represented by the x and y axes of a three-dimensional coordinate system. These are two translational axes in which the camera can register transformation of the pattern directly based upon observed position within the overall camera field of view. In addition, the camera axis perpendicular to the image plane can be represented as the z-axis, which, as noted is generally represented as a non-translational scale measurement (the larger the pattern, the closer it is to the camera and vice versa). This axes can also be measured by a special ranging camera. The three orthogonal axes (x, y and z) define three degrees of freedom with respect to the viewed object. In addition rotation of the viewed pattern of the object about three axes (typically characterized as roll, pitch and yaw rotations (ψ, φ, θ) about the respective (x, y, z) axes) can also be present with respect to the image plane. These rotations account for three additional degrees of freedom. When an object is rotated by θ, it appears, likewise rotated about the z/camera axis in the image plane with no change in width-along the image-plane other axes. When it is rotated about only one of either ψ or φ, it appears to have a changed aspect ratio (i.e. narrowed along one image plane axis as it rotates about the opposing image plane axis. Note that this form of rotation actually changes the viewed outline of the pattern with respect to the search tool's reference frame (i.e. a circle becomes an oval). If rotation about both image-plane axes occurs, then the pattern shows a shear. Using a four-degree-of-freedom search tool, it can be difficult to accurately register and locate a trained pattern that exhibits transformation along all six degrees of freedom including aspect and shear with respect to the image plane, as the trained pattern, itself undergoes change to its overall shape and size in a manner that may not be easily predicted or recognized by a search tool.</p>
    <p>Accordingly, it is an object of this invention to provide a system and method for measuring patterns transformed by six degrees of freedom using a machine vision search tool having, generally the ability to register patterns transformed by four degrees of freedom. The system and method should enable training and registration of a pattern particularly based upon transformations along degrees of freedom that are not readily accommodated by the underlying machine vision search tool including aspect and shear.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>This invention overcomes the disadvantages of the prior art by providing a system and method for utilizing a search tool that finds and/or registers (finds and locates) transformation of a trained pattern by at least four degrees of freedom to register the instance of a pattern in an arbitrary six-degree-of-freedom pose.</p>
    <p>According to a preferred embodiment the system is first trained by providing multiple patterns corresponding to different aspect and shear (pan and tilt) with respect to an original template pattern. In one embodiment, training can involve a synthetic (or physical) panning and tilting of the template pattern to generate the desired plurality of instances of the trained pattern at different amounts of aspect/shear aspect (e.g. the two degrees of freedom not handled by a four-degree-of freedom search tool). In an alternate embodiment, in cases where the search tool allows a user to specify differing levels of aspect/shear with respect to an image, training can entail the storage of a particular template pattern that is subsequently transformed based upon the user-specified aspect/shear.</p>
    <p>Once the search tool has been trained with the appropriate training patterns, the search tool is directed to acquire an image of an object containing one or more instance of the trained pattern(s). In a preferred embodiment, the instances of the trained pattern are found, and located (e.g. registered) so that the six-degree-of-freedom pose of the underlying object can be determined.</p>
    <p>According to one embodiment, a single instance of a fiducial or trained pattern on the object can be imaged and registered. The search tool can be run using the image with each of a plurality of trained patterns, each trained pattern representing a differing aspect/shear. The result from each run can be scored, and the score that is highest can be identified as the prevailing aspect and shear associated with the object. Score can be determined based upon interpolation, using a parabolic fit of a given highest-scoring fiducial versus close neighbors on a parabola. Alternatively, a gradient descent can be used to determine the closest-matching training fiducial in terms of aspect and shear to the runtime image fiducial.</p>
    <p>According to another embodiment multiple fiducials or subpatterns of a fiducial on the object are located based upon the trained pose information therefor and the relative six-degree-of-freedom pose is measured by providing a transformation between the located positions for other fiducials/subpatterns with respect to the base fiducial/subpattern and the expected positions for other fiducials/subpatterns with respect to a base fiducial/subpattern. The resulting position/orientation of the object is then determined.</p>
    <p>It is contemplated that the fiducials/subpatterns according to this invention can be found only, (e.g. not also located/registered) according to an alternate embodiment, using the techniques described herein.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p>The foregoing and other object and advantages of the invention will become clearer with reference to the following detailed description as illustrated by the drawings in which:</p>
    <p>FIG. 1 is a schematic perspective view of a machine vision system and associated object viewed by the system, in which the object is transformed by six degrees of freedom with respect to the system's reference frame;</p>
    <p>FIG. 2 is a flow diagram detailing a procedure for calibration a machine vision search tool so as to recognize transformation of a pattern in three dimensions;</p>
    <p>FIG. 3 is a plan view of an exemplary fiducial pattern for training the search tool;</p>
    <p>FIG. 4 is a plan view of a plurality of exemplary fiducial patterns based upon the pattern of FIG. 3 for training the fiducial, in which the patterns are transformed by varying degrees of aspect and shear;</p>
    <p>FIG. 5 is a flow diagram detailing a procedure for measuring aspect and shear in an object based upon patterns trained according to the procedure of FIG. 2 employing a single pattern on the object;</p>
    <p>FIG. 6 is a graph of an exemplary parabolic fit procedure for scoring a plurality of search results based upon aspect and shear according to the embodiment FIG. 5;</p>
    <p>FIG. 7 is a plan view of an exemplary multiple-pattern layout for measuring aspect and shear according to an alternate embodiment of this invention; and</p>
    <p>FIG. 8 is a flow diagram detailing a procedure for measuring aspect and shear in an object based upon patterns trained according to the procedure of FIG. 2 employing the multiple-pattern layout of FIG. <b>7</b>.</p>
    <heading>DETAILED DESCRIPTION OF AN ILLUSTRATIVE EMBODIMENT</heading> <p>FIG. 1 shows a generalized setup <b>100</b> in which a camera assembly <b>110</b> is directed at an object <b>112</b> located within a work area <b>114</b>, in which the object lies within the field of view of the camera as defined generally as a region about the camera axis <b>1</b> <b>16</b>. This camera axis defines a relative search tool reference frame/system that is represented by the set of axes <b>118</b>.</p>
    <p>The exemplary camera assembly <b>110</b> includes an imaging device such as a solid-state charge-couple device (CCD) and an appropriate image capture device/processor such as a framegrabber. The camera is interconnected with a computer system <b>120</b> having a display <b>122</b> and a graphical user interface manipulated by a keyboard <b>124</b>, mouse and/or similar devices. Residing on the computer <b>120</b> is a machine vision system and related software as described further below. Note that the term “software,” as used herein refers generally to any number of actions and data structures, whether implemented by a general-purpose processor or any an application-specific or custom processor (example—firmware. It is expressly contemplated that additional camera assemblies can be interconnected (see phantom camera and link <b>126</b>) with the computer <b>120</b> and directed at the work area <b>114</b> and/or object <b>112</b>. These cameras can be selected for imaging the object when appropriate, such as when a predetermined portion of the object is at a non-viewable orientation/pose with respect to the illustrated camera assembly <b>110</b>.</p>
    <p>As described above the x and y-axes define the image plane with respect to the orthogonal z-axis (camera axis <b>116</b>). For a conventional non-ranging camera, the z-axis is representative of scale since translation of an image along the z-axis appears as a relative change in viewed image size rather than a defined coordinate translation. Similarly, any transformation of the image along orthogonal x and y-axes are viewed as a change in the relative location of the image within the applicable camera field of view. In addition, rotation about the respective camera axis <b>130</b>, <b>132</b> (z(scale)-axis), within the image plane, is denoted as angular rotation (θ). The degree of rotation (θ) is accordingly viewed as a relative rotation in the viewed image pattern.</p>
    <p>The camera <b>110</b> transmits captured images within its field of view to a commercially available machine vision system capable of acquiring a respective image from each of at least two separate camera assemblies. The machine vision system according to this invention includes a search tool adapted to register a pattern, based upon a trained image thereof, transformed by at least four degrees of freedom—at least two translational degrees (x and y axes/image plane) and at least two non-translational (typically rotation θ in the x-y plane and z(scale)) degrees of freedom. Note that a basic search tool registering transformation of a pattern in two translational (x, y) degrees of freedom can be implemented according to an alternate embodiment described below. For the purposes of the preferred embodiment, however, an exemplary search tool that registers transformation of an acquired image by at least four degrees of freedom including the two translational degrees (x, y) and two non-translational degrees (rotation θ and z(scale)) is the rotation/scale-invariant search (RSIS) tool such as the above-referenced Patmax® or Hexavision® system.</p>
    <p>Note by “translational” it is meant, viewed linear displacement in an image plane clearly viewed by the camera, while “non-translational” means a change in the viewed image outside the parameters of the image plane such as general rotation of the image, change in size/scale due to movement toward and away from the camera, change in aspect ratio (rotation outside the image plane in one axis lying in the plane) and change in shear (simultaneous rotation outside the plane in two planar axes). In general, the search tool of the present embodiment is adapted to view the z(scale) axis as a non-translational degree of freedom.</p>
    <p>Additionally, as used herein, the term “find” or “finding” in the context of the search tool shall refer to the process of looking for an instance of a selected pattern in an acquired image, and reporting whether such an instance of the pattern has occurred. “Locate” or “locating” shall, in this context, refer to the performance of measurements of found instances of the pattern, and “register” or “registering” shall refer to the combination of finding and locating of the pattern by the search tool.</p>
    <p>In order to determine the three-dimensional location of an object, the camera is first trained to recognize a specific pattern that, according to this embodiment, defines a fiducial mark having a distinctive design and recognizable orientation. This fiducial is placed upon, or is part of the underlying object to be located by the system. The exemplary fiducial shown in FIG. 1 defines a circle <b>150</b> having alternating light and dark quadrants. However, any pattern which can be registered within the selected degrees of freedom can be used as a fiducial according to this invention. Other examples of fiducials can, therefore, include a unique object shape/outline, a logo on the object, or a particular structure/extension attached to the object. In general, the search tool uses previously trained pattern data in order to register a pattern in a currently viewed image. In other words, the software in the search tool attempts to “recognize” an instance of the trained pattern in the viewed/acquired image.</p>
    <p>As described further below, the object can include multiple fiducials, such as fiducials <b>152</b> and <b>154</b> on selected surfaces.</p>
    <p>The following is a description of a technique that enables the exemplary search tool (an RSIS tool, for example), which capable of registering transformation of a pattern by four degrees of freedom, to successfully identify and locate instances of the trained pattern in arbitrary six-degree-of freedom poses. Note that the exemplary object <b>112</b>, and underlying fiducials <b>150</b>, <b>152</b>, <b>154</b> are disposed at a shear (φ≠0, ψ≠0) implicating all six degrees of freedom.</p>
    <p>Briefly, the first part of the invention involves acquiring images of multiple patterns corresponding to synthetically panning and tilting the original pattern to thereby train the search tool with respect to the patterns. Then, the search tool procedure is run to search for all of the different patterns in the image. The second part of the invention involves registering multiple fiducials (or different parts of the same fiducial) in order to measure relative positions from which to measure shear and aspect (e.g. the two degrees of freedom not handled by a four-degree-of freedom search tool).</p>
    <p>FIG. 2 details a procedure <b>200</b> for training the search tool in view of instances of patterns in arbitrary six-degree-of-freedom poses. First, an initial pattern to be registered by the search tool, and, accordingly, applied to an object is provided. In this embodiment, the pattern is the fiducial <b>300</b> shown in FIG. 3 consisting of alternating light and dark quadrants. However, the pattern to be trained and located can be any acceptable shape that can be sufficiently discerned by the search tool including a logo or structure on the object or an underlying shape of the object itself. In this example, the pattern/fiducial comprises a circle. A template of the pattern is made (step <b>202</b>) from the basic pattern viewed in a perpendicular plane to the camera axis as shown in FIG. <b>3</b>.</p>
    <p>It is contemplated that search tool training, for the purposes of this embodiment, can be accomplished using only one fiducial pattern template—while a plurality of fiducials (typically similar fiducials) may be present on the actual object being located during a run of the search routine. Initial training with one fiducial/pattern involves the positioning of the pattern so that it is substantially orthogonal to the camera axis (e.g. ψ=φ=0).</p>
    <p>Given an original pattern template (which can be an acquired fiducial on the object), multiple aspect/shear patterns are then generated, which warp the original pattern template (step <b>204</b>). This can be performed by physically rotating a surface containing the template in the presence of a camera, or by synthetically altering the shape of the pattern using an appropriate three-dimensional drawing software application. As an alternative to synthetically/physically generating various pan and tilt (aspect/shear) patterns from the template for training purposes, certain search tools (such as Patmax®) enable a user to specify a given value aspect/shear for an image. Even though the tool does not search over a number of aspects/shears, a selected value can be entered by the user, and retraining of the tool in view of this input is not required. Accordingly, training procedure steps <b>204</b>-onward can be adapted for search tools that accommodate runtime input of specific aspect/shear values.</p>
    <p>Where the training procedure <b>200</b> of FIG. 2 is fully applied, reference is also made to FIG. 4, which shows patterns that are generated by synthetically warping the pattern to have different aspect and shear values. In particular patterns <b>402</b>, <b>404</b>, <b>406</b> and <b>408</b> show a change aspect ratio, that can be incremented according to a predetermined relationship, such as rotation by ten-degree increments from 0 degrees (pattern <b>402</b>) to 40 degrees (pattern <b>408</b>). Likewise, patterns <b>412</b>, <b>414</b>, <b>416</b> and <b>418</b> can be generated with differing levels of shear. Various combinations of aspect and shear can be generated, so that a large number of incremental, known levels of aspect and shear are provided.</p>
    <p>Then, the search tool procedure is run to find the pattern in arbitrary poses (step <b>206</b>). In particular, the search tool is run multiple times with each of the multiple template patterns corresponding to the different synthetic aspect/shears until all desired patterns have been run (decision step <b>208</b>). Each time the procedure runs through a pattern, it increments to select the next pattern in the run (step <b>209</b>). These patterns are provided as synthetic image data, preferably. Alternatively, the patterns can be provided as a series of actual image acquired by the camera, as noted generally above. However, since a run consisting of a large number of patterns is desired, the used of a succession of synthetic images is preferred. Once all patterns have been run through, and located, the training stage is complete (step <b>210</b>).</p>
    <p>Note that, during training, the user can specify the range of aspects and shears that will be required (just as the user can specify the range of expected orientations and scales for commercially available RSIS/search tools). The user can also specify the “step” or increment size, thereby defining which aspect ratios will be handled (i.e., aspects={0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.15})</p>
    <p>Note also that the above-described training procedure can also be applied to subpatterns in one or more patterns/fiducials. For example, semi-circles can be trained in a variety of aspect and shear orientations.</p>
    <p>While the creation of a synthetic pattern including various degrees of shear and aspect is described, it is expressly contemplated that certain search tools, such as the above-described Patmax® enable a user to specify a linear transform to apply to the s training procedure and/or the acquired runtime image, thus obviating the need for synthetic shear/aspect images. Accordingly, training in the presence of this transform does not entail training multiple patterns, but rather, running the search tool with different inputs that are each derived from the transform acting upon the single trained pattern.</p>
    <p>Having described the training of the search tool to recognize patterns at different aspect and shear orientations, the location and measurement of aspect and shear to define the orientation and position of an underlying object is now described in further detail.</p>
    <p>It is contemplated that either a single pattern/fiducial, or multiple patterns/fiducials can be alternatively employed according to this invention to determine the six-degree-of-freedom pose of an underlying object. The following is a description of procedures for determining the pose from a single fiducial/pattern and from multiple (example—three) fiducials/patterns.</p>
    <p>A general procedure for computing six-degree-of-freedom pose using a single fiducial or pattern is described in FIG. <b>5</b>. The procedure <b>500</b>, first acquires an image of the object and an associated fiducial or pattern (step <b>502</b>). The search tool is,run using trained patterns at all the various poses, or appropriate transforms of the basic pattern for differing aspect/shear values. Each run results in the determination of a score for the trained fiducial with respect to the runtime image (step <b>504</b>—described further below).</p>
    <p>The procedure repeats, selecting a new trained pattern (step <b>506</b>) until all relevant training patterns have been run by the search tool (decision step <b>508</b>). The aspect/shear corresponding to the run pattern with the highest score is then selected (step <b>510</b>). From this value, the relative pose of the underlying object can be derived by appropriate transforms (step <b>512</b>).</p>
    <p>To determine the highest score for aspect and shear, two alternative techniques—parabolic fit or gradient descent—can be employed.</p>
    <p>A parabolic fit of the scores associated with the “highest scoring pattern” and the “neighboring patterns” in aspect and shear can be employed with reference to FIG. <b>6</b>. For example, if pattern A (shear=0.03, aspect=0.09) had the highest score (0.75), then consider the two scores associated with the two adjacent patterns in the shear direction (pattern B (shear=0.02, aspect=0.09) score=0.74 and pattern C (shear=0.04, aspect=0.09) score=0.72), then a parabola <b>600</b> is scribed through the three numbers to locate the is maximum value (star <b>602</b>). Interpolation can be used to compute this value, approximately shear=0.0275 in this example. Note that parabolic fit techniques can be applied generally to refine the other variables for a given six-degree-of-freedom object pose (x, y, z(scale) and θ), in addition to shear and aspect according to this embodiment.</p>
    <p>Note that the x, y, z(scale) and θ variables are refined using bilinear interpolation of the x, y, z(scale) and θ variables of the search result associated with the four “neighboring patterns” in aspect and shear. This bilinear interpolation can be based upon the scores of the search results or it can be based upon the differences between the “peak” aspect/shear values and the aspect/shear values of the neighboring patterns.</p>
    <p>In addition, an exemplary computer-readable code expression for determining the parabolic peak can be written as follows:</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup align="left" colsep="0" rowsep="0" cols="2"> <colspec colname="offset" colwidth="14pt" align="left"> </colspec> <colspec colname="1" colwidth="203pt" align="left"> </colspec> <thead> <tr class="description-tr"> <td class="description-td"> </td>
                <td namest="offset" nameend="1" align="center" rowsep="1" class="description-td" colspan="2"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">double cfParabolicPeakPosition(double originalValue, double left,</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">double</td>
              </tr> </tbody> </tgroup> <tgroup align="left" colsep="0" rowsep="0" cols="1"> <colspec colname="1" colwidth="217pt" align="left"> </colspec> <tbody valign="top"> <tr class="description-tr"> <td class="description-td">center, double right, double stepSize )</td>
              </tr> <tr class="description-tr"> <td class="description-td">{</td>
              </tr> <tr class="description-tr"> <td class="description-td">double numer, denom;</td>
              </tr> <tr class="description-tr"> <td class="description-td">numer = right − left;</td>
              </tr> <tr class="description-tr"> <td class="description-td">denom = (2*center − right − left)*2;</td>
              </tr> <tr class="description-tr"> <td class="description-td">if (denom == 0.0)</td>
              </tr> </tbody> </tgroup> <tgroup align="left" colsep="0" rowsep="0" cols="2"> <colspec colname="offset" colwidth="14pt" align="left"> </colspec> <colspec colname="1" colwidth="203pt" align="left"> </colspec> <tbody valign="top"> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">return(originalValue);</td>
              </tr> </tbody> </tgroup> <tgroup align="left" colsep="0" rowsep="0" cols="1"> <colspec colname="1" colwidth="217pt" align="left"> </colspec> <tbody valign="top"> <tr class="description-tr"> <td class="description-td">else</td>
              </tr> </tbody> </tgroup> <tgroup align="left" colsep="0" rowsep="0" cols="2"> <colspec colname="offset" colwidth="14pt" align="left"> </colspec> <colspec colname="1" colwidth="203pt" align="left"> </colspec> <tbody valign="top"> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">return(originalValue + stepSize * numer/denom);</td>
              </tr> </tbody> </tgroup> <tgroup align="left" colsep="0" rowsep="0" cols="1"> <colspec colname="1" colwidth="217pt" align="left"> </colspec> <tbody valign="top"> <tr class="description-tr"> <td class="description-td">}</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="1" align="center" rowsep="1" class="description-td" colspan="1"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>According to an alternate embodiment a gradient descent (discussed in further detail below based upon three fiducials) can be employed to search for the “optimal shear and aspect.” The procedure starts with the value for shear/aspect corresponding to the pattern exhibiting the highest score, then the procedure would compute scores for shear/aspect nearby this start configuration, and updates the shear/aspect configuration until the computed score no longer increases.</p>
    <p>Note that the “derivatives” dScore/dShear, and dScore/dAspect can be numerically computed to improve the efficiency of the gradient descent procedure above. It is contemplated that certain commercially available search tools enable a user to specify values for shear/aspect with respect to an acquired image. Accordingly the search tool procedure can be run on the runtime image while slightly modifying the shear and aspect associated with this runtime image. This process allows the computation of scores without training the tool based upon multiple shear/aspect patterns. Rather, the tool is run with different inputs of aspect/shear (pan and tilt), and the scoring is computed for each search result therefrom. Note that gradient descent techniques can be applied generally to refine the other variables for a given six-degree-of-freedom object pose (x, y, z(scale) and θ), in addition to shear and aspect according to this embodiment.</p>
    <p>An alternate embodiment for computing six-degree-of-freedom pose is now described. The example herein is based upon three discrete fiducials/patterns on the object. FIG. 7 shows an exemplary group of three fiducials F<b>1</b>, F<b>2</b> and F<b>3</b>, on the surface <b>700</b> of an object. Similarly, a procedure for locating and measuring aspect and shear based upon multiple fiducials (F<b>1</b>, F<b>2</b>, F<b>3</b>) is shown in FIG. <b>8</b>. In general, the procedure <b>800</b> entails the use of multiple search tool targets (either different fiducials or subpatterns on the same fiducial) in order to provide multiple location measurements. First, the search tool acquires one or more images of the object containing the target fiducial(s) or fiducial subpatterns (step <b>802</b>).</p>
    <p>According to procedure (refer to step <b>804</b> in FIG. <b>8</b>), the fiducials F<b>1</b>, F<b>2</b> and F<b>3</b> are located by the search tool in the acquired image(s) of the object <b>700</b>. The locations of these fiducials have been determined using the scoring techniques, using search tool results for the fiducials exhibiting the best respective scores. Assume that the expected relative positions, scales, and orientations between the fiducial(s)/subpatterns are known in advance by the procedure—having been accurately measured on the object to be located. In the case of subpatterns, the search tool is trained to locate selected subpatterns of the fiducial, as described above. Note that the procedure would then be provided with the expected relative positions between the subpatterns.</p>
    <p>It is often desirable to generate multiple search tool results representing different pan and tilt angles for the trained patterns/fiducial, and select the search tool result having the best score for the respective pattern/fiducial, based upon the closest aspect and shear fit with the imaged fiducial/pattern on the object. Where multiple search results are present, and a single result is to be employed, the search result having the highest “score” typically provides the preferred solution—where score is generally based upon viewing different aspect/shear patterns (e.g. different degrees of pan and tilt) and finding the instance that is the highest-scoring/most-robust of the patterns. Score can be computed as a function of coverage or fit error between the trained pattern and the runtime instance of the pattern. For the exemplary Patmax® search tool, score would be computed as:</p>
    <p>
      <maths> <formula-text>coverage/(1+fit error).</formula-text> </maths> </p>
    <p>Having determined locations according to highest score of multiple search results, the overall procedure can now combine the best-scoring search results to determine the position of the object in six degrees of freedom with respect to expected transformation between the multiple fiducials/patterns (step <b>806</b>).</p>
    <p>By way of example, assume that F<b>2</b>'s expected position is exactly <b>60</b> scaled units to the right of F<b>1</b> along F<b>1</b>'s x-axis (<b>704</b>) as taken from an arbitrary center point on F<b>1</b>. A scaled unit is one in which the measured distance is divided by the measured scale of F<b>1</b>. If F<b>2</b>'s measured position (relative to F<b>1</b>) is 56 scaled units, then the aspect ratio is 56/60.</p>
    <p>Furthermore, assume F<b>3</b>'s expected position is exactly 40 scaled units above F<b>1</b>'s x-axis (along y-axis (<b>706</b>)). Then, if F<b>3</b>'s measured position is 40 scaled units above F<b>1</b> along its x-axis, and 2 scaled units to the right (arrow 2u) of F<b>1</b>'s y-axis, then resulting the shear ratio would be 2/40.</p>
    <p>More generally, the procedure of this embodiment attempts to solve for aspect and shear given the measured relative positions of F<b>2</b> and F<b>3</b> and their expected relative positions. It is contemplated that a single search result, extracted from a plurality of search results obtained at different pan and tilt angles, can be used to compute the six-degree-of-freedom pose. The procedure for selecting the best search result is described further below. However, the general computation of the orientation of the object based upon multiple fiducials is first described as follows:</p>
    <p>By way of example, now consider the relative positions normFid<b>21</b>, normFid<b>31</b> between F<b>2</b> and F<b>1</b> and F<b>3</b> and F<b>1</b> normalized by the orientation and scale of F<b>1</b> </p>
    <p>Let R be the rotation matrix which rotates a point by the opposite of F<b>1</b>'s measured orientation characterized as follows: <maths> <math> <mrow> <mo></mo> <mrow> <mo></mo> <mtable> <mtr> <mtd> <mrow> <mi>cos</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mo>-</mo> <mi>angle_F1</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>-</mo> <mrow> <mi>sin</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mo>-</mo> <mi>angle_F1</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>sin</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mo>-</mo> <mi>angle_F1</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> <mtd> <mrow> <mi>cos</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mo>-</mo> <mi>angle_F1</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> </mtable> <mo></mo> </mrow> </mrow> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-M00001.png"> <img id="EMI-M00001" file="US06771808-20040803-M00001.TIF" img-content="math" img-format="tif" alt="Figure US06771808-20040803-M00001" src="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-M00001.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00001" attachment-type="nb" file="US06771808-20040803-M00001.NB"> </attachment> </attachments> </maths> </p>
    <p>Next, let S be the scale matrix which scales points by the opposite of F<b>1</b>'s measured scale. S is defined as follows: <maths> <math> <mrow> <mo></mo> <mrow> <mo></mo> <mtable> <mtr> <mtd> <mrow> <mn>1</mn> <mo>/</mo> <mi>scale_F1</mi> </mrow> </mtd> <mtd> <mn>0</mn> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mrow> <mn>1</mn> <mo>/</mo> <mi>scale_F1</mi> </mrow> </mtd> </mtr> </mtable> <mo></mo> </mrow> </mrow> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-M00002.png"> <img id="EMI-M00002" file="US06771808-20040803-M00002.TIF" img-content="math" img-format="tif" alt="Figure US06771808-20040803-M00002" src="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-M00002.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00002" attachment-type="nb" file="US06771808-20040803-M00002.NB"> </attachment> </attachments> </maths> </p>
    <p>Then, the following expression is provided:</p>
    <p>
      <maths> <formula-text>NormFid<b>21</b>=<i>R*S</i>*[(<i>x, y</i>)<sub>—</sub> <i>F</i> <b>2</b>−(<i>x, y</i>)<sub>—</sub> <i>F</i> <b>1</b>]</formula-text> </maths> </p>
    <p>
      <maths> <formula-text>NormFid<b>31</b>=<i>R*S</i>*[(<i>x, y</i>)<sub>—</sub> <i>F</i> <b>3</b>−(<i>x, y</i>)<sub>—</sub> <i>F</i> <b>1</b>]</formula-text> </maths> </p>
    <p>Assume that the expected relative positions of F<b>3</b> and F<b>2</b> are provided as ExpectedFid<b>21</b>, ExpectedFid<b>31</b>.</p>
    <p>Assume also that that there is a linear transform between the expected relative positions and the normalized measured relative positions and let M be a 2×2 matrix corresponding to that linear transform. Matrix M is defined by: <maths> <math> <mrow> <mo></mo> <mrow> <mo></mo> <mtable> <mtr> <mtd> <mi>A</mi> </mtd> <mtd> <mi>B</mi> </mtd> </mtr> <mtr> <mtd> <mi>C</mi> </mtd> <mtd> <mi>D</mi> </mtd> </mtr> </mtable> <mo></mo> </mrow> </mrow> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-M00003.png"> <img id="EMI-M00003" file="US06771808-20040803-M00003.TIF" img-content="math" img-format="tif" alt="Figure US06771808-20040803-M00003" src="//patentimages.storage.googleapis.com/US6771808B1/US06771808-20040803-M00003.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00003" attachment-type="nb" file="US06771808-20040803-M00003.NB"> </attachment> </attachments> </maths> </p>
    <p>Then:</p>
    <p> NormFid<b>21</b>=<i>M</i>*ExpectedFid<b>21</b> </p>
    <p>
      <maths> <formula-text>NormFid<b>31</b>=<i>M</i>*ExpectedFid<b>31</b> </formula-text> </maths> </p>
    <p>Since each point has two coordinates (x, y), the above two expressions correspond to the following four equations:</p>
    <p>
      <maths> <formula-text>NormFid<b>21</b>.<i>x=A</i>*ExpectedFid<b>21</b>.<i>x+B</i>*ExpectedFid<b>21</b>.<i>y</i> </formula-text> </maths> </p>
    <p>
      <maths> <formula-text>NormFid<b>21</b>.<i>x=C</i>*ExpectedFid<b>21</b>.<i>x+D</i>*ExpectedFid<b>21</b>.<i>y</i> </formula-text> </maths> </p>
    <p>
      <maths> <formula-text>NormFid<b>31</b>.<i>x=A</i>*ExpectedFid<b>31</b>.<i>x+B</i>*ExpectedFid<b>31</b>.<i>y</i> </formula-text> </maths> </p>
    <p>
      <maths> <formula-text>NormFid<b>31</b>.<i>x=C</i>*ExpectedFid<b>31</b>.<i>x+D</i>*ExpectedFid<b>31</b>.<i>y</i> </formula-text> </maths> </p>
    <p>The equations are rearranged so that there are two equations about A and B and two equations about C and D as follows:</p>
    <p>
      <maths> <formula-text>NormFid<b>21</b>.<i>x=A</i>*ExpectedFid<b>21</b>.<i>x+B</i>*ExpectedFid<b>21</b>.<i>y</i> </formula-text> </maths> </p>
    <p>
      <maths> <formula-text>NormFid<b>31</b>.<i>x=A</i>*ExpectedFid<b>31</b>.<i>x+B</i>*ExpectedFid<b>31</b>.<i>y</i> </formula-text> </maths> </p>
    <p>
      <maths> <formula-text>NormFid<b>21</b>.<i>x=C</i>*ExpectedFid<b>21</b>.<i>x+D</i>*ExpectedFid<b>21</b>.<i>y</i> </formula-text> </maths> </p>
    <p>
      <maths> <formula-text>NormFid<b>31</b>.<i>x=C</i>*ExpectedFid<b>31</b>.<i>x+D</i>*ExpectedFid<b>31</b>.<i>y</i> </formula-text> </maths> </p>
    <p>Given two linear equations in two unknowns (A, B) or (C, D), it is well-known how to solve for (A, B) and (C, D).</p>
    <p>The (A, B, C, D) matrix M can be composed with F<b>1</b>'s rotation scale matrix to provide a full four-degree-of-freedom rotation matrix. When composed with F<b>1</b>'s translation vector, this provides a full six-degree-of-freedom matrix measurement.</p>
    <p>Having computed the six-degree-of-freedom orientation using either a single fiducial/pattern, or multiple fiducials/patterns above, the prevailing six-degree-of-freedom pose (location and orientation) of the object can now be determined by applying appropriate transforms based upon the known position/translation of selected fiducials patterns on the object (step <b>808</b>).</p>
    <p>Note that the use of three fiducials F<b>1</b>, F<b>2</b>, F<b>3</b> as described above generally enables measurement of the prevailing aspect and shear, therefore allowing for correction of the training fiducial's pose. It is possible to train on one of the three fiducials and run the procedure on the other two fiducials provided that the expected relative position of the other two to the training fiducial is known. If not already known, the relative positions should be ascertained during the training procedure through manual or automated procedures.</p>
    <p>Finally, it is contemplated, according to an alternate embodiment that the principles described herein can be applied to a search tool having the basic capability to register transformation in as little as two translational (x, y) degrees of freedom. To accomplish this, templates are created with varying aspects and shears. Based upon the above description for determining the best scoring aspect/shear, a procedure according to this alternate embodiment now find the best scoring scale, rotation, aspect and shear template. Where parabolic interpolation was employed on neighbors of aspect/shear to refine values for x, y, z(scale) and θ, the procedure of this alternate embodiment uses parabolic interpolation on neighbors of rotation, scale aspect, shear and refines x, y. Accordingly, as generally defined herein, the term “search” tool can also include a basic (x, y) search tool used in conjunction with the procedure of this alternate embodiment.</p>
    <p>The foregoing has been a detailed description of a preferred embodiment of the invention. Various modifications and additions can be made without departing from the spirit and scope of this invention. For example, while the image of the object is acquired using a single camera on a single surface, it is expressly contemplated that fiducials/fiducial subpatterns can be located on a plurality of surfaces and imaged by separate cameras in communication with the search tool. Likewise, a plurality of cameras and images can be applied to different parts of the same fiducial or pattern to derive a composite search tool result. Finally, it is expressly contemplated that any of the functions or operations described herein can be performed using hardware, software (e.g. any a computer-readable medium), or a combination of hardware and software. Accordingly, this description is meant to be taken only by way of example, and not to otherwise limit the scope of the invention.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5459636">US5459636</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 14, 1994</td><td class="patent-data-table-td patent-date-value">Oct 17, 1995</td><td class="patent-data-table-td ">Hughes Aircraft Company</td><td class="patent-data-table-td ">Position and orientation estimation neural network system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5621807">US5621807</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 13, 1994</td><td class="patent-data-table-td patent-date-value">Apr 15, 1997</td><td class="patent-data-table-td ">Dornier Gmbh</td><td class="patent-data-table-td ">Intelligent range image camera for object measurement</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5793901">US5793901</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 2, 1995</td><td class="patent-data-table-td patent-date-value">Aug 11, 1998</td><td class="patent-data-table-td ">Omron Corporation</td><td class="patent-data-table-td ">Device and method to detect dislocation of object image data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5974365">US5974365</a></td><td class="patent-data-table-td patent-date-value">Oct 23, 1997</td><td class="patent-data-table-td patent-date-value">Oct 26, 1999</td><td class="patent-data-table-td ">The United States Of America As Represented By The Secretary Of The Army</td><td class="patent-data-table-td ">System for measuring the location and orientation of an object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6137893">US6137893</a></td><td class="patent-data-table-td patent-date-value">Oct 7, 1996</td><td class="patent-data-table-td patent-date-value">Oct 24, 2000</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Machine vision calibration targets and methods of determining their location and orientation in an image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6173066">US6173066</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 21, 1997</td><td class="patent-data-table-td patent-date-value">Jan 9, 2001</td><td class="patent-data-table-td ">Cybernet Systems Corporation</td><td class="patent-data-table-td ">Pose determination and tracking by matching 3D objects to a 2D sensor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6173070">US6173070</a></td><td class="patent-data-table-td patent-date-value">Dec 30, 1997</td><td class="patent-data-table-td patent-date-value">Jan 9, 2001</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Machine vision method using search models to find features in three dimensional images</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="A+Tutorial+on+Visual+Servo+Control"'>A Tutorial on Visual Servo Control</a>" Seth Hutchinson, Greg Hager and Peter Corke, May 14, 1996; 42 pages.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Alfred M. Bruckstein, Larry O'Gorman and Alon Orlitsky, Design of Shapes for Precise Image Registration, IEEE Transactions on Information Theory, vol., 44, No. 7, Nov. 1998.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">David I. Havelock, Geometric Precision in Noise-Fee Digital Images, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. II, No. 10, Oct., 1989.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">John W. Hill, Machine Intelligence Research Applied to Industrial Automation, U.S. Department of Commerce, National Technical Information Service, Nov. 1980.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Lawrence O'Gorman, Subpixel Precision of Straight-Edged Shapes for Registration and Measurement, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 18, No. 7, Jul. 1996.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Neal T. Sullivan, Semiconductor Pattern Overlay, Digital Equipment Corp., Advanced Semiconductor Development, Critical Reviews vol. CR52.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">W. Makous, Optimal Patterns for Alignment, Applied Optics, vol. 13, No. 3, Mar. 1974.</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7298926">US7298926</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 15, 2005</td><td class="patent-data-table-td patent-date-value">Nov 20, 2007</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Image size reduction method and system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7400760">US7400760</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 17, 2004</td><td class="patent-data-table-td patent-date-value">Jul 15, 2008</td><td class="patent-data-table-td ">Fanuc Ltd</td><td class="patent-data-table-td ">Image processing apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7570800">US7570800</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 14, 2005</td><td class="patent-data-table-td patent-date-value">Aug 4, 2009</td><td class="patent-data-table-td ">Kla-Tencor Technologies Corp.</td><td class="patent-data-table-td ">Methods and systems for binning defects detected on a specimen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7756884">US7756884</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 12, 2006</td><td class="patent-data-table-td patent-date-value">Jul 13, 2010</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Information processing apparatus, information processing method, and program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7878402">US7878402</a></td><td class="patent-data-table-td patent-date-value">Dec 20, 2005</td><td class="patent-data-table-td patent-date-value">Feb 1, 2011</td><td class="patent-data-table-td ">Cognex Technology And Investment Corporation</td><td class="patent-data-table-td ">Decoding distorted symbols</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7965899">US7965899</a></td><td class="patent-data-table-td patent-date-value">Jul 3, 2008</td><td class="patent-data-table-td patent-date-value">Jun 21, 2011</td><td class="patent-data-table-td ">Gognex Technology and Investment Corporation</td><td class="patent-data-table-td ">Methods for locating and decoding distorted two-dimensional matrix symbols</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8045788">US8045788</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 14, 2004</td><td class="patent-data-table-td patent-date-value">Oct 25, 2011</td><td class="patent-data-table-td ">August Technology Corp.</td><td class="patent-data-table-td ">Product setup sharing for multiple inspection systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8064686">US8064686</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 24, 2006</td><td class="patent-data-table-td patent-date-value">Nov 22, 2011</td><td class="patent-data-table-td ">Micro-Epsilon Messtechnik Gmbh &amp; Co. Kg</td><td class="patent-data-table-td ">Method and device for the contactless optical determination of the 3D position of an object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8111904">US8111904</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 7, 2005</td><td class="patent-data-table-td patent-date-value">Feb 7, 2012</td><td class="patent-data-table-td ">Cognex Technology And Investment Corp.</td><td class="patent-data-table-td ">Methods and apparatus for practical 3D vision system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8126260">US8126260</a></td><td class="patent-data-table-td patent-date-value">May 29, 2007</td><td class="patent-data-table-td patent-date-value">Feb 28, 2012</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">System and method for locating a three-dimensional object using machine vision</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8270749">US8270749</a></td><td class="patent-data-table-td patent-date-value">Jun 20, 2011</td><td class="patent-data-table-td patent-date-value">Sep 18, 2012</td><td class="patent-data-table-td ">Cognex Technology And Investment Corporation</td><td class="patent-data-table-td ">Method for locating and decoding distorted two-dimensional matrix symbols</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8315457">US8315457</a></td><td class="patent-data-table-td patent-date-value">Nov 12, 2008</td><td class="patent-data-table-td patent-date-value">Nov 20, 2012</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">System and method for performing multi-image training for pattern recognition and registration</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8322620">US8322620</a></td><td class="patent-data-table-td patent-date-value">Jan 6, 2011</td><td class="patent-data-table-td patent-date-value">Dec 4, 2012</td><td class="patent-data-table-td ">Cognex Technology And Investment Corporation</td><td class="patent-data-table-td ">Decoding distorted symbols</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8360251">US8360251</a></td><td class="patent-data-table-td patent-date-value">Oct 8, 2008</td><td class="patent-data-table-td patent-date-value">Jan 29, 2013</td><td class="patent-data-table-td ">Cummins Filtration Ip, Inc.</td><td class="patent-data-table-td ">Multi-layer coalescing media having a high porosity interior layer and uses thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8422777">US8422777</a></td><td class="patent-data-table-td patent-date-value">Oct 13, 2009</td><td class="patent-data-table-td patent-date-value">Apr 16, 2013</td><td class="patent-data-table-td ">Joshua Victor Aller</td><td class="patent-data-table-td ">Target and method of detecting, identifying, and determining 3-D pose of the target</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8687920">US8687920</a></td><td class="patent-data-table-td patent-date-value">May 18, 2009</td><td class="patent-data-table-td patent-date-value">Apr 1, 2014</td><td class="patent-data-table-td ">Ecole Polytechnique</td><td class="patent-data-table-td ">Method and device for the invariant-affine recognition of shapes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130121528">US20130121528</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 7, 2012</td><td class="patent-data-table-td patent-date-value">May 16, 2013</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Information presentation device, information presentation method, information presentation system, information registration device, information registration method, information registration system, and program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE112006002674T5?cl=en">DE112006002674T5</a></td><td class="patent-data-table-td patent-date-value">Oct 6, 2006</td><td class="patent-data-table-td patent-date-value">Sep 11, 2008</td><td class="patent-data-table-td ">Cognex Corp., Natick</td><td class="patent-data-table-td ">Verfahren und Vorrichtungen für praktisches 3D-Sichtigkeitssystem</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE112010002174T5?cl=en">DE112010002174T5</a></td><td class="patent-data-table-td patent-date-value">May 26, 2010</td><td class="patent-data-table-td patent-date-value">Oct 4, 2012</td><td class="patent-data-table-td ">Cognex Technology And Investment Corp.</td><td class="patent-data-table-td ">Verfahren und vorrichtung für ein praktisches 3d-sehsystem</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2008077538A1?cl=en">WO2008077538A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 18, 2007</td><td class="patent-data-table-td patent-date-value">Jul 3, 2008</td><td class="patent-data-table-td ">Fraunhofer Ges Forschung</td><td class="patent-data-table-td ">Method for picture mark supported image evaluation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2008104082A1?cl=en">WO2008104082A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 29, 2008</td><td class="patent-data-table-td patent-date-value">Sep 4, 2008</td><td class="patent-data-table-td ">John D Unsworth</td><td class="patent-data-table-td ">Methods, systems and devices for threedimensional input, and control methods and systems based thereon</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2009150361A2?cl=en">WO2009150361A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 18, 2009</td><td class="patent-data-table-td patent-date-value">Dec 17, 2009</td><td class="patent-data-table-td ">Ecole Polytechnique</td><td class="patent-data-table-td ">Method and device for the invariant affine recognition of shapes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2010042706A1?cl=en">WO2010042706A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 8, 2009</td><td class="patent-data-table-td patent-date-value">Apr 15, 2010</td><td class="patent-data-table-td ">Cummins Filtration Ip Inc.</td><td class="patent-data-table-td ">Multi-layer coalescing media having a high porosity interior layer and uses thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2010045271A1?cl=en">WO2010045271A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 13, 2009</td><td class="patent-data-table-td patent-date-value">Apr 22, 2010</td><td class="patent-data-table-td ">Joshua Victor Aller</td><td class="patent-data-table-td ">Target and method of detecting, identifying, and determining 3-d pose of the target</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2010138565A2?cl=en">WO2010138565A2</a></td><td class="patent-data-table-td patent-date-value">May 26, 2010</td><td class="patent-data-table-td patent-date-value">Dec 2, 2010</td><td class="patent-data-table-td ">Cognex Technology And Investment Corporation</td><td class="patent-data-table-td ">Methods and apparatus for practical 3d vision system</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=be5nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S151000">382/151</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=be5nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S215000">382/215</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=be5nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S289000">382/289</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=be5nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S216000">382/216</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=be5nBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009640000">G06K9/64</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=be5nBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0007000000">G06T7/00</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=be5nBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K2009/3225">G06K2009/3225</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=be5nBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/6203">G06K9/6203</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=be5nBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06T7/004">G06T7/004</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=be5nBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/3216">G06K9/3216</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06K9/62A1A</span>, <span class="nested-value">G06T7/00P</span>, <span class="nested-value">G06K9/32P</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Feb 3, 2012</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 31, 2012</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 6, 8-12 AND 14-17 IS CONFIRMED. CLAIMS 1-5, 7 AND 13 WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 21, 2009</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090615</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 1, 2008</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 29, 2001</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COGNEX CORPORATION, MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:WALLACK, AARON S.;REEL/FRAME:011842/0855</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20010524</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COGNEX CORPORATION ONE VISION DRIVENATICK, MASSACH</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:WALLACK, AARON S. /AR;REEL/FRAME:011842/0855</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U07Q6RG6xGC1Lho9hBhh5cnPjY-Cw\u0026id=be5nBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U34Whjsul6sktP88DhAUdZc-E6WEA\u0026id=be5nBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U1vbnD5mbPtq8wxEwdpLu1zCWJS0Q","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/System_and_method_for_registering_patter.pdf?id=be5nBAABERAJ\u0026output=pdf\u0026sig=ACfU3U1L5MCXUO9tkq4KoYDq9WcTtdZ4NA"},"sample_url":"http://www.google.com/patents/reader?id=be5nBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>