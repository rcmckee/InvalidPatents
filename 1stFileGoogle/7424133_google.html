<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7424133 - Method and apparatus for capturing, geolocating and measuring oblique images - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Method and apparatus for capturing, geolocating and measuring oblique images"><meta name="DC.contributor" content="Stephen L. Schultz" scheme="inventor"><meta name="DC.contributor" content="Frank D. Giuffrida" scheme="inventor"><meta name="DC.contributor" content="Robert L. Gray" scheme="inventor"><meta name="DC.contributor" content="Charles Mondello" scheme="inventor"><meta name="DC.contributor" content="Pictometry International Corporation" scheme="assignee"><meta name="DC.date" content="2003-11-5" scheme="dateSubmitted"><meta name="DC.description" content="A computerized system for displaying, geolocating, and taking measurements from captured oblique images includes a data file accessible by the computer system. The data file includes a plurality of image files corresponding to a plurality of captured oblique images, and positional data corresponding to the images. Image display and analysis software is executed by the system for reading the data file and displaying at least a portion of the captured oblique images. The software retrieves the positional data for one or more user-selected points on the displayed image, and calculates a separation distance between any two or more selected points. The separation distance calculation is user-selectable to determine various parameters including linear distance between, area encompassed within, relative elevation of, and height difference between selected points."><meta name="DC.date" content="2008-9-9" scheme="issued"><meta name="DC.relation" content="US:20010038718:A1" scheme="references"><meta name="DC.relation" content="US:20020041328:A1" scheme="references"><meta name="DC.relation" content="US:20020121969:A1" scheme="references"><meta name="DC.relation" content="US:20060238383:A1" scheme="references"><meta name="DC.relation" content="US:20070024612:A1" scheme="references"><meta name="DC.relation" content="US:20070046448:A1" scheme="references"><meta name="DC.relation" content="US:4758850" scheme="references"><meta name="DC.relation" content="US:5247356" scheme="references"><meta name="DC.relation" content="US:5251037" scheme="references"><meta name="DC.relation" content="US:5414462" scheme="references"><meta name="DC.relation" content="US:5467271" scheme="references"><meta name="DC.relation" content="US:5798786" scheme="references"><meta name="DC.relation" content="US:5844602" scheme="references"><meta name="DC.relation" content="US:5894323" scheme="references"><meta name="DC.relation" content="US:6088055" scheme="references"><meta name="DC.relation" content="US:6108032" scheme="references"><meta name="DC.relation" content="US:6130705" scheme="references"><meta name="DC.relation" content="US:6222583" scheme="references"><meta name="DC.relation" content="US:6256057" scheme="references"><meta name="DC.relation" content="US:6373522" scheme="references"><meta name="DC.relation" content="US:6731329" scheme="references"><meta name="DC.relation" content="US:6747686" scheme="references"><meta name="DC.relation" content="US:7009638" scheme="references"><meta name="DC.relation" content="WO:1999018732:A1" scheme="references"><meta name="citation_patent_number" content="US:7424133"><meta name="citation_patent_application_number" content="US:10/701,839"><link rel="canonical" href="http://www.google.com/patents/US7424133"/><meta property="og:url" content="http://www.google.com/patents/US7424133"/><meta name="title" content="Patent US7424133 - Method and apparatus for capturing, geolocating and measuring oblique images"/><meta name="description" content="A computerized system for displaying, geolocating, and taking measurements from captured oblique images includes a data file accessible by the computer system. The data file includes a plurality of image files corresponding to a plurality of captured oblique images, and positional data corresponding to the images. Image display and analysis software is executed by the system for reading the data file and displaying at least a portion of the captured oblique images. The software retrieves the positional data for one or more user-selected points on the displayed image, and calculates a separation distance between any two or more selected points. The separation distance calculation is user-selectable to determine various parameters including linear distance between, area encompassed within, relative elevation of, and height difference between selected points."/><meta property="og:title" content="Patent US7424133 - Method and apparatus for capturing, geolocating and measuring oblique images"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("0lTsU4DDAuepsQTIt4DIBA"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("CZE"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("0lTsU4DDAuepsQTIt4DIBA"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("CZE"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7424133?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7424133"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=YEVeBQABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7424133&amp;usg=AFQjCNHecj881NNx_9OCVtHo6KvMYie-4Q" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7424133.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7424133.pdf"></a><a class="appbar-application-grant-link" data-label="Application" href="/patents/US20040105090"></a><a class="appbar-application-grant-link" data-selected="true" data-label="Grant" href="/patents/US7424133"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7424133" style="display:none"><span itemprop="description">A computerized system for displaying, geolocating, and taking measurements from captured oblique images includes a data file accessible by the computer system. The data file includes a plurality of image files corresponding to a plurality of captured oblique images, and positional data corresponding...</span><span itemprop="url">http://www.google.com/patents/US7424133?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7424133 - Method and apparatus for capturing, geolocating and measuring oblique images</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7424133 - Method and apparatus for capturing, geolocating and measuring oblique images" title="Patent US7424133 - Method and apparatus for capturing, geolocating and measuring oblique images"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7424133 B2</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 10/701,839</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Sep 9, 2008</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Nov 5, 2003</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Nov 8, 2002</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CA2505566A1">CA2505566A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/CA2505566C">CA2505566C</a>, </span><span class="patent-bibdata-value"><a href="/patents/CA2821602A1">CA2821602A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/CA2821605A1">CA2821605A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/CA2821759A1">CA2821759A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/CA2821775A1">CA2821775A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/CA2821780A1">CA2821780A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN1735897A">CN1735897A</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN100585619C">CN100585619C</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN101793514A">CN101793514A</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN101793514B">CN101793514B</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE60306301D1">DE60306301D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE60306301T2">DE60306301T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1418402A1">EP1418402A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1418402B1">EP1418402B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2261600A2">EP2261600A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2261600A3">EP2261600A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7787659">US7787659</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7995799">US7995799</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8068643">US8068643</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8204341">US8204341</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8233666">US8233666</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8634594">US8634594</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20040105090">US20040105090</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20090096884">US20090096884</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20100302243">US20100302243</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20110091075">US20110091075</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20110091076">US20110091076</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20120020571">US20120020571</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20120288158">US20120288158</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20140009626">US20140009626</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO2004044692A2">WO2004044692A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO2004044692A3">WO2004044692A3</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">10701839, </span><span class="patent-bibdata-value">701839, </span><span class="patent-bibdata-value">US 7424133 B2, </span><span class="patent-bibdata-value">US 7424133B2, </span><span class="patent-bibdata-value">US-B2-7424133, </span><span class="patent-bibdata-value">US7424133 B2, </span><span class="patent-bibdata-value">US7424133B2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Stephen+L.+Schultz%22">Stephen L. Schultz</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Frank+D.+Giuffrida%22">Frank D. Giuffrida</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Robert+L.+Gray%22">Robert L. Gray</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Charles+Mondello%22">Charles Mondello</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Pictometry+International+Corporation%22">Pictometry International Corporation</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7424133.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7424133.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7424133.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (24),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (29),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (14),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (7)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7424133&usg=AFQjCNFhcTakl938g1bGTnNQnCihaaFTYQ">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7424133&usg=AFQjCNEB0hftmyLXwHjEO8bdTCNX2zPX7w">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7424133B2%26KC%3DB2%26FT%3DD&usg=AFQjCNEc6zKKdoMHJ7KH-UbUdRM_BLVoWw">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT67853351" lang="EN" load-source="patent-office">Method and apparatus for capturing, geolocating and measuring oblique images</invention-title></span><br><span class="patent-number">US 7424133 B2</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA51356745" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">A computerized system for displaying, geolocating, and taking measurements from captured oblique images includes a data file accessible by the computer system. The data file includes a plurality of image files corresponding to a plurality of captured oblique images, and positional data corresponding to the images. Image display and analysis software is executed by the system for reading the data file and displaying at least a portion of the captured oblique images. The software retrieves the positional data for one or more user-selected points on the displayed image, and calculates a separation distance between any two or more selected points. The separation distance calculation is user-selectable to determine various parameters including linear distance between, area encompassed within, relative elevation of, and height difference between selected points.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(11)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424133B2/US07424133-20080909-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424133B2/US07424133-20080909-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424133B2/US07424133-20080909-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424133B2/US07424133-20080909-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424133B2/US07424133-20080909-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424133B2/US07424133-20080909-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424133B2/US07424133-20080909-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424133B2/US07424133-20080909-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424133B2/US07424133-20080909-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424133B2/US07424133-20080909-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424133B2/US07424133-20080909-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424133B2/US07424133-20080909-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424133B2/US07424133-20080909-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424133B2/US07424133-20080909-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424133B2/US07424133-20080909-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424133B2/US07424133-20080909-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424133B2/US07424133-20080909-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424133B2/US07424133-20080909-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424133B2/US07424133-20080909-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424133B2/US07424133-20080909-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424133B2/US07424133-20080909-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424133B2/US07424133-20080909-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(42)</span></span></div><div class="patent-text"><div mxw-id="PCLM9448775" lang="EN" load-source="patent-office" class="claims">
    <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
      <div class="claim-text">1. A computerized system for displaying, geolocating, and making measurements based upon captured oblique images, comprising:
<div class="claim-text">a computer system having a memory;</div>
<div class="claim-text">an image and data file accessible by said system and including a plurality of image files corresponding to a plurality of captured oblique images, said image and data file further including positional data corresponding to said plurality of image files;</div>
<div class="claim-text">a ground plane data file representing a tessellated ground plane, said ground plane data file accessible by said computer system, said ground plane data file representing a tessellated ground plane that closely approximates at least a portion of the terrain depicted within said captured oblique images, said tessellated ground plane further comprising a plurality of interconnected facets with the size of the facets defined using a uniform number of pixels in the captured oblique images; and</div>
<div class="claim-text">image display and analysis software executed by said system for reading said image and data file and displaying at least a portion of the captured oblique images as a displayed oblique image, said software calculating the geo-location of one or more selected points within said displayed image, said software calculating a separation distance between any two or more selected points within said displayed image.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
      <div class="claim-text">2. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of said plurality of facets of the tessellated ground plane has a respective pitch and slope.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
      <div class="claim-text">3. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein said ground plane data file comprises a plurality of vertices, each of said plurality of vertices having respective elevations and defining corners of said plurality of interconnected facets, two of said plurality of vertices shared by each of said interconnected facets.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
      <div class="claim-text">4. The system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein said image display and analysis software identifies which of said plurality of facets corresponds to a selected point on said displayed image, and calculates an elevation of said selected point dependent at least in part upon the elevation of the vertices of the facet corresponding to the selected point, said image display and analysis software using said calculated elevation for calculating said separation distance between said selected point and one or more further selected points.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
      <div class="claim-text">5. The system of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein said image display and analysis software calculates a height of an object within said displayed image by calculating the separation distance between two or more selected points.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
      <div class="claim-text">6. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said tessellated ground plane is one of superimposed upon and fit to said displayed image.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
      <div class="claim-text">7. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said image display and analysis software includes user-selectable measuring modes accessible through at least one of pull-down menus, toolbars and keyboard commands.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
      <div class="claim-text">8. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of said images were captured by an image-capturing device and at respective image capturing events, said positional data of said image and data file including:
<div class="claim-text">time data representing the time of each image-capturing event; location data representing the location of the image-capturing device at each image-capturing event;</div>
<div class="claim-text">orientation data representing the orientation of the image-capturing device at each image-capturing event;</div>
<div class="claim-text">correction data representing correction factors for the image-capturing device; and</div>
<div class="claim-text">elevation data representing an average elevation of the surface captured by the image-capturing device.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
      <div class="claim-text">9. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein said location data includes latitude, longitude, and altitude of the image-capturing apparatus at each image-capturing event.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
      <div class="claim-text">10. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein said orientation data includes roll, pitch, yaw and heading of said image-capturing device at each image-capturing event.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
      <div class="claim-text">11. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein said image-capturing device is a camera and said correction data includes at least one of focal length, sensor size, aspect ratio, principle point offset, distortion, and pixel pitch.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00012" num="00012" class="claim">
      <div class="claim-text">12. A computerized method for taking measurements within a displayed oblique image, comprising:
<div class="claim-text">selecting with an input device a starting point and an end point on the displayed image;</div>
<div class="claim-text">retrieving from a data file positional data corresponding to said starting point and said end point;</div>
<div class="claim-text">referencing a ground plane data file corresponding to a tessellated ground plane having a plurality of facets, each of said facets having a respective pitch and slope, said tessellated ground plane closely matching a terrain of said displayed oblique image;</div>
<div class="claim-text">connecting said starting and end points with line segments, said line segments conforming to said pitch and slope of said facets to thereby follow said terrain; and</div>
<div class="claim-text">calculating the linear distance along said line segments between said starting and end points thereby taking into account said pitch and slope of said facets.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
      <div class="claim-text">13. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein said tessellated ground plane is superimposed upon said displayed oblique image.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
      <div class="claim-text">14. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, comprising the further steps of:
<div class="claim-text">selecting with an input device one or more intermediate points on the displayed image;</div>
<div class="claim-text">retrieving from said data file positional data corresponding to said one or more intermediate points; and</div>
<div class="claim-text">connecting adjacent intermediate points to each other, and connecting said starting and end points to adjacent intermediate points, with line segments, said line segments conforming to said pitch and slope of said facets to thereby follow said terrain; and</div>
<div class="claim-text">calculating the distance along said line segments between said starting and end points.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
      <div class="claim-text">15. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein said plurality of facets each correspond to equal areas of said displayed oblique image.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
      <div class="claim-text">16. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein said plurality of facets each includes an equal number of pixels of said displayed oblique image.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00017" num="00017" class="claim">
      <div class="claim-text">17. A computerized method for taking measurements from an oblique image displayed on a computer system, at least one input device connected to said computer system, an image data file accessible by said computer system, said image data file including captured images and positional data corresponding thereto, said computerized method comprising:
<div class="claim-text">placing the computer system into a desired one of a plurality of measurement modes, the desired measurement mode configured for calculating a desired measurement;</div>
<div class="claim-text">selecting a starting point on the displayed image;</div>
<div class="claim-text">retrieving the positional data corresponding to said starting point;</div>
<div class="claim-text">selecting an end point on the displayed image;</div>
<div class="claim-text">retrieving the positional data corresponding to said end point; and</div>
<div class="claim-text">calculating the desired measurement dependent at least in part upon said positional data of said starting and end points;</div>
<div class="claim-text">wherein said plurality of measurement modes comprise a distance measuring mode calculating a distance between two or more selected points, a height measuring mode calculating a height difference between two or more selected points, a relative elevation measurement mode calculating the difference in elevation of two or more selected points, and an area measurement mode calculating the area encompassed by at least three points.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
      <div class="claim-text">18. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, comprising the further steps of:
<div class="claim-text">selecting one or more intermediate points on said displayed image; and</div>
<div class="claim-text">retrieving the positional data corresponding to said intermediate points.</div>
</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00019" num="00019" class="claim">
      <div class="claim-text">19. A method of capturing oblique images of an area of interest with an image-capturing device carried by a platform, each oblique image captured at a respective image-capturing event, said method comprising:
<div class="claim-text">subdividing the area of interest into a plurality of sectors; guiding the platform along a first path to thereby target one or more target sectors with the image-capturing device;</div>
<div class="claim-text">capturing with the image-capturing device one or more oblique images to thereby cover an entirety of each said target sector in oblique images captured from a first perspective;</div>
<div class="claim-text">guiding the platform along a second path to thereby target said target sectors; capturing with the image-capturing device one or more oblique images to thereby cover an entirety of each said target sector in oblique images captured from a second perspective;</div>
<div class="claim-text">repeating said guiding and capturing steps along paths substantially parallel to and spaced apart from said first and second paths and capturing one or more oblique images to thereby cover an entirety of each of said plurality of sectors in oblique images captured from each of said first and second perspectives; and</div>
<div class="claim-text">recording positional data indicative of a geo-location of said image-capturing device at each image-capturing event.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
      <div class="claim-text">20. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein said second path is substantially parallel relative to and 180° (one-hundred and eighty degrees) from said first path.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00021" num="00021" class="claim">
      <div class="claim-text">21. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein said second path is also spaced apart from said first path.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00022" num="00022" class="claim">
      <div class="claim-text">22. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, comprising the further steps of:
<div class="claim-text">guiding the platform along a third path to thereby target one or more target sectors with the image-capturing device, said third path being substantially perpendicular to said first and second paths;</div>
<div class="claim-text">capturing with the image-capturing device one or more oblique images to thereby capture an entirety of each said target sector in oblique images captured from a third perspective; and</div>
<div class="claim-text">repeating said guiding and capturing steps along paths substantially parallel to and spaced apart from said third path and capturing one or more oblique images to thereby cover an entirety of each of said plurality of sectors in oblique images captured from said third perspective.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00023" num="00023" class="claim">
      <div class="claim-text">23. The method of <claim-ref idref="CLM-00022">claim 22</claim-ref>, comprising the further steps of:
<div class="claim-text">guiding the platform along a fourth path to thereby target one or more target sectors with the image-capturing device, said fourth path being substantially parallel with said third path and 180° (one-hundred and eighty degrees) from said third path;</div>
<div class="claim-text">capturing with the image-capturing device one or more oblique images to thereby capture an entirety of each said target sector in oblique images captured from a fourth perspective; and</div>
<div class="claim-text">repeating said guiding and capturing steps along paths substantially parallel to and spaced apart from said fourth path and capturing one or more oblique images to thereby cover an entirety of each of said plurality of sectors in oblique images captured from said fourth perspective.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00024" num="00024" class="claim">
      <div class="claim-text">24. The method of <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein said fourth path is also spaced apart from said third path.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00025" num="00025" class="claim">
      <div class="claim-text">25. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<div class="claim-text">an image-capturing device, said image-capturing device capturing oblique images at image-capturing events, said image capturing device issuing image-data signals corresponding to captured images;</div>
<div class="claim-text">at least one geo-locating device, each said at least one geo-locating device issuing a corresponding at least one geo-locating signal, each said at least one geo-locating signal being indicative at least in part of a geo-location of said image-capturing device during each image capturing event; and</div>
<div class="claim-text">wherein said computer system receives and stores said image-data signals and said at least one geo-locating signal; and</div>
<div class="claim-text">wherein said image display and analysis software reads said image-data signals and said at least one geo-locating signal, said software associating each said image-data signal with a corresponding said at least one geo-locating signal for each image-capturing event.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00026" num="00026" class="claim">
      <div class="claim-text">26. The system of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein said at least one geo-locating device and said at least one geo-locating signal respectively comprise at least one of:
<div class="claim-text">a clock issuing to said image-capturing computer system time data signals indicative of a time of each said image-capturing event;</div>
<div class="claim-text">a global-positioning system (GPS) receiver receiving GPS signals and issuing to said image-capturing computer system location data signals indicative of a longitude and latitude of said image-capturing device at each said image-capturing event;</div>
<div class="claim-text">an inertial navigation unit (INU) issuing to said image-capturing computer system velocity data signals indicative of a velocity of said image-capturing device at each said image-capturing event;</div>
<div class="claim-text">a gyroscope issuing to said image-capturing computer system a pitch signal, a roll signal, and a yaw signal respectively indicative of a pitch, roll and yaw of said image capturing device at each said image-capturing event;</div>
<div class="claim-text">a compass issuing to said image-capturing computer system heading data signals indicative of a heading of said image-capturing device at each said image-capturing event; and</div>
<div class="claim-text">an altimeter issuing to said image-capturing computer system altitude data signals indicative of an altitude of said image-capturing device at each said image-capturing event.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00027" num="00027" class="claim">
      <div class="claim-text">27. The system of <claim-ref idref="CLM-00025">claim 25</claim-ref>, further comprising correction data indicative of characteristics of said image-capturing device including focal length, sensor size, radial distortion, principal point offset and alignment, said image display and analysis software utilizing said correction data to correct captured images.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00028" num="00028" class="claim">
      <div class="claim-text">28. The system of <claim-ref idref="CLM-00025">claim 25</claim-ref>, further comprising an output data file created by said image display and analysis software, said output data file including a plurality of image files and positional data corresponding to each of said plurality of image files.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00029" num="00029" class="claim">
      <div class="claim-text">29. The system of <claim-ref idref="CLM-00025">claim 25</claim-ref>, further comprising a platform carrying said image-capturing device a predetermined distance above a surface of interest.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00030" num="00030" class="claim">
      <div class="claim-text">30. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<div class="claim-text">an input device for selecting a starting point and an end point on the displayed image; and</div>
<div class="claim-text">wherein said image display and analysis software:
<div class="claim-text">retrieves from a data file positional data corresponding to said starting point and said end point;</div>
<div class="claim-text">references a ground plane data file corresponding to a tessellated ground plane having a plurality of facets, each of said facets having a respective pitch and slope, said tessellated ground plane closely matching a terrain of said displayed oblique image;</div>
<div class="claim-text">connects said starting and end points with line segments, said line segments conforming to said pitch and slope of said facets to thereby follow said terrain; and</div>
<div class="claim-text">calculates the linear distance along said line segments between said starting and end points thereby taking into account said pitch and slope of said facets.</div>
</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00031" num="00031" class="claim">
      <div class="claim-text">31. The system of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein said image display and analysis software superimposes said tessellated ground plane upon said displayed oblique image.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00032" num="00032" class="claim">
      <div class="claim-text">32. The system of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein:
<div class="claim-text">said input device selects-one or more intermediate points on the displayed image;</div>
<div class="claim-text">said image display and analysis software:
<div class="claim-text">retrieves from said data file positional data corresponding to said one or more intermediate points; and</div>
<div class="claim-text">connects adjacent intermediate points to each other, and connecting said starting and end points to adjacent intermediate points, with line segments, said line segments conforming to said pitch and slope of said facets to thereby follow said terrain; and</div>
<div class="claim-text">calculates the distance along said line segments between said starting and end points.</div>
</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00033" num="00033" class="claim">
      <div class="claim-text">33. The system of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein said plurality of facets each correspond to equal areas of said displayed oblique image.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00034" num="00034" class="claim">
      <div class="claim-text">34. The system of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein said plurality of facets each includes an equal number of pixels of said displayed oblique image.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00035" num="00035" class="claim">
      <div class="claim-text">35. The system of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein said input device selects one or more intermediate points on said displayed image, and retrieves the positional data corresponding to said intermediate points.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00036" num="00036" class="claim">
      <div class="claim-text">36. The system of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein said separation distance comprises a distance between two or more selected points, a height difference between two or more selected points, difference in elevation of two or more selected points, and an area encompassed by at least three points.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00037" num="00037" class="claim">
      <div class="claim-text">37. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein said image capturing device captures oblique images of an area of interest by:
<div class="claim-text">subdividing the area of interest into a plurality of sectors; guiding the platform along a first path to thereby target one or more target sectors with the image-capturing device;</div>
<div class="claim-text">capturing with the image-capturing device one or more oblique images to thereby cover an entirety of each said target sector in oblique images captured from a first perspective;</div>
<div class="claim-text">guiding the platform along a second path to thereby target said target sectors; capturing with the image-capturing device one or more oblique images to thereby cover an entirety of each said target sector in oblique images captured from a second perspective;</div>
<div class="claim-text">repeating said guiding and capturing steps along paths substantially parallel to and spaced apart from said first and second paths and capturing one or more oblique images to thereby cover an entirety of each of said plurality of sectors in oblique images captured from each of said first and second perspectives; and</div>
<div class="claim-text">recording positional data indicative of a geo-location of said image-capturing device at each image-capturing event.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00038" num="00038" class="claim">
      <div class="claim-text">38. The system of <claim-ref idref="CLM-00037">claim 37</claim-ref>, wherein said second path is substantially parallel relative to and 180° (one-hundred and eighty degrees) from said first path.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00039" num="00039" class="claim">
      <div class="claim-text">39. The system of <claim-ref idref="CLM-00038">claim 38</claim-ref>, wherein said second path is also spaced apart from said first path.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00040" num="00040" class="claim">
      <div class="claim-text">40. The system of <claim-ref idref="CLM-00037">claim 37</claim-ref>, wherein:
<div class="claim-text">said platform is guided along a third path to thereby target one or more target sectors with the image-capturing device, said third path being substantially perpendicular to said first and second paths;</div>
<div class="claim-text">said image capturing device captures one or more oblique images to thereby capture an entirety of each said target sector in oblique images captured from a third perspective; and</div>
<div class="claim-text">repeating said guiding and capturing steps along paths substantially parallel to and spaced apart from said third path and capturing one or more oblique images to thereby cover an entirety of each of said plurality of sectors in oblique images captured from said third perspective.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00041" num="00041" class="claim">
      <div class="claim-text">41. The system of <claim-ref idref="CLM-00040">claim 40</claim-ref>, wherein:
<div class="claim-text">said platform is guided along a fourth path to thereby target one or more target sectors with the image-capturing device, said fourth path being substantially parallel with said third path and 180° (one-hundred and eighty degrees) from said third path;</div>
<div class="claim-text">said image capturing devices capturing with the image-capturing device one or more oblique images to thereby capture an entirety of each said target sector in oblique images captured from a fourth perspective; and</div>
<div class="claim-text">repeating said guiding and capturing steps along paths substantially parallel to and spaced apart from said fourth path and capturing one or more oblique images to thereby cover an entirety of each of said plurality of sectors in oblique images captured from said fourth perspective.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00042" num="00042" class="claim">
      <div class="claim-text">42. The system of <claim-ref idref="CLM-00041">claim 41</claim-ref>, wherein said fourth path is also spaced apart from said third path.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES16471319" lang="EN" load-source="patent-office" class="description">
    <heading>CROSS-REFERENCE TO RELATED APPLICATIONS</heading> <p num="p-0002">This application claims the benefit of U.S. Provisional Patent Application Ser. No. 60/425,275, filed Nov. 8, 2002.</p>
    <heading>TECHNICAL FIELD</heading> <p num="p-0003">The present invention relates to photogrammetry. More particularly, the present invention relates to a method and apparatus for capturing oblique images and for measuring the objects and distances between the objects depicted therein.</p>
    <heading>BACKGROUND</heading> <p num="p-0004">Photogrammetry is the science of making measurements of and between objects depicted within photographs, especially aerial photographs. Generally, photogrammetry involves taking images of terrestrial features and deriving data therefrom, such as, for example, data indicating relative distances between and sizes of objects within the images. Photogrammetry may also involve coupling the photographs with other data, such as data representative of latitude and longitude. In effect, the image is overlaid and conformed to a particular spatial coordinate system.</p>
    <p num="p-0005">Conventional photogrammetry involves the capture and/or acquisition of orthogonal images. The image-capturing device, such as a camera or sensor, is carried by a vehicle or platform, such as an airplane or satellite, and is aimed at a nadir point that is directly below and/or vertically downward from that platform. The point or pixel in the image that corresponds to the nadir point is the only point/pixel that is truly orthogonal to the image-capturing device. All other points or pixels in the image are actually oblique relative to the image-capturing device. As the points or pixels become increasingly distant from the nadir point they become increasingly oblique relative to the image-capturing device and the ground sample distance (i.e., the surface area corresponding to or covered by each pixel) also increases. Such obliqueness in an orthogonal image causes features in the image to be distorted, especially images relatively distant from the nadir point.</p>
    <p num="p-0006">Such distortion is removed, or compensated for, by the process of ortho-rectification which, in essence, removes the obliqueness from the orthogonal image by fitting or warping each pixel of an orthogonal image onto an orthometric grid or coordinate system. The process of ortho-rectification creates an image wherein all pixels have the same ground sample distance and are oriented to the north. Thus, any point on an ortho-rectified image can be located using an X, Y coordinate system and, so long as the image scale is known, the length and width of terrestrial features as well as the relative distance between those features can be calculated.</p>
    <p num="p-0007">Although the process of ortho-rectification compensates to a degree for oblique distortions in an orthogonal image, it introduces other undesirable distortions and/or inaccuracies in the ortho-rectified orthogonal image. Objects depicted in ortho-rectified orthogonal images may be difficult to recognize and/or identify since most observers are not accustomed to viewing objects, particularly terrestrial features, from above. To an untrained observer an ortho-rectified image has a number of distortions. Roads that are actually straight appear curved and buildings may appear to tilt. Further, ortho-rectified images contain substantially no information as to the height of terrestrial features. The interpretation and analysis of orthogonal and/or ortho-rectified orthogonal images is typically performed by highly-trained analysts whom have undergone years of specialized training and experience in order to identify objects and terrestrial features in such images.</p>
    <p num="p-0008">Thus, although orthogonal and ortho-rectified images are useful in photogrammetry, they lack information as to the height of features depicted therein and require highly-trained analysts to interpret detail from what the images depict.</p>
    <p num="p-0009">Oblique images are images that are captured with the image-capturing device aimed or pointed generally to the side of and downward from the platform that carries the image-capturing device. Oblique images, unlike orthogonal images, display the sides of terrestrial features, such as houses, buildings and/or mountains, as well as the tops thereof. Thus, viewing an oblique image is more natural and intuitive than viewing an orthogonal or ortho-rectified image, and even casual observers are able to recognize and interpret terrestrial features and other objects depicted in oblique images. Each pixel in the foreground of an oblique image corresponds to a relatively small area of the surface or object depicted (i.e., each foreground pixel has a relatively small ground sample distance) whereas each pixel in the background corresponds to a relatively large area of the surface or object depicted (i.e., each background pixel has a relatively large ground sample distance). Oblique images capture a generally trapezoidal area or view of the subject surface or object, with the foreground of the trapezoid having a substantially smaller ground sample distance (i.e., a higher resolution) than the background of the trapezoid.</p>
    <p num="p-0010">Oblique images are considered to be of little or no use in photogrammetry. The conventional approach of forcing the variously-sized foreground and background pixels of an oblique image into a uniform size to thereby warp the image onto a coordinate system dramatically distorts the oblique image and thereby renders identification of objects and the taking of measurements of objects depicted therein a laborious and inaccurate task. Correcting for terrain displacement within an oblique image by using an elevation model further distorts the images thereby increasing the difficulty with which measurements can be made and reducing the accuracy of any such measurements.</p>
    <p num="p-0011">Thus, although oblique images are considered as being of little or no use in photogrammetry, they are easily interpreted and contain information as to the height of features depicted therein.</p>
    <p num="p-0012">Therefore, what is needed in the art is a method and apparatus for photogrammetry that enable geo-location and accurate measurements within oblique images.</p>
    <p num="p-0013">Moreover, what is needed in the art is a method and apparatus for photogrammetry that enable the measurement of heights and relative heights of objects within an image.</p>
    <p num="p-0014">Furthermore, what is needed in the art is a method and apparatus for photogrammetry that utilizes more intuitive and natural images.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p num="p-0015">The present invention provides a method and apparatus for capturing, displaying, and making measurements of objects and distances between objects depicted within oblique images.</p>
    <p num="p-0016">The present invention comprises, in one form thereof, a computerized system for displaying, geolocating, and taking measurements from captured oblique images. The system includes a data file accessible by the computer system. The data file includes a plurality of image files corresponding to a plurality of captured oblique images, and positional data corresponding to the images. Image display and analysis software is executed by the system for reading the data file and displaying at least a portion of the captured oblique images. The software retrieves the positional data for one or more user-selected points on the displayed image, and calculates a separation distance between any two or more selected points. The separation distance calculation is user-selectable to determine various parameters including linear distance between, area encompassed within, relative elevation of, and height difference between selected points.</p>
    <description-of-drawings> <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p num="p-0017">The above-mentioned and other features and advantages of this invention, and the manner of attaining them, will become apparent and be more completely understood by reference to the following description of one embodiment of the invention when read in conjunction with the accompanying drawings, wherein:</p>
      <p num="p-0018"> <figref idrefs="DRAWINGS">FIG. 1</figref> illustrates one embodiment of a platform or vehicle carrying an image-capturing system of the present invention, and shows exemplary orthogonal and oblique images taken thereby;</p>
      <p num="p-0019"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a diagrammatic view of the image-capturing system of <figref idrefs="DRAWINGS">FIG. 1</figref>;</p>
      <p num="p-0020"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a block diagram of the image-capturing computer system of <figref idrefs="DRAWINGS">FIG. 2</figref>;</p>
      <p num="p-0021"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a representation of an exemplary output data file of the image-capturing system of <figref idrefs="DRAWINGS">FIG. 1</figref>;</p>
      <p num="p-0022"> <figref idrefs="DRAWINGS">FIG. 5</figref> is a block diagram of one embodiment of an image display and measurement computer system of the present invention for displaying and taking measurements of and between objects depicted in the images captured by the image-capturing system of <figref idrefs="DRAWINGS">FIG. 1</figref>;</p>
      <p num="p-0023"> <figref idrefs="DRAWINGS">FIG. 6</figref> depicts an exemplary image displayed on the system of <figref idrefs="DRAWINGS">FIG. 5</figref>, and illustrates one embodiment of the method of the present invention for the measurement of and between objects depicted in such an image;</p>
      <p num="p-0024"> <figref idrefs="DRAWINGS">FIGS. 7 and 8</figref> illustrate one embodiment of a method for capturing oblique images of the present invention;</p>
      <p num="p-0025"> <figref idrefs="DRAWINGS">FIGS. 9 and 10</figref> illustrate a second embodiment of a method for capturing oblique images of the present invention.</p>
    </description-of-drawings> <p num="p-0026">Corresponding reference characters indicate corresponding parts throughout the several views. The exemplifications set out herein illustrate one preferred embodiment of the invention, in one form, and such exemplifications are not to be construed as limiting the scope of the invention in any manner.</p>
    <heading>DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading> <p num="p-0027">Referring now to the drawings, and particularly to <figref idrefs="DRAWINGS">FIG. 1</figref>, one embodiment of an apparatus for capturing and geolocating oblique images of the present invention is shown. Apparatus <b>10</b> includes a platform or vehicle <b>20</b> that carries image-capturing and geolocating system <b>30</b>.</p>
    <p num="p-0028">Platform <b>20</b>, such as, for example, an airplane, space shuttle, rocket, satellite, or any other suitable vehicle, carries image-capturing system <b>30</b> over a predefined area of and at one or more predetermined altitudes above surface <b>31</b>, such as, for example, the earth's surface or any other surface of interest. As such, platform <b>20</b> is capable of controlled movement or flight, either manned or unmanned, along a predefined flight path or course through, for example, the earth's atmosphere or outer space. Image-capturing platform <b>20</b> includes a system for generating and regulating power (not shown) that includes, for example, one or more generators, fuel cells, solar panels, and/or batteries, for powering image-capturing system <b>30</b>.</p>
    <p num="p-0029">Image-capturing and geo-locating system <b>30</b>, as best shown in <figref idrefs="DRAWINGS">FIG. 2</figref>, includes image capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b</i>, a global positioning system (GPS) receiver <b>34</b>, an inertial navigation unit (INU) <b>36</b>, clock <b>38</b>, gyroscope <b>40</b>, compass <b>42</b> and altimeter <b>44</b>, each of which are interconnected with image-capturing computer system <b>46</b>.</p>
    <p num="p-0030">Image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b</i>, such as, for example, conventional cameras, digital cameras, digital sensors, charge-coupled devices, or other suitable image-capturing devices, are capable of capturing images photographically or electronically. Image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>have known or determinable characteristics including focal length, sensor size and aspect ratio, radial and other distortion terms, principal point offset, pixel pitch, and alignment. Image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>acquire images and issue image data signals (IDS) <b>48</b> <i>a </i>and <b>48</b> <i>b</i>, respectively, corresponding to the particular images or photographs taken and which are stored in image-capturing computer system <b>46</b>, as will be more particularly described hereinafter.</p>
    <p num="p-0031">As best shown in <figref idrefs="DRAWINGS">FIG. 1</figref>, image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>have respective central axes A<sub>1 </sub>and A<sub>2</sub>, and are mounted to platform <b>20</b> such that axes A<sub>1 </sub>and A<sub>2 </sub>are each at an angle of declination θ relative to a horizontal plane P. Declination angle θ is virtually any oblique angle, but is preferably from approximately 20° (twenty degrees) to approximately 60° (sixty degrees) and is most preferably from approximately 40° (forty degrees) to approximately 50° (fifty degrees).</p>
    <p num="p-0032">GPS receiver <b>34</b> receives global positioning system signals <b>52</b> that are transmitted by one or more global positioning system satellites <b>54</b>. The GPS signals <b>52</b>, in known fashion, enable the precise location of platform <b>20</b> relative to surface <b>31</b> to be determined. GPS receiver <b>34</b> decodes GPS signals <b>52</b> and issues location signals/data <b>56</b>, that are dependent at least in part upon GPS signals <b>52</b> and which are indicative of the precise location of platform <b>20</b> relative to surface <b>31</b>. Location signals/data <b>56</b> corresponding to each image captured by image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>are received and stored by image-capturing computer system <b>46</b>.</p>
    <p num="p-0033">INU <b>36</b> is a conventional inertial navigation unit that is coupled to and detects changes in the velocity, including translational and rotational velocity, of image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>and/or platform <b>20</b>. INU <b>36</b> issues velocity signals/data <b>58</b> indicative of such velocities and/or changes therein to image-capturing computer system <b>46</b>, which stores velocity signals/data <b>58</b> corresponding to each image captured by image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>are received and stored by image-capturing computer system <b>46</b>.</p>
    <p num="p-0034">Clock <b>38</b> keeps a precise time measurement (time of validity) that is used to synchronize events within image-capturing and geo-locating system <b>30</b>. Clock <b>38</b> provides time data/clock signal <b>62</b> that is indicative of the precise time that an image is taken by image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b</i>. Time data <b>62</b> is also provided to and stored by image-capturing computer system <b>46</b>. Alternatively, clock <b>38</b> is integral with image-capturing computer system <b>46</b>, such as, for example, a clock software program.</p>
    <p num="p-0035">Gyroscope <b>40</b> is a conventional gyroscope as commonly found on airplanes and/or within commercial navigation systems for airplanes. Gyroscope <b>40</b> provides signals including pitch signal <b>64</b>, roll signal <b>66</b> and yaw signal <b>68</b>, which are respectively indicative of pitch, roll and yaw of platform <b>20</b>. Pitch signal <b>64</b>, roll signal <b>66</b> and yaw signal <b>68</b> corresponding to each image captured by image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>are received and stored by image-capturing computer system <b>46</b>.</p>
    <p num="p-0036">Compass <b>42</b>, such as, for example, a conventional electronic compass, indicates the heading of platform <b>20</b>. Compass <b>42</b> issues heading signal/data <b>72</b> that is indicative of the heading of platform <b>20</b>. Image-capturing computer system <b>46</b> receives and stores the heading signals/data <b>72</b> that correspond to each image captured by image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b. </i> </p>
    <p num="p-0037">Altimeter <b>44</b> indicates the altitude of platform <b>20</b>. Altimeter <b>44</b> issues altitude signal/data <b>74</b>, and image-capturing computer system <b>46</b> receives and stores the altitude signal/data <b>74</b> that correspond to each image captured by image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b. </i> </p>
    <p num="p-0038">As best shown in <figref idrefs="DRAWINGS">FIG. 3</figref>, image-capturing computer system <b>46</b>, such as, for example, a conventional laptop personal computer, includes memory <b>82</b>, input devices <b>84</b> <i>a </i>and <b>84</b> <i>b</i>, display device <b>86</b>, and input and output (I/O) ports <b>88</b>. Image-capturing computer system <b>46</b> executes image and data acquiring software <b>90</b>, which is stored in memory <b>82</b>. Memory <b>82</b> also stores data used and/or calculated by image-capturing computer system <b>46</b> during the operation thereof, and includes, for example, non-volatile read-only memory, random access memory, hard disk memory, removable memory cards and/or other suitable memory storage devices and/or media. Input devices <b>84</b> <i>a </i>and <b>84</b> <i>b</i>, such as, for example, a mouse, keyboard, joystick, or other such input devices, enable the input of data and interaction of a user with software being executed by image-capturing computer system <b>46</b>. Display device <b>86</b>, such as, for example, a liquid crystal display or cathode ray tube, displays information to the user of image-capturing computer system <b>46</b>. I/O ports <b>88</b>, such as, for example, serial and parallel data input and output ports, enable the input and/or output of data to and from image-capturing computer system <b>46</b>.</p>
    <p num="p-0039">Each of the above-described data signals is connected to image-capturing computer system <b>46</b>. More particularly, image data signals <b>48</b>, location signals <b>56</b>, velocity signals <b>58</b>, time data signal <b>62</b>, pitch, roll and yaw signals <b>64</b>, <b>66</b> and <b>68</b>, respectively, heading signal <b>72</b> and altitude signal <b>74</b> are received via I/O ports <b>88</b> by and stored within memory <b>82</b> of image-capturing computer system <b>46</b>.</p>
    <p num="p-0040">In use, image-capturing computer system <b>46</b> executes image and data acquiring software <b>90</b>, which, in general, controls the reading, manipulation, and storing of the above-described data signals. More particularly, image and data acquiring software <b>90</b> reads image data signals <b>48</b> <i>a </i>and <b>48</b> <i>b </i>and stores them within memory <b>82</b>. Each of the location signals <b>56</b>, velocity signals <b>58</b>, time data signal <b>62</b>, pitch, roll and yaw signals <b>64</b>, <b>66</b> and <b>68</b>, respectively, heading signal <b>72</b> and altitude signal <b>74</b> that represent the conditions existing at the instant an image is acquired or captured by image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>and which correspond to the particular image data signals <b>48</b> <i>a </i>and <b>48</b> <i>b </i>representing the captured images are received by image-capturing computer system <b>46</b> via I/O ports <b>88</b>. Image-capturing computer system <b>46</b> executing image and data acquiring software <b>90</b> issues image-capture signal <b>92</b> to image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>to thereby cause those devices to acquire or capture an image at predetermined locations and/or at predetermined intervals which are dependent at least in part upon the velocity of platform <b>20</b>.</p>
    <p num="p-0041">Image and data acquiring software <b>90</b> decodes as necessary and stores the aforementioned signals within memory <b>82</b>, and associates the data signals with the corresponding image signals <b>48</b> <i>a </i>and <b>48</b> <i>b</i>. Thus, the altitude, orientation in terms of roll, pitch, and yaw, and the location of image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>relative to surface <b>31</b>, i.e., longitude and latitude, for every image captured by image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>is known.</p>
    <p num="p-0042">Platform <b>20</b> is piloted or otherwise guided through an image-capturing path that passes over a particular area of surface <b>31</b>, such as, for example, a predefined area of the surface of the earth or of another planet. Preferably, the image-capturing path of platform <b>20</b> is at right angles to at least one of the boundaries of the area of interest. The number of times platform <b>20</b> and/or image-capturing devices <b>32</b> <i>a</i>, <b>32</b> <i>b </i>pass over the area of interest is dependent at least in part upon the size of the area and the amount of detail desired in the captured images. The particular details of the image-capturing path of platform <b>20</b> are described more particularly hereinafter.</p>
    <p num="p-0043">As platform <b>20</b> passes over the area of interest a number of oblique images are captured by image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b</i>. As will be understood by those of ordinary skill in the art, images are captured or acquired by image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>at predetermined image capture intervals which are dependent at least in part upon the velocity of platform <b>20</b>.</p>
    <p num="p-0044">Image data signals <b>48</b> <i>a </i>and <b>48</b> <i>b </i>corresponding to each image acquired are received by and stored within memory <b>82</b> of image-capturing computer system <b>46</b> via I/O ports <b>88</b>. Similarly, the data signals (i.e., image data signals <b>48</b>, location signals <b>56</b>, velocity signals <b>58</b>, time data signal <b>62</b>, pitch, roll and yaw signals <b>64</b>, <b>66</b> and <b>68</b>, respectively, heading signal <b>72</b> and altitude signal <b>74</b>) corresponding to each captured image are received and stored within memory <b>82</b> of image-capturing computer system <b>46</b> via I/O ports <b>88</b>. Thus, the location of image-capturing device <b>32</b> <i>a </i>and <b>32</b> <i>b </i>relative to surface <b>32</b> at the precise moment each image is captured is recorded within memory <b>82</b> and associated with the corresponding captured image.</p>
    <p num="p-0045">As best shown in <figref idrefs="DRAWINGS">FIG. 1</figref>, the location of image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>relative to the earth corresponds to the nadir point N of orthogonal image <b>102</b>. Thus, the exact geo-location of the nadir point N of orthogonal image <b>102</b> is indicated by location signals <b>56</b>, velocity signals <b>58</b>, time data signal <b>62</b>, pitch, roll and yaw signals <b>64</b>, <b>66</b> and <b>68</b>, respectively, heading signal <b>72</b> and altitude signal <b>74</b>. Once the nadir point N of orthogonal image <b>102</b> is known, the geo-location of any other pixel or point within image <b>102</b> is determinable in known manner.</p>
    <p num="p-0046">When image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>are capturing oblique images, such as oblique images <b>104</b> <i>a </i>and <b>104</b> <i>b </i>(<figref idrefs="DRAWINGS">FIG. 1</figref>), the location of image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b </i>relative to surface <b>31</b> is similarly indicated by location signals <b>56</b>, velocity signals <b>58</b>, time data signal <b>62</b>, pitch, roll and yaw signals <b>64</b>, <b>66</b> and <b>68</b>, respectively, heading signal <b>72</b>, altitude signal <b>74</b> and the known angle of declination θ of the primary axes A<sub>1 </sub>and A<sub>2 </sub>of image-capturing devices <b>32</b> <i>a </i>and <b>32</b> <i>b</i>, respectively.</p>
    <p num="p-0047">It should be particularly noted that a calibration process enables image and data acquiring software <b>90</b> to incorporate correction factors and/or correct for any error inherent in or due to image-capturing device <b>32</b>, such as, for example, error due to calibrated focal length, sensor size, radial distortion, principal point offset, and alignment.</p>
    <p num="p-0048">Image and data acquiring software <b>90</b> creates and stores in memory <b>82</b> one or more output image and data files <b>120</b>. More particularly, image and data acquiring software <b>90</b> converts image data signals <b>48</b> <i>a</i>, <b>48</b> <i>b </i>and the orientation data signals (i.e., image data signals <b>48</b>, location signals <b>56</b>, velocity signals <b>58</b>, time data signal <b>62</b>, pitch, roll and yaw signals <b>64</b>, <b>66</b> and <b>68</b>, respectively, heading signal <b>72</b> and altitude signal <b>74</b>) into computer-readable output image and data files <b>120</b>. As best shown in <figref idrefs="DRAWINGS">FIG. 4</figref>, output image and data file <b>120</b> contains a plurality of captured image files I<sub>1</sub>, I<sub>2</sub>, . . . , I<sub>n </sub>corresponding to captured oblique images, and the positional data C<sub>PD1</sub>, C<sub>PD2</sub>, . . . , C<sub>PDn </sub>corresponding thereto.</p>
    <p num="p-0049">Image files I<sub>1</sub>, I<sub>2</sub>, . . . , I<sub>n </sub>of the image and data file <b>120</b> are stored in virtually any computer-readable image or graphics file format, such as, for example, JPEG, TIFF, GIF, BMP, or PDF file formats, and are cross-referenced with the positional data C<sub>PD1</sub>, C<sub>PD2</sub>, . . . , C<sub>PDn </sub>which is also stored as computer-readable data. Alternatively, positional data C<sub>PD1</sub>, C<sub>PD2</sub>, . . . , C<sub>PDn </sub>is embedded within the corresponding image files I<sub>1</sub>, I<sub>2</sub>, . . . , I<sub>n </sub>in known manner. Image data files <b>120</b> are then processed, either by image and data acquiring software <b>90</b> or by post-processing, to correct for errors, such as, for example, errors due to flight path deviations and other errors known to one of ordinary skill in the art. Thereafter, image data files <b>120</b> are ready for use to display and make measurements of and between the objects depicted within the captured images, including measurements of the heights of such objects.</p>
    <p num="p-0050">Referring now to <figref idrefs="DRAWINGS">FIG. 5</figref>, image display and measurement computer system <b>130</b>, such as, for example, a conventional desktop personal computer or a mobile computer terminal in a police car, includes memory <b>132</b>, input devices <b>134</b> <i>a </i>and <b>134</b> <i>b</i>, display device <b>136</b>, and network connection <b>138</b>. Image-capturing computer system <b>130</b> executes image display and analysis software <b>140</b>, which is stored in memory <b>132</b>. Memory <b>132</b> includes, for example, non-volatile read-only memory, random access memory, hard disk memory, removable memory cards and/or other suitable memory storage devices and/or media. Input devices <b>134</b> <i>a </i>and <b>134</b> <i>b</i>, such as, for example, a mouse, keyboard, joystick, or other such input devices, enable the input of data and interaction of a user with image display and analysis software <b>140</b> being executed by image display and measurement computer system <b>130</b>. Display device <b>136</b>, such as, for example, a liquid crystal display or cathode ray tube, displays information to the user of image display and measurement computer system <b>130</b>. Network connection <b>138</b> connects image display and measurement computer system <b>130</b> to a network (not shown), such as, for example, a local-area network, wide-area network, the Internet and/or the World Wide Web.</p>
    <p num="p-0051">In use, and referring now to <figref idrefs="DRAWINGS">FIG. 6</figref>, image display and measurement computer system <b>130</b> executing image display and analysis software <b>140</b> accesses one or more output image and data files <b>120</b> that have been read into memory <b>132</b>, such as, for example, via network connection <b>138</b>, a floppy disk drive, removable memory card or other suitable means. One or more of the captured images I<sub>1</sub>, I<sub>2</sub>, . . . , I<sub>n </sub>of output image and data files <b>120</b> is thereafter displayed as displayed oblique image <b>142</b> under the control of image display and analysis software <b>140</b>. At approximately the same time, one or more data portions C<sub>PD1</sub>, C<sub>PD2</sub>, . . . , C<sub>PDn </sub>corresponding to displayed oblique image <b>142</b> are read into a readily-accessible portion of memory <b>132</b>.</p>
    <p num="p-0052">It should be particularly noted that displayed oblique image <b>142</b> is displayed substantially as captured, i.e., displayed image <b>142</b> is not warped or fitted to any coordinate system nor is displayed image <b>142</b> ortho-rectified. Rather than warping displayed image <b>142</b> to a coordinate system in order to enable measurement of objects depicted therein, image display and analysis software <b>140</b>, in general, determines the geo-locations of selected pixels only as needed, or “on the fly”, by referencing data portions C<sub>PD1</sub>, C<sub>PD2</sub>, . . . , C<sub>PDn </sub>of output image and data files <b>120</b> and calculating the position and/or geo-location of those selected pixels using one or more projection equations as is more particularly described hereinafter.</p>
    <p num="p-0053">Generally, a user of display and measurement computer system <b>130</b> takes measurements of and between objects depicted in displayed oblique image <b>142</b> by selecting one of several available measuring modes provided within image display and analysis software <b>140</b>. The user selects the desired measurement mode by accessing, for example, a series of pull-down menus or toolbars M, or via keyboard commands. The measuring modes provided by image display and analysis software <b>140</b> include, for example, a distance mode that enables measurement of the distance between two or more selected points, an area mode that enables measurement of the area encompassed by several selected and interconnected points, a height mode that enables measurement of the height between two or more selected points, and an elevation mode that enables the measurement of the change in elevation of one selected point relative to one or more other selected points.</p>
    <p num="p-0054">After selecting the desired measurement mode, the user of image display and analysis software <b>140</b> selects with one of input devices <b>134</b> <i>a</i>, <b>134</b> <i>b </i>a starting point or starting pixel <b>152</b> and an ending point or pixel <b>154</b> on displayed image <b>142</b>, and image display and analysis software <b>140</b> automatically calculates and displays the quantity sought, such as, for example, the distance between starting pixel <b>152</b> and ending pixel <b>154</b>.</p>
    <p num="p-0055">When the user selects starting point/pixel <b>152</b>, the geo-location of the point corresponding thereto on surface <b>31</b> is calculated by image display and analysis software <b>140</b> which executes one or more projection equations using the data portions C<sub>PD1</sub>, C<sub>PD2</sub>, . . . , C<sub>PDn </sub>of output image and data files <b>120</b> that correspond to the particular image being displayed. The longitude and latitude of the point on surface <b>31</b> corresponding to pixel <b>152</b> are then displayed by image display and analysis software <b>140</b> on display <b>136</b>, such as, for example, by superimposing the longitude and latitude on displayed image <b>142</b> adjacent the selected point/pixel or in pop-up display box elsewhere on display <b>136</b>. The same process is repeated by the user for the selection of the end pixel/point <b>154</b>, and by image display and analysis software <b>140</b> for the retrieval and display of the longitude and latitude information.</p>
    <p num="p-0056">The calculation of the distance between starting and ending points/pixels <b>152</b>, <b>154</b>, respectively, is accomplished by determining the geo-location of each selected pixel <b>152</b>, <b>154</b> “on the fly”. The data portions C<sub>PD1</sub>, C<sub>PD2</sub>, . . . , C<sub>PDn </sub>of output image and data file <b>120</b> corresponding to the displayed image are retrieved, and the geo-location of the point on surface <b>31</b> corresponding to each selected pixel are then determined. The difference between the geo-locations corresponding to the selected pixels determines the distance between the pixels.</p>
    <p num="p-0057">As an example of how the geo-location of a given point or pixel within displayed oblique image <b>142</b> is determined, we will assume that displayed image <b>142</b> corresponds to orthogonal image <b>104</b> <i>a </i>(<figref idrefs="DRAWINGS">FIG. 1</figref>). The user of image display and analysis software <b>140</b> selects pixel <b>154</b> which, for simplicity, corresponds to center C (<figref idrefs="DRAWINGS">FIG. 1</figref>) of oblique image <b>104</b> <i>a</i>. As shown in <figref idrefs="DRAWINGS">FIG. 1</figref>, line <b>106</b> extends along horizontal plane G from a point <b>108</b> thereon that is directly below image-capturing device <b>32</b> <i>a </i>to the center C of the near border or edge <b>108</b> of oblique image <b>104</b> <i>a</i>. An extension of primary axis A<sub>1 </sub>intersects with center C. Angle Ø is the angle formed between line <b>106</b> the extension of primary axis A<sub>1</sub>. Thus, a triangle (not referenced) is formed having vertices at image-capturing device <b>32</b> <i>a</i>, point <b>108</b> and center C, and having sides <b>106</b>, the extension of primary axis A<sub>1 </sub>and vertical (dashed) line <b>110</b> between point <b>108</b> and image-capturing device <b>32</b> <i>a. </i> </p>
    <p num="p-0058">Ground plane G is a substantially horizontal, flat or non-sloping ground plane (and which typically will have an elevation that reflects the average elevation of the terrain), and therefore the above-described triangle includes a right angle between side/line <b>110</b> and side/line <b>106</b>. Since angle Ø and the altitude of image-capturing device <b>32</b> (i.e., the length of side <b>110</b>) are known, the hypotenuse (i.e., the length of the extension of primary axis A<sub>1</sub>) and remaining other side of the right triangle are calculated by simple geometry. Further, since the exact position of image-capturing device <b>32</b> <i>a </i>is known at the time the image corresponding to displayed image <b>142</b> was captured, the latitude and longitude of point <b>108</b> are also known. Knowing the length of side <b>106</b>, calculated as described above, enables the exact geo-location of pixel <b>154</b> corresponding to center C of oblique image <b>104</b> <i>a </i>to be determined by image display and analysis software <b>140</b>. Once the geo-location of the point corresponding to pixel <b>154</b> is known, the geo-location of any other pixel in displayed oblique image <b>142</b> is determinable using the known camera characteristics, such as, for example, focal length, sensor size and aspect ratio, radial and other distortion terms, etc.</p>
    <p num="p-0059">The distance between the two or more points corresponding to two or more selected pixels within displayed image <b>142</b> is calculated by image display and analysis software <b>140</b> by determining the difference between the geo-locations of the selected pixels using known algorithms, such as, for example, the Gauss formula and/or the vanishing point formula, dependent upon the selected measuring mode. The measurement of objects depicted or appearing in displayed image <b>142</b> is conducted by a substantially similar procedure to the procedure described above for measuring distances between selected pixels. For example, the lengths, widths and heights of objects, such as, for example, buildings, rivers, roads, and virtually any other geographic or man-made structure, appearing within displayed image <b>142</b> are measured by selecting the appropriate/desired measurement mode and selecting starting and ending pixels.</p>
    <p num="p-0060">It should be particularly noted that in the distance measuring mode of image display and analysis software <b>140</b> the distance between the starting and ending points/pixels <b>152</b>, <b>154</b>, respectively, is determinable along virtually any path, such as, for example, a “straight-line” path P<b>1</b> or a path P<b>2</b> that involves the selection of intermediate points/pixels and one or more “straight-line” segments interconnected therewith.</p>
    <p num="p-0061">It should also be particularly noted that the distance measuring mode of image display and analysis software <b>140</b> determines the distance between selected pixels according to a “walk the earth” method. The “walk the earth method” creates a series of interconnected line segments, represented collectively by paths P<b>1</b> and P<b>2</b>, that extend between the selected pixels/points and which lie upon or conform to the planar faces of a series of interconnected facets that define a tessellated ground plane. The tessellated ground plane, as will be more particularly described hereinafter, closely follows or recreates the terrain of surface <b>31</b>, and therefore paths P<b>1</b> and P<b>2</b> also closely follow the terrain of surface <b>31</b>. By measuring the distance along the terrain simulated by the tessellated ground plane, the “walk the earth” method provides for a more accurate and useful measurement of the distance between selected points than the conventional approach, which warps the image onto a flat earth or average elevation plane system and measures the distance between selected points along the flat earth or plane and substantially ignores variations in terrain between the points.</p>
    <p num="p-0062">For example, a contractor preparing to bid on a contract for paving a roadway over uneven or hilly terrain can determine the approximate amount or area of roadway involved using image display and analysis software <b>140</b> and the “walk the earth” measurement method provided thereby. The contractor can obtain the approximate amount or area of roadway from his or her own office without having to send a surveying crew to the site to obtain the measurements necessary.</p>
    <p num="p-0063">In contrast to the “walk the earth” method provided by the present invention, the “flat earth” or average elevation distance calculating approaches include inherent inaccuracies when measuring distances between points and/or objects disposed on uneven terrain and when measuring the sizes and/or heights of objects similarly disposed. Even a modest slope or grade in the surface being captured results in a difference in the elevation of the nadir point relative to virtually any other point of interest thereon. Thus, referring again to <figref idrefs="DRAWINGS">FIG. 1</figref>, the triangle formed by line <b>106</b>, the extension of primary axis A<sub>1 </sub>and the vertical (dashed) line <b>110</b> between point <b>108</b> and image-capturing device <b>32</b> <i>a </i>may not be a right triangle. If such is the case, any geometric calculations assuming that triangle to be a right triangle would contain errors, and such calculations would be reduced to approximations due to even a relatively slight gradient or slope between the points of interest.</p>
    <p num="p-0064">For example, if surface <b>31</b> slopes upward between nadir point N and center C at the near or bottom edge <b>108</b> of oblique image <b>104</b> then second line <b>110</b> intersects surface <b>31</b> before the point at which such intersection would occur on a level or non-sloping surface <b>31</b>. If center C is fifteen feet higher than nadir point N and with a declination angle θ equal to 40° (forty degrees), the calculated location of center C would be off by approximately 17.8 feet without correction for the change in elevation between the points.</p>
    <p num="p-0065">As generally discussed above, in order to compensate at least in part for changes in elevation and the resultant inaccuracies in the measurement of and between objects within image <b>142</b>, image display and analysis software <b>140</b> references, as necessary, points within displayed image <b>142</b> and on surface <b>31</b> to a pre-calculated tessellated or faceted ground plane generally designated <b>160</b> in <figref idrefs="DRAWINGS">FIG. 6</figref>. Tessellated ground plane <b>160</b> includes a plurality of individual facets <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc., each of which are interconnected to each other and are defined by four vertices (not referenced, but shown as points) having respective elevations. Adjacent pairs of facets <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc., share two vertices. Each facet <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc., has a respective pitch and slope. Tessellated ground plane <b>160</b> is created based upon various data and resources, such as, for example, topographical maps, and/or digital raster graphics, survey data, and various other sources.</p>
    <p num="p-0066">Generally, the geo-location of a point of interest on displayed image <b>142</b> is calculated by determining which of facets <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc., correspond to that point of interest. Thus, the location of the point of interest is calculated based on the characteristics, i.e., elevation, pitch and slope, of facets <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc., rather than based upon a flat or average-elevation ground plane. Error is introduced only in so far as the topography of surface <b>31</b> and the location of the point of interest thereon deviate from the planar surface of the facet <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc, within which the point of interest lies. That error is reducible through a bilinear interpolation of the elevation of the point of interest within a particular one of facets <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc., and using that interpolated elevation in the location calculation performed by image display and analysis software <b>140</b>.</p>
    <p num="p-0067">To use tessellated ground plane <b>160</b>, image display and analysis software <b>140</b> employs a modified ray-tracing algorithm to find the intersection of the ray projected from the image-capturing device <b>32</b> <i>a </i>or <b>32</b> <i>b </i>towards surface <b>31</b> and tessellated ground plane <b>160</b>. The algorithm determines not only which of facets <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc., is intersected by the ray, but also where within the facet the intersection occurs. By use of bi-linear interpolation, a fairly precise ground location can be determined. For the reverse projection, tessellated ground plane <b>160</b> is used to find the ground elevation value for the input ground location also using bi-linear interpolation. The elevation and location are then used to project backwards through a model of the image-capturing device <b>32</b> <i>a </i>or <b>32</b> <i>b </i>to determine which of the pixels within displayed image <b>142</b> corresponds to the given location.</p>
    <p num="p-0068">More particularly, and as an example, image display and analysis software <b>140</b> performs and/or calculates the geo-location of point <b>164</b> by superimposing and/or fitting tessellated ground plane <b>160</b> to at least a portion <b>166</b>, such as, for example, a hill, of surface <b>31</b>. It should be noted that only a small portion of tessellated ground plane <b>160</b> and facets <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc., thereof is shown along the profile of portion <b>166</b> of surface <b>31</b>. As discussed above, each of facets <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c, </i>etc., are defined by four vertices, each of which have respective elevations, and each of the facets have respective pitches and slopes. The specific position of point <b>164</b> upon the plane/surface of the facet <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc., within which point <b>164</b> (or its projection) lies is determined as described above.</p>
    <p num="p-0069">Tessellated ground plane <b>160</b> is preferably created outside the operation of image display and measurement computer system <b>130</b> and image display and analysis software <b>140</b>. Rather, tessellated ground plane <b>160</b> takes the form of a relatively simple data table or look-up table <b>168</b> stored within memory <b>132</b> of and/or accessible to image display and measurement computer system <b>130</b>. The computing resources required to calculate the locations of all the vertices of the many facets of a typical ground plane do not necessarily have to reside within image display and measurement computer system <b>130</b>. Thus, image display and measurement computer system <b>130</b> is compatible for use with and executable by a conventional personal computer without requiring additional computing resources.</p>
    <p num="p-0070">Calculating tessellated ground plane <b>160</b> outside of image display and measurement computer system <b>130</b> enables virtually any level of detail to be incorporated into tessellated ground plane <b>160</b>, i.e., the size and/or area covered by or corresponding to each of facets <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc., can be as large or as small as desired, without significantly increasing the calculation time, slowing the operation of, nor significantly increasing the resources required by image display and measurement computer system <b>130</b> and/or image display and analysis software <b>140</b>. Display and measurement computer system <b>130</b> can therefore be a relatively basic and uncomplicated computer system.</p>
    <p num="p-0071">The size of facets <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc., are uniform in size throughout a particular displayed image <b>142</b>. For example, if displayed image <b>142</b> corresponds to an area that is approximately 750 feet wide in the foreground by approximately 900 feet deep, the image can be broken into facets that are approximately 50 square feet, thus yielding about 15 facets in width and 18 facets in depth. Alternatively, the size of facets <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc., are uniform in terms of the number of pixels contained therein, i.e., each facet is the same number of pixels wide and the same number of pixels deep. Facets in the foreground of displayed image <b>142</b>, where the pixel density is greatest, would therefore be dimensionally smaller than facets in the background of displayed image <b>142</b> where pixel density is lowest. Since it is desirable to take most measurements in the foreground of a displayed image where pixel density is greatest, creating facets that are uniform in terms of the number of pixels they contain has the advantage of providing more accurate measurements in the foreground of displayed image <b>142</b> relative to facets that are dimensionally uniform.</p>
    <p num="p-0072">Another advantage of using pixels as a basis for defining the dimensions of facets <b>162</b> <i>a</i>, <b>162</b> <i>b</i>, <b>162</b> <i>c</i>, etc., is that the location calculation (pixel location to ground location) is relatively simple. A user operates image display and measurement computer system <b>130</b> to select a pixel within a given facet, image display and analysis software <b>140</b> looks up the data for the facet corresponding to the selected pixel, the elevation of the selected pixel is calculated as discussed above, and that elevation is used within the location calculation.</p>
    <p num="p-0073">Generally, the method of capturing oblique images of the present invention divides an area of interest, such as, for example, a county, into sectors of generally uniform size, such as, for example, sectors that are approximately one square mile in area. This is done to facilitate the creation of a flight plan to capture oblique images covering every inch of the area of interest, and to organize and name the sectors and/or images thereof for easy reference, storage and retrieval (a process known in the art as “sectorization”). Because the edges of any geographic area of interest, such as a county, rarely falls on even square mile boundaries, the method of capturing oblique images of the present invention provides more sectors than there are square miles in the area of interest—how many more depends largely on the length of the county borders as well as how straight or jagged they are. Typically, you can expect one extra sector for every two to three miles of border. So if a county or other area of interest is roughly 20 miles by 35 miles, or 700 square miles, the area will be divided into approximately from 740 to 780 sectors.</p>
    <p num="p-0074">The method of capturing oblique images of the present invention, in general, captures the oblique images from at least two compass directions, and provides full coverage of the area of interest from at least those two compass directions. Referring now to <figref idrefs="DRAWINGS">FIGS. 7 and 8</figref>, a first embodiment of a method for capturing oblique images of the present invention is shown. For sake of clarity, <figref idrefs="DRAWINGS">FIG. 7 and 8</figref> is based on a system having only one image-capturing device. However, it is to be understood that two or more image-capturing devices can be used.</p>
    <p num="p-0075">The image-capturing device captures one or more oblique images during each pass over area <b>200</b>. The image-capturing device, as discussed above, is aimed at an angle over area <b>200</b> to capture oblique images thereof. Area <b>200</b> is traversed in a back-and-forth pattern, similar to the way a lawn is mowed, by the image-carrying device and/or the platform to ensure double coverage of area <b>200</b>.</p>
    <p num="p-0076">More particularly, area <b>200</b> is traversed by image-carrying device <b>32</b> and/or platform <b>20</b> following a first path <b>202</b> to thereby capture oblique images of portions <b>202</b> <i>a</i>, <b>202</b> <i>b</i>, and <b>202</b> <i>c </i>of area <b>200</b>. Area <b>200</b> is then traversed by image-carrying device <b>32</b> and/or platform <b>20</b> following a second path <b>204</b> that is parallel and spaced apart from, and in an opposite direction to, i.e., 180° (one-hundred and eighty degrees) from, first path <b>202</b>, to thereby capture oblique images of portions <b>204</b> <i>a</i>, <b>204</b> <i>b</i>, <b>204</b> <i>c </i>of area <b>200</b>. By comparing <figref idrefs="DRAWINGS">FIGS. 7 and 8</figref>, it is seen that a portion <b>207</b> (<figref idrefs="DRAWINGS">FIG. 8</figref>) of area <b>200</b> is covered by images <b>202</b> <i>a</i>-<i>c </i>captured from a first direction or perspective, and by images <b>204</b> <i>a</i>-<i>c </i>captured from a second direction or perspective. As such, the middle portion of area <b>200</b> is 100% (one-hundred percent) double covered. The above-described pattern of traversing or passing over area <b>200</b> along opposing paths that are parallel to paths <b>202</b> and <b>204</b> is repeated until the entirety of area <b>200</b> is completely covered by at least one oblique image captured from paths that are parallel to, spaced apart from each other as dictated by the size of area <b>200</b>, and in the same direction as paths <b>202</b> and <b>204</b> to thereby one-hundred percent double cover area <b>200</b> from those perspectives/directions.</p>
    <p num="p-0077">If desired, and for enhanced detail, area <b>200</b> is covered by two additional opposing and parallel third and fourth paths <b>206</b> and <b>208</b>, respectively, that are perpendicular to paths <b>202</b> and <b>204</b> as shown in <figref idrefs="DRAWINGS">FIGS. 9 and 10</figref>. Area <b>200</b> is therefore traversed by image-carrying device <b>32</b> and/or platform <b>20</b> following third path <b>206</b> to capture oblique images of portions <b>206</b> <i>a</i>, <b>206</b> <i>b </i>and <b>206</b> <i>c </i>of area <b>200</b>, and is then traversed along fourth path <b>208</b> that is parallel, spaced apart from, and opposite to third path <b>206</b> to capture oblique images of portions <b>208</b> <i>a</i>, <b>208</b> <i>b </i>and <b>208</b> <i>c </i>of area <b>200</b>. This pattern of traversing or passing over area <b>200</b> along opposing paths that are parallel to paths <b>206</b> and <b>208</b> is similarly repeated until the entirety of area <b>200</b> is completely covered by at least one oblique image captured from paths that are parallel to, spaced apart from as dictated by the size of area <b>200</b>, and in the same direction as paths <b>206</b> and <b>208</b> to thereby one-hundred percent double cover area <b>200</b> from those directions/perspectives.</p>
    <p num="p-0078">As described above, image-carrying device <b>32</b> and/or platform <b>20</b>, traverses or passes over area <b>200</b> along a predetermined path. However, it is to be understood that image-carrying device and/or platform <b>20</b> do not necessarily pass or traverse directly over area <b>200</b> but rather may pass or traverse an area adjacent, proximate to, or even somewhat removed from, area <b>200</b> in order to ensure that the portion of area <b>200</b> that is being imaged falls within the image-capture field of the image-capturing device. Path <b>202</b>, as shown in <figref idrefs="DRAWINGS">FIG. 7</figref>, is such a path that does not pass directly over area <b>200</b> but yet captures oblique images thereof.</p>
    <p num="p-0079">The present invention is capable of capturing images at various levels of resolution or ground sample distances. A first level of detail, hereinafter referred to as a community level, has a ground sample distance of, for example, approximately two-feet per pixel. For orthogonal community-level images, the ground sample distance remains substantially constant throughout the image. Orthogonal community-level images are captured with sufficient overlap to provide stereo pair coverage. For oblique community-level images, the ground sample distance varies from, for example, approximately one-foot per pixel in the foreground of the image to approximately two-feet per pixel in the mid-ground of the image, and to approximately four-feet per pixel in the background of the image. Oblique community-level images are captured with sufficient overlap such that each area of interest is typically covered by at least two oblique images from each compass direction captured. Approximately ten oblique community-level images are captured per sector.</p>
    <p num="p-0080">A second level of detail, hereinafter referred to as a neighborhood level, is significantly more detailed than the community-level images. Neighborhood-level images have a ground sample distance of, for example, approximately six-inches per pixel. For orthogonal neighborhood-level images, the ground sample distance remains substantially constant. Oblique neighborhood-level images have a ground sample distance of, for example, from approximately four-inches per pixel in the foreground of the image to approximately six-inches per pixel in the mid-ground of the image, and to approximately ten-inches per pixel in the background of the image. Oblique neighborhood-level images are captured with sufficient overlap such that each area of interest is typically covered by at least two oblique images from each compass direction captured, and such that opposing compass directions provide 100% overlap with each other. Approximately one hundred (100) oblique area images are captured per sector.</p>
    <p num="p-0081">It should be particularly noted that capturing oblique community and/or neighborhood-level images from all four compass directions ensures that every point in the image will appear in the foreground or lower portion of at least one of the captured oblique images, where ground sample distance is lowest and image detail is greatest.</p>
    <p num="p-0082">In the embodiment shown, image-capturing and geo-locating system <b>30</b> includes a gyroscope, compass and altimeter. However, it is to be understood that the image-capturing and geo-locating system of the present invention can be alternately configured, such as, for example, to derive and/or calculate altitude, pitch, roll and yaw, and compass heading from the GPS and INU signals/data, thereby rendering one or more of the gyroscope, compass and altimeter unnecessary. In fact,</p>
    <p num="p-0083">In the embodiment shown, image-capturing devices are at an equal angle of declination relative to a horizontal plane. However, it is to be understood that the declination angles of the image-capturing devices do not have to be equal.</p>
    <p num="p-0084">In the embodiment shown, image-capturing computer system executes image and data acquiring software that issues a common or single image-capture signal to the image-capturing devices to thereby cause those devices to acquire or capture an image. However, it is to be understood that the present invention can be alternately configured to separately cause the image-capturing devices to capture images at different instants and/or at different intervals.</p>
    <p num="p-0085">In the embodiment shown, the method of the present invention captures oblique images to provide double coverage of an area of interest from paths/perspectives that are substantially opposite to each other, i.e., 180° (one-hundred and eighty degrees) relative to each other. However, it is to be understood that the method of the present invention can be alternately configured to provide double coverage from paths/perspectives that are generally and/or substantially perpendicular relative to each other.</p>
    <p num="p-0086">While the present invention has been described as having a preferred design, the invention can be further modified within the spirit and scope of this disclosure. This disclosure is therefore intended to encompass any equivalents to the structures and elements disclosed herein. Further, this disclosure is intended to encompass any variations, uses, or adaptations of the present invention that use the general principles disclosed herein. Moreover, this disclosure is intended to encompass any departures from the subject matter disclosed that come within the known or customary practice in the pertinent art and which fall within the limits of the appended claims.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4758850">US4758850</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 1, 1986</td><td class="patent-data-table-td patent-date-value">Jul 19, 1988</td><td class="patent-data-table-td ">British Aerospace Public Limited Company</td><td class="patent-data-table-td ">Identification of ground targets in airborne surveillance radar returns</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5247356">US5247356</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 14, 1992</td><td class="patent-data-table-td patent-date-value">Sep 21, 1993</td><td class="patent-data-table-td ">Ciampa John A</td><td class="patent-data-table-td ">Method and apparatus for mapping and measuring land</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5251037">US5251037</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 18, 1992</td><td class="patent-data-table-td patent-date-value">Oct 5, 1993</td><td class="patent-data-table-td ">Hughes Training, Inc.</td><td class="patent-data-table-td ">Method and apparatus for generating high resolution CCD camera images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5414462">US5414462</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 11, 1993</td><td class="patent-data-table-td patent-date-value">May 9, 1995</td><td class="patent-data-table-td ">Veatch; John W.</td><td class="patent-data-table-td ">Method and apparatus for generating a comprehensive survey map</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5467271">US5467271</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 17, 1993</td><td class="patent-data-table-td patent-date-value">Nov 14, 1995</td><td class="patent-data-table-td ">Trw, Inc.</td><td class="patent-data-table-td ">Mapping and analysis system for precision farming applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5798786">US5798786</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 7, 1996</td><td class="patent-data-table-td patent-date-value">Aug 25, 1998</td><td class="patent-data-table-td ">Recon/Optical, Inc.</td><td class="patent-data-table-td ">Electro-optical imaging detector array for a moving vehicle which includes two axis image motion compensation and transfers pixels in row directions and column directions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5844602">US5844602</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 3, 1996</td><td class="patent-data-table-td patent-date-value">Dec 1, 1998</td><td class="patent-data-table-td ">Recon/Optical, Inc.</td><td class="patent-data-table-td ">Electro-optical imaging array and camera system with pitch rate image motion compensation which can be used in an airplane in a dive bomb maneuver</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5894323">US5894323</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 22, 1996</td><td class="patent-data-table-td patent-date-value">Apr 13, 1999</td><td class="patent-data-table-td ">Tasc, Inc,</td><td class="patent-data-table-td ">Airborne imaging system using global positioning system (GPS) and inertial measurement unit (IMU) data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6088055">US6088055</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 29, 1998</td><td class="patent-data-table-td patent-date-value">Jul 11, 2000</td><td class="patent-data-table-td ">Recon /Optical, Inc.</td><td class="patent-data-table-td ">Electro-optical imaging array and camera system with pitch rate image motion compensation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6108032">US6108032</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 23, 1997</td><td class="patent-data-table-td patent-date-value">Aug 22, 2000</td><td class="patent-data-table-td ">Lockheed Martin Fairchild Systems</td><td class="patent-data-table-td ">System and method for image motion compensation of a CCD image sensor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6130705">US6130705</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 10, 1998</td><td class="patent-data-table-td patent-date-value">Oct 10, 2000</td><td class="patent-data-table-td ">Recon/Optical, Inc.</td><td class="patent-data-table-td ">Autonomous electro-optical framing camera system with constant ground resolution, unmanned airborne vehicle therefor, and methods of use</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6222583">US6222583</a></td><td class="patent-data-table-td patent-date-value">Mar 12, 1998</td><td class="patent-data-table-td patent-date-value">Apr 24, 2001</td><td class="patent-data-table-td ">Nippon Telegraph And Telephone Corporation</td><td class="patent-data-table-td ">Device and system for labeling sight images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6256057">US6256057</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 19, 1998</td><td class="patent-data-table-td patent-date-value">Jul 3, 2001</td><td class="patent-data-table-td ">Lockhead Martin Corporation</td><td class="patent-data-table-td ">Electro-optical reconnaissance system with forward motion compensation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6373522">US6373522</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 2, 2001</td><td class="patent-data-table-td patent-date-value">Apr 16, 2002</td><td class="patent-data-table-td ">Bae Systems Information And Electronic Systems Integration Inc.</td><td class="patent-data-table-td ">Electro-optical reconnaissance system with forward motion compensation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6731329">US6731329</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 15, 2000</td><td class="patent-data-table-td patent-date-value">May 4, 2004</td><td class="patent-data-table-td ">Zsp Geodaetische Systeme Gmbh</td><td class="patent-data-table-td ">Method and an arrangement for determining the spatial coordinates of at least one object point</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6747686">US6747686</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 5, 2001</td><td class="patent-data-table-td patent-date-value">Jun 8, 2004</td><td class="patent-data-table-td ">Recon/Optical, Inc.</td><td class="patent-data-table-td ">High aspect stereoscopic mode camera and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7009638">US7009638</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 6, 2002</td><td class="patent-data-table-td patent-date-value">Mar 7, 2006</td><td class="patent-data-table-td ">Vexcel Imaging Gmbh</td><td class="patent-data-table-td ">Self-calibrating, digital, large format camera with single or multiple detector arrays and single or multiple optical systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20010038718">US20010038718</a></td><td class="patent-data-table-td patent-date-value">Mar 9, 2001</td><td class="patent-data-table-td patent-date-value">Nov 8, 2001</td><td class="patent-data-table-td ">Rakesh Kumar</td><td class="patent-data-table-td ">Method and apparatus for performing geo-spatial registration of imagery</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020041328">US20020041328</a></td><td class="patent-data-table-td patent-date-value">Mar 29, 2001</td><td class="patent-data-table-td patent-date-value">Apr 11, 2002</td><td class="patent-data-table-td ">Astrovision International, Inc.</td><td class="patent-data-table-td ">Direct broadcast imaging satellite system apparatus and method for providing real-time, continuous monitoring of earth from geostationary earth orbit and related services</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020121969">US20020121969</a></td><td class="patent-data-table-td patent-date-value">Aug 20, 2001</td><td class="patent-data-table-td patent-date-value">Sep 5, 2002</td><td class="patent-data-table-td ">Joao Raymond Anthony</td><td class="patent-data-table-td ">Monitoring apparatus and method for a vehicle and/or a premises</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20060238383">US20060238383</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 15, 2006</td><td class="patent-data-table-td patent-date-value">Oct 26, 2006</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Virtual earth rooftop overlay and bounding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20070024612">US20070024612</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 27, 2005</td><td class="patent-data-table-td patent-date-value">Feb 1, 2007</td><td class="patent-data-table-td ">Balfour Technologies Llc</td><td class="patent-data-table-td ">System for viewing a collection of oblique imagery in a three or four dimensional virtual scene</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20070046448">US20070046448</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 11, 2006</td><td class="patent-data-table-td patent-date-value">Mar 1, 2007</td><td class="patent-data-table-td ">M7 Visual Intelligence</td><td class="patent-data-table-td ">Vehicle based data collection and processing system and imaging sensor system and methods thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO1999018732A1?cl=en">WO1999018732A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 5, 1998</td><td class="patent-data-table-td patent-date-value">Apr 15, 1999</td><td class="patent-data-table-td ">John A Ciampa</td><td class="patent-data-table-td ">Digital-image mapping</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7930103">US7930103</a></td><td class="patent-data-table-td patent-date-value">Jul 15, 2008</td><td class="patent-data-table-td patent-date-value">Apr 19, 2011</td><td class="patent-data-table-td ">Vermeer Manufacturing Company</td><td class="patent-data-table-td ">Utility mapping and data distribution system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8068643">US8068643</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 19, 2010</td><td class="patent-data-table-td patent-date-value">Nov 29, 2011</td><td class="patent-data-table-td ">Pictometry International Corp.</td><td class="patent-data-table-td ">Method and apparatus for capturing, geolocating and measuring oblique images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8089390">US8089390</a></td><td class="patent-data-table-td patent-date-value">May 16, 2007</td><td class="patent-data-table-td patent-date-value">Jan 3, 2012</td><td class="patent-data-table-td ">Underground Imaging Technologies, Inc.</td><td class="patent-data-table-td ">Sensor cart positioning system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8098295">US8098295</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 6, 2006</td><td class="patent-data-table-td patent-date-value">Jan 17, 2012</td><td class="patent-data-table-td ">Given Imaging Ltd.</td><td class="patent-data-table-td ">In-vivo imaging system device and method with image stream construction using a raw images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8145578">US8145578</a></td><td class="patent-data-table-td patent-date-value">Apr 17, 2008</td><td class="patent-data-table-td patent-date-value">Mar 27, 2012</td><td class="patent-data-table-td ">Eagel View Technologies, Inc.</td><td class="patent-data-table-td ">Aerial roof estimation system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8155433">US8155433</a></td><td class="patent-data-table-td patent-date-value">Jul 10, 2008</td><td class="patent-data-table-td patent-date-value">Apr 10, 2012</td><td class="patent-data-table-td ">Goodrich Corporation</td><td class="patent-data-table-td ">Method of object location in airborne imagery using recursive quad space image processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8155935">US8155935</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 24, 2010</td><td class="patent-data-table-td patent-date-value">Apr 10, 2012</td><td class="patent-data-table-td ">Meiners Robert E</td><td class="patent-data-table-td ">System and method of sub-surface system design and installation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8170840">US8170840</a></td><td class="patent-data-table-td patent-date-value">May 15, 2009</td><td class="patent-data-table-td patent-date-value">May 1, 2012</td><td class="patent-data-table-td ">Eagle View Technologies, Inc.</td><td class="patent-data-table-td ">Pitch determination systems and methods for aerial roof estimation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8204341">US8204341</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 19, 2010</td><td class="patent-data-table-td patent-date-value">Jun 19, 2012</td><td class="patent-data-table-td ">Pictometry International Corp.</td><td class="patent-data-table-td ">Method and apparatus for capturing, geolocating and measuring oblique images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8209152">US8209152</a></td><td class="patent-data-table-td patent-date-value">May 15, 2009</td><td class="patent-data-table-td patent-date-value">Jun 26, 2012</td><td class="patent-data-table-td ">Eagleview Technologies, Inc.</td><td class="patent-data-table-td ">Concurrent display systems and methods for aerial roof estimation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8280634">US8280634</a></td><td class="patent-data-table-td patent-date-value">Apr 19, 2011</td><td class="patent-data-table-td patent-date-value">Oct 2, 2012</td><td class="patent-data-table-td ">Underground Imaging Technologies</td><td class="patent-data-table-td ">Utility mapping and data distribution system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8406513">US8406513</a></td><td class="patent-data-table-td patent-date-value">Feb 21, 2012</td><td class="patent-data-table-td patent-date-value">Mar 26, 2013</td><td class="patent-data-table-td ">Goodrich Corporation</td><td class="patent-data-table-td ">Method of object location in airborne imagery using recursive quad space image processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8427536">US8427536</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 19, 2009</td><td class="patent-data-table-td patent-date-value">Apr 23, 2013</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Orientation determination of a mobile station using side and top view images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8477560">US8477560</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 14, 2009</td><td class="patent-data-table-td patent-date-value">Jul 2, 2013</td><td class="patent-data-table-td ">Westerngeco L.L.C.</td><td class="patent-data-table-td ">Determining a position of an object of a positioning network associated with a marine survey arrangement</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8497905">US8497905</a></td><td class="patent-data-table-td patent-date-value">Sep 23, 2009</td><td class="patent-data-table-td patent-date-value">Jul 30, 2013</td><td class="patent-data-table-td ">nearmap australia pty ltd.</td><td class="patent-data-table-td ">Systems and methods of capturing large area images in detail including cascaded cameras and/or calibration features</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8505847">US8505847</a></td><td class="patent-data-table-td patent-date-value">Dec 18, 2012</td><td class="patent-data-table-td patent-date-value">Aug 13, 2013</td><td class="patent-data-table-td ">John Ciampa</td><td class="patent-data-table-td ">Lighter-than-air systems, methods, and kits for obtaining aerial images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8515207">US8515207</a></td><td class="patent-data-table-td patent-date-value">May 25, 2007</td><td class="patent-data-table-td patent-date-value">Aug 20, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Annotations in panoramic images, and applications thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8622338">US8622338</a></td><td class="patent-data-table-td patent-date-value">Jul 16, 2013</td><td class="patent-data-table-td patent-date-value">Jan 7, 2014</td><td class="patent-data-table-td ">John Ciampa</td><td class="patent-data-table-td ">Lighter-than-air systems, methods, and kits for obtaining aerial images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8634594">US8634594</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 27, 2012</td><td class="patent-data-table-td patent-date-value">Jan 21, 2014</td><td class="patent-data-table-td ">Pictometry International Corp.</td><td class="patent-data-table-td ">Method and apparatus for capturing, geolocating and measuring oblique images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8670961">US8670961</a></td><td class="patent-data-table-td patent-date-value">Nov 2, 2011</td><td class="patent-data-table-td patent-date-value">Mar 11, 2014</td><td class="patent-data-table-td ">Eagle View Technologies, Inc.</td><td class="patent-data-table-td ">Aerial roof estimation systems and methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8675068">US8675068</a></td><td class="patent-data-table-td patent-date-value">Apr 11, 2008</td><td class="patent-data-table-td patent-date-value">Mar 18, 2014</td><td class="patent-data-table-td ">Nearmap Australia Pty Ltd</td><td class="patent-data-table-td ">Systems and methods of capturing large area images in detail including cascaded cameras and/or calibration features</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8687062">US8687062</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 2011</td><td class="patent-data-table-td patent-date-value">Apr 1, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Step-stare oblique aerial camera system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8731234">US8731234</a></td><td class="patent-data-table-td patent-date-value">Nov 2, 2009</td><td class="patent-data-table-td patent-date-value">May 20, 2014</td><td class="patent-data-table-td ">Eagle View Technologies, Inc.</td><td class="patent-data-table-td ">Automated roof identification systems and methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8774525">US8774525</a></td><td class="patent-data-table-td patent-date-value">Feb 1, 2013</td><td class="patent-data-table-td patent-date-value">Jul 8, 2014</td><td class="patent-data-table-td ">Eagle View Technologies, Inc.</td><td class="patent-data-table-td ">Systems and methods for estimation of building floor area</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20070188610">US20070188610</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 13, 2006</td><td class="patent-data-table-td patent-date-value">Aug 16, 2007</td><td class="patent-data-table-td ">The Boeing Company</td><td class="patent-data-table-td ">Synoptic broad-area remote-sensing via multiple telescopes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100074050">US20100074050</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 14, 2009</td><td class="patent-data-table-td patent-date-value">Mar 25, 2010</td><td class="patent-data-table-td ">Welker Kenneth E</td><td class="patent-data-table-td ">Determining a Position of an Object of a Positioning Network Associated with a Marine Survey Arrangement</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110115902">US20110115902</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 19, 2009</td><td class="patent-data-table-td patent-date-value">May 19, 2011</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Orientation determination of a mobile station using side and top view images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120188372">US20120188372</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 25, 2011</td><td class="patent-data-table-td patent-date-value">Jul 26, 2012</td><td class="patent-data-table-td ">Tarik Ozkul</td><td class="patent-data-table-td ">Autonomous decision system for selecting target in observation satellites</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120288158">US20120288158</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 27, 2012</td><td class="patent-data-table-td patent-date-value">Nov 15, 2012</td><td class="patent-data-table-td ">Schultz Stephen L</td><td class="patent-data-table-td ">Method and Apparatus for Capturing, Geolocating and Measuring Oblique Images</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S106000">382/106</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348S144000">348/144</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc356/defs356.htm&usg=AFQjCNGiEVb0PD41nDL-c_Hc6_cJvkgI_Q#C356S003000">356/3</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S284000">382/284</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0017050000">G06T17/05</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009000000">G06K9/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04N0007180000">H04N7/18</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G01C0003000000">G01C3/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G01C0011020000">G01C11/02</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G01C11/02">G01C11/02</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N5/235">H04N5/235</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YEVeBQABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G03B37/02">G03B37/02</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G01C11/02</span>, <span class="nested-value">G03B37/02</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Oct 9, 2012</td><td class="patent-data-table-td ">CC</td><td class="patent-data-table-td ">Certificate of correction</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 26, 2012</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 17, 18 AND 43 IS CONFIRMED. CLAIMS 19-24 ARE CANCELLED. CLAIMS 1-16 AND25-42 WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 1, 2012</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 4, 2011</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SCHULTZ, STEPHEN;GIUFFRIDA, FRANK D.;GRAY, ROBERT L.;ANDOTHERS;SIGNING DATES FROM 20110126 TO 20110202;REEL/FRAME:025746/0861</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">PICTOMERTRY INTERNATIONAL CORPORATION, NEW YORK</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 21, 2011</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">JPMORGAN CHASE BANK, N.A., NEW YORK</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">NOTICE OF SECURITY INTEREST;ASSIGNOR:PICTOMETRY INTERNATIONAL CORP.;REEL/FRAME:025674/0723</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20110114</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 27, 2009</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20081201</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 17, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">PICTOMETRY INTERNATIONAL CORPORATION, NEW YORK</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SCHULTZ, STEPHEN;GIUFFRIDA, FRANK;GRAY, ROBERT;AND OTHERS;REEL/FRAME:017185/0694;SIGNING DATES FROM 20031104 TO 20040830</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U2qOpn8uaMzQLzEn-C_-3FzexJEyQ\u0026id=YEVeBQABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U3nWRR6wb4sXUsO_4YLFHUrBa6wyA\u0026id=YEVeBQABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U12mMpf8V1w4ID3fBAXSpgZMhvx8w","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Method_and_apparatus_for_capturing_geolo.pdf?id=YEVeBQABERAJ\u0026output=pdf\u0026sig=ACfU3U3_VpOcFSi_MtXeNcnFHT2ZfLLH9g"},"sample_url":"http://www.google.com/patents/reader?id=YEVeBQABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>