<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7315818 - Error correction in speech recognition - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Error correction in speech recognition"><meta name="DC.contributor" content="Daniell Stevens" scheme="inventor"><meta name="DC.contributor" content="Robert Roth" scheme="inventor"><meta name="DC.contributor" content="Joel M. Gould" scheme="inventor"><meta name="DC.contributor" content="Michael J. Newman" scheme="inventor"><meta name="DC.contributor" content="Dean Sturtevant" scheme="inventor"><meta name="DC.contributor" content="Charles E. Ingold" scheme="inventor"><meta name="DC.contributor" content="David Abrahams" scheme="inventor"><meta name="DC.contributor" content="Allan Gold" scheme="inventor"><meta name="DC.contributor" content="Nuance Communications, Inc." scheme="assignee"><meta name="DC.date" content="2005-5-11" scheme="dateSubmitted"><meta name="DC.description" content="New techniques and systems may be implemented to improve error correction in speech recognition. These new techniques and systems may be implemented to correct errors in speech recognition systems may be used in a standard desktop environment, in a mobile environment, or in any other type of environment that can receive and/or present recognized speech."><meta name="DC.date" content="2008-1-1" scheme="issued"><meta name="DC.relation" content="US:20020138265:A1" scheme="references"><meta name="DC.relation" content="US:5027406" scheme="references"><meta name="DC.relation" content="US:5594809" scheme="references"><meta name="DC.relation" content="US:5625748" scheme="references"><meta name="DC.relation" content="US:5748840" scheme="references"><meta name="DC.relation" content="US:5754978" scheme="references"><meta name="DC.relation" content="US:5794189" scheme="references"><meta name="DC.relation" content="US:5799279" scheme="references"><meta name="DC.relation" content="US:5864805" scheme="references"><meta name="DC.relation" content="US:5963903" scheme="references"><meta name="DC.relation" content="US:6064959" scheme="references"><meta name="DC.relation" content="US:6073099" scheme="references"><meta name="DC.relation" content="US:6212498" scheme="references"><meta name="DC.relation" content="US:6233553" scheme="references"><meta name="DC.relation" content="US:6374221" scheme="references"><meta name="DC.relation" content="US:6490563" scheme="references"><meta name="DC.relation" content="US:6535849" scheme="references"><meta name="DC.relation" content="US:6577999" scheme="references"><meta name="DC.relation" content="US:6912498" scheme="references"><meta name="DC.relation" content="US:6934682" scheme="references"><meta name="citation_reference" content="Elizabeth D. Liddy and Sung H. Myaeng, A System Update for TREC-2, Sep. 3, 1998, pp. 1-11; School of Information Studies, Syracuse University, Syracuse, New York."><meta name="citation_reference" content="IBM (&quot;Technical Disclosure Bulletin NB900315, Automatic Correction of Viterbi Misalignments&quot; Mar. 1990)."><meta name="citation_reference" content="Iyer et al (Analyzing And Predicting Language Model Improvements, 1997 IEEE Workshop on Automatic Speech Recognition and Understanding, Dec. 1997)."><meta name="citation_reference" content="John W. Lehman and Clifford A. Reid, et al., Knowledge-Based Searching with TOPIC, Sep. 4, 1998, pp. 1-13, Verity, Inc., Mountain View, CA."><meta name="citation_reference" content="Niyogi et al. (Incorporation Voice Onset Time To Improve Letter Recognition Accuracies, Proceedings of the 1998 IEEE International Conference on Acoustics, Speech, and Signal Processing, May 1998)."><meta name="citation_reference" content="Richard Stern, George Doddington, Dave Pallet, and Charles Wayne, Specification for the ARPA Nov. 1996 HUB 4 Evaluation, Nov. 1, 1996, pp. 1-5 National Institutes of Standards and Technology."><meta name="citation_reference" content="U.S. Appl. No. 60/201,257, filed May 2000, Roth et al."><meta name="citation_patent_number" content="US:7315818"><meta name="citation_patent_application_number" content="US:11/126,271"><link rel="canonical" href="http://www.google.com/patents/US7315818"/><meta property="og:url" content="http://www.google.com/patents/US7315818"/><meta name="title" content="Patent US7315818 - Error correction in speech recognition"/><meta name="description" content="New techniques and systems may be implemented to improve error correction in speech recognition. These new techniques and systems may be implemented to correct errors in speech recognition systems may be used in a standard desktop environment, in a mobile environment, or in any other type of environment that can receive and/or present recognized speech."/><meta property="og:title" content="Patent US7315818 - Error correction in speech recognition"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("G4btU4OnG7i-sQTJ6IKwBQ"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("GBR"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("G4btU4OnG7i-sQTJ6IKwBQ"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("GBR"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7315818?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7315818"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=_HOABAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7315818&amp;usg=AFQjCNEbgbN9OeBbRvpvz-U5l7RyqF6qlw" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7315818.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7315818.pdf"></a><a class="appbar-application-grant-link" data-label="Application" href="/patents/US20050203751"></a><a class="appbar-application-grant-link" data-selected="true" data-label="Grant" href="/patents/US7315818"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7315818" style="display:none"><span itemprop="description">New techniques and systems may be implemented to improve error correction in speech recognition. These new techniques and systems may be implemented to correct errors in speech recognition systems may be used in a standard desktop environment, in a mobile environment, or in any other type of environment...</span><span itemprop="url">http://www.google.com/patents/US7315818?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7315818 - Error correction in speech recognition</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7315818 - Error correction in speech recognition" title="Patent US7315818 - Error correction in speech recognition"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7315818 B2</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 11/126,271</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Jan 1, 2008</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">May 11, 2005</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">May 2, 2000</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6912498">US6912498</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20020138265">US20020138265</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20050203751">US20050203751</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO2001084535A2">WO2001084535A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO2001084535A3">WO2001084535A3</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">11126271, </span><span class="patent-bibdata-value">126271, </span><span class="patent-bibdata-value">US 7315818 B2, </span><span class="patent-bibdata-value">US 7315818B2, </span><span class="patent-bibdata-value">US-B2-7315818, </span><span class="patent-bibdata-value">US7315818 B2, </span><span class="patent-bibdata-value">US7315818B2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Daniell+Stevens%22">Daniell Stevens</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Robert+Roth%22">Robert Roth</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Joel+M.+Gould%22">Joel M. Gould</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Michael+J.+Newman%22">Michael J. Newman</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Dean+Sturtevant%22">Dean Sturtevant</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Charles+E.+Ingold%22">Charles E. Ingold</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22David+Abrahams%22">David Abrahams</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Allan+Gold%22">Allan Gold</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Nuance+Communications,+Inc.%22">Nuance Communications, Inc.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7315818.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7315818.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7315818.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (20),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (7),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (30),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (11),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (4)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=_HOABAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7315818&usg=AFQjCNHxhyjvvh6QR-wFt9GJznfMCqWbcg">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=_HOABAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7315818&usg=AFQjCNHvuoxxvXAXLG946_27NS3Ybz9DVQ">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=_HOABAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7315818B2%26KC%3DB2%26FT%3DD&usg=AFQjCNGhMcTQIpMmN1t6J6f30wYXcQ_GxQ">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55834058" lang="EN" load-source="patent-office">Error correction in speech recognition</invention-title></span><br><span class="patent-number">US 7315818 B2</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA51247595" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">New techniques and systems may be implemented to improve error correction in speech recognition. These new techniques and systems may be implemented to correct errors in speech recognition systems may be used in a standard desktop environment, in a mobile environment, or in any other type of environment that can receive and/or present recognized speech.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(25)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00018.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00018.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00019.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00019.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00020.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00020.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00021.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00021.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00022.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00022.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00023.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00023.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7315818B2/US07315818-20080101-D00024.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7315818B2/US07315818-20080101-D00024.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(21)</span></span></div><div class="patent-text"><div mxw-id="PCLM9322207" lang="EN" load-source="patent-office" class="claims">
  <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
    <div class="claim-text">1. A computer-implemented method for speech recognition, the method comprising:
<div class="claim-text">receiving dictated text;</div>
<div class="claim-text">generating recognized speech based on the received dictated text, the generating comprising determining acoustic models for the dictated text that best match acoustic data for the dictated text;</div>
<div class="claim-text">receiving an edited text of the recognized speech, the edited text indicating a replacement for a portion of the dictated text;</div>
<div class="claim-text">determining an acoustic model for the edited text;</div>
<div class="claim-text">determining whether to adapt acoustic models for the edited text based on the acoustic model for the edited text and the acoustic model for the dictated text portion.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
    <div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising calculating an acoustic model score based on a comparison between the acoustic model for the edited text and the acoustic data for the dictated text portion.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
    <div class="claim-text">3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> in which determining whether to adapt acoustic models for the edited text is based on the calculated acoustic model score.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
    <div class="claim-text">4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref> in which determining whether to adapt acoustic models for the edited text comprises calculating an original acoustic model score based on a comparison between the acoustic model for the dictated text portion and the acoustic data for the dictated text portion.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
    <div class="claim-text">5. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref> in which determining whether to adapt acoustic models for the edited text comprises calculating a difference between the acoustic model score and the original acoustic model score.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
    <div class="claim-text">6. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref> in which determining whether to adapt acoustic models for the edited text comprises determining whether the difference is less than a predetermined value.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
    <div class="claim-text">7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref> in which determining whether to adapt acoustic models for the edited text comprises adapting acoustic models for the edited text if the difference is less than a predetermined value.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
    <div class="claim-text">8. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref> in which determining whether to adapt acoustic models for the edited text comprises bypassing adapting acoustic models for the edited text if the difference is greater than or equal to a predetermined value.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
    <div class="claim-text">9. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> in which receiving the edited text of the recognized speech occurs during a recognition session in which the recognized speech is generated.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
    <div class="claim-text">10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> in which receiving the edited text of the recognized speech occurs after a recognition session in which the recognized speech is generated.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
    <div class="claim-text">11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> in which receiving the edited text of the recognized speech comprises receiving a selection of the portion of the dictated text.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
    <div class="claim-text">12. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> in which determining an acoustic model for the edited text comprises searching for the edited text in a vocabulary or a backup dictionary used to generate the recognized speech.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
    <div class="claim-text">13. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> in which determining an acoustic model for the edited text comprises selecting an acoustic model that best matches the edited text.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00014" num="00014" class="claim">
    <div class="claim-text">14. A computer-implemented method of speech recognition, the method comprising:
<div class="claim-text">performing speech recognition on an utterance to produce a recognition result for the utterance;</div>
<div class="claim-text">receiving a selection of the recognition result;</div>
<div class="claim-text">receiving a correction of the recognition result;</div>
<div class="claim-text">performing speech recognition on the correction using a constraint grammar that permits spelling and pronunciation in parallel; and</div>
<div class="claim-text">identifying whether the correction comprises a spelling or a pronunciation using the constraint grammar.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
    <div class="claim-text">15. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> further comprising generating a replacement result for the recognition result based on the correction.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
    <div class="claim-text">16. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref> in which the constraint grammar includes a spelling portion and a dictation vocabulary portion.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
    <div class="claim-text">17. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> in which the spelling portion indicates that the first utterance from the user is a letter in an alphabet.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
    <div class="claim-text">18. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> in which the vocabulary portion indicates that the first utterance from the user is a word from the dictation vocabulary.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
    <div class="claim-text">19. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> in which the spelling portion indicates a frequency with which letters occur in a language model.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
    <div class="claim-text">20. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> in which the dictation vocabulary portion indicates a frequency with which words occur in a language model.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00021" num="00021" class="claim">
    <div class="claim-text">21. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> further comprising introducing a biasing value between the spelling and the dictation vocabulary portions of the constraint grammar.</div>
  </div>
</div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES16344751" lang="EN" load-source="patent-office" class="description">
<heading>CROSS REFERENCE TO RELATED APPLICATION</heading> <p num="p-0002">This application is a divisional of U.S. application Ser. No. 09/845,769, filed May 2, 2001 now U.S. Pat. No. 6,912,498, which claimed priority to U.S. application Ser. No. 60/201,257, filed May 2, 2000, both of which are incorporated herein by reference.</p>
<heading>TECHNICAL FIELD</heading> <p num="p-0003">This invention relates to error correction in computer-implemented speech recognition.</p>
  <heading>BACKGROUND</heading> <p num="p-0004">A speech recognition system analyzes a user's speech to determine what the user said. Most speech recognition systems are frame-based. In a frame-based system, a processor divides a signal descriptive of the speech to be recognized into a series of digital frames, each of which corresponds to a small time increment of the speech.</p>
  <p num="p-0005">A continuous speech recognition system can recognize spoken words or phrases regardless of whether the user pauses between them. By contrast, a discrete speech recognition system recognizes discrete words or phrases and requires the user to pause briefly after each discrete word or phrase. Continuous speech recognition systems typically have a higher incidence of recognition errors in comparison to discrete recognition systems due to complexities of recognizing continuous speech.</p>
  <p num="p-0006">In general, the processor of a continuous speech recognition system analyzes utterances of speech. An utterance includes a variable number of frames and may correspond to a period of speech followed by a pause of at least a predetermined duration.</p>
  <p num="p-0007">The processor determines what the user said by finding acoustic models that best match the digital frames of an utterance, and identifying text that corresponds to those acoustic models. An acoustic model may correspond to a word, phrase or command from a vocabulary. An acoustic model also may represent a sound, or phoneme, that corresponds to a portion of a word. Collectively, the constituent phonemes for a word represent the phonetic spelling of the word. Acoustic models also may represent silence and various types of environmental noise.</p>
  <p num="p-0008">The words or phrases corresponding to the best matching acoustic models are referred to as recognition candidates. The processor may produce a single recognition candidate (that is, a single sequence of words or phrases) for an utterance, or may produce a list of recognition candidates.</p>
  <p num="p-0009">Correction mechanisms for some discrete speech recognition systems displayed a list of choices for each recognized word and permitted a user to correct a misrecognition by selecting a word from the list or typing the correct word. For example, DragonDictate for Windows, by Dragon Systems, Inc. of Newton, Mass., displayed a list of numbered recognition candidates (a choice list) for each word spoken by the user, and inserted the best-scoring recognition candidate into the text being dictated by the user. If the best-scoring recognition candidate was incorrect, the user could select a recognition candidate from the choice list by saying choose-N, where N was the number associated with the correct candidate. If the correct word was not on the choice list, the user could refine the list, either by typing in the first few letters of the correct word, or by speaking words (for example, alpha, bravo) associated with the first few letters. The user also could discard the incorrect recognition result by saying scratch that.</p>
  <p num="p-0010">Dictating a new word implied acceptance of the previous recognition. If the user noticed a recognition error after dictating additional words, the user could say Oops, which would bring up a numbered list of previously-recognized words. The user could then choose a previously-recognized word by saying word-N, where N was a number associated with the word. The system would respond by displaying a choice list associated with the selected word and permitting the user to correct the word as described above.</p>
  <heading>SUMMARY</heading> <p num="p-0011">New techniques and systems improve error correction in speech recognition. These techniques and systems may be used in a standard desktop environment, in a mobile environment, or in any other type of environment that can receive and/or present recognized speech. Moreover, the techniques and systems also may leverage the power of continuous speech recognition systems, such as Dragon NaturallySpeaking, available from Dragon Systems, Inc. of Newton, Mass., the capabilities of digital recorders and hand-held electronic devices, and the advantages of using a contact manager or similar system for personal information management.</p>
  <p num="p-0012">In one general aspect, a method of correcting incorrect text associated with recognition errors in computer-implemented speech recognition includes performing speech recognition on an utterance to produce a recognition result for the utterance and receiving a selection of a word from the recognized utterance. The selection indicates a bound of a portion of the recognized utterance to be corrected. A first recognition correction is produced based on a comparison between a first alternative transcript and the recognized utterance to be corrected. A second recognition correction is produced based on a comparison between a second alternative transcript and the recognized utterance to be corrected. A portion of the recognition result is replaced with one of the first recognition correction and the second recognition correction. A duration of the first recognition correction differs from a duration of the second recognition correction. Furthermore, the portion of the recognition result replaced includes at one bound the word indicated by the selection and extends for the duration of the one of the first recognition correction and the second recognition correction with which the portion is replaced.</p>
  <p num="p-0013">Implementations may include one or more of the following features. For example, the selection may indicate a beginning bound or a finishing bound of a recognized utterance to be corrected.</p>
  <p num="p-0014">The comparison between an alternative transcript and the recognized utterance may include selecting from the alternative transcript a test word that is not identical to the selected word. The test word begins at a time that is nearest a time at which the selected word begins. The comparison between the alternative transcript and the recognized utterance may further include searching in time through the recognized utterance and relative to the selected word and through the alternative transcript and relative to the test word until a word common to the recognized utterance and the alternative transcript is found. The common word may begin at a time in the recognized utterance that is approximately near a time at which the common word begins in the alternative transcript.</p>
  <p num="p-0015">Production of a recognition correction may include selecting a word string from the alternative transcript. The word string is bound by the test word from the alternative transcript and by a word from the alternative transcript that is adjacent to the common word and between the test word and the common word. The method may include receiving a selection of one of the first recognition correction and the second recognition correction.</p>
  <p num="p-0016">Searching in time through the recognized utterance and through the alternative transcript may include designating a word adjacent to the test word as an alternative transcript word, designating a word adjacent to the selected word as an original transcript word, and comparing the original transcript word to the alternative transcript word.</p>
  <p num="p-0017">The original transcript word and the alternative transcript word may be designated as the common word if the original transcript word is identical to the alternative transcript word and if a time at which the original transcript word begins is near a time at which the alternative transcript word begins.</p>
  <p num="p-0018">A word in the alternative transcript that is adjacent to the alternative transcript word may be designated as the alternative transcript word whether or not the original transcript word is identical to the alternative transcript word if the original transcript word begins at a time that is later than a time at which the alternative transcript word begins. A word in the original transcript that is adjacent to the original transcript word may be designated as the original transcript word whether or not the original transcript word is identical to the alternative transcript word if the original transcript word begins at a time that is earlier than a time at which the alternative transcript word begins. A word in the original transcript that is adjacent to the original transcript word may be designated as the original transcript word and a word in the alternative transcript that is adjacent to the alternative transcript word may be designated as the alternative transcript word if the original transcript word is not identical to the alternative transcript word and if a time at which the original transcript word begins is near a time at which the alternative transcript word begins.</p>
  <p num="p-0019">A floating-choice-list system provides an advantage over prior choice-list systems when used in hand-held or portable devices, which often require use of a stylus as an input device. In such a stylus system, it would be difficult for a user to select two or more words to be corrected using prior choice-list systems. In particular, users would be required to perform the difficult task of carefully selecting a range of words to be corrected using a stylus before selecting an alternative transcript. The floating-choice-list system simplifies the required stylus events needed to perform a multiword correction for speech recognition on a hand-held device. Using the floating-choice-list system, the user only needs to contact the stylus somewhere in the word that begins the error-filled region in order to obtain a list of alternative transcripts.</p>
  <p num="p-0020">In another general aspect, a method of correcting incorrect text associated with recognition errors in computer-implemented speech recognition includes receiving a text document formed by recognizing speech utterances using a vocabulary. The method also includes receiving a general confusability matrix and receiving corrected text. The general confusability matrix has one or more values, each value indicating a likelihood of confusion between a first phoneme and a second phoneme. The corrected text corresponds to misrecognized text from the text document. If the corrected text is not in the vocabulary, the method includes generating a sequence of phonemes for the corrected text. The generated sequence of phonemes is aligned with phonemes of the misrecognized text and one or more values of the general confusability matrix are adjusted based on the alignment to form a specific confusability matrix. The method further includes searching the text document for additional instances of the corrected text using the specific confusability matrix.</p>
  <p num="p-0021">Implementations may include one or more of the following features. The method may further include outputting the text document. A list of recognition candidates may be associated with each recognized speech utterance. The step of generating the sequence of phonemes for the corrected text may include using a phonetic alphabet.</p>
  <p num="p-0022">The method may also include generating the general confusability matrix using empirical data. In that case, the empirical data may include information relating to a rate of confusion of phonemes for a preselected population, information relating to frequency characteristics of different phonemes, or information acquired during an adaptive training of a user.</p>
  <p num="p-0023">The step of searching the text document for the corrected text may include searching the text document for the sequence of phonemes for the corrected text. The step of searching the text document for the corrected text may include searching the text document for a sequence of phonemes that is likely to be confused with the sequence of phonemes for the corrected text.</p>
  <p num="p-0024">The step of searching the text document for the corrected text may include scoring a portion of the text document and comparing the score of the portion to an empirically determined threshold value to determine whether the portion of the text document includes a word that is not in the vocabulary. In this case, the method may further include outputting a result if it is determined that the portion of the text document includes a word that is not in the vocabulary. Moreover, the step of outputting the result may include highlighting the portion of the text document or re-recognizing the portion of the text document.</p>
  <p num="p-0025">In another general aspect a computer-implemented method for speech recognition includes receiving dictated text, generating recognized speech based on the received dictated text, receiving an edited text of the recognized speech, and determining an acoustic model for the edited text. The step of generating includes determining acoustic models for the dictated text that best match acoustic data for the dictated text. The edited text indicates a replacement for a portion of the dictated text. The method also includes determining whether to adapt acoustic models for the edited text based on the acoustic model for the edited text and the acoustic model for the dictated text portion.</p>
  <p num="p-0026">Implementations may include one or more of the following features. The method may also include calculating an acoustic model score based on a comparison between the acoustic model for the edited text and the acoustic data for the dictated text portion. In this case, the step of determining whether to adapt acoustic models for the edited text may be based on the calculated acoustic model score. The step of determining whether to adapt acoustic models may include calculating an original acoustic model-score based on a comparison between the acoustic model for the dictated text portion and the acoustic data for the dictated text portion. The step of determining whether to adapt acoustic models may include calculating a difference between the acoustic model score and the original acoustic model score. The step of determining whether to adapt acoustic models may include determining whether the difference is less than a predetermined value. The step of determining whether to adapt acoustic models may include adapting acoustic models for the edited text if the difference is less than a predetermined value. The step of determining whether to adapt acoustic models for the edited text may include bypassing adapting acoustic models for the edited text if the difference is greater than or equal to a predetermined value.</p>
  <p num="p-0027">The step of receiving the edited text of the recognized speech may occur during a recognition session in which the recognized speech is generated or after a recognition session in which the recognized speech is generated. The step of receiving the edited text of the recognized speech may include receiving a selection of the portion of the dictated text.</p>
  <p num="p-0028">The step of determining an acoustic model for the edited text may include searching for the edited text in a vocabulary or a backup dictionary used to generate the recognized speech. The step of determining an acoustic model for the edited text may include selecting an acoustic model that best matches the edited text.</p>
  <p num="p-0029">In another general aspect, a computer-implemented method of speech recognition includes performing speech recognition on an utterance to produce a recognition result for the utterance, receiving a selection of the recognition result, receiving a correction of the recognition result, and performing speech recognition on the correction using a constraint grammar that permits spelling and pronunciation in parallel. The method includes identifying whether the correction comprises a spelling or a pronunciation using the constraint grammar.</p>
  <p num="p-0030">Implementations may include one or more of the following features. The method may include generating a replacement result for the recognition result based on the correction.</p>
  <p num="p-0031">The constraint grammar may include a spelling portion and a dictation vocabulary portion. In that case, the spelling portion may indicate that the first utterance from the user is a letter in an alphabet. The vocabulary portion may indicate that the first utterance from the user is a word from the dictation vocabulary. The spelling portion may indicate a frequency with which letters occur in a language model. The dictation vocabulary portion may indicate a frequency with which words occur in a language model. The method may also include introducing a biasing value between the spelling and the dictation vocabulary portions of the constraint grammar.</p>
  <p num="p-0032">Systems and computer programs for implementing the described techniques and systems are also contemplated.</p>
  <p num="p-0033">The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features, objects, and advantages will be apparent from the description, the drawings, and the claims.</p>
<description-of-drawings> <heading>DESCRIPTION OF THE DRAWINGS</heading> <p num="p-0034"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a block diagram of a speech recognition system.</p>
    <p num="p-0035"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a block diagram of speech recognition software of the system of <figref idrefs="DRAWINGS">FIG. 1</figref>.</p>
    <p num="p-0036"> <figref idrefs="DRAWINGS">FIGS. 3A and 3B</figref> are state diagrams of a constraint grammar.</p>
    <p num="p-0037"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a flow chart of a speech recognition procedure.</p>
    <p num="p-0038"> <figref idrefs="DRAWINGS">FIG. 5</figref> is a block diagram of a speech recognition system.</p>
    <p num="p-0039"> <figref idrefs="DRAWINGS">FIGS. 6-8</figref> are block diagrams of other implementations of the system of <figref idrefs="DRAWINGS">FIG. 5</figref>.</p>
    <p num="p-0040"> <figref idrefs="DRAWINGS">FIG. 9</figref> is a block diagram of a recorder of the system of <figref idrefs="DRAWINGS">FIG. 5</figref>.</p>
    <p num="p-0041"> <figref idrefs="DRAWINGS">FIG. 10</figref> is a block diagram of a computer of the system of <figref idrefs="DRAWINGS">FIG. 5</figref>.</p>
    <p num="p-0042"> <figref idrefs="DRAWINGS">FIGS. 11A-11C</figref> are screen displays of a user interface of the speech recognition system of <figref idrefs="DRAWINGS">FIGS. 1 and 5</figref>.</p>
    <p num="p-0043"> <figref idrefs="DRAWINGS">FIGS. 12 and 16</figref> are flow charts of procedures implemented by a speech recognition system such as the system shown in <figref idrefs="DRAWINGS">FIG. 5</figref>.</p>
    <p num="p-0044"> <figref idrefs="DRAWINGS">FIG. 13</figref> is a block diagram of a procedure for retrieving transcripts from a speech recognition result determined using the procedures of <figref idrefs="DRAWINGS">FIGS. 12 and 16</figref>.</p>
    <p num="p-0045"> <figref idrefs="DRAWINGS">FIGS. 14</figref>, <b>17</b>, <b>18</b>, and <b>19</b>A-<b>19</b>C are screen displays of a user interface of the speech recognition system of <figref idrefs="DRAWINGS">FIGS. 1 and 5</figref>.</p>
    <p num="p-0046"> <figref idrefs="DRAWINGS">FIG. 15</figref> is a table showing synchronization between first and alternative transcripts used to determine a choice list using the procedures of <figref idrefs="DRAWINGS">FIG. 12 and 16</figref>.</p>
    <p num="p-0047"> <figref idrefs="DRAWINGS">FIG. 20</figref> is a flow chart of a procedure implemented by a speech recognition system such as the system shown in <figref idrefs="DRAWINGS">FIG. 5</figref>.</p>
    <p num="p-0048"> <figref idrefs="DRAWINGS">FIG. 21</figref> is a table showing a phoneme confusability matrix.</p>
    <p num="p-0049"> <figref idrefs="DRAWINGS">FIG. 22</figref> is a diagram showing correction of a word using the procedure of <figref idrefs="DRAWINGS">FIG. 20</figref>.</p>
    <p num="p-0050"> <figref idrefs="DRAWINGS">FIG. 23</figref> is a flow chart of a procedure implemented by a speech recognition system such as the system shown in <figref idrefs="DRAWINGS">FIG. 5</figref>.</p>
    <p num="p-0051"> <figref idrefs="DRAWINGS">FIGS. 24A and 24B</figref> are graphs showing correction of errors using the procedure of <figref idrefs="DRAWINGS">FIG. 23</figref> in comparison to random editing.</p>
    <p num="p-0052"> <figref idrefs="DRAWINGS">FIG. 25</figref> is a flow chart of a procedure implemented by a speech recognition system such as the system shown in <figref idrefs="DRAWINGS">FIG. 5</figref>.</p>
    <p num="p-0053"> <figref idrefs="DRAWINGS">FIG. 26</figref> shows a constraint grammar used in the procedure of <figref idrefs="DRAWINGS">FIG. 25</figref>.</p>
  </description-of-drawings> <p num="p-0054">Like reference symbols in the various drawings indicate like elements.</p>
  <heading>DESCRIPTION</heading> <p num="p-0055">Referring to <figref idrefs="DRAWINGS">FIG. 1</figref>, one implementation of a speech recognition system <b>100</b> includes input/output (I/O) devices (for example, microphone <b>105</b>, mouse <b>110</b>, keyboard <b>115</b>, and display <b>120</b>) and a general-purpose computer <b>125</b> having a processor <b>130</b>, an I/O unit <b>135</b> and a sound card <b>140</b>. A memory <b>145</b> stores data and programs such as an operating system <b>150</b>, an application program <b>155</b> (for example, a word processing program), and speech recognition software <b>160</b>.</p>
  <p num="p-0056">The microphone <b>105</b> receives the user's speech and conveys the speech, in the form of an analog signal, to the sound card <b>140</b>, which in turn passes the signal through an analog-to-digital (A/D) converter to transform the analog signal into a set of digital samples. Under control of the operating system <b>150</b> and the speech recognition software <b>160</b>, the processor <b>130</b> identifies utterances in the user's continuous speech. Utterances are separated from one another by a pause having a sufficiently large, predetermined duration (for example, 160-250 milliseconds). Each utterance may include one or more words of the user's speech.</p>
  <p num="p-0057">The system may also include an analog recorder port <b>165</b> and/or a digital recorder port <b>170</b>. The analog recorder port <b>165</b> is connected to the sound card <b>140</b> and is used to transmit speech recorded using a hand-held recorder to the sound card. The analog recorder port may be implemented as a microphone positioned to be next to the speaker of the hand-held recorder when the recorder is inserted into the port <b>165</b>, and may be implemented using the microphone <b>105</b>. Alternatively, the analog recorder port <b>165</b> may be implemented as a tape player that receives a tape recorded using a hand-held recorder and transmits information recorded on the tape to the sound card <b>140</b>.</p>
  <p num="p-0058">The digital recorder port <b>170</b> may be implemented to transfer a digital file generated using a hand-held digital recorder. This file may be transferred directly into memory <b>145</b>. The digital recorder port <b>170</b> may be implemented as a storage device (for example, a floppy drive or CD-ROM drive) of the computer <b>125</b>.</p>
  <p num="p-0059"> <figref idrefs="DRAWINGS">FIG. 2</figref> illustrates typical components of the speech recognition software <b>160</b>. For ease of discussion, the following description indicates that the components carry out operations to achieve specified results. However, it should be understood that each component actually causes the processor <b>130</b> to operate in the specified manner.</p>
  <p num="p-0060">Initially, a front end processing module <b>200</b> converts the digital samples <b>205</b> from the sound card <b>140</b> (or from the digital recorder port <b>170</b>) into frames of parameters <b>210</b> that represent the frequency content of an utterance. Each frame includes <b>24</b> parameters and represents a short portion (for example, 10 milliseconds) of the utterance.</p>
  <p num="p-0061">A recognizer <b>215</b> receives and processes the frames of an utterance to identify text corresponding to the utterance. The recognizer entertains several hypotheses about the text and associates a score with each hypothesis. The score reflects the probability that a hypothesis corresponds to the user's speech. For ease of processing, scores are maintained as negative logarithmic values. Accordingly, a lower score indicates a better match (a high probability) while a higher score indicates a less likely match (a lower probability), with the likelihood of the match decreasing as the score increases. After processing the utterance, the recognizer provides the best-scoring hypotheses to the control/interface module <b>220</b> as a list of recognition candidates, where each recognition candidate corresponds to a hypothesis and has an associated score. Some recognition candidates may correspond to text while other recognition candidates correspond to commands. Commands may include words, phrases, or sentences.</p>
  <p num="p-0062">The recognizer <b>215</b> processes the frames <b>210</b> of an utterance in view of one or more constraint grammars <b>225</b>. A constraint grammar, also referred to as a template or restriction rule, may be a limitation on the words that may correspond to an utterance, a limitation on the order or grammatical form of the words, or both. For example, a constraint grammar for menu-manipulation commands may include only entries from the menu (for example, file, edit) or command words for navigating through the menu (for example, up, down, top, bottom). Different constraint grammars may be active at different times. For example, a constraint grammar may be associated with a particular application program <b>155</b> and may be activated when the user opens the application program and deactivated when the user closes the application program. The recognizer <b>215</b> discards any hypothesis that does not comply with an active constraint grammar. In addition, the recognizer <b>215</b> may adjust the score of a hypothesis associated with a particular constraint grammar based on characteristics of the constraint grammar.</p>
  <p num="p-0063"> <figref idrefs="DRAWINGS">FIG. 3A</figref> illustrates an example of a constraint grammar for a select command used to select previously recognized text. As shown, a constraint grammar may be illustrated as a state diagram <b>400</b>. The select command includes the word select followed by one or more previously-recognized words, with the words being in the order in which they were previously recognized. The first state <b>405</b> of the constraint grammar indicates that the first word of the select command must be select. After the word select, the constraint grammar permits a transition along a path <b>410</b> to a second state <b>415</b> that requires the next word in the command to be a previously-recognized word. A path <b>420</b>, which returns to the second state <b>415</b>, indicates that the command may include additional previously-recognized words. A path <b>425</b>, which exits the second state <b>415</b> and completes the command, indicates that the command may include only previously-recognized words. <figref idrefs="DRAWINGS">FIG. 3B</figref> illustrates the state diagram <b>450</b> of the constraint grammar for the select command when a previously-recognized utterance is four score and seven. This state diagram could be expanded to include words from additional utterances. The select command and techniques for generating its constraint grammar are described further in U.S. Pat. No. 5,794,189, entitled CONTINUOUS SPEECH RECOGNITION and issued Aug. 11, 1998, which is incorporated herein by reference.</p>
  <p num="p-0064">The constraint grammar also may be expressed in Backus-Naur Form (BNF) or Extended BNF (EBNF). In EBNF, the grammar for the Select command is:
</p> <ul> <li id="ul0001-0001" num="0000"> <ul> <li id="ul0002-0001" num="0064">&lt;recognition result&gt;::=Select &lt;words&gt;,</li> </ul> </li> </ul> <p num="p-0065">where
</p> <ul> <li id="ul0003-0001" num="0000"> <ul> <li id="ul0004-0001" num="0066">&lt;words&gt;::=[PRW<b>1</b>[PRW<b>2</b>[PRW<b>3</b> . . . PRWn]]]|</li> <li id="ul0004-0002" num="0067">[PRW<b>2</b>[PRW<b>3</b> . . . PRWn]] | . . . [PRWn],</li> <li id="ul0004-0003" num="0068">PRWi is the previously-recognized word i,</li> <li id="ul0004-0004" num="0069">[ ] means optional,</li> <li id="ul0004-0005" num="0070">&lt; &gt; means a rule,</li> <li id="ul0004-0006" num="0071">| means an OR function, and</li> <li id="ul0004-0007" num="0072">::=means is defined as or is.</li> </ul> </li> </ul> <p num="p-0066">As illustrated in <figref idrefs="DRAWINGS">FIGS. 3A and 3B</figref>, this notation indicates that select may be followed by any ordered sequence of previously-recognized words. This grammar does not permit optional or alternate words. In some instances, the grammar may be modified to permit optional words (for example, an optional and to permit four score and seven or four score seven) or alternate words or phrases (for example, four score and seven or eighty seven). Constraint grammars are discussed further in U.S. Pat. No. 5,799,279, entitled CONTINUOUS SPEECH RECOGNITION OF TEXT AND COMMANDS and issued Aug. 25, 1998, which is incorporated herein by reference.</p>
  <p num="p-0067">Another constraint grammar <b>225</b> that may be used by the speech recognition software <b>160</b> is a large vocabulary dictation grammar. The large vocabulary dictation grammar identifies words included in the active vocabulary <b>230</b>, which is the vocabulary of words available to the software during recognition. The large vocabulary dictation grammar also indicates the frequency with which words occur. A language model associated with the large vocabulary dictation grammar may be a unigram model that indicates the frequency with which a word occurs independently of context, or a bigram model that indicates the frequency with which a word occurs in the context of a preceding word. For example, a bigram model may indicate that a noun or adjective is more likely to follow the word the than is a verb or preposition.</p>
  <p num="p-0068">Other constraint grammars <b>225</b> include an in-line dictation macros grammar for dictation commands, such as CAP or Capitalize to capitalize a word and New-Paragraph to start a new paragraph; the select X Y Z grammar discussed above and used in selecting text; an error correction commands grammar; a dictation editing grammar; an application command and control grammar that may be used to control a particular application program <b>155</b>; a global command and control grammar that may be used to control the operating system <b>150</b> and the speech recognition software <b>160</b>; a menu and dialog tracking grammar that may be used to manipulate menus; and a keyboard control grammar that permits the use of speech in place of input devices, such as the keyboard <b>115</b> or the mouse <b>110</b>.</p>
  <p num="p-0069">The active vocabulary <b>230</b> uses a pronunciation model in which each word is represented by a series of phonemes that comprise the phonetic spelling of the word. Each phoneme may be represented as a triphone that includes multiple nodes. A triphone is a context-dependent phoneme. For example, the triphone abc represents the phoneme b in the context of the phonemes a and c, with the phoneme b being preceded by the phoneme a and followed by the phoneme c.</p>
  <p num="p-0070">One or more vocabulary files may be associated with each user. The vocabulary files contain all of the words, pronunciations, and language model information for the user. Dictation and command grammars may be split between vocabulary files to optimize language model information and memory use, and to keep each single vocabulary file under 64,000 words.</p>
  <p num="p-0071">Separate acoustic models <b>235</b> are provided for each user of the system. Initially speaker-independent acoustic models of male or female speech are adapted to a particular user's speech using an enrollment program. The acoustic models may be further adapted as the system is used. The acoustic models are maintained in a file separate from the active vocabulary <b>230</b>.</p>
  <p num="p-0072">The acoustic models <b>235</b> represent phonemes. In the case of triphones, the acoustic models <b>235</b> represent each triphone node as a mixture of Gaussian probability density functions (PDFs). For example, node i of a triphone abc may be represented as ab<sup>i</sup>c:</p>
  <p num="p-0073"> <maths id="MATH-US-00001" num="00001"> <math overflow="scroll"> <mrow> <mrow> <mrow> <msup> <mi>ab</mi> <mi>i</mi> </msup> <mo></mo> <mi>c</mi> </mrow> <mo>=</mo> <mrow> <munder> <mo></mo> <mi>k</mi> </munder> <mo></mo> <mrow> <msub> <mi>w</mi> <mi>k</mi> </msub> <mo></mo> <mrow> <mi>N</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi></mi> <mi>k</mi> </msub> <mo>,</mo> <msub> <mi>c</mi> <mi>k</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> <mo>,</mo> </mrow> </math> </maths> <br>
where each W<sub>k </sub>is a mixture weight,
</p>
  <p num="p-0074"> <maths id="MATH-US-00002" num="00002"> <math overflow="scroll"> <mrow> <mrow> <mrow> <munder> <mo></mo> <mi>k</mi> </munder> <mo></mo> <msub> <mi>w</mi> <mi>k</mi> </msub> </mrow> <mo>=</mo> <mn>1</mn> </mrow> <mo>,</mo> </mrow> </math> </maths> <br>
<sub>k </sub>is a mean vector for the probability density function (PDF) N<sub>k</sub>, and C<sub>k </sub>is the covariance matrix for the PDF N<sub>k</sub>. Like the frames in the sequence of frames, the vectors <sub>k </sub>each include twenty four parameters. The matrices c<sub>k </sub>are twenty four by twenty four matrices. Each triphone node may be represented as a mixture of up to, for example, sixteen different PDFs.
</p>
  <p num="p-0075">A particular PDF may be used in the representation of multiple triphone nodes. Accordingly, the acoustic models <b>235</b> represent each triphone node as a collection of mixture weights w<sub>k </sub>associated with up to sixteen different PDFs N<sub>k </sub>and separately represent each PDF N<sub>k </sub>using a mean vector <sub>k </sub>and a covariance matrix c<sub>k</sub>. Use of a particular PDF to represent multiple triphone nodes permits the models to include a smaller number of PDFs than would be required if each triphone node included entirely separate PDFs. Since the English language may be roughly represented using 50 different phonemes, there may be up to 125,000 (50<sup>3</sup>) different triphones, which would result in a huge number of PDFs if each triphone node were represented by a separate set of PDFs. Representing multiple nodes with common PDFs also may remedy or reduce a data sparsity problem that results because some triphones (for example, tzp in the English language) rarely occur. These rare triphones may be represented by having closely-related triphones share the same set of PDFs.</p>
  <p num="p-0076">A large vocabulary dictation grammar may include multiple dictation topics (for example, medical or legal), each having its own vocabulary file and its own language model. A dictation topic includes a set of words, which represents the active vocabulary <b>230</b>, as well as an associated language model.</p>
  <p num="p-0077">A complete dictation vocabulary may consist of the active vocabulary <b>230</b> plus a backup vocabulary <b>245</b>. The backup vocabulary may include files that contain user-specific backup vocabulary words and system-wide backup vocabulary words.</p>
  <p num="p-0078">User-specific backup vocabulary words include words that a user has created while using the speech recognition software. These words are stored in vocabulary files for the user and for the dictation topic, and are available as part of the backup dictionary for the dictation topic regardless of user, and to the user regardless of which dictation topic is being used. For example, if a user is using a medical topic and adds the word ganglion to the dictation vocabulary, any other user of the medical topic will have immediate access to the word ganglion. In addition, the word will be written into the user-specific backup vocabulary. Then, if the user says ganglion while using a legal topic, the word ganglion will be available during correction from the backup dictionary.</p>
  <p num="p-0079">In addition to the user-specific backup vocabulary noted above, there is a system-wide backup vocabulary. The system-wide backup vocabulary contains all the words known to the system, including words that may currently be in an active vocabulary.</p>
  <p num="p-0080">The recognizer <b>215</b> may operate in parallel with a pre-filtering procedure <b>240</b>. Upon initiating processing of an utterance, the recognizer <b>215</b> requests from the pre-filtering procedure <b>240</b> a list of words that may have been spoken as the first word of the utterance (that is, words that may correspond to the first and subsequent frames of the utterance). The pre-filtering procedure <b>240</b> performs a coarse comparison of the sequence of frames with the active vocabulary <b>230</b> to identify a subset of the vocabulary for which a more extensive comparison using the recognizer is justified.</p>
  <p num="p-0081">After the pre-filtering procedure responds with the requested list of words, the recognizer initiates a hypothesis for each word from the list and compares acoustic models for the word to the frames of parameters representing the utterance. The recognizer uses the results of these comparisons to generate scores for the hypotheses. Hypotheses having excessive scores are eliminated from further consideration. As noted above, hypotheses that comply with no active constraint grammar also are eliminated.</p>
  <p num="p-0082">When the recognizer determines that a word of a hypothesis has ended, the recognizer requests from the pre-filtering procedure a list of words that may have been spoken just after the ending-time of the word. The recognizer then generates a new hypothesis for each word on the list, where each new hypothesis includes the words of the old hypothesis plus the corresponding new word from the list.</p>
  <p num="p-0083">In generating the score for a hypothesis, the recognizer uses acoustic scores for words of the hypothesis, a language model score that indicates the likelihood that words of the hypothesis are used together, and scores provided for each word of the hypothesis by the pre-filtering procedure. The recognizer may eliminate any hypothesis that is associated with a constraint grammar (for example, a command hypothesis), but does not comply with the constraint grammar.</p>
  <p num="p-0084">Referring to <figref idrefs="DRAWINGS">FIG. 4</figref>, the recognizer <b>215</b> may operate according to a procedure <b>1200</b>. First, prior to processing, the recognizer <b>215</b> initializes a lexical tree (step <b>1205</b>). The recognizer <b>215</b> then retrieves a frame of parameters (step <b>1210</b>) and determines whether there are hypotheses to be considered for the frame (step <b>1215</b>). The first frame always corresponds to silence so that there are no hypotheses to be considered for the first frame.</p>
  <p num="p-0085">If hypotheses need to be considered for the frame (step <b>1215</b>), the recognizer <b>215</b> goes to the first hypothesis (step <b>1220</b>). The recognizer then compares the frame to acoustic models <b>235</b> for the last word of the hypothesis (step <b>1225</b>) and, based on the comparison, updates a score associated with the hypothesis (step <b>1230</b>).</p>
  <p num="p-0086">After updating the score (step <b>1230</b>), the recognizer determines whether the user was likely to have spoken the word or words corresponding to the hypothesis (step <b>1235</b>). The recognizer makes this determination by comparing the current score for the hypothesis to a threshold value. If the score exceeds the threshold value, then the recognizer <b>215</b> determines that the hypothesis is too unlikely to merit further consideration and deletes the hypothesis (step <b>1240</b>).</p>
  <p num="p-0087">If the recognizer determines that the word or words corresponding to the hypothesis were likely to have been spoken by the user, then the recognizer determines whether the last word of the hypothesis is ending (step <b>1245</b>). The recognizer determines that a word is ending when the frame corresponds to the last component of the model for the word. If the recognizer determines that a word is ending (step <b>1245</b>), the recognizer sets a flag that indicates that the next frame may correspond to the beginning of a word (step <b>1250</b>).</p>
  <p num="p-0088">If there are additional hypotheses to be considered for the frame (step <b>1255</b>), then the recognizer selects the next hypothesis (step <b>1260</b>) and repeats the comparison (step <b>1225</b>) and other steps. If there are no more hypotheses to be considered for the frame (step <b>1255</b>), then the recognizer determines whether there are more frames to be considered for the utterance (step <b>1265</b>). The recognizer determines that there are more frames to be considered when two conditions are met. First, more frames must be available. Second, the best scoring node for the current frame or for one or more of a predetermined number of immediately preceding frames must have been a node other than the silence node (that is, the utterance has ended when the silence node is the best scoring node for the current frame and for a predetermined number of consecutive preceding frames).</p>
  <p num="p-0089">If there are more frames to be considered (step <b>1265</b>) and the flag indicating that a word has ended is set (step <b>1270</b>), or if there were no hypotheses to be considered for the frame (step <b>1215</b>), then the recognizer requests from the pre-filtering procedure <b>240</b> a list of words that may start with the next frame (step <b>1275</b>). Upon receiving the list of words from the pre-filtering procedure, the recognizer uses the list of words to create hypotheses or to expand any hypothesis for which a word has ended (step <b>1280</b>). Each word in the list of words has an associated score. The recognizer uses the list score to adjust the score for the hypothesis and compares the result to a threshold value. If the result is less than the threshold value, then the recognizer maintains the hypothesis. Otherwise, the recognizer determines that the hypothesis does not merit further consideration and abandons the hypothesis. As an additional part of creating or expanding the hypotheses, the recognizer compares the hypotheses to the active constraint grammars <b>225</b> and abandons any hypothesis that corresponds to no active constraint grammar. The recognizer then retrieves the next frame (step <b>1210</b>) and repeats the procedure.</p>
  <p num="p-0090">If there are no more speech frames to process, then the recognizer <b>215</b> provides the most likely hypotheses to the control/interface module <b>220</b> as recognition candidates (step <b>1285</b>).</p>
  <p num="p-0091">The control/interface module <b>220</b> controls operation of the speech recognition software and provides an interface to other software or to the user. The control/interface module receives the list of recognition candidates for each utterance from the recognizer. Recognition candidates may correspond to dictated text, speech recognition commands, or external commands. When the best-scoring recognition candidate corresponds to dictated text, the control/interface module provides the text to an active application, such as a word processor. The control/interface module also may display the best-scoring recognition candidate to the user through a graphical user interface. When the best-scoring recognition candidate is a command, the control/interface module <b>220</b> implements the command. For example, the control/interface module may control operation of the speech recognition software in response to speech recognition commands (for example, wake up, make that), and may forward external commands to the appropriate software.</p>
  <p num="p-0092">The control/interface module also controls the active vocabulary, acoustic models, and constraint grammars that are used by the recognizer. For example, when the speech recognition software is being used in conjunction with a particular application (for example, Microsoft Word), the control/interface module updates the active vocabulary to include command words associated with that application and activates constraint grammars associated with the application.</p>
  <p num="p-0093">Other functions provided by the control/interface module <b>220</b> may include a vocabulary customizer and a vocabulary manager. The vocabulary customizer optimizes the language model of a specific topic by scanning user supplied text. The vocabulary manager is a developer tool that is used to browse and manipulate vocabularies, grammars, and macros. Each such function of the control/interface module <b>220</b> may be implemented as an executable program that is separate from the main speech recognition software. Similarly, the control/interface module <b>220</b> also may be implemented as a separate executable program.</p>
  <p num="p-0094">The control/interface module <b>220</b> also may provide an enrollment program that uses an enrollment text and a corresponding enrollment grammar to customize the speech recognition software to a specific user. The enrollment program may operate in an interactive mode that guides the user through the enrollment process, or in a non-interactive mode that permits the user to enroll independently of the computer. In the interactive mode, the enrollment program displays the enrollment text to the user and the user reads the displayed text. As the user reads, the recognizer <b>215</b> uses the enrollment grammar to match a sequence of utterances by the user to sequential portions of the enrollment text. When the recognizer <b>215</b> is unsuccessful, the enrollment program prompts the user to repeat certain passages of the text. The recognizer uses acoustic information from the user's utterances to train or adapt acoustic models <b>235</b> based on the matched portions of the enrollment text. One type of interactive enrollment program is discussed in U.S. Pat. No. 6,212,498, entitled ENROLLMENT IN SPEECH RECOGNITION and issued Apr. 3, 2001, which is incorporated herein by reference.</p>
  <p num="p-0095">In the non-interactive mode, the user reads the text without prompting from the computer. This offers the considerable advantage that, in addition to reading text displayed by the computer, the user can read from a printed text independent of the computer. Thus, the user could read the enrollment text into a portable recording device and later download the recorded information into the computer for processing by the recognizer. In addition, the user is not required to read every word of the enrollment text, and may skip words or paragraphs as desired. The user also may repeat portions of the text. This adds substantial flexibility to the enrollment process.</p>
  <p num="p-0096">The enrollment program may provide a list of enrollment texts, each of which has a corresponding enrollment grammar, for the user's selection. Alternatively, the user may input an enrollment text from another source. In this case, the enrollment program may generate the enrollment grammar from the input enrollment text, or may employ a previously generated enrollment grammar.</p>
  <p num="p-0097">The control/interface module <b>220</b> may also implement error correction and cursor/position manipulation procedures of the software <b>160</b>. Error correction procedures include a make that command and a spell that command. Cursor/position manipulation procedures include the select command discussed above and variations thereof (for example, select [start] through [end]), insert before/after commands, and a resume with command.</p>
  <p num="p-0098">During error correction, word searches of the backup vocabularies start with the user-specific backup dictionary and then check the system-wide backup dictionary. The backup dictionaries also are searched when there are new words in text that a user has typed.</p>
  <p num="p-0099">When the system makes a recognition error, the user may invoke an appropriate correction command to remedy the error. Various correction commands are discussed in U.S. Pat. No. 5,794,189, entitled CONTINUOUS SPEECH RECOGNITION and issued Aug. 11, 1998, U.S. Pat. No. 6,064,959, entitled ERROR CORRECTION IN SPEECH RECOGNITION and issued May 16, 2000, and U.S. application Ser. No. 09/094,611, entitled POSITION MANIPULATION IN SPEECH RECOGNITION and filed Jun. 15, 1998, all of which are incorporated herein by reference.</p>
  <p num="p-0100">Referring to <figref idrefs="DRAWINGS">FIG. 5</figref>, the speech recognition system may be implemented using a system <b>1400</b> for performing recorded actions that includes a pocket-sized recorder <b>1405</b> and a computer <b>1410</b> (not shown to scale). When data is to be transmitted, the recorder <b>1405</b> may be connected to the computer <b>1410</b> using a cable <b>1415</b>. Other data transmission techniques, such as infrared data transmission, also may be used.</p>
  <p num="p-0101">In the described implementation, the recorder <b>1405</b> is a digital recorder having time stamp capabilities. One recorder meeting these criteria is the Dragon Naturally Mobile Pocket Recorder RI manufactured for Dragon Systems, Inc., of Newton, Mass. by Voice It Worldwide, Inc. In other implementations, the recorder may be a digital recorder lacking time stamp capabilities, or an analog recorder using a magnetic tape.</p>
  <p num="p-0102"> <figref idrefs="DRAWINGS">FIG. 6</figref> illustrates a variation <b>1400</b>A of the system in which an output device <b>1420</b> is attached to the recorder <b>1405</b>. Information about action items recorded using the recorder <b>1405</b> and processed by the computer <b>1410</b> is transferred automatically via the cable <b>1415</b> for display on the output device <b>1420</b>. This variation permits the user to access, for example, appointments and contact information using the display <b>1420</b>. Keys <b>1425</b> on the recorder are used to navigate through displayed information.</p>
  <p num="p-0103"> <figref idrefs="DRAWINGS">FIG. 7</figref> illustrates another variation <b>1400</b>B in which the recording and output functionality are implemented using a PDA or a hand-held computer <b>1430</b>. With this variation, it is contemplated that some instances of the hand-held computer <b>1430</b> may have sufficient processing capacity to perform some or all of the speech recognition, parsing, and other processing tasks described below.</p>
  <p num="p-0104"> <figref idrefs="DRAWINGS">FIG. 8</figref> illustrates another variation <b>1400</b>C in which the user's speech is immediately transmitted to the computer <b>1410</b> using, for example, a cellular telephone <b>1435</b>. This variation permits the user to dictate actions over an extended period that might exceed the capacity of a recorder. Audio feedback may be provided to permit immediate review of an action item, interactive correction, and performance of the action item. The interactive correction may be provided using spoken commands, telephone key strokes, or a combination of the two.</p>
  <p num="p-0105">Referring also to <figref idrefs="DRAWINGS">FIG. 9</figref>, the recorder <b>1405</b> includes a record button <b>1500</b> that activates the recorder, a microphone <b>1505</b> that converts a user's speech into an analog electrical signal, an analog-to-digital converter <b>1510</b> that converts the analog electrical signal into a series of digital samples, a processor <b>1515</b>, a memory <b>1520</b>, and an output port <b>1525</b> for connection to the cable <b>1415</b>. When the user presses the record button <b>1500</b> and speaks into the microphone <b>1505</b>, the processor creates a file <b>1530</b> in memory <b>1520</b> and stores in the file a time stamp <b>1535</b> corresponding to the time at which the button was pressed in the file. The processor then stores the digital samples <b>1540</b> corresponding to the user's speech in the same file. In some implementations, the processor uses compression techniques to compress the digital samples to reduce storage and data transfer requirements. The user may use the recorder multiple times before transferring data to the computer <b>1410</b>.</p>
  <p num="p-0106">Referring also to <figref idrefs="DRAWINGS">FIG. 10</figref>, the computer <b>1410</b> may be a standard desktop computer. In general, such a computer includes input/output (I/O) devices (for example, microphone <b>1605</b>, mouse <b>1610</b>, keyboard <b>1615</b>, and display <b>1620</b>) and a console <b>1625</b> having a processor <b>1630</b>, an I/O unit <b>1635</b> and a sound card <b>1640</b>. A memory <b>1645</b> stores data and programs such as an operating system <b>1650</b>, an application program <b>1655</b> (for example, a word processing program), and speech recognition software <b>1660</b>.</p>
  <p num="p-0107">The computer <b>1410</b> may be used for traditional speech recognition. In this case, the microphone <b>1605</b> receives the user's speech and conveys the speech, in the form of an analog signal, to the sound card <b>1640</b>, which in turn passes the signal through an analog-to-digital (A/D) converter to transform the analog signal into a set of digital samples. Under control of the operating system <b>1650</b> and the speech recognition software <b>1660</b>, the processor <b>1630</b> identifies utterances in the user's continuous speech. Utterances are separated from one another by a pause having a sufficiently large, predetermined duration (for example, 160-250 milliseconds). Each utterance may include one or more words of the user's speech.</p>
  <p num="p-0108">The system also includes a digital recorder port <b>1665</b> and/or an analog recorder port <b>1670</b> for connection to the cable <b>1415</b>. The digital recorder port <b>1665</b> is used to transfer files generated using the recorder <b>1405</b>. These files may be transferred directly into memory <b>1645</b>, or to a storage device such as hard drive <b>1675</b>. The analog recorder port <b>1670</b> is connected to the sound card <b>1640</b> and is used to transmit speech recorded using an analog or digital recorder to the sound card. The analog recorder port may be implemented using a line in port. The hand-held recorder is connected to the port using a cable connected between the line in port and a line out or speaker port of the recorder. The analog recorder port also may be implemented using a microphone, such as the microphone <b>1605</b>. Alternatively, the analog recorder port <b>1670</b> may be implemented as a tape player that receives a tape recorded using a hand-held recorder and transmits information recorded on the tape to the sound card <b>1640</b>.</p>
  <p num="p-0109">To implement the speech recognition and processing functions of the system <b>1400</b>, the computer <b>1410</b> runs interface software <b>1680</b>, the speech recognition software <b>1660</b>, a parser <b>1685</b>, and back-end software <b>1690</b>. Dragon NaturallySpeaking Preferred Edition <b>3</b>.<b>1</b>, available from Dragon Systems, Inc. of Newton, Mass., offers one example of suitable speech recognition software. The interface software <b>1680</b> provides a user interface for controlling the transfer of data from the digital recorder and the generation of action items for use by the back-end software <b>1690</b>. In general, the user interface may be controlled using input devices such as a mouse or keyboard, or using voice commands processed by the speech recognition software.</p>
  <p num="p-0110">After transferring data from the recorder, the interface software <b>1680</b> provides the digital samples for an action item to the speech recognition software <b>1660</b>. If the digital samples have been stored using compression techniques, the interface software <b>1680</b> decompresses them prior to providing them to the speech recognition software. In general, the speech recognition software analyzes the digital samples to produce a sequence of text, and provides this sequence to the interface software <b>1680</b>. The interface software <b>1680</b> then transfers the text and the associated time stamp, if any, to the parser <b>1685</b>, which processes the text in conjunction with the time stamp to generate a parsed version of the action item. The parser returns the parsed action item to the interface software, which displays it to the user. After any editing by the user, and with user approval, the interface software then transfers the action item to the appropriate back-end software <b>1690</b>. An example of back-end software with which the system works is personal information management software, such as Microsoft Outlook, which is available from Microsoft Corporation of Redmond, Wash. Other suitable back-end software includes contact management software, time management software, expense reporting applications, electronic mail programs, and fax programs.</p>
  <p num="p-0111">Various systems for recognizing recorded speech and performing actions identified in the speech are discussed in U.S. application Ser. No. 09/432,155, entitled PERFORMING RECORDED ACTIONS and filed Jun. 10, 1999, which is incorporated herein by reference.</p>
  <p num="p-0112">A user may dictate a document into an audio recorder such as recorder <b>1405</b> and then may download the dictated audio information into a speech recognition system like the one described above. Likewise, the user may dictate a document directly into a microphone connected to the speech recognition system, which may be implemented in a desktop computer or a hand-held electronic device.</p>
  <p num="p-0113">In a large vocabulary continuous speech recognition system, the user may correct misrecognition errors by selecting a range of characters from the speech recognition results. The speech recognition system presents a list of alternative recognitions for that selected range of characters by, for example, opening the correction window with a choice list.</p>
  <p num="p-0114">This type of error correction is used in Dragon NaturallySpeaking and other commercial large vocabulary continuous speech recognition systems currently on the market. Correction in speech recognition systems typically requires the user to perform two steps. First, the user identifies the range of words that are incorrect, which may be referred to as an error-filled region. The error-filled region includes a beginning character position and an ending character position. Second, the user selects a replacement from a list of alternatives for the selected error-filled region.</p>
  <p num="p-0115">Correction in the speech recognition system may include a feature called double click to correct, in which the user double clicks on the first word of the error-filled region in order to correct two or more words in the recognition result. (In a system, such as one employing a handheld device, in which a stylus is used instead of a mouse, this feature may be implemented by tapping, or double tapping, the stylus on the first word in the error-filled region.) The speech recognition system automatically selects n words from the user's document beginning with the word that was selected, where n is a predetermined integer that indicates the number of selected words. In an implementation in which n equals three, the speech recognition system displays a list of alternative recognition results, where each alternative recognition result replaces the three words that begin at the location of the word the user selected.</p>
  <p num="p-0116">Although the double-click-to-correct feature relieves the user of the burden of having to select the end of the error-filled region, the end of the error-filled region is always computed to be the end of the group of n words including the word that was selected. Accordingly, the selected range of words to be corrected (that is, n words including the selected word) may be larger than the actual error-filled region, thus complicating the error correction process. In some cases, the selected range of words to be corrected (n words including the selected word) may be smaller than the actual error-filled region, thus forcing the user to cancel the list of alternatives and directly reselect the appropriate range of characters.</p>
  <p num="p-0117">The following description provides a discussion of additional systems and methods that may be implemented to further improve error correction in speech recognition. These additional systems and methods may be implemented to correct errors in any speech recognition environment, and are not limited to the speech recognition systems described in detail and referenced above.</p>
  <p num="p-0118">Choice List for Recognition Results</p>
  <p num="p-0119">In the example shown in <figref idrefs="DRAWINGS">FIG. 11</figref> A, the recognizer <b>215</b> misrecognizes the sentence let's recognize speech and the control/interface module <b>220</b> responds by inserting the incorrect text let's wreck a nice beach <b>1700</b> in dictation window <b>1702</b>. In a conventional speech recognition system, as shown in <figref idrefs="DRAWINGS">FIG. 11B</figref>, the user causes the control/interface module <b>220</b> to generate a choice list <b>1705</b> by selecting the word wreck <b>1710</b> in the recognition result <b>1700</b>. The choice list <b>1705</b> includes a list of alternative recognition candidates for the word wreck <b>1710</b>.</p>
  <p num="p-0120">A speech recognition system may determine the error-filled region on the fly during correction. In this way, the user selects (by clicking, double-clicking, tapping, double tapping, or in some other way) the first word in an error-filled region and the speech recognition system automatically computes a width of the error-filled region to determine alternative recognition results. The number of words in each of the alternative recognition results in the choice list varies (that is, the length of each of the elements in the choice list is floating) because there is no rigidly defined end to the error-filled region.</p>
  <p num="p-0121">In <figref idrefs="DRAWINGS">FIG. 11C</figref>, a speech recognition system has provided an improved list <b>1720</b> (also referred to as a floating choice list) of alternatives for the selected word (wreck) <b>1710</b>. The improved list <b>1720</b> includes alternatives for the selected word <b>1710</b> along with alternatives for one or more words following wreck in the document. In this way, the user need not identify the end of an error-filled region. For example, the first entry <b>1730</b> in the choice list is recognize speech.</p>
  <p num="p-0122">Referring to <figref idrefs="DRAWINGS">FIG. 12</figref>, the speech recognition system performs a procedure <b>1800</b> for providing the floating choice list. Initially, the speech recognition system receives input from a user indicating an incorrect word in an original transcript (step <b>1805</b>). For example, the user may position a cursor on the screen over a word to select the incorrect word. The speech recognition system converts the screen coordinate into a character position in the original transcript. Then, using that character position, the speech recognition system finds the beginning of the word that includes that character positionthis word corresponds to the incorrect word.</p>
  <p num="p-0123">The speech recognition system retrieves a list of transcripts based on the indicated incorrect word (step <b>1810</b>). The speech recognition system accomplishes this retrieval by first retrieving a result object that created the incorrect word and includes the character position. Each transcript includes a sequence of words and start times (called index times), where a start time is associated with each word in the transcript. The index time may be given in units of milliseconds relative to the start of an utterance.</p>
  <p num="p-0124">For example, referring to <figref idrefs="DRAWINGS">FIG. 13</figref>, a result object <b>1900</b> is retrieved from an array <b>1905</b> of result objects for an original transcript <b>1910</b>, where each result object <b>1900</b> describes a recognition. A list <b>1915</b> of transcripts for result object <b>1900</b> is retrieved. Each transcript in the list includes a set of one or more words (W<sub>ij</sub>) and associated index times (t<sub>ij</sub>), where the index i indicates the transcript and the index j indicates the word in the transcript. The first (or original) transcript in the list <b>1915</b> of transcripts corresponds to the best-scoring recognition result presented to the user. The remaining transcripts in the list <b>1915</b> correspond to alternative transcripts that will be compared to the original transcript in subsequent analysis by the speech recognition system.</p>
  <p num="p-0125">Referring again to <figref idrefs="DRAWINGS">FIG. 12</figref>, after the list of transcripts is retrieved (step <b>1810</b>), the speech recognition system analyzes the original transcript to determine the index time of the incorrect word (step <b>1815</b>). The speech recognition system then selects one of the alternative transcripts from the list of transcripts for analysis (step <b>1820</b>). In one implementation, the speech recognition system selects the next best-scoring alternative transcript from the list of transcripts.</p>
  <p num="p-0126">After the alternative transcript is selected (step <b>1820</b>), the speech recognition system analyzes the alternative transcript (step <b>1825</b>) by searching for an end of an error-filled region that begins with a word whose index time most closely matches that of the incorrect word selected by the user. As discussed in detail below, the speech recognition system searches for the location at which that alternative result transcript resynchronizes, or matches in time, with the original transcript. The speech recognition system searches forward in both the original transcript and the alternative transcript until the system finds a word that is the same in both transcripts and that begins at approximately the same time in both transcripts. If the speech recognition system finds such a word, then the speech recognition system produces a replacement result that extends from the selected word to the matching word. The speech recognition system may also produce a replacement result when the incorrect word is positioned near the end of the original transcript, with the replacement result extending from the selected word to the end of the transcript.</p>
  <p num="p-0127">If the speech recognition system produces a replacement result (step <b>1830</b>), the speech recognition system compares the replacement result to other replacement results (step <b>1840</b>). If the replacement result has not been encountered before (step <b>1840</b>), the speech recognition system saves the replacement result to the choice list (step <b>1845</b>) and checks for additional alternative transcripts (step <b>1850</b>). The system also checks for additional alternative transcripts (step <b>1850</b>) if the replacement result has been encountered before and, therefore, is not saved (step <b>1840</b>), or if the speech recognition system does not produce a replacement result (step <b>1830</b>).</p>
  <p num="p-0128">If there are additional alternative transcripts (step <b>1850</b>), the speech recognition system selects a next alternative transcript (step <b>1855</b>) for analysis (step <b>1825</b>). If there are no additional alternative transcripts for analysis (step <b>1850</b>), the speech recognition system presents the choice list (step <b>1860</b>) and performs post-presentation updating (step <b>1865</b>).</p>
  <p num="p-0129">Referring to <figref idrefs="DRAWINGS">FIGS. 14 and 15</figref>, for example, the speech recognition system has recognized the user's utterance as I am dictating about the new Yorkshire taste which is delicious, as indicated in the dictation window <b>2000</b>. The user has selected the word new <b>2005</b> in dictation window <b>2000</b>, thus indicating that new is a word to be corrected. The speech recognition system has retrieved an original transcript I am dictating about the new Yorkshire taste which is delicious <b>2100</b> and an alternative transcript I am dictating about the New York shirt taste which is delicious <b>2105</b>.</p>
  <p num="p-0130">In <figref idrefs="DRAWINGS">FIG. 15</figref>, the index times <b>2110</b> of the words of the original transcript are shown below the words of the original transcript <b>2100</b> and the index times <b>2115</b> of the words in the alternative transcript <b>2105</b> are shown below the words of the alternative transcript. After the occurrence of the word new, the alternative transcript resynchronizes with the original transcript at the word taste because the word taste in transcript <b>2100</b> and the word taste in transcript <b>2105</b> occur at approximately the same index time. Thus, because the alternative transcript resynchronizes with the original transcript at the word taste, the speech recognition system computes the end of the error-filled region of the alternative transcript <b>2105</b> to be at the word taste.</p>
  <p num="p-0131">As shown in <figref idrefs="DRAWINGS">FIG. 14</figref>, the speech recognition system produces a list of replacement results including replacement result New York shirt <b>2007</b> for the transcript <b>2105</b> and presents the list of replacement results in a choice list <b>2010</b>.</p>
  <p num="p-0132">Referring also to <figref idrefs="DRAWINGS">FIG. 16</figref>, the speech recognition system performs a procedure <b>1825</b> for analyzing the alternative transcript. First, the speech recognition system finds a test word in the alternative transcript that has an index time nearest to the index time of the word to be corrected from the original transcript (step <b>2200</b>). If the test word is identical to the word to be corrected (step <b>2205</b>), then the speech recognition system ignores the alternative transcript and exits the procedure <b>1825</b>.</p>
  <p num="p-0133">If the test word is not identical to the word to be corrected (step <b>2205</b>), then the speech recognition system designates a word immediately following the word to be corrected in the original transcript as an original transcript word and designates a word immediately following the test word in the alternative transcript as an alternative transcript word for subsequent analysis (step <b>2207</b>). The speech recognition system then determines if the original transcript word is identical to the alternative transcript word (step <b>2210</b>).</p>
  <p num="p-0134">If the original transcript word is identical to the alternative transcript word (step <b>2210</b>), the speech recognition system computes whether the index time of the original transcript word is near the index time of the alternative transcript word (step <b>2215</b>). If the index times of the original transcript word and the alternative transcript word are near each other (step <b>2215</b>), then the speech recognition system extracts a replacement result that begins with the test word and ends with the word prior to the alternative transcript word (step <b>2220</b>).</p>
  <p num="p-0135">The required level of nearness between the index times may be controlled using a parameter that may be manually adjusted and fine-tuned by a developer of the speech recognition system. For example, the system may calculate a difference between index times for different words, and may designate index times as near each other when this difference is less than a threshold amount.</p>
  <p num="p-0136">If the original transcript word is not identical to the alternative transcript word (step <b>2210</b>), then the speech recognition system computes whether the index time of the original transcript word is near the index time of the alternative transcript word (step <b>2225</b>). If the index times of the original transcript word and the alternative transcript word are near each other (step <b>2225</b>), then the speech recognition system selects the word adjacent to the original transcript word in the original transcript as the original transcript word for subsequent analysis and selects the word adjacent to the alternative transcript word in the alternative transcript as the alternative transcript word for subsequent analysis (step <b>2230</b>).</p>
  <p num="p-0137">If the index time of the original transcript word is not near the index time of the alternative transcript word (steps <b>2215</b> or <b>2225</b>), the speech recognition system computes whether the index time of the original transcript word is later than the index time of the alternative transcript word (step <b>2235</b>).</p>
  <p num="p-0138">If the index time of the original transcript word is later than the index time of the alternative transcript word (step <b>2235</b>), then the speech recognition system designates the word adjacent to the alternative transcript word in the alternative transcript as the alternative transcript word for subsequent analysis (step <b>2240</b>). If the index time of the original transcript word is not near (steps <b>2215</b> or <b>2225</b>) or is not later than (step <b>2235</b>) the index time of the alternative transcript word, then the index time of the original transcript word is earlier than the index time of the alternative transcript word (step <b>2245</b>). In this case, the speech recognition system selects the word adjacent to the original transcript word in the original transcript as the original transcript word for subsequent analysis (step <b>2250</b>).</p>
  <p num="p-0139">The example of <figref idrefs="DRAWINGS">FIGS. 14 and 15</figref> will now be analyzed with respect to the procedures <b>1800</b> and <b>1825</b>. In <figref idrefs="DRAWINGS">FIG. 14</figref>, the speech recognition system has received input from a user indicating that the word new <b>2005</b> from the original transcript <b>2100</b> is to be corrected (step <b>1805</b>). After selecting alternative transcript <b>2105</b> for examination (step <b>1820</b>), the speech recognition system finds the test word New York <b>2120</b> (where New York is a single lexical entry that is treated as a word by the system) in the alternative transcript <b>2105</b> (step <b>2200</b>). The test word New York has an index time of <b>1892</b>, which is nearest the index time <b>1892</b> of the word new. Next, the speech recognition system compares the test word New York to the word new to determine that these words are not identical (step <b>2205</b>). Therefore, the speech recognition system sets the word Yorkshire as the original transcript word and sets the word shirt as the alternative transcript word (step <b>2207</b>).</p>
  <p num="p-0140">When the speech recognition system compares the word Yorkshire to the word shirt the speech recognition system determines that these words are not identical (step <b>2210</b>). Furthermore, because the index time of the word Yorkshire is earlier than the index time of the word shirt (step <b>2245</b>), the speech recognition system selects the word taste, which follows the word Yorkshire in the original transcript <b>2100</b>, and has an index time of <b>2729</b>, as the original transcript word (step <b>2250</b>).</p>
  <p num="p-0141">At this point, the original transcript word is taste with an index of <b>2729</b> and the alternative transcript word is shirt with an index of <b>2490</b>. Because the original transcript word and the alternative transcript word are not identical (step <b>2210</b>), and because the index time of the original transcript word taste is later than the index time of the alternative transcript word shirt (step <b>2235</b>), the speech recognition system selects the word taste, which has an index of <b>2809</b> and follows shirt in the alternative transcript <b>2105</b>, as the alternative transcript word (step <b>2240</b>). At this point, the original transcript word is taste with an index of <b>2729</b> (from the original transcript <b>2100</b>) and the alternative transcript word is taste with an index of <b>2809</b> (from the alternative transcript <b>2105</b>).</p>
  <p num="p-0142">Because the original transcript word and the alternative transcript word are identical to each other (step <b>2210</b>), and because the index times of the original transcript word and the alternative transcript word are near each other (step <b>2215</b>), the speech recognition system extracts a replacement result from the alternative transcript <b>2105</b> that corresponds to New York shirt <b>2007</b> (step <b>2220</b>).</p>
  <p num="p-0143">Referring also to <figref idrefs="DRAWINGS">FIG. 17</figref>, dictation window <b>2300</b> is shown in which the user has selected the word the to be corrected in the original transcript I am dictating about the new Yorkshire taste which is delicious. In this case, the speech recognition system has provided a choice list <b>2305</b> that includes the replacement result thee and the original result the.</p>
  <p num="p-0144">In <figref idrefs="DRAWINGS">FIG. 18</figref>, for example, a dictation window <b>2400</b> is shown in which the user has selected the word taste to be corrected in the original transcript I am dictating about the new Yorkshire taste which is delicious. In this case, the speech recognition system has provided a choice list <b>2405</b> that includes the replacement results paste, faced witch's, and case to, and the original result taste.</p>
  <p num="p-0145">Upon receiving input from the user that indicates the word to be corrected, the speech recognition system may highlight the word to be corrected from the original transcript. For example, in <figref idrefs="DRAWINGS">FIG. 14</figref>, the word new is highlighted, in <figref idrefs="DRAWINGS">FIG. 17</figref>, the word the is highlighted, and in <figref idrefs="DRAWINGS">FIG. 18</figref>, the word taste is highlighted.</p>
  <p num="p-0146">Referring again to <figref idrefs="DRAWINGS">FIG. 12</figref>, the speech recognition system performs post-presentation updating at step <b>1865</b> when the choice list is presented to the user (step <b>1860</b>). Post-presentation updating includes updating the dictation window transcript to reflect a user selection of a replacement result from the choice list. For example, referring also to <figref idrefs="DRAWINGS">FIGS. 19A-19C</figref>, the transcript may be updated with the replacement result and the replacement result in the updated transcript may be highlighted when the user selects a replacement result from the choice list. In <figref idrefs="DRAWINGS">FIG. 19A</figref>, the user selects New York sure <b>2500</b> from choice list <b>2505</b> and the replacement result New York sure is highlighted in updated transcript <b>2510</b>. As shown in <figref idrefs="DRAWINGS">FIG. 19B</figref>, the user selects New York shirt paste <b>2515</b> from choice list <b>2505</b> and the replacement result New York shirt paste is highlighted in updated transcript <b>2520</b>. In <figref idrefs="DRAWINGS">FIG. 19C</figref>, the user selects New York shirt faced witch's <b>2525</b> from choice list <b>2505</b> and the replacement result New York shirt faced witch's <b>2525</b> is highlighted in updated transcript <b>2530</b>.</p>
  <p num="p-0147">As shown in <figref idrefs="DRAWINGS">FIGS. 17 and 18</figref>, replacement results produced by the speech recognition system during procedure <b>1800</b> and shown in the choice lists may include just a single word that either matches the original transcript or does not match the original transcript. Thus, the word the, which matches the original transcript in <figref idrefs="DRAWINGS">FIG. 17</figref>, and the word thee, which does not match the original transcript, are displayed in choice list <b>2305</b>.</p>
  <p num="p-0148">Post-presentation updating (step <b>1865</b>) may include ending a correction session upon receipt of an indication from the user that an alternative result reflects the user's original intent. For example, the speech recognition system may terminate the correction session when the user clicks (or double clicks) a button that closes the choice list or when the user selects an appropriate alternative result.</p>
  <p num="p-0149">Finding Multiple Misrecognitions of Utterances in a Transcript</p>
  <p num="p-0150">A user may dictate a document into an audio recorder such as recorder <b>1405</b> and then may download the dictated audio information into a speech recognition system like the one described above. Likewise, the user may dictate a document directly into a microphone connected to the speech recognition system, which may be implemented in a desktop computer or a hand-held electronic device. In either case, the speech recognition system may be unable to recognize particular words that are not in the speech recognition system's vocabulary. These words are referred to as out-of-vocabulary (OOV) words.</p>
  <p num="p-0151">For example, the speech recognition system's vocabulary may not contain proper names, such as the name Fooberman, or newer technical terms, such as the terms edutainment and winsock. When it encounters an OOV word, the speech recognition system may represent the word using combinations of words and phonemes in its vocabulary that most closely resemble the OOV word. For example, the speech recognition system may recognize the word Fooberman as glue bar man, in which case the speech recognition system has replaced the phoneme for f with the phonemes for gl and the phoneme r with the phoneme r.</p>
  <p num="p-0152">A user may proofread a text document representing recognized speech to correct OOV words and other misrecognitions within the text document. The user uses a keyboard, a mouse or speech to select what appears to be a misrecognized word, plays back the audio signal that produced the apparently misrecognized word, and then manually corrects the misrecognized word using, for example, a keyboard or speech. The user performs this manual correction for each apparently misrecognized word in the text document. The user must remain alert while reading over the text document because it is sometimes difficult to detect a misrecognized word. This may be particularly important when detecting OOV words, which tend to be uncommon.</p>
  <p num="p-0153">Optionally, the user (or the speech recognition system) may add the OOV word to the speech recognition system's vocabulary once the user realizes that the system has misrecognized the word. The speech recognition system may then re-recognize the whole text document using a new vocabulary that now includes what was previously an OOV word. This re-recognition process may take a relatively long time.</p>
  <p num="p-0154">Referring also to <figref idrefs="DRAWINGS">FIG. 20</figref>, the speech recognition system may substantially reduce delays associated with correcting the OOV word by implementing an OOV global correction according to a procedure <b>2600</b>. Initially, the speech recognition system receives a general phoneme confusability matrix (step <b>2605</b>) and the text document representing recognized speech (step <b>2610</b>). The text document includes associated lists of recognition candidates for each recognized utterance. The lists are created by the system during recognition.</p>
  <p num="p-0155">The general phoneme confusability matrix is built before the procedure of <figref idrefs="DRAWINGS">FIG. 20</figref> is implemented using the premise that any phoneme may be confused for another phoneme. The probability of confusion depends on the characteristics of the two phonemes and the characteristics of the speaker's pronunciation. For example, the phoneme for m is commonly mistaken for the phoneme for n, and the phoneme for t is commonly mistaken for the phoneme for d.</p>
  <p num="p-0156"> <figref idrefs="DRAWINGS">FIG. 21</figref> shows a general phoneme confusability matrix <b>2700</b> for a subset of the phonemes in a one type of phonetic alphabet. Using the phonetic alphabet, for example, the phrase the term of this agreement shall begin on the 31st day of Jan., \comma 1994, \comma may translate into phonemes:</p>
  <p num="h-0007">D/tVm@vDis@grEmtS@lb/ginonD/TVt/fVstA@vjanyUer/kom@nIntEnnIn/f{rkom@.</p>
  <p num="p-0157">In the general phoneme confusability matrix <b>2700</b>, scores for confused pronunciation matches are represented as negative logarithms of a rate or likelihood that a spoken phoneme corresponding to the row is recognized as the phoneme corresponding to the column. Therefore, a higher number indicates a lower probability of confusion and a lower number indicates a higher probability of confusion. The phoneme confusability matrix may be adapted continually for a particular user's speech patterns.</p>
  <p num="p-0158">For example, the phoneme z is recognized as the phoneme s at a rate of e<sup>15 </sup>or about 310<sup>7</sup>, whereas the phoneme i is recognized as the phoneme <b>6</b> at a rate of e<sup>10 </sup>or about 510<sup>5</sup>. As another example, the phoneme q is confused with (or recognized correctly as) the phoneme q at a rate of e<sup>1 </sup>or about 0.4. This occurs because the speech recognition system is often unable to identify the phoneme q in speech.</p>
  <p num="p-0159">The phoneme &lt;b&gt; listed in the matrix <b>2700</b> corresponds to a blank. Therefore, the entry forw, &lt;b&gt;represents the probability that the phoneme w is deleted, whereas the entry for&lt;b&gt;, wrepresents the probability that the phoneme w is inserted. Thus, for example, the phoneme t is deleted at a rate of e<sup>7 </sup>or 910<sup>4 </sup>and the phoneme e in inserted at a rate of e<sup>19 </sup>or 610<sup>9</sup>.</p>
  <p num="p-0160">A determination of which phonemes may be confused with each other and the probability of that confusion may be based on empirical data. Such empirical data may be produced, for example, by gathering a speech recognition system's rate of confusion of phonemes for a preselected population or by studying frequency characteristics of different phonemes. A speech recognition system also may gather this data for pronunciations by a single user as part of the system's continuous training.</p>
  <p num="p-0161">Scores for confused pronunciation matches in the general phoneme confusability matrix may be generated using three sources of information: the probability that a sequence of phonemes for which the matches were sought (a recognized sequence) was the actual sequence of phonemes produced by the speaker, the probability that a particular confused pronunciation (the confused sequence) was confused for the recognized sequence, and the probability that the confused sequence occurs in the language (for example, English) with which the speech recognition system is used. These probabilities correspond to the scores produced by, respectively, the recognizer for the recognized sequence, a dynamic programming match of the recognized phonemes with the dictionary pronunciation using a priori probabilities of phoneme confusion, and an examination of a unigram language model for the words corresponding to the pronunciation of the recognized sequence.</p>
  <p num="p-0162">Referring again to <figref idrefs="DRAWINGS">FIG. 20</figref>, the user is able to view the text document using a word processing program, or another program that displays the text document. The user corrects mistakes found in the text document by, for example, typing or dictating the correct spelling of a word. In this way, the user provides the speech recognition system with corrected text <b>5</b> for a misrecognized word (step <b>2615</b>). The speech recognition system searches the vocabulary for the corrected text (step <b>2620</b>). If the corrected text is in the vocabulary, the speech recognition system awaits another correction from the user (step <b>2615</b>).</p>
  <p num="p-0163">If the corrected text is not in the vocabulary, the corrected text is an OOV word. In this case, the speech recognition system generates a sequence of phonemes for the corrected text (step <b>2625</b>). In generating the sequence of phonemes for the corrected text, the speech recognition system uses a phonetic alphabet.</p>
  <p num="p-0164">The speech recognition system aligns the phonemes for the corrected text with the phonemes in each of the misrecognized words in the choice list for the utterance that includes the corrected text (step <b>2630</b>) and then adjusts a copy of a general phoneme confusability matrix based on the alignment (step <b>2635</b>). The speech recognition system searches the recognized speech for the OOV word using the adjusted phoneme confusability matrix (step <b>2640</b>).</p>
  <p num="p-0165">In general, after completing procedure <b>2600</b>, the speech recognition system adds an OOV word to its vocabulary to improve future recognition accuracy. When the OOV word is added to the vocabulary, the speech recognition system need not save the adjusted phoneme confusability matrix. However, if the OOV word is not added, the adjusted phoneme confusability matrix for the OOV word may be saved and accessed in the future by the speech recognition system when encountering the OOV word in a user's utterance.</p>
  <p num="p-0166">During alignment (step <b>2630</b>), the speech recognition system compares a sequence of phonemes corresponding to the misrecognized portion of the utterance including the OOV word with the sequence of phonemes for the OOV word. The speech recognition system also generates a list of phoneme confusions that are likely to be associated with the OOV word. The speech recognition system generates this list by determining which phonemes in the corrected sequence are deleted, inserted, or substituted to map from the sequence of phonemes for the OOV word to the misrecognized sequence.</p>
  <p num="p-0167">Initially, during the recognition, the speech recognition system attempts to recognize an OOV word or phrase using combinations of words, letters, and phrases in its vocabulary that most closely resemble the OOV word. Such combinations may match the OOV word closely but not necessarily exactly. For example, if the phrase recognize is misrecognized because recognize is not in the vocabulary, the speech recognition system may substitute the words wreck a nice for recognize during the recognition. To do this, the speech recognition system substitutes an s sound for the z sound at the end of the word and completely drops the g sound from the word.</p>
  <p num="p-0168">Referring also to <figref idrefs="DRAWINGS">FIG. 22</figref>, for example, the user has corrected the misrecognized phrase wreck a nice with the corrected text recognize in a first example <b>2800</b>. <figref idrefs="DRAWINGS">FIG. 22</figref> illustrates two possible alignments from the substantially larger set of all possible alignments. In general, the alignments illustrated are more likely to score well than alignments that are not shown. The first alignment <b>2805</b> is shown using the solid arrows from the phonemes of recognize to the phonemes of wreck a nice. In this alignment, the g is deleted and nize is substituted with nice. The second alignment <b>2810</b> is shown using the dotted arrows from the phonemes of recognize to the phonemes of wreck a nice. In this alignment, the g is replaced with nice and nize is deleted. Scores for each of the alignments are determined using the general phoneme confusability matrix. For example, if the likelihood of deleting g and substituting nize with nice is greater than the likelihood of substituting g with nice and deleting nize, then the speech recognition system outputs a better score for the first alignment <b>2805</b> in <figref idrefs="DRAWINGS">FIG. 22</figref>.</p>
  <p num="p-0169">In a more general example <b>2815</b>, the proofreader has corrected the misrecognized sequence of phonemes ABDE with the sequence of phonemes ABCDE. In this case, the speech recognition system determines a first alignment <b>2820</b> (shown as solid arrows) as: replace A with A, replace B with B, delete C, replace D with D, and replace E with E. The speech recognition system determines a second alignment <b>2825</b> (shown as dotted arrows) as: replace A with A, replace B with B, replace C with D, replace D with E, and delete E. Scores for each of the alignments are determined using the general phoneme confusability matrix. For example, if the likelihood of deleting C, substituting D with D, and substituting E with E is greater than the likelihood of substituting C with D, substituting D with E, and deleting E, then the speech recognition system produces a better score for the first alignment <b>2820</b> in <figref idrefs="DRAWINGS">FIG. 22</figref>.</p>
  <p num="p-0170">Referring again to <figref idrefs="DRAWINGS">FIG. 20</figref>, the speech recognition system adjusts the copy of the general phoneme confusability matrix based on one or more best scoring alignments (step <b>2635</b>). The speech recognition system makes this adjustment based on the information about deletion, insertion, or substitution obtained from the alignment. For example, the speech recognition system may adjust the rate or score in the general phoneme confusability matrix <b>2700</b> to reflect a change in the rate of substitution of the phoneme s for the phoneme z. Thus, the entry for confusing z with s has a value of 15 in the general phoneme confusability matrix <b>2700</b>. After adjustment, the entry for confusing z with s may have a value of 1 in an adjusted phoneme confusability matrix, which indicates that z is confused with s 36% of the time for this particular OOV word. Although the value may be adjusted to 1 in this example, the value also may be set empirically. For example, the entry may be changed to 0 for those phonemes that are more confusable. Each time that a particular phoneme confusion is seen in the entries, that number may be used when considering how to adjust the matrix for that pair of phonemes.</p>
  <p num="p-0171">After the speech recognition system has adjusted the general phoneme confusability matrix (step <b>2635</b>), the speech recognition system searches for the OOV word in the text document using the adjusted matrix (step <b>2640</b>). In general, the search procedure (step <b>2640</b>) involves searching for the phoneme string associated with the OOV word, or likely confused variations of the phoneme string, in each utterance in the text document. Such a search makes use of the same alignment and scoring procedures as described above, but now compares the phoneme string for the OOV word to candidate substrings of each recognized utterance, systematically progressing through the recognized text. If an utterance receives a score above an empirically-determined threshold (step <b>2645</b>), the speech recognition system assumes that the utterance includes the OOV word (step <b>2650</b>) and outputs results (step <b>2655</b>). Results may be output by, for example, highlighting the recognized utterances in the text document that are likely to include the misrecognized word. In this way, the proofreader or user may review the highlighted utterances to determine if further action is needed. Thus, the speech recognition system may present the utterances to the proofreader or user in a fashion similar to the highlighting of misspelled words from a spell checker. Alternatively or in addition, results may be output by, for example, automatically re-recognizing those utterances that receive a score above the threshold, now using a vocabulary extended to include the OOV word.</p>
  <p num="p-0172">If an utterance receives a score below or equal to the threshold (step <b>2645</b>), the speech recognition system assumes that the utterance does not include the OOV word (step <b>2660</b>).</p>
  <p num="p-0173">Using the procedure <b>2600</b>, one implementation of the speech recognition system is able to identify approximately 95% of the utterances that include occurrences of the misrecognized word. Moreover, the same implementation is able to reject around 95% of the utterances that do not include the misrecognized word. This has resulted in a dramatic improvement in the proofreading process with respect to correcting OOV words.</p>
  <p num="p-0174">While being applicable primarily to the correction of OOV word misrecognitions, the techniques described above also may be applied to detect and correct other recognition errors that are repeated throughout a document. For example, if non-traditional pronunciation by the user results in the system misrecognizing one vocabulary word for one or more other vocabulary words, the techniques may be used to detect and highlight (or even correct) other potential occurrences of the same misrecognition. It is also important to note that the system does not need to have produced the same incorrect result for each occurrence of a word in order for those occurrences to be detected. For example, a single instantiation of the procedure <b>2600</b> would detect the misrecognition of recognize as both wreck a nice and wreck at night. When procedure <b>2600</b> is used to correct misrecognitions of vocabulary words, the speech recognition system would adapt the speech models for the user to prevent such misrecognitions from occurring in future recognitions.</p>
  <p num="p-0175">The techniques described above also may be applied to perform efficient text searching through a large body of speech that has been recognized, a technique referred to as audio mining. For example, when searching audio recordings for a unique name (such as the name Fooberman), it would be beneficial to use the above described technique because the unique name may not be in an accessed vocabulary.</p>
  <p num="p-0176">Conditional Adaptation</p>
  <p num="p-0177">One problem with prior speech recognition systems is the necessity of obtaining user input to adapt or train the speech models. For example, traditional training techniques fall into one of two distinct categories: 1) solicitation of user participation in adapting speech models and 2) conservative adaptation of speech models. In the first technique, the speech recognition system asks or forces the user to correct any mistakes and trains the speech models using the corrected text. This technique, however, is often tedious to the user because the user must correct any mistakes. Moreover, this technique is impractical when using a recording device or any sort of mobile speech recognition system because user feedback in that type of system is reduced. In the second technique, the speech recognition system rarely adapts the speech models, which reduces the time that the user must spend correcting mistakes. However, this technique results in a reduced accuracy in the speech recognition results because the speech models are adapted infrequently. Both of these techniques fail to account for the case when a user is actually changing her mind about the wording and not correcting an error in the speech recognition system. When the user changes her mind, speech models should not be updated.</p>
  <p num="p-0178">In a conditional adaptation strategy, the speech recognition system automatically determines whether a person is correcting a mistake or changing her mind during dictation. In one implementation, the user has dictated a body of text and a speech recognition system has recognized the body of text. When the user reads the recognized body of text, the user may select and edit some text to reflect 1) corrections to a misrecognition and/or 2) revisions to the text that reflect a change of mind for the user. During the user editing period, the speech recognition system uses information from the recording of the user's speech, the recognition results, and the user's edited text to determine whether the user is correcting or revising the text.</p>
  <p num="p-0179">Referring also to <figref idrefs="DRAWINGS">FIG. 23</figref>, the speech recognition system performs a procedure <b>2900</b> for adapting acoustic models for a user's speech patterns. Initially, the speech recognition system receives the user-dictated text by, for example, receiving a recording from a recorder or receiving user-dictated text directly through a microphone (step <b>2905</b>).</p>
  <p num="p-0180">The speech recognition system then generates the recognized speech (step <b>2910</b>). In one implementation, the speech recognition system recognizes the user's speech at the same time as adapting acoustic models for the user's speech patterns. In this case, the user may be editing the dictated text while speaking the dictated text. This may occur when the user is at a desktop computer dictating text and receiving immediate feedback from the speech recognition system.</p>
  <p num="p-0181">In another implementation, the speech recognition system has already recognized the user-dictated text or speech and has stored it for later use in memory. In this case, the user may have already finished speaking the dictated text into the mobile recorder or directly into the microphone and the speech recognition system has stored the dictated text into memory.</p>
  <p num="p-0182">Next, the speech recognition system receives one or more edits from the user while the user is reviewing the recognized text (step <b>2915</b>). The user may edit the recognized text using any of the techniques described above. For example, the user may edit the recognized text in a correction dialog or by selecting the text and speaking the correction.</p>
  <p num="p-0183">The speech recognition system then determines or builds an acoustic model for the user-edited text (step <b>2920</b>). The speech recognition system may determine an acoustic model for the user-edited text by looking up the text in the vocabulary or in a backup dictionary. Alternatively, if the text is not in the vocabulary or the backup dictionary, the speech recognition system may select acoustic models for the user-edited text by finding acoustic models that best match the user-edited text.</p>
  <p num="p-0184">As discussed above, an acoustic model may correspond to a word, a phrase or a command from a vocabulary. An acoustic model also may represent a sound, or phoneme, which corresponds to a portion of a word. Collectively, the constituent phonemes for a word represent the phonetic spelling of the word. Acoustic models also may represent silence and various types of environmental noise.</p>
  <p num="p-0185">The speech recognition system calculates an edited acoustic model score based on a comparison between the acoustic model for the user-edited text and acoustic data for the original utterance that the user had spoken (this acoustic data for the original utterance is stored in the memory) (step <b>2925</b>). The speech recognition system receives an original acoustic model score that was determined during recognition and is based on a comparison between the acoustic model for the recognized utterance and the acoustic data for the original utterance that the user had spoken (step <b>2930</b>). The speech recognition system then calculates a difference between these scores (step <b>2935</b>) and determines if this difference is within a tunable threshold (step <b>2940</b>) to determine whether the user-edited text is a correction or a revision to the recognized utterance. If the difference is within a tunable threshold (step <b>2940</b>), the speech recognition system adapts acoustic models for the correction of the recognized utterance (step <b>2945</b>). On the other hand, if the difference is not within a tunable threshold (step <b>2940</b>), the speech recognition system does not adapt the acoustic models for the revision to the recognized utterance.</p>
  <p num="p-0186">For example, suppose that the user had originally spoken the cat sat on the mat and the speech recognition system recognized this utterance as the hat sat on the mat. The acoustic data for the originally spoken the cat sat on the mat are stored in memory for future reference. The user reviews the recognized text and edits it, in one instance, for correction or for revision. If the user decides to correct the misrecognition, the user may select hat in the recognized text and spell out the word cat. On the other hand, if the user decides to revise the recognition to read the dog sat on the mat, then the user may select hat in the recognized text and speak the word dog.</p>
  <p num="p-0187">When considering the score difference, it is worthwhile to question how the edited acoustic model score for the user-edited text (called the new score) could be better than the original acoustic model score for the recognized utterance (called the old score). In this situation, it seems plausible that the speech recognition system should have detected a mistake in the recognized utterance. However, the speech recognition system may fail to detect a mistake. This could occur because the speech recognition system considers, in addition to the acoustic model, the language model. Another reason for the mistake or oversight could be that the speech recognition system may have produced a search error during recognition, that is, a correct hypothesis could have been pruned during recognition. One more reason that the speech recognition system may fail to detect a mistake may be that the recognized utterance included a new word that was pulled from the backup dictionary.</p>
  <p num="p-0188">Another question that arises is how the new score could be a little worse than the old score. For example, when there is something wrong with the acoustic model for such a word, the speech recognition system should adapt the acoustics or guess a new pronunciation. However, in general, if the new score is much worse than the old score (relative to the tunable threshold), then the speech recognition system hypothesizes that the user-edited text corresponds to revisions.</p>
  <p num="p-0189">Referring also to <figref idrefs="DRAWINGS">FIGS. 24A and 24B</figref>, graphs <b>3000</b> and <b>3005</b> are shown that model the difference between the old score and the new score for each edited utterance in a sample block of recognition text. The recognition used a <b>50</b>,<b>000</b> word vocabulary. The tester has identified all regions of the recognition text that contain errors, and, for each of these regions, the tester has edited the recognized utterance. The graphs show the cumulative distribution of these regions, with the score difference between the new score and the old score being graphed in a histogram.</p>
  <p num="p-0190">In graphs <b>3000</b> and <b>3005</b>, the speech recognition system has performed word recognition on the original user utterance. In graph <b>3000</b>, the speech recognition system uses a word vocabulary as a rejection grammar, and in graph <b>3005</b>, the speech recognition system uses a phoneme sequence as a rejection grammar.</p>
  <p num="p-0191">In graphs <b>3000</b> and <b>3005</b>, the tester has corrected errors in the speech recognition and results are shown, respectively, in curves <b>3010</b> and <b>3015</b>. For example, if the tester originally spoke the utterance the cat sat on the mat and the speech recognition system incorrectly recognized this utterance as the hat sat on the mat, the tester may correct the recognition by selecting hat and spelling out cat. In this case, the old score for the original utterance cat would match very nearly the new score for the recognized utterance hat and that is why the speech recognition system initially made the recognition error. Thus, the score difference determined at step <b>2925</b> would be relatively small.</p>
  <p num="p-0192">Furthermore, the tester has modeled user revisions by making random edits to the text. Random edits include, in the simplest model, picking text at random from the recognition text and deleting that picked text, picking text at random from a choice list and inserting it somewhere in the recognition text, and picking text at random from the recognition text and substituting that picked text with random text from a choice list.</p>
  <p num="p-0193">The random edits are also graphed in histogram form. For graph <b>3000</b>, these curves are labeled as <b>3020</b> (deletion), <b>3025</b> (insertion), and <b>3030</b> (substitution) and for graph <b>3005</b>, these curves are labeled as <b>3035</b> (deletion), <b>3040</b> (insertion), and <b>3045</b> (substitution). Other techniques for picking text at random are possible. For example, the tester may have picked only function words or only content words.</p>
  <p num="p-0194">Using the example in which the user had originally spoken the cat sat on the mat and the speech recognition system recognized this as the hat sat on the mat, the user may, during editing, replace the recognized word hat with the word dog. In that case, the original acoustic model score (the old score) is fairly good, while the edited acoustic model score (the new score) is poor because the word dog sounds nothing like cat, which is what the user originally spoke. Thus, the score difference would be rather large in this case (for example, 800 on the graph).</p>
  <p num="p-0195">On the other hand, if the user replaced the recognized word hat with the word rat, then the old score and the new score are both fairly good. Therefore, the score difference may be relatively small (for example, 150 on the graph).</p>
  <p num="p-0196">Using the above example and graph <b>3000</b>, if the threshold difference is 200 points, then the speech recognition system would adapt on the correction from hat to cat, adapt on the revision from hat to rat, and ignore the revision from hat to dog. If the threshold difference is 100 points, the speech recognition system would adapt on the correction from hat to cat, and ignore the revisions from hat to rat and from hat to dog.</p>
  <p num="p-0197">Evident from the randomly-generated curves <b>3020</b>-<b>3045</b> is that they are very similar to each other in shape and magnitude. Using a threshold of 200 difference points, about 5-15% of the randomly-generated revisions are used by the speech recognition system in adaptation and about 60-95% (where 60% corresponds to phoneme correction and 95% corresponds to word correction) of the corrected text is used by the speech recognition system in adaptation. If the threshold is reduced more, for example, to 50 difference points, then many more of the randomly-generated revisions may be eliminated from adaptation. However, there will be fewer corrections with which to adapt the speech models.</p>
  <p num="p-0198">The techniques and systems described above benefit from the knowledge that the original recognition results are a fairly good acoustic fit to the user's speech. Moreover, when language model scores are included, the original recognition results are considered a very good fit to the user's speech. Additionally, when finding the difference in acoustic scores, the scores cancel for those utterances that are unchanged in the edited text and the scores for the corrected or revised utterances remain to be further analyzed by the speech recognition system. Thus, the techniques and systems may be applied to arbitrarily long utterances, without needing to normalize for the length of the utterance.</p>
  <p num="p-0199">Distinguishing Spelling and Dictation During Correction</p>
  <p num="p-0200">Referring to <figref idrefs="DRAWINGS">FIG. 25</figref>, a speech recognition system may be configured to distinguish between correction in the form of spelling correction and in the form of dictation according to a procedure <b>3100</b>. When a user selects misrecognized text, the user can either speak the pronunciation of the correct word or the user can spell the correct word. The speech recognition system distinguishes between these two correction mechanisms without requiring the user to indicate which correction mechanism is being used. For example, the user need not speak the command SPELL THAT before spelling out the corrected text. As another example, the user need not speak the command MAKE THAT before pronouncing the corrected text.</p>
  <p num="p-0201">Referring also to <figref idrefs="DRAWINGS">FIG. 26</figref>, a constraint grammar <b>3200</b> that permits spelling and pronunciation in parallel is established (step <b>3105</b>). The constraint grammar <b>3200</b> includes a spelling portion in which a first state <b>3205</b> indicates that the first utterance from the user must be a letter in an alphabet and a large vocabulary dictation portion in which a first state <b>3210</b> indicates that the first utterance from the user must be a word from the dictation vocabulary. A path <b>3215</b>, which returns to the first state <b>3205</b>, indicates that the utterance may include additional letters. A path <b>3220</b>, which exits the first state <b>3205</b> and completes the utterance, indicates that the utterance may include only letters. A path <b>3225</b>, which returns to the second state <b>3210</b>, indicates that the utterance may include additional words from the dictation vocabulary. A path <b>3230</b>, which exits the second state <b>3210</b> and completes the utterance, indicates that the utterance may include only words from the dictation vocabulary.</p>
  <p num="p-0202">The large vocabulary dictation portion also indicates the frequency with which words occur. For example, a language model associated with the large vocabulary dictation portion may be a unigram model that indicates the frequency with which a word occurs independently of context or a bigram model that indicates the frequency with which a word occurs in the context of a preceding word. For example, a bigram model may indicate that a noun or adjective is more likely to follow the word the than a verb or preposition.</p>
  <p num="p-0203">Similarly, the spelling portion may indicate the frequency with which letters occur. For example, a language model associated with the spelling portion may be a unigram model that indicates the frequency with which a letter occurs independently of context or a bigram model that indicates the frequency with which a letter occurs in the context of a preceding letter. For example, a bigram model may indicate that a vowel is more likely to follow the letter m than a consonant.</p>
  <p num="p-0204">Referring again to <figref idrefs="DRAWINGS">FIG. 25</figref>, a fixed biasing value between the spelling and dictation grammars may be introduced to improve the chances that the speech recognition system distinguishes a spelled correction from a pronounced correction (step <b>3110</b>).</p>
  <p num="p-0205">After the constraint grammar is established (step <b>3105</b>), the constraint grammar may be implemented during error correction (step <b>3115</b>). In this manner, during correction, the speech recognition system initially determines if the user is correcting an error. If so, the system recognizes the user's correction using the established constraint grammar. If the user corrects the misrecognition by spelling out the correct word, the speech recognition system determines that the correction follows the path through the state <b>3205</b> and determines the correction accordingly. If the user corrects the misrecognition by pronouncing the correct word, the speech recognition system determines that the correction follows the path through the state <b>3210</b> and determines the correction accordingly.</p>
  <p num="p-0206">Because both of the constraint grammar portions are used in parallel by the speech recognition system, the system is able to determine which portion gives the most likely recognition result.</p>
  <p num="p-0207">If a fixed biasing value is introduced between the spelling portion and the dictation portion, then the speech recognition system considers the biasing when selecting between the portions. For example, the biasing value may indicate that the user is more likely to dictate a correction than to spell it, such that the score for spelling portions will need to be better than that of the dictation portion by more than the biasing value in order to be selected.</p>
  <p num="p-0208">The techniques described here are not limited to any particular hardware or software configuration; they may find applicability in any computing or processing environment that may be used for speech recognition. The techniques may be implemented in hardware or software, or a combination of the two. Preferably, the techniques are implemented in computer programs executing on programmable computers that each include a processor, a storage medium readable by the processor (including volatile and non-volatile memory and/or storage elements), at least one input device, and at least one output device. Program code is applied to data entered using the input device to perform the functions described and to generate output information. The output information is applied to one or more output devices.</p>
  <p num="p-0209">Each program is preferably implemented in a high level procedural or object oriented programming language to communicate with a computer system. However, the programs can be implemented in assembly or machine language, if desired. In any case, the language may be a compiled or interpreted language.</p>
  <p num="p-0210">Each such computer program is preferably stored on a storage medium or device (for example, CD-ROM, hard disk or magnetic diskette) that is readable by a general or special purpose programmable computer for configuring and operating the computer when the storage medium or device is read by the computer to perform the procedures described in this document. The system may also be considered to be implemented as a computer-readable storage medium, configured with a computer program, where the storage medium so configured causes a computer to operate in a specific and predefined manner.</p>
</div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5027406">US5027406</a></td><td class="patent-data-table-td patent-date-value">Dec 6, 1988</td><td class="patent-data-table-td patent-date-value">Jun 25, 1991</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Method for interactive speech recognition and training</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5594809">US5594809</a></td><td class="patent-data-table-td patent-date-value">Apr 28, 1995</td><td class="patent-data-table-td patent-date-value">Jan 14, 1997</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Automatic training of character templates using a text line image, a text line transcription and a line image source model</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5625748">US5625748</a></td><td class="patent-data-table-td patent-date-value">Apr 18, 1994</td><td class="patent-data-table-td patent-date-value">Apr 29, 1997</td><td class="patent-data-table-td ">Bbn Corporation</td><td class="patent-data-table-td ">Topic discriminator using posterior probability or confidence scores</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5748840">US5748840</a></td><td class="patent-data-table-td patent-date-value">May 9, 1995</td><td class="patent-data-table-td patent-date-value">May 5, 1998</td><td class="patent-data-table-td ">Audio Navigation Systems, Inc.</td><td class="patent-data-table-td ">Methods and apparatus for improving the reliability of recognizing words in a large database when the words are spelled or spoken</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5754978">US5754978</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 1995</td><td class="patent-data-table-td patent-date-value">May 19, 1998</td><td class="patent-data-table-td ">Speech Systems Of Colorado, Inc.</td><td class="patent-data-table-td ">Speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5794189">US5794189</a></td><td class="patent-data-table-td patent-date-value">Nov 13, 1995</td><td class="patent-data-table-td patent-date-value">Aug 11, 1998</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Continuous speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5799279">US5799279</a></td><td class="patent-data-table-td patent-date-value">Nov 13, 1995</td><td class="patent-data-table-td patent-date-value">Aug 25, 1998</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Continuous speech recognition of text and commands</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5864805">US5864805</a></td><td class="patent-data-table-td patent-date-value">Dec 20, 1996</td><td class="patent-data-table-td patent-date-value">Jan 26, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and apparatus for error correction in a continuous dictation system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5963903">US5963903</a></td><td class="patent-data-table-td patent-date-value">Jun 28, 1996</td><td class="patent-data-table-td patent-date-value">Oct 5, 1999</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for dynamically adjusted training for speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6064959">US6064959</a></td><td class="patent-data-table-td patent-date-value">Mar 28, 1997</td><td class="patent-data-table-td patent-date-value">May 16, 2000</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Error correction in speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6073099">US6073099</a></td><td class="patent-data-table-td patent-date-value">Nov 4, 1997</td><td class="patent-data-table-td patent-date-value">Jun 6, 2000</td><td class="patent-data-table-td ">Nortel Networks Corporation</td><td class="patent-data-table-td ">Predicting auditory confusions using a weighted Levinstein distance</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6212498">US6212498</a></td><td class="patent-data-table-td patent-date-value">Mar 28, 1997</td><td class="patent-data-table-td patent-date-value">Apr 3, 2001</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Enrollment in speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6233553">US6233553</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 4, 1998</td><td class="patent-data-table-td patent-date-value">May 15, 2001</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Method and system for automatically determining phonetic transcriptions associated with spelled words</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6374221">US6374221</a></td><td class="patent-data-table-td patent-date-value">Jun 22, 1999</td><td class="patent-data-table-td patent-date-value">Apr 16, 2002</td><td class="patent-data-table-td ">Lucent Technologies Inc.</td><td class="patent-data-table-td ">Automatic retraining of a speech recognizer while using reliable transcripts</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6490563">US6490563</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 17, 1998</td><td class="patent-data-table-td patent-date-value">Dec 3, 2002</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Proofreading with text to speech feedback</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6535849">US6535849</a></td><td class="patent-data-table-td patent-date-value">Jan 18, 2000</td><td class="patent-data-table-td patent-date-value">Mar 18, 2003</td><td class="patent-data-table-td ">Scansoft, Inc.</td><td class="patent-data-table-td ">Method and system for generating semi-literal transcripts for speech recognition systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6577999">US6577999</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 8, 1999</td><td class="patent-data-table-td patent-date-value">Jun 10, 2003</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and apparatus for intelligently managing multiple pronunciations for a speech recognition vocabulary</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6912498">US6912498</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 2, 2001</td><td class="patent-data-table-td patent-date-value">Jun 28, 2005</td><td class="patent-data-table-td ">Scansoft, Inc.</td><td class="patent-data-table-td ">Error correction in speech recognition by correcting text around selected area</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6934682">US6934682</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 1, 2001</td><td class="patent-data-table-td patent-date-value">Aug 23, 2005</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Processing speech recognition errors in an embedded speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020138265">US20020138265</a></td><td class="patent-data-table-td patent-date-value">May 2, 2001</td><td class="patent-data-table-td patent-date-value">Sep 26, 2002</td><td class="patent-data-table-td ">Daniell Stevens</td><td class="patent-data-table-td ">Error correction in speech recognition</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Elizabeth D. Liddy and Sung H. Myaeng, A System Update for TREC-2, Sep. 3, 1998, pp. 1-11; School of Information Studies, Syracuse University, Syracuse, New York.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">IBM ("<a href='http://scholar.google.com/scholar?q="Technical+Disclosure+Bulletin+NB900315%2C+Automatic+Correction+of+Viterbi+Misalignments"'>Technical Disclosure Bulletin NB900315, Automatic Correction of Viterbi Misalignments</a>" Mar. 1990).</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Iyer et al (Analyzing And Predicting Language Model Improvements, 1997 IEEE Workshop on Automatic Speech Recognition and Understanding, Dec. 1997).</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">John W. Lehman and Clifford A. Reid, et al., Knowledge-Based Searching with TOPIC, Sep. 4, 1998, pp. 1-13, Verity, Inc., Mountain View, CA.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Niyogi et al. (Incorporation Voice Onset Time To Improve Letter Recognition Accuracies, Proceedings of the 1998 IEEE International Conference on Acoustics, Speech, and Signal Processing, May 1998).</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Richard Stern, George Doddington, Dave Pallet, and Charles Wayne, Specification for the ARPA Nov. 1996 HUB 4 Evaluation, Nov. 1, 1996, pp. 1-5 National Institutes of Standards and Technology.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Appl. No. 60/201,257, filed May 2000, Roth et al.</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7693717">US7693717</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 12, 2006</td><td class="patent-data-table-td patent-date-value">Apr 6, 2010</td><td class="patent-data-table-td ">Custom Speech Usa, Inc.</td><td class="patent-data-table-td ">Session file modification with annotation using speech recognition or text to speech</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7809566">US7809566</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 13, 2006</td><td class="patent-data-table-td patent-date-value">Oct 5, 2010</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">One-step repair of misrecognized recognition strings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7983914">US7983914</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 10, 2005</td><td class="patent-data-table-td patent-date-value">Jul 19, 2011</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Method and system for improved speech recognition by degrading utterance pronunciations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8086444">US8086444</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 1, 2009</td><td class="patent-data-table-td patent-date-value">Dec 27, 2011</td><td class="patent-data-table-td ">Resolvity, Inc.</td><td class="patent-data-table-td ">Method and system for grammar relaxation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8229735">US8229735</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 27, 2008</td><td class="patent-data-table-td patent-date-value">Jul 24, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Grammar checker for visualization</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8355914">US8355914</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 17, 2009</td><td class="patent-data-table-td patent-date-value">Jan 15, 2013</td><td class="patent-data-table-td ">Lg Electronics Inc.</td><td class="patent-data-table-td ">Mobile terminal and method for correcting text thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8473295">US8473295</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 21, 2005</td><td class="patent-data-table-td patent-date-value">Jun 25, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Redictation of misrecognized words using a list of alternatives</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8515745">US8515745</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 24, 2012</td><td class="patent-data-table-td patent-date-value">Aug 20, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Selecting speech data for speech recognition vocabulary</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8515746">US8515746</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 24, 2012</td><td class="patent-data-table-td patent-date-value">Aug 20, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Selecting speech data for speech recognition vocabulary</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8515753">US8515753</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 30, 2007</td><td class="patent-data-table-td patent-date-value">Aug 20, 2013</td><td class="patent-data-table-td ">Gwangju Institute Of Science And Technology</td><td class="patent-data-table-td ">Acoustic model adaptation methods based on pronunciation variability analysis for enhancing the recognition of voice of non-native speaker and apparatus thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8521523">US8521523</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 24, 2012</td><td class="patent-data-table-td patent-date-value">Aug 27, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Selecting speech data for speech recognition vocabulary</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8626506">US8626506</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 20, 2006</td><td class="patent-data-table-td patent-date-value">Jan 7, 2014</td><td class="patent-data-table-td ">General Motors Llc</td><td class="patent-data-table-td ">Method and system for dynamic nametag scoring</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8626511">US8626511</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 22, 2010</td><td class="patent-data-table-td patent-date-value">Jan 7, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Multi-dimensional disambiguation of voice commands</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8655664">US8655664</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 11, 2011</td><td class="patent-data-table-td patent-date-value">Feb 18, 2014</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Text presentation apparatus, text presentation method, and computer program product</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8694309">US8694309</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 12, 2007</td><td class="patent-data-table-td patent-date-value">Apr 8, 2014</td><td class="patent-data-table-td ">West Corporation</td><td class="patent-data-table-td ">Automatic speech recognition tuning management</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8738377">US8738377</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 7, 2010</td><td class="patent-data-table-td patent-date-value">May 27, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Predicting and learning carrier phrases for speech input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8762156">US8762156</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 28, 2011</td><td class="patent-data-table-td patent-date-value">Jun 24, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Speech recognition repair using contextual information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090030680">US20090030680</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 23, 2007</td><td class="patent-data-table-td patent-date-value">Jan 29, 2009</td><td class="patent-data-table-td ">Jonathan Joseph Mamou</td><td class="patent-data-table-td ">Method and System of Indexing Speech Data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090037171">US20090037171</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 4, 2008</td><td class="patent-data-table-td patent-date-value">Feb 5, 2009</td><td class="patent-data-table-td ">Mcfarland Tim J</td><td class="patent-data-table-td ">Real-time voice transcription system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090119105">US20090119105</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 30, 2007</td><td class="patent-data-table-td patent-date-value">May 7, 2009</td><td class="patent-data-table-td ">Hong Kook Kim</td><td class="patent-data-table-td ">Acoustic Model Adaptation Methods Based on Pronunciation Variability Analysis for Enhancing the Recognition of Voice of Non-Native Speaker and Apparatus Thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090299730">US20090299730</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 17, 2009</td><td class="patent-data-table-td patent-date-value">Dec 3, 2009</td><td class="patent-data-table-td ">Joh Jae-Min</td><td class="patent-data-table-td ">Mobile terminal and method for correcting text thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100070263">US20100070263</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 30, 2007</td><td class="patent-data-table-td patent-date-value">Mar 18, 2010</td><td class="patent-data-table-td ">National Institute Of Advanced Industrial Science And Technology</td><td class="patent-data-table-td ">Speech data retrieving web site system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100332230">US20100332230</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 25, 2009</td><td class="patent-data-table-td patent-date-value">Dec 30, 2010</td><td class="patent-data-table-td ">Adacel Systems, Inc.</td><td class="patent-data-table-td ">Phonetic distance measurement system and related methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110184730">US20110184730</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 22, 2010</td><td class="patent-data-table-td patent-date-value">Jul 28, 2011</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Multi-dimensional disambiguation of voice commands</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110218802">US20110218802</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 8, 2010</td><td class="patent-data-table-td patent-date-value">Sep 8, 2011</td><td class="patent-data-table-td ">Shlomi Hai Bouganim</td><td class="patent-data-table-td ">Continuous Speech Recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110301955">US20110301955</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 7, 2010</td><td class="patent-data-table-td patent-date-value">Dec 8, 2011</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Predicting and Learning Carrier Phrases for Speech Input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120065981">US20120065981</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 11, 2011</td><td class="patent-data-table-td patent-date-value">Mar 15, 2012</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Text presentation apparatus, text presentation method, and computer program product</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120109646">US20120109646</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 2, 2011</td><td class="patent-data-table-td patent-date-value">May 3, 2012</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Speaker adaptation method and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120239395">US20120239395</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 14, 2011</td><td class="patent-data-table-td patent-date-value">Sep 20, 2012</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Selection of Text Prediction Results by an Accessory</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130080177">US20130080177</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 28, 2011</td><td class="patent-data-table-td patent-date-value">Mar 28, 2013</td><td class="patent-data-table-td ">Lik Harry Chen</td><td class="patent-data-table-td ">Speech recognition repair using contextual information</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=_HOABAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S235000">704/235</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=_HOABAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S256000">704/256</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=_HOABAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704SE15040">704/E15.04</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=_HOABAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S260000">704/260</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=_HOABAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S251000">704/251</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=_HOABAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S258000">704/258</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=_HOABAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0015260000">G10L15/26</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=_HOABAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0015220000">G10L15/22</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=_HOABAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L15/22">G10L15/22</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=_HOABAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L2015/0631">G10L2015/0631</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G10L15/22</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Nov 22, 2011</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 1-21 IS CONFIRMED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 16, 2011</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 29, 2011</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20110110</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 6, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">NUANCE COMMUNICATIONS, INC., MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:SCANSOFT, INC.;REEL/FRAME:016851/0772</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20051017</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U3GpodKjSSX6ERPvyhGnSof55xQ8g\u0026id=_HOABAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U1reFkd6zdCDOV-CVOBandZSc2l4w\u0026id=_HOABAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U1c_pkGvBjqGPYO2d286UhFIMk77w","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Error_correction_in_speech_recognition.pdf?id=_HOABAABERAJ\u0026output=pdf\u0026sig=ACfU3U1aWCZYRw6dRJs_ZmQZtoDaClw3Zg"},"sample_url":"http://www.google.com/patents/reader?id=_HOABAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>