<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6233571 - Method and apparatus for indexing, searching and displaying data - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Method and apparatus for indexing, searching and displaying data"><meta name="DC.contributor" content="Daniel Egger" scheme="inventor"><meta name="DC.contributor" content="Shawn Cannon" scheme="inventor"><meta name="DC.contributor" content="Ronald D. Sauers" scheme="inventor"><meta name="DC.contributor" content="Daniel Egger" scheme="assignee"><meta name="DC.date" content="1998-5-4" scheme="dateSubmitted"><meta name="DC.description" content="A computer research tool for indexing, searching and displaying data is disclosed. Specifically, a computer research tool for performing computerized research of data including textual objects in a database or a network and for providing a user interface that significantly enhances data presentation is described. Textual objects and other data in a database or network is indexed by creating a numerical representation of the data. The indexing technique called proximity indexing generates a quick-reference of the relations, patterns and similarity found among the data in the database. Proximity indexing indexes the data by using statistical techniques and empirically developed algorithms. Using this proximity index, an efficient search for pools of data having a particular relation, pattern or characteristic can be effectuated. The Computer Search program, called the Computer Search Program for Data represented in Matrices (CSPDM), provides efficient computer search methods. The CSPDM rank orders data in accordance with the data&#39;s relationship to time, a paradigm datum, or any similar reference. An alternative embodiment of the invention employs a cluster link generation algorithm which uses links and nodes to index and search a database or network. The algorithm searches for direct and indirect links to a search node and retrieves the nodes which are most closely related to the search node. The user interface program, called the Graphical User Interface (GUI), provides a user friendly method of interacting with the CSPDM program and prepares and presents a visual graphical display. The graphical display provides the user with a two or three dimensional spatial orientation of the data."><meta name="DC.date" content="2001-5-15" scheme="issued"><meta name="DC.relation" content="US:5265065" scheme="references"><meta name="DC.relation" content="US:5341293" scheme="references"><meta name="DC.relation" content="US:5446842" scheme="references"><meta name="DC.relation" content="US:5530852" scheme="references"><meta name="DC.relation" content="US:5542024" scheme="references"><meta name="DC.relation" content="US:5544352" scheme="references"><meta name="DC.relation" content="US:5617565" scheme="references"><meta name="DC.relation" content="US:5649186" scheme="references"><meta name="DC.relation" content="US:5749785" scheme="references"><meta name="DC.relation" content="US:5794001" scheme="references"><meta name="DC.relation" content="US:5832494" scheme="references"><meta name="DC.relation" content="US:5898434" scheme="references"><meta name="DC.relation" content="US:6098081" scheme="references"><meta name="citation_reference" content="a) Agosti, et al., &quot;A Two-Level Hypertext Retrieval Model for Legal Data,&quot; SIGIR &#39;91 (1991)."><meta name="citation_reference" content="b) Fowler, et al., &quot;Integrating Query, Thesaurus and Documents Through a Common Visual Representation,&quot; SIGIR &#39;91 (1991)."><meta name="citation_reference" content="c) Rose &amp; Belew, &quot;Legal Information Retrieval: A Hybrid Approach,&quot; ICAIL &#39;89 (1989)."><meta name="citation_reference" content="d) Belew, Richard, &quot;A Connectionist Approach to Conceptual Information Retrieval,&quot; ICAIL &#39;87 (1987)."><meta name="citation_reference" content="e) Gelbart &amp; Smith, &quot;Beyond Boolean Search: FLEXICON, A Legal Text-Based Intelligent System,&quot; ICAIL &#39;91 (1991)."><meta name="citation_reference" content="f) Lin, &quot;A Self-Organizing Semantic Map for Information Retrieval,&quot; SIGIR &#39;91 (1991)."><meta name="citation_reference" content="g) Turtle &amp; Croft, &quot;Inference Networks for Document Retrieval,&quot; SIGR &#39;90 (1990)."><meta name="citation_patent_number" content="US:6233571"><meta name="citation_patent_application_number" content="US:09/071,120"><link rel="canonical" href="http://www.google.com/patents/US6233571"/><meta property="og:url" content="http://www.google.com/patents/US6233571"/><meta name="title" content="Patent US6233571 - Method and apparatus for indexing, searching and displaying data"/><meta name="description" content="A computer research tool for indexing, searching and displaying data is disclosed. Specifically, a computer research tool for performing computerized research of data including textual objects in a database or a network and for providing a user interface that significantly enhances data presentation is described. Textual objects and other data in a database or network is indexed by creating a numerical representation of the data. The indexing technique called proximity indexing generates a quick-reference of the relations, patterns and similarity found among the data in the database. Proximity indexing indexes the data by using statistical techniques and empirically developed algorithms. Using this proximity index, an efficient search for pools of data having a particular relation, pattern or characteristic can be effectuated. The Computer Search program, called the Computer Search Program for Data represented in Matrices (CSPDM), provides efficient computer search methods. The CSPDM rank orders data in accordance with the data&#39;s relationship to time, a paradigm datum, or any similar reference. An alternative embodiment of the invention employs a cluster link generation algorithm which uses links and nodes to index and search a database or network. The algorithm searches for direct and indirect links to a search node and retrieves the nodes which are most closely related to the search node. The user interface program, called the Graphical User Interface (GUI), provides a user friendly method of interacting with the CSPDM program and prepares and presents a visual graphical display. The graphical display provides the user with a two or three dimensional spatial orientation of the data."/><meta property="og:title" content="Patent US6233571 - Method and apparatus for indexing, searching and displaying data"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("94ntU7zjNJSpsASSlIGYAw"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("FRA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("94ntU7zjNJSpsASSlIGYAw"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("FRA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6233571?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6233571"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=zwVVBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6233571&amp;usg=AFQjCNGkEj35sR_D1UE3-rTjot7LIvo-aQ" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6233571.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6233571.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6233571" style="display:none"><span itemprop="description">A computer research tool for indexing, searching and displaying data is disclosed. Specifically, a computer research tool for performing computerized research of data including textual objects in a database or a network and for providing a user interface that significantly enhances data presentation...</span><span itemprop="url">http://www.google.com/patents/US6233571?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6233571 - Method and apparatus for indexing, searching and displaying data</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6233571 - Method and apparatus for indexing, searching and displaying data" title="Patent US6233571 - Method and apparatus for indexing, searching and displaying data"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6233571 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/071,120</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">May 15, 2001</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">May 4, 1998</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Jun 14, 1993</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/DE69431351D1">DE69431351D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69431351T2">DE69431351T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0704075A1">EP0704075A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0704075A4">EP0704075A4</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0704075B1">EP0704075B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5544352">US5544352</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5832494">US5832494</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7840524">US7840524</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8555196">US8555196</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20060242564">US20060242564</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1995000896A2">WO1995000896A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1995000896A3">WO1995000896A3</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">071120, </span><span class="patent-bibdata-value">09071120, </span><span class="patent-bibdata-value">US 6233571 B1, </span><span class="patent-bibdata-value">US 6233571B1, </span><span class="patent-bibdata-value">US-B1-6233571, </span><span class="patent-bibdata-value">US6233571 B1, </span><span class="patent-bibdata-value">US6233571B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Daniel+Egger%22">Daniel Egger</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Shawn+Cannon%22">Shawn Cannon</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Ronald+D.+Sauers%22">Ronald D. Sauers</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Daniel+Egger%22">Daniel Egger</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6233571.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6233571.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6233571.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (13),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (7),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (150),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (31),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (16)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6233571&usg=AFQjCNG4una3oymMBTLR__fWWD_xSUQbVQ">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6233571&usg=AFQjCNHdwObm1LWrRJ6aAy5EcpN5IjpxSA">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6233571B1%26KC%3DB1%26FT%3DD&usg=AFQjCNHa8P2c5ObdQ4fkIZmnCIiLs-HTug">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54773382" lang="EN" load-source="patent-office">Method and apparatus for indexing, searching and displaying data</invention-title></span><br><span class="patent-number">US 6233571 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA72558573" lang="EN" load-source="patent-office"> <div class="abstract">A computer research tool for indexing, searching and displaying data is disclosed. Specifically, a computer research tool for performing computerized research of data including textual objects in a database or a network and for providing a user interface that significantly enhances data presentation is described. Textual objects and other data in a database or network is indexed by creating a numerical representation of the data. The indexing technique called proximity indexing generates a quick-reference of the relations, patterns and similarity found among the data in the database. Proximity indexing indexes the data by using statistical techniques and empirically developed algorithms. Using this proximity index, an efficient search for pools of data having a particular relation, pattern or characteristic can be effectuated. The Computer Search program, called the Computer Search Program for Data represented in Matrices (CSPDM), provides efficient computer search methods. The CSPDM rank orders data in accordance with the data's relationship to time, a paradigm datum, or any similar reference. An alternative embodiment of the invention employs a cluster link generation algorithm which uses links and nodes to index and search a database or network. The algorithm searches for direct and indirect links to a search node and retrieves the nodes which are most closely related to the search node. The user interface program, called the Graphical User Interface (GUI), provides a user friendly method of interacting with the CSPDM program and prepares and presents a visual graphical display. The graphical display provides the user with a two or three dimensional spatial orientation of the data.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(57)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00018.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00018.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00019.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00019.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00020.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00020.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00021.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00021.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00022.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00022.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00023.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00023.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00024.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00024.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00025.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00025.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00026.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00026.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00027.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00027.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00028.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00028.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00029.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00029.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00030.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00030.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00031.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00031.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00032.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00032.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00033.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00033.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00034.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00034.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00035.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00035.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00036.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00036.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00037.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00037.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00038.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00038.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00039.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00039.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00040.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00040.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00041.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00041.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00042.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00042.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00043.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00043.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00044.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00044.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00045.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00045.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00046.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00046.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00047.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00047.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00048.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00048.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00049.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00049.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00050.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00050.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00051.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00051.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00052.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00052.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00053.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00053.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00054.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00054.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00055.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00055.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6233571B1/US06233571-20010515-D00056.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6233571B1/US06233571-20010515-D00056.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(22)</span></span></div><div class="patent-text"><div mxw-id="PCLM28640798" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is: </claim-statement> <div class="claim"> <div num="1" id="US-6233571-B1-CLM-00001" class="claim">
      <div class="claim-text">1. A method for using active links within the data of an object stored in a database of a computer so that a user may jump from viewing the data of the object in the database to a position outside the object in the database and outside the computer, comprising:</div>
      <div class="claim-text">storing one or more links within data of the object in the database to positions outside of the computer, wherein the stored links are active links; </div>
      <div class="claim-text">displaying the data of the object within the database, wherein one or more active links are displayed with the data from the object in the database, wherein positions are nodes in a network that may be accessed, the active links including hyperjump links between nodes in the network and the objects, and the step of displaying comprises: </div>
      <div class="claim-text">generating a source map, wherein the source map represents hyperjump links that identify a chosen node as a destination of a link, and </div>
      <div class="claim-text">wherein the method further comprises activating a link represented on the source map, wherein a user may hyperjump to a node represented as a node of the link; </div>
      <div class="claim-text">selecting one of the displayed active links from those displayed with the displayed data; and </div>
      <div class="claim-text">jumping to the position outside the object in the database. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" id="US-6233571-B1-CLM-00002" class="claim">
      <div class="claim-text">2. The method of claim <b>1</b>, wherein the active links are embedded icons and wherein the step of selecting comprises activating an embedded icon.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" id="US-6233571-B1-CLM-00003" class="claim">
      <div class="claim-text">3. The method of claim <b>1</b>, wherein the active links are embedded text and wherein the step of selecting comprises activating the embedded text.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" id="US-6233571-B1-CLM-00004" class="claim">
      <div class="claim-text">4. The method of claim <b>1</b>, wherein computer software is used, further comprising:</div>
      <div class="claim-text">generating an active link, wherein the active link can be used to jump from a location in the database to another database. </div>
    </div>
    </div> <div class="claim"> <div num="5" id="US-6233571-B1-CLM-00005" class="claim">
      <div class="claim-text">5. A method for displaying information about a network that has hyperjump data, comprising:</div>
      <div class="claim-text">choosing a node; </div>
      <div class="claim-text">accessing the hyperjump data; </div>
      <div class="claim-text">identifying hyperjump data from within the accessed hyperjump data that has a direct reference to the chosen node; </div>
      <div class="claim-text">determining hyperjump data from within the accessed hyperjump data that has an indirect reference to the chosen node using the identified hyperjump data, wherein the step of determining comprises proximity analyzing the identified hyperjump data; and </div>
      <div class="claim-text">displaying one or more determined hyperjump data. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" id="US-6233571-B1-CLM-00006" class="claim">
      <div class="claim-text">6. The method of claim <b>5</b>, wherein the hyperjump data includes pointers and wherein the direct reference is a pointer pointing to the chosen node or from the chosen node, and the step of determining comprises analyzing the pointers.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" id="US-6233571-B1-CLM-00007" class="claim">
      <div class="claim-text">7. The method of claim <b>5</b>, wherein the node represents a topic, the determined hyperjump data has a relationship to the topic, and the step of displaying displays determined hyperjump data that has a relationship to the topic.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" id="US-6233571-B1-CLM-00008" class="claim">
      <div class="claim-text">8. The method of claim <b>5</b>, wherein the node is a web page in the network, the accessed hypejump data are Universal Resource Locators of linked pages, and the step of determining hyperjump data comprises analyzing the identified hyperjump data.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" id="US-6233571-B1-CLM-00009" class="claim">
      <div class="claim-text">9. The method of claim <b>5</b>, wherein the node is a document in the network and the determined hyperjump data has a relationship to the document, the step of displaying comprising the step of listing the hyperjump data that has a relationship to the document.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" id="US-6233571-B1-CLM-00010" class="claim">
      <div class="claim-text">10. The method of claim <b>5</b>, wherein the step of displaying comprises generating a graphical user display, and wherein information is displayed on a graphical display visually representing more than one coordinate plane.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" id="US-6233571-B1-CLM-00011" class="claim">
      <div class="claim-text">11. The method of claim <b>5</b>, wherein the nodes are nodes in the network that may be accessed, the hyperjump data includes hyperjump links between nodes in the network, and the step of displaying comprises:</div>
      <div class="claim-text">generating a source map using one or more of the determined hyperjump data, wherein the source map represents hyperjump links that identify the chosen node as a destination of a link; and </div>
      <div class="claim-text">wherein the method further comprises activating a link represented on the source map, wherein a user may hyperjump to a node represented as a node of the link. </div>
    </div>
    </div> <div class="claim"> <div num="12" id="US-6233571-B1-CLM-00012" class="claim">
      <div class="claim-text">12. A method for visually displaying data related to a web having identifiable web pages and Universal Resource Locators with pointers, comprising:</div>
      <div class="claim-text">choosing an identifiable web page; </div>
      <div class="claim-text">identifying Universal Resource Locators for the web pages, wherein the identified Universal Resource Locators either point to or point away from the chosen web page; </div>
      <div class="claim-text">analyzing Universal Resource Locators, including the identified Universal Resource Locators, wherein Universal Resource Locators which have an indirect relationship to the chosen web page are located, wherein the step of analyzing further comprises cluster analyzing the Universal Resource Locators for indirect relationships; and </div>
      <div class="claim-text">displaying identities of web pages, wherein the located Universal Resource Locators are used to identify web pages. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" id="US-6233571-B1-CLM-00013" class="claim">
      <div class="claim-text">13. The method of claim <b>12</b>, further comprising selecting a web page using the displayed identities of web pages.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" id="US-6233571-B1-CLM-00014" class="claim">
      <div class="claim-text">14. The method of claim <b>12</b>, further comprising hyperjumping to the selected web page.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" id="US-6233571-B1-CLM-00015" class="claim">
      <div class="claim-text">15. The method of claim <b>12</b>, wherein the step of displaying the identities of web pages comprises generating a graphical user display wherein information within the Universal Resource Locators is parsed and used to generate the graphical user display.</div>
    </div>
    </div> <div class="claim"> <div num="16" id="US-6233571-B1-CLM-00016" class="claim">
      <div class="claim-text">16. A method for navigating documents on the world wide web, comprising: choosing a document;</div>
      <div class="claim-text">identifying documents that have a direct relationship to the chosen document; </div>
      <div class="claim-text">locating documents that have an indirect relationship to the chosen document identifying Universal Resource Locators for the documents, wherein the identified Universal Resource Locators either point to or point away from the chosen document; </div>
      <div class="claim-text">analyzing Universal Resource Locators, including the identified Universal Resource Locators, wherein Universal Resource Locators which have an indirect relationship to the chosen document are located, wherein the step of analyzing further comprises cluster analyzing the Universal Resource Locators for indirect relationships; and </div>
      <div class="claim-text">displaying a located document. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" id="US-6233571-B1-CLM-00017" class="claim">
      <div class="claim-text">17. The method of claim <b>16</b>, wherein pages and their respective Universal Resource Locators are used and the step of locating documents comprises analyzing the pages and their respective Universal Resource Locators.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" id="US-6233571-B1-CLM-00018" class="claim">
      <div class="claim-text">18. The method of claim <b>17</b>, wherein the step of analyzing pages comprises cluster analyzing the pages.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" id="US-6233571-B1-CLM-00019" class="claim">
      <div class="claim-text">19. The method of claim <b>16</b>, wherein the step of displaying a located document comprises:</div>
      <div class="claim-text">generating a screen display of identities of one or more located documents; and </div>
      <div class="claim-text">selecting one or more of the located documents. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" id="US-6233571-B1-CLM-00020" class="claim">
      <div class="claim-text">20. The method of claim <b>19</b>, wherein the step of generating a screen display comprises generating a graphical display.</div>
    </div>
    </div> <div class="claim"> <div num="21" id="US-6233571-B1-CLM-00021" class="claim">
      <div class="claim-text">21. A method for displaying information about a network that has hyperjump data, comprising:</div>
      <div class="claim-text">choosing a node; </div>
      <div class="claim-text">accessing the hyperjump data; </div>
      <div class="claim-text">identifying hyperjump data from within the accessed hyperjump data that has a direct reference to the chosen node; </div>
      <div class="claim-text">determining hyperjump data from within the accessed hyperjump data that has an indirect reference to the chosen node using the identified hyperjump data, wherein the step of determining comprises cluster analyzing the hyperjump data; and </div>
      <div class="claim-text">displaying one or more determined hyperjump data. </div>
    </div>
    </div> <div class="claim"> <div num="22" id="US-6233571-B1-CLM-00022" class="claim">
      <div class="claim-text">22. A method for displaying information about a network that has hyperjump data, comprising:</div>
      <div class="claim-text">choosing a node; </div>
      <div class="claim-text">accessing the hyperjump data; </div>
      <div class="claim-text">identifying hyperjump data from within the accessed hyperjump data that has a direct reference to the chosen node; </div>
      <div class="claim-text">determining hyperjump data from within the accessed hyperjump data that has an indirect reference to the chosen node using the identified hyperjump data; and </div>
      <div class="claim-text">displaying one or more determined hyperjump data, wherein the nodes are nodes in the network that may be accessed, the hypejump data includes hyperjump links between nodes in the network, and the step of displaying comprises: </div>
      <div class="claim-text">generating a source map using one or more of the determined hyperjump data, wherein the source map represents hyperjump links that identify the chosen node as a destination of a link, and wherein the method further comprises activating a link represented on the source map, wherein a user may hyperjump to a node represented as a node of the link.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES54547515" lang="EN" load-source="patent-office" class="description">
    <heading>RELATED APPLICATIONS</heading> <p>This application is a divisional of U.S. patent application Ser. No. 08/649,304, filed May 17, 1996 entitled METHOD APPARATUS FOR INDEXING, SEARCHING AND DISPLAYING DATA, now U.S. Pat. No. 5,832,494, (which is hereby incorporated by reference) which is a continuation-in-part of U.S. application Ser. No. 08/076,658, filed Jun. 14, 1993 with the same title now U.S. Pat. No. 5,544,352.</p>
    <heading>TECHNICAL FIELD</heading> <p>This invention pertains to computerized research tools. More particularly, it relates to computerized research on databases. Specifically, the invention indexes data, searches data, and graphically displays search results with a user interface.</p>
    <heading>BACKGROUND</heading> <p>Two manuals containing background materials are hereby incorporated by reference V-Search Integration Tool Kit For Folio VIEWS, containing thirty-six (36) pages, V-Search Publisher's Tool Kit User's Manual, containing one hundred sixty (160) pages.</p>
    <p>Our society is in the information age. Computers maintaining databases of information have become an everyday part of our lives. The ability to efficiently perform computer research has become increasingly more important. Recent efforts in the art of computer research have been aimed at reducing the time required to accomplish research. Computer research on non-textual objects is very limited. Current computer search programs use a text-by-text analysis procedure (Boolean Search) to scan a database and retrieve items from a database. The user must input a string of text, and the computer evaluates this string of text. Then the computer retrieves items from the database that match the string of text. The two popular systems for computerized searching of data used in the legal profession are Westlaw, a service sold by West Publishing Company, 50 W. Kellogg Blvd., P.O. Box 64526, St. Paul, Minn. 55164-0526, and Lexis, a service sold by Mead Data Central, P.O. Box 933, Dayton, Ohio 45401.</p>
    <p>However, Boolean searches of textual material are not very efficient. Boolean searches only retrieve exactly what the computer interprets the attorney to have requested. If the attorney does not phrase his or her request in the exact manner in which the database represents the textual object, the Boolean search will not retrieve the desired textual object. Therefore, the researcher may effectively by denied access to significant textual objects that may be crucial to the project on which the researcher is working. A second problem encountered with Boolean searches is that the search retrieves a significant amount of irrelevant textual objects. (It should be noted that in the context of research, a textual object could be any type of written material. The term textual object is used to stress the fact that the present invention applies to all types of databases. The only requirement that a textual object must satisfy in order to be selected by a Boolean search program is that part of the textual object match the particular request of the researcher. Since the researcher cannot possibly know all of the groupings of text within all the textual objects in the database, the researcher is unable to phrase his request to only retrieve the textual objects that are relevant.</p>
    <p>Aside from the inefficiency of Boolean searches, the present systems for computerized searching of data are inadequate to serve the needs of a researcher for several other reasons. Even if one assumes that all the textual objects retrieved from a Boolean search are relevant, the listing of the textual objects as done by any currently available systems does not convey some important and necessary information to the researcher. The researcher does not know which textual objects are the most significant (i.e., which textual object is referred to the most by another textual object) or which textual objects are considered essential precedent (i.e., which textual objects describe an important doctrine).</p>
    <p>In the legal research field, both Westlaw and Lexis have a Shepardizing feature that enables the researcher to view a list of textual objects that mention a particular textual object. The Shepardizing feature does not indicate how many times a listed textual object mentions the particular textual object. Although the Shepardizing feature uses letter codes to indicate the importance of a listed textual object (e.g., an f beside a listed textual object indicates that the legal rule contained in particular textual object was followed in the listed textual object), data on whether a listed textual object followed the rule of a particular textual object is entered manually by employees of Shepard's/McGraw Hill, Inc., Div. of McGraw-Hill Book Co., 420 N. Cascade Ave., Colorado Springs, Colo. 80901, toll free 1-800-525-2474. Such a process is subjective and is prone to error.</p>
    <p>Another legal research system that is available is the Westlaw key number system. The Westlaw key number system has problems similar to the shepardizing feature on the Lexis and Westlaw systems.</p>
    <p>The video displays of both the West and Lexis systems are difficult to use. The simple text displays of these systems do not provide a researcher with all the information that is available in the database.</p>
    <p>Computerized research tools for legal opinions and related documents are probably the most sophisticated computer research tools available and therefore form the background for this invention. However, the same or similar computer research tools are used in many other areas. For example, computer research tools are used for locating prior art for a patent application. The same problems of inefficiency discussed above exist for computer research tools in many areas of our society.</p>
    <p>What is needed is a system for computerized searching of data that is faster than the available systems of research.</p>
    <p>What is needed is a system for computerized searching of data that enables researchers to research in a manner in which they are familiar.</p>
    <p>What is needed is a computerized research tool that will reorganize, re-index or reformat the data into a more efficient format for searching.</p>
    <p>What is needed are more sophisticated methods to search data.</p>
    <p>What is needed is a system for computerized searching of data that will significantly reduce the number of irrelevant textual objects it retrieves.</p>
    <p>What is needed is a user friendly computerized research tool.</p>
    <p>What is needed is a visual user interface which can convey information to a user conveniently.</p>
    <p>What is needed is a system for computerized searching of data that easily enables the researcher to classify the object according to his or her own judgment.</p>
    <p>What is needed is a system for computerized searching of data that provides a visual representation of lead objects and lines of objects, permitting a broad overview of the shape of the relevant landscape.</p>
    <p>What is needed is a system for computerized searching of data that provides an easily-grasped picture or map of vast amounts of discrete information, permitting researchers to zero in on the most relevant material.</p>
    <p>What is needed is a system for computer searching of data that provides a high degree of virtual orientation and tracking, the vital sense of where one has been and where one is going, and that prevent researchers from becoming confused while assimilating a large amount of research materials.</p>
    <p>Accordingly, there is an unanswered need for a user friendly computerized research tool. There is a need for intelligent research technology that emulates human methods of research. There is a need in the marketplace for a more efficient and intelligent computerized research tool.</p>
    <p>The present invention is designed to address these needs.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>This invention is a system for computerized searching of data. Specifically, the present invention significantly aids a researcher in performing computerized research on a database or a network. The invention simplifies the research task by improving upon methods of searching for data including textual objects, and by implementing a user interface that significantly enhances the presentation of the data.</p>
    <p>The invention can be used with an existing database by indexing the data and creating a numerical representation of the data. This indexing technique called proximity indexing generates a quick-reference of the relations, patterns, and similarity found among the data in the database. Using this proximity index, an efficient search for pools of data having a particular relation, pattern or characteristic can be effectuated. This relationship can then be graphically displayed.</p>
    <p>There are three main components to the invention: a data indexing applications program, a Computer Search Program for Data Represented by Matrices (CSPDM), and a user interface. Each component may be used individually. Various indexing application programs, CSPDMs, and user interface programs can be used in combination to achieve the desired results. The data indexing program indexes data into a more useful format. The CSPDM provides efficient computer search methods. The preferred CSPDM includes multiple search subroutines. The user interface provides a user friendly method of interacting with the indexing and CSPDM programs. The preferred user interface program allows for easy entry of commands and visual display of data via a graphical user interface.</p>
    <p>The method which the invention uses to index textual objects in a database is called Proximity Indexing. This method can also be used to index objects located on a network. The application of this method to network domains is discussed in greater detail later in this specification. Proximity Indexing is a method of preparing data in a database for subsequent searching by advanced data searching programs. Proximity Indexing indexes the data by using statistical techniques and empirically developed algorithms. The resulting search by an advanced data searching program of the Proximity Indexed data is significantly more efficient and accurate than a simple Boolean search.</p>
    <p>The Proximity Indexing Application Program indexes (or represents) the database in a more useful format to enable the Computer Search Program for Data Represented by Matrices (CSPDM) to efficiently search the database. The Proximity Indexing Application Program may include one or more of the following subroutines: an Extractor: a Patterner: and a Weaver. The Proximity Indexing Application Program indexes (or represents) data in a locally located database or remotely located database. The database can contain any type of data, including text, alphanumerics, or graphical information.</p>
    <p>In one embodiment, the database is located remotely from the Computer Processor and contains some data in the form of textual objects. The Proximity Indexing Application Program indexes the textual objects by determining how each full textual object (e.g., whole judicial opinion, statute, etc.) relates to every other full textual object by using empirical data and statistical techniques. Once each full textual object is related to each other full textual object, the Proximity Indexing Application Program compares each paragraph of each full textual object with every other full textual object as described above. The Proximity Indexing Application Program then clusters related contiguous paragraphs into sections. Subsequently, the Proximity Indexing Application Program indexes each section and the CSPDM evaluates the indexed sections to determine which sections to retrieve from the database. Such organization and classification of all of the textual objects in the database before any given search commences significantly limits the number of irrelevant textual objects that the CSPDM program retrieves during the subsequent search and allows retrieval of material based on its degree of relevancy.</p>
    <p>In a preferred embodiment, the Proximity Indexing Application Program includes a link generation subroutine wherein direct and indirect relationships between or among data is used to generate a representation of the data. Generally, direct and indirect relationships in the database are identified as links and placed in a table.</p>
    <p>Again, this method of computerized research can be used for nearly any database including those containing non-textual material, graphical material, newspapers material, data on personal identification, data concerning police records, etc.</p>
    <p>The remaining two programs in the present invention are the CSPDM and the GUI Program. The CSPDM has seven subroutines that each search for different pools of objects. The GUI Program also has seven subroutines. Each CSPDM subroutine performs a different type of search. Each of the subroutines of the GUI uses the results of the corresponding subroutine of the CSPDM to create the proper display on the display.</p>
    <p>After the Proximity Indexing Application Program indexes a database, the CSPDM application program is used to search the indexed database. For example, the CSPDM program can either be located in memory that is remote from the Computer Processor or local to the Computer Processor. In addition, the CSPDM program can either be remote or local in relation to the database.</p>
    <p>The subroutines of the CSPDM utilize the coefficients and other data created by the Proximity Indexing Application Program to facilitate its search. However, if the researcher does not have the particular object citation available, the researcher can perform a Boolean search to retrieve and organize a pool of objects. Alternatively, the researcher can subsequently search for related objects by using the Pool-Similarity Subroutine, the Pool-Paradigm Subroutine, the Pool-Importance Subroutine or the Pool-Paradigm-Similarity Subroutine as defined below.</p>
    <p>If the researcher already has the citation of a particular object available, the researcher can search for related objects by utilizing the Cases-In Subroutine, Cases-After Subroutine or Similar-Cases Subroutine. The Cases-In Subroutine retrieves all of the objects from the database to which a selected object refers. In addition, the subroutine determines the number of times the selected object refers to each retrieved object and other characteristics of each object, including its importance, and degree of relatedness to the selected object.</p>
    <p>The Cases-After Subroutine retrieves all of the objects from the database that refer to the selected object. Also, the subroutine determines the number of times each retrieved object refers to the selected object and other characteristics of each object, including its importance and degree of relatedness to the particular object to which it refers.</p>
    <p>The Similar-Cases Subroutine determines the degree of similarity between the retrieved objects and the selected object. Similarity may be defined, in the context of legal cases, as the extent to which the two objects lie in the same lines of precedent or discuss the same legal topic or concept. Numerous other relationships may be used to define similarity.</p>
    <p>In addition, for a textual, object, if the researcher does not know of a particular textual object on which to base his or her search, the researcher may execute a Boolean word search. After a standard Boolean word search has been run, the researcher may run the Pool-Similarity Subroutine to retrieve information containing the degree of similarity between each textual object in the pool and a particular textual object selected by the user. Similarly, the Pool-Importance Subroutine can be used to determine the degree of importance (i.e., whether a judicial opinion is a Supreme Court opinion or a District Court opinion) and other characteristics of each textual object retrieved using the Boolean word search.</p>
    <p>The Pool-Paradigm Subroutine calculates the geographic center in vector space of the pool of textual objects retrieved by the Boolean word search or other pool generating method. It then orders the retrieved textual objects by their degree of similarity to that center or paradigm. The researcher can then evaluate this typical textual object and utilize it to help him or her find other relevant textual objects. In addition, the researcher can scan through neighboring typical textual objects to evaluate legal subjects that are closely related to the subject of the researcher's search.</p>
    <p>The Pool-Paradigm-Similarity Subroutine similarly creates a paradigm textual object from the retrieved textual objects. However, the subroutine calculates the similarity of all textual objects in the database to the paradigm textual object in addition to the similarity of the retrieved textual objects to the paradigm textual object.</p>
    <p>After the CSPDM has retrieved the desired objects, the Graphical User Interface (GUI) Program may be used to display the results of the search on the display. In one embodiment, the GUI is a user interface program. The GUI Program contains three main subroutines: Cases-In Display Subroutine (CIDS), Cases-After Display Subroutine (CADS) and Similar-Cases Display Subroutine (SCDS). The main subroutines receive information from the corresponding subroutines Cases-In, Cases-After and Similar-Cases of the CSPDM. The GUI Program also contains four secondary subroutines: Pool-Similarity Display Subroutine (PSDS), Pool-Paradigm Display Subroutine (PPDS), Pool-Importance Display Subroutine (PIDS), and the Pool-Paradigm-Similarity Subroutine (PPSDS). The secondary subroutines also receive information from the corresponding subroutines Pool-Similarity Subroutine, Pool-Paradigm Subroutine, Pool-Importance Subroutine and the Pool-Paradigm Similarity Subroutine of the CSPDM.</p>
    <p>The CIDS subroutine receives information gathered from the Cases-In Subroutine of the CSPDM. The CIDS subroutine displays user friendly active boxes and windows on the display which represent the textual objects retrieved from the database represented in Euclidean space. It can also use the boxes to represent objects retrieved from a network. Various active box formats and arranging of information within the boxes may be utilized. The display depicts the appropriate location of textual objects in Euclidean space on a coordinate means. An algorithm may be used to determine the appropriate location of the boxes. The coordinate means may have one or more axis. In one embodiment, the horizontal axis of the coordinate means may represent the time of textual object creation; the vertical axis could represent a weighted combination of the number of sections in which that particular retrieved text is cited or discussed, its degree of importance, and its degree of similarity to the host textual object and the depth axis (Z-axis) represents the existence of data and length of the textual data or object.</p>
    <p>The invention can also alter the background color of the window itself to communicate additional information graphically to the user. For example, if the horizontal axis represented time, then the invention could display the portion of the window containing objects occurring previous to the search object in one color and the portion containing the objects occurring after in another. Thus, the researcher can understand at a glance the relative position of his search target in relation to all the other objects related to it.</p>
    <p>CIDS also enables the researcher to open up various active boxes on the display by entering a command into the computer processor with the input means. After entering the proper command, the active box transforms into a window displaying additional information about the selected textual object. These windows can be moved about the display and stacked on top or placed beside each other via the input means to facilitate viewing of multiple windows of information simultaneously. In one embodiment, the windows are automatically arranged by the computer system. Since the number of textual objects retrieved in a single search may exceed the amount which could be displayed simultaneously, the GUI Program enables the researcher to zoom in or zoom out to different scales of measurement on both the horizontal and vertical axis.</p>
    <p>The CADS receives information gathered by the Cases-After Subroutine of the CSPDM. The CADS creates a display similar to the CIDS display. However, the active boxes representing the retrieved textual objects indicate which textual objects in the database refer to a selected textual object as opposed to which textual objects a selected textual object refers.</p>
    <p>The SCDS receives information gathered by the Similar-Cases Subroutine of the CSPDM. The SCDS causes a similar display on the display as the CIDS and the CADS except that the vertical axis indicates the degree of similarity between the retrieved textual objects and the selected textual object.</p>
    <p>The GUI Program contains four secondary subroutines: Pool-Search Display Subroutine (PSDS), Pool-Paradigm Display Subroutine (PPDS), Pool-Importance Display Subroutine (PIDS) and the Pool-Paradigm-Similarity Display Subroutine (PPSDS). The PSDS receives the results gathered by the Pool-Search Subroutine of the CSPDM. The PPDS receives the results gathered by the Pool-Paradigm Subroutine of the CSPDM. The PIDS receives the results gathered by the Pool-Importance Subroutine of the CSPDM. The PPSDS receives the results gathered by the Pool-Paradigm-Similarity Subroutine of the CSPDM. The results of the PSDS, PPDS, PIDS and PPSDS are then displayed in a user friendly graphical manner similar to the results of the CIDS, CADS and SCDS. A researcher can access the PSDS, PIDS, PSDS or PPSDS from any of the three main or four secondary subroutines of the GUI to gather information corresponding to the active boxes that represent the pool of textual objects retrieved by the corresponding subroutine of the CSPDM.</p>
    <p>By using the graphical display, the researcher can view immediately a visual representation of trends in the data (for example, trends developing in the law and current and past legal doctrines). In addition, the researcher can immediately identify important data or important precedent and which object serving as the precedent is most important to the project on which the researcher is working. This visual representation is a vast improvement over the current computerized research tools. Furthermore, the researcher using the present invention does not have to rely on the interpretation of another person to categorize different textual objects because the researcher can immediately visualize the legal trends and categories of law. In addition, new topic areas can be recognized without direct human intervention. The current research programs require a researcher to view objects in a database or to read through the actual text of a number of objects in order to determine which objects are important, interrelated, or most closely related to the topic at hand and which ones are not.</p>
    <p>It is an object of this invention to create an efficient and intelligent system for computerized searching of data that is faster than available systems of research.</p>
    <p>It is an object of the invention to integrate the system of computerized searching into the techniques to which researchers are already accustomed.</p>
    <p>It is an object of the invention to utilize statistical techniques along with empirically generated algorithms to reorganize, re-index and reformat data in a database into a more efficient model for searching.</p>
    <p>It is an object of the invention to utilize statistical techniques along with empirically generated methods to increase the efficiency of a computerized research tool.</p>
    <p>It is an object of the invention to create a system of computerized searching of data that significantly reduces the number of irrelevant objects retrieved.</p>
    <p>It is an object of this invention to create a user friendly interface for computer search tools which can convey a significant amount of information quickly.</p>
    <p>It is an object of the invention to enable the researcher to easily and immediately classify retrieved database objects according to the researcher's own judgment.</p>
    <p>It is an object of the invention to provide a visual representation of lead objects and lines of objects, permitting a broad overview of the shape of the relevant landscape.</p>
    <p>It is an object of the invention to provide an easily-grasped picture or map of vast amounts of discrete information, permitting researchers to zero in on the most relevant material.</p>
    <p>It is an object of the invention to provide a high degree of virtual orientation and tracking that enables a researcher to keep track of exactly what information the researcher has already researched and what information the researcher needs to research.</p>
    <p>These and other objects and advantages of the invention will become obvious to those skilled in the art upon review of the description of a preferred embodiment, and the appended drawings and claims.</p>
    <heading>DESCRIPTION OF THE DRAWINGS</heading> <p>FIG. 1 is a high level diagram of the hardware for the system for computerized searching of data.</p>
    <p>FIG. 2 is high level diagram of the software for the system for computerized searching of data. The three main programs are the Proximity Indexing Application Program, the Computer Search Program for Data Represented by Matrices (CSPDM) Application Program and the Graphical User Interface (GUI) Program.</p>
    <p>FIG. 3A is a flow chart illustrating a possible sequence of procedures that are executed during the Proximity Indexing Application Program.</p>
    <p>FIG. 3B is a flow chart illustrating a possible sequence of the specific subroutines that are executed during one stage of the Proximity Indexing Application Program. The subroutines are the Initial Extractor Subroutine, Opinion Patterner Subroutine, the Opinion Weaver Subroutine, the Paragraph Patterner Subroutine (Optional), the Paragraph Weaver Subroutine and the Section Comparison Subroutine.</p>
    <p>FIG. 3C is flow chart illustrating a possible sequence of subroutines that are executed after the Section Comparison Subroutine. The Section Comparison Subroutine may comprise the Sectioner-Geographic Subroutine and the Section-Topical Subroutine (Optional). The sequence of subroutines executed after the Section Comparison Subroutine are the Section Extractor Subroutine, the Section Patterner Subroutine and the Section Weaver Subroutine.</p>
    <p>FIG. 3D is a high level flow chart illustrating a possible sequence of subroutines that comprise the Boolean Indexing Subroutine which are executed during another stage of the Proximity Indexing Application Program. The first two subroutines, Initialize Core English Words and Create pw Boolean Matrix, are executed by the Initial Extractor Subroutine. The results are then run through the Pool-Patterner Subroutine, the Pool-Weaver Subroutine, the Pool-Sectioner Subroutine, the Section-Extractor Subroutine, the Section-Patterner Subroutine and the Section Weaver Subroutine.</p>
    <p>FIG. 3E is a chart illustrating the database format. The figure shows the types of structures contained within the database, links, link types, link subtypes, nodes, node types, node subtypes, and visual styles and also shows the various types of information that can be assigned to the links and nodes, including weights, identifications, names, comments, icons, and attributes.</p>
    <p>FIG. 3F is a high level diagram showing a sequence of nodes, N<sub>o</sub>-N<sub>3</sub>, connected by direct links which have weights W<sub>1</sub>-W<sub>3</sub>.</p>
    <p>FIG. 3G is a high level diagram showing a sequence of nodes, N<sub>1</sub>-N<sub>3</sub>, connected by direct and indirect links. The set of cluster links are also shown in the figure as functions of the weights associated with the direct links and the weight of the previous cluster link.</p>
    <p>FIG. 3H is a flow chart which depicts the Cluster Link Generation Algorithm.</p>
    <p>FIG. 4A is a high level diagram illustrating the flow of various search routines depending on the type of search initiated by the user by inputing commands to the Computer Processor via the input means. The diagram further illustrates the interaction between the CSPDM and the GUI Program.</p>
    <p>FIG. 4B is a high level flow chart illustrating the sequence of subroutines in the CSPDM program and user interactions with the subroutines.</p>
    <p>FIG. 4C is a high level flow chart for the Cases-In Subroutine.</p>
    <p>FIG. 4D is a high level flow chart for the Cases-After Subroutine.</p>
    <p>FIG. 4E is a high level flow chart for the Similar-Cases Subroutine.</p>
    <p>FIG. 4F is a high level flow chart for the Pool-Similarity Subroutine.</p>
    <p>FIG. 4G is a high level flow chart for the Pool-Paradigm Subroutine.</p>
    <p>FIG. 4H is a high level flow chart for the Pool-Importance Subroutine.</p>
    <p>FIG. 4I is a high level flow chart showing two possible alternate Pool-Paradigm-Similarity Subroutines.</p>
    <p>FIG. 5A is a high level diagram illustrating the interaction between respective subroutines of the CSPDM and of the GUI Program. The diagram further illustrates the interaction between the GUI Program and the display.</p>
    <p>FIG. 5B is an example of the display once the Cases-After Display Subroutine (CADS) is executed.</p>
    <p>FIG. 5C is an example of the display after a user selects an active box representing a textual object retrieved by the Cases-After Subroutine and chooses to open the full text window relating to the icon.</p>
    <p>FIG. 5D is an example of the display once the Cases-In Display Subroutine (CIDS) is executed.</p>
    <p>FIG. 5E is an example of the display once the Similar-Cases Display Subroutine (SCDS) is executed.</p>
    <p>FIG. 5F is an example of the display after a user chooses to execute the Similar Cases Subroutine for a textual object retrieved by the Similar-Cases Subroutine represented in FIG. <b>5</b>E.</p>
    <p>FIG. 5G is an example of the display after a user chooses to execute the Similar Cases Subroutine for one of the cases retrieved by the Similar-Cases Subroutine represented in FIG. <b>5</b>F.</p>
    <p>FIG. 5H depicts an Executive Search Window.</p>
    <p>FIG. 6 depicts a schematic representation of eighteen patterns.</p>
    <p>FIG. 7 is a high level diagram of the Layout of Boxes Algorithm.</p>
    <p>FIG. 8 is a diagram of a screen showing execution of a show usage command.</p>
    <p>FIG. 9 is a diagram of the Internal Box Layout Algorithm.</p>
    <p>FIG. 10A is a diagram of a screen showing an Influence Map, which is a screen used in one embodiment of this invention.</p>
    <p>FIG. 10B is a diagram of a screen showing a Source Map, which is a screen used in one embodiment of this invention.</p>
    <p>FIG. 10C is a diagram of a screen showing a Cluster Map, which is a screen used in one embodiment of the invention.</p>
    <p>FIG. 11 depicts a Look-Up Table for Bitmaps.</p>
    <p>FIG. 12 is a software flow chart for the auto arranging window feature.</p>
    <p>FIG. 13A is a depiction of a display with vertically tiled windows.</p>
    <p>FIG. 13B is a depiction of a display with horizontally tiled windows.</p>
    <p>FIG. 14A is a high level diagram of a method for searching, indexing, and displaying data stored in a network.</p>
    <p>FIG. 14B is a high level diagram of a method for searching, indexing, and displaying data stored in a network using the cluster generation algorithm.</p>
    <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT</heading> <p>Referring now to the drawings, the preferred embodiment of the present invention will be described.</p>
    <p>FIG. 1 is an overview of the preferred embodiment of the hardware system <b>26</b> for computerized searching of data. The hardware system <b>26</b> comprises a Computer Processor <b>30</b>, a database <b>54</b> for storing data, input means, display <b>38</b>, and RAM <b>34</b>.</p>
    <p>The Computer Processor <b>30</b> can be a processor that is typically found in Macintosh computers, IBM computers, portable PCs, clones of such PC computers (e.g. Dell computers), any other type of PC, or a processor in a more advanced or more primitive computing device. Parallel processing techniques may also be utilized with this invention.</p>
    <p>The database <b>54</b> is connected to the Computer Processor <b>30</b> and can be any device which will hold data. For example, the database <b>54</b> can consist of any type of magnetic or optical storing device for a computer. The database <b>54</b> can be located either remotely from the Computer Processor <b>30</b> or locally to the Computer Processor <b>30</b>. The preferred embodiment shows a database <b>54</b> located remotely from the Computer Processor <b>30</b> that communicates with the personal computer <b>28</b> via modem or leased line. In this manner, the database <b>54</b> is capable of supporting multiple remote Computer Processors <b>30</b>. The preferred connection <b>48</b> between the database <b>54</b> and the Computer Processor <b>30</b> is a network type connection over a leased line. It is obvious to one skilled in the art that the database <b>54</b> and the Computer Processor <b>30</b> may be electronically connected in a variety of ways. In the preferred embodiment the database <b>54</b> provides the large storage capacity necessary to maintain the many records of textual objects.</p>
    <p>The input means is connected to the Computer Processor <b>30</b>. The user enters input commands into the Computer Processor <b>30</b> through the input means. The input means could consist of a keyboard <b>46</b>, a mouse <b>42</b>, or both working in tandem. Alternatively, the input means could comprise any device used to transfer information or commands from the user to the Computer Processor <b>30</b>.</p>
    <p>The display <b>38</b> is connected to the Computer Processor <b>30</b> and operates to display information to the user. The display <b>38</b> could consist of a computer monitor, television, LCD, LED, or any other means to convey information to the user.</p>
    <p>The Random Access Memory (RAM <b>34</b>) is also connected to the Computer Processor <b>30</b>. The software system <b>60</b> for computerized searching of data may reside in the RAM <b>34</b>, which can be accessed by the Computer Processor <b>30</b> to retrieve information from the software routines. A Read Only Memory (ROM), Erasable Programmable Read Only Memory (EPROM), disk drives, or any other magnetic storage device could be used in place of the RAM <b>34</b>. Furthermore, the RAM <b>34</b> may be located within the structure of the Computer Processor <b>30</b> or external to the structure.</p>
    <p>The hardware system <b>26</b> for computerized searching of data shown in FIG. 1 supports any one, or any combination, of the software programs contained in the software system <b>60</b> for computerized searching of data. The software system <b>60</b> for the computerized searching of data comprises one or more of the following programs: the Proximity Indexing Application Program <b>62</b>, the Computer Search Program for Data Represented by Matrices (CSPDM <b>66</b>) and the Graphical User Interface (GUI <b>70</b>) Program. The Proximity Indexing Application Program <b>62</b> could reside in RAM <b>34</b> or in separate memory <b>58</b> connected to the database <b>54</b>. The Computer Processor <b>30</b> or a separate computer processor <b>50</b> attached to the database <b>54</b> could execute the Proximity Indexing Application Program <b>62</b>. In the preferred embodiment the Proximity Indexing Application Program <b>62</b> resides in separate memory <b>58</b> that is accessible to the database <b>54</b>, and a separate computer processor <b>50</b> attached to the database <b>54</b> executes the Proximity Indexing Application Program <b>62</b>.</p>
    <p>The CSPDM <b>66</b> could reside in the RAM <b>34</b> connected to the Computer Processor <b>30</b> or in the separate memory connected to the database <b>54</b>. In the preferred embodiment, the CSPDM <b>66</b> is located in the RAM <b>34</b> connected to the Computer Processor <b>30</b>. This is also the preferred embodiment for the application of this method to network searching. For network application, a separate database <b>54</b> storing information to be analyzed is remotely connected to the computer processor <b>30</b>. The CSPDM <b>66</b> may use the display <b>38</b> to depict input screens for user entry of information.</p>
    <p>The GUI Program <b>70</b> could likewise reside in the RAM <b>34</b> connected to the Computer Processor <b>30</b> or in separate memory <b>58</b> connected to the database <b>54</b>. In the preferred embodiment, the GUI Program <b>70</b> is located in the RAM <b>34</b> connected to the Computer Processor <b>30</b>. The GUl Program <b>70</b> also communicates with the display <b>38</b> to enhance the manner in which the display <b>38</b> depicts information.</p>
    <p>FIG. 2 is an overview of the preferred embodiment of the software system <b>60</b> for computerized searching of data. The software system <b>60</b> for computerized searching of data comprises at least one or more of the following programs: the Proximity Indexing Application Program <b>62</b>, the Computer Search Program for Data Represented by Matrices (CSPDM <b>66</b>) and the Graphical User Interface (GUI <b>70</b>) Program. Proximity Indexing is a method of identifying relevant data by using statistical techniques and empirically developed algorithms. (See Appendix #2) The Proximity Indexing Application Program <b>62</b> is an application program which represents or indexes the database <b>54</b> to a proper format to enable the Computer Search Program for Data Represented by Matrices (CSPDM <b>66</b>) to properly search the database <b>54</b>. The Proximity Indexing Application Program <b>62</b> can index data in a local database <b>54</b> or a remote database <b>54</b>. The Proximity Indexing Application Program <b>62</b> is shown in more detail in FIGS. 3A to <b>3</b>H.</p>
    <p>After the Proximity Indexing Application Program <b>62</b> indexes the database <b>54</b>, the CSPDM <b>66</b> application program can adequately search the database <b>54</b>. The CSPDM <b>66</b> program searches the database <b>54</b> for objects according to instructions that the user enters into the Computer Processor <b>30</b> via the input means. The CSPDM <b>66</b> then retrieves the requested objects. The CSPDM <b>66</b> either relays the objects and other information to the GUI Program <b>70</b> in order for the GUI Program <b>70</b> to display this information on the display <b>38</b>, or the CSPDM <b>66</b> sends display commands directly to the Computer Processor <b>30</b> for display of this information. However, in the preferred embodiment, the CSPDM <b>66</b> relays the objects and other commands to the GUI Program <b>70</b>. The CSPDM <b>66</b> is described in more detail in FIGS. 4A to <b>4</b>I.</p>
    <p>After the CSPDM <b>66</b> has retrieved the objects, the Graphical User Interface (GUI <b>70</b>) Program, which is a user interface program, causes the results of the search to be depicted on the display <b>38</b>. The GUI Program <b>70</b> enhances the display of the results of the search conducted by the CSPDM <b>66</b>. The GUI Program <b>70</b>, its method and operation, can be applied to other computer systems besides a system for computerized searching of data. The GUI Program <b>70</b> is described in more detail in FIGS. 5A to <b>5</b>H.</p>
    <p>FIGS. 3A to <b>3</b>D depict examples of the procedures and subroutines of a Proximity Indexing Application Program <b>62</b>, and possible interactions among the subroutines. FIG. 3A depicts a sequence of procedures followed by the Proximity Indexing Application Program <b>62</b> to index textual objects for searching by the CSPDM <b>66</b>. FIG. 3B depicts specific subroutines that the Proximity Indexing Application Program <b>62</b> executes to partition full textual objects into smaller sections. FIG. 3C depicts subroutines executed by the Section Comparison Routine of FIG. <b>3</b>B and subsequent possible subroutines to format and index the sections. FIG. 3D depicts a sequence of subroutines of the Proximity Indexing Application Program <b>62</b> which first sections and then indexes these sections of core english words <b>140</b> contained in the database <b>54</b>. Core english words <b>140</b> are words that are uncommon enough to somewhat distinguish one textual object from another. The word searches of the CSPDM <b>66</b> search these sections of core English words to determine which textual objects to retrieve.</p>
    <p>FIGS. 3E-3H show a preferred embodiment for representing the data in a database <b>54</b> or documents in a network in accordance with the present invention. The application of this method for representing documents on a network is described in greater detail later in this specification.</p>
    <p>FIG. 3E shows a method for representing the data using the present invention. Specifically, FIG. 3E shows a method in which links <b>2004</b> and nodes <b>2008</b> can be used along with link types <b>2012</b>, link subtypes <b>2020</b>, node types <b>2016</b> and node subtypes <b>2024</b> to represent the data.</p>
    <p>A node <b>2008</b> is any entity that can be represented by a box on a display <b>38</b> such as a GUI <b>70</b>. A node <b>2008</b> might be, for example, an object in a database <b>54</b>, a portion of an object in a database <b>54</b>, a document, a section of a document, a World Wide Web page, or an idea or concept, such as a topic name. A node <b>2008</b> need not represent any physical entity such as an actual document. It is preferred that a node <b>2008</b> have links <b>2004</b>, specifically, it is preferred that a node <b>2008</b> have links to other nodes <b>2008</b> (for example source links (a source link is a link <b>2004</b>), or influence links (an influence link is a link <b>2004</b>)). A node <b>2008</b> can represent any idea or concept that has links to other ideas or concepts. For example, two nodes <b>2008</b> can exist such as a node <b>2008</b> called Modem Architecture (not shown) and a node <b>2008</b> called Classical Architecture (not shown) and the links would show that Classical Architecture is a source for Modern Architecture and that Modem Architecture is influenced by Classical Architecture. In this example, a source link <b>2004</b> and an influence link <b>2004</b> would exist between the two nodes <b>2008</b>. (Many times, links <b>2004</b> represent inverse relationships such as source links <b>2004</b> and influence links <b>2004</b>, and one type of link may be derived or generated from analysis of another link.)</p>
    <p>More specifically, in the preferred embodiment, the software defines a node <b>2010</b> as something that has a unique node <b>2008</b> identification, a node type <b>2016</b>, a node subtype <b>2024</b>, and an associated date (or plot date <b>2011</b>). Node types <b>2016</b> or subtypes <b>2024</b> may have names <b>2021</b> or identifications, title descriptors <b>2026</b> and external attributes <b>2018</b>. A node <b>2008</b> may have a corresponding numerical representation assigned, a vector, a matrix, or a table. In the preferred embodiment a table format is used for the nodes.</p>
    <p>Referring to FIGS. 3E, <b>3</b>F, and <b>3</b>G, a link <b>2004</b> is another name or identification for a relationship between two nodes <b>2008</b>. The relationship may be semantical, non-semantical, stated, implied, direct <b>2032</b>, indirect <b>2036</b>, actual, statistical and/or theoretical. A link <b>2004</b> can be represented by a vector or an entry on a table and contain information for example, a from-node identification <b>2010</b> (ID), a to-node ID <b>2010</b>, a link type <b>2012</b>, and a weight <b>2034</b>. A group of links <b>2004</b> may be represented by a series of vectors or entries in a table, a link table. Link subtypes <b>2020</b> may be used, named and assigned comments.</p>
    <p>In addition, to better integrate the GUI <b>70</b> and the data representation, visual styles <b>2028</b> may be assigned for example to nodes <b>2008</b>, links <b>2004</b>, link types <b>2012</b>, and link subtypes <b>2020</b> to assist in the visual displays <b>38</b>.</p>
    <p>In the preferred embodiment, three types of links <b>2004</b> are used: source links <b>2004</b>, influence links <b>2004</b> and cluster links <b>2004</b>. Source links <b>2004</b> generally link a first node <b>2008</b> to second node <b>2008</b> that represents information or documentation specifically cited or referred to by the first node <b>2008</b>. Influence links <b>2004</b> are generally the inverse of a source link <b>2004</b>. The relationships represented by these links <b>2004</b> may be explicit or implied.</p>
    <p>Links <b>2004</b> and nodes <b>2008</b> may be manually entered by a user or automatically generated by a computer <b>30</b>. It is preferred that duster links <b>2004</b> be generated automatically by a processor. A cluster link <b>2004</b> is a relationship between two nodes <b>2008</b>, for example, two nodes <b>2008</b> both directly linked to the same intermediate nodes <b>2008</b>, may be indirectly linked through many paths and therefore have a cluster link <b>2004</b> between them. The cluster links <b>2004</b> may be determined using the specific or general methods described later for finding relationships in a database <b>54</b>. However, the preferred method is through using a Proximity Indexing Application Program <b>62</b>.</p>
    <p>Proximity indexing is a method of indexing that uses statistical techniques and empirically generated algorithms to organize and categorize data stored in databases or on a network. The Proximity Indexing Application Program <b>62</b> applies the Proximity indexing method to a database <b>54</b>. One embodiment of the present invention uses the Proximity Indexing Application Program <b>62</b> to Proximity index textual objects used for legal research by indexing objects based on their degree of relatednessin terms of precedent and topicto one another.</p>
    <p>Applying the method to legal research, the Proximity indexing system treats any discrete text as a textual object. Textual objects may contain citations, which are explicit references to other textual objects. Any legal textual object may have a number of different designations of labels. For example, 392 U.S. 1, 102 S.Ct 415, 58 U.S.L.W. 1103, etc. may all refer to the same textual object.</p>
    <p>Cases are full textual objects that are not subsets of other textual objects. Subjects of a full textual object include words, phrases, paragraphs, or portions of other full textual objects that are referred to in a certain full textual object. (The system does not treat textual objects as subsets of themselves.)</p>
    <p>Every case, or full textual object, is assigned a counting-number name designated by a letter of the alphabet in this descriptioncorresponding to its chronological order in the database <b>54</b>. Obviously, textual objects may contain citations only to textual objects that precede them. In other words, for full textual objects, if B cites A, (i.e. A is an element of B or the set B contains the name A), textual object A came before B, or symbolically, A&lt;B. Every textual object B contains a quantity of citations to full textual objects, expressed as Q(B), greater than or equal to zero, such that Q(B)&lt;B.</p>
    <p>Textual objects other than full textual objects may be subsets of full textual objects and of each other. For example, a section, page, or paragraph of text taken from a longer text may be treated as a textual object. Phrases and words are treated as a special kind of textual object, where Q(w)=0. Sections, pages, and paragraphs are generally subsets of only one full textual object, and may be organized chronologically under the numerical name of that full textual object. For purposes of chronology, phrases and words are treated as textual objects that precede every full textual object, and can generally be treated as members of a set with name 0, or be assigned arbitrary negative numbers.</p>
    <p>Any two textual objects may be related to each other through a myriad of patterns. Empirical research demonstrates that eighteen patterns capture most of the useful relational information in a cross-referenced database <b>54</b>. A list of these eighteen patterns, in order of importance, follows:</p>
    <p>Given that:</p>
    <p>a, b, c&lt;A;</p>
    <p>A&lt;d, e, f&lt;B; and</p>
    <p>B&lt;g, h, i.</p>
    <heading>Patterns Between A and B Include</heading> <p>1. B cites A.</p>
    <p>2. A cites c, and B cites c.</p>
    <p>3. g cites A, and g cites B.</p>
    <p>4. B cites f, and f cites A.</p>
    <p>5. B cites f, f cites e, and e cites A.</p>
    <p>6. B cites f, f cites e, e cites d, and d cites A.</p>
    <p>7. g cites A, h cites B. g cites a, and h cites a.</p>
    <p>8. i cites B, i cites f [or g], and f [or g] cites A.</p>
    <p>9. i cites g, i cites A, and g cites B.</p>
    <p>10. i cites g [or d], i cites h, g [or d] cites A, and h cites B.</p>
    <p>11. i cites a, i cites B, and A cites a.</p>
    <p>12. i cites A, i cites e, B cites e.</p>
    <p>13. g cites A, g cites a, A cites a, h cites B, and h cites a.</p>
    <p>14. A cites a, B cites d, i cites a, and i cites d.</p>
    <p>15. i cites B, i cites d, A cites a, and d cites a.</p>
    <p>16. A cites b, B cites d [or c], and d [or c] cites b.</p>
    <p>17. A cites b, B cites d, b cites a, and d cites a.</p>
    <p>18. A cites a, B cites b, d [or c] cites a, and d [or c] cites b.</p>
    <p>These 18 patterns are shown schematically in FIG. <b>6</b>.</p>
    <p>(For a discussion on probability theory and statistics, see Wilkinson, Leland; SYSTAT: The System for Statistics; Evanston, Ill.: SYSTAT Inc., 1989 incorporated herein by reference.) Some patterns occur only between two full textual objects, and others between any two textual objects; this distinction is explained below.</p>
    <p>Semantical patterning is only run on patterns number one and number two, shown above.</p>
    <p>For purposes of explaining how patterns are used to generate the Proximity Index, only the two simplest patterns are illustrated.</p>
    <p>The simplest, Pattern #1, is B cites A. See FIG. <b>6</b>. In the notation developed, this can be diagramed: a b c A d e f B g h i, where the letters designate textual objects in chronological order, the most recent being on the right, arrows above the text designate citations to A or B, and -arrows below the text designate all other citations. The next simplest pattern between A and B, Pattern #2, is A cites c, and B cites c, which can also be expressed as there exists c, such that c is an element of (A intersect B). See Appendix #1. This can be diagramed: a b c A d e f B g h i. For every textual object c from 0 to (A1), the existence of Pattern #2 on A and B is signified by 1, its absence by 0. This function is represented as P#2AB(c)=1 or P#2AB(c)=0. The complete results of P#1AB and P#2AB can be represented by an (A)(1) citation vector designated X.</p>
    <p>The functions of some Patterns require an (n)(1) matrix, a pattern vector. Therefore it is simplest to conceive of every Pattern function generating an (n)(1) vector for every ordered pair of full textual objects in the database <b>54</b>, with missing arrays filled in by 0s. Pattern Vectors can be created for Pattern #1 through Pattern #4 by just using the relationships among textual object A and the other textual objects in the database <b>54</b> and among textual object B and the other textual objects in the database <b>54</b>. Pattern Vectors for Patterns # 5 through # 18 can only be created if the relationship of every textual object to every other textual object is known. In other words, Pattern Vectors for Patterns # 1 through # 4, can be created from only the rows A and B to the Citation Matrix but Pattern Vectors for Patterns #5 through #18 can only be created from the whole Citation Matrix.</p>
    <p>(total textual objects c)/(theoretical maximum textual objects c) [(x)(x)<sup>T</sup>/TMax],</p>
    <p>(total textual objects c)/(actual maximum textual objects c) [(x)(x)<sup>T</sup>/AMax]</p>
    <p>frequency of object c per year [f], and</p>
    <p>the derivative of the frequency [f].</p>
    <p>In pattern # 2, given that A&lt;B, the theoretical maximum (TMax) number Q(A intersect B)=A minus 1. The actual maximum possible (AMax), given A and B, is the lesser of Q(A) and Q(B). The ratios X(X)<sup>T</sup>/TMax and X(X)<sup>T</sup>/AMax, as well as the frequency of occurrence of textual objects c per year, f<b>2</b>(A, B), and the first derivative f<b>2</b>(A, B), which gives the instantaneous rate of change in the frequency of hits, are all defined as numerical factors generated from patterns #1 and #2. These are the raw numbers that are used in the weighing algorithm.</p>
    <p>For Pattern #2, the total number of possible textual objects c subject to analysis, i.e., TMax, is A1, one only for the years at issue which are those up to the year in which A occurred. However, a relationship may remain open, that is, it may require recalculation of f(x) and f(x) as each new textual object is added to the database <b>54</b>, (for a total of n cases subject to analysis).</p>
    <p>The numerical factors for all eighteen patterns are assigned various weights in a weighing algorithm used to generate a scalar F(A, B). The function F generates a scalar derived from a weighted combination of the factors from all eighteen patterns. The patterns are of course also weighted by importance, allowing Supreme Court full textual objects to impose more influence on the final scalar than District Court full textual objects, for example. The weighing of the more than 100 factors is determined by empirical research to give results closest to what an expert human researcher would achieve. The weighing will vary depending upon the type of material that is being compared and the type of data in the database <b>54</b>. (See Thurstone. The Vectors of Mind, Chicago, Ill.: University of Chicago Press, 1935, for a description of factor loading and manipulating empirical data incorporated herein by reference.) In a commercial Proximity Indexer it will be possible to reset the algorithm to suit various types of databases.</p>
    <p>A scalar F(A, B) is generated for every ordered pair of full cases in the database <b>54</b>, from F(1, 2) to F(n1, n). F(z,z) is defined as equal to 0.</p>
    <p>The full results of F(A,B) are arranged in an (n)(n) matrix designated F. Note that F(B, A) is defined as equal to F(A, B), and arrays that remain empty are designated by 0. For every possible pairing of cases (A,B), a Euclidean distance D(A,B) is calculated by subtracting the Bth row of Matrix F from the Ath row of Matrix F. In other words:</p>
    <p>
      <maths> <formula-text> <i>D</i>(<i>A,B</i>)=[(<i>F</i>(1, <i>A</i>)<i>F</i>(1, <i>B</i>))<sup>2</sup>+(<i>F</i>(2, <i>A</i>)<i>F</i>(2, <i>B</i>))<sup>2</sup>+ . . . +(<i>F</i>(<i>n, A</i>)<i>F</i>(<i>n, B</i>))<sup>2</sup>]<sup></sup>.</formula-text> </maths> </p>
    <p>A function designated D(A,B) generates a scalar for every ordered pair (A,B), and hence for every ordered pair of textual objects (A,B) in the database <b>54</b>. The calculations D(A,B) for every ordered pair from D(1,1) to D(n,n) are then arranged in an (n)(n) proximity matrix D. Every column vector in D represents the relationship between a given case A and every other case in the database <b>54</b>. Comparing the column vectors from column A (representing textual object A) and column B (representing textual object B) allows one to identify their comparative positions in n-dimensional vector space, and generate a coefficient of similarity, S(A,B), from 0-100%, which is more precise and sophisticated than F(A,B) or D(A,B) alone. A similarity subroutine can run directly on F(A,B). However, the real power of the Proximity Matrix D is that it allows one to identify groups or clusters of interrelated cases.</p>
    <p>Through factor loading algorithms, the relationships represented by D for n cases can be re-represented in a vector space containing fewer than n orthogonal vectors. This knowledge can be reflected in S(A,B).</p>
    <p>The Proximity Indexing Application Program <b>62</b> is an application program that applies the above techniques and algorithms to index and format data to be searched by the CSPDM <b>66</b>.</p>
    <p>FIG. 3A describes the overall procedure of the Proximity Indexing Application Program <b>62</b>. The first stage initializes the data <b>74</b> in the database <b>54</b>. The second stage determines the relationships between full textual objects <b>78</b>. The third stage determines the relationships between paragraphs of each textual object and each full textual object <b>80</b>. The fourth stage clusters related paragraphs using factor loading and empirical data and then groups the paragraphs into sections based on such data <b>84</b>. The fifth stage determines the relationships between the sections <b>88</b>. In the final stage, the sectioned textual objects are not further processed until commands are received from the CSPDM Routine <b>92</b>.</p>
    <p>The following description of FIG. <b>3</b>B and FIG. 3C elaborates on this general procedure by describing specific subroutines of a Proximity Indexing Application Program <b>62</b>. The following is a step by step description of the operation of the Proximity Indexing Application Program <b>62</b>.</p>
    <p>Section A Initial Extractor Subroutine <b>96</b> </p>
    <p>FIG. 3B describes subroutines for the first portion of the preferred Proximity Indexing Application Program <b>62</b>. The first subroutine of the Proximity Indexing Applications Program is the Initial Extractor Subroutine <b>96</b>. The Initial Extractor Subroutine <b>96</b> performs three primary functions: Creation of the Opinion Citation Matrix, creation of the Paragraph Citation Matrix, and creation of Boolean Word Index.</p>
    <p>The following steps are performed by the Initial extractor subroutine <b>96</b>.</p>
    <p>1. Number all full textual objects chronologically with arabic numbers from 1 through n.</p>
    <p>2. Number all paragraphs in all the full textual objects using arabic numbers from 1 through p.</p>
    <p>3. Identify the page number upon which each paragraph numbered in step two above begins.</p>
    <p>4. Create Opinion Citation Vectors (X). By comparing each full textual object in the data base to every other full textual object in the data base that occurred earlier in time.</p>
    <p>5. Combine Opinion Citation Vectors to create the bottom left half portion of the nn Opinion citation matrix.</p>
    <p>6. Create a mirror image of the bottom left half portion of the Opinion citation matrix in the top right half portion of the same matrix, to complete the matrix. In this manner only n<sup>2</sup>/2 comparisons need to be conducted. The other  of the comparisons are eliminated.</p>
    <p>7. Create the pn Paragraph Citation Vectors by comparing each paragraph to each full textual object that occurred at an earlier time. This will require (n/2)p searches.</p>
    <p>8. Create a Paragraph Citation Matrix by combining Paragraph Citation Vectors to create the bottom left half portion of the matrix.</p>
    <p>9. Complete the creation of the Paragraph Citation Matrix by copying a mirror image of the bottom left half portion of the matrix into the top right half portion of the matrix.</p>
    <p>10. Initialize the Initial Extractor Subroutine <b>96</b> with a defined set of core English words <b>140</b>.</p>
    <p>11. Assign identification numbers to the core English words <b>140</b>. In the preferred embodiment 50,000 English words are used and they are assigned for identification the numbers from 50,000 to 1.</p>
    <p>12. Create a Boolean Index Matrix <b>144</b> with respect to the core English words by searching the database <b>54</b> for the particular word and assigning the paragraph number of each location of the particular word to each particular word. This procedure is described in greater detail in FIG. <b>3</b>D.</p>
    <p>Section B Opinion Patterner Subroutine <b>100</b> </p>
    <p>The Opinion Patterner Subroutine <b>100</b> performs three primary functions: Pattern analysis 6n matrices, calculation of the numerical factors and weighing the numerical factors to reach resultant numbers.</p>
    <p>13. Process the Opinion Citation Matrix through each of the pattern algorithms described above and in FIG. 6 for each ordered pair of full textual objects to create opinion pattern vectors for each pattern and for each pair of full textual objects. The pattern algorithms determine relationships which exist between the ordered pair of textual objects. The first four pattern algorithms can be run utilizing just the Opinion Citation Vector for the two subject full textual objects. Each pattern algorithm produces a opinion pattern vector as a result. The fifth through eighteenth pattern algorithms require the whole Opinion Citation Matrix to be run through the Opinion Patterner Subroutine <b>100</b>.</p>
    <p>14. Calculate total hits (citation) for each pattern algorithm. This can be done by taking the resultant opinion pattern vector (<u>OPV</u>) and multiplying it by the transposed opinion pattern vector (<u>OPV</u>)<sup>T </sup>to obtain a scalar number representing the total hits.</p>
    <p>15. Calculate the theoretical maximum number of hits. For example, in the second pattern, the theoretical maximum is all of the full textual objects that occur prior in time to case A (A1).</p>
    <p>16. Calculate the actual maximum number of hits. For example, in the second pattern, the actual maximum possible number of hits is the lesser of the number of citations in full textual object Q(A) or full textual object Q(B).</p>
    <p>17. Calculate the total number of hits (citations) per year. This is labeled f(A,B).</p>
    <p>18. Calculate the derivative of the total change in hits per year. This is the rate of change in total hits per year and is labeled f (A,B).</p>
    <p>19. Calculate the ratio of total hits divided by theoretical max [((O<u>FV</u>)(O<u>VP</u>)<sup>t/TMAX</sup>).</p>
    <p>20. Calculate the ratio of the total hits divided by the actual maximum [(O<u>PV</u>)(O<u>PV</u>)<sup>t</sup> <sub>/AMAX</sub>].</p>
    <p>21. Calculate a weighted number F(A,B) which represents the relationship between full textual object A and full textual object B. The weighted number is calculated using the four raw data numbers, two ratios and one derivative calculated above in steps 14 through 20 for each of the 18 patterns. The weighing algorithm uses empirical data or loading factors to calculate the resulting weighted number.</p>
    <p>22. The Opinion Patterner Subroutine <b>100</b> sequence for the Opinion Citation Matrix is repeated n1 times to compare each of the ordered pairs of full textual objects. Therefore, during the process, the program repeats steps 13 through 21, n1 times.</p>
    <p>23. Compile the Opinion Pattern Matrix by entering the appropriate resulting numbers from the weighing algorithm into the appropriate cell locations to form an nn Opinion Pattern Matrix.</p>
    <p>Section C The Opinion Weaver Subroutine <b>104</b> </p>
    <p>The Opinion Weaver Subroutine <b>104</b> shown in FIG. 3B, performs two primary tasks: calculation of the Opinion Proximity Matrix and calculation of the Opinion Similarity Matrix. The Opinion Proximity Matrix D is generated by calculating the Euclidean Distance between each row A and B of the Opinion Pattern Matrix (D(A,B)) for each cell DC(A,B). The Opinion Similarity Matrix is generated by calculating the similarity coefficient from 0 to 100 between each row A and B of the Opinion Proximity Matrix (S(A,B)) in each cell SC(A,B) in matrix S.</p>
    <p>24. Calculate the nn Opinion Proximity Matrix. To calculate D(A,B) the program takes the absolute Euclidian distance between column A and column B of the nn Opinion Pattern Matrix. The formula for calculating such a distance is the square root of the sum of the squares of the distances between the columns in each dimension, or:</p>
    <p>
      <maths> <formula-text> <i>D</i>(<i>A,B</i>)=[(<i>F</i>(1<i>,A</i>)<i>F</i>(1,<i>B</i>))<sup>2</sup>+(<i>F</i>(2,<i>A</i>)<i>F</i>(2,<i>B</i>))<sup>2</sup>+ . . . +(<i>F</i>(<i>N,A</i>)<i>F</i>(<i>N,B</i>))<sup>2</sup>]<sup></sup> </formula-text> </maths> </p>
    <p>The Opinion Proximity Matrix created will be an nn matrix. The smaller the numbers in the Opinion Proximity Matrix the closer the relationship between full textual object A and full textual object B.</p>
    <p>25. Create nn Opinion Similarity Matrix. To calculate the Opinion Similarity Matrix each scalar number in the Opinion Proximity Matrix is processed through a coefficient of similarity subroutine which assigns it a number between 0 and 100. By taking the coefficient of similarity, the program is able to eliminate full textual objects which have Euclidian distances that are great. (For example, a Euclidean distance that is very large and is run through the coefficient of similarity would result in a very low coefficient of similarity. Euclidean distances resulting in similarities below four are eliminated in the preferred embodiment).</p>
    <p>Section D Paragraph Patterner Subroutine <b>108</b> (Optional)</p>
    <p>26. Obtain the pn Paragraph Citation Matrix calculated by the Initial Extractor Subroutine <b>96</b>.</p>
    <p>27. Run each ordered pair of rows of the pn Paragraph Citation Matrix for an individual full textual object i through the pattern algorithms number one and two and determine the resultant Paragraph Pattern Vector.</p>
    <p>28. Calculate the various numerical factors (AMax, TMax, etc.) by evaluating the values in the Paragraph Pattern Vector.</p>
    <p>29. Run the Paragraph Pattern Vector and the numerical factors through the weighing algorithm to determine the appropriate value for each cell of the c<sub>i</sub>n Partial Paragraph Pattern Matrix where c<sub>i </sub>is the number of paragraphs in full textual object i.</p>
    <p>30. Repeat steps 27 through 29 for each full textual object i where i=1 to n, to create the pn Paragraph Pattern Matrix.</p>
    <p>Section E Paragraph Weaver Subroutine <b>112</b> </p>
    <p>31. Calculate the Euclidean distance of each ordered pair of rows of either the pn Paragraph Citation Matrix or the pn Paragraph Pattern Matrix for a single full textual object i.</p>
    <p>32. Place the resultant Euclidean distance values in the appropriate cell of the c<sub>i</sub>c<sub>i </sub>Paragraph Proximity Matrix where c<sub>i </sub>is the number of paragraphs in full textual object i, where 0&lt;i&lt;n+1.</p>
    <p>33. Repeat steps 31 through 32 n times in order to calculate n different Paragraph Proximity Matrices (one for each full textual object i).</p>
    <p>34. The Section Comparison Subroutine <b>116</b> clusters all p paragraphs in the database <b>54</b> into sections. Then the sections are compared and indexed in the database <b>54</b>. This procedure is described in greater detail in FIG. <b>3</b>C.</p>
    <p>FIG. 3C depicts possible subroutines that the Section Comparison Subroutine <b>116</b> comprises. The subroutines are the Sectioner Geographical Subroutine <b>120</b>, the Sectioner Topical Subroutine <b>124</b> (Optional), the Section Extractor Subroutine <b>128</b>, the Section Patterner Subroutine <b>132</b> and the Section Weaver Subroutine <b>136</b>. Section F Sectioner Geographical Subroutine <b>120</b> </p>
    <p>35. For each full textual object i, the Sectioner Geographical Subroutine <b>120</b> uses the corresponding c<sub>i</sub>c<sub>i </sub>Paragraph Proximity Matrix and a contiguity factor for each paragraph to determine which paragraphs may be clustered into sections. Sections are made up of continuous paragraphs that are combined based upon weighing their Euclidean distances and contiguity.</p>
    <p>36. Repeat step 35 for all n full textual objects until all p paragraphs are grouped into q sections.</p>
    <p>Section H Sectioner Topical Subroutine <b>124</b> (Optional)</p>
    <p>37. The Sectioner Topical Subroutine <b>124</b> provides additional assistance to the Sectioner Geographical Subroutine <b>120</b> by considering the factor of topical references to determine the q sections.</p>
    <p>38. For the total number of discrete references z to each full textual object in a particular full textual object, a zz Citation Proximity Matrix is formed by comparing the Euclidean distances between each reference to a full textual object contained in each paragraph and calculating the topical weight given to each paragraph.</p>
    <p>Section I Section Extractor Subroutine <b>128</b> </p>
    <p>39. The Section Extractor Subroutine <b>128</b> numbers each section created by the Sectioner Geographical Subroutine <b>120</b> and Sectioner Topical Subroutine <b>124</b> Subroutines from 1 to q.</p>
    <p>40. The Sectioner Extractor Subroutine <b>128</b> creates a qq Section Citation Matrix by determining which sections refer to every other section.</p>
    <p>Section J Section Patterner Subroutine <b>132</b> (shown in FIG. 3C)</p>
    <p>41. The Section Patterner Subroutine <b>132</b> then calculates 18 Section Pattern Vectors corresponding to each row of the qq Section Citation Matrix using the 18 pattern algorithms.</p>
    <p>42. From the Section Pattern Vectors, the numerical factors (AMax, TMax, etc.) are calculated.</p>
    <p>43. The weighing algorithm evaluates the numerical factors and the Section Pattern Vectors and determines the values for each cell of the qq Section Pattern Matrix.</p>
    <p>Section K Section Weaver Subroutine <b>136</b> </p>
    <p>44. The Section Weaver Subroutine <b>136</b> calculates the Euclidean distances between each row of the qq Section Pattern Matrix and creates a qq Section Proximity Matrix.</p>
    <p>45. The Section Weaver Subroutine <b>136</b> then creates a qq Section Similarity Matrix with coefficients 0 to 100 using the values of the Section Proximity Matrix and empirical data and factor loading.</p>
    <p>Section L Semantical Clustering of a Boolean Index Routine <b>138</b> </p>
    <p>FIG. 3D depicts a possible Semantical Clustering of a Boolean Index Routine <b>138</b>. (See Hartigan, J. A. <i>Clustering Algorithms. </i>New York: John Wiley &amp; Sons, Inc., 1975, for detailed description of clustering algorithms incorporated herein by reference.) The Semantical Clustering routine of a Boolean Index <b>138</b> indexes the textual objects according to the similarity of phrases and words contained within each textual object in a database <b>54</b>. The routine comprises seven possible subroutines: the Initial Extractor Subroutine <b>96</b>, the Pool Patterner Subroutine <b>152</b>, the Pool Weaver Subroutine <b>96</b> the Pool Sectioner Subroutine <b>160</b>, the Section Extractor Subroutine <b>128</b>, the Section Patterner Subroutine <b>132</b> and the Section Weaver Subroutine <b>136</b>, In fact, it is quite possible, using only semantical statistical techniques, to Proximity-index documents that do not refer to one another at all based on there Boolean indices.</p>
    <p>Section M Initial Extractor Subroutine <b>96</b> </p>
    <p>46. As described in steps 10 and 11, the Initial Extractor Subroutine <b>96</b> initializes a set of core English words <b>140</b> and assigns each word a number. The preferred embodiment uses 50,000 discrete core English words and assigns each discrete core English word a number from 50,000 to 1.</p>
    <p>47. The Initial Extractor Subroutine <b>96</b> then converts the core English words into a pw matrix. The number of columns (w) represents the number of discrete core English words in the database <b>54</b> and the number of rows (p) represents the number of paragraphs in the database <b>54</b>.</p>
    <p>48. The Initial Extractor Subroutine <b>96</b> fills the pw matrix by inserting a 1 in the matrix cell where a certain paragraph contains a certain word.</p>
    <p>Section N Pool Patterner Subroutine <b>152</b> </p>
    <p>49. The Pool Patterner Subroutine <b>152</b> creates two pattern algorithm vectors for only the first two patterns and determines values for the total number of hits, the theoretical maximum number of hits, the actual maximum number of hits, the total number of hits per year and the derivative of the total number of hits per year.</p>
    <p>50. The weighing algorithm of the Pool Patterner Subroutine <b>152</b> uses empirical data and factor loading to determine values to enter into a pw Paragraph/Word Pattern Matrix.</p>
    <p>51. The Pool Weaver Subroutine <b>156</b> creates a pw Paragraph/Word Pattern Matrix by filling the appropriate cell of the Matrix with the appropriate value calculated by the weighing algorithm.</p>
    <p>52. The Pool Patterner Subroutine <b>152</b> creates a pw Paragraph/Word Proximity Matrix taking the Euclidean distance between the rows of the Paragraph/Word Pattern Matrix.</p>
    <p>Section O Pool Sectioner Subroutine <b>160</b> </p>
    <p>53. The Pool Sectioner Subroutine <b>160</b> evaluates the Euclidean distances in the Paragraph/Word Proximity Matrix and the contiguity factor of each paragraph to cluster the paragraphs (p) into a group of (v) sections and create a vw Preliminary Cluster Word Matrix.</p>
    <p>Section P Section Extractor Subroutine <b>128</b> </p>
    <p>54. The-Section Extractor Subroutine <b>128</b> numbers each section chronologically and creates a vv Section Word Citation Matrix.</p>
    <p>Section Q Section Patterner Subroutine <b>132</b> </p>
    <p>55. The Section Patterner Subroutine <b>132</b> evaluates the vv Section Word Citation Matrix to create two word pattern vectors for only the first two patterns algorithms (described above and shown in FIG. 6) and determines numerical factors for the total number of hits, the theoretical maximum number of hits, the actual maximum number of hits, the total number of hits per year and the derivative of the total number of hits per year.</p>
    <p>56. The Weighing algorithm uses empirical data and factor loading to weigh the numerical factors created from the word pattern vectors and uses the numerical factors and the word pattern vectors to determine values t o enter into a vv Section Word Pattern Matrix.</p>
    <p>Section R Section Weaver Subroutine <b>136</b> </p>
    <p>57. The Section Weaver Subroutine <b>136</b> creates a vv Section Word Proximity Matrix by taking the Euclidean distance between the rows of the Section Word Pattern Matrix and placing the appropriate Euclidean distance value in the appropriate cell of the Section Word Proximity Matrix.</p>
    <p>58. The Section Weaver Subroutine <b>136</b> create a vv Section Word Similarity Matrix by evaluating the Euclidean distances from the Section Word Proximity Matrix and empirical data, and calculating the similarity coefficient for each order ed pair of sections, and places the value in the appropriate cell of the Section Word Similarity Matrix.</p>
    <p>59. The Pool Searches of the CSPDM <b>66</b> evaluate the Section Word Similarity Matrix as well as other matrices to determine whether or not to retrieve a full textual object.</p>
    <p>The following describes a preferred cluster link generator <b>2044</b> which implements a specific type of patterer or clustering system for use alone or in conjunction with other proximity indexing subroutines, and prior to searching. The cluster link generator <b>2044</b> analyzes a set of numerical representations of a database <b>54</b> and generates a second set of numerical representations of the database <b>54</b>. This second set is stored in the RAM <b>34</b>. This second set of numerical data can represent indirect <b>2036</b>, direct <b>2032</b>, or a combination of both direct <b>2032</b> and indirect <b>2036</b> relationships in the database <b>54</b>. Preferably, the second set of numerical representations accounts for indirect <b>2036</b> relationships in the database <b>54</b>. It is preferred that the first and second set of numerical data be in a table format and that-the first set represent direct <b>2032</b> relationships or links and the second set represent cluster links <b>2004</b>.</p>
    <p>Referring to FIG. 3H, the cluster link generation algorithm <b>2044</b> analyzes links to generate a set of cluster links <b>2004</b>. More specifically, the cluster link generation algorithm <b>2044</b> generates a set of cluster links <b>2004</b> by analyzing direct <b>2032</b> and/or indirect relationships <b>2036</b> between nodes <b>2008</b> or between objects in a database <b>54</b> and generates a set of cluster links <b>2004</b>.</p>
    <p>In the preferred embodiment, the cluster link generator <b>2044</b> analyzes direct links <b>2004</b> (for example source links <b>2004</b> and influence links <b>2004</b>). These direct links <b>2032</b> may be represented by a table or series of vectors. The cluster link generator <b>2044</b> then locates indirect relationships <b>2036</b> between nodes <b>2008</b> or objects in a database <b>54</b>. The indirect relationships <b>2036</b> are preferably made up of direct links <b>2032</b>. The indirect relationship <b>2036</b> paths are preferably made up of direct links <b>2004</b>. The cluster link generator <b>2040</b> then generates a set of cluster links <b>2004</b> based upon both the direct links <b>2032</b> and on the indirect relationships. The set of cluster links <b>2004</b> may be represented by a table or a series of vectors. Another embodiment of this invention uses candidate cluster links <b>2004</b> to provide a more efficient search. Candidate cluster links are the set of all possible cluster links <b>2004</b> between a search node <b>2008</b> and a target node <b>2004</b>. In this embodiment, only a subset of the candidate cluster links <b>2004</b>, the actual cluster links <b>2004</b>, which meet a certain criteria are used to locate nodes <b>2008</b> for display.</p>
    <p>Consider a set of nodes <b>2008</b> N<sub>0 </sub>. . . N<sub>3 </sub>connected by a sequence of direct links <b>2032</b> whose weights <b>2034</b> are given by W<sub>1 </sub>. . . W<sub>3</sub>, as shown in FIGS. <b>3</b>F.</p>
    <p>Node <b>2008</b> N<sub>1 </sub>is reachable from N<sub>0 </sub>through a path P<sub>1 </sub>of length <b>1</b> (that is, N<sub>0</sub>N<sub>1</sub>); node <b>2008</b> N<sub>2 </sub>is reachable through a path P<sub>2 </sub>of length <b>2</b> (N<sub>0</sub>N<sub>1</sub>N<sub>2</sub>); and so on.</p>
    <p>Each path P provides some evidence that the start node <b>2008</b> (N<sub>0</sub>) and destination node <b>2008</b> (N<sub>1</sub>, N<sub>2</sub>, or N<sub>3</sub>) are related to some extent. The strength of the indirect relationship <b>2036</b> depends on a length L of the path P and on the weights <b>2034</b> of the individual direct links <b>2032</b> along that path P.</p>
    <p>In FIG. 3G, the indirect relationship <b>2036</b> from N<sub>0 </sub>to N<sub>1</sub>, N<sub>2</sub>, and N<sub>3 </sub>are shown as arcs.</p>
    <p>The weight C<sub>1 </sub>. . . C<sub>3</sub>of each implied relationship, is a function of the weight <b>2034</b> from the path to the previous node <b>2008</b> and the weight <b>2034</b> of the last direct link <b>2032</b>.</p>
    <p>The individual functions F<b>1</b> . . . F<b>3</b> describe how to combine the weights <b>2034</b> of the direct links <b>2004</b> to determine the weight c of an indirect link <b>2036</b>. Selecting appropriate functions is the key to making cluster link generation work well. A preferred definition of F<sub>N </sub>is as follows:</p>
    <p>
      <maths> <formula-text> <i>C</i> <sub>N</sub> <i>=F</i> <sub>N</sub>(<i>C</i> <sub>N1</sub> <i>, W</i> <sub>N</sub>)=min(<i>C</i> <sub>N1</sub> <i>, D</i> <sub>N</sub> <i>* W</i> <sub>N</sub>),</formula-text> </maths> </p>
    <p>where D<sub>N </sub>is a damping factor that decreases rapidly as N increases.</p>
    <p>The cluster link algorithm <b>2044</b> determines the set of all paths P from a given start node <b>2008</b> N<sub>0 </sub>that have a length less than or equal to a given length L. Each path is rated using the method described above. The paths are then grouped by destination node <b>2008</b>; the candidate cluster link <b>2004</b> C(N<sub>0</sub>, N<sub>N</sub>) between N<sub>0 </sub>and a given destination node <b>2008</b> N<sub>N </sub>has a weight C<sub>N </sub>equal to the sum of the weights <b>2034</b> of all paths P<sub>N </sub>leading to N<sub>N. </sub> </p>
    <p>The set of all candidate cluster links <b>2004</b> is then sorted by weight <b>2034</b>. A subset of the candidate links <b>2004</b> is chosen as actual cluster links <b>2004</b>. The number of cluster links <b>2032</b> chosen may vary, depending on the number of direct links <b>2004</b> from N<sub>0</sub>, and on the total number of candidate cluster links <b>2004</b> available to choose from.</p>
    <p>Performance considerations and efficiency are more important with large databases than for small databases. For large databases, finding the set of all paths P from a given node <b>2008</b> N<sub>0 </sub>that have a length less than or equal to a given length L may be impractical, since the number of unique paths may number in the tens of millions.</p>
    <p>One embodiment of this invention uses candidate cluster links <b>2004</b> to provide a more efficient search. Candidate cluster links <b>2004</b> are the set of all possible cluster links <b>2004</b> between a start node (<b>2008</b>) and a destination node (<b>2008</b>).</p>
    <p>Clearly, it is not necessary to examine millions of paths when the goal is to select the top or strongest duster links <b>2004</b> for each start node <b>2008</b> N<sub>0 </sub>(for example, the top 20 to 25 cluster links <b>2004</b>). The great majority of paths have an insignificant effect on the final results. What is needed is an implementation of the cluster link algorithm <b>2044</b> where the total number of paths examined is bounded, independent of the size of the database <b>54</b>, without a loss in effectiveness. To this end, we have an implementation of the algorithm <b>2044</b> such that a cluster link <b>2004</b> is defined recursively.</p>
    <p>We define C<sub>L </sub>(N<sub>0</sub>, N<sub>N</sub>), the order-L cluster link <b>2004</b> from start node <b>2008</b> to destination node <b>2008</b>, as the cluster link <b>2004</b> between N<sub>0 </sub>and N<sub>N</sub>, considering only paths of length less than or equal to L. Then, we can derive C<sub>L+1 </sub>(N<sub>0</sub>, N<sub>N</sub>) from C<sub>L </sub>(N<sub>0</sub>, N<sub>N</sub>) and C<sub>1 </sub>(N<sub>0</sub>, N<sub>N</sub>).</p>
    <p>The assumption is that most of the paths P<sub>L </sub>(N<sub>0</sub>, N<sub>N</sub>) of length L (or greater) from N<sub>0 </sub>to N<sub>N </sub>will not have a significant impact on cluster link generation. Therefore, we an use a set of candidate duster links <b>2004</b> C<sub>L </sub>(N<sub>0</sub>, N<sub>N</sub>) as a summary of that path information for the purpose of determining C<sub>L+1 </sub>(N<sub>0</sub>, N<sub>N</sub>). This assumption has a significant impact on the performance of the algorithm <b>2044</b> in this implementation, since the search space is significantly reduced at each step. The computer processing cost of generating cluster links <b>2004</b> is bounded by the size of the candidate cluster link <b>2004</b> sets generated at the intermediate steps, rather than by the total number of relevant paths in the database <b>54</b>.</p>
    <p>The size of the candidate duster link <b>2004</b> set generated at each intermediate step affects the speed of the algorithm <b>2044</b> in this implementation. If too many candidate cluster links <b>2004</b> are generated at each intermediate step, the algorithm <b>2044</b> is too slow. On the other hand, if too few candidate cluster links <b>2004</b> are generated, and too many paths are pruned, then C<sub>L </sub>(N<sub>0</sub>, N<sub>N</sub>) is no longer an accurate summary of P<sub>L </sub>(N<sub>0</sub>, N<sub>N</sub>).</p>
    <p>Finally, since the weights <b>2034</b> of the individual candidate cluster links <b>2004</b> in C<sub>L </sub>(N<sub>0</sub>, N<sub>N</sub>) are generally much greater than the weights <b>2034</b> of the individual paths in P<sub>L </sub>(N<sub>0</sub>, N<sub>N</sub>), the damping factors D<sub>N </sub>used to derive the combined weights <b>2034</b> at each step must be decreased accordingly in this implementation.</p>
    <p>The specifics for the basic generator algorithm <b>2044</b> of this implementation, for determining the set of order N cluster links <b>2004</b> from a given start node <b>2008</b> N<sub>0</sub>, are shown in FIG. <b>3</b>H. The generator algorithm <b>2044</b> works for any value of N greater than zero. If N=1, the set of candidate cluster links <b>2004</b> generated is simple. The processing cost of determining the candidate cluster links <b>2004</b> increases with N. In practice, N=3 appears to yield the best results.</p>
    <p>The generator algorithm <b>2044</b> starts by initializing the candidate cluster link <b>2004</b> set <b>2048</b> and creating a loop for i=0 to N <b>2052</b>. The generator algorithm <b>2044</b> then performs a series of steps for each path P <b>2056</b>. First, it selects the destination node <b>2008</b> as the node to analyze and retrieves the set of direct links <b>2032</b> (L) from the selected node <b>2008</b> to any other node <b>2008</b> in the database <b>54</b>, N<sub>i+1</sub>. Second, for each direct link <b>2032</b> L, the generator algorithm <b>2044</b> performs a series of steps.</p>
    <p>The generator algorithm <b>2044</b> creates a new path P of length i+1 consisting of the path P plus the direct link <b>2064</b> L from the selected node <b>2008</b> to the node <b>2008</b> N<sub>i+1 </sub> <b>2056</b>. The algorithm <b>2044</b> then determines the combined weight <b>2034</b> WC<sub>i+1 </sub>from WC<sub>i</sub>, the weight <b>2034</b> of the path P, and W<sub>i+1</sub>, the weight <b>2034</b> of Link <b>2004</b> L <b>2064</b>, using the following preferred formula:</p>
    <p>
      <maths> <formula-text> <i>WC</i> <sub>i+1</sub>=min(<i>WC</i> <sub>i</sub> <i>, D</i> <sub>i+1</sub> <i>*W</i> <sub>i+1</sub>).</formula-text> </maths> </p>
    <p>Following these computations, the generator algorithm <b>2044</b> decides whether there already is a paths P in the cluster link <b>2004</b> from N<sub>0 </sub>to N<sub>i+1 </sub> <b>2068</b>. If there is a not already a path, the algorithm <b>2044</b> adds P to C<sub>i+1 </sub> <b>2072</b>. If there already is a path, the algorithm <b>2044</b> adds WC<sub>i+1 </sub>to the weight <b>2034</b> of the existing path in C<sub>i+1 </sub> <b>2076</b>. These steps are then repeated as necessary.</p>
    <p>Once the candidate cluster link <b>2004</b> set has been generated, deriving the actual cluster links <b>2004</b> is a simple matter of selecting or choosing the T top rated candidate links <b>2004</b>, and eliminating the rest. In practice, the following formula has yielded good results:</p>
    <p>
      <maths> <formula-text> <i>T</i>=min(constant, 4<i>*d</i>),</formula-text> </maths> </p>
    <p>where d is the number of direct links <b>2004</b> from N<sub>0</sub>. Setting the constant equal to twenty has yielded good results. More than T cluster links <b>2004</b> may be generated if there are ties in the ratings. After each iteration, the candidate cluster link <b>2004</b> set C<sub>i </sub>may be pruned so that it contains only the top candidate cluster links <b>2004</b> (for example, the top <b>200</b>).</p>
    <p>FIGS. 4A and 4B are high level flow charts that illustrate the general flow of the subroutines of the CSPDM <b>66</b>. FIG. 4A illustrates that the flow of various search routines depend on the type of search initiated by the researcher. The diagram further illustrates the interaction between the CSPDM <b>66</b> and the GUI Program <b>70</b>. FIG. 4B illustrates the sequence of subroutines in the CSPDM <b>66</b> program and the user interactions with the subroutines. FIG. 4B further shows that the researcher can access the different search subroutines and use information that the researcher has already received to find new information.</p>
    <p>FIG. 4B provides a high level flow chart illustrating the sequence of subroutines in the CSPDM <b>66</b> program and the researcher's interactions with the subroutines. Assuming that the database <b>54</b> the researcher desires to access has been proximity indexed, the researcher must log on <b>260</b> to the database <b>54</b>. By entering the appropriate information into the Computer Processor <b>30</b> via the input means, the researcher electronically accesses <b>264</b> the database <b>54</b> and enables the CSPDM <b>66</b> to search <b>200</b> the database <b>54</b>.</p>
    <p>FIGS. 4A and 4B both show the preliminary options that the researcher can choose from before selecting one of the searching subroutines of the CSPDM <b>66</b>. The CSPDM <b>66</b> questions the researcher on whether the researcher has identified a pool of textual objects <b>204</b>. If the researcher has selected a pool of textual objects <b>204</b>, then the researcher is able to choose one of the pool search <b>208</b> subroutines <b>212</b>. If the researcher has not selected a pool of textual objects, the CSPDM <b>66</b> questions the researcher on whether the researcher has selected a single textual object <b>216</b>. If the researcher has selected a single textual object <b>216</b>, then the researcher is able to choose one <b>220</b> of the textual object searches <b>224</b>. If the researcher has not selected either a pool of textual objects <b>204</b> or a single textual object <b>216</b>, then the researcher must execute a Boolean Word Search or alternate Pool-Generation Method <b>228</b> to retrieve textual objects <b>268</b>, <b>272</b>.</p>
    <p>After CSPDM <b>66</b> subroutine has executed a particular search, the CSPDM <b>66</b> retrieves the appropriate data from the database <b>54</b>, analyzes the data, and sends the data to the GUI Program <b>70</b> in order for the GUI Program <b>70</b> to display the results of the search on the display <b>38</b>.</p>
    <p>FIG. 4B illustrates that after the CSPDM <b>66</b> has completed the above procedure, the researcher has the option to exit the CSPDM <b>66</b> by logging off, executing a search based on the results of a previous search, or executing a new search.</p>
    <p>FIGS. 4A and 4B also depict the seven subroutines of the CSPDM <b>66</b>. There are three textual object search subroutines <b>224</b> and four pool search subroutines <b>212</b>. The three textual object search subroutines <b>224</b> are: the Cases-In Subroutine <b>232</b>, the Cases-After Subroutine <b>236</b> and the Similar Cases Subroutine <b>240</b>. The four pool search subroutines <b>212</b> are the Pool-Similarity Subroutine <b>244</b>, the Pool-Paradigm Subroutine <b>248</b>, the Pool-Importance Subroutine <b>252</b>, and the Pool-Paradigm-Similarity Subroutine <b>256</b>. Each of these subroutines are described in more detail in FIGS. 4C to <b>4</b>I. The following is a step by step description of the subroutines <b>224</b>, <b>212</b> of the CSPDM <b>66</b>.</p>
    <p>Section A Cases-In Subroutine <b>232</b> </p>
    <p>FIG. 4C is a high level flow chart for the Cases-In Subroutine <b>232</b>.</p>
    <p>1. The researcher must select a single textual object <b>400</b>.</p>
    <p>2. The researcher selects the Cases-In Subroutine <b>232</b> option.</p>
    <p>3. The Cases-In Subroutine <b>232</b> examines the nn Opinion Citation Matrix and other matrices <b>404</b> created by the Proximity Indexing Application Program <b>62</b> and retrieves the textual objects to which the selected textual object refers <b>408</b>, data relating to the number of times the selected textual object refers to the retrieved textual objects, data relating to the importance of each textual object, and other relevant data.</p>
    <p>Section B Cases-After Subroutine <b>236</b> </p>
    <p>FIG. 4D is a high level flow chart for the Cases-After Subroutine <b>236</b>.</p>
    <p>4. The researcher must select a single textual object <b>400</b>.</p>
    <p>5. The researcher selects the Cases-After Subroutine <b>236</b> option.</p>
    <p>6. The Cases-After Subroutine <b>236</b> examines the nn Opinion Citation Matrix and other matrices <b>412</b> created by the Proximity Indexing Application Program <b>62</b> and retrieves the textual objects that refer to the selected textual object <b>416</b>, data relating to the number of times the retrieved textual objects refer to the selected textual object, data relating to the importance of each textual object, and other relevant data.</p>
    <p>Section C Similar-Cases Subroutine <b>240</b> </p>
    <p>FIG. 4E is a high level flow chart for the Similar-Cases Subroutine <b>240</b>.</p>
    <p>7. The researcher must select a single textual object <b>400</b>.</p>
    <p>8. The researcher selects the Similar-Cases Subroutine <b>240</b> option,</p>
    <p>9. The Similar-Cases Subroutine examines the qq Section Similarity Matrix and other matrices <b>420</b> created by the Proximity Indexing Application Program <b>62</b> and retrieves the textual objects that are similar to the selected textual object <b>424</b>, data relating to the degree of similarity between the selected textual object and the retrieved textual objects, data relating to the importance of each textual object, and other relevant data. In order to be retrieved, a textual object must have a similarity coefficient with respect to the selected textual object of at least a minimum value. The preferred embodiment sets the minimum similarity coefficient of four percent (4%).</p>
    <p>Section D Pool-Similarity Subroutine <b>244</b> </p>
    <p>FIG. 4F is a high level flow chart for the Pool-Similarity Subroutine <b>244</b>.</p>
    <p>10. The researcher must select a pool of full textual objects <b>428</b>.</p>
    <p>11. The researcher must then select a single full textual object <b>400</b> to which in compare the pool of full textual objects. It should be noted that the researcher can select the single textual object from the selected pool of textual objects, or the researcher can select a textual object from outside of the pool <b>432</b>.</p>
    <p>12. The Pool-Similarity Subroutine <b>244</b> examines the nn Opinion Similarity Matrix and other matrices <b>436</b> and values created by the Proximity Indexing Application Program <b>62</b> for the selected full textual object and the pool of full textual objects.</p>
    <p>13. The Pool-Similarity Subroutine <b>244</b> determines the degree of similarity of other full textual objects in the pool to the selected full textual object <b>440</b>.</p>
    <p>Section E Pool-Paradigm</p>
    <p>FIG. 4G is a high level flow chart for the Pool-Paradigm Subroutine <b>248</b>.</p>
    <p>14. The researcher must select a pool of full textual objects <b>428</b>.</p>
    <p>15. The Pool-Paradigm Subroutine <b>248</b> examines the nn Opinion Proximity Matrix, the nn Opinion Similarity Matrix and other matrices and values created by the Proximity Indexing Application Program <b>62</b> for the pool of full textual objects <b>448</b>.</p>
    <p>16. The Pool-Paradigm Subroutine <b>248</b> determines the Paradigm full textual object by calculating the mean of the Euclidean distances of all the textual objects in the pool <b>452</b>.</p>
    <p>17. The Pool-Paradigm Subroutine <b>248</b> determines the similarity of the other full textual objects in the pool to the Paradigm full textual object <b>456</b>.</p>
    <p>Section F Pool-Importance Subroutine <b>252</b> </p>
    <p>FIG. 4H is a high level flow chart for the Pool-Importance Subroutine <b>252</b>.</p>
    <p>18. The researcher must select a pool of full textual objects <b>428</b>.</p>
    <p>19. The Pool-Importance Subroutine <b>252</b> examines <b>448</b> the nn Opinion Citation Matrix, the nn Opinion Similarity Matrix, numerical factors and other matrices and values created by the Proximity Indexing Application Program <b>62</b> for the pool of full textual objects <b>460</b>.</p>
    <p>20. The Pool-Importance Subroutine <b>252</b> then ranks the importance of each of the full textual objects in the pool <b>464</b>.</p>
    <p>FIG. 4I is a high level flow chart showing two possible alternate Pool-Paradigm-Similarity Subroutines <b>256</b>.</p>
    <p>Section G Pool-Paradigm-Similarity Subroutine <b>256</b> (Option 1) <b>256</b> </p>
    <p>21. The researcher must select a pool of k full textual objects where k equals the number of full textual objects in the pool <b>428</b>.</p>
    <p>22. For each of the k full textual objects, the Pool-Paradigm-Similarity Subroutine <b>256</b> selects a n1 vector from the corresponding column of the nn <b>468</b>.</p>
    <p>23. The Pool-Paradigm-Similarity Subroutine <b>256</b> creates an nk matrix by grouping the n1 vector representing each of the k full textual objects beside each other.</p>
    <p>24. The Pool-Paradigm-Similarity Subroutine <b>256</b> calculates the mean of each row of the nk matrix and enters the mean in the corresponding row of an n1 Paradigm Proximity Vector <b>472</b>.</p>
    <p>25. The Pool-Paradigm-Similarity Subroutine <b>256</b> combines the n1 Paradigm Proximity Vector with the nn Opinion Proximity Matrix to create an (n+1)(n+1) Paradigm Proximity Matrix <b>476</b>.</p>
    <p>26. From the (n+1)(n+1) Paradigm Proximity Matrix, the Pool-Paradigm-Similarity Subroutine <b>256</b> evaluates the Euclidian distances and empirical data to create an (n+1)(n+1) Paradigm Similarity Matrix <b>480</b>.</p>
    <p>27. The Pool-Paradigm Similarity Subroutine <b>256</b> searches the row in the (n+1)(n+1) Paradigm Similarity Matrix that corresponds to the Paradigm full textual object and retrieves the full textual objects that have a maximum degree of similarity with the Paradigm full textual object <b>500</b>.</p>
    <p>Section H Pool-Paradigm Similarity Subroutine <b>256</b> (Option 2)</p>
    <p>28. The researcher must select a pool of k full textual objects where k equals the number of full textual objects in the pool <b>428</b>.</p>
    <p>29. For each of the k full textual objects, the Pool-Paradigm-Similarity Subroutine <b>256</b> selects an n1 vector from the corresponding column of the nn <b>484</b>.</p>
    <p>30. The Pool-Paradigm-Similarity Subroutine <b>256</b> creates an nk matrix by grouping the n1 vector for each of the k full textual objects beside each other.</p>
    <p>31. The Pool-Paradigm-Similarity Subroutine <b>256</b> calculates the mean of each row of the nk matrix and enters the mean in the corresponding row of an n1 Paradigm Pattern Vector PF <b>488</b>.</p>
    <p>32. The Pool-Paradigm-Similarity Subroutine <b>256</b> combines the n1 Paradigm Pattern Vector PF with the nn Opinion Pattern Matrix to create a (n+1)(n+1) Paradigm Pattern Matrix <b>492</b>.</p>
    <p>33. From the (n+1)(n+1) Paradigm Pattern Matrix, the Pool-Paradigm-Similarity Subroutine <b>256</b> evaluates the Euclidean distances between the rows of the Paradigm Pattern Matrix and creates an (n+1)(n+1) Paradigm Proximity Matrix <b>496</b>.</p>
    <p>34. From the (n+1)(n+1) Proximity Matrix, the Pool-Paradigm-Similarity Subroutine <b>256</b> evaluates the Euclidean distances between the rows of the (n1)(n1) Paradigm Proximity Matrix and empirical data to create an (n+1)(n+1) Paradigm Similarity Matrix <b>480</b>.</p>
    <p>35. The Pool-Paradigm Similarity Subroutine <b>256</b> searches the row in the (n+1)(n+1) Paradigm Similarity Matrix that corresponds to the Paradigm full textual object and retrieves the full textual objects that have a minimum degree of similarity with the Paradigm full textual object <b>500</b>.</p>
    <p>Application of the Proximity Indexing Technique</p>
    <p>The above Proximity Indexing Application Program <b>62</b> and CSPDM <b>66</b> have a number of different applications and versions. Three of the most useful applications are described below.</p>
    <p>The first type of Proximity Indexing Application Programs <b>62</b> is for use on very large databases. The matrices generated by this type of Proximity Indexer are attached to the database <b>54</b>, along with certain clustering information, so that the database <b>54</b> can be searched and accessed using the Cases-In Subroutine <b>232</b>, Cases-After Subroutine <b>236</b>, Similar Cases Subroutine <b>240</b>, Pool-Similarity Subroutine <b>244</b>, Pool-Paradigm Subroutine <b>248</b>, Pool-Importance Subroutine <b>252</b> and Pool-Paradigm-Similarity Subroutine <b>256</b> of the CSPDM <b>66</b>.</p>
    <p>The second type of Proximity Indexing Application Program <b>62</b> is a Proximity Indexer that law firms, businesses, government agencies, etc. can use to Proximity Index their own documents in their own databases <b>54</b>. The researcher can navigate through the small business's preexisting database <b>54</b> using the Cases-In Subroutine <b>232</b>, Cases-After Subroutine <b>236</b>, Similar Cases Subroutine <b>240</b>, Pool-Similarity Subroutine <b>244</b>, Pool-Paradigm Subroutine <b>248</b>, Pool-Importance Subroutine <b>252</b> and Pool-Paradigm-Similarity Subroutine <b>256</b> of the CSPDM <b>66</b>. In addition, this type of Proximity Indexer Application Program <b>62</b> will be designed to be compatible with the commercial third-party databases <b>54</b> which are Proximity Indexed using the first type of program. In other words, the researcher in a small business may weave in-house documents into a commercial database <b>54</b> provided by a third party, so that searches in the large database <b>54</b> will automatically bring up any relevant in-house documents, and vice versa.</p>
    <p>The third type of Proximity Indexing Application Program <b>62</b> involves the capacity to do Proximity indexing of shapes. Each image or diagram will be treated as a textual object. The various matrix coefficients can be generated purely from topological analysis of the object itself, or from accompanying textual information about the object, or from a weighted combination of the two. The text is analyzed using the Proximity Indexing Application Program <b>62</b> as explained above. Shapes are analyzed according to a coordinate mapping procedure similar to that used in Optical Character Recognition (OCR). The numerical maps resulting from scanning the images are treated as textual objects that can be compared through an analogous weighing algorithm to generate a proximity matrix for every ordered pair of textual objects in the database <b>54</b>. A similarity matrix can then be generated for each ordered pair, and the results organized analogous to a database <b>54</b> totally comprised of actual text.</p>
    <p>This third type of Proximity Indexing Applications Program <b>62</b> can provide Proximity Indexed organization access to many different types of objects. For example, it can be used to search patent diagrams, or compare line drawings of known pottery to a newly discovered archeological find. It can be used to scan through and compare police composite drawings, while simultaneously scanning for similar partial descriptions of suspects. It can be used to locate diagrams of molecular structures, appraise furniture by comparing a new item to a database <b>54</b> of past sales, identify biological specimens, etc., etc.</p>
    <p>FIG. 5A is a high level drawing that depicts one embodiment of the GUI Program <b>70</b> and its interaction with both the CSPDM <b>66</b> and the display <b>38</b>. The GUI Program <b>70</b> has one or more display subroutines. One embodiment contains seven display subroutines. The seven subroutines comprise three textual object display subroutines <b>504</b> and four pool display subroutines <b>508</b>. The three textual object display subroutines <b>504</b> are the Cases-In Display Subroutine (CIDS) <b>512</b>, the Cases-After Display Subroutine (CADS) <b>516</b> and the Similar-Cases Display Subroutine (SCDS) <b>520</b>. The four pool display subroutines <b>508</b> are the Pool-Similarity Display Subroutine (PSDS) <b>524</b>, the Pool-Paradigm Display Subroutine (PPDS) <b>528</b>, the Pool-Importance Display Subroutine (PIDS) <b>532</b> and the Pool-Paradigm-Similarity Display Subroutine (PPSDS) <b>536</b>. The three textual object display subroutines <b>504</b> receive data from the corresponding textual object search subroutine <b>224</b> of the CSPDM <b>66</b>. Similarly, the four pool display subroutines <b>508</b> receive data from the corresponding pool search subroutine <b>212</b> of the CSPDM <b>66</b>. Once the display subroutines have processed the data received by the search subroutines, the data is sent to the integrator <b>540</b>. The integrator <b>540</b> prepares the data to be displayed in the proper format on the display <b>38</b>.</p>
    <p>FIGS. 5B through 5H depict screens generated by the textual object display subroutines, CIDS <b>512</b>, CADS <b>516</b> and SCDS <b>520</b>. The three types of screens are the Cases In screen <b>1000</b>, the Cases After screen <b>1004</b> and the Similarity Screen <b>1008</b>, respectively. The Similarity Screen <b>1008</b> provides the most intelligent information, but all three screens generated by the textual object display subroutines <b>504</b> work in tandem as a system. The other screens created by the pool display subroutines are variances of these three, and also work in tandem with each other and with the three textual object display screens.</p>
    <p>FIG. 5B depicts the Cases After <b>1004</b> Screen created by the CADS <b>516</b> for the textual object, <i>Terry </i>v. <i>Ohio, </i>392 U.S. 1 (1968). The Cases-After subroutine <b>236</b> search produces all of the textual objects. in the designated field (here D.C. Circuit criminal cases since 1990) that cite <i>Terry. </i>The number 12 <b>1080</b> in the upper left hand corner indicates that there are a total of 12 such textual objects. The vertical axis <b>1012</b> indicates the degree to which a given textual object relied upon <i>Terry. </i>The number 10 immediately below the 12 indicates that the textual object in the field which most relied upon <i>Terry </i>namely <i>U.S. </i>v. <i>Tavolacci, </i>895 F.2d 1423 (D.C. Cir. 1990), discusses or refers to <i>Terry </i>in ten of its paragraphs.</p>
    <p>The Tear-Off Window <b>1016</b> feature is illustrated in FIG. 5B by the Tear-Off Window <b>1016</b> for <i>U.S. </i>V. <i>McCrory, </i>930 F.2d 63 (D.C. Cir. 1991). The four Tear-Off Window active boxes <b>1020</b> (displayed on the Tear-Off Window <b>1016</b>): 1) open up the full text <b>1104</b> of <i>McCrory </i>to the first paragraph that cites <i>Terry; </i>2) run any of the three searches, namely Cases-In Subroutine <b>232</b> Cases-After Subroutine <b>236</b> or similar cases Subroutine <b>240</b> for <i>McCrory </i>itself (the default is to run the same type of search, namely Cases-After Subroutine <b>236</b> again); 3) hide the <i>Terry </i>execute search window <b>1024</b>; and 4) bring the <i>Terry </i>Execute Search window to the foreground, respectively. The weight numeral <b>1028</b> indicates the number of paragraphs in <i>McCrory </i>that discusses or refers to <i>Terry, </i>in this textual object (in this example there is only one).</p>
    <p>The Cases After screen <b>1004</b> for a given Textual object B displays a Textual Object Active Box <b>1032</b> representing every subsequent textual object in the database <b>54</b> that refers explicitly to Textual object B. The analysis starts with the same pool of material as a Shepards list for Textual object B. As well as some additional material not gathered by Shepards. However, the Cases After screen <b>1004</b> conveys a wealth of information not conveyed by a Shepards list.</p>
    <p>The horizontal axis <b>1036</b> may represent time, importance or any other means of measurement to rank the textual objects. The Shepards list itself contains no information as to when a case was decided. The vertical axis <b>1012</b> similarly may represent any means of measurement to rank the textual objects. In the preferred embodiment, the vertical axis <b>1012</b> represents the degree to which the subsequent Textual object C relied upon the original Textual object B. The display <b>38</b> makes it obvious when a textual object has received extensive discussion in another textual object, or provides key precedent for a subsequent textual object, or merely mentions the earlier textual object in passing. It also provides guidance as to possible gradations in between extensive, or merely citing.</p>
    <p>The shape of the overall pattern of active boxes on the Cases After screen <b>1004</b> provides a rich lode of information to be investigated. For example, a dip in citation frequency immediately after a particular textual object suggests that the particular textual object, while not formally overruling Textual object B, has largely superseded it. A sudden surge in citation frequency after a particular Supreme Court case may indicate that the Supreme Court has picked up and adopted the doctrine first enunciated in Textual object B. The researcher can instantly determine if the holding of Textual object B has been adopted in some circuits but not in others, if Textual object B is losing strength as a source of controlling precedent, etc. None of this information is now available to lawyers in graphical or any other form.</p>
    <p>As with the Cases In screen <b>1000</b>, every Textual Object Active Box <b>1032</b> on the Cases After screen <b>1004</b> is active, and includes a Tear-Off Window <b>1016</b> that may be moved by dragging on the tear-off window <b>1016</b> with a mouse <b>42</b>, and that tear-off window <b>1016</b> becomes a text Tear-Off Window <b>1040</b>, visible even when one moves on to other searches and other screens. Thus one may tear off for later examination every relevant citation to Textual object B, or even for a group of textual objects. The text tear-off windows <b>1040</b> tile; that is, they can be stacked on top of one another to take up less room. There is also a Select All feature (not shown), that creates a file containing the citations of every textual object retrieved in a given search.</p>
    <p>In Cases After screen <b>1004</b> mode, clicking on the expanded-view button <b>1044</b> of the text tear-off window <b>1040</b> opens the text of the subsequent Textual object C to the first place where Textual object B is cited. A paragraph window <b>1048</b> displays a paragraph selection box <b>1052</b> indicating what paragraph in Textual object C the researcher is reading, and a total paragraph box <b>1056</b> indication how many paragraphs Textual object C contains in total. The user can view paragraphs sequentially simply by scrolling through them, or see any paragraph immediately by typing its number in the paragraph selection box <b>1052</b>. Clicking on a Next paragraph active box <b>1060</b> immediately takes the researcher to the next paragraph in Textual object C where Textual object B is mentioned. Traditional Shepardizing allows the researcher to explore the subsequent application of a doctrine in a range of different factual situations, situations that help to define the outer contours of the applicability of a rule. Combining the expanded-view button <b>1044</b> functions and Next Paragraph active box <b>1060</b> functions allows the researcher to study how Textual object B has been used in all subsequent textual objects, in a fraction of the time the same task currently requires with available searching methods.</p>
    <p>Perhaps the most fundamental form of legal research is Shepardizing. A researcher starts with a textual object known to be relevant, Textual object B, and locates the Shepards for that textual object. The Shepards is a list of every subsequent textual object that explicitly refers to Textual object B. The researcher then looks at every single textual object on the list. Shepardizing is often painstaking work. Many subsequent references are made in passing and have almost no legal significance. Although Shepards includes some codes next to its long lists of citations, such as f for followed and o for overruled, the experience of most lawyers is that such letters cannot be relied upon. For example, the researcher may be citing Textual object B for a different holding than that recognized by the anonymous Shepards reader, interpreting Textual object B differently, or interpreting the subsequent textual object differently. However, for really thorough research, checking a Shepards type of list is essential. The researcher must make absolutely sure that any textual object cited as legal authority in a brief, for instance, has not been superseded by later changes in the law.</p>
    <p>Very often, textual objects located on the Shepards list for Textual object B refer back to other important textual objects, some of which may predate Textual object B, all of which may be Shepardized in turn. This zig-zag method of research is widely recognized as the only way to be sure that one has considered the full line of textual objects developing and interpreting a doctrine. The real power of the Cases After screen <b>1004</b> emerges when it is used in conjunction with the Cases In screens <b>1000</b> and Similarity screens <b>1008</b>. Using the preferred embodiment, the researcher may engage in the same kind of careful zig-zag study of a legal doctrine in a much more efficient manner.</p>
    <p>For example, consider the following hypothetical search. The researcher reads Textual object B, and makes a list of every Supreme Court textual object it substantially relies upon, perhaps six textual objects. The researcher then Shepardizes Textual object B and reads each of those textual objects, in order to find other Supreme Court textual objects that they relied upon, perhaps eight. One then Shepardizes those fourteen Supreme Court decisions, in order to find any Court of Appeals cases in a selected circuit within the last three years on the same basic topic. This process would take at least an hour, even using Shepards through an on-line service. The same search can be performed with the present invention using the Cases In screens <b>1000</b> and Cases After screens <b>1004</b> in under five minutes.</p>
    <p>In order to perform the same search, a researcher can pull up both the Cases In screens <b>1000</b> and Cases After screens <b>1004</b> for Textual object B simultaneously. The researcher can then tear-off all of the Supreme Court Cases on both lists, run Cases-After Subroutine <b>236</b> searches on every Supreme Court Case mentioned on either list, then examine the Cases In screens <b>1000</b> for all of the Supreme Court cases produced by these searches. The researcher can locate every recent Court of Appeals case from a selected circuit mentioned in any of those Supreme Court cases. Use of the Similarity screen <b>1008</b> as well, allows the researcher to find the pool of relevant Court of Appeals full textual objects even faster.</p>
    <p>FIG. 5C depicts the Cases After Screen <b>1004</b> for <i>U.S. </i>v. <i>Lam Kwong-Wah, </i>924 F.2d 298 (D.C. Cir. 1991). FIG. 5C shows a text Tear-Off Window <b>1040</b> on a Cases After Screen <b>1004</b>, (in this textual object the Tear-Off Window <b>1016</b> for <i>U.S. </i>v. <i>Barry, </i>938 F.2d 1327 (D.C. Cir. 1991), is opened using the full text active box <b>1064</b>. A text Tear-Off Window <b>1040</b> containing the text of <i>Barry </i>opens, to the first cite of <i>U.S. </i>v. <i>Lam Kwong</i>-<i>Wah </i>at paragraph <b>15</b>. Clicking on the Next Paragraph active box <b>1060</b> will open the text of <i>Barry </i>to the next paragraph that cites <i>Lam Kwong</i>-<i>Wah. </i> </p>
    <p>The number 34 in the lower-left corner of the total paragraph box <b>1056</b> indicates that Barry has a total of <b>84</b> paragraphs in the cite <i>U.S. </i>v. <i>Lam Kwong</i>-<i>Wah. </i>Dragging the small squares <b>1068</b> to the left and below the text allow the researcher to move within a paragraph, and from paragraph to paragraph, in the text of <i>Barry, </i>respectively. The empty space below the text <b>1072</b> would contain the text of any footnote in paragraph 15. The compress window active box <b>1074</b> now closes the window and replaces it with the corresponding Textual Object Active Box <b>1032</b>.</p>
    <p>FIG. 5D depicts the Cases In Screen <b>1000</b> for <i>U.S. </i>v. <i>North, </i>910 F.2d 843 (D.C. Cir. 1990). FIG. 5D contains a Textual Object Active Box <b>1032</b> representing every textual object or node with persuasive authority, cited in the text of <i>North. </i>The vertical axis <b>1012</b> represents the degree to which <i>North </i>relied upon a given textual object. In this example it is immediately apparent that <i>Kastigar </i>v. <i>United States </i>406 U.S. 441 (1972) is the most important precedent, and its Tear-Off Window <b>1016</b> have been activated. The weight numeral <b>1028</b> indicates that <i>Kastigar </i>is referred to in 77 paragraphs of <i>North. </i> </p>
    <p>A highlighted Textual Object Active Box <b>1076</b> can be created by clicking on it, as has been done with <i>U.S. </i>v. <i>Lily, </i>651 F.2d 611. The number 212 in the case number box <b>1080</b> indicates that citations to two-hundred-twelve distinct texts appear in <i>North. </i>Fewer are visible because the textual object active boxes <b>1032</b> tile on top of one another; the Zoom feature is used to focus on a smaller area of the screen, and ultimately resolves down to a day-by-day level, making all the textual object active boxes <b>1032</b> visible.</p>
    <p>The unique Cases In screen <b>1000</b> provides a schematic representation of the precedent from which Textual object A is built. The Cases In screen <b>1000</b> contains a textual object active box <b>1032</b> representing every textual object which is relied upon, or even mentioned, in Textual object A. Any citation in textual object A to a textual object that possesses potential persuasive authority, whether a statute, constitutional provision, treatise, scholarly article, Rule of Procedure, etc., is treated as a textual object. The textual object active boxes <b>1032</b> are color-coded to indicate the court or other source of each textual object. Supreme Court cases are red, Court of Appeals cases are green, District Court cases are blue, and statutes are purple, for example. Each Textual Object Active Box <b>1032</b> contains the full official citation <b>1084</b> of its textual object. Clicking on any Textual Object Active Box <b>1032</b> immediately pulls up a larger window, known as a tear-off window <b>1016</b>, also containing the full citation <b>1084</b> to the textual object (Tear-Off Window Citation <b>1088</b>), its date <b>1092</b>, its circuit <b>1096</b>, and its weight numeral <b>1028</b> to the textual object being analyzed. The user may then drag the Tear-Off Window <b>1016</b> free of the Textual Object Active Box <b>1032</b> and release it.</p>
    <p>This creates a text Tear-Off Window <b>1040</b> that remains visible until the researcher chooses to close it, no matter how many subsequent screens the researcher examines. The text Tear-Off Window <b>1040</b> can be moved anywhere by dragging it with the mouse <b>42</b>. The text Tear-Off Window <b>1040</b> contains small text active boxes <b>1100</b> allowing the researcher to access or pull up the full text <b>1104</b> of the textual object it represents with a single click of the mouse <b>42</b>. This feature also allows the researcher to run Cases-In Subroutine <b>232</b>, Cases-After Subroutine <b>236</b>, and Similar Cases Subroutine <b>240</b> searches on the textual object. (See below for a description of the Similarity screen <b>1008</b>).</p>
    <p>The organization of the boxes on the screen, including their position on the horizontal axis <b>1036</b> and vertical axis <b>1012</b>, represents the real intelligence behind the Cases-In screen <b>1000</b>. The horizontal axis <b>1036</b> in the preferred embodiment represents time, with the left margin <b>1108</b> corresponding to the present, i.e., the date 1992 when the search is run. The right margin <b>1112</b> represents the date of decision of the earliest textual object cited in Textual object A. (Certain special materials, such as treatises updated annually, and the U.S. Constitution, are located in a column <b>1116</b> to the left of the margin.)</p>
    <p>The vertical axis <b>1012</b> in the preferred embodiment represents the degree to which Textual object A relied upon each particular textual object it contains. For example, if the Cases In screen <b>1000</b> is run on a district court case (Textual object A) which happens to be a stop and search textual object that mainly relies upon <i>Terry </i>v. <i>Ohio, </i>392 U.S. 1 (1968), <i>Terry </i>will be at the top of the screen, with all other textual object active boxes <b>1032</b> appearing far below. The researcher can thus access the text of <i>Terry </i>directly without ever reading the text of Textual object A. Of course, the full text <b>1104</b> of Textual object A is also instantly available if desired. If the researcher wants to see where <i>Terry </i>came from, the researchers can instantly, by clicking on a text active box <b>1100</b> within the <i>Terry </i>text Tear-Off Window <b>1040</b>, run the Cases-In Subroutine <b>232</b> for <i>Terry</i>and so on. There is no limit to the number of levels or generations the researchers may explore using this technique. It is therefore possible (assuming a sufficient database <b>54</b>) to find, in a matter of seconds, without having to read through layers of texts, the possibly long-forgotten eighteenth-century precursors to a modern doctrine.</p>
    <p>The Cases In screen <b>1000</b> creates an instant visual summary or blueprint of a textual object. The blueprint can help a researcher make a preliminary judgment about whether a particular textual object is worth closer examination. Viewing the Cases In screens <b>1000</b> for a group of textual objects allows a researcher to recognize whether there are precedents common to that group. The blueprint tells the researcher whether Textual object A is primarily a statutory construction case, a textual object that relies on local Court of Appeals cases without Supreme Court support, a textual object relying on precedent outside the circuit <b>1096</b> as persuasive authority, etc.</p>
    <p>The initial Cases In screen <b>1000</b> presents every citation within a given textual object. In a textual object with an unusually large number of citations, the screen will be crowded with textual object active boxes <b>1032</b>. The GUI therefore contains a zoom feature that allows the researcher to expand any small portion of the screen. To get back to the big picture, the researcher simply selects the Fit in Window menu item, or else selects the zoom out feature. The same zoom, zoom out, and Fit in Window functions are present in the Cases After screen <b>1004</b> and Similarity screen <b>1008</b> as well.</p>
    <p>The routine that calculates degree to which Textual object A relies upon the cited textual object clearly ranks major textual objects at the top, textual objects mentioned only in passing at the bottom, and textual objects of potentially greater relevance in between via display the appropriate textual object active boxes <b>1032</b> in the appropriate place. In addition, the routine can recognize when a highly relevant textual object is mentioned only in passing and give a higher weight to that textual object than it would otherwise receive in the ranking procedure.</p>
    <p>The intelligence behind the entire GUI is driven by the knowledge that the lawyers do not want the computer to do legal analysis or make judgments for them, but simply guide them through the great mass of irrelevant material to those texts where lawyerly analysis of a problem begins.</p>
    <p>The Cases In screen <b>1000</b> is designed with practical legal research in mind. It is common in legal research to locate a lower court textual object on the correct topic, call it local Textual object A. However, the researcher desired to find the most persuasive authority available. The aim of this type of research is to find the lead textual object or textual objects on a particular topic. The researcher ultimately desires the first textual object, most famous textual object, and most recent textual objects of the Supreme Court (or state Supreme Court in state law issues) that stand for the same principle. (Lead textual objects also occur at the intermediate and trial court level.)</p>
    <p>The standard way to find lead textual objects is to read through the text of a local Textual object A until one finds references to higher court textual objects, then look up each of those higher court textual objects in turn. The researcher then reads the text of those textual objects until the researcher determines the textual objects they have in common, the textual objects that appear many times. Very often, the lower court textual object from which the researcher started is of no real value in and of itselfit may well be from a different local jurisdictionand the researcher reads through it only to find citations within it. Since the GUI quickly locates and schematically diagrams the textual objects, this process is accelerated dramatically using the GUI.</p>
    <p>FIGS. 5E through 5G depict multiple Similar Case Subroutine <b>240</b> searches run in sequence. A Similarity Screen <b>1008</b> for <i>U.S. </i>v. <i>Caballero </i>936 F.2d 1292 (D.C. Cir. 1991), reveals via the case number box <b>1080</b>, that 17 textual objects were retrieved by Similar Cases Subroutine <b>240</b> search. The vertical axis <b>1012</b> indicates that the textual objects retrieved had similarity coefficients <b>1120</b> between 4% and 15% with respect to <i>U.S. </i>v. <i>Caballero. </i>Textual objects with less than 4% similarity are not shown. The vertical axis <b>1012</b> represents degree of similarity, or topical relatedness, so that 100% would be two identical texts. The Tear-Off Window <b>1016</b> of <i>U.S. </i>v. <i>Nurse, </i>916 F.2d 20 (D.C. Cir. 1990) shows that the textual object has a similarity of 9%.</p>
    <p>The Similarity screen for a given Textual object C is organized like the Cases In screen <b>1000</b> and Cases After screen <b>1004</b>, with the same color-coded textual object active boxes representing textual objects, and time on the horizontal axis <b>1036</b>. However, the vertical axis <b>1012</b> represents the degree to which the represented textual object is related to Textual object C. The system is built on the principle that legal doctrines tend to emerge out of lines of textual objects developing a legal principle. Lines of textual objects contain lead textual objects that establish basic rules and subsequent textual objects that do not establish new rules, but apply and re-interpret the pre-existing rules in various circumstances. Some lead textual objects invent new doctrines, while others modify or redirect the law based on earlier precedent.</p>
    <p>The routine that operates behind the Similarity screen <b>1008</b> determines which line or lines of textual objects that Textual object C can be grouped. The routine then ranks the textual objects in that line depending on how closely they are related to Textual object C. For example, a typical similarity search starting with a Court of Appeals case in a certain circuit, Textual object D, will find the Supreme Court and Court of Appeals cases that have established the principles followed in Textual object D. The Supreme Court and Court of Appeals case will appear as textual object active boxes whether or not they are cited in Textual object D. Furthermore, the Similar Cases Subroutine <b>240</b> search will find the textual objects decided subsequent to Textual object D that have applied, and possibly modified, those principles, whether or not those textual objects cite Textual object D.</p>
    <p>Similarity searches allow a researcher to find textual objects on the same topic that do not share common phrases and might be overlooked by a Boolean word search. Similarity searches also allow researchers, who only have an obscure district court case, to tap in to the lead textual objects in any area. By organizing all case law in conceptual space, the Similarity screens <b>1008</b> allow one to locate emerging topics that have not been formally recognized by those assigning key numbers or otherwise manually classifying textual objectsor even by the authors of the textual objects themselves.</p>
    <p>The shape of a Similarity Screen <b>1008</b> may convey a great deal of information about a particular legal concept. For example, the screen conveys to the researcher whether a certain concept, which is essentially novel, is supported by Supreme Court case law. Or is an old doctrine that has been recently applied in a new context. The system as a whole gives lawyers the ability to assess what textual objects are available on their topic, and to zero in on the textual objects that are most useful. The researcher has the ability to track down every subsequent reference to any particular textual objects by utilizing multiple Cases After searches, identifying core precedents through Cases In searches, and by running new Similarity searches to obtain any textual objects that emerge in closely related topic areas. The Similarity algorithm is more aggressive then the others, since it contains built-in judgments as to what relatedness means. It also judges what is no longer sufficient to display on the screen. The bottom edge of the screen represents a minimum degree of similarity below which the connections are too tenuous to be worth pursuing. In the commercial product, this minimum level can be reset at the preference of the user.</p>
    <p>FIG. 5F is the Similarity Screen <b>1008</b> for <i>U.S. </i>v. <i>Nurse. </i>Clicking on the run search Tear-Off Window active box <b>1128</b>, which is on the Tear-Off Window <b>1016</b> for Nurse produces FIG. <b>5</b>F. Clicking on the Textual Object Active Box <b>1032</b> for <i>U.S. </i>v. <i>Jordan, </i>951 F.2d 1278 (D.C. Cir. 1991) long enough to pull up its Tear-Off Window <b>1016</b>, and then clicking on <i>Jordan's </i>run search Tear-Off Window active box <b>1020</b> (not shown), produces the Similarity Screen <b>1008</b> shown in FIG. <b>5</b>G.</p>
    <p>FIG. 5G shows how multiple tear-off windows <b>1016</b> can be shown at the same time, here the <i>U.S. </i>v. <i>Jordan </i>similarity Tear-Off Window <b>1016</b> depicts for the three textual objects most similar to <i>Jordan. </i>Note that <i>U.S. </i>v. <i>Jordan. </i>958 F.2d 1085 (D.C. Cir. 1992), is very closely related, i.e., 41%, to <i>U.S. </i>v. <i>Jordan, </i>951 F.2d 1278 (D.C. Cir. 1991), apparently as it is a subsequent full textual object decision of the same dispute as the first textual object.</p>
    <p>FIG. 5H depicts a close-up view of an Execute Search Window <b>1024</b>. The researcher can input a selected textual object that is either represented or not represented on a display <b>38</b> screen as a Textual Object Active Box <b>1032</b>. The researcher can title his search by inputing the title in the Title Search box <b>1132</b>. The researcher can then input the reference to the selected textual object in the reference input boxes <b>1136</b>. The reference input boxes of the preferred embodiment allow the researcher to refer to the selected textual object by Volume, Category, Page and/or Section by inputing the appropriate values in the volume reference box, category reference box, page reference box, and/or section reference box, respectively.</p>
    <p>The researcher can also identify the type of search to be performed on the selected textual object by selecting the appropriate search in the Analysis box.</p>
    <p>Once the researcher has inputed all the appropriate values, the researcher executes the search by activating the execute search button.</p>
    <p>Referring generally to FIGS. 5A through 5H, the PSDS <b>524</b>, PPDS <b>528</b>, PIDS <b>532</b> and PPSDS <b>536</b> of the GUI Program <b>70</b>, also create similar displays to the CIDS <b>512</b>, CADS <b>516</b>, and SCDS <b>520</b> subroutines. The only major difference between the screens created by the three textual object display subroutines and the four pool display subroutines is the information contained in the Execute Search window and the options available in the analysis box.</p>
    <p>The options in the analysis box enable a researcher to select a textual object outside the pool of textual objects and compare how the selected textual object relates to the pool of textual objects by selecting to the Pool-Similarity Subroutine <b>244</b>, the Pool-Paradigm Subroutine <b>248</b> or Pool-Importance Subroutine <b>252</b> of the CSPDM <b>66</b>.</p>
    <p>The PSDS <b>524</b> creates a Pool-Similarity Screen <b>1008</b>. The vertical axis <b>1012</b> ranks the similarity of the objects in a pool of textual objects with respect to a selected textual object. All of the other aspects of this display <b>38</b> are similar to the Similar Cases Screen.</p>
    <p>PPDS <b>528</b> creates a Pool-Paradigm Screen. The vertical axis <b>1012</b> ranks the similarity of the pool of textual objects on the screen with respect to the paradigm textual object. The paradigm textual object is calculated by averaging the mean of all the Euclidean distances of the pool of textual objects on the screen. All of the other aspects of this display <b>38</b> are similar to the Similar-Cases Screen.</p>
    <p>The PIDS <b>532</b> creates a Pool-Importance Screen. The vertical axis <b>1012</b> ranks the importance of the pool of textual objects on the screen. All other aspects of the PIDS <b>532</b> display <b>38</b> are similar to the Cases-In Screen <b>1000</b> and Cases-After Screen <b>1004</b>.</p>
    <p>The PPSDS <b>536</b> creates a Pool-Paradigm Similarity Screen <b>1008</b>. The vertical axis <b>1012</b> represents the similarity of all textual objects in the database <b>54</b> to the paradigm textual object created by a selected pool of textual objects. All other aspects of the PPSDS <b>536</b> display <b>38</b> are similar to Similar-Cases Screen <b>1008</b>.</p>
    <p>Before displaying the text boxes <b>1032</b> representing result nodes <b>2104</b> on the screen to the user, the graphical user interface program <b>70</b> optimally organizes and arranges the location of text boxes <b>1032</b> on the X and Y axis. In the preferred embodiment, the GUI Program, <b>70</b> uses a layout of boxes algorithm to optimally place boxes within a window.</p>
    <p>Referring to FIG. 7, generally, a layout algorithm plots text boxes <b>1032</b> on a cartesian axis as determined by their X and Y values <b>1200</b>. The algorithm compares the locations of boxes <b>1032</b> within a display window to determine if there are any overlapping boxes <b>1204</b>. In order to perform this comparison, the preferred algorithm initializes a first loop, for i=0 to N, and chooses box<sub>i </sub>to begin the comparison. The algorithm next creates a second loop, for j=1 to N, and chooses box<sub>j </sub>to compare with box<sub>i</sub>. For both loops, N is the number of result nodes obtained. Performing N<sup>2 </sup>comparisons provides the optimal number of comparisons needed to determine the existence of any overlaps. If the boxes <b>1032</b> are a known size, certain steps may be eliminated from the method.</p>
    <p>More particularly, a preferred algorithm compares the X and Y values of a first and second box <b>1032</b> to determine if the boxes <b>1032</b> are occupying the same Cartesian space <b>1204</b>. This comparison is accomplished by identifying the X and Y coordinate pairs of the corners of the two boxes <b>1032</b>, and then choosing one of the coordinate pairs of a corner of the second box <b>1032</b> to be compared. The X value of that pair is compared to the X values of all of the coordinate pairs of the corners of the first box <b>1032</b>. If the X value of the chosen coordinate pair is a value less than all of the first box corner X values or a value greater than all of the first box corner X values, then the algorithm compares the Y value of the chosen pair to all of the first box corner Y values. If the Y value of the chosen pair is either a value less than all of the first box corner Y values or a value greater than all of the first box corner Y values of the first box <b>1032</b>, then the algorithm determines that the boxes <b>1032</b> do not overlap. The algorithm adds 1 to counter j and then repeats the routine. The routine is repeated until j reaches N and then 1 is added to the i value, and the entire process is repeated again. This particular method ensures that every box is compared with every other box <b>1032</b>.</p>
    <p>If during the comparison of the X values the algorithm finds that the X value of the chosen pair is greater than one of the first box corner X values, but is less than one of the first box corner X values, then the algorithm determines that the boxes <b>1032</b> overlap. If during the comparison of the Y values the algorithm finds that the Y value of the chosen pair is greater than one of the first box corner Y values but is less than one of the first box corner Y values, then the algorithm determines that the boxes <b>1032</b> overlap.</p>
    <p>If the preferred algorithm has determined that two boxes <b>1032</b> overlap, then the algorithm moves <b>1208</b> one of the boxes <b>1032</b>. Preferably, this is accomplished by adjusting the Y values of the second box <b>1032</b> by increasing or adding a predetermined value (to its Y values.) The algorithm performs the above comparison routine <b>1204</b> again to see if there is an overlap. If there is an overlap, it moves <b>1208</b> the box again. Preferably, it adjusts <b>1208</b> the second box's Y value again, and compares <b>1204</b> again. If there is no overlap, then the algorithm adds 1 to the j counter and repeats the comparison routine with another box <b>1032</b> until N<sup>2 </sup>comparisons have been completed.</p>
    <p>When the preferred layout algorithm has ensured that no boxes <b>1032</b> overlaps, the algorithm determines whether the results of the search win fit on one screen <b>1212</b>. The algorithm compares the Y values of each of the boxes <b>1032</b> with the highest Y value represented on a single screen display. If the Y value of one or more boxes <b>1032</b> exceeds the highest Y value represented on the screen display, then the preferred algorithm increases the length of the X axis and rescales the Y axis to match (e.g., doubling the length of the X axis) <b>1216</b>. The algorithm again compares the Y values of each of the boxes <b>1032</b> to the highest Y value on the screen to determine if the search results will fit on one screen <b>1212</b>. If they will not, then the algorithm adjusts the X axis <b>1216</b> again and compares the Y values <b>1212</b> again until the search results fit on one screen.</p>
    <p>Once the search results fit on one screen, the algorithm replots <b>1200</b> all of the boxes <b>1032</b> to their coordinate positions, and then performs the overlap comparison check <b>1204</b> again to see if any boxes <b>1032</b> are overlapping. If boxes <b>1032</b> are overlapping, the algorithm performs its adjustment step <b>1208</b> and the axis resizing step <b>1216</b> until the window displays <b>1220</b> all of the result nodes on a single screen without any of the boxes overlapping.</p>
    <p>At the option of the user, the algorithm can allow the display <b>1220</b> to scroll off the screen in the Y direction or the X direction without resizing <b>1216</b> the axes. This option enhances the information content of the map by keeping the scale of the axes small.</p>
    <p>The preferred algorithm can perform this routine by adjusting <b>1208</b> the X axis, the Y axis, or both axes. The algorithm has the additional capability of graphically breaking an axis, if one or a few result nodes <b>2104</b> are so far away graphically from the main body of result nodes <b>2104</b> that representing the far away result nodes <b>2104</b> would unnecessarily encumber the graphical display of the main nodes. This graphical break may be represented by a squiggly line at the break point in the axis. Using this axis break allows all of the result nodes <b>2104</b> to be displayed on one window, and still maximizes the informational content that the relative spacing on the X or Y axis provides for the result nodes <b>2104</b> which are positioned closer together.</p>
    <p>Various other specific methods of optimally organizing and locating boxes <b>1032</b> on a graphical computer display <b>38</b> may be used with the GUI Program <b>70</b>.</p>
    <p>In the preferred embodiment, the graphical user interface <b>70</b> maximizes the types and quantity of information about particular boxes <b>1032</b>, nodes <b>2008</b>, objects in the database <b>54</b> that can be displayed without visually overloading the user. The preferred embodiments ergonomically and efficiently represent complex data sets. Each embodiment must strike a balance between that which is technically and intellectually possible to be displayed on a screen, and that which can be visually understood and comprehended by the typical user of the database <b>54</b> (on a screen). This method can also be used to display objects retrieved from a network, but it is not the preferred method at the present time.</p>
    <p>An important feature of the invention is its use of a three-dimensional box to communicate information to the user, in addition to the information provided by the location of the box in the X and Y coordinates. Referring to FIG. 8, in the preferred embodiment a three coordinate view or map is displayed on a two dimensional CRT screen. The variables represented by the X, Y, and Z coordinate planes may be interchanged from one coordinate to another. In other words, the X, Y or Z coordinate plane may represent, for example, the variable time. For use of the Z coordinate, it is preferred that a six-sided box <b>1033</b> be used and appear to be floating at its appropriate location in the Z direction. More importantly, the invention also can use the depth of the box <b>1032</b> (size of the box <b>1032</b> in the Z direction) to convey additional information to the user (in addition to the information provided by the location of the box <b>1032</b> in the X, Y, Z coordinate).</p>
    <p>First, using box depth, a bit of binary information is passed along to the user by the fact that the box <b>1033</b> has no depth (little or nominal depth) or the box <b>1033</b> has a significant depth in the Z direction. In the preferred embodiment, this binary piece of information informs the user of whether or not there is available (hidden) data associated with that box <b>1033</b>. For example, a box <b>1033</b> or node which represents an object in the database <b>54</b> may have associated graphics, maps, menus, or text which is not shown. If the box <b>1033</b> is shown on the screen as having a significant depth then additional data associated with that box <b>1033</b> is available for viewing by the user. If the box <b>1033</b> has nominal or no depth then there is no additional data available to the user.</p>
    <p>In addition to the binary information of whether or not additional data is available to the user, in the preferred embodiment the magnitude of the depth of the box <b>1033</b> corresponds to the amount of additional or hidden data available to the user. For example, if the box <b>1033</b> represents an object in the database <b>54</b> which has an extensive amount of associated data the magnitude of the depth of the box <b>1033</b> would be large in comparison to other boxes <b>1033</b> on the same screen. In this manner, a box <b>1033</b> which represents, for example, a textual object of great length would have a larger depth than a box <b>1033</b> representing a textual object with little or no text associated with that object in the data base. In this manner, important information is visually passed to the user easily and on the same screen on which other information about the database <b>54</b> is being presented.</p>
    <p>Also, in advanced embodiments, the depth of a box <b>1033</b> or the fact that a box <b>1033</b> has depth may be used to represent to the user that the box <b>1033</b> enables the user to tie-in or access another application, program, menu, extension, and/or another database <b>54</b>. In this way, active boxes <b>1033</b> with depth can allow a user great flexibility to move around within the database <b>54</b> or within associated database <b>54</b>s or even to access other applications. This can be particularly useful when the underlying data supporting a node or box <b>1032</b> is not located locally at the user's location and requires the user to access communication links or a second database <b>54</b> in order to obtain the underlying data. With this invention, the user is able to access the underlying data from the graphical user interface screen.</p>
    <p>Also in advanced embodiments, an axis may represent a variable such as cost data associated with a node <b>2008</b> or the cost of accessing the underlying data. In one example, if the data is available only through a separate application which may impose a cost, the box's depth would increase in proportion to that cost Or, if the application itself imposes a cost for accessing data, then the depth of each box <b>1033</b> would represent the cost of accessing that box's data.</p>
    <p>In summary, the depth of a box <b>1033</b> provides two types of information. First, binary-type information regarding the presence of additional data or information, or lack thereof, and second, based on the relative measure of the depth of the box, <b>1033</b> the amount or size of the underlying data or information which is available and associated with that box <b>1033</b>. Thus, a box <b>1033</b> can be activated and brought to life so that there is an extension that points to either data or another entity independent of the original database <b>54</b> which assist the GUI <b>70</b> user.</p>
    <p>Additional information concerning the database is presented by this invention by the intelligent use of comments. Comments attached to the textual object boxes provide the user with easy access to vital information contained in the database. FIG. 3<i>e </i>shows the various information types which can be added to the database <b>54</b>. For Example, FIG. 3<i>e </i>shows that links <b>2004</b> are assigned weights <b>2032</b>, that nodes <b>2008</b> are assigned node identifications (IDs) <b>2010</b> and plot dates <b>2011</b> (creation date or the like), that link sub-types <b>2020</b> can be assigned names <b>2021</b>, comment descriptors <b>2022</b>, comment display orders <b>2023</b>, comment place holders <b>2027</b> and always display comment commands <b>2030</b>, that node sub-types <b>2024</b> can be assigned names <b>2021</b> and title descriptors <b>2026</b>, that node types <b>2016</b> can also be assigned names <b>2021</b> as well as extra attributes in an extra-attributes table <b>2016</b>, that link types <b>2012</b> can be assigned names <b>2021</b> and icon files <b>2014</b> for icon graphics and various visual styles <b>2028</b> can be assigned to nodes <b>2008</b> and links <b>2004</b>. In addition to those items specifically described, various attributes can be assigned to links <b>2004</b>, nodes <b>2008</b> and link sub-types <b>2020</b> and node sub-types <b>2024</b>. The various additional information which is stored in the database <b>54</b> can be shown on maps or on menus when using the database <b>54</b>. These identifications can be used as part of the searching algorithms discussed previously.</p>
    <p>A unique feature of the graphical user interface program <b>70</b> is its ability to optimally space the information within displayed objects. More particularly, the GUI program <b>70</b> arranges text and graphics within boxes <b>1032</b> or the like on a computer display <b>38</b> screen. The preferred GUI Program achieves this by using a box spacing algorithm as shown in FIG. 9. A preferred box spacing algorithm is described below.</p>
    <p>The boxes <b>1032</b> used by the preferred GUI Program <b>70</b> generally include different types of information or data such as box titles, textual information, and graphical information within the box <b>1032</b>, as discussed previously. The information types may be assigned to nodes <b>2008</b>, node sub-types <b>2024</b>, links <b>2004</b>, or link sub-types <b>2020</b>. Preferably, the GUI <b>70</b> defines and/or selects points in the box <b>1032</b> to serve as anchor points <b>2200</b> for each type of information. For example, the GUI <b>70</b> may designate a point <b>2200</b> near the upper right hand corner of a box <b>1032</b> as the anchor point for the graphical information, the lower left hand corner as the anchor for the textual information, and the upper left hand corner as the anchor point for the box title. In the preferred embodiment, the algorithm finds an arrangement which keeps the size of the boxes as small as possible while preventing overlaps between the different types of information.</p>
    <p>Also, the preferred embodiment adjusts the positioning <b>2212</b> of the information or data within the box <b>1032</b> to make the box <b>1032</b> aesthetically pleasing. Preferably, the anchor points are moved or adjusted to arrange or rearrange the content within the box.</p>
    <p>Referring to FIG. 9, generally, the box spacing algorithm plots <b>2204</b> the information types at their designated anchor points and determines whether the plotted information fits within the default box size <b>2208</b>. If necessary, the box <b>1032</b> is resized <b>2212</b>. Following, the algorithm checks for any overlap of information types within the box <b>2216</b> and adjusts the location of anchor points <b>2212</b>, if necessary.</p>
    <p>The overlap checking function performed by the preferred box spacing algorithm is similar to the overlap checking function performed by the preferred layout algorithm discussed above. Various tolerances or thresholds may be set to ensure that the information or data within the box <b>1032</b> not only does not overlap, but is sufficiently spaced so that it can be easily understood by a user. If the information overlaps, the anchor points are moved and/or, the boxes are reshaped. Finally, the processed boxes are displayed <b>2220</b>.</p>
    <p>More particularly, to perform the function of arranging anchor points and reshaping boxes, the preferred box spacing algorithm initializes a loop, for i=0 to N, where N is the number of information types to be displayed on the box; chooses information type<sub>i </sub>and then initialize a second loop, for j=1 to N; and chooses information type<sub>j </sub>to compare to information type<sub>i</sub>.</p>
    <p>The preferred algorithm plots <b>2204</b> the first information type on the box <b>1032</b> at its designated anchor point. The algorithm plots <b>2204</b> the information beginning at the anchor point and fills out horizontally or vertically from there until the information is plotted. After plotting, the algorithm determines if the information fits within a normal or default box size <b>2208</b>. If the box is too small, the algorithm may adjust <b>2212</b> the box <b>1032</b> dimensions horizontally or vertically to accommodate the size of the information.</p>
    <p>The algorithm then plots <b>2204</b> the second type information in the box <b>1032</b>. After this information is plotted, the algorithm determines X and Y values of the first information type (using the left and lower edges of the main box as coordinate axes) and compares <b>2216</b> them with the X and Y values of the second information type. The box spacing algorithm performs this function in the same manner as the layout algorithm performs its comparison function.</p>
    <p>If there is an overlap, then the algorithm preferably attempts to adjust the location of anchor points <b>2212</b> to eliminate overlap. If this is not possible, the algorithm adjusts the size <b>2212</b> of the box by a set value in either the X or Y direction. The algorithm re-plots the information types <b>2204</b> at their anchor points. Preferably, the anchor points generally remain in the same relative position in the box, but as the box increases in size, the anchor points are in an absolute sense farther away from each other. After adjustments, the algorithm runs the comparison routine <b>2216</b> again to determine if the two information types overlap or are aesthetically displeasing. A box <b>1032</b> may be aesthetically displeasing if the data within the box <b>1032</b> is not evenly or symmetrically distributed, or if data is too close.</p>
    <p>If the information types are appropriately spaced within the box, the algorithm adds 1 to the j counter, and compares <b>2216</b> the first information type to the j+1 information type. The algorithm continues to compare <b>2216</b> information types until the first information type has been compared <b>2216</b> with all of the information types in the box <b>1032</b>. Then, the algorithm routine returns to the i loop, adds 1 to i, and then compares <b>2216</b> the second information type to the other information types until the second has been compared <b>2216</b> to all of the information types. If the algorithm ever finds an overlap, the algorithm adjusts <b>2212</b> the location of the anchor points and/or the size of the box as described earlier to fit in all of the information. Once the algorithm has compared the information types <b>2216</b>, found no overlap, and found that the information fits <b>2208</b> within the box <b>1032</b>, it displays <b>2220</b> the box <b>1032</b>. In this way, the graphical user interface program <b>70</b> ensures that the information types displayed by the boxes <b>1032</b> do not overlap and are aesthetically pleasing, while keeping the size of the box <b>1032</b> to a minimum.</p>
    <p>Many box spacing algorithms may be used with the GUI <b>70</b>. Many variations of the described algorithm are possible which will perform the function of spacing text and/or graphics within a circumscribed space on a display <b>38</b>. In the preferred embodiment, as shown in FIGS. 10A and 10B, comments <b>2112</b> are used extensively on graphical displays to assist the user in understanding the data and relationships of the data the user is viewing. In this manner, a great deal of information about a node <b>2008</b> or a link <b>2004</b> can be placed in or around a graphical box <b>1032</b> display for the node <b>2008</b>. With this information a user has a better understanding of the relationship between data and the database <b>54</b> and the graphical box <b>1032</b> represents more than just a location in the X, Y and/or Z coordinate plane.</p>
    <p>The comment descriptor <b>2022</b> shown on FIG. 3<i>e </i>allows comments <b>2112</b> to be assigned to a particular link sub-type <b>2020</b> and for these comments <b>2112</b> to be displayed on the node box <b>1032</b> of a linked node <b>2008</b>. It is preferred that this comment descriptor <b>2022</b> assigned to a link sub-type <b>2020</b> be placed on the to-node <b>2008</b> of the link <b>2004</b>. Some examples of possible comment descriptors are overruled by, criticizes, distinguishes. When a node box <b>1032</b> is displayed, these comment descriptors may be shown in any portion of the node box <b>1032</b>. In a preferred embodiment, the node box <b>1032</b> is subdivided into three parts: (1) a title place holder part; (2) a graphics place holder part; and (3) an indicator part.</p>
    <p>To specify the specific place within the node box <b>1032</b> that the comment <b>2112</b> will be displayed, a comment place holder <b>2027</b>, which is a more specific type of the anchor point discussed previously. In the preferred embodiment, a comment place holder <b>2027</b>, may specify three different place holder <b>2027</b> areas (title area, indicator area, or graphics area) in the node box <b>1032</b> in which the comment is to be displayed. Various other place holder <b>2027</b> options within or in the vicinity of a box are possible.</p>
    <p>Also, using the commands available through the comment display order <b>2023</b> or the always display comment commands <b>2030</b>, the user or designer of the database <b>54</b> may specify when particular comments <b>2112</b> will or will not be displayed and in what position the comments <b>2112</b> will be displayed. In the preferred embodiment, the always display comment <b>2030</b> is used to make a comment <b>2112</b> always available or globally available at any time it is relevant. In other words, the comment will be displayed whenever the to-node box <b>1032</b> is drawn on any map. It is preferred that this global comment be used whenever a comment is so important that it should be shown whenever relevant.</p>
    <p>The comment display order <b>2023</b> specifies the order or preference in which to display multiple comments <b>2116</b> in one comment place holder <b>2027</b>. In the preferred embodiment, a number in the range of zero (0) to two hundred fifty-five (255) is assigned as the priority of any specific comment <b>2112</b>. Wherein zero (0) signifies that the comment <b>2112</b> has high priority and should be displayed at the top of the title or indicator place holder or on the left in the graphics place holder while a value of two hundred fifty-five (255) means that the comment <b>2112</b> has very low priority and should be located at the bottom of the title or indicator place holder <b>2027</b> or on the right in the graphics place holder.</p>
    <p>The always display comment <b>2030</b> can be simply a binary value of zero (0) or one (1) wherein if the value is zero (0) the comment <b>2112</b> is only displayed on the to-node <b>2008</b> when a link <b>2004</b> of the specified link type <b>2012</b> is represented on a map and the from-node <b>2008</b> is also on the map. A one (1) means that the comment <b>2112</b> is displayed on the to-node <b>2008</b> at all times whether or not the from-node <b>2008</b> appears on the map.</p>
    <p>Comments <b>2112</b> may be active or inactive. Active comments <b>2112</b> provide another means for a user to navigate in the database <b>54</b> in a customized and flexible manner. Active comments <b>2112</b> allow a user to jump or to access a menu, a map or an extension by selecting the comment <b>2112</b>. The active comments <b>2112</b> may also allow a user to jump into a particular object in the database <b>54</b>. In the preferred embodiment, comments <b>2112</b> which are always displayed or are global comments are preferably active comments. Comments <b>2112</b> which are assigned low priorities and/or are not global are preferably not active comments <b>2112</b>. Referring to FIG. 10C, the comment <b>2112</b> may be an icon or graphics such as the red flag <b>2120</b> shown in the node boxes <b>1032</b>.</p>
    <p>Coloring, shading, texture and background can be useful and very effective tools for visually passing information to a user. Shading, texture or coloring can be used both within boxes <b>1032</b> on the screen and in the background area of the maps or screen displays. The coloring or background inside a box <b>1032</b> can represent a particular data type. In one embodiment, the user chooses a color to assign to all the distinct data types used in the database <b>54</b>. When the user subsequent uses the invention, the invention will display those data types in the color chosen by the user. For example, in a medical database where boxes <b>1032</b> represent patients, patients admitted through an emergency room can be assigned a different color box <b>1032</b> than patients admitted through a normal process, or patients that survive a procedure may have a different color box <b>1032</b> than patients who die. This allows a user to see at a glance what type of data he or she is looking at. Changing the color between boxes <b>1032</b> is particularly useful and is discussed in further detail later.</p>
    <p>Some of the preferred uses for passing additional information through the background are changing the background type of a map at a particular point on the X, Y, or Z coordinate. A specific example would be changing the background coloring on a map at a particular point on an axis where that point on the axis represents an average, a median, or an important date. Another example is creating a background coloring band between two points on the same axis representing an acceptable or ideal range for a variable. Either the computer or the user can choose what value to change the background type around. The background can change on more than one axis creating panels or areas within a map or screen.</p>
    <p>Finally, for purposes of consistency within a particular application of the graphical user interface <b>70</b>, the coloring of the background of maps of the same type are preferably the same or similar. For example, source maps showing the source for a particular searched object may all have yellow background while influence maps which show objects that have been influenced from an identified object may all have a blue background for the map. In this way, background, coloring and texture can play an important role in visually providing information to the user on a map or screen with the present invention.</p>
    <p>In order to present the most aesthetically pleasing display or output, the preferred GUI Program <b>70</b> chooses an optimal bit map <b>2300</b> or swatch to create a graphical display. In particular, the GUI <b>70</b> determines the color, resolution, and style supported by a display <b>38</b>, output from a printing device or any other computer output. The GUI Program <b>70</b> preferably accomplishes this by categorizing general types of displays <b>38</b> and output devices and assigning bit maps <b>2300</b> or swatches for use with those general types. These general types may include types of printing devices such as color printers, laser printers, inkjet printers, dot matrix printers and types of displays such <b>38</b> as black and white monitors and color monitors with differing resolution capabilities. This feature of the GUI <b>70</b> chooses the optimal bit maps <b>2300</b> or swatches to use as fill-in on boxes <b>1032</b> and the like used in the graphical display.</p>
    <p>To achieve this capability the GUI Program <b>70</b> preferably uses an algorithm to determine what type of display <b>38</b> or printer or other output device is being used by the user. The algorithm then matches that type with one of the general types of categories stored in a look up table <b>2304</b>, as shown in FIG. <b>11</b>. If the type of display <b>38</b> or output device is an exact match with one of the stored types, then the algorithm instructs the GUI Program to use the bit map <b>2300</b> indicated by the table. If the type of display or printer does not match with one of the stored types, then the algorithm determines the optimal bitmap <b>2300</b> for this display <b>38</b> or printer.</p>
    <p>The preferred bit map fill algorithm determines an optimal fill by determining the category the display <b>38</b> or printer being used is closest to, and then picking a bitmap <b>2300</b> according to certain weighted factors. The algorithm preferably chooses a bitmap <b>2300</b> or swatch that will optimize the color depth and resolution of the display <b>38</b> or printer. If both color depth and resolution cannot be optimized by one bitmap <b>2300</b>, the algorithm preferably chooses a category of bitmaps which will optimize the display <b>38</b> or printer's color depth, and then looks in that sub-category for bitmaps <b>2300</b> which will optimize its resolution. The use of this algorithm results in graphical outputs that take advantage of the user's hardware capabilities.</p>
    <p>The GUI Program <b>70</b> also preferably uses the look up table to determine the best bit map <b>2300</b> to be used as a background for the windows <b>2300</b>. The GUI Program <b>70</b> executes an algorithm which determine what type of display <b>38</b> is being used and accesses the look up table <b>2304</b> to determine the preferred bitmap <b>2300</b>. If the type of display <b>38</b> being used is not in the table, the algorithm preferably selects the bitmap <b>2300</b> that is the best fit, again weighing factors such as color depth and resolution in determining the best bitmap <b>2300</b> for display as a window background <b>2308</b>.</p>
    <p>It is preferred that the graphical user interface (GUI <b>70</b>) use a windows approach or a Windows type application. The preferred GUI <b>70</b> for a database <b>54</b> is unusual in that during the normal course of operation it is common (in fact preferred) for many search map windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) to be visible at any given time. The preferred GUI <b>70</b> embodiments utilize various mechanisms to help manage these windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) and avoid confusing the user with too many open or active windows (<b>1000</b>, <b>1004</b>, <b>1008</b>).</p>
    <p>An example of the type of hardware which may be used to implement a preferred window management system is shown in FIG. <b>1</b>. Specifically, it is preferred that a processor <b>30</b>, display <b>38</b>, memory <b>34</b>, <b>58</b>, and a input device such as a mouse <b>42</b> or keyboard <b>46</b> are used. Although the GUI <b>70</b> is described primarily for use with a database management system, the GUI <b>70</b> may be used with many other software applications and in many other hardware configurations.</p>
    <p>For the preferred GUI <b>70</b> window management system embodiments, a parent window (or parent frame window) is used with multiple active child windows. Various mechanisms or commands may be utilized to help manage a plurality of active windows (<b>1000</b>, <b>1004</b>, <b>1008</b>). For example, cascading may be used to arrange the currently displayed windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) in an orderly, consistently-overlapping fashion. The windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) are arranged such that each newly activated window (<b>1000</b>, <b>1004</b>, <b>1008</b>) is a fixed size, and the title bars of previous windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) are still visible. Tiling may also be used to arrange the currently displayed windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) in an orderly, non-overlapping fashion. When using tiling, the child windows are drawn as large as possible within the parent frame window, covering the entire frame window area. There are two preferred methods of tiling, Tile Vertical and Tile Horizontal. Vertical tiling generally involves the side by side display of child windows (e.g., two windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) side by side), shown in FIG. 13A, while horizontal is above and below (e.g. two windows (<b>1000</b>, <b>1004</b>,<b>1008</b>), one above and one below), shown in FIG. <b>13</b>B. Minimizing may be used to display or represent a particular child window in a very small space, examples of representative displays include graphics, icons and/or a text titles. The minimized child window may be displayed at various places in the parent window (e.g. at the bottom of the parent window, taskbar, or titlebar). Maximizing may also be used to display a particular child window as large as possible within a parent windows area. A maximized child window covers or obscures all other active child windows. Restoring may be used to restore a minimized or maximized child window to its previous state. Icon arranging may be performed to arrange all child windows being represented as icons in an orderly fashion.</p>
    <p>In addition, in the preferred GUI <b>70</b> embodiment an auto arrange feature is utilized for enhanced window management. The auto arrange feature solves many of the problems inherent in an interface which creates a large number of child windows. When the number of child windows is large, no arrangement that tries to display all windows at the same time works very well. The child windows either become too small or too cluttered. Forcing the user to manually select a subset of the child windows in which the user is most interested, manually arranging those windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) to be viewed in a primary format and minimizing the rest of the windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) for viewing in a secondary format (or ignored). The user must perform this window management each time a new arrangement is desired which often means each time a new window (<b>1000</b>, <b>1004</b>, <b>1008</b>) is activated or displayed. The auto arrange feature automates this process for the user and intelligently arranges the windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) for the user's screen.</p>
    <p>With the auto arrange feature a limit is placed on the number of windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) to be displayed in the primary format at any one time, a desired number of activated windows (<b>1000</b>, <b>1004</b>, <b>1008</b>). This limit may be set by default, by the user, or by an intelligent process which analyzes for example, the amount of data to be visually represented, screen size, and other variables to determine an optimum number of windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) and a layout for those windows (<b>1000</b>, <b>1004</b>, <b>1008</b>).</p>
    <p>Referring generally to FIG. 12, one version of the auto arrange process involves the following general steps: (1) Based on a default value or through an intelligent process, identify the most recently activated windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) which will be allocated the greatest amount of screen space <b>2080</b>; (2) Using one of several methods, minimize the screen size of the remaining windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) so that their identities may be recognized by the user but only need a small amount of screen space (e.g. icons, text) <b>2084</b>; (3) Arrange the identified windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) in a useful and space efficient manner (e.g. vertically, horizontally, cubes etc. <b>2088</b>) (4) Arrange the minimized but recognizable windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) in an orderly but non-obtrusive manner on the screen (e.g. arrange icons in lower comer of screen <b>2092</b>). Using this automated process, the windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) can be automatically rearranged each time a new window (<b>1000</b>, <b>1004</b>, <b>1008</b>) is activated (by repeating the above steps) or whenever the user initiates the process. The windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) are kept in an organized and useable fashion with little effort on the part of the user. The auto arrange can use different formats (primary, secondary, tertiary, etc) for different levels of interest in the window (<b>1000</b>, <b>1004</b>, <b>1008</b>). The auto arrange feature can also be turned on or off at the will of the user.</p>
    <p>Instead of recognizing and minimizing the windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) which are beyond the desired number of active windows (<b>1000</b>, <b>1004</b>, <b>1008</b>), the system may simply ignore these windows (<b>1000</b>, <b>1004</b>, <b>1008</b>), or some combination of minimizing and ignoring may be used <b>2084</b>. For example, if the desired number of active windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) for display is two, the last two activated windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) may by arranged on the screen side by side in a full format and an additional three windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) may be recognized and minimized to icons for display on a small portion of the screen. Any windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) beyond the last five activated windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) are ignored by the GUI <b>70</b> window management system.</p>
    <p>By providing some options, preferences options, for the auto arrange feature, the feature can be customized to the particular taste of a user. The desired arrangement of the windows (<b>1000</b>, <b>1004</b>, <b>1008</b>), or target arrangement can be explicitly chosen by the user and changed at will by the user (or chosen from a list of available formats). For example, a user can specify the number of windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) to display, the particular format each window (<b>1000</b>, <b>1004</b>, <b>1008</b>) will appear on the screen, and the layout of the screen. There possible screen layouts are nearly limitless. Various formats are possible for each window (<b>1000</b>, <b>1004</b>, <b>1008</b>), for example as , , full, vertically stretched, horizontally stretched or enlarged format. The user can chose the number of windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) to be displayed in each format for example two, three, or four windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) in the  format. And therefore, a target arrangement can be chosen such as full format, two windows (<b>1000</b>, <b>1004</b>, <b>1008</b>), vertically side by side. Thus, when step three of the auto arrange process is preformed, the system will identify the two most recently activated windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) and arrange the two windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) in the target arrangement, side by side, rather than in some other manner. In the preferred embodiment, a menu is provided to the user permitting the user to chose a target arrangement including number of windows (<b>1000</b>, <b>1004</b>, <b>1008</b>), format of windows (<b>1000</b>, <b>1004</b>, <b>1008</b>), and screen arrangement.</p>
    <p>For high-end power users, the window management system can be modified to allow the user to custom build nearly any arbitrary layout for the screen. The user creates any number of arbitrary layouts, each of which is given a name that is inserted into a window menu and is stored in a database. After a layout has been named, it is then treated as a new window management command which can be executed. In the most sophisticated embodiments, through the use of pointer and/or a mouse <b>42</b> the user selects anchor points, such as center points, or upper left, upper right, lower left, lower right, and various shapes for the windows (<b>1000</b>, <b>1004</b>, <b>1008</b>) such as hand sketched, rectangular, triangular, rhomboids, octagons etc. In this manner displays can be generated which are suited for specific uses. Also, through this medium, the artistry and creativity of the user may be expressed in aesthetically pleasing displays.</p>
    <p>An innovative feature of the preferred embodiment is the ability to call up a search screen or map while viewing the data of a particular object in the database <b>54</b>. This feature is implemented through the use of embedded active links <b>2004</b>. By using embedded icons that are active within the data of an object being viewed or by using embedded text which is active within the data of an object in the database <b>54</b>, this feature allows the user to jump from viewing data to a search screen, menu, map or the like. The search screen or map can be one which has been previously generated or can be generated at the time of selecting the embedded active icon or active text.</p>
    <p>The preferred method of using this feature is with text documents. Active icons or active text are embedded within the text documents and the user is alerted to these active icons or text through the use of highlighting or different coloring of the active icon or text. When the user sees an active icon or active text while viewing an object in the database <b>54</b>, the user may choose to jump out of the object and into a map, search screen, or the like.</p>
    <p>The system may be configured so that upon selection of an active icon or active portion of text, a menu is displayed to the user wherein the user may select the generation of a particular map or the return to an existing map that was previously generated.</p>
    <p>Although these active links <b>2004</b> within an object in the database <b>54</b> have been described for use in jumping from an object in the database <b>54</b> to a map, the active links <b>2004</b> may be used to jump to other objects in the database <b>54</b> or extensions to other databases <b>54</b>, other applications, or communication programs. Providing active links <b>2004</b> within objects being viewed in a database <b>54</b> allows great flexibility for the user to navigate through data in any manner he chooses.</p>
    <p>To allow access to extensions to other databases, the preferred embodiment is set up in a modular fashion in order to be able to modularly add extensions or add on links to connect to other applications or programs which can be called up from the present invention.</p>
    <p>In the preferred embodiment, the invention is set up in a modular fashion to accept one or more extensions. An extension can be another application or can be a communications link to connect to another computer or application. Use of these connections is particularly well suited for the invention in that the underlying data need not be stored locally with the user, but instead, through the use of extensions, the underlying data can be accessed by the user through an extension, another application and/or through a communications link.</p>
    <p>Multiple extensions are possible and it is possible for the same underlying data to be available through one or more extensions, this allows the user to choose which extension or communication link it will use to access underlying data. In the preferred implementation, a box <b>1032</b> is given depth to signify that an extension associated with a particular box <b>1032</b> is available to the user. By activating the box <b>1032</b>, the user is given the opportunity to use the extension. In the preferred embodiment with modular implementations of extensions, a user can add on or plug in further extensions or eliminate extensions.</p>
    <p>Another feature of the preferred embodiment is the show usage command. FIG. 8 is a screen display <b>38</b> depicting the use of the show usage command. The preferred embodiment includes this command to allow the user to see a portion of an object in the database <b>54</b> which uses cites or refers to the node <b>2008</b> from which the show usage command is requested. More specifically, the show usage command allows the user to see the text or data of a portion of the document that is represented by the node <b>2008</b> being searched. In the preferred embodiment, the show usage command is only available from a result node <b>2104</b>. When a map or graphics display is shown the search node <b>2100</b> is the node <b>2008</b> upon which the search being displayed is based. The result nodes <b>2104</b> are the nodes <b>2008</b> which are graphically displayed as a result of the search conducted upon the search node <b>2100</b>. Referring to FIG. 8, the search node <b>2100</b> is Alves v. Commissioner and the result node <b>2104</b> is 26 U.S.C.  83.</p>
    <p>Through the use of the show usage command, a user may immediately access that portion of the search node object <b>2108</b> or document <b>2108</b> which refers to a specific result node <b>2104</b>. This is accomplished by breaking up the data connected to the search node <b>2100</b> into groups of records with header identifiers. The data attached to the result node <b>2104</b> will also have an identifier, a header identifier, which particularly identifies the data attached to it. When the user executes the show usage command after activating the result node <b>2104</b>, the record or records in the data attached to the source node <b>2100</b> which match the result node <b>2104</b> identifier will be displayed and highlighted. For example, in FIG. 8, the Alves v. Commissioner document <b>2108</b> is shown highlighted at the appropriate location identifying 26 U.S.C.  83 2104. The show usage command is accessed through the use of a pull-down menu from the result node 26 U.S.C.  83 2104. Using the earlier example of modem an classical architecture, if the search node <b>2100</b> was modem architecture and the search requested items influencing modem architecture an influence map or graphic display would be generated which would include the classical architecture node. By selecting the classical architecture node and using the show usage command on the classical architecture node (for example through a pull-down menu or directly in the node box) the invention will immediately bring the user to the first location in the modem architectural data where classical architecture is referred to as influencing modem architecture. Thus, in effect, the show usage command allows the user to jump from a result node <b>2104</b> to the specific location <b>2108</b> in the search node <b>2100</b> where the result node <b>2104</b> is referenced or identified.</p>
    <p>Finally, one of the most important features of the invention is its method of integrating itself with third party software applications. Although useful with many third party software applications, it is particularly useful to integrate the present invention with third party database applications which operate in a windows type environment. Nearly all of the database management functions and graphical user interface <b>70</b> features can be used in an integrated scheme with third party database management software.</p>
    <p>The preferred method of integrating the present invention with third party software is through the use of a subclassing technique in a windows multiple document interface (MDI) environment. Specifically, the present invention can take advantage of the common behavior exhibited by MDI applications to integrate with third party software operating in a windows environment.</p>
    <p>When the preferred embodiment of the invention is loaded to be used in conjunction with third party software application, the invention immediately subclasses the third party software applications frame window. Through this subclassing technique, the present invention receives (intercepts) every message or command originally intended for the third party software. Since the invention is the first to receive each window message, it acts as a message arbiter. The message arbiter has the ability to recognize the message or command and decide how each message should be processed. For example, the arbiter decides whether any given message should be processed by the master program (the invention) or by the subclassed third party software.</p>
    <p>The precise processing that is appropriate for a given message is somewhat message dependent. However, the general message scheme dictates that messages intended for one of the child MDI windows (or that depend in some way on the content of a child window) are dispatched to the application that is the real owner or creator of that child window. Thus, most messages are dispatched to the software application to which the child window belongs (to which the window is a native), if it the window belongs to a subclassed application, the message is directed or forwarded to the subclassed application. The subclassed application then processes the forwarded message and changes a child window display if necessary. Using this technique, the subclassed software application acts as if it alone owns the main frame window. Thus, operation of the master program has little affect on the performance of subclassed program. Further, the operation of the subsclassed program is transparent to the user. To the casual user, the master program operates all the windows and is the only user interface used.</p>
    <p>Using this technique, more than one software application may be subclassed with the present invention. Also, each subclassed application may have multiple child window displays. And finally, the master application may generate its own native windows which may be displayed simultaneously with the child windows of a subclassed application.</p>
    <p>This computerized system for researching data is also effective with any type of internal or global network application (see generally FIGS. <b>14</b>A and <b>14</b>B). As long as a network stores data and provides links <b>2004</b> between that data, this system can provide an effective and efficient system for indexing, searching, and displaying that data. For example, this system can be applied to the Internet and the World Wide Web. The World Wide Web is made up of numerous web sites which contain documents and internet or web pages. Documents are usually defined in the art as unique pieces of data, which includes text files, graphic files, audio files, and video files. A web page is usually a document with its own Universal Resource Locator (URL). URLs are the standardized addresses commonly used for web pages. Generally, web sites are a collection of web pages and documents. Web sites are usually identified by a home page, which may contain an overall starting point for the web site and a summary of what is to be found at the web site. Hyperjump links, or hyperlinks, is the name commonly given to the links which connect web pages, web sites, and documents on the web. Hyperlinks are electronic links which allow end users to jump to the specified web page or web site. The software code commonly used to create the majority of web pages containing text files is HyperText Markup Language (HTML). Other pages containing graphics, audio, video, and other resources may not be coded in HTML, but still will be connected by hyperlinks.</p>
    <p>The Internet can be viewed as an immense collection of linked documents providing varied information to the public via an elaborate electronic distribution channel. In the past, the end user's ability to search, find, index, and navigate through relevant documents of interest has been primarily limited to word based queries which primarily rely on the target document's text indexing. Instead of relying on textual searching, this method and apparatus for indexing, searching, and displaying data analyzes hyperlinks which connect web pages to other web pages in order to help the end user to search, find, and navigate through the relevant documents of interest. This system analyzes hyperlinks using proximity indexing or clustering technology discussed previously. Once identified, the system displays the results in a variety of ways and end users are able to navigate directly to the documents identified by this system's analyzation technology.</p>
    <p>In the preferred embodiment, this system uses the cluster link generation algorithm described in FIG. 3H to search and identify closely associated documents located on the Internet in the same manner as described above. The system treats hyperlinks <b>2004</b> on the Web in the same manner as it treats links <b>2004</b> in a database, and it treats web pages on the Web in the same manner as it treats nodes <b>2008</b> in a database <b>54</b>. Source links <b>2004</b> on the Web link a source node <b>2008</b> (or source web page) to a second node (or second web page). Influence links <b>2004</b> perform the same function in reverse. Direct links <b>2032</b> (as described above) are the same as hyperlinks <b>2004</b>, which use URLs, in the World Wide Web, and they directly link one web page (or node) to another. Indirect links <b>2036</b> link two web pages or nodes <b>2008</b> through more than one path. A cluster link, for purposes of the Web, is any relationship between two web pages.</p>
    <p>To begin the process, as shown generally in FIG. 14A, a node <b>2008</b> is chosen <b>3000</b> for analysis. Next, the system accesses link data <b>3004</b> or crawls the source web page (or source node <b>2008</b>) looking for URLs which directly link the source web page to other web pages. Web crawling is a known technique in the art, performed by most World WIde Web search services, such as Yahoo (located at www.yahoo.com) or Alta Vista. Crawling is accomplished by the use of automated programs called robots or spiders, which analyze a web page for objects which provide URL links to other web pages or documents. The source node <b>2008</b>, whether it is a web page, the home page of a web site, or a document with no links <b>2004</b>, is a data document which may have been encoded in HTML or some other language. The encoded data document includes commands such as insert picture here or begin a new paragraph or place a link here to another document along with the normal text of the document. These coded commands are generally invisible to the end user, although many Web documents reveal text containing coded links <b>2004</b> to other documents in different colors. The system reads the coded HTML instructions to identify <b>3008</b> the coded links, which are direct links <b>2032</b>. There are many publicly known methods of identifying links <b>2004</b> from a coded document that one skilled in the art could employ to perform this function.</p>
    <p>FIG. 14B describes the embodiment of the invention which executes <b>3020</b> the cluster link generator algorithm <b>2044</b> to generate direct and indirect links <b>2004</b> to find the set of candidate duster links. After identifying <b>3008</b> all of the URLs referenced in the source web page, in the preferred embodiment, the cluster link generation algorithm <b>2044</b> retrieves <b>2056</b> a list of URLs and classifies them as the direct links <b>2032</b> to be analyzed. The cluster link generator <b>2044</b> traces the links <b>2032</b> to their destination nodes <b>2008</b> (a web site or web page) and performs a web crawl to retrieve <b>2056</b> a list of URLs referenced by the source nodes <b>2008</b>. The generator <b>2044</b> classifies the second set of nodes <b>2008</b> as being indirectly linked to the source node <b>2004</b>, and the links <b>2036</b> to these nodes <b>2008</b> are added <b>2072</b> to the list of candidate cluster links. In order to find the set of candidate cluster links, the cluster link generator <b>2044</b> repeats the above steps <b>2052</b>. In the more general method described in FIG. 14A, the system identifies <b>3012</b> the links <b>2036</b> which have an indirect relationship and then displays <b>3020</b> the direct <b>2032</b> and indirect <b>2036</b> links.</p>
    <p>Once a candidate cluster link set is identified, the generator <b>2044</b> assigns <b>2064</b>, <b>2076</b> weights <b>2034</b> to the candidate cluster links <b>2004</b>. The weight <b>2034</b> of each individual path or link <b>2004</b> is a function of the weight <b>2034</b> of the path to the previous node <b>2008</b> and the weight <b>2034</b> of the last link <b>2004</b>. In order to determine the weight <b>2034</b> of an implied link <b>2004</b>, the preferred formula, WC<sub>i+1</sub>=min(WC<sub>i</sub>, D<sub>i+1</sub>* W<sub>i+1</sub>) <b>2064</b>, as previously discussed, is used. Following weighting, the generator <b>2044</b> sorts the set of candidate cluster links <b>2004</b> by weight, and a subset of these links <b>2004</b> (those links <b>2004</b> above a specified cut-off weight) are retained for display <b>3020</b> to the end user. In the preferred embodiment, the formula T=min (constant, 4*d), discussed before determines the optimal cut-off weight.</p>
    <p>In another embodiment, the Proximity Indexing Application Program (Program) <b>62</b> organizes and categorizes the crawled links <b>2004</b> using the statistical techniques and empirically generated algorithms described earlier in this application. The Program <b>62</b> treats URL addresses as citations and web pages as textual objects. The Program <b>62</b> applies some or all of the eighteen pattern list to determine the relatedness of the web pages (or nodes) which are linked to the source web page (or node). The Program <b>62</b> weighs the patterns by importance, giving one type of data document more importance than another type. For example, it may give more importance to a web site than to a single document which has no other links. The Program <b>62</b> may use other factors to weigh the data documents, such as the number of hits (visits by other end users to the site, a number which is available to web users) a data document receives in a specific time frame or the number of hyperlinks within a page. The Program <b>62</b> then forms a matrix based on ordered pairs of documents, and the matrix calculations discussed before of this specification can be carried out. The Program <b>62</b> generates a coefficient of similarity which will determine the relatedness of web pages to each other and to the source web page. The Program <b>62</b> displays the most similar web pages to the user.</p>
    <p>The preferred embodiment of the network application of this system uses the graphical user interface program <b>70</b> to display the results of the algorithm as a list showing the selected links <b>2004</b> and the various data associated with the links <b>2004</b>. The links <b>2004</b> shown on the screen to the end user are active links <b>2004</b>, similar to the active comments used in the text boxes <b>1032</b> described previously in this application. The end user may instantaneously link to the destination node <b>2008</b> that the user selects. The list format provides link information in a style familiar to user of the Internet. However, this system is also capable of displaying the results in the user-friendly graphical format as described above. The graphical user interface program <b>70</b> described previously uses box coloring and sizing to communicate large amounts of information quickly and intelligibly to the user. In a preferred embodiment, different colors for boxes <b>1032</b> are assigned depending on what type of node <b>2008</b> they represent (e.g., a web page, web site, a document, a file transfer protocol (FTP) (a common internet designation for news sites)). Preferably, the box <b>1032</b> is given depth. The amount of URL links a node <b>2008</b> contains may determine the amount of depth.</p>
    <p>The graphical user interface program <b>70</b> displays a list of the most related web pages to the source web page. This list includes documents, web sites, and pages which are directly or indirectly linked to the subject document or the subject topic. The links <b>2004</b> can be source links <b>2004</b> or influence links <b>2004</b>, so the end user may monitor the sites to which his site (the source web page) is referring, and the end user may view the sites which are referring to his site. The system can parse the URL of the destination nodes <b>2008</b> for a variety of information. Thus, the end user may monitor whether the connections to which his web site refers are still open, the end user may view the date and time a destination node <b>2008</b> was modified, and the end user may view the identification of the organization or author of the destination node that directly or indirectly links to the source node <b>2008</b>. The GUI program <b>70</b> displays all of this information either in the list format or in the text box <b>1032</b> used in the graphical format Graphical comments may be placed in the text box to communicate information quickly, such as showing a happy face for a connected application, and so forth. Hyperlinks can appear as active comments in a text box in order to allow the user to instantaneously jump to the web page represented by the text box.</p>
    <p>Although this computerized system for researching data is described as functioning in the World Wide Web environment, it can function equally well in any network system. A network that utilizes any type of hyperjump <b>2004</b> to connect documents together can serve as the links <b>2004</b> analyzed by this invention. This system therefore can be modified to navigate and search through internal company networks, and provide the same features as described above for the Web application. Additionally, the comment boxes can be tailored to display critical information about company files, thus enhancing its usefulness for the company employee who is attempting to sort through company documents stored on a network.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5265065">US5265065</a></td><td class="patent-data-table-td patent-date-value">Oct 8, 1991</td><td class="patent-data-table-td patent-date-value">Nov 23, 1993</td><td class="patent-data-table-td ">West Publishing Company</td><td class="patent-data-table-td ">Method and apparatus for information retrieval from a database by replacing domain specific stemmed phases in a natural language to create a search query</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5341293">US5341293</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 3, 1992</td><td class="patent-data-table-td patent-date-value">Aug 23, 1994</td><td class="patent-data-table-td ">Apple Computer, Inc.</td><td class="patent-data-table-td ">User interface system having programmable user interface elements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5446842">US5446842</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 26, 1993</td><td class="patent-data-table-td patent-date-value">Aug 29, 1995</td><td class="patent-data-table-td ">Taligent, Inc.</td><td class="patent-data-table-td ">Object-oriented collaboration system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5530852">US5530852</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 20, 1994</td><td class="patent-data-table-td patent-date-value">Jun 25, 1996</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Method for extracting profiles and topics from a first file written in a first markup language and generating files in different markup languages containing the profiles and topics for use in accessing data described by the profiles and topics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5542024">US5542024</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 9, 1992</td><td class="patent-data-table-td patent-date-value">Jul 30, 1996</td><td class="patent-data-table-td ">Johnson &amp; Johnson</td><td class="patent-data-table-td ">Graphically used expert system tool background of the invention</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5544352">US5544352</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 14, 1993</td><td class="patent-data-table-td patent-date-value">Aug 6, 1996</td><td class="patent-data-table-td ">Libertech, Inc.</td><td class="patent-data-table-td ">Method and apparatus for indexing, searching and displaying data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5617565">US5617565</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 29, 1994</td><td class="patent-data-table-td patent-date-value">Apr 1, 1997</td><td class="patent-data-table-td ">Hitachi America, Ltd.</td><td class="patent-data-table-td ">Broadcast interactive multimedia system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5649186">US5649186</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 7, 1995</td><td class="patent-data-table-td patent-date-value">Jul 15, 1997</td><td class="patent-data-table-td ">Silicon Graphics Incorporated</td><td class="patent-data-table-td ">System and method for a computer-based dynamic information clipping service</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5749785">US5749785</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 8, 1995</td><td class="patent-data-table-td patent-date-value">May 12, 1998</td><td class="patent-data-table-td ">Rossides; Michael T.</td><td class="patent-data-table-td ">Communications system using bets</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5794001">US5794001</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 7, 1995</td><td class="patent-data-table-td patent-date-value">Aug 11, 1998</td><td class="patent-data-table-td ">Massachusetts Institute Of Technology</td><td class="patent-data-table-td ">Object-oriented computer user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5832494">US5832494</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 17, 1996</td><td class="patent-data-table-td patent-date-value">Nov 3, 1998</td><td class="patent-data-table-td ">Libertech, Inc.</td><td class="patent-data-table-td ">Method and apparatus for indexing, searching and displaying data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5898434">US5898434</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 22, 1994</td><td class="patent-data-table-td patent-date-value">Apr 27, 1999</td><td class="patent-data-table-td ">Apple Computer, Inc.</td><td class="patent-data-table-td ">User interface system having programmable user interface elements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6098081">US6098081</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 6, 1996</td><td class="patent-data-table-td patent-date-value">Aug 1, 2000</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Hypermedia navigation using soft hyperlinks</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">a) Agosti, et al., "<a href='http://scholar.google.com/scholar?q="A+Two-Level+Hypertext+Retrieval+Model+for+Legal+Data%2C"'>A Two-Level Hypertext Retrieval Model for Legal Data,</a>" SIGIR '91 (1991).</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">b) Fowler, et al., "<a href='http://scholar.google.com/scholar?q="Integrating+Query%2C+Thesaurus+and+Documents+Through+a+Common+Visual+Representation%2C"'>Integrating Query, Thesaurus and Documents Through a Common Visual Representation,</a>" SIGIR '91 (1991).</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">c) Rose &amp; Belew, "<a href='http://scholar.google.com/scholar?q="Legal+Information+Retrieval%3A+A+Hybrid+Approach%2C"'>Legal Information Retrieval: A Hybrid Approach,</a>" ICAIL '89 (1989).</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">d) Belew, Richard, "<a href='http://scholar.google.com/scholar?q="A+Connectionist+Approach+to+Conceptual+Information+Retrieval%2C"'>A Connectionist Approach to Conceptual Information Retrieval,</a>" ICAIL '87 (1987).</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">e) Gelbart &amp; Smith, "<a href='http://scholar.google.com/scholar?q="Beyond+Boolean+Search%3A+FLEXICON%2C+A+Legal+Text-Based+Intelligent+System%2C"'>Beyond Boolean Search: FLEXICON, A Legal Text-Based Intelligent System,</a>" ICAIL '91 (1991).</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">f) Lin, "<a href='http://scholar.google.com/scholar?q="A+Self-Organizing+Semantic+Map+for+Information+Retrieval%2C"'>A Self-Organizing Semantic Map for Information Retrieval,</a>" SIGIR '91 (1991).</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">g) Turtle &amp; Croft, "<a href='http://scholar.google.com/scholar?q="Inference+Networks+for+Document+Retrieval%2C"'>Inference Networks for Document Retrieval,</a>" SIGR '90 (1990).</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6362839">US6362839</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 29, 1998</td><td class="patent-data-table-td patent-date-value">Mar 26, 2002</td><td class="patent-data-table-td ">Rockwell Software Inc.</td><td class="patent-data-table-td ">Method and apparatus for displaying mechanical emulation with graphical objects in an object oriented computing environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6424969">US6424969</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 20, 1999</td><td class="patent-data-table-td patent-date-value">Jul 23, 2002</td><td class="patent-data-table-td ">Inmentia, Inc.</td><td class="patent-data-table-td ">System and method for organizing data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6529898">US6529898</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 22, 1999</td><td class="patent-data-table-td patent-date-value">Mar 4, 2003</td><td class="patent-data-table-td ">Matthew Shawn Fortner</td><td class="patent-data-table-td ">Method and system for electronically retrieving door hardware data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6542896">US6542896</a></td><td class="patent-data-table-td patent-date-value">Jul 14, 2000</td><td class="patent-data-table-td patent-date-value">Apr 1, 2003</td><td class="patent-data-table-td ">Primentia, Inc.</td><td class="patent-data-table-td ">System and method for organizing data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6622139">US6622139</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 19, 1999</td><td class="patent-data-table-td patent-date-value">Sep 16, 2003</td><td class="patent-data-table-td ">Fuji Xerox Co., Ltd.</td><td class="patent-data-table-td ">Information retrieval apparatus and computer-readable recording medium having information retrieval program recorded therein</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6633817">US6633817</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 29, 1999</td><td class="patent-data-table-td patent-date-value">Oct 14, 2003</td><td class="patent-data-table-td ">Incyte Genomics, Inc.</td><td class="patent-data-table-td ">Sequence database search with sequence search trees</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6633903">US6633903</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 23, 2000</td><td class="patent-data-table-td patent-date-value">Oct 14, 2003</td><td class="patent-data-table-td ">Monkeymedia, Inc.</td><td class="patent-data-table-td ">Method and article of manufacture for seamless integrated searching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6665661">US6665661</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 29, 2000</td><td class="patent-data-table-td patent-date-value">Dec 16, 2003</td><td class="patent-data-table-td ">Battelle Memorial Institute</td><td class="patent-data-table-td ">System and method for use in text analysis of documents and records</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6708162">US6708162</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 8, 2000</td><td class="patent-data-table-td patent-date-value">Mar 16, 2004</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for unifying search strategy and sharing search output data across multiple program modules</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6757870">US6757870</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 22, 2000</td><td class="patent-data-table-td patent-date-value">Jun 29, 2004</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">Automatic table detection method and system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6785684">US6785684</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 27, 2001</td><td class="patent-data-table-td patent-date-value">Aug 31, 2004</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Apparatus and method for determining clustering factor in a database using block level sampling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6868525">US6868525</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 26, 2000</td><td class="patent-data-table-td patent-date-value">Mar 15, 2005</td><td class="patent-data-table-td ">Alberti Anemometer Llc</td><td class="patent-data-table-td ">Computer graphic display visualization system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6918085">US6918085</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 10, 1999</td><td class="patent-data-table-td patent-date-value">Jul 12, 2005</td><td class="patent-data-table-td ">Cora Sa</td><td class="patent-data-table-td ">Process for storing text and procedure for searching stored texts for those pertinent to a question</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6934702">US6934702</a></td><td class="patent-data-table-td patent-date-value">Mar 26, 2002</td><td class="patent-data-table-td patent-date-value">Aug 23, 2005</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Method and system of routing messages in a distributed search network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6944619">US6944619</a></td><td class="patent-data-table-td patent-date-value">Apr 12, 2001</td><td class="patent-data-table-td patent-date-value">Sep 13, 2005</td><td class="patent-data-table-td ">Primentia, Inc.</td><td class="patent-data-table-td ">System and method for organizing data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6950821">US6950821</a></td><td class="patent-data-table-td patent-date-value">Mar 26, 2002</td><td class="patent-data-table-td patent-date-value">Sep 27, 2005</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">System and method for resolving distributed network search queries to information providers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6961723">US6961723</a></td><td class="patent-data-table-td patent-date-value">Mar 26, 2002</td><td class="patent-data-table-td patent-date-value">Nov 1, 2005</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">System and method for determining relevancy of query responses in a distributed network search mechanism</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6993475">US6993475</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 3, 2000</td><td class="patent-data-table-td patent-date-value">Jan 31, 2006</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Methods, apparatus, and data structures for facilitating a natural language interface to stored information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6999963">US6999963</a></td><td class="patent-data-table-td patent-date-value">May 3, 2000</td><td class="patent-data-table-td patent-date-value">Feb 14, 2006</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Methods, apparatus, and data structures for annotating a database design schema and/or indexing annotations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7000183">US7000183</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 27, 2000</td><td class="patent-data-table-td patent-date-value">Feb 14, 2006</td><td class="patent-data-table-td ">John M. Crawford, Jr.</td><td class="patent-data-table-td ">Method and apparatus for viewer-specific presentation of information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7003566">US7003566</a></td><td class="patent-data-table-td patent-date-value">Jun 29, 2001</td><td class="patent-data-table-td patent-date-value">Feb 21, 2006</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and system for predictive directional data caching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7013303">US7013303</a></td><td class="patent-data-table-td patent-date-value">Mar 26, 2002</td><td class="patent-data-table-td patent-date-value">Mar 14, 2006</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">System and method for multiple data sources to plug into a standardized interface for distributed deep search</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7099871">US7099871</a></td><td class="patent-data-table-td patent-date-value">Mar 26, 2002</td><td class="patent-data-table-td patent-date-value">Aug 29, 2006</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">System and method for distributed real-time search</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7117434">US7117434</a></td><td class="patent-data-table-td patent-date-value">Jun 29, 2001</td><td class="patent-data-table-td patent-date-value">Oct 3, 2006</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Graphical web browsing interface for spatial data navigation and method of navigating data blocks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7127685">US7127685</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 2002</td><td class="patent-data-table-td patent-date-value">Oct 24, 2006</td><td class="patent-data-table-td ">America Online, Inc.</td><td class="patent-data-table-td ">Instant messaging interface having a tear-off element</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7162428">US7162428</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 31, 2001</td><td class="patent-data-table-td patent-date-value">Jan 9, 2007</td><td class="patent-data-table-td ">Allen Rosenthal</td><td class="patent-data-table-td ">System and method for online creation and integration of service of process functions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7171415">US7171415</a></td><td class="patent-data-table-td patent-date-value">May 31, 2001</td><td class="patent-data-table-td patent-date-value">Jan 30, 2007</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Distributed information discovery through searching selected registered information providers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7188141">US7188141</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 29, 2001</td><td class="patent-data-table-td patent-date-value">Mar 6, 2007</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and system for collaborative web research</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7197697">US7197697</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 15, 2000</td><td class="patent-data-table-td patent-date-value">Mar 27, 2007</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Apparatus for retrieving information using reference reason of document</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7213205">US7213205</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 2, 2000</td><td class="patent-data-table-td patent-date-value">May 1, 2007</td><td class="patent-data-table-td ">Seiko Epson Corporation</td><td class="patent-data-table-td ">Document categorizing method, document categorizing apparatus, and storage medium on which a document categorization program is stored</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7272594">US7272594</a></td><td class="patent-data-table-td patent-date-value">May 31, 2001</td><td class="patent-data-table-td patent-date-value">Sep 18, 2007</td><td class="patent-data-table-td ">Autonomy Corporation Ltd.</td><td class="patent-data-table-td ">Method and apparatus to link to a related document</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7281215">US7281215</a></td><td class="patent-data-table-td patent-date-value">Jul 31, 2002</td><td class="patent-data-table-td patent-date-value">Oct 9, 2007</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">IM conversation counter and indicator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7284207">US7284207</a></td><td class="patent-data-table-td patent-date-value">Sep 6, 2006</td><td class="patent-data-table-td patent-date-value">Oct 16, 2007</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">Instant messaging interface having a tear-off element</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7293005">US7293005</a></td><td class="patent-data-table-td patent-date-value">Jan 26, 2004</td><td class="patent-data-table-td patent-date-value">Nov 6, 2007</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Pipelined architecture for global analysis and index building</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7310642">US7310642</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 14, 2005</td><td class="patent-data-table-td patent-date-value">Dec 18, 2007</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Methods, apparatus and data structures for facilitating a natural language interface to stored information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7328136">US7328136</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 15, 2004</td><td class="patent-data-table-td patent-date-value">Feb 5, 2008</td><td class="patent-data-table-td ">Council Of Scientific &amp; Industrial Research</td><td class="patent-data-table-td ">Computer based method for finding the effect of an element in a domain of N-dimensional function with a provision for N+1 dimensions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7349894">US7349894</a></td><td class="patent-data-table-td patent-date-value">Jul 30, 2004</td><td class="patent-data-table-td patent-date-value">Mar 25, 2008</td><td class="patent-data-table-td ">Sidestep, Inc.</td><td class="patent-data-table-td ">Method and apparatus for dynamic information connection search engine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7370277">US7370277</a></td><td class="patent-data-table-td patent-date-value">Dec 23, 2002</td><td class="patent-data-table-td patent-date-value">May 6, 2008</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">E-mail interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7373353">US7373353</a></td><td class="patent-data-table-td patent-date-value">May 10, 2002</td><td class="patent-data-table-td patent-date-value">May 13, 2008</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Reducing index size for multi-level grid indexes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7379944">US7379944</a></td><td class="patent-data-table-td patent-date-value">Oct 20, 2005</td><td class="patent-data-table-td patent-date-value">May 27, 2008</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Reducing index size for multi-level grid indexes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7383269">US7383269</a></td><td class="patent-data-table-td patent-date-value">Sep 12, 2003</td><td class="patent-data-table-td patent-date-value">Jun 3, 2008</td><td class="patent-data-table-td ">Accenture Global Services Gmbh</td><td class="patent-data-table-td ">Navigating a software project repository</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7383275">US7383275</a></td><td class="patent-data-table-td patent-date-value">May 10, 2002</td><td class="patent-data-table-td patent-date-value">Jun 3, 2008</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Methods to improve indexing of multidimensional databases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7389283">US7389283</a></td><td class="patent-data-table-td patent-date-value">Dec 7, 2004</td><td class="patent-data-table-td patent-date-value">Jun 17, 2008</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method for determining an optimal grid index specification for multidimensional data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7395259">US7395259</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 30, 2004</td><td class="patent-data-table-td patent-date-value">Jul 1, 2008</td><td class="patent-data-table-td ">A9.Com, Inc.</td><td class="patent-data-table-td ">Search engine system and associated content analysis methods for locating web pages with product offerings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7421661">US7421661</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 2002</td><td class="patent-data-table-td patent-date-value">Sep 2, 2008</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">Instant messaging interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7424467">US7424467</a></td><td class="patent-data-table-td patent-date-value">Jan 26, 2004</td><td class="patent-data-table-td patent-date-value">Sep 9, 2008</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Architecture for an indexer with fixed width sort and variable width sort</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7430561">US7430561</a></td><td class="patent-data-table-td patent-date-value">Mar 30, 2006</td><td class="patent-data-table-td patent-date-value">Sep 30, 2008</td><td class="patent-data-table-td ">A9.Com, Inc.</td><td class="patent-data-table-td ">Search engine system for locating web pages with product offerings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7437372">US7437372</a></td><td class="patent-data-table-td patent-date-value">Oct 20, 2005</td><td class="patent-data-table-td patent-date-value">Oct 14, 2008</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Systems, methods, and computer program products to reduce computer processing in grid cell size determination for indexing of multidimensional databases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7457818">US7457818</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 14, 2006</td><td class="patent-data-table-td patent-date-value">Nov 25, 2008</td><td class="patent-data-table-td ">Accenture Global Services Gmbh</td><td class="patent-data-table-td ">Context-based display technique with hierarchical display format</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7461064">US7461064</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 2004</td><td class="patent-data-table-td patent-date-value">Dec 2, 2008</td><td class="patent-data-table-td ">International Buiness Machines Corporation</td><td class="patent-data-table-td ">Method for searching documents for ranges of numeric values</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7499913">US7499913</a></td><td class="patent-data-table-td patent-date-value">Jan 26, 2004</td><td class="patent-data-table-td patent-date-value">Mar 3, 2009</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method for handling anchor text</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7512609">US7512609</a></td><td class="patent-data-table-td patent-date-value">Jul 22, 2005</td><td class="patent-data-table-td patent-date-value">Mar 31, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Methods, apparatus, and data structures for annotating a database design schema and/or indexing annotations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7512900">US7512900</a></td><td class="patent-data-table-td patent-date-value">Jul 28, 2004</td><td class="patent-data-table-td patent-date-value">Mar 31, 2009</td><td class="patent-data-table-td ">Autonomy Corporation Ltd.</td><td class="patent-data-table-td ">Methods and apparatuses to generate links from content in an active window</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7519576">US7519576</a></td><td class="patent-data-table-td patent-date-value">Sep 13, 2001</td><td class="patent-data-table-td patent-date-value">Apr 14, 2009</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Integrated user interface mechanism for recursive searching and selecting of items</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7523115">US7523115</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 20, 2001</td><td class="patent-data-table-td patent-date-value">Apr 21, 2009</td><td class="patent-data-table-td ">Definiens Ag</td><td class="patent-data-table-td ">Method for finding objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7574659">US7574659</a></td><td class="patent-data-table-td patent-date-value">Mar 4, 2005</td><td class="patent-data-table-td patent-date-value">Aug 11, 2009</td><td class="patent-data-table-td ">Andrew Szabo</td><td class="patent-data-table-td ">Computer graphic display visualization system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7596597">US7596597</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 2006</td><td class="patent-data-table-td patent-date-value">Sep 29, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Recommending contacts in a social network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7596746">US7596746</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 6, 2005</td><td class="patent-data-table-td patent-date-value">Sep 29, 2009</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Information processing method and apparatus, and computer-readable program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7640254">US7640254</a></td><td class="patent-data-table-td patent-date-value">Jul 22, 2005</td><td class="patent-data-table-td patent-date-value">Dec 29, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Methods, apparatus, and data structures for annotating a database design schema and/or indexing annotations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7644082">US7644082</a></td><td class="patent-data-table-td patent-date-value">Mar 2, 2007</td><td class="patent-data-table-td patent-date-value">Jan 5, 2010</td><td class="patent-data-table-td ">Perfect Search Corporation</td><td class="patent-data-table-td ">Abbreviated index</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7653704">US7653704</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 5, 2003</td><td class="patent-data-table-td patent-date-value">Jan 26, 2010</td><td class="patent-data-table-td ">Gould Eric J</td><td class="patent-data-table-td ">System, method, and article of manufacture for seamless integrated searching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7660815">US7660815</a></td><td class="patent-data-table-td patent-date-value">Jun 30, 2006</td><td class="patent-data-table-td patent-date-value">Feb 9, 2010</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">Method and system for occurrence frequency-based scaling of navigation path weights among online content sources</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7664655">US7664655</a></td><td class="patent-data-table-td patent-date-value">Jan 8, 2007</td><td class="patent-data-table-td patent-date-value">Feb 16, 2010</td><td class="patent-data-table-td ">Allen Rosenthal</td><td class="patent-data-table-td ">Electronic service of process system and method for carrying out service of court papers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7680812">US7680812</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 16, 2005</td><td class="patent-data-table-td patent-date-value">Mar 16, 2010</td><td class="patent-data-table-td ">Telenor Asa</td><td class="patent-data-table-td ">Method, system, and computer program product for searching for, navigating among, and ranking of documents in a personal web</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7685192">US7685192</a></td><td class="patent-data-table-td patent-date-value">Jun 30, 2006</td><td class="patent-data-table-td patent-date-value">Mar 23, 2010</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">Method and system for displaying interest space user communities</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7698283">US7698283</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 22, 2002</td><td class="patent-data-table-td patent-date-value">Apr 13, 2010</td><td class="patent-data-table-td ">Primentia, Inc.</td><td class="patent-data-table-td ">System and method for organizing data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7702753">US7702753</a></td><td class="patent-data-table-td patent-date-value">Dec 13, 2005</td><td class="patent-data-table-td patent-date-value">Apr 20, 2010</td><td class="patent-data-table-td ">Accenture Global Services Gmbh</td><td class="patent-data-table-td ">Unified directory and presence system for universal access to telecommunications services</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7716256">US7716256</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 14, 2004</td><td class="patent-data-table-td patent-date-value">May 11, 2010</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Information navigation system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7739357">US7739357</a></td><td class="patent-data-table-td patent-date-value">Aug 29, 2006</td><td class="patent-data-table-td patent-date-value">Jun 15, 2010</td><td class="patent-data-table-td ">Eric Justin Gould</td><td class="patent-data-table-td ">System, method, and article of manufacture for seamless integrated searching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7743060">US7743060</a></td><td class="patent-data-table-td patent-date-value">Aug 6, 2007</td><td class="patent-data-table-td patent-date-value">Jun 22, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Architecture for an indexer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7765176">US7765176</a></td><td class="patent-data-table-td patent-date-value">Nov 13, 2006</td><td class="patent-data-table-td patent-date-value">Jul 27, 2010</td><td class="patent-data-table-td ">Accenture Global Services Gmbh</td><td class="patent-data-table-td ">Knowledge discovery system with user interactive analysis view for analyzing and generating relationships</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7769733">US7769733</a></td><td class="patent-data-table-td patent-date-value">Jan 28, 2008</td><td class="patent-data-table-td patent-date-value">Aug 3, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and computer program products to improve indexing of multidimensional databases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7774335">US7774335</a></td><td class="patent-data-table-td patent-date-value">Dec 29, 2005</td><td class="patent-data-table-td patent-date-value">Aug 10, 2010</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">Method and system for determining interest levels of online content navigation paths</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7774347">US7774347</a></td><td class="patent-data-table-td patent-date-value">Aug 30, 2007</td><td class="patent-data-table-td patent-date-value">Aug 10, 2010</td><td class="patent-data-table-td ">Perfect Search Corporation</td><td class="patent-data-table-td ">Vortex searching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7774353">US7774353</a></td><td class="patent-data-table-td patent-date-value">Aug 30, 2007</td><td class="patent-data-table-td patent-date-value">Aug 10, 2010</td><td class="patent-data-table-td ">Perfect Search Corporation</td><td class="patent-data-table-td ">Search templates</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7779038">US7779038</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 2007</td><td class="patent-data-table-td patent-date-value">Aug 17, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Reducing index size for multi-level grid indexes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7783626">US7783626</a></td><td class="patent-data-table-td patent-date-value">Aug 17, 2007</td><td class="patent-data-table-td patent-date-value">Aug 24, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Pipelined architecture for global analysis and index building</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7792811">US7792811</a></td><td class="patent-data-table-td patent-date-value">Dec 22, 2005</td><td class="patent-data-table-td patent-date-value">Sep 7, 2010</td><td class="patent-data-table-td ">Transaxtions Llc</td><td class="patent-data-table-td ">Intelligent search with guiding info</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7797421">US7797421</a></td><td class="patent-data-table-td patent-date-value">Dec 15, 2006</td><td class="patent-data-table-td patent-date-value">Sep 14, 2010</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">Method and system for determining and notifying users of undesirable network content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7801896">US7801896</a></td><td class="patent-data-table-td patent-date-value">Feb 19, 2007</td><td class="patent-data-table-td patent-date-value">Sep 21, 2010</td><td class="patent-data-table-td ">Andrew J Szabo</td><td class="patent-data-table-td ">Database access system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7801901">US7801901</a></td><td class="patent-data-table-td patent-date-value">Sep 15, 2006</td><td class="patent-data-table-td patent-date-value">Sep 21, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Tracking storylines around a query</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7809704">US7809704</a></td><td class="patent-data-table-td patent-date-value">Jun 15, 2006</td><td class="patent-data-table-td patent-date-value">Oct 5, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Combining spectral and probabilistic clustering</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7831582">US7831582</a></td><td class="patent-data-table-td patent-date-value">Dec 29, 2005</td><td class="patent-data-table-td patent-date-value">Nov 9, 2010</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">Method and system for associating keywords with online content sources</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7836082">US7836082</a></td><td class="patent-data-table-td patent-date-value">Jan 25, 2008</td><td class="patent-data-table-td patent-date-value">Nov 16, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Reducing index size for multi-level grid indexes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7853556">US7853556</a></td><td class="patent-data-table-td patent-date-value">May 2, 2008</td><td class="patent-data-table-td patent-date-value">Dec 14, 2010</td><td class="patent-data-table-td ">Accenture Global Services Limited</td><td class="patent-data-table-td ">Navigating a software project respository</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7856350">US7856350</a></td><td class="patent-data-table-td patent-date-value">Aug 11, 2006</td><td class="patent-data-table-td patent-date-value">Dec 21, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Reranking QA answers using language modeling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7860891">US7860891</a></td><td class="patent-data-table-td patent-date-value">Oct 20, 2005</td><td class="patent-data-table-td patent-date-value">Dec 28, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Reducing index size for multi-level grid indexes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7860895">US7860895</a></td><td class="patent-data-table-td patent-date-value">Dec 29, 2005</td><td class="patent-data-table-td patent-date-value">Dec 28, 2010</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">Method and system for determining interest spaces among online content sources</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7870113">US7870113</a></td><td class="patent-data-table-td patent-date-value">Sep 9, 2005</td><td class="patent-data-table-td patent-date-value">Jan 11, 2011</td><td class="patent-data-table-td ">Primentia, Inc.</td><td class="patent-data-table-td ">System and method for organizing data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7877697">US7877697</a></td><td class="patent-data-table-td patent-date-value">Oct 5, 2007</td><td class="patent-data-table-td patent-date-value">Jan 25, 2011</td><td class="patent-data-table-td ">Aol Inc.</td><td class="patent-data-table-td ">IM conversation counter and indicator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7900148">US7900148</a></td><td class="patent-data-table-td patent-date-value">May 5, 2008</td><td class="patent-data-table-td patent-date-value">Mar 1, 2011</td><td class="patent-data-table-td ">Aol Inc.</td><td class="patent-data-table-td ">E-mail interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7904411">US7904411</a></td><td class="patent-data-table-td patent-date-value">May 11, 2005</td><td class="patent-data-table-td patent-date-value">Mar 8, 2011</td><td class="patent-data-table-td ">Accenture Global Services Limited</td><td class="patent-data-table-td ">Knowledge discovery tool relationship generation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7908277">US7908277</a></td><td class="patent-data-table-td patent-date-value">Feb 5, 2007</td><td class="patent-data-table-td patent-date-value">Mar 15, 2011</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Annotating links in a document based on the ranks of documents pointed to by the links</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7912840">US7912840</a></td><td class="patent-data-table-td patent-date-value">Jun 20, 2008</td><td class="patent-data-table-td patent-date-value">Mar 22, 2011</td><td class="patent-data-table-td ">Perfect Search Corporation</td><td class="patent-data-table-td ">Indexing and filtering using composite data stores</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7913163">US7913163</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 22, 2004</td><td class="patent-data-table-td patent-date-value">Mar 22, 2011</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Determining semantically distinct regions of a document</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7941447">US7941447</a></td><td class="patent-data-table-td patent-date-value">Feb 24, 2010</td><td class="patent-data-table-td patent-date-value">May 10, 2011</td><td class="patent-data-table-td ">Mekiki Co., Ltd.</td><td class="patent-data-table-td ">Human relationships registering system and device for registering human relationships, program for registering human relationships, and medium storing human relationships registering program and readable by computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7953687">US7953687</a></td><td class="patent-data-table-td patent-date-value">Jul 26, 2010</td><td class="patent-data-table-td patent-date-value">May 31, 2011</td><td class="patent-data-table-td ">Accenture Global Services Limited</td><td class="patent-data-table-td ">Knowledge discovery system with user interactive analysis view for analyzing and generating relationships</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7953738">US7953738</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 2004</td><td class="patent-data-table-td patent-date-value">May 31, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">System and method for visualization of categories</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7966395">US7966395</a></td><td class="patent-data-table-td patent-date-value">Sep 28, 2005</td><td class="patent-data-table-td patent-date-value">Jun 21, 2011</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">System and method for indicating interest of online content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7991755">US7991755</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 17, 2004</td><td class="patent-data-table-td patent-date-value">Aug 2, 2011</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Dynamically ranking nodes and labels in a hyperlinked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8005858">US8005858</a></td><td class="patent-data-table-td patent-date-value">Sep 13, 2007</td><td class="patent-data-table-td patent-date-value">Aug 23, 2011</td><td class="patent-data-table-td ">Autonomy Corporation PLC</td><td class="patent-data-table-td ">Method and apparatus to link to a related document</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8010581">US8010581</a></td><td class="patent-data-table-td patent-date-value">Jul 18, 2008</td><td class="patent-data-table-td patent-date-value">Aug 30, 2011</td><td class="patent-data-table-td ">Accenture Global Services Limited</td><td class="patent-data-table-td ">Knowledge discovery tool navigation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8032495">US8032495</a></td><td class="patent-data-table-td patent-date-value">Jun 20, 2008</td><td class="patent-data-table-td patent-date-value">Oct 4, 2011</td><td class="patent-data-table-td ">Perfect Search Corporation</td><td class="patent-data-table-td ">Index compression</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8037075">US8037075</a></td><td class="patent-data-table-td patent-date-value">Sep 16, 2008</td><td class="patent-data-table-td patent-date-value">Oct 11, 2011</td><td class="patent-data-table-td ">Perfect Search Corporation</td><td class="patent-data-table-td ">Pattern index</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8126884">US8126884</a></td><td class="patent-data-table-td patent-date-value">Jan 28, 2010</td><td class="patent-data-table-td patent-date-value">Feb 28, 2012</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8131715">US8131715</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 2010</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8131717">US8131717</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 2010</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8131816">US8131816</a></td><td class="patent-data-table-td patent-date-value">Mar 14, 2003</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">Citrix Systems, Inc.</td><td class="patent-data-table-td ">Methods and apparatus for generating graphical and media displays at a client</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8131817">US8131817</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 23, 2008</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">Citrix Systems, Inc.</td><td class="patent-data-table-td ">Method and system for generating a graphical display for a remote terminal session</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8136025">US8136025</a></td><td class="patent-data-table-td patent-date-value">Jul 3, 2003</td><td class="patent-data-table-td patent-date-value">Mar 13, 2012</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Assigning document identification tags</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8176052">US8176052</a></td><td class="patent-data-table-td patent-date-value">Mar 2, 2007</td><td class="patent-data-table-td patent-date-value">May 8, 2012</td><td class="patent-data-table-td ">Perfect Search Corporation</td><td class="patent-data-table-td ">Hyperspace index</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8195651">US8195651</a></td><td class="patent-data-table-td patent-date-value">Feb 2, 2010</td><td class="patent-data-table-td patent-date-value">Jun 5, 2012</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8230364">US8230364</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 30, 2004</td><td class="patent-data-table-td patent-date-value">Jul 24, 2012</td><td class="patent-data-table-td ">Sony United Kingdom Limited</td><td class="patent-data-table-td ">Information retrieval</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8266152">US8266152</a></td><td class="patent-data-table-td patent-date-value">Aug 30, 2007</td><td class="patent-data-table-td patent-date-value">Sep 11, 2012</td><td class="patent-data-table-td ">Perfect Search Corporation</td><td class="patent-data-table-td ">Hashed indexing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8271498">US8271498</a></td><td class="patent-data-table-td patent-date-value">Aug 12, 2008</td><td class="patent-data-table-td patent-date-value">Sep 18, 2012</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Searching documents for ranges of numeric values</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8285724">US8285724</a></td><td class="patent-data-table-td patent-date-value">Dec 3, 2008</td><td class="patent-data-table-td patent-date-value">Oct 9, 2012</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and program for handling anchor text</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8285744">US8285744</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 2005</td><td class="patent-data-table-td patent-date-value">Oct 9, 2012</td><td class="patent-data-table-td ">Rockwell Automation Technologies, Inc.</td><td class="patent-data-table-td ">Indexing and searching manufacturing process related information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8286082">US8286082</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 12, 2008</td><td class="patent-data-table-td patent-date-value">Oct 9, 2012</td><td class="patent-data-table-td ">Citrix Systems, Inc.</td><td class="patent-data-table-td ">Methods and systems for providing, by a remote machine, access to a desk band associated with a resource executing on a local machine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8296304">US8296304</a></td><td class="patent-data-table-td patent-date-value">Jan 26, 2004</td><td class="patent-data-table-td patent-date-value">Oct 23, 2012</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method, system, and program for handling redirects in a search engine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8296666">US8296666</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 30, 2005</td><td class="patent-data-table-td patent-date-value">Oct 23, 2012</td><td class="patent-data-table-td ">Oculus Info. Inc.</td><td class="patent-data-table-td ">System and method for interactive visual representation of information content and relationships using layout and gestures</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8346759">US8346759</a></td><td class="patent-data-table-td patent-date-value">Aug 6, 2008</td><td class="patent-data-table-td patent-date-value">Jan 1, 2013</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Searching documents for ranges of numeric values</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8356036">US8356036</a></td><td class="patent-data-table-td patent-date-value">Feb 19, 2008</td><td class="patent-data-table-td patent-date-value">Jan 15, 2013</td><td class="patent-data-table-td ">Accenture Global Services</td><td class="patent-data-table-td ">Knowledge discovery tool extraction and integration</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8364667">US8364667</a></td><td class="patent-data-table-td patent-date-value">Mar 1, 2010</td><td class="patent-data-table-td patent-date-value">Jan 29, 2013</td><td class="patent-data-table-td ">Alibaba Group Holding Limited</td><td class="patent-data-table-td ">Evaluation of web pages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8370362">US8370362</a></td><td class="patent-data-table-td patent-date-value">Sep 14, 2010</td><td class="patent-data-table-td patent-date-value">Feb 5, 2013</td><td class="patent-data-table-td ">Alberti Anemometer Llc</td><td class="patent-data-table-td ">Database access system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8386509">US8386509</a></td><td class="patent-data-table-td patent-date-value">Jun 30, 2006</td><td class="patent-data-table-td patent-date-value">Feb 26, 2013</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">Method and system for associating search keywords with interest spaces</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8392417">US8392417</a></td><td class="patent-data-table-td patent-date-value">May 22, 2007</td><td class="patent-data-table-td patent-date-value">Mar 5, 2013</td><td class="patent-data-table-td ">David P. Gold</td><td class="patent-data-table-td ">System and method for organizing, processing and presenting information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8392426">US8392426</a></td><td class="patent-data-table-td patent-date-value">Mar 21, 2011</td><td class="patent-data-table-td patent-date-value">Mar 5, 2013</td><td class="patent-data-table-td ">Perfect Search Corporation</td><td class="patent-data-table-td ">Indexing and filtering using composite data stores</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8417693">US8417693</a></td><td class="patent-data-table-td patent-date-value">Jul 14, 2005</td><td class="patent-data-table-td patent-date-value">Apr 9, 2013</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Enforcing native access control to indexed documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8417700">US8417700</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 1, 2005</td><td class="patent-data-table-td patent-date-value">Apr 9, 2013</td><td class="patent-data-table-td ">Northrop Grumman Systems Corporation</td><td class="patent-data-table-td ">Interactive tool for constructing and editing process diagrams</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8429543">US8429543</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 2011</td><td class="patent-data-table-td patent-date-value">Apr 23, 2013</td><td class="patent-data-table-td ">Facebook, Inc.</td><td class="patent-data-table-td ">E-mail interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8458162">US8458162</a></td><td class="patent-data-table-td patent-date-value">Aug 11, 2011</td><td class="patent-data-table-td patent-date-value">Jun 4, 2013</td><td class="patent-data-table-td ">Kayak Software Corporation</td><td class="patent-data-table-td ">Purchase handoff in a travel search engine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8484290">US8484290</a></td><td class="patent-data-table-td patent-date-value">May 18, 2011</td><td class="patent-data-table-td patent-date-value">Jul 9, 2013</td><td class="patent-data-table-td ">Citrix Systems, Inc.</td><td class="patent-data-table-td ">Methods and systems for providing, by a remote machine, access to a desk band associated with a resource executing on a local machine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8484548">US8484548</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 7, 2007</td><td class="patent-data-table-td patent-date-value">Jul 9, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Anchor tag indexing in a web crawler system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8521730">US8521730</a></td><td class="patent-data-table-td patent-date-value">May 30, 2012</td><td class="patent-data-table-td patent-date-value">Aug 27, 2013</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8560519">US8560519</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 19, 2010</td><td class="patent-data-table-td patent-date-value">Oct 15, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Indexing and searching employing virtual documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8655888">US8655888</a></td><td class="patent-data-table-td patent-date-value">Dec 22, 2011</td><td class="patent-data-table-td patent-date-value">Feb 18, 2014</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Searching documents for ranges of numeric values</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8660977">US8660977</a></td><td class="patent-data-table-td patent-date-value">Jan 28, 2011</td><td class="patent-data-table-td patent-date-value">Feb 25, 2014</td><td class="patent-data-table-td ">Accenture Global Services Limited</td><td class="patent-data-table-td ">Knowledge discovery tool relationship generation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8671213">US8671213</a></td><td class="patent-data-table-td patent-date-value">Dec 23, 2011</td><td class="patent-data-table-td patent-date-value">Mar 11, 2014</td><td class="patent-data-table-td ">Citrix Systems, Inc.</td><td class="patent-data-table-td ">Methods and apparatus for generating graphical and media displays at a client</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8719255">US8719255</a></td><td class="patent-data-table-td patent-date-value">Sep 28, 2005</td><td class="patent-data-table-td patent-date-value">May 6, 2014</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">Method and system for determining interest levels of online content based on rates of change of content access</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8725726">US8725726</a></td><td class="patent-data-table-td patent-date-value">Sep 14, 2012</td><td class="patent-data-table-td patent-date-value">May 13, 2014</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20070130193">US20070130193</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 1, 2005</td><td class="patent-data-table-td patent-date-value">Jun 7, 2007</td><td class="patent-data-table-td ">Northrop Grumman Corporation</td><td class="patent-data-table-td ">Interactive tool for constructing and editing process diagrams</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090070687">US20090070687</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 12, 2008</td><td class="patent-data-table-td patent-date-value">Mar 12, 2009</td><td class="patent-data-table-td ">Richard James Mazzaferri</td><td class="patent-data-table-td ">Methods and Systems for Providing, by a Remote Machine, Access to a Desk Band Associated with a Resource Executing on a Local Machine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110040555">US20110040555</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 13, 2010</td><td class="patent-data-table-td patent-date-value">Feb 17, 2011</td><td class="patent-data-table-td ">Wegner Peter Juergen</td><td class="patent-data-table-td ">System and method for creating and playing timed, artistic multimedia representations of typed, spoken, or loaded narratives, theatrical scripts, dialogues, lyrics, or other linguistic texts</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110231386">US20110231386</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 19, 2010</td><td class="patent-data-table-td patent-date-value">Sep 22, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Indexing and searching employing virtual documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130073563">US20130073563</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 26, 2012</td><td class="patent-data-table-td patent-date-value">Mar 21, 2013</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Electronic computing device and image search method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130097494">US20130097494</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 17, 2011</td><td class="patent-data-table-td patent-date-value">Apr 18, 2013</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Method and system for visual cues to facilitate navigation through an ordered set of documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2006082096A1?cl=en">WO2006082096A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 6, 2006</td><td class="patent-data-table-td patent-date-value">Aug 10, 2006</td><td class="patent-data-table-td ">Accenture Global Services Gmbh</td><td class="patent-data-table-td ">Knowledge discovery tool relationship generation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2007001331A2?cl=en">WO2007001331A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 1, 2005</td><td class="patent-data-table-td patent-date-value">Jan 4, 2007</td><td class="patent-data-table-td ">Microsoft Corp</td><td class="patent-data-table-td ">Back-off mechanism for search</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2007082212A2?cl=en">WO2007082212A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 9, 2007</td><td class="patent-data-table-td patent-date-value">Jul 19, 2007</td><td class="patent-data-table-td ">Millett Ronald P</td><td class="patent-data-table-td ">Pattern index</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2007090106A2?cl=en">WO2007090106A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 30, 2007</td><td class="patent-data-table-td patent-date-value">Aug 9, 2007</td><td class="patent-data-table-td ">Axioma Inc</td><td class="patent-data-table-td ">Identifying and compensating for model mis-specification in factor risk models</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S737000">707/737</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc715/defs715.htm&usg=AFQjCNEsvPOacMHcG92b6Esmd6a_S2NVkg#C715S207000">715/207</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc715/defs715.htm&usg=AFQjCNEsvPOacMHcG92b6Esmd6a_S2NVkg#C715S205000">715/205</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc715/defs715.htm&usg=AFQjCNEsvPOacMHcG92b6Esmd6a_S2NVkg#C715S230000">715/230</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc715/defs715.htm&usg=AFQjCNEsvPOacMHcG92b6Esmd6a_S2NVkg#C715S206000">715/206</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999002">707/999.002</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999005">707/999.005</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999004">707/999.004</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999003">707/999.003</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S716000">707/716</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S748000">707/748</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0017300000">G06F17/30</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0007000000">G06F7/00</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99933">Y10S707/99933</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99934">Y10S707/99934</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99943">Y10S707/99943</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99932">Y10S707/99932</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99935">Y10S707/99935</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99945">Y10S707/99945</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30882">G06F17/30882</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30864">G06F17/30864</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30705">G06F17/30705</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30321">G06F17/30321</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30554">G06F17/30554</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=zwVVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30522">G06F17/30522</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06F17/30S4V</span>, <span class="nested-value">G06F17/30S4P7</span>, <span class="nested-value">G06F17/30S2P</span>, <span class="nested-value">G06F17/30W1</span>, <span class="nested-value">G06F17/30T4</span>, <span class="nested-value">G06F17/30W5H</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Sep 3, 2013</td><td class="patent-data-table-td ">IPR</td><td class="patent-data-table-td ">Aia trial proceeding filed before the patent and appeal board: inter partes review</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20130730</span></div><div class="nested-key-value"><span class="nested-key">Opponent name: </span><span class="nested-value">FACEBOOK, INC., LINKEDIN CORP., AND TWITTER, INC.</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">TRIAL NO: IPR2013-00481</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 8, 2012</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 4, 2011</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 12-15 IS CONFIRMED. CLAIMS 1, 5, 16, 21 AND 22 ARE DETERMINED TO BE PATENTABLE AS AMENDED. CLAIMS 3, 4, 6-11 AND 17-20, DEPENDENT ON AN AMENDED CLAIM, ARE DETERMINED TO BE PATENTABLE. NEW CLAIMS 23-32 ARE ADDED AND DETERMINED TO BE PATENTABLE. CLAIMS 2 WAS NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 17, 2010</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100525</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 1, 2010</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 1, 2010</td><td class="patent-data-table-td ">PRDP</td><td class="patent-data-table-td ">Patent reinstated due to the acceptance of a late maintenance fee</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100301</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 1, 2010</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 7, 2009</td><td class="patent-data-table-td ">FP</td><td class="patent-data-table-td ">Expired due to failure to pay maintenance fee</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090515</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 15, 2009</td><td class="patent-data-table-td ">REIN</td><td class="patent-data-table-td ">Reinstatement after maintenance fee payment confirmed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 24, 2008</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 4, 2008</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EGGER, DANIEL, NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SITE TECHNOLOGIES, INC.;SITE/TECHNOLOGIES/INC.;REEL/FRAME:021794/0648</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080813</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, LLC, NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:EGGER, DANIEL;REEL/FRAME:021794/0661</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080926</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EGGER, DANIEL,NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SITE TECHNOLOGIES, INC.;SITE/TECHNOLOGIES/INC.;US-ASSIGNMENT DATABASE UPDATED:20100504;REEL/FRAME:21794/648</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, LLC,NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:EGGER, DANIEL;US-ASSIGNMENT DATABASE UPDATED:20100504;REEL/FRAME:21794/661</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 13, 2007</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, LLC, NEW YORK</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:SOFTWARE RIGHTS ARCHIVE, INC.;REEL/FRAME:019714/0723</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20070518</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, LLC,NEW YORK</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:SOFTWARE RIGHTS ARCHIVE, INC.;US-ASSIGNMENT DATABASE UPDATED:20100504;REEL/FRAME:19714/723</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 13, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EGGER, DANIEL, NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">BILL OF SALE, ASSIGNMENT &amp; LICENSE AGREEMENT;ASSIGNOR:SITE TECHNOLOGIES, INC.;REEL/FRAME:018160/0500</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19980916</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 23, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, INC., NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:EGGER, MR. DANIEL;REEL/FRAME:015698/0357</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20050222</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, INC. 905 WEST MAIN STREET</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:EGGER, MR. DANIEL /AR;REEL/FRAME:015698/0357</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 1, 2004</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 8, 2004</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U1oW0YwA84thSNlhgC-qRJRQjaiWw\u0026id=zwVVBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U0nFaZ42VqwIWZm6etpqFiYhWGSbQ\u0026id=zwVVBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U1645SHzGF7XmhCaSz2sVPrkjoD-g","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Method_and_apparatus_for_indexing_search.pdf?id=zwVVBAABERAJ\u0026output=pdf\u0026sig=ACfU3U07bnRNmrequxYsJtVByZCsR6LGWg"},"sample_url":"http://www.google.com/patents/reader?id=zwVVBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>