<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6775404 - Apparatus and method for interactive 3D registration of ultrasound and ... - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_4ff636b3d23669b7103f3b3a3a18b4cd/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_4ff636b3d23669b7103f3b3a3a18b4cd__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Apparatus and method for interactive 3D registration of ultrasound and magnetic resonance images based on a magnetic position sensor"><meta name="DC.contributor" content="Niko Pagoulatos" scheme="inventor"><meta name="DC.contributor" content="David R. Haynor" scheme="inventor"><meta name="DC.contributor" content="Warren S. Edwards" scheme="inventor"><meta name="DC.contributor" content="Yongmin Kim" scheme="inventor"><meta name="DC.contributor" content="University Of Washington" scheme="assignee"><meta name="DC.date" content="2000-3-15" scheme="dateSubmitted"><meta name="DC.description" content="Intraoperative ultrasound (US) is integrated with stereotactic systems, where a system interactively registers two-dimensional (2D) US and three-dimensional (3D) magnetic resonance (MR) images. The registration is based on tracking a US probe with a bC magnetic position sensor. A transformation algorithm is performed to transform coordinates of points between two different spaces, where MR and US image spaces are independently registered with the position sensor space and where coordinate points can be registered between the MR and US spaces. A calibration procedure can be performed, and a phantom can be used to determine and analyze registration errors. The registered MR images can reconstructed using either zero-order or first-order interpolation."><meta name="DC.date" content="2004-8-10" scheme="issued"><meta name="DC.relation" content="US:5383454" scheme="references"><meta name="DC.relation" content="US:5517990" scheme="references"><meta name="DC.relation" content="US:5531520" scheme="references"><meta name="DC.relation" content="US:5682890" scheme="references"><meta name="DC.relation" content="US:5787886" scheme="references"><meta name="DC.relation" content="US:5891034" scheme="references"><meta name="DC.relation" content="US:6006126" scheme="references"><meta name="DC.relation" content="US:6351573" scheme="references"><meta name="DC.relation" content="US:RE30397" scheme="references"><meta name="citation_reference" content="A. Jodicke et al., &quot;Intraoperative three-dimensional ultrasonography: an approach to register brain shift using multidimensional image processing,&quot; Minimally Invasive Neurosurgery, vol. 41, pp. 13-19, 1998."><meta name="citation_reference" content="C. Kresmer et al., &quot;Image registration of MR and CT images using a frameless fiducial marker system,&quot; Magnetic Resonance Imaging, vol. 15, pp. 579-585, Nov. 5, 1997."><meta name="citation_reference" content="C. R. Maurer Jr. et al., &quot;Registration of head volume images using implantable fiducial markers,&quot; IEEE Transactions on Medical Imaging, vol. 16, pp. 447-462, Aug. 1997."><meta name="citation_reference" content="C. R. Maurer, Jr. et al., &quot;Registration of 3-D images using weighted geometrical features,&quot; IEEE Transactions on Medical Imaging, vol. 15, pp. 836-849, Dec. 1996."><meta name="citation_reference" content="D. F. Leotta et al., &quot;Performance of a miniature magnetic position sensor for three-dimensional ultrasonic imaging,&quot; Ultrasound in Medicine and Biology, vol. 23, pp. 597-609, 1997."><meta name="citation_reference" content="D. L. G. Hill et al., &quot;Estimation of intraoperative brain surface movement,&quot; in Proceedings of the CVRMed-MRCAS&#39;97, pp. 449-458, 1997."><meta name="citation_reference" content="H. Erbe et al., &quot;3-D ultrasonography and image matching for detection of brain shift during intracranial surgery,&quot; in Proceedings of the Computer Assisted Radiology, pp. 225-230, 1996."><meta name="citation_reference" content="H. Hirschberg et al., &quot;Incorporation of ultrasonic imaging in an optically coupled frameless stereotactic system,&quot; Acta Neurochir, vol. 68, pp. 75-80, 1997."><meta name="citation_reference" content="J. M. Fitzpatrick et al., &quot;Predicting error in rigid-body point-based registration,&quot; IEEE Transactions on Medical Imaging, vol. 17, pp. 694-702, Oct. 1998."><meta name="citation_reference" content="J. T. Lewis et al., &quot;An ultrasonic approach to localization of fiducial markers for interactive, image-guided neurosurgery-part I; principles,&quot; IEEE Transactions on Biomedical Engineering, vol. 45, pp. 621-630, May 1998."><meta name="citation_reference" content="J. T. Lewis et al., “An ultrasonic approach to localization of fiducial markers for interactive, image-guided neurosurgery—part I; principles,” IEEE Transactions on Biomedical Engineering, vol. 45, pp. 621-630, May 1998."><meta name="citation_reference" content="J. W. Trobaugh et al., &quot;Frameless stereotactic ultrasonography: method and applications,&quot; Computerized Medical Imaging and Graphics, vol. 18, pp. 235-246, Jul.-Aug. 1994."><meta name="citation_reference" content="K. K. Shung et al., &quot;Ultrasonic transducers and arrays&quot;, IEEE Engineering in Medicine and Biology Magazine, vol. 15, pp. 20-30, Nov.-Dec. 1996."><meta name="citation_reference" content="K. R. Smith et al., &quot;The neurostation-a highly accurate, minimally invasive solution to frameless stereotactic neurosurgery,&quot; Computerized Medical Imaging and Graphics, vol. 18, pp. 247-256, Jul.-Aug. 1994."><meta name="citation_reference" content="K. R. Smith et al., “The neurostation—a highly accurate, minimally invasive solution to frameless stereotactic neurosurgery,” Computerized Medical Imaging and Graphics, vol. 18, pp. 247-256, Jul.-Aug. 1994."><meta name="citation_reference" content="K. S. Arun et al., &quot;Least-squares fitting of two 3-D point sets,&quot; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 9, pp. 698-700, Sep. 1987."><meta name="citation_reference" content="M. D. Mitchell et al., &quot;Agarose as a tissue equivalent phantom material for NRM imaging,&quot; Magnetic Resonance Imaging, vol. 4, pp. 263-266, 1996."><meta name="citation_reference" content="M. S. Alp et al., &quot;Head registration techniques for image-guided surgery,&quot; Neurological Research, vol. 20, pp. 31-37, Jan. 1998."><meta name="citation_reference" content="N. Di-Lorenzo et al., &quot;A comparison of computerized tomography-guided sterotactic and ultrasound-guided techniques for brain biopsy,&quot; Journal of Neurosurgery, vol. 76, pp. 1044-1045, Nov. 1991."><meta name="citation_reference" content="N. Hata et al., &quot;Development of a frameless and armless stereotactic neuronavigation system with ultrasonographic registration,&quot; Neurosurgery, vol. 41, pp. 608-612, Sep. 1997."><meta name="citation_reference" content="N. Pagoulatos et al., &quot;Calibration and validation of free-hand 3D ultrasound systems based on DC magnetic tracking,&quot; in Proceedings of the SPIE, vol. 3335, pp. 59-71, 1998."><meta name="citation_reference" content="N.D. Kitchen et al., &quot;Accuracy in frame-based and frameless stereotaxy,&quot; Stereotactic Functional Neurosurgery, vol. 61, pp. 195-206, 1993."><meta name="citation_reference" content="P. Couillard et al., &quot;Focus on peroperative ultrasonography,&quot; Neurochirgurie, vol. 42, pp. 91-94, 1996."><meta name="citation_reference" content="P. R. Detmer et al., &quot;3D ultrasonic image feature localization based on magnetic scanhead tracking: in vitro calibration and validation,&quot; Ultrasound in Medicine and Biology, vol. 20, pp. 923-936, 1994."><meta name="citation_reference" content="R. A. Brown, &quot;A stereotactic head frame for use with CT body scanners,&quot; Investigative Radiology, vol. 14, pp. 300-304, Jul.-Aug. 1979."><meta name="citation_reference" content="R. D. Bucholz et al., &quot;The correction of stereotactic inaccuracy caused by brain shift using an intraoperative ultrasound device,&quot; in Proceedings of the CVRMed-MRCAS&#39;97, pp. 459-466, 1997."><meta name="citation_reference" content="R. L. Galloway, Jr., &quot;Frameless stereotatic systems,&quot; in Textbook of Sterotactic and Functional Neurosurgery, P. L. Gildenberg and R.R. Tasker, Eds., ch. 21, pp. 177-182, McGraw Hill, New York, 1998."><meta name="citation_reference" content="R. M. Comeau et al., &quot;Integrated MR and ultrasound imaging for improved image guidance in neurosurgery&quot;, in Proceedings of the SPIE, vol. 3338, pp. 747-754, Feb. 1998."><meta name="citation_reference" content="R. M. Comeau et al., &quot;Intraoperative US in interactive image-guided neurosurgery,&quot; Radiographics, vol. 18, pp. 1019-1027, Jul.-Aug. 1998."><meta name="citation_reference" content="R. W. Prager et al., &quot;Rapid calibration for 3D free-hand ultrasound,&quot; Ultrasound Med. Biol., vol. 24, pp. 855-869, Mar. 16, 1998."><meta name="citation_reference" content="S. J. Goerss et al., &quot;A sterotactic magnetic field digitizer,&quot; in Stereotactic and Functional Neurosurgery, vol. 63, pp. 89-92, 1994."><meta name="citation_reference" content="S. Lavallee, &quot;Registration for computer-integrated surgery: methodology, state of the art,&quot; in Computer-Integrated Surgery: Technology and Clinical Applications, pp. 77-97, MIT Press, Cambridge, MA, 1996."><meta name="citation_reference" content="V. M. Tronnier et al., &quot;Intraoperative diagnostic and interventional magnetic resonance imaging in neurosurgery,&quot; Neurosurgery, vol. 40, pp. 891-898, May 1997."><meta name="citation_reference" content="W. E. Butler et al., &quot;A mobile computed tomographic scanner with intraoperative and intensive care unit applications,&quot; Neurosurgery, vol. 42, pp. 1305-1310, Jun. 1998."><meta name="citation_reference" content="W. S. Edwards et al., &quot;PC-based workstation for three-dimensional visualization of ultrasound images,&quot; in Proceedings of the SPIE, vol. 3031, pp. 1163-1176, 1995."><meta name="citation_reference" content="Y. Kim et al., &quot;Programmable ultrasound imaging using multimedia technologies: A next-generation ultrasound machine,&quot; IEEE Transactions on Information Technology in Biomedicine, vol. 1, pp. 19-29, Mar. 1997."><meta name="citation_patent_number" content="US:6775404"><meta name="citation_patent_application_number" content="US:09/526,656"><link rel="canonical" href="http://www.google.com/patents/US6775404"/><meta property="og:url" content="http://www.google.com/patents/US6775404"/><meta name="title" content="Patent US6775404 - Apparatus and method for interactive 3D registration of ultrasound and magnetic resonance images based on a magnetic position sensor"/><meta name="description" content="Intraoperative ultrasound (US) is integrated with stereotactic systems, where a system interactively registers two-dimensional (2D) US and three-dimensional (3D) magnetic resonance (MR) images. The registration is based on tracking a US probe with a bC magnetic position sensor. A transformation algorithm is performed to transform coordinates of points between two different spaces, where MR and US image spaces are independently registered with the position sensor space and where coordinate points can be registered between the MR and US spaces. A calibration procedure can be performed, and a phantom can be used to determine and analyze registration errors. The registered MR images can reconstructed using either zero-order or first-order interpolation."/><meta property="og:title" content="Patent US6775404 - Apparatus and method for interactive 3D registration of ultrasound and magnetic resonance images based on a magnetic position sensor"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("L-LoU9CfAoyxyASv2oFw"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407291699.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("LUX"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("L-LoU9CfAoyxyASv2oFw"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407291699.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("LUX"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6775404?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6775404"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=ZOJnBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6775404&amp;usg=AFQjCNHMJyKSEwCZUR7fmmJCiggI6-F0IA" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6775404.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6775404.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6775404" style="display:none"><span itemprop="description">Intraoperative ultrasound (US) is integrated with stereotactic systems, where a system interactively registers two-dimensional (2D) US and three-dimensional (3D) magnetic resonance (MR) images. The registration is based on tracking a US probe with a bC magnetic position sensor. A transformation algorithm...</span><span itemprop="url">http://www.google.com/patents/US6775404?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6775404 - Apparatus and method for interactive 3D registration of ultrasound and magnetic resonance images based on a magnetic position sensor</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6775404 - Apparatus and method for interactive 3D registration of ultrasound and magnetic resonance images based on a magnetic position sensor" title="Patent US6775404 - Apparatus and method for interactive 3D registration of ultrasound and magnetic resonance images based on a magnetic position sensor"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6775404 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/526,656</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Aug 10, 2004</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Mar 15, 2000</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Mar 18, 1999</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">09526656, </span><span class="patent-bibdata-value">526656, </span><span class="patent-bibdata-value">US 6775404 B1, </span><span class="patent-bibdata-value">US 6775404B1, </span><span class="patent-bibdata-value">US-B1-6775404, </span><span class="patent-bibdata-value">US6775404 B1, </span><span class="patent-bibdata-value">US6775404B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Niko+Pagoulatos%22">Niko Pagoulatos</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22David+R.+Haynor%22">David R. Haynor</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Warren+S.+Edwards%22">Warren S. Edwards</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Yongmin+Kim%22">Yongmin Kim</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22University+Of+Washington%22">University Of Washington</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6775404.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6775404.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6775404.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (9),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (36),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (57),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (20),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (6)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6775404&usg=AFQjCNEI44BblWnoBcWZAXmqLTC4nUAamQ">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6775404&usg=AFQjCNGsH_3nlvVeoTjR8hCfd7LUmWCerg">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6775404B1%26KC%3DB1%26FT%3DD&usg=AFQjCNH69YWxIHA1z-FVd4M402ILfNjRtw">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55301567" lang="EN" load-source="patent-office">Apparatus and method for interactive 3D registration of ultrasound and magnetic resonance images based on a magnetic position sensor</invention-title></span><br><span class="patent-number">US 6775404 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA50704456" lang="EN" load-source="patent-office"> <div class="abstract">Intraoperative ultrasound (US) is integrated with stereotactic systems, where a system interactively registers two-dimensional (2D) US and three-dimensional (3D) magnetic resonance (MR) images. The registration is based on tracking a US probe with a bC magnetic position sensor. A transformation algorithm is performed to transform coordinates of points between two different spaces, where MR and US image spaces are independently registered with the position sensor space and where coordinate points can be registered between the MR and US spaces. A calibration procedure can be performed, and a phantom can be used to determine and analyze registration errors. The registered MR images can reconstructed using either zero-order or first-order interpolation.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(6)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6775404B1/US06775404-20040810-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6775404B1/US06775404-20040810-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6775404B1/US06775404-20040810-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6775404B1/US06775404-20040810-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6775404B1/US06775404-20040810-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6775404B1/US06775404-20040810-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(19)</span></span></div><div class="patent-text"><div mxw-id="PCLM8714499" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is: </claim-statement> <div class="claim"> <div num="1" id="US-6775404-B1-CLM-00001" class="claim">
      <div class="claim-text">1. A method, comprising:</div>
      <div class="claim-text">registering a coordinate space associated with images of a first modality to a coordinate space of a magnetic position sensor, to obtain a first transformation; </div>
      <div class="claim-text">registering a coordinate space associated with images of a second modality to the coordinate space of the magnetic position sensor, to obtain a second transformation; and </div>
      <div class="claim-text">converting coordinates of images associated with one of the modalities to coordinates of images associated with the other one of the modalities based on the first and second transformations, </div>
      <div class="claim-text">wherein the magnetic position sensor comprises a receiver and a transmitter, the method further comprising: </div>
      <div class="claim-text">transforming the coordinate space associated with images of the second modality to a coordinate space of the receiver; </div>
      <div class="claim-text">transforming the coordinate space of the receiver to a coordinate space of the transmitter; and </div>
      <div class="claim-text">transforming the coordinate space of the transmitter to the coordinate space associated with images of the first modality. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" id="US-6775404-B1-CLM-00002" class="claim">
      <div class="claim-text">2. The method of <claim-ref idref="US-6775404-B1-CLM-00001">claim 1</claim-ref> wherein the first modality comprises a magnetic resonance system.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" id="US-6775404-B1-CLM-00003" class="claim">
      <div class="claim-text">3. The method of <claim-ref idref="US-6775404-B1-CLM-00001">claim 1</claim-ref> wherein the second modality comprises an ultrasound system.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" id="US-6775404-B1-CLM-00004" class="claim">
      <div class="claim-text">4. The method of <claim-ref idref="US-6775404-B1-CLM-00001">claim 1</claim-ref>, further comprising superimposing images associated with the first and second modalities based on the converted coordinates.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" id="US-6775404-B1-CLM-00005" class="claim">
      <div class="claim-text">5. The method of <claim-ref idref="US-6775404-B1-CLM-00001">claim 1</claim-ref> wherein registering the coordinate space associated with images of the first modality to the coordinate space of the magnetic position sensor comprises:</div>
      <div class="claim-text">positioning point fiducial markers adjacent to a target space, the point fiducial markers having first coordinates belonging to the coordinate space associated with images of the first modality; </div>
      <div class="claim-text">localizing the first coordinates of the point fiducial markers to corresponding coordinates of the magnetic position sensor, to obtain second coordinates; and </div>
      <div class="claim-text">deriving the first transformation based on the first and second coordinates. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" id="US-6775404-B1-CLM-00006" class="claim">
      <div class="claim-text">6. The method of <claim-ref idref="US-6775404-B1-CLM-00001">claim 1</claim-ref> wherein the magnetic position sensor comprises a transmitter and a receiver mounted on an object, the method further comprising:</div>
      <div class="claim-text">calibrating the magnetic position sensor by determining coordinates of a point on the object with respect to a coordinate space of the receiver; </div>
      <div class="claim-text">determining the coordinates of the point on the object with respect to a coordinate space of the transmitter by using a rigid-body transformation. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" id="US-6775404-B1-CLM-00007" class="claim">
      <div class="claim-text">7. The method of <claim-ref idref="US-6775404-B1-CLM-00001">claim 1</claim-ref>, further comprising using an interpolation method to determine coordinate point intensity in images obtained using the first and second transformations.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" id="US-6775404-B1-CLM-00008" class="claim">
      <div class="claim-text">8. The method of <claim-ref idref="US-6775404-B1-CLM-00001">claim 1</claim-ref>, further comprising determining a registration error.</div>
    </div>
    </div> <div class="claim"> <div num="9" id="US-6775404-B1-CLM-00009" class="claim">
      <div class="claim-text">9. An apparatus, comprising:</div>
      <div class="claim-text">a magnetic position sensor; and </div>
      <div class="claim-text">a control unit coupled to the magnetic position sensor and to a processor, the control unit being capable of cooperating with the processor to obtain a first transformation by registering a coordinate space associated with images of a first modality to a coordinate space of the magnetic position sensor, the control unit being capable of cooperating with the processor to obtain a second transformation by registering a coordinate space associated with images of a second modality to the coordinate space of the magnetic position sensor, the processor being capable of converting coordinates of images associated with one of the modalities to coordinates of images associated with the other one of the modalities based on the first and second transformations, </div>
      <div class="claim-text">wherein the magnetic position sensor comprises a transmitter and a receiver mounted on an object, the control unit being capable of cooperating with the processor to transform the coordinate space associated with images of the second modality to a coordinate space of the receiver, to transform the coordinate space of the receiver to a coordinate space of the transmitter, and to transform the coordinate space of the transmitter to the coordinate space associated with images of the first modality. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" id="US-6775404-B1-CLM-00010" class="claim">
      <div class="claim-text">10. The apparatus of <claim-ref idref="US-6775404-B1-CLM-00009">claim 9</claim-ref>, further comprising a graphical interface unit capable of superimposing images associated with the first and second modalities based on the converted coordinates.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" id="US-6775404-B1-CLM-00011" class="claim">
      <div class="claim-text">11. The apparatus of <claim-ref idref="US-6775404-B1-CLM-00009">claim 9</claim-ref> wherein the magnetic position sensor comprises a transmitter and a receiver mounted on an object, the control unit being capable of cooperating with the processor to calibrate the magnetic position sensor by determining coordinates of a point on the object with respect to a coordinate space of the receiver and to determine the coordinates of the point on the object with respect to a coordinate space of the transmitter by using a rigid-body transformation.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" id="US-6775404-B1-CLM-00012" class="claim">
      <div class="claim-text">12. The apparatus of <claim-ref idref="US-6775404-B1-CLM-00009">claim 9</claim-ref> wherein the magnetic position sensor comprises a transmitter and a receiver mounted to an ultrasound probe.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" id="US-6775404-B1-CLM-00013" class="claim">
      <div class="claim-text">13. The apparatus of <claim-ref idref="US-6775404-B1-CLM-00009">claim 9</claim-ref>, further comprising an ultrasound scanner communicatively coupled to the processor to generate images associated with the second modality, the control unit being capable of cooperating with the processor to continuously change the second transformation while the ultrasound scanner generates images as the probe moves freely in a three-dimensional space.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" id="US-6775404-B1-CLM-00014" class="claim">
      <div class="claim-text">14. The apparatus of <claim-ref idref="US-6775404-B1-CLM-00009">claim 9</claim-ref>, further comprising a localization tool coupled to the magnetic sensor, the control unit being capable of cooperating with the processor to obtain the first transformation by localizing coordinates of a point on the localization tool with corresponding coordinates of the magnetic position sensor.</div>
    </div>
    </div> <div class="claim"> <div num="15" id="US-6775404-B1-CLM-00015" class="claim">
      <div class="claim-text">15. A method of calibrating a magnetic position sensor having a receiver and a transmitter, the method comprising:</div>
      <div class="claim-text">obtaining a first coordinate transformation between coordinate spaces of the receiver and transmitter; </div>
      <div class="claim-text">obtaining a second coordinate transformation between the coordinate space of the transmitter and a coordinate space of a target region, based on point fiducials positioned adjacent to the target region; </div>
      <div class="claim-text">obtaining a third coordinate transformation between a coordinate space of images associated with an imaging modality and the coordinate space of the target region, based on N-fiducials positioned within the target region; and </div>
      <div class="claim-text">based on the first, second, and third coordinate transformations, calculating a fourth coordinate transformation to associate the coordinate space of images associated with the imaging modality with the coordinate space of the receiver. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" id="US-6775404-B1-CLM-00016" class="claim">
      <div class="claim-text">16. The method of <claim-ref idref="US-6775404-B1-CLM-00015">claim 15</claim-ref> wherein the imaging modality comprises a two dimensional ultrasound system and the coordinate space of the target region comprises a three-dimensional space.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" id="US-6775404-B1-CLM-00017" class="claim">
      <div class="claim-text">17. The method of <claim-ref idref="US-6775404-B1-CLM-00015">claim 15</claim-ref> wherein the imaging modality comprises a three dimensional ultrasound system and the coordinate space of the target region comprises a three-dimensional space.</div>
    </div>
    </div> <div class="claim"> <div num="18" id="US-6775404-B1-CLM-00018" class="claim">
      <div class="claim-text">18. A method, comprising:</div>
      <div class="claim-text">registering a coordinate space associated with images of a first modality to a coordinate space of a magnetic position sensor, to obtain a first transformation; </div>
      <div class="claim-text">registering a coordinate space associated with images of a second modality to the coordinate space of the magnetic position sensor, to obtain a second transformation; and </div>
      <div class="claim-text">converting coordinates of images associated with one of the modalities to coordinates of images associated with the other one of the modalities based on the first and second transformations, </div>
      <div class="claim-text">wherein the position sensor comprises a receiver and a transmitter and wherein registering the coordinate space associated with images of the second modality to the coordinate space of the magnetic position sensor comprises: </div>
      <div class="claim-text">performing a transformation of coordinates of images associated with the second modality to the coordinate space of the receiver; and </div>
      <div class="claim-text">performing a rigid-body transformation from the coordinate space of the receiver to the coordinate space of the transmitter. </div>
    </div>
    </div> <div class="claim"> <div num="19" id="US-6775404-B1-CLM-00019" class="claim">
      <div class="claim-text">19. A method, comprising:</div>
      <div class="claim-text">registering a coordinate space associated with images of a first modality to a coordinate space of a magnetic position sensor, to obtain a first transformation; </div>
      <div class="claim-text">registering a coordinate space associated with images of a second modality to the coordinate space of the magnetic position sensor, to obtain a second transformation; and </div>
      <div class="claim-text">converting coordinates of images associated with one of the modalities to coordinates of images associated with the other one of the modalities based on the first and second transformations, </div>
      <div class="claim-text">wherein the magnetic position sensor includes a receiver and a transmitter, the method further comprising performing a calibration operation comprising: </div>
      <div class="claim-text">obtaining a first coordinate transformation between coordinate spaces of the receiver and transmitter; </div>
      <div class="claim-text">obtaining a second coordinate transformation between the coordinate space of the transmitter and a coordinate space of a target region, based on point fiducials positioned adjacent to the target region; </div>
      <div class="claim-text">obtaining a third coordinate transformation between a coordinate space of images associated with the second modality and the coordinate space of the target region, based on N-fiducials positioned within the target region; and </div>
      <div class="claim-text">based on the first, second, and third coordinate transformations, calculating a fourth coordinate transformation to associate the coordinate space of images associated with the second modality with the coordinate space of the receiver.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES54278667" lang="EN" load-source="patent-office" class="description">
    <heading>CROSS-REFERENCE TO RELATED APPLICATIONS</heading> <p>This application claims the benefit of U.S. Provisional Patent Application Serial No. 60/125,017, filed Mar. 18, 1999, entitled “INTERACTIVE 3D REGISTRATION OF ULTRASOUND AND MAGNETIC RESONANCE IMAGES BASED ON A MAGNETIC POSITION SENSOR,” and U.S. Provisional Patent Application Serial No. 60/135,065, filed May 20, 1999, entitled “FAST CALIBRATION FOR 3D ULTRASOUND IMAGING AND MULTIMODALITY IMAGE REGISTRATION,” both currently pending and incorporated by reference.</p>
    <heading>BACKGROUND OF THE INVENTION</heading> <p>1. Field of the Invention</p>
    <p>The present invention relates generally to image processing, and in particular, relates to interactively registering ultrasound and magnetic resonance images.</p>
    <p>2. Background Information</p>
    <p>Accurate guidance and localization of the surgical tool within the brain is essential for the success of various neurosurgical procedures, such as biopsy or tumor resection. In addition, minimum interference of the surgical tool with healthy brain tissues reduces the risk of postoperative complications for the patient. The location of the brain within the skull and its dense nature prevent the direct visualization of the surgical tool and the associated structures. To address these problems in neurosurgery, stereotactic systems have been introduced.</p>
    <p>Stereotactic systems provide guidance to the surgeon based on preoperative tomographic images, such as computed tomography (CT) and magnetic resonance (MR) images. The first stereotactic systems were based on specially designed frames (called “stereotactic frames”) that were attached to the patient's head both during the preoperative image scan and during the surgery. These stereotactic frames have an inherent three-dimensional (3D) coordinate system, which is associated, through a coordinate transformation, with the preoperative image coordinate system. Based on the preoperative images, surgeons select the target and the surgical path, and refer to the coordinate system of the stereotactic frame to perform the craniotomy and surgery. Stereotactic frames provide high accuracy, but they have several disadvantages:</p>
    <p>They are bulky and interfere with the surgical procedure;</p>
    <p>Surgical path planning and target localization in the stereotactic frame coordinates are time-consuming and tedious;</p>
    <p>There is no real-time feedback on the preoperative images; and</p>
    <p>They are invasive.</p>
    <p>With advances in sensing and computing technologies, a new generation of frameless stereotactic systems has been developed. These systems use a position sensor (usually optical) to interactively track the position of the surgical tool during the course of surgery. Interactive display of the preoperative images showing the location of the surgical tool provides the surgeon with real-time feedback. Frameless stereotactic systems are easier to use compared to the frame-based stereotactic systems. In addition, there is no bulky equipment involved. Depending on the method used for registering image and physical (e.g., surgical) space, they can be minimally invasive or even non-invasive. A main limitation associated with both frame-based and frameless stereotactic systems is that the intraoperative surgical guidance is based on preoperative images. Thus, if the brain shifts with respect to the skull or deforms during surgery, the guidance becomes inaccurate. Brain shifts or deformations can be caused by surgical manipulations or cerebrospinal fluid flowing out after the craniotomy.</p>
    <p>Intraoperative brain imaging has been an alternative solution in providing the surgeon with visual feedback during surgery. Several imaging modalities have been used intraoperatively. These include CT, MR, and ultrasound (US). Intraoperative US has been widely used compared to intraoperative CT and MR imaging because it is: (1) relatively safe (non-ionizing radiation is used), (2) relatively inexpensive, (3) reasonably easy to use in the operating room, and (4) provides a high update rate. However, the problem with US imaging is its low signal-to-noise ratio (SNR) due to the physics associated with the formation of US images. Moreover, US images contain errors associated with the variation of the speed of sound in different media and the low resolution in the axis perpendicular to the US plane (e.g., azimuthal resolution). Therefore, accurate target localization cannot be solely based on US images. When high accuracy is needed, stereotactic systems currently provide an available option, provided that no brain shifts and deformations occur intraoperatively. Finally, the numerous positions and orientations of the US image planes with respect to the skull, combined with their low SNR, make it difficult for the neurosurgeons to interpret the intraoperative US images and associate them with known brain structures.</p>
    <p>Recently, several researchers have tried to integrate intraoperative US images with stereotactic systems. The motivation behind this approach is to combine the real-time intraoperative information contained in US images with the rich anatomical content of preoperative MR/CT images. The main approach for performing this integration has been to use the patient's skull as a reference in order to register each two-dimensional (2D) US image with the preoperative 3D MR/CT images, where a position sensor is used to track the position and orientation of the US probe in 3D space. An articulated arm, an optical position sensor and an ultrasonic sensor have been used. This US-MR/CT registration enables reconstruction of the 2D preoperative MR/CT images of the brain with the same position, orientation and scaling as the intraoperative 2D US images.</p>
    <p>Although comparison of these corresponding US and MR/CT images provides the surgeon with (i) better assessment of the orientation and content of the intraoperative US images and (ii) easier visualization of the intraoperative changes in the brain, the existing integration methods of US systems with stereotactic systems suffer from a number of drawbacks. For example, the optical position sensors require a line of sight. Further, the equipment used by existing methods is cumbersome, complex, and expensive.</p>
    <p>Accordingly, there is a need for improved methods of registering US and MR images.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>According to one aspect of the invention, a method registers a coordinate space associated with images of a first modality to a coordinate space of a magnetic position sensor, to obtain a first transformation. A coordinate space associated with images of a second modality is registered to the coordinate space of the magnetic position sensor, to obtain a second transformation. The method converts coordinates of images associated with one of the modalities to coordinates of images associated with the other one of the modalities based on the first and second transformations.</p>
    <p>Another aspect of the invention provides a calibration method.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p>Non-limiting and non-exhaustive embodiments of the present invention will be described in the following figures, wherein like reference numerals refer to like parts throughout the various views unless otherwise specified.</p>
    <p>FIG. 1 shows an embodiment of a system according to an embodiment of the invention.</p>
    <p>FIG. 2 shows an embodiment of a phantom that can be used in conjunction with the system of FIG. <b>1</b>.</p>
    <p>FIGS. <b>3</b>(<i>a</i>) and <b>3</b>(<i>b</i>) are MR images of the phantom shown in FIG. <b>2</b>.</p>
    <p>FIG. 4 shows an embodiment of a localization tool that can be used for the system of FIG. 1 to determine coordinates of the phantom of FIG. 2 in a transmitter coordinate space.</p>
    <p>FIG. 5 illustrates a method for determining registration error for the system of FIG. <b>1</b>.</p>
    <p>FIG. 6 is a schematic diagram illustrating the relationship of US and MR images for the system of FIG. <b>1</b>.</p>
    <p>FIGS. <b>7</b>(<i>a</i>) and <b>7</b>(<i>b</i>) visually illustrate a registration of the localization tool of FIG. 4 to real-time MR images.</p>
    <p>FIGS. <b>8</b>(<i>a</i>) and <b>8</b>(<i>b</i>) are US and MR images, respectively, produced by the system of FIG. <b>1</b>.</p>
    <p>FIGS. <b>9</b>(<i>a</i>)-<b>9</b>(<i>c</i>) are US, MR, and superimposed images, respectively, produced by the system of FIG. <b>1</b>.</p>
    <p>FIGS. <b>10</b>(<i>a</i>)-<b>10</b>(<i>c</i>) are tables listing illustrative registration error data for the system of FIG. <b>1</b>.</p>
    <p>FIGS. <b>11</b>(<i>a</i>) and <b>11</b>(<i>b</i>) show an example of a phantom that can be used in conjunction with an embodiment of a calibration procedure of the invention.</p>
    <p>FIG. 12 illustrates geometric configurations and geometric transformations that can be used in conjunction with an embodiment of a calibration procedure and with the phantom of FIGS. <b>11</b>(<i>a</i>) and <b>11</b>(<i>b</i>).</p>
    <heading>DETAILED DESCRIPTION OF THE ILLUSTRATED EMBODIMENTS</heading> <p>Embodiments of an apparatus and method for interactive 3D registration of US and MR images are described in detail herein. According to one embodiment, an interactive frameless stereotactic system integrates US with MR images using a low-cost and easy-to-use position sensor (e.g., a DC magnetic position sensor), with the system having improved registration accuracy. In the following description, numerous specific details are provided, such as the description of a phantom in FIG. 2, to provide a thorough understanding of embodiments of the invention. One skilled in the relevant art will recognize, however, that the invention can be practiced without one or more of the specific details, or with other methods, components, etc. In other instances, well-known structures or operations are not shown or described in detail to avoid obscuring aspects of various embodiments of the invention.</p>
    <p>Reference throughout this specification to “one embodiment” or “an embodiment” means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Thus, the appearances of the phrases “in one embodiment” or “in an embodiment” in various places throughout this specification are not necessarily all referring to the same embodiment. Furthermore, the particular features, structures, or characteristics may be combined in any suitable manner in one or more embodiments.</p>
    <heading>I. Illustrative System and Methods</heading> <p>A. Overview of Algorithm and System</p>
    <p>According to one embodiment of an algorithm, registration of the US and MR images is based on a position sensor. The position sensor serves as a link between the 2D US and the 3D MR image coordinate systems. A position sensor (or 3D digitizer) has a 3D coordinate system embedded in it to serve as a reference between various image and physical coordinate systems.</p>
    <p>Physical coordinate systems are inherent in physical objects, such as a phantom, a stereotactic frame, or a 3D digitizer. In general, a calibration is performed before the association between various geometric spaces can be established through the position sensor. The calibration method may be based on the specific position sensor and the geometric spaces to be associated, with an objective of both calibration and registration procedures being the establishment of a geometric transformation between two coordinate systems.</p>
    <p>Typically, the term “registration” may be used when the two coordinate systems are completely independent of each other (e.g., MR and US images), and the term “calibration” may be used when the coordinate systems are rigidly connected to the same object (e.g., a US image and position sensor mounted on a US probe). An embodiment of the invention independently registers both the MR and US image spaces with the position sensor reference space. A different calibration procedure may be applied for each registration. After the necessary calibrations and registrations have been performed, the US image pixel coordinates can be interactively transformed to their corresponding MR coordinates and vice versa.</p>
    <p>FIG. 1 illustrates components of a system <b>10</b> according to an embodiment of the invention. The system <b>10</b> includes a commercially available US scanner <b>12</b> (such as a Siemens SONOLINE Elegra), a DC magnetic position sensor <b>14</b> (such as a Flock of Birds 6DFOB manufactured by Ascension Technology Corporation), and a personal computer (PC) <b>16</b> (such as a 450-MHz Pentium II PC). The PC <b>16</b> is equipped with a video capture card <b>18</b> (such as a Matrox Meteor-II). The system <b>10</b> may use a 3.5-MHz phased 1-D array transducer as part of a US probe <b>30</b>. In addition, the PC <b>16</b> may use an integrated graphical user interface (GUI) <b>20</b>, utilizing Microsoft Visual C++ for example, where the registration and calibration methods described in the following sections can be performed.</p>
    <p>The position sensor <b>14</b> includes a transmitter <b>22</b>, a receiver <b>24</b>, and a control unit <b>26</b>. The control unit <b>26</b> is connected to the PC <b>16</b> through a serial port <b>28</b>, such as an RS232 port. The transmitter <b>22</b> can remain stationary (as a reference coordinate system), and the receiver <b>24</b> is mounted on an object (e.g., a surgical tool or the US probe <b>30</b>) to be tracked. As the receiver <b>24</b> moves along with the object <b>30</b>, the control unit <b>26</b> computes a rigid-body transformation that associates the coordinate systems of the transmitter <b>22</b> and the receiver <b>24</b>. This computation is based, for example, on three pulsed DC magnetic fields that are sequentially emitted by the transmitter <b>22</b> and sensed by the receiver <b>24</b>. Object tracking uses a calibration procedure to determine a geometric relation of the receiver <b>24</b> and the object <b>30</b>. Once this calibration has been performed, the various locations of the tracked object <b>30</b> with respect to the transmitter's <b>22</b> reference coordinate system can be determined. The control unit <b>26</b>, the PC <b>16</b>, or both can be used to perform the various registration, calibration, registration error analysis, and other functions described later below.</p>
    <p>Referring next to FIG. 2, a phantom <b>32</b> is shown, comprising a plastic container <b>34</b> (having dimensions of 26 cm in length, 15 cm in width, and 10 cm in height, for example) filled with a 4-mM copper sulfate distilled water solution. The phantom <b>32</b> can be used in a laboratory setting to perform registration and calibration procedures, and it is understood that principles of these procedures (and their results) can be applied in a neurosurgical operating room setting. Within the phantom <b>32</b>, embedded plastic objects, such as spheres <b>36</b> and beads <b>38</b>, can be identified both in US and MR images. The plastic beads <b>38</b> can be 2.5 mm in diameter and can be used to quantitatively measure registration error. The other larger objects, such as the spheres <b>36</b>, are used to visually identify the alignment of large structures in the original US and registered MR images. The phantom <b>32</b> further includes multiple markers <b>40</b> attached along the circumference of the container <b>34</b>.</p>
    <p>FIGS. <b>3</b>(<i>a</i>) and <b>3</b>(<i>b</i>) show MR image data of the phantom <b>32</b>. FIG. <b>3</b>(<i>a</i>) is an image among a set of coronal images with 1.07 mm×1.07 mm in-plane resolution and 3-mm slice thickness (0.3-mm gap between consecutive slices). FIG. <b>3</b>(<i>b</i>) is an image among a set of axial images with 0.78 mm×0.78 mm in-plane resolution and 4-mm slice thickness (1-mm gap between consecutive slices). The copper sulfate solution in the phantom <b>32</b> gives a bright signal, whereas the plastic objects (such as the spheres <b>36</b>) give no signal. This contrast is the reverse from one obtained in a US image. Furthermore in FIG. <b>3</b>(<i>a</i>), one can observe the strong signal generated from three of the multiple markers <b>40</b> that are attached along the circumference of the phantom <b>32</b>. These markers <b>40</b> can be used for registering the MR with the transmitter <b>22</b> space. As shown in FIG. 2, four markers <b>40</b> may be symmetrically placed in the upper four corners of the phantom <b>32</b> and another four markers <b>40</b> may be similarly placed in the lower four corners.</p>
    <p>B. Registration of MR and Transmitter Space</p>
    <p>One approach taken for registering an image to a position sensor space is the use of distinct features that can be accurately identified in both spaces. These distinct features can be specially designed landmarks (sometimes referred to as “markers” or “fiducials”) or natural anatomical landmarks. Usually, the former has been used due to the difficulty in accurately identifying the latter. One kind of fiducial that has been widely used in medical image registration provides pairs of corresponding 3D points in the spaces to be registered. Typical fiducials of this kind are hollow spheres or cylinders filled with some substance that produce a strong signal in the considered imaging modality. The hollow part of the marker is the part that shows up in the images. In addition, with the use of a special tool, the same fiducials can be localized (e.g., computation of the fiducial's 3D coordinates) in the position sensor space. Once the 3D coordinates of the point fiducials have been determined in the two coordinate spaces, the geometric transformation that associates these two spaces can be derived.</p>
    <p>According to one embodiment shown in FIG. 4, the markers <b>40</b> serve as point fiducial markers constructed from Plexiglas, having a cylindrical cavity <b>42</b> of 6.35 mm both in height and diameter, for example. In addition, a tool <b>44</b>, referred to herein as a localization tool (LT), is used to determine the coordinate of the markers <b>40</b> in the transmitter's <b>22</b> 3D coordinate system. The LT <b>44</b> comprises a 15-cm long plastic arm <b>46</b> with a stainless steel spherical bead <b>48</b> (6.35-mm diameter, for example) attached to one end of it. On the other end of the arm <b>46</b>, the receiver <b>24</b> is mounted in a fixed position and orientation with respect to the arm <b>46</b>. In this way, the receiver <b>24</b> can be repeatedly mounted to the LT <b>46</b>, with the center of the spherical bead <b>48</b> always being in the same position with respect to the receiver <b>24</b>.</p>
    <p>An objective is to determine the coordinates of the center of the cylindrical cavity <b>42</b> of each marker <b>40</b> in both the MR and transmitter <b>22</b> spaces, so that coordinates of each marker <b>40</b> can be used to extrapolate coordinates of other points within the volume of the phantom <b>32</b>. In the MR images, the markers <b>40</b> appear as cylinders (having 6.35-mm height and diameter), since the cylindrical cavity <b>42</b> of each marker <b>40</b> is filled with a 4-mM copper sulfate distilled water solution, thereby providing a bright signal in the MR images. Each cylindrical-looking marker <b>40</b> is manually segmented in each coronal and axial slice, and then the center of the cylinder is determined by the intensity-weighted centroid of the segmented regions.</p>
    <p>In the transmitter <b>22</b> coordinate system, the center of each cylinder is determined by inserting the spherical bead <b>48</b> of the LT <b>44</b> inside the cylindrical cavity <b>42</b> of the marker <b>40</b>. Since the diameter and height of the cylindrical cavity <b>42</b> (e.g., the cylinder) and the diameter of the spherical bead <b>48</b> are all identical (e.g., 6.35 mm), the center of the cylinder and the center of the spherical bead <b>48</b> coincide. Thus, localization of each point fiducial in the transmitter <b>22</b> space is equivalent to the localization (in the transmitter <b>22</b> space) of the center of the spherical bead <b>48</b> (attached to the arm <b>46</b>) when the spherical bead <b>48</b> is inserted in the cylindrical cavity <b>42</b> of each point fiducial.</p>
    <p>The determination of the coordinates of the center of the spherical bead <b>48</b> in the transmitter <b>22</b> space can be based on a calibration method, such as that described in N. Pagoulatos, W. S. Edwards, D. R. Haynor, and Y. Kim, “Calibration and Validation of Free-Hand 3D Ultrasound Systems Based on DC Magnetic Tracking,” in <i>Proceedings of the SPIE</i>, Vol. 3335, pp. 59-71, 1998, incorporated by reference (hereinafter referred to as “Pagoulatos et al.”). The result of this calibration is the determination of the coordinates of the center of the spherical bead <b>48</b> with respect to the receiver <b>24</b> coordinate system. These coordinates are fixed for the given LT <b>44</b>, since the receiver <b>24</b> and the spherical bead <b>48</b> are always in the same location on the LT <b>44</b>. As the LT <b>44</b> moves in space, the coordinates of the center of the spherical bead <b>48</b> in the transmitter <b>22</b> space can be determined by using the following equation: <maths> <math> <mtable> <mtr> <mtd> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>x</mi> <mi>TR</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>y</mi> <mi>TR</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>z</mi> <mi>TR</mi> </msub> </mtd> </mtr> <mtr> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo>=</mo> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>R</mi> <mn>11</mn> </msub> </mtd> <mtd> <msub> <mi>R</mi> <mn>12</mn> </msub> </mtd> <mtd> <msub> <mi>R</mi> <mn>13</mn> </msub> </mtd> <mtd> <msub> <mi>T</mi> <mi>x</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>R</mi> <mn>21</mn> </msub> </mtd> <mtd> <msub> <mi>R</mi> <mn>22</mn> </msub> </mtd> <mtd> <msub> <mi>R</mi> <mn>23</mn> </msub> </mtd> <mtd> <msub> <mi>T</mi> <mi>y</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>R</mi> <mn>31</mn> </msub> </mtd> <mtd> <msub> <mi>R</mi> <mn>32</mn> </msub> </mtd> <mtd> <msub> <mi>R</mi> <mn>33</mn> </msub> </mtd> <mtd> <msub> <mi>T</mi> <mi>z</mi> </msub> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo>·</mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>x</mi> <mi>REC</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>y</mi> <mi>REC</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>z</mi> <mi>REC</mi> </msub> </mtd> </mtr> <mtr> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>(</mo> <mn>1</mn> <mo>)</mo> </mrow> <mo>.</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-M00001.png"> <img id="EMI-M00001" file="US06775404-20040810-M00001.TIF" img-content="math" img-format="tif" alt="Figure US06775404-20040810-M00001" src="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-M00001.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00001" attachment-type="nb" file="US06775404-20040810-M00001.NB"> </attachment> </attachments> </maths> </p>
    <p>The subscript TR denotes coordinates in the transmitter <b>22</b> space and the subscript REC in the receiver <b>24</b> space. The 4×4 matrix is the rigid-body transformation given by the control unit <b>26</b> of the position sensor <b>14</b>.</p>
    <p>Consider the coordinates of a set of N corresponding points <maths> <math> <msub> <mrow> <mo>{</mo> <mtable> <mtr> <mtd> <mi>ρ</mi> </mtd> <mtd> <mi>ρ</mi> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>a</mi> <mi>i</mi> </msub> <mo>,</mo> </mrow> </mtd> <mtd> <msub> <mi>b</mi> <mi>i</mi> </msub> </mtd> </mtr> </mtable> <mo>}</mo> </mrow> <mrow> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mo>,</mo> <mi>…</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo>,</mo> <mi>N</mi> </mrow> </msub> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-M00002.png"> <img id="EMI-M00002" file="US06775404-20040810-M00002.TIF" img-content="math" img-format="tif" alt="Figure US06775404-20040810-M00002" src="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-M00002.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00002" attachment-type="nb" file="US06775404-20040810-M00002.NB"> </attachment> </attachments> </maths> </p>
    <p>in two different coordinate systems. An objective is to determine a geometric transformation P that associates the two coordinate spaces. The solution to this problem may be given by the transformation that minimizes the scalar quantity Δ(P), which is defined as: <maths> <math> <mtable> <mtr> <mtd> <mrow> <mrow> <mi>Δ</mi> <mo></mo> <mrow> <mo>(</mo> <mi>P</mi> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mrow> <msqrt> <mrow> <mfrac> <mn>1</mn> <mi>N</mi> </mfrac> <mo>·</mo> <mrow> <munderover> <mo>∑</mo> <mrow> <mi>j</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>N</mi> </munderover> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <msup> <mrow> <mo></mo> <mrow> <mover> <msub> <mi>a</mi> <mi>i</mi> </msub> <mi>ρ</mi> </mover> <mo>-</mo> <mrow> <mi>P</mi> <mo>·</mo> <mover> <msub> <mi>b</mi> <mi>i</mi> </msub> <mi>ρ</mi> </mover> </mrow> </mrow> <mo></mo> </mrow> <mn>2</mn> </msup> </mrow> </mrow> </msqrt> <mo>=</mo> <msqrt> <mrow> <mfrac> <mn>1</mn> <mi>N</mi> </mfrac> <mo>·</mo> <mrow> <munderover> <mo>∑</mo> <mrow> <mi>j</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>N</mi> </munderover> <mo></mo> <msup> <mrow> <mo></mo> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>a</mi> <mi>xi</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>a</mi> <mi>yi</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>a</mi> <mi>zi</mi> </msub> </mtd> </mtr> <mtr> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo>-</mo> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>P</mi> <mn>11</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>12</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>13</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>14</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>P</mi> <mn>21</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>22</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>23</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>24</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>P</mi> <mn>31</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>32</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>33</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>34</mn> </msub> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo>·</mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>b</mi> <mi>xi</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>b</mi> <mi>yi</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>b</mi> <mi>zi</mi> </msub> </mtd> </mtr> <mtr> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </mrow> <mo></mo> </mrow> <mn>2</mn> </msup> </mrow> </mrow> </msqrt> </mrow> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>(</mo> <mn>2</mn> <mo>)</mo> </mrow> <mo>.</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-M00003.png"> <img id="EMI-M00003" file="US06775404-20040810-M00003.TIF" img-content="math" img-format="tif" alt="Figure US06775404-20040810-M00003" src="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-M00003.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00003" attachment-type="nb" file="US06775404-20040810-M00003.NB"> </attachment> </attachments> </maths> </p>
    <p>When P is considered as a rigid-body transformation, a closed-form solution has been proposed by K. S. Arun, T. S. Huang, and S. D. Blostein, “Least-Squares Fitting of Two 3-D Point Sets,” <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, Vol. 9, pp. 698-700, 1987 (hereinafter Arun et al.), incorporated by reference, which is based on the singular value decomposition (SVD) of the covariance matrix of the coordinates of the points in the two coordinates systems. According to an embodiment of the invention, the two coordinate systems comprise those of the transmitter <b>22</b> and the MR images, and the coordinates of the points can be determined based on the methods previously described above. That is, the coordinates of the points in the two spaces can be obtained by inserting the spherical bead <b>48</b> into the cylindrical cavity <b>42</b> of each marker <b>40</b>, sequentially pulsing magnetic fields from the transmitter <b>22</b> to the receiver <b>24</b>, and then receiving and processing returned coordinate information through the control unit <b>26</b> and serial port <b>28</b>. After the transformation P is determined by processing the coordinates of the points of the point fiducials/markers <b>40</b>, the transmitter <b>22</b> and MR spaces are registered (e.g., the coordinates of a given point can be converted from one coordinate system to the other). In an operating room setting, the transformation P can be determined by placing the markers <b>40</b> on a patient's cranium and then performing the described coordinate point registration and localization.</p>
    <p>C. Determination of Registration Error</p>
    <p>With registration, particularly in systems designed for image-guided surgery (where the degree of accurate localization could potentially affect the result of the surgical procedure), it is often important to measure the statistical distribution (including average value, standard deviation, and range) of the registration error throughout the volume of interest. Based on this, embodiments of the invention can utilize several different methods to quantitatively measure the registration error between the MR and transmitter <b>22</b> in different parts of the phantom <b>32</b>.</p>
    <p>One quantitative measure of registration accuracy between two spaces is the value of the scalar quantity Δ in Eq. (2). This scalar quantity, which is usually referred to as fiducial registration error (FRE), represents the rms distance between corresponding fiducial points in the same space after the transformation P between the two spaces has been performed. Another measure of registration accuracy is the distance between corresponding points that are not used to determine the transformation P. This distance is usually referred to as target registration error (TRE), and is based on the location of the target with respect to the fiducials. In one embodiment, four markers <b>40</b> are used to determine the rigid-body transformation P and the FRE, and another four markers are used to measure the TRE. All these markers <b>40</b> are placed along the periphery of the phantom <b>32</b>, such as shown in FIG. <b>2</b>.</p>
    <p>In order to validate the registration within the volume of the phantom <b>32</b>, the four plastic spherical beads <b>38</b> (e.g., 2.5 mm diameter) are located in different parts of the phantom <b>32</b> (see, e.g., FIG. <b>2</b>). The plastic beads <b>38</b> are primarily designed to serve as well-identifiable imaging targets in US, and as a result, the LT <b>44</b> is not used (as previously described) for localizing their centers. Therefore, the TRE is not measured based on the plastic beads <b>38</b>. Instead, another measure for validating the registration inside the volume of the phantom <b>32</b> is used, with the measure being based on the difference between true and measured distances between two given points. The better the registration, the closer the measured distance to the true distance. A mathematical expression of this statement is derived in U.S. Provisional Patent Application Serial No. 60/125,017 identified above.</p>
    <p>As shown in FIG. 5, the tip <b>50</b> of the LT <b>44</b> (having the spherical bead <b>48</b>) is placed on the surface of each plastic bead <b>38</b>, and the distance D between the two centers is measured. Ideally, this distance D is the sum of the radii of the two beads <b>38</b> and <b>48</b> (e.g., 3.175 mm+1.25 mm=4.425 mm). This distance D is measured for different orientations of the LT tip <b>50</b> with respect to each plastic bead <b>38</b>.</p>
    <p>One of the tools available in the GUI <b>20</b> is an interactive visualization of the LT tip <b>50</b> in the MR space (in axial or coronal images) as one moves the LT <b>44</b> (with the receiver <b>24</b> attached on it) within the phantom <b>32</b>. The spherical LT tip <b>48</b>/<b>50</b> appears in MR slices as a circle of the equivalent size. In this way, the registration between the two spaces can be visually evaluated.</p>
    <p>D. Calibration of the US Probe</p>
    <p>To track the position and orientation of the US images with respect to the transmitter <b>22</b> reference coordinate system, the receiver <b>24</b> is mounted on the US probe <b>30</b>, as schematically shown in FIG. <b>1</b>. The geometric transformation that associates the US image with the transmitter <b>22</b> coordinate system continuously changes during a US scan as the US probe <b>30</b> moves freely in the 3D space. That is, accurate localization of 2D US images in the 3D space of the receiver <b>24</b> (or transmitter <b>22</b>) is needed. This transformation comprises two independent parts: a transformation from the US image to the receiver <b>24</b>, and a rigid-body transformation from the receiver <b>24</b> to the transmitter <b>22</b> (e.g., the output of the position tracking system or control unit <b>24</b>, in a manner represented by Eq. 1 using the 4×4 matrix).</p>
    <p>One method to determine the first transformation can be based on a calibration procedure described in Pagoulatos et al. A 9-DOF (degree of freedom) affine warp (2D to 3D) is used, which transforms the pixels of the US image to the receiver's <b>24</b> coordinate system as shown in the following equation: <maths> <math> <mtable> <mtr> <mtd> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>x</mi> <mi>REC</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>y</mi> <mi>REC</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>z</mi> <mi>REC</mi> </msub> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo>=</mo> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>a</mi> <mn>11</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>12</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>13</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>a</mi> <mn>21</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>22</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>23</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>a</mi> <mn>31</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>32</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>33</mn> </msub> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo>·</mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>x</mi> <mi>US</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>y</mi> <mi>US</mi> </msub> </mtd> </mtr> <mtr> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>(</mo> <mn>3</mn> <mo>)</mo> </mrow> <mo>.</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-M00004.png"> <img id="EMI-M00004" file="US06775404-20040810-M00004.TIF" img-content="math" img-format="tif" alt="Figure US06775404-20040810-M00004" src="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-M00004.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00004" attachment-type="nb" file="US06775404-20040810-M00004.NB"> </attachment> </attachments> </maths> </p>
    <p>Briefly, the calibration procedure comprises imaging a single point from different positions and orientations of the US probe <b>30</b> and then applying an iterative procedure minimizing the variance of the target pixel coordinates in the transmitter's <b>22</b> space (ideally the variance is zero since the same point is imaged). For this procedure, a phantom (not shown) can be constructed that makes the calibration procedure fast, accurate and easily reproducible. The use of an affine (instead of rigid-body) transformation enables automatic scaling of the US image pixels to physical units (e.g., cm). Furthermore, the scaling derived from the affine transformation automatically accounts for the difference in the speed of sound in the medium where the calibration is being performed and the 1540 m/s, which is assumed in all the commercial US scanners.</p>
    <p>Another embodiment of a method to determine/calculate the first transformation from the US image space to the receiver <b>24</b> space uses a simpler algorithm based on acquisition of a single US image. The transformation is based on the position and orientation of the receiver <b>24</b> with respect to the US probe's <b>30</b> face. For the purpose of calibration, a phantom is built that contains its own 3D coordinate system.</p>
    <p>An example of such a phantom <b>52</b> is illustrated in FIGS. <b>11</b>(<i>a</i>) and <b>11</b>(<i>b</i>). The phantom <b>52</b> of FIG. <b>11</b>(<i>a</i>) comprises a plexiglass container <b>54</b> (e.g., 21-cm length, 24-cm width, 21-cm height, for example) filled with distilled water. In the outer walls of the phantom, there are several hemispherical divots <b>56</b>, which provide a set of point-fiducials used to register the phantom and the transmitter coordinate systems. Immersed in distilled water, a set of fish line strings <b>58</b> are appropriately placed (in five rows <b>60</b> of six N-fiducials as shown in FIG. <b>11</b>(<i>b</i>)) to provide a set of N-fiducials (e.g., fiducials with the shape of “N”) used to register the US with the phantom coordinate systems. According to one embodiment, the size and direction of the N-fiducials are different so that they can be distinguished in the US image. Further details regarding the phantom <b>52</b> can be found in U.S. Provisional Patent Application Serial No. 60/135,065 identified above.</p>
    <p>Assuming that X<sub>US </sub>and X<sub>Ph </sub>denote the US and phantom coordinates, the following two equations are provided, with the various geometric transformations being illustrated in FIG. <b>12</b>:</p>
    <p> <i>X</i> <sub>Ph</sub> <i>=A·R·P·X</i> <sub>US</sub>  (3a),</p>
    <p>
      <maths> <formula-text> <i>X</i> <sub>Ph</sub> <i>=T·X</i> <sub>US</sub>  (3b),</formula-text> </maths> </p>
    <p>where P is the unknown transformation (objective of the calibration method) from the US to the receiver <b>24</b> coordinate system, R is the transformation from the receiver <b>24</b> to the transmitter <b>22</b> coordinate system (given by the position sensor <b>14</b>), A is the transformation from the transmitter <b>22</b> to the phantom coordinate system, and T is the transformation from the US to the phantom coordinate system. From Eqs. (3a) and (3b), the unknown matrix P can be solved according to the following equation:</p>
    <p>
      <maths> <formula-text> <i>A·R·P=T⇄P=R</i>
          <sup>−1</sup> <i>·A</i>
          <sup>−1</sup> <i>·T</i>
        </formula-text> </maths> </p>
    <p>3(c).Transformation A is determined based on the six point-fiducials <b>56</b> placed in the outer walls of the phantom <b>52</b> (e.g., Transformation A can be determined in a manner similar to that described in Section I.B above using the LT <b>46</b> according to one embodiment). In the inner part of the phantom <b>52</b>, immersed in distilled water, the set of fish line strings <b>58</b> are appropriately placed to provide the set of N-fiducials used for the determination of transformation T. For both cases of point-fiducials and N-fiducials, a set of homologous points in the two corresponding coordinate systems can be used for registration based on a closed-form solution, such as that proposed by Arun et al.</p>
    <p>The intersection of a US image with an N-fiducial is visualized in the US image as a set of three ellipses, as shown in FIG. <b>12</b>. The coordinates of the middle ellipse with respect to the phantom coordinate system can be determined from the ratio of the distances between the three ellipses (computed from manual identification of the ellipses in the US image) and the coordinates of the vertices of the N-fiducial in the phantom coordinate system (known from the phantom construction). Therefore, for each N-fiducial, the middle ellipse provides a pair of homologous points with known coordinates in the US and the phantom coordinate systems. An example of a method to extract homologous points from N-fiducials can be found in R. A. Brown, “A Stereotactic Head Frame for Use with CT Body Scanners,” Investigative Radiology, vol. 14, pp. 300-304, 1979, incorporated by reference.</p>
    <p>To gauge the precision of the method, 10 images are acquired, and the calibration matrix is computed for each one of them. Illustrative mean and standard deviations of each matrix element are as follows: <maths> <math> <mrow> <mo></mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mrow> <mn>0.017</mn> <mo>±</mo> <mn>0.003</mn> </mrow> </mtd> <mtd> <mrow> <mn>0.999</mn> <mo>±</mo> <mn>0.001</mn> </mrow> </mtd> <mtd> <mrow> <mn>0.022</mn> <mo>±</mo> <mn>0.035</mn> </mrow> </mtd> <mtd> <mrow> <mn>5.017</mn> <mo>±</mo> <mn>0.008</mn> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mo>-</mo> <mn>0.999</mn> </mrow> <mo>±</mo> <mn>0.001</mn> </mrow> </mtd> <mtd> <mrow> <mn>0.017</mn> <mo>±</mo> <mn>0.002</mn> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>-</mo> <mn>0.007</mn> </mrow> <mo>±</mo> <mn>0.037</mn> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>-</mo> <mn>1.000</mn> </mrow> <mo>±</mo> <mn>0.035</mn> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mo>-</mo> <mn>0.008</mn> </mrow> <mo>±</mo> <mn>0.037</mn> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>-</mo> <mn>0.022</mn> </mrow> <mo>±</mo> <mn>0.035</mn> </mrow> </mtd> <mtd> <mrow> <mn>0.998</mn> <mo>±</mo> <mn>0.001</mn> </mrow> </mtd> <mtd> <mrow> <mn>2.803</mn> <mo>±</mo> <mn>0.232</mn> </mrow> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-M00005.png"> <img id="EMI-M00005" file="US06775404-20040810-M00005.TIF" img-content="math" img-format="tif" alt="Figure US06775404-20040810-M00005" src="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-M00005.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00005" attachment-type="nb" file="US06775404-20040810-M00005.NB"> </attachment> </attachments> </maths> </p>
    <p>The first three rows of the first three columns are dimensionless, and represent the rotational part of the P transformation. The first three rows of the fourth column are in cm, and represent the coordinates of the origin of the US image with respect to the receiver <b>24</b> coordinate system.</p>
    <p>To verify the accuracy of the matrix elements, the rms localization error in determining the coordinates of a single 3D point for different positions and orientations of the US probe <b>30</b> are computed. The target point (e.g., a 1.5-mm stainless steel bead) was contained within an agar-based spherical phantom. The rms error may be 2.0 mm, for example. The effectiveness of this calibration method by using only one image is due to the fact that in each US image, there is a large number of points resulting in a representative sampling of the US plane. Thus, this simpler method requires only one image and provides results that are comparable to conventional calibration methods where approximately <b>30</b> images are required. The various calculations and user interactions used for this calibration method can be performed by software in the PC <b>16</b> and/or control unit <b>26</b>, using a Visual C++ program, for example. Such calibration may be used for quality control in clinical sites.</p>
    <p>An embodiment of the method of US probe calibration can be also used in 3D US imaging applications, where a position sensor is used to track a set of 2D US images which are further processed to form a 3D US image. In these applications, US probe calibration is used to create geometrically accurate 3D US data sets. Geometric accuracy of the 3D US data sets is critical, in order to be able to make quantitative measurements (e.g., volume, area and distance measurements of anatomy) based on the 3D US image.</p>
    <p>E. Registration of US and MR Coordinate Systems</p>
    <p>The coordinate transformation that associates the 2D US and 3D MR coordinate systems can be described by the following equation: <maths> <math> <mtable> <mtr> <mtd> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>x</mi> <mi>MR</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>y</mi> <mi>MR</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>z</mi> <mi>MR</mi> </msub> </mtd> </mtr> <mtr> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo>=</mo> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>P</mi> <mn>11</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>12</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>13</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>14</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>P</mi> <mn>21</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>22</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>23</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>24</mn> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>P</mi> <mn>31</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>32</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>33</mn> </msub> </mtd> <mtd> <msub> <mi>P</mi> <mn>34</mn> </msub> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo>·</mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>R</mi> <mn>11</mn> </msub> </mtd> <mtd> <msub> <mi>R</mi> <mn>12</mn> </msub> </mtd> <mtd> <msub> <mi>R</mi> <mn>13</mn> </msub> </mtd> <mtd> <msub> <mi>T</mi> <mi>x</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>R</mi> <mn>21</mn> </msub> </mtd> <mtd> <msub> <mi>R</mi> <mn>22</mn> </msub> </mtd> <mtd> <msub> <mi>R</mi> <mn>23</mn> </msub> </mtd> <mtd> <msub> <mi>T</mi> <mi>y</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>R</mi> <mn>31</mn> </msub> </mtd> <mtd> <msub> <mi>R</mi> <mn>32</mn> </msub> </mtd> <mtd> <msub> <mi>R</mi> <mn>33</mn> </msub> </mtd> <mtd> <msub> <mi>T</mi> <mi>z</mi> </msub> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo>·</mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>a</mi> <mn>11</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>12</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>13</mn> </msub> </mtd> <mtd> <mn>0</mn> </mtd> </mtr> <mtr> <mtd> <msub> <mi>a</mi> <mn>21</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>21</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>23</mn> </msub> </mtd> <mtd> <mn>0</mn> </mtd> </mtr> <mtr> <mtd> <msub> <mi>a</mi> <mn>31</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>32</mn> </msub> </mtd> <mtd> <msub> <mi>a</mi> <mn>33</mn> </msub> </mtd> <mtd> <mn>0</mn> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mn>0</mn> </mtd> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo>·</mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>x</mi> <mi>US</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>y</mi> <mi>US</mi> </msub> </mtd> </mtr> <mtr> <mtd> <mn>1</mn> </mtd> </mtr> <mtr> <mtd> <mn>1</mn> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mrow> <mo>(</mo> <mn>4</mn> <mo>)</mo> </mrow> <mo>.</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-M00006.png"> <img id="EMI-M00006" file="US06775404-20040810-M00006.TIF" img-content="math" img-format="tif" alt="Figure US06775404-20040810-M00006" src="//patentimages.storage.googleapis.com/US6775404B1/US06775404-20040810-M00006.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00006" attachment-type="nb" file="US06775404-20040810-M00006.NB"> </attachment> </attachments> </maths> </p>
    <p>The notations for the matrices are the same as previously used above, where each matrix was independently described. In addition in the above equation, the P transformation is considered to be rigid, and thus the units of the 4×1 vector on the left part of the equation are in cm (the US pixels are transformed in physical units from the affine transformation a). Once the necessary registration and calibration procedures described in the previous sections have been performed (e.g., the matrices P and a are determined), the US pixel coordinates can be interactively transformed to the MR space during a US scan based on Eq. (4) and the position sensor <b>14</b> output.</p>
    <p>The series of coordinate transformations shown in Eq. (4) allows reconstruction of 2D MR images with the same position, orientation and scaling as the acquired 2D US images. Since the transformed US pixel coordinates in the MR space (left part of Eq. (4)) are not integer values, an interpolation is used to determine the MR pixel intensity. Two interpolation methods can be used: zero-order (commonly known as “nearest neighbor”) and first-order (trilinear) interpolation. The relation between the US and MR images and the need for interpolation are shown in FIG. <b>6</b>.</p>
    <p>US and MR images of the phantom <b>32</b> are registered by locating the US probe <b>30</b> in several positions and orientations with respect to the phantom <b>32</b>. A correction may be implemented to account for the difference of the speed of sound in distilled water (1498 m/s) and in the medium where the US probe <b>30</b> was calibrated (1540 m/s). The quantitative validation of the registration between the US and MR spaces is based on the spherical plastic beads <b>38</b> inside the phantom <b>32</b>. The coordinates of each bead <b>38</b> are identified in the US images and are converted based on Eq. (4) to the corresponding MR coordinates. Then, based on these coordinates and on the MR coordinates of the same bead computed directly from the MR data set, the distance between those two corresponding points are computed and used as a measure of the TRE between the US and MR spaces.</p>
    <p>Moreover, registration may be validated qualitatively by visually observing how similar structures in the phantom are aligned in the two modalities. The GUI <b>20</b> program can have two windows next to each other, where the US and MR images are shown. Tools for superimposing the two images use alternate image stripes (of user-specified thickness) of the two images in both vertical and horizontal directions. In that way, the alignment of similar structures in the two images can be observed (see, e.g., FIGS. <b>9</b>(<i>a</i>)-<b>9</b>(<i>c</i>)). Such a GUI <b>20</b> program can work both in interactive and non-interactive modes. In the interactive mode, the registered MR images are continuously reconstructed during the US scan, whereas in the non-interactive mode the captured US image and the corresponding MR image are shown each time a user clicks a button.</p>
    <heading>II. Illustrative Results</heading> <p>A. Registration Between MR and Transmitter Space</p>
    <p>In one exercise, the MR is registered with the transmitter <b>22</b> space for 12 different configurations (positions and orientations) of the phantom <b>32</b> and the transmitter <b>22</b>. Since the performance of the magnetic position sensor <b>14</b> is often dependent on the distance between the receiver <b>24</b> and the transmitter <b>22</b>, the phantom <b>32</b> is positioned such that the distance between the receiver <b>24</b> and the transmitter <b>22</b> is in the range of 40 cm to 50 cm, for example. Some of the descriptive statistics of the FRE<sub>MR-tran </sub>(based on four markers <b>40</b>) and the average TRE<sub>MR-tran </sub>(based on the remaining four markers <b>40</b>) are shown in Table I of FIG. <b>10</b>(<i>a</i>). As is noted, the average TRE<sub>MR-tran </sub>in the circumference of the phantom is higher than the average FRE<sub>MR-tran</sub>.</p>
    <p>For a certain configuration between the transmitter <b>22</b> and MR, the distance D (see, e.g., FIG. 5) is also measured for 30 different orientations of the spherical bead <b>48</b> of the LT <b>44</b> on the surface of each plastic bead <b>38</b>. The differences between the measured D<sub>meas </sub>and true value D<sub>true </sub>(e.g., 4.425 mm) of D for each bead <b>38</b> are summarized in Table II of FIG. <b>10</b>(<i>b</i>).</p>
    <p>For each configuration between transmitter <b>22</b> and phantom <b>32</b>, the accuracy of registration is also examined visually during one exercise. It is observed how correctly the physical location of the LT bead <b>48</b> (attached to the plastic arm <b>46</b> as shown in FIG. 4) within the phantom <b>32</b> is mapped to the real-time displayed MR images. For example in FIGS. <b>7</b>(<i>a</i>) and <b>7</b>(<i>b</i>), the corresponding coronal and axial MR slices, respectively, showing the location of the spherical bead <b>48</b> (the circle) when the LT <b>44</b> is inserted in the cylindrical cavity <b>42</b> of one of the markers <b>40</b>. From this, it can be verified (as discussed in Section I.B above) how the center of the spherical bead <b>48</b> corresponds to the center of the marker <b>40</b>.</p>
    <p>B. Registration Between US and MR Space</p>
    <p>As in the previous case, an objective is to determine the statistical distribution of the registration error between US and MR images throughout the volume of the phantom <b>32</b>. For these exercises, one particular configuration of the transmitter <b>22</b> and the phantom <b>32</b> can be used. For each bead <b>38</b>, 40 different images are acquired that correspond to 40 different positions and orientations of the US probe <b>30</b>. The depth setting in the US scanner <b>12</b> is set at 12 cm for all the images. The statistical distribution of the registration error (as defined is Section I.D above) for each bead <b>38</b> is shown in Table III of FIG. <b>10</b>(<i>c</i>), where the depth represents the distance of each bead <b>38</b> from the US probe's <b>30</b> face.</p>
    <p>A material characteristic of the bead <b>38</b> was such that it produced a lot of reverberation. Although this is an artifact, it is helpful when trying to locate the bead <b>38</b> in US images. A typical US image of the bead <b>38</b> and the corresponding MR image are shown in FIGS. <b>8</b>(<i>a</i>) and <b>8</b>(<i>b</i>). FIG. <b>9</b>(<i>a</i>) shows a typical US image of the phantom <b>32</b>, while FIG. <b>9</b>(<i>b</i>) shows the registered MR image (note that the contrast in the MR images is reversed so that bright signals in US correspond to bright signals in MR). In FIG. <b>9</b>(<i>c</i>), the resulting image is shown by superimposing the upper two images of FIGS. <b>9</b>(<i>a</i>) and <b>9</b>(<i>b</i>). This can be done by displaying even rows with image data from one modality and odd rows from the other modality. In this interlaced image of FIG. <b>9</b>(<i>c</i>), the accurate alignment of the registered images can be observed.</p>
    <p>When an embodiment of the GUI <b>20</b> program is operated in an interactive mode, frame rates of approximately 8 frames/second for zero-order interpolation and 6.5 frames/second for first-order interpolation with 344×310 images can be achieved.</p>
    <heading>III. Additional Discussion</heading> <p>The TRE between MR and transmitter <b>22</b> spaces depends on the target localization error (TLE) in each coordinate system and on the true registration error (RE) between the two spaces. The mathematical definition of these variables is presented in U.S. Provisional Patent Application Serial No. 60/125,017 identified above, where based on a mathematical model of the noise introduced in measurements, the relation between TRE, TLE and RE is derived. In the case of the registration between MR and transmitter <b>22</b> spaces, the relation is:</p>
    <p>
      <maths> <formula-text> <i>TRE</i> <sup>2</sup> <sub>MR-tran</sub> <i>=TLE</i> <sup>2</sup> <sub>MR</sub> <i>+TLE</i> <sup>2</sup> <sub>tran</sub> <i>+RE</i> <sup>2</sup>  (5).</formula-text> </maths> </p>
    <p>From Table I of FIG. <b>10</b>(<i>a</i>), it is noted that TRE<sub>MR-tran </sub>has a mean value and a standard deviation of 1.78 mm and 0.18 mm, respectively. Based on the standard deviation, it can be surmised that the dependence of the registration accuracy (between MR and position sensor <b>14</b> spaces) on the relative position of the phantom <b>32</b> and the transmitter <b>22</b> is small. This conclusion may be useful in a clinical application, where the position of the transmitter <b>22</b> with respect to the patient head has to be decided.</p>
    <p>The measured distance between two given points can be compared to their true distance to validate the accuracy of registration. The illustrative data in Table II of FIG. <b>10</b>(<i>b</i>) depicts the accuracy according to which a given distance (e.g., 4.425 mm) in four different locations of the phantom <b>32</b> can be measured. From Table II, it is observed that the maximum error in measuring the given distance is about 1.5 mm. It is noted that a small part of the error in measuring the given distance comes from imperfections on the surface of the plastic beads <b>38</b> (e.g., the plastic beads <b>38</b> are not perfect spheres of radius 2.5 mm).</p>
    <p>When registering US and MR spaces, the US pixels are first transferred to the transmitter <b>22</b> space and then to the MR space. So the TRE between US and MR spaces can be computed from the following equation:</p>
    <p>
      <maths> <formula-text> <i>TRE</i> <sup>2</sup> <sub>US-MR</sub> <i>=TLE</i> <sup>2</sup> <sub>MR</sub> <i>+TLE′</i> <sup>2</sup> <sub>tran</sub> <i>+RE</i> <sup>2</sup>  (6).</formula-text> </maths> </p>
    <p>The difference between Eqs. (5) and (6) is on the TLE in the transmitter <b>22</b> space. In Eq. (6), this error is larger because the target is localized through the US image. Therefore, the errors associated with the resolution of the US image increase the TLE in the transmitter <b>22</b> space. According to one set of empirical results, TLE′<sub>tran </sub>is approximately equal to 1.8 mm, whereas TLE<sub>tran </sub>is approximately 0.5 mm. From Table III of FIG. <b>10</b>(<i>c</i>), it is illustratively indicated that the average TRE<sub>US-MR </sub>(averaged over the four beads <b>38</b>) is 2.7 mm. The maximum value of the error is 8.49 mm due to azimuthal resolution deficiencies of some transducers with 1-D arrays, where focus on the direction perpendicular to the US plane can be unsatisfactory. This resolution is a function of the distance from the face of the transducer (e.g., US probe <b>30</b>), as illustrated by the data in Table III where the TRE<sub>US-MR </sub>improves as the depth increases.</p>
    <p>To avoid the registration error introduced in the direction perpendicular to the US plane, a straight line, provided by the wooden stick shown both in US and MR images in FIGS. <b>8</b>(<i>a</i>) and <b>8</b>(<i>b</i>), respectively, is imaged with the US plane perpendicular to it. The intersection of this US plane with the straight line is visualized as a circular dot both in the US and registered MR images. The distance of the centroid of these two circular areas in the US and MR images can represent a measure of the in-plane accuracy of a registration method according to one embodiment of the invention. According to a set of empirical results, this TRE<sub>US-MR </sub>error has a mean value of approximately 1.5 mm and does not exceed 3 mm.</p>
    <p>One of the problems in using a rigid-body transformation in Eq. (2) is associated with the geometric distortions found in MR images. These distortions are due to errors in gradient strength and static field inhomogeneity. The former effect causes linear scale distortions whereas the latter causes nonlinear distortions. To correct for the linear distortions, an embodiment of the invention uses a 12-DOF affine (3D to 3D) transformation. To determine this transformation, a method computes the transformation based on corresponding points in the two spaces. Five markers <b>40</b> are used in the circumference of the phantom <b>32</b> and then a closed-form solution of the transformation P (Eq. 2) is computed. Compared to the rigid-body transformation, a statistically significant improvement is observed in the FRE<sub>MR-tran </sub>and TRE<sub>MR-tran</sub>.</p>
    <p>The DC magnetic position sensor <b>14</b> incorporated in the system <b>10</b> comprises a 6-DOF sensor according to one embodiment. The position sensor <b>14</b> is easy to use in the operating room and tracking of the head is not required (compared to the optical and sonic sensors) because the transmitter <b>22</b> can be mounted to a head-holding device. Other advantages of the magnetic position sensor <b>14</b> are: 1) no line of sight is required (as in the case of the optical sensor); 2) more than a single object (e.g., a surgical tool or intraoperative US probe <b>30</b>) can be tracked simultaneously via a separate receiver <b>24</b> mounted on each object; and 3) lower cost.</p>
    <p>In terms of registration accuracy, the accuracy provided by the magnetic position sensor <b>14</b> is satisfactory for tracking US images. However, because of the possibility of sensitivity of the magnetic position sensor <b>14</b> to metals, DC magnetic tracking may be used, since it less susceptible to errors due to metals as compared to AC magnetic tracking.</p>
    <p>For 344×310 images, the GUI <b>20</b> software can display the original US and registered MR images with a frame rate of 6.5 frames/second with first-order interpolation and 8 frames/second with zero-order interpolation. With the use of a programmable digital signal processor, the performance of the system <b>10</b> can increase significantly. For example, a programmable mediaprocessor (such as TMS320C80) that has good performance in algorithms with similar memory access and computation load, can be used.</p>
    <p>Although the system <b>10</b> has been described herein as being applied to the phantom <b>32</b>, the same principles are applicable in the neurosurgical operating room setting. For example, the magnetic position sensor <b>14</b> can be used in the operating room, while point fiducial markers <b>49</b> can be attached to the skin of the head or screwed to the skull. Furthermore, the system <b>10</b> is capable of displaying the US and corresponding registered MR images side-by-side (or superimposed one on the other) nearly in real time, which improves the usefulness of the intraoperative US images and leads to better understanding of intraoperative brain changes.</p>
    <p>According to one embodiment, the registration method uses the skull as a reference and consequently does not account for changes in brain anatomy that might occur between preoperative and intraoperative images. According to another embodiment, the registration method can be refined to account for intraoperative changes and to enable the modification of the preoperative images to reflect more accurately the intraoperative status of the brain.</p>
    <heading>IV. Conclusions</heading> <p>In summary, a system and method for interactive registration of US and MR images is described. The system <b>10</b> uses a DC magnetic position sensor <b>14</b> for interactively registering MR to physical space as well as MR to US space. To perform these registrations, a point fiducial system and a localization tool <b>44</b> are provided. In addition, the phantom <b>32</b> can be used to test the accuracy of the system <b>10</b> and to demonstrate its features. Registration errors can also be determined. All the calibration and registration procedures can be performed in a graphical environment through custom-developed software in Microsoft Visual C++, for example. On the PC <b>16</b>, the original US and registered MR images each with size of 344×310 can be displayed with a frame rate of 6.5 frames/second using first-order interpolation and 8 frames/second using zero-order interpolation. Any other tomographic data set other than MR (e.g., CT, positron emission tomography or PET, and single photon emission computed tomography or SPECT) with point fiducial information could be used by the system <b>10</b>.</p>
    <p>Accordingly, the above description of illustrated embodiments of the invention is not intended to be exhaustive or to limit the invention to the precise forms disclosed. While specific embodiments of, and examples for, the invention are described herein for illustrative purposes, various equivalent modifications are possible within the scope of the invention, as those skilled in the relevant art will recognize.</p>
    <p>These modifications can be made to the invention in light of the above detailed description. The terms used in the following claims should not be construed to limit the invention to the specific embodiments disclosed in the specification and the claims. Rather, the scope of the invention is to be determined entirely by the following claims, which are to be construed in accordance with established doctrines of claim interpretation.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5383454">US5383454</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 2, 1992</td><td class="patent-data-table-td patent-date-value">Jan 24, 1995</td><td class="patent-data-table-td ">St. Louis University</td><td class="patent-data-table-td ">System for indicating the position of a surgical probe within a head on an image of the head</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5517990">US5517990</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 8, 1994</td><td class="patent-data-table-td patent-date-value">May 21, 1996</td><td class="patent-data-table-td ">The Cleveland Clinic Foundation</td><td class="patent-data-table-td ">Stereotaxy wand and tool guide</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5531520">US5531520</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 1, 1994</td><td class="patent-data-table-td patent-date-value">Jul 2, 1996</td><td class="patent-data-table-td ">Massachusetts Institute Of Technology</td><td class="patent-data-table-td ">System and method of registration of three-dimensional data sets including anatomical body data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5682890">US5682890</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 26, 1995</td><td class="patent-data-table-td patent-date-value">Nov 4, 1997</td><td class="patent-data-table-td ">Picker International, Inc.</td><td class="patent-data-table-td ">Magnetic resonance stereotactic surgery with exoskeleton tissue stabilization</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5787886">US5787886</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 10, 1995</td><td class="patent-data-table-td patent-date-value">Aug 4, 1998</td><td class="patent-data-table-td ">Compass International Incorporated</td><td class="patent-data-table-td ">Magnetic field digitizer for stereotatic surgery</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5891034">US5891034</a></td><td class="patent-data-table-td patent-date-value">Jun 7, 1995</td><td class="patent-data-table-td patent-date-value">Apr 6, 1999</td><td class="patent-data-table-td ">St. Louis University</td><td class="patent-data-table-td ">System for indicating the position of a surgical probe within a head on an image of the head</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6006126">US6006126</a></td><td class="patent-data-table-td patent-date-value">Jun 7, 1995</td><td class="patent-data-table-td patent-date-value">Dec 21, 1999</td><td class="patent-data-table-td ">Cosman; Eric R.</td><td class="patent-data-table-td ">System and method for stereotactic registration of image scan data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6351573">US6351573</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 7, 1996</td><td class="patent-data-table-td patent-date-value">Feb 26, 2002</td><td class="patent-data-table-td ">Schneider Medical Technologies, Inc.</td><td class="patent-data-table-td ">Imaging device and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/USRE30397">USRE30397</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 2, 1979</td><td class="patent-data-table-td patent-date-value">Sep 9, 1980</td><td class="patent-data-table-td "> </td><td class="patent-data-table-td ">Three-dimensional ultrasonic imaging of animal soft tissue</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">A. Jodicke et al., "<a href='http://scholar.google.com/scholar?q="Intraoperative+three-dimensional+ultrasonography%3A+an+approach+to+register+brain+shift+using+multidimensional+image+processing%2C"'>Intraoperative three-dimensional ultrasonography: an approach to register brain shift using multidimensional image processing,</a>" Minimally Invasive Neurosurgery, vol. 41, pp. 13-19, 1998.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">C. Kresmer et al., "<a href='http://scholar.google.com/scholar?q="Image+registration+of+MR+and+CT+images+using+a+frameless+fiducial+marker+system%2C"'>Image registration of MR and CT images using a frameless fiducial marker system,</a>" Magnetic Resonance Imaging, vol. 15, pp. 579-585, Nov. 5, 1997.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">C. R. Maurer Jr. et al., "<a href='http://scholar.google.com/scholar?q="Registration+of+head+volume+images+using+implantable+fiducial+markers%2C"'>Registration of head volume images using implantable fiducial markers,</a>" IEEE Transactions on Medical Imaging, vol. 16, pp. 447-462, Aug. 1997.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">C. R. Maurer, Jr. et al., "<a href='http://scholar.google.com/scholar?q="Registration+of+3-D+images+using+weighted+geometrical+features%2C"'>Registration of 3-D images using weighted geometrical features,</a>" IEEE Transactions on Medical Imaging, vol. 15, pp. 836-849, Dec. 1996.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">D. F. Leotta et al., "<a href='http://scholar.google.com/scholar?q="Performance+of+a+miniature+magnetic+position+sensor+for+three-dimensional+ultrasonic+imaging%2C"'>Performance of a miniature magnetic position sensor for three-dimensional ultrasonic imaging,</a>" Ultrasound in Medicine and Biology, vol. 23, pp. 597-609, 1997.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">D. L. G. Hill et al., "<a href='http://scholar.google.com/scholar?q="Estimation+of+intraoperative+brain+surface+movement%2C"'>Estimation of intraoperative brain surface movement,</a>" in Proceedings of the CVRMed-MRCAS'97, pp. 449-458, 1997.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">H. Erbe et al., "<a href='http://scholar.google.com/scholar?q="3-D+ultrasonography+and+image+matching+for+detection+of+brain+shift+during+intracranial+surgery%2C"'>3-D ultrasonography and image matching for detection of brain shift during intracranial surgery,</a>" in Proceedings of the Computer Assisted Radiology, pp. 225-230, 1996.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">H. Hirschberg et al., "<a href='http://scholar.google.com/scholar?q="Incorporation+of+ultrasonic+imaging+in+an+optically+coupled+frameless+stereotactic+system%2C"'>Incorporation of ultrasonic imaging in an optically coupled frameless stereotactic system,</a>" Acta Neurochir, vol. 68, pp. 75-80, 1997.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">J. M. Fitzpatrick et al., "<a href='http://scholar.google.com/scholar?q="Predicting+error+in+rigid-body+point-based+registration%2C"'>Predicting error in rigid-body point-based registration,</a>" IEEE Transactions on Medical Imaging, vol. 17, pp. 694-702, Oct. 1998.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">J. T. Lewis et al., "<a href='http://scholar.google.com/scholar?q="An+ultrasonic+approach+to+localization+of+fiducial+markers+for+interactive%2C+image-guided+neurosurgery-part+I%3B+principles%2C"'>An ultrasonic approach to localization of fiducial markers for interactive, image-guided neurosurgery-part I; principles,</a>" IEEE Transactions on Biomedical Engineering, vol. 45, pp. 621-630, May 1998.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">J. T. Lewis et al., "<a href='http://scholar.google.com/scholar?q="An+ultrasonic+approach+to+localization+of+fiducial+markers+for+interactive%2C+image-guided+neurosurgery%E2%80%94part+I%3B+principles%2C"'>An ultrasonic approach to localization of fiducial markers for interactive, image-guided neurosurgery—part I; principles,</a>" IEEE Transactions on Biomedical Engineering, vol. 45, pp. 621-630, May 1998.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">J. W. Trobaugh et al., "<a href='http://scholar.google.com/scholar?q="Frameless+stereotactic+ultrasonography%3A+method+and+applications%2C"'>Frameless stereotactic ultrasonography: method and applications,</a>" Computerized Medical Imaging and Graphics, vol. 18, pp. 235-246, Jul.-Aug. 1994.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">K. K. Shung et al., "<a href='http://scholar.google.com/scholar?q="Ultrasonic+transducers+and+arrays"'>Ultrasonic transducers and arrays</a>", IEEE Engineering in Medicine and Biology Magazine, vol. 15, pp. 20-30, Nov.-Dec. 1996.</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">K. R. Smith et al., "<a href='http://scholar.google.com/scholar?q="The+neurostation-a+highly+accurate%2C+minimally+invasive+solution+to+frameless+stereotactic+neurosurgery%2C"'>The neurostation-a highly accurate, minimally invasive solution to frameless stereotactic neurosurgery,</a>" Computerized Medical Imaging and Graphics, vol. 18, pp. 247-256, Jul.-Aug. 1994.</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">K. R. Smith et al., "<a href='http://scholar.google.com/scholar?q="The+neurostation%E2%80%94a+highly+accurate%2C+minimally+invasive+solution+to+frameless+stereotactic+neurosurgery%2C"'>The neurostation—a highly accurate, minimally invasive solution to frameless stereotactic neurosurgery,</a>" Computerized Medical Imaging and Graphics, vol. 18, pp. 247-256, Jul.-Aug. 1994.</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">K. S. Arun et al., "<a href='http://scholar.google.com/scholar?q="Least-squares+fitting+of+two+3-D+point+sets%2C"'>Least-squares fitting of two 3-D point sets,</a>" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 9, pp. 698-700, Sep. 1987.</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">M. D. Mitchell et al., "<a href='http://scholar.google.com/scholar?q="Agarose+as+a+tissue+equivalent+phantom+material+for+NRM+imaging%2C"'>Agarose as a tissue equivalent phantom material for NRM imaging,</a>" Magnetic Resonance Imaging, vol. 4, pp. 263-266, 1996.</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">M. S. Alp et al., "<a href='http://scholar.google.com/scholar?q="Head+registration+techniques+for+image-guided+surgery%2C"'>Head registration techniques for image-guided surgery,</a>" Neurological Research, vol. 20, pp. 31-37, Jan. 1998.</td></tr><tr><td class="patent-data-table-td ">19</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">N. Di-Lorenzo et al., "<a href='http://scholar.google.com/scholar?q="A+comparison+of+computerized+tomography-guided+sterotactic+and+ultrasound-guided+techniques+for+brain+biopsy%2C"'>A comparison of computerized tomography-guided sterotactic and ultrasound-guided techniques for brain biopsy,</a>" Journal of Neurosurgery, vol. 76, pp. 1044-1045, Nov. 1991.</td></tr><tr><td class="patent-data-table-td ">20</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">N. Hata et al., "<a href='http://scholar.google.com/scholar?q="Development+of+a+frameless+and+armless+stereotactic+neuronavigation+system+with+ultrasonographic+registration%2C"'>Development of a frameless and armless stereotactic neuronavigation system with ultrasonographic registration,</a>" Neurosurgery, vol. 41, pp. 608-612, Sep. 1997.</td></tr><tr><td class="patent-data-table-td ">21</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">N. Pagoulatos et al., "<a href='http://scholar.google.com/scholar?q="Calibration+and+validation+of+free-hand+3D+ultrasound+systems+based+on+DC+magnetic+tracking%2C"'>Calibration and validation of free-hand 3D ultrasound systems based on DC magnetic tracking,</a>" in Proceedings of the SPIE, vol. 3335, pp. 59-71, 1998.</td></tr><tr><td class="patent-data-table-td ">22</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">N.D. Kitchen et al., "<a href='http://scholar.google.com/scholar?q="Accuracy+in+frame-based+and+frameless+stereotaxy%2C"'>Accuracy in frame-based and frameless stereotaxy,</a>" Stereotactic Functional Neurosurgery, vol. 61, pp. 195-206, 1993.</td></tr><tr><td class="patent-data-table-td ">23</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">P. Couillard et al., "<a href='http://scholar.google.com/scholar?q="Focus+on+peroperative+ultrasonography%2C"'>Focus on peroperative ultrasonography,</a>" Neurochirgurie, vol. 42, pp. 91-94, 1996.</td></tr><tr><td class="patent-data-table-td ">24</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">P. R. Detmer et al., "<a href='http://scholar.google.com/scholar?q="3D+ultrasonic+image+feature+localization+based+on+magnetic+scanhead+tracking%3A+in+vitro+calibration+and+validation%2C"'>3D ultrasonic image feature localization based on magnetic scanhead tracking: in vitro calibration and validation,</a>" Ultrasound in Medicine and Biology, vol. 20, pp. 923-936, 1994.</td></tr><tr><td class="patent-data-table-td ">25</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">R. A. Brown, "<a href='http://scholar.google.com/scholar?q="A+stereotactic+head+frame+for+use+with+CT+body+scanners%2C"'>A stereotactic head frame for use with CT body scanners,</a>" Investigative Radiology, vol. 14, pp. 300-304, Jul.-Aug. 1979.</td></tr><tr><td class="patent-data-table-td ">26</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">R. D. Bucholz et al., "<a href='http://scholar.google.com/scholar?q="The+correction+of+stereotactic+inaccuracy+caused+by+brain+shift+using+an+intraoperative+ultrasound+device%2C"'>The correction of stereotactic inaccuracy caused by brain shift using an intraoperative ultrasound device,</a>" in Proceedings of the CVRMed-MRCAS'97, pp. 459-466, 1997.</td></tr><tr><td class="patent-data-table-td ">27</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">R. L. Galloway, Jr., "<a href='http://scholar.google.com/scholar?q="Frameless+stereotatic+systems%2C"'>Frameless stereotatic systems,</a>" in Textbook of Sterotactic and Functional Neurosurgery, P. L. Gildenberg and R.R. Tasker, Eds., ch. 21, pp. 177-182, McGraw Hill, New York, 1998.</td></tr><tr><td class="patent-data-table-td ">28</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">R. M. Comeau et al., "<a href='http://scholar.google.com/scholar?q="Integrated+MR+and+ultrasound+imaging+for+improved+image+guidance+in+neurosurgery"'>Integrated MR and ultrasound imaging for improved image guidance in neurosurgery</a>", in Proceedings of the SPIE, vol. 3338, pp. 747-754, Feb. 1998.</td></tr><tr><td class="patent-data-table-td ">29</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">R. M. Comeau et al., "<a href='http://scholar.google.com/scholar?q="Intraoperative+US+in+interactive+image-guided+neurosurgery%2C"'>Intraoperative US in interactive image-guided neurosurgery,</a>" Radiographics, vol. 18, pp. 1019-1027, Jul.-Aug. 1998.</td></tr><tr><td class="patent-data-table-td ">30</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">R. W. Prager et al., "<a href='http://scholar.google.com/scholar?q="Rapid+calibration+for+3D+free-hand+ultrasound%2C"'>Rapid calibration for 3D free-hand ultrasound,</a>" Ultrasound Med. Biol., vol. 24, pp. 855-869, Mar. 16, 1998.</td></tr><tr><td class="patent-data-table-td ">31</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">S. J. Goerss et al., "<a href='http://scholar.google.com/scholar?q="A+sterotactic+magnetic+field+digitizer%2C"'>A sterotactic magnetic field digitizer,</a>" in Stereotactic and Functional Neurosurgery, vol. 63, pp. 89-92, 1994.</td></tr><tr><td class="patent-data-table-td ">32</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">S. Lavallee, "<a href='http://scholar.google.com/scholar?q="Registration+for+computer-integrated+surgery%3A+methodology%2C+state+of+the+art%2C"'>Registration for computer-integrated surgery: methodology, state of the art,</a>" in Computer-Integrated Surgery: Technology and Clinical Applications, pp. 77-97, MIT Press, Cambridge, MA, 1996.</td></tr><tr><td class="patent-data-table-td ">33</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">V. M. Tronnier et al., "<a href='http://scholar.google.com/scholar?q="Intraoperative+diagnostic+and+interventional+magnetic+resonance+imaging+in+neurosurgery%2C"'>Intraoperative diagnostic and interventional magnetic resonance imaging in neurosurgery,</a>" Neurosurgery, vol. 40, pp. 891-898, May 1997.</td></tr><tr><td class="patent-data-table-td ">34</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">W. E. Butler et al., "<a href='http://scholar.google.com/scholar?q="A+mobile+computed+tomographic+scanner+with+intraoperative+and+intensive+care+unit+applications%2C"'>A mobile computed tomographic scanner with intraoperative and intensive care unit applications,</a>" Neurosurgery, vol. 42, pp. 1305-1310, Jun. 1998.</td></tr><tr><td class="patent-data-table-td ">35</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">W. S. Edwards et al., "<a href='http://scholar.google.com/scholar?q="PC-based+workstation+for+three-dimensional+visualization+of+ultrasound+images%2C"'>PC-based workstation for three-dimensional visualization of ultrasound images,</a>" in Proceedings of the SPIE, vol. 3031, pp. 1163-1176, 1995.</td></tr><tr><td class="patent-data-table-td ">36</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Y. Kim et al., "<a href='http://scholar.google.com/scholar?q="Programmable+ultrasound+imaging+using+multimedia+technologies%3A+A+next-generation+ultrasound+machine%2C"'>Programmable ultrasound imaging using multimedia technologies: A next-generation ultrasound machine,</a>" IEEE Transactions on Information Technology in Biomedicine, vol. 1, pp. 19-29, Mar. 1997.</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7090639">US7090639</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 29, 2003</td><td class="patent-data-table-td patent-date-value">Aug 15, 2006</td><td class="patent-data-table-td ">Biosense, Inc.</td><td class="patent-data-table-td ">Ultrasound catheter calibration system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7505809">US7505809</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 9, 2003</td><td class="patent-data-table-td patent-date-value">Mar 17, 2009</td><td class="patent-data-table-td ">Mediguide Ltd.</td><td class="patent-data-table-td ">Method and system for registering a first image with a second image relative to the body of a patient</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7561051">US7561051</a></td><td class="patent-data-table-td patent-date-value">Apr 18, 2006</td><td class="patent-data-table-td patent-date-value">Jul 14, 2009</td><td class="patent-data-table-td ">Creare Inc.</td><td class="patent-data-table-td ">Magnet locating apparatus and method of locating a magnet using such apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7697973">US7697973</a></td><td class="patent-data-table-td patent-date-value">Aug 20, 2007</td><td class="patent-data-table-td patent-date-value">Apr 13, 2010</td><td class="patent-data-table-td ">MediGuide, Ltd.</td><td class="patent-data-table-td ">Medical imaging and navigation system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7711405">US7711405</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 28, 2004</td><td class="patent-data-table-td patent-date-value">May 4, 2010</td><td class="patent-data-table-td ">Siemens Corporation</td><td class="patent-data-table-td ">Method of registering pre-operative high field closed magnetic resonance images with intra-operative low field open interventional magnetic resonance images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7778688">US7778688</a></td><td class="patent-data-table-td patent-date-value">Sep 9, 2004</td><td class="patent-data-table-td patent-date-value">Aug 17, 2010</td><td class="patent-data-table-td ">MediGuide, Ltd.</td><td class="patent-data-table-td ">System and method for delivering a stent to a selected position within a lumen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7782473">US7782473</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 21, 2004</td><td class="patent-data-table-td patent-date-value">Aug 24, 2010</td><td class="patent-data-table-td ">Ricoh Company, Ltd.</td><td class="patent-data-table-td ">Apparatus for transforming image data for another and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7813591">US7813591</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 3, 2006</td><td class="patent-data-table-td patent-date-value">Oct 12, 2010</td><td class="patent-data-table-td ">3M Innovative Properties Company</td><td class="patent-data-table-td ">Visual feedback of 3D scan parameters</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7831015">US7831015</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2009</td><td class="patent-data-table-td patent-date-value">Nov 9, 2010</td><td class="patent-data-table-td ">General Electric Company</td><td class="patent-data-table-td ">Combining X-ray and ultrasound imaging for enhanced mammography</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7840042">US7840042</a></td><td class="patent-data-table-td patent-date-value">Sep 3, 2006</td><td class="patent-data-table-td patent-date-value">Nov 23, 2010</td><td class="patent-data-table-td ">3M Innovative Properties Company</td><td class="patent-data-table-td ">Superposition for visualization of three-dimensional data acquisition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7840252">US7840252</a></td><td class="patent-data-table-td patent-date-value">Sep 23, 2005</td><td class="patent-data-table-td patent-date-value">Nov 23, 2010</td><td class="patent-data-table-td ">MediGuide, Ltd.</td><td class="patent-data-table-td ">Method and system for determining a three dimensional representation of a tubular organ</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7881767">US7881767</a></td><td class="patent-data-table-td patent-date-value">Aug 18, 2004</td><td class="patent-data-table-td patent-date-value">Feb 1, 2011</td><td class="patent-data-table-td ">Mediguide Ltd.</td><td class="patent-data-table-td ">Method and system for registering a medical situation associated with a first coordinate system, in a second coordinate system using an MPS system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7912257">US7912257</a></td><td class="patent-data-table-td patent-date-value">Sep 3, 2006</td><td class="patent-data-table-td patent-date-value">Mar 22, 2011</td><td class="patent-data-table-td ">3M Innovative Properties Company</td><td class="patent-data-table-td ">Real time display of acquired 3D dental data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7930014">US7930014</a></td><td class="patent-data-table-td patent-date-value">Jan 11, 2006</td><td class="patent-data-table-td patent-date-value">Apr 19, 2011</td><td class="patent-data-table-td ">Volcano Corporation</td><td class="patent-data-table-td ">Vascular image co-registration</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8090168">US8090168</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 15, 2007</td><td class="patent-data-table-td patent-date-value">Jan 3, 2012</td><td class="patent-data-table-td ">General Electric Company</td><td class="patent-data-table-td ">Method and system for visualizing registered images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8131344">US8131344</a></td><td class="patent-data-table-td patent-date-value">Feb 6, 2009</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">MediGuide, Ltd.</td><td class="patent-data-table-td ">Method and system for registering a medical situation associated with a first coordinate system, in a second coordinate system using an MPS system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8131345">US8131345</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 20, 2007</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">Esaote S.P.A.</td><td class="patent-data-table-td ">Combining first and second image data of an object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8189889">US8189889</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 20, 2009</td><td class="patent-data-table-td patent-date-value">May 29, 2012</td><td class="patent-data-table-td ">Loma Linda University Medical Center</td><td class="patent-data-table-td ">Systems and methods for characterizing spatial distortion in 3D imaging systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8214021">US8214021</a></td><td class="patent-data-table-td patent-date-value">Dec 16, 2008</td><td class="patent-data-table-td patent-date-value">Jul 3, 2012</td><td class="patent-data-table-td ">General Electric Company</td><td class="patent-data-table-td ">Medical imaging system and method containing ultrasound docking port</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8215956">US8215956</a></td><td class="patent-data-table-td patent-date-value">Feb 23, 2010</td><td class="patent-data-table-td patent-date-value">Jul 10, 2012</td><td class="patent-data-table-td ">3M Innovative Properties Company</td><td class="patent-data-table-td ">Dental articulator with positioning key</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8219179">US8219179</a></td><td class="patent-data-table-td patent-date-value">Mar 6, 2008</td><td class="patent-data-table-td patent-date-value">Jul 10, 2012</td><td class="patent-data-table-td ">Vida Diagnostics, Inc.</td><td class="patent-data-table-td ">Systems and methods for navigation within a branched structure of a body</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8219181">US8219181</a></td><td class="patent-data-table-td patent-date-value">Dec 16, 2008</td><td class="patent-data-table-td patent-date-value">Jul 10, 2012</td><td class="patent-data-table-td ">General Electric Company</td><td class="patent-data-table-td ">Medical imaging system and method containing ultrasound docking port</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8226560">US8226560</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 10, 2004</td><td class="patent-data-table-td patent-date-value">Jul 24, 2012</td><td class="patent-data-table-td ">Hitachi Medical Corporation</td><td class="patent-data-table-td ">Reference image display method for ultrasonography and ultrasonic diagnosis apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8235909">US8235909</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 11, 2005</td><td class="patent-data-table-td patent-date-value">Aug 7, 2012</td><td class="patent-data-table-td ">Guided Therapy Systems, L.L.C.</td><td class="patent-data-table-td ">Method and system for controlled scanning, imaging and/or therapy</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8262388">US8262388</a></td><td class="patent-data-table-td patent-date-value">Feb 23, 2010</td><td class="patent-data-table-td patent-date-value">Sep 11, 2012</td><td class="patent-data-table-td ">3M Innovative Properties Company</td><td class="patent-data-table-td ">Local enforcement of accuracy in fabricated models</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8290303">US8290303</a></td><td class="patent-data-table-td patent-date-value">Oct 11, 2007</td><td class="patent-data-table-td patent-date-value">Oct 16, 2012</td><td class="patent-data-table-td ">General Electric Company</td><td class="patent-data-table-td ">Enhanced system and method for volume based registration</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8298147">US8298147</a></td><td class="patent-data-table-td patent-date-value">Jun 23, 2006</td><td class="patent-data-table-td patent-date-value">Oct 30, 2012</td><td class="patent-data-table-td ">Volcano Corporation</td><td class="patent-data-table-td ">Three dimensional co-registration for intravascular diagnosis and therapy</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8303502">US8303502</a></td><td class="patent-data-table-td patent-date-value">Mar 6, 2007</td><td class="patent-data-table-td patent-date-value">Nov 6, 2012</td><td class="patent-data-table-td ">General Electric Company</td><td class="patent-data-table-td ">Method and apparatus for tracking points in an ultrasound image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8340379">US8340379</a></td><td class="patent-data-table-td patent-date-value">Mar 6, 2009</td><td class="patent-data-table-td patent-date-value">Dec 25, 2012</td><td class="patent-data-table-td ">Inneroptic Technology, Inc.</td><td class="patent-data-table-td ">Systems and methods for displaying guidance data based on updated deformable imaging data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8350902">US8350902</a></td><td class="patent-data-table-td patent-date-value">Dec 21, 2011</td><td class="patent-data-table-td patent-date-value">Jan 8, 2013</td><td class="patent-data-table-td ">Inneroptic Technology, Inc.</td><td class="patent-data-table-td ">System and method of providing real-time dynamic imagery of a medical procedure site using multiple modalities</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8364242">US8364242</a></td><td class="patent-data-table-td patent-date-value">Apr 1, 2008</td><td class="patent-data-table-td patent-date-value">Jan 29, 2013</td><td class="patent-data-table-td ">General Electric Company</td><td class="patent-data-table-td ">System and method of combining ultrasound image acquisition with fluoroscopic image acquisition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8374714">US8374714</a></td><td class="patent-data-table-td patent-date-value">Feb 23, 2010</td><td class="patent-data-table-td patent-date-value">Feb 12, 2013</td><td class="patent-data-table-td ">3M Innovative Properties Company</td><td class="patent-data-table-td ">Local enforcement of accuracy in fabricated models</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8380007">US8380007</a></td><td class="patent-data-table-td patent-date-value">Dec 5, 2006</td><td class="patent-data-table-td patent-date-value">Feb 19, 2013</td><td class="patent-data-table-td ">Koninklijke Philips Electronics N.V.</td><td class="patent-data-table-td ">System and method for enabling selection of an image registration transformation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8428690">US8428690</a></td><td class="patent-data-table-td patent-date-value">Apr 1, 2008</td><td class="patent-data-table-td patent-date-value">Apr 23, 2013</td><td class="patent-data-table-td ">General Electric Company</td><td class="patent-data-table-td ">Intracardiac echocardiography image reconstruction in combination with position tracking system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8442618">US8442618</a></td><td class="patent-data-table-td patent-date-value">Sep 16, 2005</td><td class="patent-data-table-td patent-date-value">May 14, 2013</td><td class="patent-data-table-td ">Mediguide Ltd.</td><td class="patent-data-table-td ">Method and system for delivering a medical device to a selected position within a lumen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8454365">US8454365</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 2007</td><td class="patent-data-table-td patent-date-value">Jun 4, 2013</td><td class="patent-data-table-td ">3M Innovative Properties Company</td><td class="patent-data-table-td ">Digital dentistry</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8480407">US8480407</a></td><td class="patent-data-table-td patent-date-value">Aug 11, 2009</td><td class="patent-data-table-td patent-date-value">Jul 9, 2013</td><td class="patent-data-table-td ">National Research Council Of Canada</td><td class="patent-data-table-td ">Tissue-mimicking phantom for prostate cancer brachytherapy</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8527032">US8527032</a></td><td class="patent-data-table-td patent-date-value">Mar 27, 2008</td><td class="patent-data-table-td patent-date-value">Sep 3, 2013</td><td class="patent-data-table-td ">General Electric Company</td><td class="patent-data-table-td ">Imaging system and method of delivery of an instrument to an imaged subject</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8538108">US8538108</a></td><td class="patent-data-table-td patent-date-value">Dec 19, 2006</td><td class="patent-data-table-td patent-date-value">Sep 17, 2013</td><td class="patent-data-table-td ">University Of Maryland, Baltimore</td><td class="patent-data-table-td ">Method and apparatus for accelerated elastic registration of multiple scans of internal properties of a body</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8560050">US8560050</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 21, 2008</td><td class="patent-data-table-td patent-date-value">Oct 15, 2013</td><td class="patent-data-table-td ">Siemens Aktiengesellschaft</td><td class="patent-data-table-td ">Method and device for imaging objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8619237">US8619237</a></td><td class="patent-data-table-td patent-date-value">Dec 6, 2010</td><td class="patent-data-table-td patent-date-value">Dec 31, 2013</td><td class="patent-data-table-td ">The Trustees Of Columbia University In The City Of New York</td><td class="patent-data-table-td ">Laser-scanning intersecting plane tomography such as for high speed volumetric optical imaging</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8700132">US8700132</a></td><td class="patent-data-table-td patent-date-value">Jun 13, 2012</td><td class="patent-data-table-td patent-date-value">Apr 15, 2014</td><td class="patent-data-table-td ">Vida Diagnostics, Inc.</td><td class="patent-data-table-td ">Systems and methods for navigation within a branched structure of a body</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8731264">US8731264</a></td><td class="patent-data-table-td patent-date-value">Nov 26, 2007</td><td class="patent-data-table-td patent-date-value">May 20, 2014</td><td class="patent-data-table-td ">Koninklijke Philips N.V.</td><td class="patent-data-table-td ">System and method for fusing real-time ultrasound images with pre-acquired medical images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8738340">US8738340</a></td><td class="patent-data-table-td patent-date-value">Feb 23, 2010</td><td class="patent-data-table-td patent-date-value">May 27, 2014</td><td class="patent-data-table-td ">3M Innovative Properties Company</td><td class="patent-data-table-td ">Local enforcement of accuracy in fabricated models</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090097723">US20090097723</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 15, 2007</td><td class="patent-data-table-td patent-date-value">Apr 16, 2009</td><td class="patent-data-table-td ">General Electric Company</td><td class="patent-data-table-td ">Method and system for visualizing registered images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090307628">US20090307628</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 21, 2009</td><td class="patent-data-table-td patent-date-value">Dec 10, 2009</td><td class="patent-data-table-td ">Metala Michael J</td><td class="patent-data-table-td ">Non-Destructive Examination Data Visualization and Analysis</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100021029">US20100021029</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 20, 2009</td><td class="patent-data-table-td patent-date-value">Jan 28, 2010</td><td class="patent-data-table-td ">Pearlstein Robert D</td><td class="patent-data-table-td ">Systems and methods for characterizing spatial distortion in 3d imaging systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120320083">US20120320083</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 24, 2012</td><td class="patent-data-table-td patent-date-value">Dec 20, 2012</td><td class="patent-data-table-td ">Koninklijke Philips Electronics N.V.</td><td class="patent-data-table-td ">Multi-modality medical image viewing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN1760915B?cl=en">CN1760915B</a></td><td class="patent-data-table-td patent-date-value">Sep 16, 2005</td><td class="patent-data-table-td patent-date-value">Mar 14, 2012</td><td class="patent-data-table-td ">Medcom医学图像处理有限公司</td><td class="patent-data-table-td ">Registration of first and second image data of an object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE102004048066A1?cl=en">DE102004048066A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 2004</td><td class="patent-data-table-td patent-date-value">Apr 20, 2006</td><td class="patent-data-table-td ">Tecmedic Gmbh</td><td class="patent-data-table-td ">Device for geometric calibration of different measurement devices, especially when using image generating operating, therapeutic or diagnostic methods, balances measurement devices using known relative position, orientation of two markings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2279695A1?cl=en">EP2279695A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 16, 2010</td><td class="patent-data-table-td patent-date-value">Feb 2, 2011</td><td class="patent-data-table-td ">Medison Co., Ltd.</td><td class="patent-data-table-td ">Sensor coordinate calibration in an ultrasound system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2347718A1?cl=en">EP2347718A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 30, 2008</td><td class="patent-data-table-td patent-date-value">Jul 27, 2011</td><td class="patent-data-table-td ">Biosense Webster, Inc.</td><td class="patent-data-table-td ">Ultrasound catheter calibration with enhanced accuracy</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2006092594A2?cl=en">WO2006092594A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 1, 2006</td><td class="patent-data-table-td patent-date-value">Sep 8, 2006</td><td class="patent-data-table-td ">King S College London</td><td class="patent-data-table-td ">3d ultrasound registration</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2008065600A2?cl=en">WO2008065600A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 26, 2007</td><td class="patent-data-table-td patent-date-value">Jun 5, 2008</td><td class="patent-data-table-td ">Koninkl Philips Electronics Nv</td><td class="patent-data-table-td ">System and method for fusing real-time ultrasound images with pre-acquired medical images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2009005748A1?cl=en">WO2009005748A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 27, 2008</td><td class="patent-data-table-td patent-date-value">Jan 8, 2009</td><td class="patent-data-table-td ">Sean A Burgess</td><td class="patent-data-table-td ">Optical imaging or spectroscopy systems and methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2012149357A1?cl=en">WO2012149357A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 27, 2012</td><td class="patent-data-table-td patent-date-value">Nov 1, 2012</td><td class="patent-data-table-td ">Medtronic Navigation, Inc.</td><td class="patent-data-table-td ">Method and apparatus for calibrating and re-aligning an ultrasound image plane to a navigation tracker</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2014016736A2?cl=en">WO2014016736A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 17, 2013</td><td class="patent-data-table-td patent-date-value">Jan 30, 2014</td><td class="patent-data-table-td ">Koninklijke Philips N.V.</td><td class="patent-data-table-td ">Accurate and rapid mapping of points from ultrasound images to tracking systems</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S154000">382/154</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc600/defs600.htm&usg=AFQjCNGoRRWCr3AADgOfjOLv9kXGux4XKA#C600S429000">600/429</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc128/defs128.htm&usg=AFQjCNGjl9q_l-XTxZ44q3zsgsK3JYPF-w#C128S916000">128/916</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc600/defs600.htm&usg=AFQjCNGoRRWCr3AADgOfjOLv9kXGux4XKA#C600S443000">600/443</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S276000">382/276</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc600/defs600.htm&usg=AFQjCNGoRRWCr3AADgOfjOLv9kXGux4XKA#C600S426000">600/426</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S151000">382/151</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc606/defs606.htm&usg=AFQjCNEEvaL03g4JHfjHz3wZXhk_WBlsyw#C606S130000">606/130</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0007000000">G06T7/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0003000000">G06T3/00</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S128/916">Y10S128/916</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06T7/0038">G06T7/0038</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06T3/00">G06T3/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06T7/0034">G06T7/0034</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A61B8/587">A61B8/587</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A61B8/58">A61B8/58</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZOJnBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=A61B8/0816">A61B8/0816</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06T3/00</span>, <span class="nested-value">G06T7/00D1Z</span>, <span class="nested-value">G06T7/00D1S</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Mar 4, 2014</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIMS 1-14 AND 18 ARE CANCELLED.CLAIMS 15-17 AND 19 WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 10, 2012</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 19, 2011</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20110527</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 18, 2008</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 11, 2008</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 17, 2000</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">WASHINGTON, UNIVERSITY OF, WASHINGTON</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:PAGOULATOS, NIKO;HAYNOR, DAVID R.;EDWARDS, WARREN S.;ANDOTHERS;REEL/FRAME:010972/0807;SIGNING DATES FROM 20000621 TO 20000627</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_4ff636b3d23669b7103f3b3a3a18b4cd.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U3udalY8AjieC-l921PWxQTvP73iQ\u0026id=ZOJnBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U34WDdSaqexx1pSsA_Bbq2PRRi5nQ\u0026id=ZOJnBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U2EfH6RTpMr3_vm8DyIva2Co_1EAg","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Apparatus_and_method_for_interactive_3D.pdf?id=ZOJnBAABERAJ\u0026output=pdf\u0026sig=ACfU3U1_28iI-YzHhJLld8rdx5-AqrB4oQ"},"sample_url":"http://www.google.com/patents/reader?id=ZOJnBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>