<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7424175 - Video segmentation using statistical pixel modeling - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Video segmentation using statistical pixel modeling"><meta name="DC.contributor" content="Alan J. Lipton" scheme="inventor"><meta name="DC.contributor" content="Niels Haering" scheme="inventor"><meta name="DC.contributor" content="Zeeshan Rasheed" scheme="inventor"><meta name="DC.contributor" content="Omar Javed" scheme="inventor"><meta name="DC.contributor" content="Zhong Zhang" scheme="inventor"><meta name="DC.contributor" content="Weihong Yin" scheme="inventor"><meta name="DC.contributor" content="PÃ©ter L. Venetianer" scheme="inventor"><meta name="DC.contributor" content="Gary W. Myers" scheme="inventor"><meta name="DC.contributor" content="Objectvideo, Inc." scheme="assignee"><meta name="DC.date" content="2007-2-27" scheme="dateSubmitted"><meta name="DC.description" content="A method for segmenting video data into foreground and background portions utilizes statistical modeling of the pixels. A statistical model of the background is built for each pixel, and each pixel in an incoming video frame is compared with the background statistical model for that pixel. Pixels are determined to be foreground or background based on the comparisons. The method for segmenting video data may be further incorporated into a method for implementing an intelligent video surveillance system. The method for segmenting video data may be implemented in hardware."><meta name="DC.date" content="2008-9-9" scheme="issued"><meta name="DC.relation" content="US:20020063154:A1" scheme="references"><meta name="DC.relation" content="US:20040151374:A1" scheme="references"><meta name="DC.relation" content="US:20040240546:A1" scheme="references"><meta name="DC.relation" content="US:20050169367:A1" scheme="references"><meta name="DC.relation" content="US:20060066722:A1" scheme="references"><meta name="DC.relation" content="US:20060222209:A1" scheme="references"><meta name="DC.relation" content="US:20060268111:A1" scheme="references"><meta name="DC.relation" content="US:4764971" scheme="references"><meta name="DC.relation" content="US:4949389" scheme="references"><meta name="DC.relation" content="US:5048095" scheme="references"><meta name="DC.relation" content="US:5448651" scheme="references"><meta name="DC.relation" content="US:5519789" scheme="references"><meta name="DC.relation" content="US:5586200" scheme="references"><meta name="DC.relation" content="US:5671294" scheme="references"><meta name="DC.relation" content="US:5696551" scheme="references"><meta name="DC.relation" content="US:5724456" scheme="references"><meta name="DC.relation" content="US:5764306" scheme="references"><meta name="DC.relation" content="US:5768413" scheme="references"><meta name="DC.relation" content="US:5802203" scheme="references"><meta name="DC.relation" content="US:5875305" scheme="references"><meta name="DC.relation" content="US:5969755" scheme="references"><meta name="DC.relation" content="US:5990955" scheme="references"><meta name="DC.relation" content="US:6008865" scheme="references"><meta name="DC.relation" content="US:6049363" scheme="references"><meta name="DC.relation" content="US:6058210" scheme="references"><meta name="DC.relation" content="US:6078619" scheme="references"><meta name="DC.relation" content="US:6084912" scheme="references"><meta name="DC.relation" content="US:6195458" scheme="references"><meta name="DC.relation" content="US:6249613" scheme="references"><meta name="DC.relation" content="US:6278466" scheme="references"><meta name="DC.relation" content="US:6337917" scheme="references"><meta name="DC.relation" content="US:6349113" scheme="references"><meta name="DC.relation" content="US:6393054" scheme="references"><meta name="DC.relation" content="US:6396876" scheme="references"><meta name="DC.relation" content="US:6404925" scheme="references"><meta name="DC.relation" content="US:6625310" scheme="references"><meta name="DC.relation" content="US:6658136" scheme="references"><meta name="DC.relation" content="US:6738424" scheme="references"><meta name="DC.relation" content="US:6930689" scheme="references"><meta name="DC.relation" content="US:6987451" scheme="references"><meta name="citation_reference" content="U.S. Appl. No. 09/472,162, filed Dec. 27, 1999, Strat."><meta name="citation_reference" content="U.S. Appl. No. 11/139,986, filed May 31, 2005, Zhang."><meta name="citation_reference" content="U.S. Appl. No. 11/288,200, filed Nov. 29, 2005, Venetianer."><meta name="citation_patent_number" content="US:7424175"><meta name="citation_patent_application_number" content="US:11/711,063"><link rel="canonical" href="http://www.google.com/patents/US7424175"/><meta property="og:url" content="http://www.google.com/patents/US7424175"/><meta name="title" content="Patent US7424175 - Video segmentation using statistical pixel modeling"/><meta name="description" content="A method for segmenting video data into foreground and background portions utilizes statistical modeling of the pixels. A statistical model of the background is built for each pixel, and each pixel in an incoming video frame is compared with the background statistical model for that pixel. Pixels are determined to be foreground or background based on the comparisons. The method for segmenting video data may be further incorporated into a method for implementing an intelligent video surveillance system. The method for segmenting video data may be implemented in hardware."/><meta property="og:title" content="Patent US7424175 - Video segmentation using statistical pixel modeling"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("tEnsU7D_OdLzgwSmnYCgCA"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("BRA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("tEnsU7D_OdLzgwSmnYCgCA"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("BRA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7424175?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7424175"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=ykReBQABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7424175&amp;usg=AFQjCNHJnHVBiPo6l6-jfeILZidmgGEAJg" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7424175.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7424175.pdf"></a><a class="appbar-application-grant-link" data-label="Application" href="/patents/US20070160289"></a><a class="appbar-application-grant-link" data-selected="true" data-label="Grant" href="/patents/US7424175"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7424175" style="display:none"><span itemprop="description">A method for segmenting video data into foreground and background portions utilizes statistical modeling of the pixels. A statistical model of the background is built for each pixel, and each pixel in an incoming video frame is compared with the background statistical model for that pixel. Pixels are...</span><span itemprop="url">http://www.google.com/patents/US7424175?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7424175 - Video segmentation using statistical pixel modeling</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7424175 - Video segmentation using statistical pixel modeling" title="Patent US7424175 - Video segmentation using statistical pixel modeling"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7424175 B2</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 11/711,063</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Sep 9, 2008</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Feb 27, 2007</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Mar 23, 2001</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US20070160289">US20070160289</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20130242095">US20130242095</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">11711063, </span><span class="patent-bibdata-value">711063, </span><span class="patent-bibdata-value">US 7424175 B2, </span><span class="patent-bibdata-value">US 7424175B2, </span><span class="patent-bibdata-value">US-B2-7424175, </span><span class="patent-bibdata-value">US7424175 B2, </span><span class="patent-bibdata-value">US7424175B2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Alan+J.+Lipton%22">Alan J. Lipton</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Niels+Haering%22">Niels Haering</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Zeeshan+Rasheed%22">Zeeshan Rasheed</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Omar+Javed%22">Omar Javed</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Zhong+Zhang%22">Zhong Zhang</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Weihong+Yin%22">Weihong Yin</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22P%C3%A9ter+L.+Venetianer%22">PÃ©ter L. Venetianer</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Gary+W.+Myers%22">Gary W. Myers</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Objectvideo,+Inc.%22">Objectvideo, Inc.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7424175.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7424175.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7424175.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (40),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (3),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (11),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (10),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (8)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=ykReBQABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7424175&usg=AFQjCNGlIk6n-zG0medJbhAcqaEvaGY3lQ">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=ykReBQABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7424175&usg=AFQjCNFSLFlwhAipRQTdcjJjR2WYlLvT7w">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=ykReBQABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7424175B2%26KC%3DB2%26FT%3DD&usg=AFQjCNGX_RKrocAO2WDZfFFBty6g5CnJ9A">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT67853202" lang="EN" load-source="patent-office">Video segmentation using statistical pixel modeling</invention-title></span><br><span class="patent-number">US 7424175 B2</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA51356596" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">A method for segmenting video data into foreground and background portions utilizes statistical modeling of the pixels. A statistical model of the background is built for each pixel, and each pixel in an incoming video frame is compared with the background statistical model for that pixel. Pixels are determined to be foreground or background based on the comparisons. The method for segmenting video data may be further incorporated into a method for implementing an intelligent video surveillance system. The method for segmenting video data may be implemented in hardware.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(20)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00018.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00018.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7424175B2/US07424175-20080909-D00019.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7424175B2/US07424175-20080909-D00019.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(19)</span></span></div><div class="patent-text"><div mxw-id="PCLM9448626" lang="EN" load-source="patent-office" class="claims">
  <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
    <div class="claim-text">1. Circuitry adapted to perform a one-pass video segmentation for differentiating between foreground and background portions of video, the circuitry comprising:
<div class="claim-text">circuitry to obtain a frame sequence from a video stream; and</div>
<div class="claim-text">for each frame in the frame sequence:
<div class="claim-text">circuitry to align the frame with a scene model;</div>
<div class="claim-text">circuitry to build a background statistical model, the background statistical model comprising values corresponding to regions of frames of the frame sequence and variances for the regions;</div>
<div class="claim-text">circuitry to label the regions of the frame; and</div>
<div class="claim-text">circuitry to perform spatial or temporal filtering.</div>
</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00002" num="00002" class="claim">
    <div class="claim-text">2. A one-pass method of video segmentation, for differentiating between foreground and background portions of video, comprising the steps of:
<div class="claim-text">obtaining a real-time video stream; and</div>
<div class="claim-text">for each frame in the real-time frame stream, performing the following steps:
<div class="claim-text">labeling pixels in the frame;</div>
<div class="claim-text">performing spatial or temporal filtering;</div>
<div class="claim-text">updating a background statistical model, after the pixels are labeled; and</div>
<div class="claim-text">at least one of building or updating at least one foreground statistical model, after the pixels are labeled.</div>
</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
    <div class="claim-text">3. A method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein for each frame in the real-time frame stream, further performing the following step:
<div class="claim-text">inserting an object into the background statistical model.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
    <div class="claim-text">4. A method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein labeling pixels in the frame comprises:
<div class="claim-text">compensating for global lighting change to the frame.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
    <div class="claim-text">5. A method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein updating the background statistical model and the foreground statistical model comprises:
<div class="claim-text">compensating for global lighting change to the frame.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
    <div class="claim-text">6. A method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein pixels are labeled in the frame for an area of interest in the frame.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
    <div class="claim-text">7. A method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:
<div class="claim-text">at least one of building or updating the background statistical model and the foreground statistical model pixels for an area of interest in the frame.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
    <div class="claim-text">8. A method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:
<div class="claim-text">updating the background statistical model based on an infinite impulse response filter applied to previous statistical values of the background statistical model.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
    <div class="claim-text">9. A method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:
<div class="claim-text">updating the foreground statistical model based on an infinite impulse response filter applied to previous statistical values of the foreground statistical model.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00010" num="00010" class="claim">
    <div class="claim-text">10. Circuitry adapted to perform a one-pass video segmentation for differentiating between foreground and background portions of video, comprising:
<div class="claim-text">circuitry to obtain a real-time video stream; and</div>
<div class="claim-text">for each frame in the real-time frame stream:
<div class="claim-text">circuitry to label pixels in the frame;</div>
<div class="claim-text">circuitry to perform spatial or temporal filtering;</div>
<div class="claim-text">circuitry to update a background statistical model, after the pixels are labeled; and</div>
<div class="claim-text">circuitry to at least one of build or update at least one foreground statistical model, after the pixels are labeled.</div>
</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
    <div class="claim-text">11. A circuitry according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:
<div class="claim-text">for each frame in the real-time frame stream: circuitry to insert an object into the background statistical model.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
    <div class="claim-text">12. A circuitry according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the circuitry to label pixels in the frame comprises:
<div class="claim-text">circuitry to compensate for global lighting change to the frame.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
    <div class="claim-text">13. A circuitry according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein at least one of the circuitries to update the background statistical model and the foreground statistical model comprises:
<div class="claim-text">circuitry to compensate for global lighting change to the frame.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
    <div class="claim-text">14. A circuitry according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein pixels are labeled in the frame for an area of interest in the frame.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
    <div class="claim-text">15. A circuitry according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the background statistical model and the foreground statistical model pixels are built or updated for an area of interest in the frame.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
    <div class="claim-text">16. A circuitry according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:
<div class="claim-text">circuitry to update the background statistical model based on an infinite impulse response filter applied to previous statistical values of the background statistical model.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
    <div class="claim-text">17. A circuitry according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:
<div class="claim-text">circuitry to update the foreground statistical model based on an infinite impulse response filter applied to previous statistical values of the foreground statistical model.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00018" num="00018" class="claim">
    <div class="claim-text">18. A computer-readable medium comprising software for differentiating between foreground and background portions of video, the medium comprising:
<div class="claim-text">instructions for obtaining a real-time video stream; and</div>
<div class="claim-text">for each frame in the real-time frame stream, instructions for:
<div class="claim-text">labeling pixels in the frame;</div>
<div class="claim-text">performing spatial or temporal filtering;</div>
<div class="claim-text">updating a background statistical model, after the pixels are labeled; and</div>
<div class="claim-text">at least one of building or updating a foreground statistical model, after the pixels are labeled.</div>
</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00019" num="00019" class="claim">
    <div class="claim-text">19. A computer-readable medium comprising software for differentiating between foreground and background portions of video, the medium comprising:
<div class="claim-text">instructions for obtaining a frame sequence from a video stream; and</div>
<div class="claim-text">for each frame in the frame sequence, instructions for:
<div class="claim-text">aligning the frame with a scene model,</div>
<div class="claim-text">building a background statistical model, the background statistical model comprising values corresponding to regions of frames of the frame sequence and variances for the regions,</div>
<div class="claim-text">labeling the regions of the frame, and</div>
<div class="claim-text">performing spatial or temporal filtering. </div>
</div>
</div>
  </div>
</div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES16471170" lang="EN" load-source="patent-office" class="description">
<heading>CROSS-REFERENCE TO RELATED APPLICATION</heading> <p num="p-0002">This application claims priority to the following, all of which are incorporated herein by reference in their entirety: co-pending U.S. Provisional Patent Application No. 60/792,313, filed Apr. 17, 2006; co-pending and commonly-assigned U.S. patent application Ser. No. 10/667,148, filed Sep. 22, 2003 now U.S. Pat. No. 7,224,852 B2; and commonly-assigned U.S. patent application Ser. No. 09/815,385, filed on Mar. 23, 2001, now U.S. Pat. No. 6,625,310. This application is a continuation-in-part of co-pending U.S. patent application Ser. No. 10/667,148, which is a continuation-in-part of U.S. patent application Ser. No. 09/815,385, now U.S. Pat. No. 6,625,310.</p>
<heading>FIELD OF THE INVENTION</heading> <p num="p-0003">The present invention relates to processing of video frames for use in video processing systems, for example, intelligent video surveillance (IVS) systems that are used as a part of or in conjunction with Closed Circuit Television Systems (CCTV) that are utilized in security, surveillance and related homeland security and anti-terrorism systems, IVS systems that process surveillance video in retail establishments for the purposes of establishing in-store human behavior trends for market research purposes, IVS systems that monitor vehicular traffic to detect wrong-way traffic, broken-down vehicles, accidents and road blockages, and video compression systems. IVS systems are systems that further process video after video segmentation steps to perform object classification in which foreground objects may be classified as a general class such as animal, vehicle, or other moving but-unclassified object, or may be classified in more specific classes as human, small- or large- non-human animal, automobile, aircraft, boat, truck, tree, flag, or water region. In IVS systems, once such video segmentation and classification occurs, then detected objects are processed to determine how their positions, movements and behaviors relate to user defined virtual video tripwires, and virtual regions of interest (where a region of interest may be an entire field of view, or scene). User defined events that occur will then be flagged as events of interest that will be communicated to the security officer or professional on duty. Examples of such events include a human or a vehicle crossing a virtual video tripwire, a person or vehicle loitering or entering a virtual region of interest or scene, or an object being left behind or taken away from a virtual region or scene. In particular, the present invention deals with ways of segmenting video frames into their component parts using statistical properties of regions comprising the video frames.</p>
  <heading>BACKGROUND OF THE INVENTION</heading> <p num="p-0004">In object-based video compression, video segmentation for detecting and tracking video objects, as well as in other types of object-oriented video processing, the input video is separated into two streams. One stream contains the information representing stationary background information, and the other stream contains information representing the moving portions of the video, to be denoted as foreground information. The background information is represented as a background model, including a scene model, i.e., a composite image composed from a series of related images, as, for example, one would find in a sequence of video frames; the background model may also contain additional models and modeling information. Scene models are generated by aligning images (for example, by matching points and/or regions) and determining overlap among them; generation of scene models is discussed in further depth in commonly-assigned U.S. patent application Ser. No. 09/472,162, filed Dec. 27, 1999, and Ser. No. 09/609,919, filed Jul. 3, 2000, both incorporated by reference in their entireties herein. In an efficient transmission or storage scheme, the scene model need be transmitted only once, while the foreground information is transmitted for each frame. For example, in the case of an observer (i.e., camera or the like, which is the source of the video) that undergoes only pan, tilt, roll, and zoom types of motion, the scene model need be transmitted only once because the appearance of the scene model does not change from frame to frame, except in a well-defined way based on the observer motion, which can be easily accounted for by transmitting motion parameters. Note that such techniques are also applicable in the case of other forms of motion, besides pan, tilt, roll, and zoom. In IVS systems, the creation of distinct moving foreground and background objects allows the system to attempt classification on the moving objects of interest, even when the background pixels may be undergoing apparent motion due to pan, tilt and zoom motion of the camera.</p>
  <p num="p-0005">To make automatic object-oriented video processing feasible, it is necessary to be able to distinguish the regions in the video sequence that are moving or changing and to separate (i.e., segment) them from the stationary background regions. This segmentation must be performed in the presence of apparent motion, for example, as would be induced by a panning, tilting, rolling, and/or zooming observer (or due to other motion-related phenomena, including actual observer motion). To account for this motion, images are first aligned; that is, corresponding locations in the images (i.e., frames) are determined, as discussed above. After this alignment, objects that are truly moving or changing, relative to the stationary background, can be segmented from the stationary objects in the scene. The stationary regions are then used to create (or to update) the scene model, and the moving foreground objects are identified for each frame.</p>
  <p num="p-0006">It is not an easy thing to identify and automatically distinguish between video objects that are moving foreground and stationary background, particularly in the presence of observer motion, as discussed above. Furthermore, to provide the maximum degree of compression or the maximum fineness or accuracy of other video processing techniques, it is desirable to segment foreground objects as finely as possible; this enables, for example, the maintenance of smoothness between successive video frames and crispness within individual frames. Known techniques have proven, however, to be difficult to utilize and inaccurate for small foreground objects and have required excessive processing power and memory. It would, therefore, be desirable to have a technique that permits accurate segmentation between the foreground and background information and accurate, crisp representations of the foreground objects, without the limitations of prior techniques.</p>
  <heading>SUMMARY OF THE INVENTION</heading> <p num="p-0007">The present invention is directed to a method for segmentation of video into foreground information and background information, based on statistical properties of the source video. More particularly, the method is based on creating and updating statistical information pertaining to a characteristic of regions of the video and the labeling of those regions (i.e., as foreground or background) based on the statistical information. For example, in one embodiment, the regions are pixels, and the characteristic is chromatic intensity. Many other possibilities exist, as will become apparent. In more particular embodiments, the invention is directed to methods of using the inventive video segmentation methods to implement intelligent video surveillance systems.</p>
  <p num="p-0008">In embodiments of the invention, a background model is developed containing at least two components. A first component is the scene model, which may be built and updated, for example, as discussed in the aforementioned U.S. patent applications. A second component is a background statistical model.</p>
  <p num="p-0009">In a first embodiment, the inventive method comprises a two-pass process of video segmentation. The two passes of the embodiment comprise a first pass in which a background statistical model is built and updated and a second pass in which regions in the frames are segmented. An embodiment of the first pass comprises steps of aligning each video frame with a scene model and updating the background statistical model based on the aligned frame data. An embodiment of the second pass comprises, for each frame, steps of labeling regions of the frame and performing spatial filtering.</p>
  <p num="p-0010">In a second embodiment, the inventive method comprises a one-pass process of video segmentation. The single pass comprises, for each frame in a frame sequence of a video stream, steps of aligning the frame with a scene model; building a background statistical model; labeling the regions of the frame, and performing spatial/temporal filtering.</p>
  <p num="p-0011">In yet another embodiment, the inventive method comprises a modified version of the aforementioned one-pass process of video segmentation. This embodiment is similar to the previous embodiment, except that the step of building a background statistical model is replaced with a step of building a background statistical model and a secondary statistical model.</p>
  <p num="p-0012">In a fourth embodiment, the inventive method comprises a one-pass process of video segmentation. The single pass comprises, for each frame in a real-time video stream, steps of labeling the pixels in the frame, performing spatial/temporal filtering of the pixels in the frame, optionally refining the pixel labeling, building/updating background and foreground statistical model(s); and inserting objects into the background statistical model.</p>
  <p num="p-0013">Each of these embodiments may be embodied in the forms of a computer system running software executing their steps and a computer-readable medium containing software representing their steps.</p>
  <p num="p-0014">Each of these embodiments may be embodied in the form of a hardware apparatus.</p>
  <heading>DEFINITIONS</heading> <p num="p-0015">In describing the invention, the following definitions are applicable throughout (including above).</p>
  <p num="p-0016">A âcomputerâ refers to any apparatus that is capable of accepting a structured input, processing the structured input according to prescribed rules, and producing results of the processing as output. Examples of a computer include: a computer; a general purpose computer; a supercomputer; a mainframe; a super mini-computer; a mini-computer; a workstation; a micro-computer; a server; an interactive television; a hybrid combination of a computer and an interactive television; and application-specific hardware to emulate a computer and/or software. A computer can have a single processor or multiple processors, which can operate in parallel and/or not in parallel. A computer also refers to two or more computers connected together via a network for transmitting or receiving information between the computers. An example of such a computer includes a distributed computer system for processing information via computers linked by a network.</p>
  <p num="p-0017">A âcomputer-readable mediumâ refers to any storage device used for storing data accessible by a computer. Examples of a computer-readable medium include: a magnetic hard disk; a floppy disk; an optical disk, like a CD-ROM or a DVD; a magnetic tape; and a memory chip.</p>
  <p num="p-0018">âSoftwareâ refers to prescribed rules to operate a computer. Examples of software include: software; code segments; instructions; computer programs; and programmed logic.</p>
  <p num="p-0019">A âcomputer systemâ refers to a system having a computer, where the computer comprises a computer-readable medium embodying software to operate the computer.</p>
  <p num="p-0020">A ânetworkâ refers to a number of computers and associated devices that are connected by communication facilities. A network involves permanent connections such as cables or temporary connections such as those made through telephone or other communication links. Examples of a network include: an internet, such as the Internet; an intranet; a local area network (LAN); a wide area network (WAN); and a combination of networks, such as an internet and an intranet.</p>
  <p num="p-0021">âVideoâ refers to motion pictures represented in analog and/or digital form. Examples of video include video feeds from CCTV systems in security, surveillance and anti-terrorism applications, television, movies, image sequences from a camera or other observer, and computer-generated image sequences. These can be obtained from, for example, a wired or wireless live feed, a storage device, a firewire interface, a video digitizer, a video streaming server, device or software component, a computer graphics engine, or a network connection.</p>
  <p num="p-0022">âVideo processingâ refers to any manipulation of video, including, for example, compression and editing.</p>
  <p num="p-0023">A âframeâ refers to a particular image or other discrete unit within a video.</p>
  <p num="p-0024">A âvideo cameraâ may refer to an apparatus for visual recording. Examples of a video camera may include one or more of the following: a video camera; a digital video camera; a color camera; a monochrome camera; a camera; a camcorder; a PC camera; a webcam; an infrared (IR) video camera; a low-light video camera; a thermal video camera; a closed-circuit television (CCTV) camera; a pan, tilt, zoom (PTZ) camera; and a video sensing device. A video camera may be positioned to perform surveillance of an area of interest.</p>
<description-of-drawings> <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p num="p-0025">The invention will now be described in further detail in connection with the attached drawings, in which:</p>
    <p num="p-0026"> <figref idrefs="DRAWINGS">FIG. 1</figref> shows a flowchart corresponding to an implementation of a first embodiment of the invention;</p>
    <p num="p-0027"> <figref idrefs="DRAWINGS">FIGS. 2</figref> <i>a </i>and <b>2</b> <i>b </i>show flowcharts corresponding to two alternative embodiments of the labeling step in the flowchart of <figref idrefs="DRAWINGS">FIG. 1</figref>;</p>
    <p num="p-0028"> <figref idrefs="DRAWINGS">FIG. 3</figref> <i>a </i>and <b>3</b> <i>b </i>show flowcharts corresponding to implementations of the spatial/temporal filtering step in the flowchart of <figref idrefs="DRAWINGS">FIG. 1</figref>;</p>
    <p num="p-0029"> <figref idrefs="DRAWINGS">FIG. 4</figref> shows a flowchart corresponding to an implementation of a second embodiment of the invention;</p>
    <p num="p-0030"> <figref idrefs="DRAWINGS">FIG. 5</figref> shows a flowchart corresponding to an implementation of one of the steps in the flowchart of <figref idrefs="DRAWINGS">FIG. 4</figref>;</p>
    <p num="p-0031"> <figref idrefs="DRAWINGS">FIGS. 6</figref> <i>a </i>and <b>6</b> <i>b </i>together show a flowchart corresponding to an implementation of another one of the steps in the flowchart of <figref idrefs="DRAWINGS">FIG. 4</figref>;</p>
    <p num="p-0032"> <figref idrefs="DRAWINGS">FIG. 7</figref> shows a flowchart corresponding to an implementation of a third embodiment of the invention;</p>
    <p num="p-0033"> <figref idrefs="DRAWINGS">FIGS. 8</figref> <i>a </i>and <b>8</b> <i>b </i>together show a flowchart corresponding to an implementation of one of the steps in the flowchart of <figref idrefs="DRAWINGS">FIG. 7</figref>;</p>
    <p num="p-0034"> <figref idrefs="DRAWINGS">FIG. 9</figref> depicts an embodiment of the invention in the form of software embodied on a computer-readable medium, which may be part of a computer system;</p>
    <p num="p-0035"> <figref idrefs="DRAWINGS">FIG. 10</figref> depicts a flowchart of a method of implementing an intelligent video surveillance system according to an embodiment of the invention;</p>
    <p num="p-0036"> <figref idrefs="DRAWINGS">FIG. 11</figref> shows a flowchart corresponding to an implementation of a fourth embodiment of the invention;</p>
    <p num="p-0037"> <figref idrefs="DRAWINGS">FIG. 12</figref> shows a flowchart corresponding to an implementation of one of the blocks in the flowchart of <figref idrefs="DRAWINGS">FIG. 11</figref>;</p>
    <p num="p-0038"> <figref idrefs="DRAWINGS">FIG. 13</figref> shows a flowchart corresponding to an implementation of one of the blocks in the flowchart of <figref idrefs="DRAWINGS">FIG. 11</figref>;</p>
    <p num="p-0039"> <figref idrefs="DRAWINGS">FIG. 14</figref> shows a flowchart corresponding to an implementation of one of the blocks in the flowchart of <figref idrefs="DRAWINGS">FIGS. 12 and 13</figref>; and</p>
    <p num="p-0040"> <figref idrefs="DRAWINGS">FIG. 15</figref> shows a flowchart corresponding to an implementation of one of the blocks in the flowchart of <figref idrefs="DRAWINGS">FIG. 11</figref>.</p>
  </description-of-drawings> <p num="p-0041">Note that identical objects are labeled with the same reference numerals in all of the drawings that contain them.</p>
  <heading>DETAILED DESCRIPTION OF THE INVENTION</heading> <p num="p-0042">As discussed above, the present invention is directed to the segmentation of video streams into foreground information, which corresponds to moving objects, and background information, which corresponds to the stationary portions of the video. The present invention may be embodied in a number of ways, of which four specific ones are discussed below. These embodiments are meant to be exemplary, rather than exclusive.</p>
  <p num="p-0043">The ensuing discussion refers to âpixelsâ and âchromatic intensity;â however, the inventive method is not so limited. Rather, the processing may involve any type of region (including regions comprising multiple pixels), not just a pixel, and may use any type of characteristic measured with respect to or related to such a region, not just chromatic intensity.</p>
  <heading>1. FIRST EMBODIMENT</heading> <heading>Two-Pass Segmentation</heading> <p num="p-0044">The first embodiment of the invention is depicted in <figref idrefs="DRAWINGS">FIG. 1</figref> and corresponds to a two-pass method of segmentation. As shown in <figref idrefs="DRAWINGS">FIG. 1</figref>, the method begins by obtaining a frame (or video) sequence from a video stream (Step <b>1</b>). The frame sequence preferably includes two or more frames of the video stream. The frame sequence can be, for example, a portion of the video stream or the entire video stream. As a portion of the video stream, the frame sequence can be, for example, one continuous sequence of frames of the video stream or two or more discontinuous sequences of frames of the video stream. As part of the alignment step, the scene model is also built and updated.</p>
  <p num="p-0045">After Step <b>1</b>, in Step <b>2</b>, it is determined whether or not all frames have yet been processed. If not, the next frame is taken and aligned with the underlying scene model of the video stream (Step <b>3</b>); such alignment is discussed above, and more detailed discussions of alignment techniques may be found, for example, in commonly-assigned U.S. patent application Ser. No. 09/472,162, filed Dec. 27, 1999, and Ser. No. 09/609,919, filed Jul. 3, 2000, both incorporated by reference in their entireties herein, as discussed above, as well as in numerous other references.</p>
  <p num="p-0046">The inventive method is based on the use of statistical modeling to determine whether a particular pixel should be classified as being a foreground object or a part thereof or as being the background or a part thereof. Step <b>4</b> deals with the building and updating of a statistical model of the background, using each frame aligned in Step <b>3</b>.</p>
  <p num="p-0047">The statistical model of the present invention comprises first- and second-order statistics. In the ensuing discussion, mean and standard deviation will be used as such first- and second-order statistics; however, this is meant to be merely exemplary of the statistics that may be used.</p>
  <p num="p-0048">In general, the mean of N samples, <o>x</o>, is computed by taking the sum of the samples and dividing it by N, i.e.,</p>
  <p num="p-0049"> <maths id="MATH-US-00001" num="00001"> <math overflow="scroll"> <mtable> <mtr> <mtd> <mrow> <mrow> <mover> <mi>x</mi> <mi>_</mi> </mover> <mo>=</mo> <mfrac> <mrow> <munderover> <mo>â</mo> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>N</mi> </munderover> <mo>â¢</mo> <msub> <mi>x</mi> <mi>i</mi> </msub> </mrow> <mi>N</mi> </mfrac> </mrow> <mo>,</mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>1</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> </maths> <br>
where x<sub>i </sub>is a particular sample corresponding to a given pixel (or region), which in the present case could be, for example, the measured chromatic intensity of the i<sup>th </sup>sample corresponding to the given pixel (or region). In the present setting, then, such a mean would be computed for each pixel or region.
</p>
  <p num="p-0050">While Eqn. (1) gives the general formula for a sample mean, it may not always be optimal to use this formula. In video processing applications, a pixel's sample value may change drastically when an object moves through the pixel and change (drastically) back to a value around its previous value after the moving object is no longer within that pixel. In order to address this type of consideration, the invention utilizes a weighted average, in which the prior values are weighted more heavily than the present value. In particular, the following equation may be used:
<br> <i> <o>x</o> </i> <sub>N</sub> <i>=W</i> <sub>p</sub> <i> <o>x</o> </i> <sub>N</sub>â1<i>+W</i> <sub>n</sub> <i>x</i> <sub>N</sub>,ââ(2)<br>
where W<sub>p </sub>is the weight of the past values and W<sub>n </sub>is the weight assigned to the newest value. Additionally, <o>x</o> <sub>J </sub>represents the weighted average taken over J samples, and x<sub>K </sub>represents the K<sup>th </sup>sample. W<sub>p </sub>and W<sub>n </sub>may be set to any pair of values between zero and one such that their sum is one and such that W<sub>n</sub>&lt;W<sub>p</sub>, so as to guarantee that the past values are more heavily weighted than the newest value. As an example, the inventors have successfully used W<sub>p</sub>=0.9 and W<sub>n</sub>=0.1.
</p>
  <p num="p-0051">Standard deviation, Ï, is determined as the square root of the variance, Ï<sup>2</sup>, of the values under consideration. In general, variance is determined by the following formula:
<br>Ï<sup>2</sup>= <o> <i>x</i> <sup>2</sup> </o>â(<i> <o>x</o> </i>)<sup>2</sup>ââ,(3)<br>
where <o>x<sup>2</sup> </o> represents the average of x<sup>2</sup>; thus, the standard deviation is given by
<br>Ï=â{square root over ( <o> <i>x</i> <sup>2</sup> </o>â(<i> <o>x</o> </i>)<sup>2</sup>)}.ââ(4)<br>
Because the inventive method uses running statistics, this becomes
<br>Ï<sub>N</sub>=â{square root over ({ <o>x<sup>2</sup> </o>}<sub>N</sub>â( <o> <i>x</i> <sub>N</sub> </o>)<sup>2</sup>)},ââ(4a)<br>
where <o>x<sub>N</sub> </o> is as defined in Eqn. (2) above, and { <o>x<sup>2</sup> </o>}<sub>N </sub>is defined as the weighted average of the squared values of the samples, through the N<sup>th </sup>sample, and is given by
<br>{ <o> <i>x</i> <sup>2</sup> </o>}<sub>N</sub> <i>=W</i> <sub>p</sub>{ <o> <i>x</i> <sup>2</sup> </o>}<sub>Nâ1</sub> <i>+W</i> <sub>n</sub> <i>x</i> <sup>2</sup> <sub>N</sub>.ââ(5)<br>
As in the case of the weighted average of the sample values, the weights are used to assure that past values are more heavily weighted than the present value.
</p>
  <p num="p-0052">Given this, Step <b>4</b> works to create and update the statistical model by computing the value of Eqn. (4a) for each pixel, for each frame. In Step <b>4</b>, the values for the pixels are also stored on a pixel-by-pixel basis (as opposed to how they are received, i.e., on a frame-by-frame basis); that is, an array of values is compiled for each pixel over the sequence of frames. Note that in an alternative embodiment, Step <b>4</b> only performs this storage of values.</p>
  <p num="p-0053">Following Step <b>4</b>, the method returns to Step <b>2</b> to check whether or not all of the frames have been processed. If they have, then the method proceeds to Step <b>5</b>, which commences the second pass of the embodiment.</p>
  <p num="p-0054">In Step <b>5</b>, the statistical background model is finalized. This is done by using the stored values for each pixel and determining their mode, the mode being the value that occurs most often. This may be accomplished, for example, by taking a histogram of the stored values and selecting the value for which the histogram has the highest value. The mode of each pixel is then assigned as the value of the background statistical model for that pixel.</p>
  <p num="p-0055">Following Step <b>5</b>, the method proceeds to Step <b>6</b>, which determines whether or not all of the frames have been processed yet. If not, then the method proceeds to Step <b>7</b>, in which each pixel in the frame is labeled as being a foreground (FG) pixel or a background (BG) pixel. Two alternative embodiments of the workings of this step are shown in the flowcharts of <figref idrefs="DRAWINGS">FIGS. 2</figref> <i>a </i>and <b>2</b> <i>b. </i> </p>
  <p num="p-0056"> <figref idrefs="DRAWINGS">FIG. 2</figref> <i>a </i>depicts a two decision level method. In <figref idrefs="DRAWINGS">FIG. 2</figref> <i>a</i>, the pixel labeling Step <b>7</b> begins with Step <b>71</b>, where it is determined whether or not all of the pixels in the frame have been processed. If not, then the method proceeds to Step <b>72</b> to examine the next pixel. Step <b>72</b> determines whether or not the pixel matches the background statistical model, i.e., whether the value of the pixel matches the model for that pixel. This is performed by taking the absolute difference between the pixel value and the value of the background statistical model for the pixel (i.e., the mode) and comparing it with a threshold; that is,
<br>Î=|<i>x</i> <sub>pixel</sub> <i>âm</i> <sub>pixel</sub>|ââ(6)<br>
is compared with a threshold Î¸. In Eqn. (6), x<sub>pixel </sub>denotes the value of the pixel, while m<sub>pixel </sub>represents the value of the statistical background model for that pixel.
</p>
  <p num="p-0057">The threshold Î¸ may be determined in many ways. For example, it may be taken to be a function of standard deviation (of the given pixel), Ï. In a particular exemplary embodiment, Î¸=3Ï; in another embodiment, Î¸=KÏ, where K is chosen by the user. As another example, Î¸ may be assigned a predetermined value (again, for each pixel) or one chosen by the user.</p>
  <p num="p-0058">If Îâ¦Î¸, then the pixel value is considered to match the background statistical model. In this case, the pixel is labeled as background (BG) in Step <b>73</b>, and the algorithm proceeds back to Step <b>71</b>. Otherwise, if Î&gt;Î¸, then the pixel value is considered not to match the background statistical model, and the pixel is labeled as foreground (FG) in Step <b>74</b>. Again, the algorithm then proceeds back to Step <b>71</b>. If Step <b>71</b> determines that all of the pixels (in the frame) have been processed, then Step <b>7</b> is finished.</p>
  <p num="p-0059"> <figref idrefs="DRAWINGS">FIG. 2</figref> <i>b </i>depicts a three decision level method, labeled <b>7</b>â². In <figref idrefs="DRAWINGS">FIG. 2</figref> <i>b</i>, the process once again begins with Step <b>71</b>, a step of determining whether or not all pixels have yet been processed. If not, the process considers the next pixel to be processed and executes Step <b>72</b>, the step of determining whether or not the pixel being processed matches the background statistical model; this is done in the same way as in <figref idrefs="DRAWINGS">FIG. 2</figref> <i>a</i>. If yes, then the pixel is labeled as BG (Step <b>73</b>), and the process loops back to Step <b>71</b>. If not, then the process proceeds to Step <b>75</b>; this is where the process of <figref idrefs="DRAWINGS">FIG. 2</figref> <i>b </i>is distinguished from that of <figref idrefs="DRAWINGS">FIG. 2</figref> <i>a. </i> </p>
  <p num="p-0060">In Step <b>75</b>, the process determines whether or not the pixel under consideration is far from matching the background statistical model. This is accomplished via a threshold test similar to Step <b>72</b>, only in Step <b>75</b>, Î¸ is given a larger value. As in Step <b>72</b>, Î¸ may be user-assigned or predetermined. In one embodiment, Î¸=NÏ, where N is a either a predetermined or user-set number, N&gt;K. In another embodiment, N=6.</p>
  <p num="p-0061">If the result of Step <b>75</b> is that Î&lt;Î¸, then the pixel is labeled as FG (Step <b>74</b>). If not, then the pixel is labeled definite foreground (DFG), in Step <b>76</b>. In each case, the process loops back to Step <b>71</b>. Once Step <b>71</b> determines that all pixels in the frame have been processed, Step <b>7</b>â² is complete.</p>
  <p num="p-0062">Returning to <figref idrefs="DRAWINGS">FIG. 1</figref>, once all of the pixels of a frame have been labeled, the process proceeds to Step <b>8</b>, in which spatial/temporal filtering is performed. While shown as a sequential step in <figref idrefs="DRAWINGS">FIG. 1</figref>, Step <b>8</b> may alternatively be performed in parallel with Step <b>7</b>. Details of Step <b>8</b> are shown in the flowcharts of <figref idrefs="DRAWINGS">FIGS. 3</figref> <i>a </i>and <b>3</b> <i>b. </i> </p>
  <p num="p-0063">In <figref idrefs="DRAWINGS">FIG. 3</figref> <i>a</i>, Step <b>8</b> commences with a test as to whether or not all the pixels of the frame have been processed (Step <b>81</b>). If not, in Step <b>85</b>, the algorithm selects the next pixel, P<sub>i</sub>, for processing and proceeds to Step <b>82</b>, where it is determined whether or not the pixel is labeled as BG. If it is, then the process goes back to Step <b>81</b>. If not, then the pixel undergoes further processing in Steps <b>83</b> and <b>84</b>.</p>
  <p num="p-0064">Step <b>83</b>, neighborhood filtering, is used to correct for misalignments when the images are aligned. If the current image is slightly misaligned with the growing background statistical model, then, particularly near strong edges, the inventive segmentation procedure, using the background statistical model, will label pixels as foreground. Neighborhood filtering will correct for this. An embodiment of Step <b>83</b> is depicted in the flowchart of <figref idrefs="DRAWINGS">FIG. 3</figref> <i>b. </i> </p>
  <p num="p-0065">In <figref idrefs="DRAWINGS">FIG. 3</figref> <i>b</i>, Step <b>83</b> begins with Step <b>831</b>, where a determination is made of the scene model location, P<sub>m</sub>, corresponding to P<sub>i</sub>. Next, a neighborhood, comprising the pixels, Pâ²<sub>m</sub>, surrounding P<sub>m </sub>in the scene model, is selected (Step <b>832</b>). Step <b>833</b> next determines if all of the pixels in the neighborhood have been processed. If yes, Step <b>83</b> is complete, and the label of P<sub>i </sub>remains as it was; if not, the process proceeds to Step <b>834</b>, where the next neighborhood pixel Pâ²<sub>m </sub>is considered. Step <b>835</b> then tests to determine whether or not P<sub>i </sub>matches Pâ²<sub>m</sub>. This matching test is accomplished by executing the labeling step (Step <b>7</b> or <b>7</b>â²) in a modified fashion, using P<sub>i </sub>as the pixel under consideration and Pâ²<sub>m </sub>as the âcorrespondingâ background statistical model point. If the labeling step returns a label of FG or DFG, there is no match, whereas if it returns a label of BG, there is a match. If there is no match, the process loops back to Step <b>833</b>; if there is a match, then this is an indication that P<sub>i </sub>might be mislabeled, and the process continues to Step <b>836</b>. In Step <b>836</b>, a neighborhood, comprising the pixels, Pâ²<sub>i</sub>, surrounding P<sub>i </sub>in the frame, is selected, and an analogous process is performed. That is, in Step <b>833</b>, it is determined whether or not all of the pixels, Pâ²<sub>i </sub>in the neighborhood have yet been considered. If yes, then Step <b>83</b> is complete, and the label of P<sub>i </sub>remains as it was; if not, then the process proceeds to Step <b>838</b>, where the next neighborhood pixel, Pâ²<sub>i</sub>, is considered. Step <b>839</b> tests to determine if P<sub>m </sub>matches Pâ²<sub>i</sub>; this is performed analogously to Step <b>833</b>, with the Pâ²<sub>i </sub>under consideration being used as the pixel being considered and P<sub>m </sub>as its âcorrespondingâ background statistical model point. If it does not, then the process loops back to Step <b>837</b>; if it does, then P<sub>i </sub>is relabeled as BG, and Step <b>83</b> is complete.</p>
  <p num="p-0066">Returning to <figref idrefs="DRAWINGS">FIG. 3</figref> <i>a</i>, following Step <b>83</b>, Step <b>84</b> is executed, in which morphological erosions and dilations are performed. First, a predetermined number, n, of erosions are performed to remove incorrectly labeled foreground. Note that pixels labeled DFG may not be eroded because they represent either a pixel that is almost certainly foreground. This is followed by n dilations, which restore the pixels that were correctly labeled as foreground but were eroded. Finally, a second predetermined number, m, of dilations are performed to fill in holes in foreground objects. The erosions and dilations may be performed using conventional erosion and dilation techniques, applied in accordance with user-specified parameters, and modified, as discussed above, such that pixels labeled DFG are not eroded.</p>
  <p num="p-0067">In alternative embodiments, Step <b>84</b> may comprise filtering techniques other than or in addition to morphological erosions and dilations. In general, Step <b>84</b> may employ any form or forms of spatial and/or temporal filtering.</p>
  <p num="p-0068">Returning to <figref idrefs="DRAWINGS">FIG. 1</figref>, following Step <b>8</b>, the algorithm returns to Step <b>6</b>, to determine whether or not all frames have been processed. If yes, then the processing of the frame sequence is complete, and the process ends (Step <b>9</b>).</p>
  <p num="p-0069">This two-pass embodiment has the advantage of relative simplicity, and it is an acceptable approach for applications not requiring immediate or low-latency processing. Examples of such applications include off-line video compression and non-linear video editing and forensic processing of security and surveillance video. On the other hand, many other applications such as video security and surveillance in which timely event reporting is critical do have such requirements, and the embodiments to be discussed below are tailored to address these requirements.</p>
  <heading>2. SECOND EMBODIMENT</heading> <heading>One-Pass Segmentation</heading> <p num="p-0070"> <figref idrefs="DRAWINGS">FIG. 4</figref> depicts a flowchart of a one-pass segmentation process, according to a second embodiment of the invention. Comparing <figref idrefs="DRAWINGS">FIG. 4</figref> with <figref idrefs="DRAWINGS">FIG. 1</figref> (the first embodiment), the second embodiment differs in that there is only a single pass of processing for each frame sequence. This single pass, as shown in Steps <b>2</b>, <b>3</b>, <b>31</b>, <b>32</b>, <b>8</b> in <figref idrefs="DRAWINGS">FIG. 4</figref>, incorporates the processes of the second pass (Steps <b>5</b>-<b>8</b> in <figref idrefs="DRAWINGS">FIG. 1</figref>) with the first pass (Steps <b>2</b>-<b>4</b> in <figref idrefs="DRAWINGS">FIG. 1</figref>), albeit in a modified form, as will be discussed below.</p>
  <p num="p-0071">As in the case of the first embodiment, the second embodiment (one-pass process), shown in <figref idrefs="DRAWINGS">FIG. 4</figref>, begins by obtaining a frame sequence (Step <b>1</b>). As in the first embodiment, the process then performs a test to determine whether or not all of the frames have yet been processed (Step <b>2</b>). Also as in the first embodiment, if the answer is no, then the next frame to be processed is aligned with the scene model (Step <b>3</b>). As discussed above, the scene model component of the background model is built and updated as part of Step <b>3</b>, so there is always at least a deterministically-determined value in the background model at each location.</p>
  <p num="p-0072">At this point, the process includes a step of building a background statistical model (Step <b>31</b>). This differs from Step <b>4</b> of <figref idrefs="DRAWINGS">FIG. 1</figref>, and is depicted in further detail in <figref idrefs="DRAWINGS">FIG. 5</figref>. The process begins with a step of determining whether or not all pixels in the frame being processed have been processed (Step <b>311</b>). If not, then the process determines whether or not the background statistical model is âmatureâ (Step <b>312</b>) and âstableâ (Step <b>313</b>).</p>
  <p num="p-0073">The reason for Steps <b>312</b> and <b>313</b> is that, initially, the statistical background model will not be sufficiently developed to make accurate decisions as to the nature of pixels. To overcome this, some number of frames should be processed before pixels are labeled (i.e., the background statistical model should be âmatureâ); in one embodiment of the present invention, this is a user-defined parameter. This may be implemented as a âlook-aheadâ procedure, in which a limited number of frames are used to accumulate the background statistical model prior to pixel labeling (Step <b>32</b> in <figref idrefs="DRAWINGS">FIG. 4</figref>).</p>
  <p num="p-0074">While simply processing a user-defined number of frames may suffice to provide a mature statistical model, stability is a second concern (Step <b>313</b>), and it depends upon the standard deviation of the background statistical model. In particular, as will be discussed below, the statistical background model includes a standard deviation for each pixel. The statistical model (for a particular pixel) is defined as having become âstableâ when its variance (or, equivalently, its standard deviation) is reasonably small. In an embodiment of the present invention, Step <b>313</b> determines this by comparing the standard deviation with a user-defined threshold parameter; if the standard deviation is less than this threshold, then the statistical background model (for that pixel) is determined to be stable.</p>
  <p num="p-0075">As to the flow of Step <b>31</b>, in <figref idrefs="DRAWINGS">FIG. 5</figref>, if the background statistical model is determined to be mature (Step <b>312</b>), it is determined whether or not the background statistical model is stable (Step <b>313</b>). If either of these tests (Steps <b>312</b> and <b>313</b>) fails, the process proceeds to Step <b>315</b>, in which the background statistical model of the pixel being processed is updated using the current value of that pixel. Step <b>315</b> will be explained further below.</p>
  <p num="p-0076">If the background statistical model is determined to be both mature and stable (in Steps <b>312</b> and <b>313</b>), the process proceeds to Step <b>314</b>, where it is determined whether or not the pixel being processed matches the background statistical model. If yes, then the background statistical model is updated using the current pixel value (Step <b>315</b>); if no, then the process loops back to Step <b>311</b> to determine if all pixels in the frame have been processed.</p>
  <p num="p-0077">Step <b>314</b> operates by determining whether or not the current pixel value is within some range of the mean value of the pixel, according to the current background statistical model. In one embodiment of the invention, the range is a user-defined range. In yet another embodiment, it is determined to be a user-defined number of standard deviations; i.e., the pixel value, x, matches the background statistical model if
<br>|<i>x</i> <sub>pixel</sub>â <o> <i>x</i> <sub>pixel</sub> </o>|â¦<i>KÏ,</i>ââ(7)<br>
where K is the user-defined number of standard deviations, Ï; x<sub>pixel </sub>is the current pixel value; and <o>x<sub>pixel</sub> </o> is the mean value of the current pixel in the background statistical model. The purpose of performing Step <b>314</b> is to ensure, to the extent possible, that only background pixels are used to develop and update the background statistical model.
</p>
  <p num="p-0078">In Step <b>315</b>, the background statistical model is updated. In this embodiment, the background statistical model consists of the mean and standard deviation of the values for each pixel (over the sequence of frames). These are computed according to Eqns. (2) and (4a) above.</p>
  <p num="p-0079">Following Step <b>315</b>, the process loops back to Step <b>311</b>, to determine if all pixels (in the current frame) have been processed. Once all of the pixels have been processed, the process proceeds to Step <b>316</b>, where the background statistical model is finalized. This finalization consists of assigning to each pixel its current mean value and standard deviation (i.e., the result of processing all of the frames up to that point).</p>
  <p num="p-0080">Note that it is possible for the background statistical model for a given pixel never to stabilize. This generally indicates that the particular pixel is not a background pixel in the sequence of frames, and there is, therefore, no need to assign it a value for the purposes of the background statistical model. Noting that, as discussed above, a scene model is also built and updated, there is always at least a deterministically-determined value associated with each pixel in the background model.</p>
  <p num="p-0081">Following Step <b>316</b>, the process goes to Step <b>32</b>, as shown in <figref idrefs="DRAWINGS">FIG. 4</figref>, where the pixels in the frame are labeled according to their type (i.e., definite foreground, foreground or background). Step <b>32</b> is shown in further detail in the flowchart of <figref idrefs="DRAWINGS">FIGS. 6</figref> <i>a </i>and <b>6</b> <i>b. </i> </p>
  <p num="p-0082">The following concepts are embodied in the description of Step <b>32</b> to follow. Ideally, labeling would always be done by testing each pixel against its corresponding point in the background statistical model, but this is not always possible. If the background statistical model is not ready to use on the basis of number of frames processed (i.e., âmatureâ), then the process must fall back on testing against the corresponding point in the scene model. If the background statistical model is ready to use but has not yet settled down (i.e., is not âstableâ), this is a sign that the pixel is varying and should be labeled as being foreground. If the background statistical model has, for some reason (i.e., because it fails to match the scene model or because it has become unsettled again), become unusable, the process must once again fall back on testing against the scene model.</p>
  <p num="p-0083">As shown in <figref idrefs="DRAWINGS">FIG. 6</figref> <i>a</i>, Step <b>32</b> begins with Step <b>321</b>, where it is determined whether or not all pixels (in the current frame) have been processed. If yes, Step <b>32</b> is complete; if not, the next pixel is processed in Steps <b>322</b> et seq.</p>
  <p num="p-0084">Step <b>322</b> determines whether or not the background statistical model is mature. This is done in the same manner as in Step <b>312</b> of <figref idrefs="DRAWINGS">FIG. 5</figref>, discussed above. If not, the process proceeds to Step <b>323</b>, where it is determined whether or not the pixel matches the background chromatic data of the corresponding point of the scene model.</p>
  <p num="p-0085">Step <b>323</b> is performed by carrying out a test to determine whether or not the given pixel falls within some range of the background chromatic data value. This is analogous to Step <b>314</b> of <figref idrefs="DRAWINGS">FIG. 5</figref>, substituting the background chromatic data value for the statistical mean. The threshold may be determined in a similar fashion (predetermined, user-determined, or the like).</p>
  <p num="p-0086">If Step <b>323</b> determines that the pixel does match the background chromatic data, then the pixel is labeled BG (following connector A) in Step <b>329</b> of <figref idrefs="DRAWINGS">FIG. 6</figref> <i>b</i>. From Step <b>329</b>, the process loops back (via connector D) to Step <b>321</b>.</p>
  <p num="p-0087">If Step <b>323</b> determines that the pixel does not match the background chromatic data, then the pixel is labeled FG (following connector B) in Step <b>3210</b> of <figref idrefs="DRAWINGS">FIG. 6</figref> <i>b</i>. From the Step <b>3210</b>, the process loops back (via connector D) to Step <b>321</b>.</p>
  <p num="p-0088">If Step <b>322</b> determines that the background statistical model is mature, processing proceeds to Step <b>324</b>, which determines whether or not the background statistical model is stable. Step <b>324</b> performs this task in the same manner as Step <b>313</b> of <figref idrefs="DRAWINGS">FIG. 5</figref>, discussed above. If not, the process proceeds to Step <b>325</b>, where it is determined if the background statistical model was ever stable (i.e., if it was once stable but is now unstable). If yes, then the process branches to Step <b>323</b>, and the process proceeds from there as described above. If no, the pixel is labeled DFG (following connector C) in Step <b>3211</b> of <figref idrefs="DRAWINGS">FIG. 6</figref> <i>b</i>, after which the process loops back (via connector D) to Step <b>321</b>.</p>
  <p num="p-0089">If Step <b>324</b> determines that the background statistical model is stable, the process goes to Step <b>326</b>. Step <b>326</b> tests whether the background statistical model matches the background chromatic data. Similar to the previous matching tests above, this test takes an absolute difference between the value of the background statistical model (i.e., the mean) for the pixel and the background chromatic data (i.e., of the scene model) for the pixel. This absolute difference is then compared to some threshold value, as above (predetermined, user-determined, or the like).</p>
  <p num="p-0090">If Step <b>326</b> determines that there is not a match between the background statistical model and the background chromatic data, the process branches to Step <b>323</b>, where processing proceeds in the same fashion as described above. If Step <b>326</b>, on the other hand, determines that there is a match, the process continues to Step <b>327</b>.</p>
  <p num="p-0091">Step <b>327</b> determines whether or not the current pixel matches the background statistical model. This step is performed in the same manner as Step <b>314</b> of <figref idrefs="DRAWINGS">FIG. 5</figref>, discussed above. If the current pixel does match (which, as discussed above, is determined by comparing it to the mean value corresponding to the current pixel), the pixel is labeled BG (following connector A) in Step <b>329</b> of <figref idrefs="DRAWINGS">FIG. 6</figref> <i>b</i>, and the process then loops back (via connector D) to Step <b>321</b>. If not, then further testing is performed in Step <b>328</b>.</p>
  <p num="p-0092">Step <b>328</b> determines whether, given that the current pixel value does not reflect a BG pixel, it reflects a FG pixel or a DFG pixel. This is done by determining if the pixel value is far from matching the background statistical model. As discussed above, a FG pixel is distinguished from a BG pixel (in Step <b>325</b>) by determining if its value differs from the mean by more than a particular amount, for example, a number of standard deviations (see Eqn. (7)). Step <b>328</b> applies the same test, but using a larger range. Again, the threshold may set as a predetermined parameter, as a computed parameter, or as a user-defined parameter, and it may be given in terms of a number of standard deviations from the mean, i.e.,
<br>|<i>x</i> <sub>pixel</sub>â <o> <i>x</i> <sub>pixel</sub> </o> <i>|â¦NÏ,</i>ââ(8)<br>
where N is a number greater than K of Eqn. (7). If the pixel value lies outside the range defined, for example, by Eqn. (8), it is labeled DFG (following connector C) in Step <b>3211</b> of <figref idrefs="DRAWINGS">FIG. 6</figref> <i>b</i>, and the process loops back (via connector D) to Step <b>321</b>. If it lies within the range, the pixel is labeled FG (following connector B) in Step <b>3210</b> of <figref idrefs="DRAWINGS">FIG. 6</figref> <i>b</i>, and the process proceeds (via connector D) to Step <b>321</b>.
</p>
  <p num="p-0093">After Step <b>32</b> is complete, the process proceeds to Step <b>8</b>, as shown in <figref idrefs="DRAWINGS">FIG. 4</figref>, where spatial/temporal filtering is performed on the pixels in the frame. Step <b>8</b> is implemented, in this embodiment of the invention, in the same manner in which it is implemented for the two-pass embodiment, except that the pixel labeling algorithm of <figref idrefs="DRAWINGS">FIGS. 6</figref> <i>a </i>and <b>6</b> <i>b </i>is used for Steps <b>833</b> and <b>837</b> of Step <b>83</b> (as opposed to the pixel labeling algorithms used in the two-pass embodiment). Following Step <b>8</b>, the process loops back to Step <b>2</b>, where, if all frames have been processed, the process ends.</p>
  <p num="p-0094">A single-pass approach, like the one present here, has the advantage of not requiring a second pass, thus, reducing the latency associated with the process. This is useful for applications in which high latencies would be detrimental, for example, video teleconferencing, webcasting, real-time gaming, and the like.</p>
  <heading>3. THIRD EMBODIMENT</heading> <heading>Modified One-Pass Segmentation</heading> <p num="p-0095">While the one-pass approach described above has a lower latency than the two-pass approach, it does have a disadvantage in regard to the background statistical model. In particular, the cumulative statistical modeling approach used in the one-pass embodiment of the invention may stabilize on a non-representative statistical model for an element (i.e., pixel, region, etc.; that is, whatever size element is under consideration). If the values (e.g., chromatic values) of frame elements corresponding to a particular element of the video scene fundamentally change (i.e., something happens to change the video, for example, a parked car driving away, a moving car parking, the lighting changes, etc.), then the scene model element will no longer accurately represent the true scene. This can be addressed by utilizing a mechanism for dynamically updating the background statistical model so that at any given time it accurately represents the true nature of the scene depicted in the video. Such a mechanism is depicted in the embodiment of the invention shown in <figref idrefs="DRAWINGS">FIG. 7</figref>.</p>
  <p num="p-0096">In <figref idrefs="DRAWINGS">FIG. 7</figref>, Steps <b>1</b>-<b>3</b>, <b>32</b>, <b>8</b>, and <b>9</b> are as described in the one-pass embodiment above. The embodiment of <figref idrefs="DRAWINGS">FIG. 7</figref> differs from that of <figref idrefs="DRAWINGS">FIG. 4</figref> in that after a given frame is aligned with the scene model (Step <b>3</b>), the process executes Step <b>310</b>, in which the background statistical model and, simultaneously, a secondary background statistical model are built. Step <b>310</b> is more fully described in connection with <figref idrefs="DRAWINGS">FIGS. 8</figref> <i>a </i>and <b>8</b> <i>b. </i> </p>
  <p num="p-0097">As shown in <figref idrefs="DRAWINGS">FIG. 8</figref> <i>a</i>, Step <b>310</b> includes all of the steps shown in Step <b>31</b> in <figref idrefs="DRAWINGS">FIG. 5</figref> (which are shown using the same reference numerals), and it begins with a step of determining whether or not all pixels have yet been processed (Step <b>311</b>). If not, the next pixel is processed by proceeding to Step <b>312</b>. In Step <b>312</b>, it is determined whether or not the background statistical model is mature. If not, the process branches to Step <b>315</b>, where the pixel is used to update the background statistical model. Following Step <b>315</b>, the process loops back to Step <b>311</b>.</p>
  <p num="p-0098">If Step <b>312</b> determines that the background statistical model is mature, the process proceeds to Step <b>313</b>, where it is determined whether or not the background statistical model is stable. If it is not, then, as in the case of a negative determination in Step <b>312</b>, the process branches to Step <b>315</b> (and then loops back to Step <b>311</b>). Otherwise, the process proceeds to Step <b>314</b>.</p>
  <p num="p-0099">In Step <b>314</b>, it is determined whether or not the pixel under consideration matches the background statistical model. If it does, the process proceeds with Step <b>315</b> (and then loops back to Step <b>311</b>); otherwise, the process executes the steps shown in <figref idrefs="DRAWINGS">FIG. 8</figref> <i>b</i>, which build and update a secondary background statistical model. This secondary background statistical model is built in parallel with the background statistical model, as reflected in <figref idrefs="DRAWINGS">FIG. 8</figref> <i>b</i>; uses the same procedures as are used to build and update the background statistical model; and represents the pixel values that do not match the background statistical model.</p>
  <p num="p-0100">Following a negative determination in Step <b>314</b>, the process then makes a determination as to whether or not the secondary background statistical model is mature (Step <b>3107</b>). This determination is made in the same fashion as in Step <b>313</b>. If not, the process branches to Step <b>3109</b>, where the secondary background statistical model is updated, using the same procedures as for the background statistical model (Step <b>315</b>). From Step <b>3109</b>, the process loops back to Step <b>311</b> (in <figref idrefs="DRAWINGS">FIG. 8</figref> <i>a</i>).</p>
  <p num="p-0101">If Step <b>3107</b> determines that the secondary background statistical model is mature, the process proceeds to Step <b>3108</b>, which determines (using the same procedures as in Step <b>314</b>) whether or not the secondary background statistical model is stable. If not, the process proceeds to Step <b>3109</b> (and from there to Step <b>311</b>). If yes, then the process branches to Step <b>31010</b>, in which the background statistical model is replaced with the secondary background statistical model, after which the process loops back to Step <b>311</b>. Additionally, concurrently with the replacement of the background statistical model by the secondary background statistical model in Step <b>31010</b>, the scene model data is replaced with the mean value of the secondary statistical model. At this point, the secondary background statistical model is reset to zero, and a new one will be built using subsequent data.</p>
  <p num="p-0102">This modified one-pass embodiment has the advantage of improved statistical accuracy over the one-pass embodiment, and it solves the potential problem of changing background images. It does this while still maintaining improved latency time over the two-pass embodiment, and at only a negligible decrease in processing speed compared with the one-pass embodiment.</p>
  <heading>4. FOURTH EMBODIMENT</heading> <heading>Real-Time Video Stream One-Pass Segmentation</heading> <p num="p-0103">A focus of the first, second, and third exemplary embodiments is segmentation for compression applications. For compression applications, the idea is to keep a statistical background model representing a video sequence or a section of a video sequence. Thus, the idea is to create a background model and then keep the background model constant for a period of time while segmenting a video sequence.</p>
  <p num="p-0104">In surveillance applications, processing is generally not performed on a video sequence (i.e., a finite set of video frames), but rather on a real-time video stream (e.g., a continuous set of video frames with no discemable end-point). Consequently, it is, in general, impossible to create a background model to represent the video scene for two reasons. First, the scene is dynamically changing due to lighting conditions and meteorological conditions (e.g., due to rain, shadows, clouds, day/night changes, etc), and second, components in the scene change (e.g., cars are parked, objects are added, removed, or moved within the scene, etc.). To accommodate these conditions in real-time surveillance applications, six modifications to the one-pass algorithm are employed in this embodiment: (1) one or more foreground models are added in addition to the background model(s); (2) the concept of a background or foreground model being âmatureâ is removed; (3) automatic gain control (AGC) compensation is added; (4) an object insertion mechanism is added; (5) the processing order is different to account for real-time processing requirements; and (6) a masking feature is used to reduce the number of pixels that need to be processed.</p>
  <p num="p-0105">As to the six differences, first, foreground model(s) are used in addition to the background model(s) to describe regions that are labeled as foreground. In the third embodiment described above, a secondary background model is used to model a change in the background scene that occurs after the primary background has âmatured.â In the fourth embodiment, one (or more) foreground models are used to describe pixels (or objects) detected as foreground. The reason for creating one or more foreground models is to cover the case when a foreground object (e.g., a car) stops in a region of the scene. In such a case, it becomes desirable to start treating the foreground object as background for the purpose of object detection (e.g., a car parks, and a person walks in front of the car). Foreground models are created and maintained in exactly the same way as background models, but apply to pixels labeled as âforeground.â It is possible to have multiple foreground models to describe multiple objects that occlude each other. For example, a car parks and is modeled by one foreground model. Next, another car parks in front of the first car and is modeled by a second foreground model.</p>
  <p num="p-0106">Second, the concept of a model being mature is removed from the fourth embodiment. For this embodiment, in which a video stream rather than a video sequence is processed, it is assumed that the model will not mature and, instead, will continuously and dynamically change to accommodate slow environmental changes such as, for example: the shadows shortening and lengthening as the sun moves; the sun becoming occluded by clouds or coming out from occlusion; rain, snow or fog starting or stopping in the scene; and day and night changes in the scene. In this embodiment, the background model(s) (as well as the foreground models) are continually being modified on a frame-by-frame and pixel-by-pixel basis so that the models best reflect the âcurrentâ state of the background rather than on a mature model, which was created previously and may even have been created a long time ago in the past.</p>
  <p num="p-0107">Third, AGC compensation is employed in the fourth embodiment. AGC is a process by which video imagers automatically adjust the brightness and contrast of the whole image to try and optimize the dynamic range of the image. The process can take place quite quickly, and can change the intensity of the background pixels so that they appear as foreground pixels when, in fact, there is no foreground object present. Consequently, an AGC compensation component is added to modify the background model in the event of AGC adjustments in the video image</p>
  <p num="p-0108">Fourth, an object insertion mechanism is added to the fourth embodiment to allow an external signal to control the insertion of objects in the background model. The idea here is that when, for example, a car parks in the scene, there will be a foreground model for all the pixels that represent that car. An external process may determine that these pixels represent a car and that the car has, in fact, parked. Once this determination is made, the external process provides a notification indicating that the foreground model should be added (e.g, âburned inâ) to the background model. The foreground model is treated as part of the background for the purpose of segmentation.</p>
  <p num="p-0109">Fifth, in real-time processing, there is no time to go back and improve pixel labeling after the frame is processed (as is the case in off-line processing or processing with latency). Consequently, the order of steps of the real-time algorithm is different. Initially, when a new frame comes in, the existing background model is used for labeling the pixels. Next, various other processes (such as, for example, spatio-temporal filtering) are used to refine the labeling and, then, the model is updated. This order provides superior segmentation results for each frame in real-time processing.</p>
  <p num="p-0110">Sixth, a mask is added in the fourth embodiment to designate pixels to be ignored. The ability is added to set a region of the video image where segmentation should not be applied. A reason for doing this is to conserve processing resources so as to maintain real-time performance. So, if there are areas of a video scene where it is known a priori that no segmentation is required (so-called âareas of disinterestâ), these pixels can be masked out. (Alternately, âareas of interestâ may be defined.) Further, automated algorithms may be employed to determine these areas of disinterest where there is no need to apply segmentation. Such areas of disinterest may exist because a camera produces various âunusedâ pixels around the edge of the video frames where there is no real image data. Such. areas of disinterest may also exist in areas of a scene (such as, for example, the sky) where processing is not desired or would not work very well.</p>
  <p num="p-0111"> <figref idrefs="DRAWINGS">FIG. 11</figref> illustrates a flow chart for the exemplary fourth embodiment of the invention. In block <b>1101</b>, a video frame is extracted from a real-time video stream.</p>
  <p num="p-0112">In optional block <b>1102</b> (where the optional nature of the block is indicated by the dashed outline), the extracted frame may be aligned with the scene model to accommodate for camera motion (e.g., either jitter or deliberate motion such as pan, tilt, zoom, or translational motion).</p>
  <p num="p-0113">In block <b>1103</b>, each pixel in the frame is labeled as background, foreground, or definite foreground (or more levels of granularity as desired). This constitutes segmentation of a frame into background and foreground components. In one embodiment, the technique discussed with respect to <figref idrefs="DRAWINGS">FIG. 2</figref> <i>b</i>, which uses the background statistical model, may be used for block <b>1103</b>. In other embodiments, the background statistical model and/or the foreground statistical models may be used.</p>
  <p num="p-0114">In block <b>8</b>, spatio-temporal filtering is performed on the segmentation to improve the results.</p>
  <p num="p-0115">In optional block <b>1104</b>, additional processes may be included that might bear on or refine the segmentation. For example, object tracking and classification may be included in block <b>1104</b>. Object tracking and classification are discussed in, for example, U.S. application Ser. No. 11/098,579, filed Apr. 5, 2005, U.S. application Ser. No. 11/139,600, filed May 31, 2005, and U.S. application Ser. No. 11/139,986, filed May 31, 2005, all of which are incorporated herein by reference in their entirety. As another example, stationary target detection may be included in block <b>1104</b>. Stationary target detection is discussed in, for example, U.S. application Ser. No. 11/288,200, filed Nov. 29, 2005, which is incorporated herein by reference in its entirety.</p>
  <p num="p-0116">In block <b>1105</b>, the segmentation is used to build and/or update the background and foreground statistical models for each pixel.</p>
  <p num="p-0117">In block <b>1106</b>, objects are inserted into the background statistical model. As an option, an external process from block <b>1107</b> may decide that the insertion should be performed.</p>
  <p num="p-0118">In optional block <b>1107</b>, a stationary target detector may determine that a certain group, or groups, of pixels represent an object that has moved into the scene and stopped (e.g., a car moves in and parks in the scene). The process may decide that from henceforth these pixels should be treated as background (because it is determined that there is a stopped object at these pixels). Stationary target detection is discussed in, for example, U.S. application Ser. No. 11/288,200, filed Nov. 29, 2005, which is incorporated herein by reference in its entirety.</p>
  <p num="p-0119"> <figref idrefs="DRAWINGS">FIG. 12</figref> illustrates a flowchart for block <b>1103</b> of <figref idrefs="DRAWINGS">FIG. 11</figref>. In block <b>1201</b>, each frame is processed accordingly.</p>
  <p num="p-0120">In optional block <b>1202</b>, compensation for AGC (or other global lighting change) is performed.</p>
  <p num="p-0121">In block <b>1203</b>, each pixel in the frame is processed accordingly.</p>
  <p num="p-0122">In block <b>1204</b>, if each pixel in the frame has been analyzed, flow proceeds to block <b>1214</b>; otherwise, flow proceeds to block <b>1205</b>.</p>
  <p num="p-0123">In optional block <b>1205</b>, it is determined whether a pixel is in the area of interest or not. Pixels inside the area of interest are labeled, while pixels outside the area of interest are not labeled. The determination of whether a pixel is inside the area of interest may be performed with a mask (from optional block <b>1213</b>) or any other such mechanism. The mask may be generated manually (as in optional block <b>1212</b>), or by an automatic process that determines where the area of interest might be within a frame (as in optional block <b>1211</b>). The area of interest may be continuous or discontinuous across frame and may include one or more groups of pixels in the frame. If the pixel is in the area of interest, flow proceeds to block <b>1206</b>; otherwise, flow proceeds back to block <b>1203</b>.</p>
  <p num="p-0124">Blocks <b>1206</b> through <b>1210</b> perform the labeling of the pixel in a manner similar to those in the other previous embodiments. In block <b>1206</b>, if the value of the intensity of the pixel is close enough to the mean for the pixel in the background statistical model, the pixel is labeled as background in block <b>1209</b>. In block <b>1207</b>, if the value of the intensity of the pixel is further away from the mean for the pixel in the background statistical model, the pixel is labeled foreground in block <b>1210</b>. In block <b>1208</b>, if the value of the intensity of the pixel is far away from the mean for the pixel in the background statistical model, the pixel is labeled as definite foreground. After blocks <b>1208</b>, <b>1209</b>, and <b>1210</b>, flow proceeds back to block <b>1203</b>.</p>
  <p num="p-0125">Mathematically, blocks <b>1206</b> through <b>1210</b> may be summarized as follows. For blocks <b>1206</b> and <b>1209</b>, if
<br>|<i>i</i>(<i>x</i>)â<i>Ä«</i>(<i>x</i>)|&lt;<i>T</i> <sub>1</sub>Ï(<i>x</i>),<br>
label pixel i(x) as background, where i(x) is the pixel intensity at location x, Ä«(x) is the mean of the background statistical model mean at location x, T<sub>i </sub>is a threshold, and Ï(x) is the standard deviation of the background statistical model at location x. For blocks <b>1207</b> and <b>1210</b>, if
<br> <i>T</i> <sub>1</sub>Ï(<i>x</i>)â¦|<i>i</i>(<i>x</i>)â<i>Ä«</i>(<i>x</i>)|&lt;<i>T</i> <sub>2</sub>Ï(<i>x</i>),<br>
label pixel i(x) as foreground, where T<b>2</b> is a threshold higher than T<b>1</b>. For blocks <b>1207</b> and <b>1208</b>, if
<br>|<i>i</i>(<i>x</i>)â<i>Ä«</i>(<i>x</i>)|â§<i>T</i> <sub>2</sub>Ï(<i>x</i>),<br>
label pixel i(x) as definite foreground.
</p>
  <p num="p-0126"> <figref idrefs="DRAWINGS">FIG. 13</figref> illustrates a flowchart for block <b>1105</b> of <figref idrefs="DRAWINGS">FIG. 11</figref>. In block <b>1301</b>, each frame is processed accordingly.</p>
  <p num="p-0127">In optional block <b>1202</b>, compensation for AGC (or other global lighting change) is performed.</p>
  <p num="p-0128">In block <b>1303</b>, each pixel in the frame is processed accordingly.</p>
  <p num="p-0129">In block <b>1304</b>, if each pixel in the frame has been analyzed, flow proceeds to block <b>1318</b>; otherwise, flow proceeds to block <b>1305</b>.</p>
  <p num="p-0130">In optional block <b>1305</b>, it is determined whether a pixel is in the area of interest or not. Pixels inside the area of interest are labeled, while pixels outside the area of interest are not labeled. The determination of whether a pixel is inside the area of interest may be performed with a mask (from optional block <b>1313</b>) or any other such mechanism. The mask may be generated manually (as in optional block <b>1313</b>), or by an automatic process that determines where the area of interest might be within a frame (as in optional block <b>1311</b>). The area of interest may be continuous or discontinuous across frame and may include one or more groups of pixels in the frame. If the pixel is in the area of interest, flow proceeds to block <b>1306</b>; otherwise, flow proceeds back to block <b>1303</b>.</p>
  <p num="p-0131">Next, the foreground mask that was generated in blocks <b>1103</b>, <b>8</b>, and <b>1104</b> in <figref idrefs="DRAWINGS">FIG. 11</figref> is used as a filter to update the background and foreground statistical models. In block <b>1316</b>, the foreground mask is obtained.</p>
  <p num="p-0132">In optional block <b>1317</b>, the foreground mask may be filtered by some morphology, such as dilation, to make sure that pixels on the edge of foreground objects do not corrupt the background model.</p>
  <p num="p-0133">In block <b>1306</b>, if the pixel is not a foreground or definite foreground pixel, flow proceeds to block <b>1307</b>; otherwise flow proceeds to block <b>1308</b>.</p>
  <p num="p-0134">In block <b>1307</b>, the mean and variance of the background statistical model are be updated with the information from the current frame at that pixel location. If the background statistical model does not yet exist, the background statistical model may be built based on the discussions in the previous embodiments.</p>
  <p num="p-0135">In block <b>1308</b>, the pixel is a foreground or definite foreground pixel. If a foreground model exists, flow processed to block <b>1309</b>; otherwise flow proceeds to block <b>1312</b>.</p>
  <p num="p-0136">In block <b>1312</b>, a foreground statistical model does not exist at the location of the pixel, and a new foreground statistical model is created. The model is of the form <o>i<sub>f</sub> </o>(x)=i(x);Ï<sub>f</sub>(x)=D, where <o>i<sub>f</sub> </o>(x) represents the mean of the foreground statistical model at pixel location x, Ï<sub>f</sub>(x) represents the standard deviation of the foreground statistical model at pixel location x, and D is a default value.</p>
  <p num="p-0137">In block <b>1309</b>, the foreground statistical model exist at this location and is updated with the data from the current pixel.</p>
  <p num="p-0138">In block <b>1310</b>, if the pixel has been in a foreground state for a long period of time, flow proceeds to block <b>1311</b>; otherwise, flow proceeds to block <b>1303</b>.</p>
  <p num="p-0139">In block <b>1311</b>, the pixel has been in a foreground state for a long period of time and may begin to be considered as background. This shift in segmentation is accomplished by replacing the background model with the foreground model at this location:
<br> <i>Ä«</i>(<i>x</i>)= <o> <i>i</i> <sub>f</sub> </o>(<i>x</i>); Ï(<i>x</i>)=Ï<sub>f</sub>(<i>x</i>).</p>
  <p num="p-0140">In the fourth embodiment, unlike the previous three embodiments, the updating of the background and foreground models in blocks <b>1307</b> and <b>1309</b> may be different. In the previous embodiments, a running mean and standard deviation are used. In the real-time case for this embodiment, where the background may be continually undergoing drastic change (e.g., as day becomes night, and vice versa), a running mean and standard deviation would provide statistical models that may never be accurate. In this real-time embodiment, the current mean and standard deviation should represent the background at the current time. Hence, more weight should be given to recent information than to old information. For this reason, a filter that considers previous statistical values, such as, for example, an infinite impulse response (IIR) filter, should be used to update the background and foreground models. The filter may be applied as follows:
<br> <i>Ä«</i>(<i>x</i>)=(1âÎ±)<i>i</i>(<i>x</i>)+Î±<i>Ä«</i>(<i>x</i>)<br>Ï(<i>x</i>)=(1âÎ±)(|<i>i</i>(<i>x</i>)â<i>Ä«</i>(<i>x</i>)|)+Î±Ï(<i>x</i>)<br>
where Î± is a blending constant. The blending constant Î± may be mapped to a standard blending time constant.
</p>
  <p num="p-0141"> <figref idrefs="DRAWINGS">FIG. 14</figref> illustrates a flowchart for block <b>1202</b> of <figref idrefs="DRAWINGS">FIGS. 2 and 13</figref>. In block <b>1401</b>, each frame is processed accordingly.</p>
  <p num="p-0142">In optional block <b>1402</b>, each pixel in the area of interest is processed accordingly. The area of interest may be provided by optional block <b>1408</b>.</p>
  <p num="p-0143">In block <b>1403</b>, each background pixel is processed accordingly. The background pixels may be determined to be those that are not in a foreground mask provided by block <b>1409</b>.</p>
  <p num="p-0144">In block <b>1404</b>, a difference histogram is generated between the background model from block <b>1411</b> and the current frame from block <b>1410</b>. The difference histogram measures the amount of difference that exists between the background model and the current frame for each pixel intensity value in the background model. As an example, for pixels in the background model with intensity value 10, the average intensity difference between these pixels and their corresponding pixels in the current frame may be represented as 5 grey levels. For pixels with intensity value 100, the average difference may be 30 grey levels.</p>
  <p num="p-0145">In block <b>1405</b>, the frame is analyzed to detect any global AGC effects. The frame is analyzed by examining the average of the histogram values. If the histogram values are all quite low, this may mean that the background pixels basically agree with the current frame pixels, in terms of intensity. If the histogram values are all quite high, this may mean that there is, on average, a large difference in intensity between the current frame and the background model, and thus, there may be a global AGC effect (or a global lighting change) in the frame. If global AGC effects are detected in the frame, flow proceeds to block <b>1406</b>; otherwise flow proceeds back to bloc <b>1401</b>.</p>
  <p num="p-0146">In block <b>1406</b>, AGC effects are detected, and the background model is updated. The background model is updated by using the difference histogram from block <b>1404</b> as a lookup table. The mean value of each pixel in the background model is adjusted by the average difference between the current frame intensity data and the background model intensity data (for this value of background mean intensity). This update may be summarized as follows:
<br> <i>i</i>(<i>x</i>)=<i>H</i>(<i>i</i>(<i>x</i>))+<i>i</i>(<i>x</i>),<br>
where H(i) is the average intensity difference between pixels of intensity i in the background model and their corresponding pixels in the current frame.
</p>
  <p num="p-0147">In block <b>1407</b>, the model parameters are adjusted due to the presence of AGC. For example, if AGC is detected, the blending constant Î± used for updating in blocks <b>1307</b> and <b>1309</b> in <figref idrefs="DRAWINGS">FIG. 13</figref> may be decreased. By decreasing the blending constant Î±, more weight is given to data from the current frame so that when the background and foreground models are updated in blocks <b>1307</b> and <b>1309</b>, respectively, the models update faster and are less sensitive to global intensity changes. When no AGC is detected, Î± is increased, which makes the model update process in blocks <b>1307</b> and <b>1309</b> more sensitive to intensity changes.</p>
  <p num="p-0148"> <figref idrefs="DRAWINGS">FIG. 5</figref> illustrates a flowchart for block <b>1106</b> in <figref idrefs="DRAWINGS">FIG. 11</figref>. As an example, if objects are detected and tracked, and determined to be stationary (e.g, a car parking), it might be desirable to burn these objects into the background model so that the background model may continue to detect foreground in that region.</p>
  <p num="p-0149">In block <b>1502</b>, a determination is made whether to insert an object in the background model. As an option, an external process (from optional block <b>1107</b>) may determine that an object should be inserted in the background model. If an object is to be inserted, flow proceeds to block <b>1503</b>; otherwise, flow proceeds to block <b>1505</b> and ends.</p>
  <p num="p-0150">In block <b>1503</b>, for each pixel in the object, flow proceeds accordingly. The object may be described by an optional object mask (from optional block <b>1506</b>) or any other mechanism.</p>
  <p num="p-0151">In block <b>1504</b>, for each pixel in the mask, the background model(s) at that location (from block <b>1507</b>) are replaced by the foreground model(s) at that location (from block <b>1508</b>) resulting in the entire object en masse being inserted in the background in one step.</p>
  <heading>5. Additional Embodiments and Remarks</heading> <p num="p-0152">While the above discussion considers two-level and three-level pixel labeling algorithms, this embodiment is not limited only to these cases. Indeed, it is contemplated that an arbitrary number of decision levels, corresponding to different ranges (i.e., threshold values) may be used. In such a case, fuzzy or soft-decision logic would be used to make decisions in subsequent steps of the segmentation process.</p>
  <p num="p-0153">The above discussion primarily discusses pixels and chromatic values (which may be RGB, YUV, intensity, etc.); however, as discussed above, the invention is not limited to these quantities. Regions other than pixels may be used, and quantities other than chromatic values may be used.</p>
  <p num="p-0154">As discussed above, the invention, including all of the embodiments discussed in the preceding sections, maybe embodied in the form of a computer system or in the form of a computer-readable medium containing software implementing the invention. This is depicted in <figref idrefs="DRAWINGS">FIG. 9</figref>, which shows a plan view for a computer system for the invention. The computer <b>91</b> includes a computer-readable medium <b>92</b> embodying software for implementing the invention and/or software to operate the computer <b>91</b> in accordance with the invention. Computer <b>91</b> receives a video stream and outputs segmented video, as shown. Alternatively, the segmented video may be further processed within the computer.</p>
  <p num="p-0155">In addition to implementing all of the embodiments described herein with a computer and software, all of the embodiments discussed herein may also be implemented in circuitry and/or hardware. The circuitry may include, for example: a field programmable gate array (FPGA), an application specific integrated circuit (ASIC), or a hardware accelerator on or coupled to a digital signal processor (DSP), a general-purpose preprocessor (GPP), or other processing device. The circuitry may be implemented, for example, with a chip, chips, and/or a chip set. The circuitry may be located, for example, on a printed circuit board, on an add-on card, and/or in an add-on slot. The circuitry may be located, for example, in a video camera, a video router, a video encoder, and/or a digital video recorder (DVR). Other circuit-based and/or hardware-based implementations will become apparent to those of ordinary skill in the art.</p>
  <p num="p-0156">Also as discussed above, the statistical pixel modeling methods described above may be incorporated into a method of implementing an intelligent video surveillance system. <figref idrefs="DRAWINGS">FIG. 10</figref> depicts an embodiment of such a method. In particular, block <b>1001</b> represents the use of statistical pixel modeling, e.g., as described above. Once the statistical pixel modeling has been completed, block <b>1002</b> uses the results to identify and classify objects. Block <b>1002</b> may use, for example, statistical or template-oriented methods for performing such identification and classification. In performing identification and classification, it is determined whether or not a given object is an object of interest; for example, one may be interested in tracking the movements of people through an area under surveillance, which would make people âobjects of interest.â In Block <b>1003</b>, behaviors of objects of interest are analyzed; for example, it may be determined if a person has entered a restricted area. Finally, in Block <b>1004</b>, if desired, various notifications may be sent out or other appropriate actions taken.</p>
  <p num="p-0157">The invention has been described in detail with respect to preferred embodiments, and it will now be apparent from the foregoing to those skilled in the art that changes and modifications may be made without departing from the invention in its broader aspects. The invention, therefore, as defined in the appended claims, is intended to cover all such changes and modifications as fall within the true spirit of the invention.</p>
</div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4764971">US4764971</a></td><td class="patent-data-table-td patent-date-value">Nov 25, 1985</td><td class="patent-data-table-td patent-date-value">Aug 16, 1988</td><td class="patent-data-table-td ">Eastman Kodak Company</td><td class="patent-data-table-td ">Image processing method including image segmentation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4949389">US4949389</a></td><td class="patent-data-table-td patent-date-value">Oct 9, 1987</td><td class="patent-data-table-td patent-date-value">Aug 14, 1990</td><td class="patent-data-table-td ">The United States Of America As Represented By The United States Department Of Energy</td><td class="patent-data-table-td ">Optical ranked-order filtering using threshold decomposition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5048095">US5048095</a></td><td class="patent-data-table-td patent-date-value">Mar 30, 1990</td><td class="patent-data-table-td patent-date-value">Sep 10, 1991</td><td class="patent-data-table-td ">Honeywell Inc.</td><td class="patent-data-table-td ">Adaptive image segmentation system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5448651">US5448651</a></td><td class="patent-data-table-td patent-date-value">Jun 21, 1994</td><td class="patent-data-table-td patent-date-value">Sep 5, 1995</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Texture discrimination method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5519789">US5519789</a></td><td class="patent-data-table-td patent-date-value">Oct 26, 1993</td><td class="patent-data-table-td patent-date-value">May 21, 1996</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Image clustering apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5586200">US5586200</a></td><td class="patent-data-table-td patent-date-value">Jan 7, 1994</td><td class="patent-data-table-td patent-date-value">Dec 17, 1996</td><td class="patent-data-table-td ">Panasonic Technologies, Inc.</td><td class="patent-data-table-td ">Segmentation based image compression system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5671294">US5671294</a></td><td class="patent-data-table-td patent-date-value">Sep 15, 1994</td><td class="patent-data-table-td patent-date-value">Sep 23, 1997</td><td class="patent-data-table-td ">The United States Of America As Represented By The Secretary Of The Navy</td><td class="patent-data-table-td ">System and method for incorporating segmentation boundaries into the calculation of fractal dimension features for texture discrimination</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5696551">US5696551</a></td><td class="patent-data-table-td patent-date-value">Sep 27, 1996</td><td class="patent-data-table-td patent-date-value">Dec 9, 1997</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Disparity extracting unit</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5724456">US5724456</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 1995</td><td class="patent-data-table-td patent-date-value">Mar 3, 1998</td><td class="patent-data-table-td ">Polaroid Corporation</td><td class="patent-data-table-td ">Brightness adjustment of images using digital scene analysis</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5764306">US5764306</a></td><td class="patent-data-table-td patent-date-value">Mar 18, 1997</td><td class="patent-data-table-td patent-date-value">Jun 9, 1998</td><td class="patent-data-table-td ">The Metaphor Group</td><td class="patent-data-table-td ">Real-time method of digitally altering a video data stream to remove portions of the original image and substitute elements to create a new image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5768413">US5768413</a></td><td class="patent-data-table-td patent-date-value">Oct 4, 1995</td><td class="patent-data-table-td patent-date-value">Jun 16, 1998</td><td class="patent-data-table-td ">Arch Development Corp.</td><td class="patent-data-table-td ">Method and apparatus for segmenting images using stochastically deformable contours</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5802203">US5802203</a></td><td class="patent-data-table-td patent-date-value">Jun 7, 1995</td><td class="patent-data-table-td patent-date-value">Sep 1, 1998</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Image segmentation using robust mixture models</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5875305">US5875305</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 1996</td><td class="patent-data-table-td patent-date-value">Feb 23, 1999</td><td class="patent-data-table-td ">Sensormatic Electronics Corporation</td><td class="patent-data-table-td ">Video information management system which provides intelligent responses to video data content features</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5969755">US5969755</a></td><td class="patent-data-table-td patent-date-value">Feb 5, 1997</td><td class="patent-data-table-td patent-date-value">Oct 19, 1999</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Motion based event detection system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5990955">US5990955</a></td><td class="patent-data-table-td patent-date-value">Oct 3, 1997</td><td class="patent-data-table-td patent-date-value">Nov 23, 1999</td><td class="patent-data-table-td ">Innovacom Inc.</td><td class="patent-data-table-td ">Dual encoding/compression method and system for picture quality/data density enhancement</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6008865">US6008865</a></td><td class="patent-data-table-td patent-date-value">Sep 19, 1997</td><td class="patent-data-table-td patent-date-value">Dec 28, 1999</td><td class="patent-data-table-td ">Eastman Kodak Company</td><td class="patent-data-table-td ">Segmentation-based method for motion-compensated frame interpolation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6049363">US6049363</a></td><td class="patent-data-table-td patent-date-value">Feb 5, 1997</td><td class="patent-data-table-td patent-date-value">Apr 11, 2000</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Object detection method and system for scene change analysis in TV and IR data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6058210">US6058210</a></td><td class="patent-data-table-td patent-date-value">Sep 15, 1997</td><td class="patent-data-table-td patent-date-value">May 2, 2000</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Using encoding cost data for segmentation of compressed image sequences</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6078619">US6078619</a></td><td class="patent-data-table-td patent-date-value">Mar 10, 1999</td><td class="patent-data-table-td patent-date-value">Jun 20, 2000</td><td class="patent-data-table-td ">University Of Bath</td><td class="patent-data-table-td ">Object-oriented video system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6084912">US6084912</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 27, 1997</td><td class="patent-data-table-td patent-date-value">Jul 4, 2000</td><td class="patent-data-table-td ">Sarnoff Corporation</td><td class="patent-data-table-td ">Very low bit rate video coding/decoding method and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6195458">US6195458</a></td><td class="patent-data-table-td patent-date-value">Jul 29, 1997</td><td class="patent-data-table-td patent-date-value">Feb 27, 2001</td><td class="patent-data-table-td ">Eastman Kodak Company</td><td class="patent-data-table-td ">Method for content-based temporal segmentation of video</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6249613">US6249613</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 1998</td><td class="patent-data-table-td patent-date-value">Jun 19, 2001</td><td class="patent-data-table-td ">Sharp Laboratories Of America, Inc.</td><td class="patent-data-table-td ">Mosaic generation and sprite-based coding with automatic foreground and background separation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6278466">US6278466</a></td><td class="patent-data-table-td patent-date-value">Jun 11, 1998</td><td class="patent-data-table-td patent-date-value">Aug 21, 2001</td><td class="patent-data-table-td ">Presenter.Com, Inc.</td><td class="patent-data-table-td ">Creating animation from a video</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6337917">US6337917</a></td><td class="patent-data-table-td patent-date-value">Jun 21, 1999</td><td class="patent-data-table-td patent-date-value">Jan 8, 2002</td><td class="patent-data-table-td ">Levent Onural</td><td class="patent-data-table-td ">Rule-based moving object segmentation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6349113">US6349113</a></td><td class="patent-data-table-td patent-date-value">Oct 22, 1998</td><td class="patent-data-table-td patent-date-value">Feb 19, 2002</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Method for detecting moving cast shadows object segmentation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6393054">US6393054</a></td><td class="patent-data-table-td patent-date-value">Apr 20, 1998</td><td class="patent-data-table-td patent-date-value">May 21, 2002</td><td class="patent-data-table-td ">Hewlett-Packard Company</td><td class="patent-data-table-td ">System and method for automatically detecting shot boundary and key frame from a compressed video data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6396876">US6396876</a></td><td class="patent-data-table-td patent-date-value">Aug 3, 1998</td><td class="patent-data-table-td patent-date-value">May 28, 2002</td><td class="patent-data-table-td ">Thomson Licensing S.A.</td><td class="patent-data-table-td ">Preprocessing process and device for motion estimation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6404925">US6404925</a></td><td class="patent-data-table-td patent-date-value">Mar 11, 1999</td><td class="patent-data-table-td patent-date-value">Jun 11, 2002</td><td class="patent-data-table-td ">Fuji Xerox Co., Ltd.</td><td class="patent-data-table-td ">Methods and apparatuses for segmenting an audio-visual recording using image similarity searching and audio speaker recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6625310">US6625310</a></td><td class="patent-data-table-td patent-date-value">Mar 23, 2001</td><td class="patent-data-table-td patent-date-value">Sep 23, 2003</td><td class="patent-data-table-td ">Diamondback Vision, Inc.</td><td class="patent-data-table-td ">Video segmentation using statistical pixel modeling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6658136">US6658136</a></td><td class="patent-data-table-td patent-date-value">Dec 6, 1999</td><td class="patent-data-table-td patent-date-value">Dec 2, 2003</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">System and process for locating and tracking a person or object in a scene using a series of range images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6738424">US6738424</a></td><td class="patent-data-table-td patent-date-value">Jul 3, 2000</td><td class="patent-data-table-td patent-date-value">May 18, 2004</td><td class="patent-data-table-td ">Objectvideo, Inc.</td><td class="patent-data-table-td ">Scene model generation from video for use in video processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6930689">US6930689</a></td><td class="patent-data-table-td patent-date-value">Dec 26, 2000</td><td class="patent-data-table-td patent-date-value">Aug 16, 2005</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Hardware extensions for image and video processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6987451">US6987451</a></td><td class="patent-data-table-td patent-date-value">Dec 3, 2003</td><td class="patent-data-table-td patent-date-value">Jan 17, 2006</td><td class="patent-data-table-td ">3Rd Millennium Solutions. Ltd.</td><td class="patent-data-table-td ">Surveillance system with identification correlation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020063154">US20020063154</a></td><td class="patent-data-table-td patent-date-value">May 29, 2001</td><td class="patent-data-table-td patent-date-value">May 30, 2002</td><td class="patent-data-table-td ">Hector Hoyos</td><td class="patent-data-table-td ">Security system database management</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20040151374">US20040151374</a></td><td class="patent-data-table-td patent-date-value">Sep 22, 2003</td><td class="patent-data-table-td patent-date-value">Aug 5, 2004</td><td class="patent-data-table-td ">Lipton Alan J.</td><td class="patent-data-table-td ">Video segmentation using statistical pixel modeling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20040240546">US20040240546</a></td><td class="patent-data-table-td patent-date-value">May 29, 2003</td><td class="patent-data-table-td patent-date-value">Dec 2, 2004</td><td class="patent-data-table-td ">Lsi Logic Corporation</td><td class="patent-data-table-td ">Method and/or apparatus for analyzing the content of a surveillance image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20050169367">US20050169367</a></td><td class="patent-data-table-td patent-date-value">Apr 5, 2005</td><td class="patent-data-table-td patent-date-value">Aug 4, 2005</td><td class="patent-data-table-td ">Objectvideo, Inc.</td><td class="patent-data-table-td ">Video surveillance system employing video primitives</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20060066722">US20060066722</a></td><td class="patent-data-table-td patent-date-value">Sep 28, 2004</td><td class="patent-data-table-td patent-date-value">Mar 30, 2006</td><td class="patent-data-table-td ">Objectvideo, Inc.</td><td class="patent-data-table-td ">View handling in video surveillance systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20060222209">US20060222209</a></td><td class="patent-data-table-td patent-date-value">Apr 5, 2005</td><td class="patent-data-table-td patent-date-value">Oct 5, 2006</td><td class="patent-data-table-td ">Objectvideo, Inc.</td><td class="patent-data-table-td ">Wide-area site-based video surveillance system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20060268111">US20060268111</a></td><td class="patent-data-table-td patent-date-value">May 31, 2005</td><td class="patent-data-table-td patent-date-value">Nov 30, 2006</td><td class="patent-data-table-td ">Objectvideo, Inc.</td><td class="patent-data-table-td ">Multi-state target tracking</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Appl. No. 09/472,162, filed Dec. 27, 1999, Strat.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Appl. No. 11/139,986, filed May 31, 2005, Zhang.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Appl. No. 11/288,200, filed Nov. 29, 2005, Venetianer.</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8164655">US8164655</a></td><td class="patent-data-table-td patent-date-value">May 5, 2009</td><td class="patent-data-table-td patent-date-value">Apr 24, 2012</td><td class="patent-data-table-td ">Spatial Cam Llc</td><td class="patent-data-table-td ">Systems and methods for concurrently playing multiple images from a storage medium</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8295597">US8295597</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 7, 2008</td><td class="patent-data-table-td patent-date-value">Oct 23, 2012</td><td class="patent-data-table-td ">Videomining Corporation</td><td class="patent-data-table-td ">Method and system for segmenting people in a physical space based on automatic behavior analysis</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8355042">US8355042</a></td><td class="patent-data-table-td patent-date-value">Aug 10, 2009</td><td class="patent-data-table-td patent-date-value">Jan 15, 2013</td><td class="patent-data-table-td ">Spatial Cam Llc</td><td class="patent-data-table-td ">Controller in a camera for creating a panoramic image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8416282">US8416282</a></td><td class="patent-data-table-td patent-date-value">May 7, 2009</td><td class="patent-data-table-td patent-date-value">Apr 9, 2013</td><td class="patent-data-table-td ">Spatial Cam Llc</td><td class="patent-data-table-td ">Camera for creating a panoramic image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8457401">US8457401</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 1, 2007</td><td class="patent-data-table-td patent-date-value">Jun 4, 2013</td><td class="patent-data-table-td ">Objectvideo, Inc.</td><td class="patent-data-table-td ">Video segmentation using statistical pixel modeling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8718387">US8718387</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 12, 2011</td><td class="patent-data-table-td patent-date-value">May 6, 2014</td><td class="patent-data-table-td ">Edge 3 Technologies, Inc.</td><td class="patent-data-table-td ">Method and apparatus for enhanced stereo vision</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8771064">US8771064</a></td><td class="patent-data-table-td patent-date-value">May 25, 2011</td><td class="patent-data-table-td patent-date-value">Jul 8, 2014</td><td class="patent-data-table-td ">Aristocrat Technologies Australia Pty Limited</td><td class="patent-data-table-td ">Gaming system and a method of gaming</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090297023">US20090297023</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 1, 2007</td><td class="patent-data-table-td patent-date-value">Dec 3, 2009</td><td class="patent-data-table-td ">Objectvideo Inc.</td><td class="patent-data-table-td ">Video segmentation using statistical pixel modeling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120002112">US20120002112</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 2, 2010</td><td class="patent-data-table-td patent-date-value">Jan 5, 2012</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Tail the motion method of generating simulated strobe motion videos and pictures using image cloning</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130027549">US20130027549</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 29, 2011</td><td class="patent-data-table-td patent-date-value">Jan 31, 2013</td><td class="patent-data-table-td ">Technische Universitat Berlin</td><td class="patent-data-table-td ">Method and device for video surveillance</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130027550">US20130027550</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 29, 2011</td><td class="patent-data-table-td patent-date-value">Jan 31, 2013</td><td class="patent-data-table-td ">Technische Universitat Berlin</td><td class="patent-data-table-td ">Method and device for video surveillance</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=ykReBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S294000">382/294</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ykReBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc375/defs375.htm&usg=AFQjCNFcuagMfSu6xvEzMh0uBF37Cw37ZA#C375S240110">375/240.11</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=ykReBQABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009320000">G06K9/32</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ykReBQABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04N0007180000">H04N7/18</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ykReBQABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009360000">G06K9/36</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=ykReBQABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/38">G06K9/38</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ykReBQABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06T7/0081">G06T7/0081</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ykReBQABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/00771">G06K9/00771</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06K9/00V4</span>, <span class="nested-value">G06K9/38</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Mar 5, 2013</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIMS 2, 3, 8-11 AND 16-18 ARE CANCELLED.CLAIMS 1, 4-7, 12-15 AND 19 WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 10, 2012</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20120525</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 24, 2012</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20101230</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE OF SECURITY AGREEMENT/INTEREST;ASSIGNOR:RJF OV, LLC;REEL/FRAME:027810/0117</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">OBJECTVIDEO, INC., VIRGINIA</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 8, 2012</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 10, 2009</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">OBJECTVIDEO, INC., VIRGINIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CORRECTIVE ASSIGNMENT TO CORRECT THE TYPOGRAPHICAL ERROR IN THE NAME OF THE ASSIGNEE PREVIOUSLY RECORDED ON  REEL 019044 FRAME 0838;ASSIGNORS:LIPTON, ALAN J.;HAERING, NIELS;RASHEED, ZEESHAN;AND OTHERS;REEL/FRAME:023208/0302</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20070131</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 28, 2008</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">RJF OV, LLC, DISTRICT OF COLUMBIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">GRANT OF SECURITY INTEREST IN PATENT RIGHTS;ASSIGNOR:OBJECTVIDEO, INC.;REEL/FRAME:021744/0464</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20081016</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">RJF OV, LLC,DISTRICT OF COLUMBIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">GRANT OF SECURITY INTEREST IN PATENT RIGHTS;ASSIGNOR:OBJECTVIDEO, INC.;US-ASSIGNMENT DATABASE UPDATED:20100204;REEL/FRAME:21744/464</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">GRANT OF SECURITY INTEREST IN PATENT RIGHTS;ASSIGNOR:OBJECTVIDEO, INC.;US-ASSIGNMENT DATABASE UPDATED:20100211;REEL/FRAME:21744/464</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">GRANT OF SECURITY INTEREST IN PATENT RIGHTS;ASSIGNOR:OBJECTVIDEO, INC.;REEL/FRAME:21744/464</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 8, 2008</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">RJF OV, LLC, DISTRICT OF COLUMBIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:OBJECTVIDEO, INC.;REEL/FRAME:020478/0711</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080208</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">RJF OV, LLC,DISTRICT OF COLUMBIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:OBJECTVIDEO, INC.;US-ASSIGNMENT DATABASE UPDATED:20100204;REEL/FRAME:20478/711</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:OBJECTVIDEO, INC.;REEL/FRAME:20478/711</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 27, 2007</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">OBJECT VIDEO, INC., VIRGINIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LIPTON, ALAN J.;HAERING, NIELS;RASHEED, ZEESHAN;AND OTHERS;REEL/FRAME:019044/0838</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20070131</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U19LpaCntjsUl1aHubHKwI2OjF-Sg\u0026id=ykReBQABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U2seGrh4ZzeplW6II6q1150jmxfsQ\u0026id=ykReBQABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U35ROg4hM61N8QhhBA1BLC1lpus4w","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Video_segmentation_using_statistical_pix.pdf?id=ykReBQABERAJ\u0026output=pdf\u0026sig=ACfU3U3WtaZFXlHEQMISeNUNFeVo1Sx45A"},"sample_url":"http://www.google.com/patents/reader?id=ykReBQABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>