<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US5544352 - Method and apparatus for indexing, searching and displaying data - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Method and apparatus for indexing, searching and displaying data"><meta name="DC.contributor" content="Daniel Egger" scheme="inventor"><meta name="DC.contributor" content="Libertech, Inc." scheme="assignee"><meta name="DC.date" content="1993-6-14" scheme="dateSubmitted"><meta name="DC.description" content="A computer research tool for indexing, searching and displaying data is disclosed. Specifically, a computer research tool for performing computerized research of data including textual objects in a database and for providing a user interface that significantly enhances data presentation is described. Textual objects and other data in a database are indexed by creating a numerical representation of the data. The indexing technique called proximity indexing generates a quick-reference of the relations, patterns and similarity found among the data in the database. Proximity indexing indexes the data by using statistical techniques and empirically developed algorithms. Using this proximity index, an efficient search for pools of data having a particular relation, pattern or characteristic can be effectuated. The Computer Search program, called the Computer Search Program for Data represented in Matrices (CSPDM), provides efficient computer search methods. The CSPDM rank orders data in accordance with the data&#39;s relationship to time, a paradigm datum, or any similar reference. The user interface program, called the Graphical User Interface (GUI), provides a user friendly method of interacting with the CSPDM program and prepares and presents a visual graphical display. The graphical display provides the user with a two dimensional spatial orientation of the data."><meta name="DC.date" content="1996-8-6" scheme="issued"><meta name="DC.relation" content="US:4839853" scheme="references"><meta name="DC.relation" content="US:4945476" scheme="references"><meta name="DC.relation" content="US:5122951" scheme="references"><meta name="DC.relation" content="US:5157783" scheme="references"><meta name="DC.relation" content="US:5206949" scheme="references"><meta name="DC.relation" content="US:5241671" scheme="references"><meta name="DC.relation" content="US:5243655" scheme="references"><meta name="DC.relation" content="US:5301109" scheme="references"><meta name="DC.relation" content="US:5325298" scheme="references"><meta name="DC.relation" content="US:5418948" scheme="references"><meta name="citation_reference" content="Agosti, et al., &quot;A Two-Level Hypertext Retrieval Model for Legal Data,&quot; SIGIR &#39;91 (1991)."><meta name="citation_reference" content="Agosti, et al., A Two Level Hypertext Retrieval Model for Legal Data, SIGIR 91 (1991)."><meta name="citation_reference" content="Belew, Richard, &quot;A Connectionist Approach to Conceptual Information Retrieval,&quot; ICAIL &#39;87 (1987)."><meta name="citation_reference" content="Belew, Richard, A Connectionist Approach to Conceptual Information Retrieval, ICAIL 87 (1987)."><meta name="citation_reference" content="Fowler, et al., &quot;Integrating Query, Thesaurus and Documents Through a Commn Visual Representation,&quot; SIGIR &#39;91 (1991)."><meta name="citation_reference" content="Fowler, et al., Integrating Query, Thesaurus and Documents Through a Commn Visual Representation, SIGIR 91 (1991)."><meta name="citation_reference" content="Gelbart &amp; Smith, &quot;Beyond Boolean Search: FLEXICON, A Legal Text-Based Intelligent System,&quot; ICAIL &#39;91 (1991)."><meta name="citation_reference" content="Gelbart &amp; Smith, Beyond Boolean Search: FLEXICON, A Legal Text Based Intelligent System, ICAIL 91 (1991)."><meta name="citation_reference" content="Lin, &quot;A Self-Organizing Semantic Map for Information Retrieval,&quot; SIGIR &#39;91 (1991)."><meta name="citation_reference" content="Lin, A Self Organizing Semantic Map for Information Retrieval, SIGIR 91 (1991)."><meta name="citation_reference" content="Rose &amp; Belew, &quot;Legal Information Retrieval: a Hybrid Approach,&quot; ICAIL &#39;89 (1989)."><meta name="citation_reference" content="Rose &amp; Belew, Legal Information Retrieval: a Hybrid Approach, ICAIL 89 (1989)."><meta name="citation_reference" content="Turtle &amp; Croft, &quot;Inference Networks for Document Retrieval,&quot; SIGR &#39;90 (1990)."><meta name="citation_reference" content="Turtle &amp; Croft, Inference Networks for Document Retrieval, SIGR 90 (1990)."><meta name="citation_patent_number" content="US:5544352"><meta name="citation_patent_application_number" content="US:08/076,658"><link rel="canonical" href="http://www.google.com/patents/US5544352"/><meta property="og:url" content="http://www.google.com/patents/US5544352"/><meta name="title" content="Patent US5544352 - Method and apparatus for indexing, searching and displaying data"/><meta name="description" content="A computer research tool for indexing, searching and displaying data is disclosed. Specifically, a computer research tool for performing computerized research of data including textual objects in a database and for providing a user interface that significantly enhances data presentation is described. Textual objects and other data in a database are indexed by creating a numerical representation of the data. The indexing technique called proximity indexing generates a quick-reference of the relations, patterns and similarity found among the data in the database. Proximity indexing indexes the data by using statistical techniques and empirically developed algorithms. Using this proximity index, an efficient search for pools of data having a particular relation, pattern or characteristic can be effectuated. The Computer Search program, called the Computer Search Program for Data represented in Matrices (CSPDM), provides efficient computer search methods. The CSPDM rank orders data in accordance with the data&#39;s relationship to time, a paradigm datum, or any similar reference. The user interface program, called the Graphical User Interface (GUI), provides a user friendly method of interacting with the CSPDM program and prepares and presents a visual graphical display. The graphical display provides the user with a two dimensional spatial orientation of the data."/><meta property="og:title" content="Patent US5544352 - Method and apparatus for indexing, searching and displaying data"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("N4ntU4mwKcXXsQSmqIGICw"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("FRA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("N4ntU4mwKcXXsQSmqIGICw"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("FRA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us5544352?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US5544352"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=YPs_BAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS5544352&amp;usg=AFQjCNF_K8mEz7ZPQXPLxBx_j-KL8NP-cA" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US5544352.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US5544352.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US5544352" style="display:none"><span itemprop="description">A computer research tool for indexing, searching and displaying data is disclosed. Specifically, a computer research tool for performing computerized research of data including textual objects in a database and for providing a user interface that significantly enhances data presentation is described....</span><span itemprop="url">http://www.google.com/patents/US5544352?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US5544352 - Method and apparatus for indexing, searching and displaying data</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US5544352 - Method and apparatus for indexing, searching and displaying data" title="Patent US5544352 - Method and apparatus for indexing, searching and displaying data"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US5544352 A</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 08/076,658</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Aug 6, 1996</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Jun 14, 1993</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Jun 14, 1993</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/DE69431351D1">DE69431351D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69431351T2">DE69431351T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0704075A1">EP0704075A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0704075A4">EP0704075A4</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0704075B1">EP0704075B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5832494">US5832494</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6233571">US6233571</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7840524">US7840524</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8555196">US8555196</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20060242564">US20060242564</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1995000896A2">WO1995000896A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1995000896A3">WO1995000896A3</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">076658, </span><span class="patent-bibdata-value">08076658, </span><span class="patent-bibdata-value">US 5544352 A, </span><span class="patent-bibdata-value">US 5544352A, </span><span class="patent-bibdata-value">US-A-5544352, </span><span class="patent-bibdata-value">US5544352 A, </span><span class="patent-bibdata-value">US5544352A</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Daniel+Egger%22">Daniel Egger</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Libertech,+Inc.%22">Libertech, Inc.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US5544352.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5544352.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5544352.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (10),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (14),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (166),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (24),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (15)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/5544352&usg=AFQjCNEr1V_orw3wd0yh3HwMICMXEA9GJQ">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D5544352&usg=AFQjCNGYrJwkpjkgemxHzqhlMj3j32KnIA">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D5544352A%26KC%3DA%26FT%3DD&usg=AFQjCNEsAio3Ez9ndftvku0fNg5cAwS2-A">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54081275" lang="EN" load-source="patent-office">Method and apparatus for indexing, searching and displaying data</invention-title></span><br><span class="patent-number">US 5544352 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA37553277" lang="EN" load-source="patent-office"> <div class="abstract">A computer research tool for indexing, searching and displaying data is disclosed. Specifically, a computer research tool for performing computerized research of data including textual objects in a database and for providing a user interface that significantly enhances data presentation is described. Textual objects and other data in a database are indexed by creating a numerical representation of the data. The indexing technique called proximity indexing generates a quick-reference of the relations, patterns and similarity found among the data in the database. Proximity indexing indexes the data by using statistical techniques and empirically developed algorithms. Using this proximity index, an efficient search for pools of data having a particular relation, pattern or characteristic can be effectuated. The Computer Search program, called the Computer Search Program for Data represented in Matrices (CSPDM), provides efficient computer search methods. The CSPDM rank orders data in accordance with the data's relationship to time, a paradigm datum, or any similar reference. The user interface program, called the Graphical User Interface (GUI), provides a user friendly method of interacting with the CSPDM program and prepares and presents a visual graphical display. The graphical display provides the user with a two dimensional spatial orientation of the data.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(24)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-1.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-1.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-2.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-2.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-3.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-3.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-4.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-4.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-5.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-5.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-6.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-6.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-7.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-7.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-8.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-8.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-9.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-9.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-10.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-10.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-11.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-11.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-12.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-12.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-13.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-13.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-14.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-14.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-15.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-15.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-16.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-16.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-17.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-17.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-18.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-18.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-19.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-19.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-20.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-20.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-21.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-21.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-22.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-22.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-23.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-23.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5544352-24.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5544352-24.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(52)</span></span></div><div class="patent-text"><div mxw-id="PCLM4995807" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is:</claim-statement> <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1. A research system for computerized searching of textual objects, wherein the textual objects are stored in a database, comprising:<div class="claim-text">a computer processor for processing commands and manipulating the textual objects stored in the database;</div> <div class="claim-text">a means, coupled to the computer processor, for entering the commands to be processed by the computer processor;</div> <div class="claim-text">a means for indexing the textual objects using the computer processor and the entered commands comprising:<div class="claim-text">a means for creating vectors representing the textual objects wherein the vectors are created using non-semantical relationships that exist among or between the textual objects;</div> </div> <div class="claim-text">a means for searching the indexed textual objects using the vectors to obtain a pool of textual objects comprising a means for vector searching of the indexed textual objects using the vectors;</div> <div class="claim-text">a graphical user interface means for converting the pool of textual objects into a graphical view comprising:<div class="claim-text">a means for forming a box to graphically represent one or more of the textual objects in the pool; and</div> </div> <div class="claim-text">a display, operably coupled to the graphical user interface means, for showing the graphical view including any of the boxes formed.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2. The research system of claim 1 wherein the means for searching the indexed textual objects further comprises:<div class="claim-text">means for receiving processed commands from the computer processor which identify a pool of textual objects;</div> <div class="claim-text">means for locating textual objects similar to those textual objects in the pool; and</div> <div class="claim-text">means for ranking the importance of the textual objects in the pool.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3. The research system of claim 1 wherein the means for searching the indexed textual objects further comprises:<div class="claim-text">means for creating a vector representing a paradigm textual object.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4. The research system of claim 1 wherein the graphical user interface means further comprises:<div class="claim-text">means for selecting one of the boxes;</div> <div class="claim-text">means for displaying further information on the selected box;</div> <div class="claim-text">zoom-in means for enlarging the size of a portion of the graphical view; and</div> <div class="claim-text">zoom-out means for decreasing the size of a portion of the graphical view.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5. The research system of claim 1 wherein the textual objects contain core words, the means for entering commands comprises a keyboard, and wherein the means for indexing further comprises:<div class="claim-text">a means for generating a boolean word index of the core words found in the textual objects; and</div> <div class="claim-text">wherein the means for searching uses both pattern vectors and the boolean word index and further comprises:<div class="claim-text">a means for performing boolean word searches of the textual objects using the boolean word index.</div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6. The research system of claim 1 wherein a textual object may be selected using the means for entering the commands and wherein the means for searching the indexed textual objects further comprises:<div class="claim-text">means for receiving the identity of a selected textual object;</div> <div class="claim-text">cases-after means for identifying textual objects that refer to the selected textual object; and</div> <div class="claim-text">cases-in means for identifying textual objects to which the selected textual object refers.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7. The research system of claim 1 wherein a pool of textual objects may be selected using the means for entering the commands and wherein the means for searching the indexed textual objects further comprises:<div class="claim-text">means for receiving the identity of the selected pool of textual objects;</div> <div class="claim-text">pool-similarity means for identifying a similar pool of textual objects wherein the objects in the similar pool are similar to the objects in the selected pool; and</div> <div class="claim-text">pool-importance means to identify an important pool of textual objects wherein the objects in the important pool are important in relation to the objects in the selected pool.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8. The research system of claim 1 wherein the means for creating vectors representing the textual objects further comprises:<div class="claim-text">extractor means for creating an initial numerical representation of each textual object wherein the initial numerical representations are based on direct non-semantical relationships between the textual objects;</div> <div class="claim-text">patterner means for analyzing the initial numerical representations of each textual object to find relationships that exist between or among the textual objects comprising:<div class="claim-text">means for calculating a pattern vector representation for each textual object.</div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9. The research system of claim 8 wherein the means for creating vectors representing the textual objects further comprises:<div class="claim-text">weaver means for generating proximity vectors based upon the pattern vector representations for each object.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" class="claim">
      <div class="claim-text">10. The research system of claim 9 wherein the weaver means for generating proximity vectors comprises:<div class="claim-text">a means for calculating euclidean distances between pattern vectors.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" class="claim">
      <div class="claim-text">11. The research system of claim 9 wherein the means for creating vectors further comprises:<div class="claim-text">means for calculating similarity vectors representing the similarity between textual objects using the proximity vectors.</div> </div>
    </div>
    </div> <div class="claim"> <div num="12" class="claim">
      <div class="claim-text">12. A legal research system for computerized searching of textual objects containing core words, wherein the textual objects are stored in a database, comprising:<div class="claim-text">a computer processor for processing commands and manipulating the textual objects stored in the database;</div> <div class="claim-text">a keyboard means, coupled to the computer processor, for entering the commands to be processed by the computer processor;</div> <div class="claim-text">a means for indexing the textual objects using the computer processor and the entered commands comprising;<div class="claim-text">a means for creating vectors representing the textual objects; and</div> <div class="claim-text">a means for generating a boolean word index of the core words found in the textual objects;</div> </div> <div class="claim-text">a means for searching the indexed textual objects using the vectors and the boolean word index to obtain a pool of textual objects comprising;<div class="claim-text">a means for performing boolean word searches using the boolean word index: and</div> <div class="claim-text">a means for vector searching of the indexed textual objects using</div> <div class="claim-text">the vectors, comprising:<div class="claim-text">means for receiving processed commands from the computer processor which identify a selected textual object;</div> <div class="claim-text">cases-after means for identifying textual objects that refer to the selected textual object;</div> <div class="claim-text">cases-in means for identifying textual object to which the selected textual object refers; and</div> <div class="claim-text">similarity means for identifying textual objects which have similar characteristics to the selected textual object;</div> </div> <div class="claim-text">a graphical user interface means for converting the pool of textual objects into a graphical view comprising:</div> <div class="claim-text">a means for forming a box to graphically represent one or more of the textual objects in the pool; and</div> </div> <div class="claim-text">a display, operably coupled to the graphical user interface means, for showing the graphical view including any of the boxes formed.</div> </div>
    </div>
    </div> <div class="claim"> <div num="13" class="claim">
      <div class="claim-text">13. A system for proximity indexing a plurality of data comprising:<div class="claim-text">storage means, connected to the grouping means, for storing a plurality of data in a database;</div> <div class="claim-text">a computer processor for manipulating the plurality of data;</div> <div class="claim-text">means for enabling the computer processor to access the plurality of data stored in the database;</div> <div class="claim-text">extractor means for creating a numerical representation of each accessed datum;</div> <div class="claim-text">patterner means for analyzing the numerical representation of the plurality of data for patterns comprising:<div class="claim-text">means for a calculating a pattern representation for each datum based upon that datums relationship to every other datum; and</div> <div class="claim-text">means for weighing the significance of the pattern representation;</div> </div> <div class="claim-text">weaver means for generating an index on the proximity of each datum to every other datum comprising:<div class="claim-text">a means for determining the Euclidian distance between two pattern representations; and</div> </div> <div class="claim-text">memory for storing the index on the proximity of each datum to every other datum.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" class="claim">
      <div class="claim-text">14. The system of claim 13 wherein each of the purality of data is a non-textual object and wherein the extractor means further comprises:<div class="claim-text">means for numerically representing with a vector the non-textual objects; and</div> <div class="claim-text">means for clustering non-textual objects having similar characteristics.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" class="claim">
      <div class="claim-text">15. The system of claim 13 wherein the extractor means further comprises:<div class="claim-text">means for generating a reference number for each of the plurality of data;</div> <div class="claim-text">means for determining which of the plurality of data refer to any other of the plurality of data; and</div> <div class="claim-text">means for creating a core word index.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" class="claim">
      <div class="claim-text">16. The system of claim 13 wherein the patterner means further comprises:<div class="claim-text">means for analyzing the numerical representation against a plurality of empirically defined patterns, wherein certain of the patterns are more important than others; and</div> <div class="claim-text">wherein the means for weighing further comprises means for heavily weighing certain patterns.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" class="claim">
      <div class="claim-text">17. The system of claim 13 wherein the weaver means further comprises:<div class="claim-text">means for making a similarity determination based upon the Euclidian distances calculated.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" class="claim">
      <div class="claim-text">18. The system of claim 13 wherein the plurality of data are received in a digital signal, the system further comprising:<div class="claim-text">means for receiving the digital signals;</div> <div class="claim-text">means, connected to the receiving means, for interpreting the digital signals into data;</div> <div class="claim-text">means, connected to the interpreting means and storage means, for grouping the plurality of data into a database format;</div> <div class="claim-text">means, connected to the memory, for converting the index into a transmission signal; and</div> <div class="claim-text">means, connected to the converting means, for transmitting the transmission signal representing the index.</div> </div>
    </div>
    </div> <div class="claim"> <div num="19" class="claim">
      <div class="claim-text">19. A system for computerized searching of an index which catalogs a database of objects comprising:<div class="claim-text">key means for entering search commands;</div> <div class="claim-text">a processor, connected to the key means, for processing the search commands;</div> <div class="claim-text">means to retrieve the index utilizing the processor;</div> <div class="claim-text">multiple search means to analyze the index and identify a pool of one or more of the objects based upon a processed search command comprising:<div class="claim-text">means for interpreting a processed search command as a selection of an object;</div> <div class="claim-text">means for identifying a pool of objects that have a relation to the selected object;</div> <div class="claim-text">means for generating a paradigm object; and</div> <div class="claim-text">means for defining a pool of objects that have non-semantical characteristics similar to the paradigm object; and</div> </div> <div class="claim-text">a display for viewing the objects in a pool.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" class="claim">
      <div class="claim-text">20. The system of claim 19 further comprising:<div class="claim-text">means for creating an alphanumeric list of names of the objects in a pool for display.</div> </div>
    </div>
    </div> <div class="claim"> <div num="21" class="claim">
      <div class="claim-text">21. A graphical user interface to display a pool of identified objects stored in a database comprising:<div class="claim-text">means for receiving the identity of objects to be displayed;</div> <div class="claim-text">means for collecting data indicating a first relationship between objects in the pool and data indicating a second relationship between objects in the pool;</div> <div class="claim-text">means for determining a coordinate X/Y location for each identified object in the pool based upon the data indicating a first and second relationship comprising:<div class="claim-text">means for comparing the data indicating the first relationship for determining an X coordinate for each object; and</div> <div class="claim-text">means for comparing the data indicating the second relationship for determining a Y coordinate for each object;</div> </div> <div class="claim-text">means for generating a first window with an X axis and Y axis;</div> <div class="claim-text">means for creating a box for each identified object;</div> <div class="claim-text">means for placing the box for each identified object in the correct X/Y position in the first window;</div> <div class="claim-text">means for displaying the first window with one or more boxes; and</div> <div class="claim-text">means to select a displayed box and obtain further information about the object represented by the displayed box.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="22" class="claim">
      <div class="claim-text">22. The graphical user interface of claim 21 further comprising:<div class="claim-text">means for displaying a second window stacked on top of the first window; and</div> <div class="claim-text">means for moving the second window on the display.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="23" class="claim">
      <div class="claim-text">23. The graphical user interface of claim 21, wherein the first relationship is importance and the second relationship is similarity, further comprising:<div class="claim-text">means to zoom in on a particular portion of the first window; and</div> <div class="claim-text">means to zoom out to view a greater proportion of the first window.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="24" class="claim">
      <div class="claim-text">24. The graphical user interface of claim 21 further comprising:<div class="claim-text">means for requesting a database search comprising:<div class="claim-text">an active display box;</div> <div class="claim-text">a mouse for entry of commands by a user based on the display; and</div> <div class="claim-text">means for converting the mouse entered commands into a database search request.</div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" class="claim">
      <div class="claim-text">25. The graphical user interface of claim 21,<div class="claim-text">wherein the means for displaying further comprises a color monitor;</div> <div class="claim-text">wherein the means for creating a box further comprises means to color the box;</div> <div class="claim-text">wherein the means for generating a first window further comprises a means to generate a light colored background and dark lines representing a coordinate grid.</div> </div>
    </div>
    </div> <div class="claim"> <div num="26" class="claim">
      <div class="claim-text">26. A non-semantical method for numerically representing objects in a computer database and for computerized searching of the numerically represented objects in the database, wherein direct and indirect relationships exist between objects in the database, comprising:<div class="claim-text">marking objects in the database so that each marked object may be individually identified by a computerized search;</div> <div class="claim-text">creating a first numerical representation for each identified object in the database based upon the object's direct relationship with other objects in the database;</div> <div class="claim-text">storing the first numerical representations for use in computerized searching;</div> <div class="claim-text">analyzing the first numerical representations for indirect relationships existing between or among objects in the database;</div> <div class="claim-text">generating a second numerical representation of each object based on the analysis of the first numerical representation;</div> <div class="claim-text">storing the second numerical representation for use in computerized searching; and</div> <div class="claim-text">searching the objects in the database using a computer and the stored second numerical representations, wherein the search identifies one or more of the objects in the database.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="27" class="claim">
      <div class="claim-text">27. The non-semantical method of claim 26, wherein the objects in the database include words, and semantic indexing techniques are used in combination with the non-semantical method, the method further comprising the step of creating and storing a boolean word index for the words of the objects in the database.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="28" class="claim">
      <div class="claim-text">28. The non-semantical method of claim 26, wherein<div class="claim-text">the first and second numerical representations are vectors that are arranged in first and second matrices;</div> <div class="claim-text">the direct relationships are express references from a one object to another object in the database;</div> <div class="claim-text">the objects in the database are assigned chronological data; and</div> <div class="claim-text">wherein the step of searching comprises the steps of<div class="claim-text">matrix searching of the second matrices; and</div> <div class="claim-text">examining the chronological data.</div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="29" class="claim">
      <div class="claim-text">29. The non-semantical method of claim 26 wherein the step of analyzing the first numerical representation further comprises:<div class="claim-text">examining the first numerical representation for patterns which indicate the indirect relationships.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="30" class="claim">
      <div class="claim-text">30. The non-semantical method of claim 29, given that object A occurs before object B and object c occurs before object A, and wherein the step of creating a first numerical representation comprises examining for the direct relationship B cites A and wherein the step of examining for patterns further comprises the step of examining for the following pattern:<div class="claim-text">A cites c, and B cites c.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="31" class="claim">
      <div class="claim-text">31. The non-semantical method of claim 29, wherein a, b, c, A, d, e, f, B, g, h, and i are objects in the database and given that;<div class="claim-text">a, b, and c occur before A;</div> <div class="claim-text">A occurs before d, e, and f, which occur before B; and</div> <div class="claim-text">B occurs before g, h, and i;</div> <div class="claim-text">and wherein the step of examining for patterns further comprises the step of examining for one or more of the following patterns:<div class="claim-text">(i) g cites A, and g cites B;</div> <div class="claim-text">(ii) B cites f, and f cites A;</div> <div class="claim-text">(iii) B cites f, f cites e, and e cites A;</div> <div class="claim-text">(iv) B cites f, f cites e, e cites d, and d cites A;</div> <div class="claim-text">(v) g cites A, h cites B, g cites a, and h cites a;</div> <div class="claim-text">(vi) i cites B, i cites f (or g), and f (or g) cites A;</div> <div class="claim-text">(vii) i cites g, i cites A, and g cites B;</div> <div class="claim-text">(viii) i cites g (or d), i cites h, g (or d) cites A, and h cites B;</div> <div class="claim-text">(ix) i cites a, i cites B, and A cites a;</div> <div class="claim-text">(x) i cites A, i cites e, B cites e;</div> <div class="claim-text">(xi) g cites A, g cites a, A cites a, h cites B, and h cites a;</div> <div class="claim-text">(xii) A cites a, B cites d, i cites a, and i cites d;</div> <div class="claim-text">(xiii) i cites B, i cites d, A cites a, and d cites a;</div> <div class="claim-text">(xiv) A cites b, B cites d (or c), and d (or c) cites b;</div> <div class="claim-text">(xv) A cites b, B cites d, b cites a, and d cites a;</div> <div class="claim-text">(xvi) A cites a, B cites b, d (or c) cites a, and d (or c) cites b.</div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="32" class="claim">
      <div class="claim-text">32. The non-semantical method of claim 26, wherein the step of analyzing further comprises the step of weighing, wherein some indirect relationships are weighed more heavily than other indirect relationships.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="33" class="claim">
      <div class="claim-text">33. The non-semantical method of claim 26, wherein the step of analyzing the first numerical representations for indirect relationships further comprises:<div class="claim-text">creating an interim vector representing each object; and wherein the step of generating a second numerical representation uses coefficients of similarity and further comprises:</div> <div class="claim-text">calculating euclidean distances between interim vector representations of each object;</div> <div class="claim-text">creating proximity vectors representing the objects using the calculated euclidean distances; and</div> <div class="claim-text">using the proximity vectors and using coefficients of similarity to calculate the second numerical representations.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="34" class="claim">
      <div class="claim-text">34. The non-semantical method of claim 26, wherein objects in the database may be divided into subsets and wherein the marking step includes the step of marking subsets of objects in the database and wherein relationships exist between or among subsets of objects in the database.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="35" class="claim">
      <div class="claim-text">35. The non-semantical method of claim 34 wherein the objects are textual objects with paragraphs and the subsets are the paragraphs of the textual objects, the method further comprising the steps of:<div class="claim-text">creating a subset numerical representation for each subset based upon the relationships between or among subsets;</div> <div class="claim-text">analyzing the subset numerical representations;</div> <div class="claim-text">clustering the subsets into sections based upon the subset analysis; and</div> <div class="claim-text">generating a section numerical representation for each section,</div> <div class="claim-text">wherein the section numerical representations are available for searching.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="36" class="claim">
      <div class="claim-text">36. The non-semantical method of claim 26, wherein the step of searching the objects comprises the steps of:<div class="claim-text">selecting an object;</div> <div class="claim-text">using the second numerical representation to search for objects similar to the selected object.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="37" class="claim">
      <div class="claim-text">37. The non-semantical method of claim 26, wherein the step of searching includes the step of graphically displaying one or more of the identified objects.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="38" class="claim">
      <div class="claim-text">38. The non-semantical method of claim 26, wherein the step of searching comprises the step of identifying a paradigm object.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="39" class="claim">
      <div class="claim-text">39. The non-semantical method of claim 26, wherein the step of searching the objects comprises the steps of:<div class="claim-text">selecting a pool of objects;</div> <div class="claim-text">pool-similarity searching to identify a similar pool of textual objects, similar in relation to the objects in marked pool; and</div> <div class="claim-text">pool-importance searching to identify an important pool of textual objects, important in relation to the objects in the selected pool.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="40" class="claim">
      <div class="claim-text">40. The non-semantical method of claim 26, the step of searching comprising the steps of:<div class="claim-text">identifying a paradigm pool of objects; and</div> <div class="claim-text">searching for relationships between the objects and the paradigm pool of objects;</div> <div class="claim-text">wherein the searched for relationship is pool importance or pool similarity.</div> </div>
    </div>
    </div> <div class="claim"> <div num="41" class="claim">
      <div class="claim-text">41. A method for the non-semantical indexing of objects stored in a computer database, the method for use in searching the database for the objects, comprising the steps of:<div class="claim-text">extracting, comprising the steps of:<div class="claim-text">labeling objects with a first numerical representation; and</div> <div class="claim-text">generating a second numerical representation for each object based on each object's references to other objects;</div> </div> <div class="claim-text">patterning, comprising the step of creating a third numerical representation for each object using the second numerical representations, wherein the third numerical representation for each object is determined from an examination of the second numerical representations for occurrences of patterns that define indirect relations between or among objects;</div> <div class="claim-text">weaving, comprising the steps of:<div class="claim-text">calculating a fourth numerical representation for each object based on the euclidean distances between the third numerical representations; and</div> <div class="claim-text">determining a fifth numerical representation for each object by processing the fourth numerical representations through similarity processing; and</div> </div> <div class="claim-text">storing the fifth numerical representations in the computer database as the index for use in searching for objects in the database.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="42" class="claim">
      <div class="claim-text">42. The method of claim 41 wherein the first through fifth numerical representations are vector representations and further comprises the step of clustering objects having similar characteristics.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="43" class="claim">
      <div class="claim-text">43. The method of claim 42, wherein the objects are paragraphs and the step of clustering objects comprises the step of clustering adjacent paragraphs that have similar characteristics.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="44" class="claim">
      <div class="claim-text">44. The method of claim 41 wherein the step of creating the third numerical representations further comprises the steps of:<div class="claim-text">analyzing the second numerical representation against a plurality of empirically defined patterns, wherein certain patterns are more important than others; and</div> <div class="claim-text">weighing the analyzed second numerical representations according to the importance of the patterns.</div> </div>
    </div>
    </div> <div class="claim"> <div num="45" class="claim">
      <div class="claim-text">45. A method for searching indexed objects, wherein the index is stored, comprising the steps of:<div class="claim-text">entering search commands;</div> <div class="claim-text">processing the search commands with a processor;</div> <div class="claim-text">retrieving the stored index using the processor;</div> <div class="claim-text">analyzing the index to identify a pool of objects, comprising the steps of:<div class="claim-text">interpreting the processed searched commands as a selection of an object;</div> <div class="claim-text">identifying a group of objects that have a relationship to the selected object, wherein the step of identifying comprises the steps of:<div class="claim-text">identifying objects that are referred to by the selected object; and</div> <div class="claim-text">identifying objects that refer to the selected object</div> </div> <div class="claim-text">quantifying the relationship of the selected object to each object in the group of objects; and</div> <div class="claim-text">ranking the objects in the group of objects in accordance to the quantified relationship to the selected object; and</div> </div> <div class="claim-text">presenting one or more objects from the group of objects in ranked order.</div> </div>
    </div>
    </div> <div class="claim"> <div num="46" class="claim">
      <div class="claim-text">46. A method for searching indexed objects, wherein the index is stored, chronological information is associated with each object in the group, and a paradigm object may be identified, comprising the steps of:<div class="claim-text">entering search commands;</div> <div class="claim-text">processing the search commands with a processor;</div> <div class="claim-text">retrieving the stored index using the processor;</div> <div class="claim-text">analyzing the index to identify a pool of objects, comprising the steps of:<div class="claim-text">interpreting the processed searched commands as a selection of an object;</div> <div class="claim-text">identifying a group of objects that have a relationship to the selected object;</div> <div class="claim-text">quantifying the relationship of the selected object to each object in the group of objects;</div> <div class="claim-text">ranking the objects in the group of objects in accordance to the quantified relationship to the selected object;</div> </div> <div class="claim-text">chronologically ordering the objects in the group to form a pool of objects;</div> <div class="claim-text">ordering the objects in the pool by rank based upon their relationship to a paradigm object; and</div> <div class="claim-text">presenting one or more objects from the pool of objects in ranked order.</div> </div>
    </div>
    </div> <div class="claim"> <div num="47" class="claim">
      <div class="claim-text">47. A method for graphically displaying and interfacing with a pool of identified objects stored in a database using information indicating relationships, comprising the steps of:<div class="claim-text">receiving the identity of objects in the pool;</div> <div class="claim-text">collecting information indicating a first relationship among objects in the pool;</div> <div class="claim-text">gathering information indicating a second relationship among objects in the pool;</div> <div class="claim-text">determining a coordinate X/Y position for each identified object in the pool based upon the information indicating a first and second relationship comprising the steps of:<div class="claim-text">comparing the information indicating the first relationship for determining an X coordinate for each identified object; and</div> <div class="claim-text">comparing the information indicating the second relationship for determining a Y coordinate for each identified object;</div> </div> <div class="claim-text">generating a first window with an X axis and Y axis, wherein the X and Y axis are able to accommodate the X and Y coordinate for each object;</div> <div class="claim-text">creating a graphical box for each identified object, the box having sides and a bottom;</div> <div class="claim-text">placing a side and the bottom of the graphical box for each identified object in the correct X/Y axis position in the first window;</div> <div class="claim-text">labeling the placed box;</div> <div class="claim-text">displaying the first window with one or more labeled boxes; and</div> <div class="claim-text">selecting a displayed box to obtain further information about the identified object represented by the displayed box.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="48" class="claim">
      <div class="claim-text">48. The method of claim 47 further comprising further comprising the steps of:<div class="claim-text">generating a second window stacked on top of the first window; and</div> <div class="claim-text">moving the second window on the display.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="49" class="claim">
      <div class="claim-text">49. The method of claim 47, wherein the first relationship is importance and the second relationship is similarity, and wherein the step of displaying comprises the steps of:<div class="claim-text">zooming in on a particular portion of the displayed first window; and</div> <div class="claim-text">zooming out to view a different proportion of the displayed first window.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="50" class="claim">
      <div class="claim-text">50. The method of claim 47 wherein an active display box is used, further comprising the steps of:<div class="claim-text">requesting a database search, comprising the steps of:<div class="claim-text">displaying the active display box;</div> <div class="claim-text">entering commands from a user by operation of a mouse on the active display box; and</div> <div class="claim-text">converting the entered commands into a database search request.</div> </div> </div>
    </div>
    </div> <div class="claim"> <div num="51" class="claim">
      <div class="claim-text">51. A system for computerized searching of an index which catalogs a database of objects comprising:<div class="claim-text">key means for entering search commands;</div> <div class="claim-text">a processor, connected to the key means, for processing the search commands;</div> <div class="claim-text">means to retrieve the index utilizing the processor;</div> <div class="claim-text">multiple search means to analyze the index and identify a pool of one or more of the objects based upon a processed search command comprising:<div class="claim-text">means for interpreting a processed search command as a selection of an object;</div> <div class="claim-text">means for identifying a pool of objects that have a relation to the selected object, wherein the means for identifying a pool of objects further comprises:<div class="claim-text">means for identifying objects that are referred to by the selected object;</div> <div class="claim-text">means for identifying objects that refer to the selected object; and</div> <div class="claim-text">means for identifying objects that have a similar characteristic to the selected object;</div> <div class="claim-text">means for generating a paradigm object; and</div> <div class="claim-text">means for defining a pool of objects that have characteristics similar to the paradigm object; and</div> <div class="claim-text">a display for viewing the objects in a pool.</div> </div> </div> </div>
    </div>
    </div> <div class="claim"> <div num="52" class="claim">
      <div class="claim-text">52. A system for computerized searching of an index which catalogs a database of objects comprising:<div class="claim-text">key means for entering search commands;</div> <div class="claim-text">a processor, connected to the key means, for processing the search commands;</div> <div class="claim-text">means to retrieve the index utilizing the processor;</div> <div class="claim-text">multiple search means to analyze the index and identify a pool of one or more of the objects based upon a processed search command comprising:<div class="claim-text">means for interpreting a processed search command as a selection of an object;</div> <div class="claim-text">means for identifying a pool of objects that have a relation to the selected object;</div> <div class="claim-text">means for generating a paradigm object; and</div> <div class="claim-text">means for defining a pool of objects that have characteristics similar to the paradigm object;</div> </div> <div class="claim-text">means for chronologically ordering the objects in a pool;</div> <div class="claim-text">means for rank ordering the objects in a pool based upon their relationship to the selected object;</div> <div class="claim-text">means for rank ordering the objects in a pool based upon their relationship to the paradigm object; and</div> <div class="claim-text">a display for viewing the objects in a pool.</div> </div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES66813312" lang="EN" load-source="patent-office" class="description">
    <heading>TECHNICAL FIELD</heading> <p>This invention pertains to computerized research tools. More particularly, it relates to computerized research on stored databases. Specifically, the invention indexes data, searches data, and graphically displays search results with a user interface.</p>
    <heading>BACKGROUND</heading> <p>Our society is in the information age. Computers maintaining databases of information have become an everyday part of our lives. The ability to efficiently perform computer research has become increasingly more important. The area in our society in which this is most evident is the legal profession. A major problem in the legal profession today is the great deal of time spent performing legal research. Many aspects of legal research are tedious and time consuming. Therefore, performing legal research detracts from the amount of time the attorney is able to spend on tasks that actually require him to utilize his legal judgment and reasoning. Recent efforts in the art of computer research have been aimed at reducing the time required to accomplish legal research. Current computer search programs use a text-by-text analysis procedure (Boolean Search) to scan a database and retrieve items from a database. The attorney must input a string of text, and the computer evaluates this string of text. Then the computer retrieves items from the database that match the string of text. The two most popular systems for computerized searching of data used in the legal profession are Westlaw™, a service sold by West Publishing Company, 50 W. Kellogg Blvd., P.O. Box 64526, St. Paul, Minn. 55164-0526, and Lexis™, a service sold by Mead Data Central, P.O. Box 933, Dayton, Ohio 45401.</p>
    <p>However, Boolean searches of textual material are not very efficient. Boolean searches only retrieve exactly what the computer interprets the attorney to have requested. If the attorney does not phrase his or her request in the exact manner in which the database represents the textual object, the Boolean search will not retrieve the desired textual object. For example, if the attorney desires to retrieve cases in which a judge decided the issue before the jury could decide it, the attorney may enter "Summary Judgment" as his textual string. However, such a request will not retrieve cases that were decided by the judge under a motion to dismiss. Therefore, the researcher may effectively be denied access to significant cases, statutes, laws or other textual objects that may be crucial to the project on which the attorney is working. A second problem encountered with Boolean searches is that the search retrieves a significant amount of irrelevant textual objects. (It should be noted that in the context of legal research, a textual object could be any type of written legal material such as a judicial opinion, a statute, a treatise, a law review article, etc. The term textual object is used to stress the fact that the present invention applies to all types of databases, and not just legal research databases). The only requirement that a textual object must satisfy in order to be selected by a Boolean search program is that part of the textual object match the particular request of the researcher. For example, if the researcher desires to recover all cases that relate to a Fourth Amendment issue, the researcher may input "search and seizure" as his textual string. However, the computer will retrieve every case that happens to mention "search and seizure" one time, even if the case has nothing to do with a Fourth Amendment issue. Since the researcher cannot possibly know all of the groupings of text within all the textual objects in the database, the researcher is unable to phrase his request to only retrieve the textual objects that are relevant.</p>
    <p>Aside from the inefficiency of Boolean searches, the present systems for computerized searching of data are inadequate to serve the needs of a researcher for several other reasons. Even if one assumes that all the textual objects retrieved from a Boolean search are relevant, the listing of the textual objects as done by Westlaw™ or Lexis™ does not convey some important and necessary information to the researcher. The researcher does not know which textual objects are the most significant (i.e., which textual object is referred to the most by another textual object) or which textual objects are considered essential precedent (i.e., which textual objects describe legal doctrines).</p>
    <p>In addition, both Westlaw™ and Lexis™ have a Shepardizing™ feature that enables the researcher to view a list of textual objects that mention a particular textual object. The shepardizing feature does not indicate how many times a listed textual object mentions the particular textual object. Although the shepardizing feature uses letter codes to indicate the importance of a listed textual object (e.g. an "f" beside a listed textual object indicates that the legal rule contained in particular textual object was followed in the listed textual object), data on whether a listed textual object followed the rule of a particular textual object is entered manually by employees of Shepard's™/McGraw Hill, Inc., Div. of McGraw-Hill Book Co., 420 N. Cascade Ave., Colorado Springs, Colo. 80901, toll free 1-800-525-2474. Therefore, such process is subjective and is prone to error.</p>
    <p>Another legal research system that is available is the Westlaw™ key number system. The Westlaw™ key number system has a problem similar to the shepardizing feature on the Lexis™ and Westlaw™ systems. West key numbers are groups of textual objects organized by topic. The West key numbers enable a researcher to search for textual objects on a computerized system via the key numbers. However, the employees of West™ manually determine which cases should be categorized under which key number. Therefore, such a numbering process is subjective and is prone to error. Furthermore, many people in the legal profession have criticized the West key number system because the system is very slow to recognize new topic areas, very rigid and very difficult to keep up to date. In addition, the West™ key number system, like Boolean searches, produces pools of cases that are over-inclusive or under-inclusive.</p>
    <p>The video displays of both the West™ and Lexis™ systems are difficult to use. The simple text displays of these systems do not provide a researcher with all the information that is available in the database.</p>
    <p>Computerized research tools for legal opinions and related documents are probably the most sophisticated computer research tools available and therefore form the background for this invention. However, the same or similar computer research tools are used in many other areas. For example, computer research tools are used for locating prior art for a patent application. The same problems of inefficiency discussed above exist for computer research tools in many areas of our society.</p>
    <p>What is needed is a system for computerized searching of data that is faster than the available systems of research.</p>
    <p>What is needed is a system for computerized searching of data that enables attorneys to research in a manner in which they are familiar.</p>
    <p>What is needed is a computerized research tool that will reorganize, re-index or reformat the data into a more efficient format for searching.</p>
    <p>What is needed are more sophisticated methods to search data.</p>
    <p>What is needed is a system for computerized searching of data that will significantly reduce the number of irrelevant textual objects it retrieves.</p>
    <p>What is needed is a user friendly computerized research tool.</p>
    <p>What is needed is a visual user interface which can convey information to a user conveniently.</p>
    <p>What is needed is a system for computerized searching of data that easily enables the attorney himself to classify the textual object according to his or her own judgment.</p>
    <p>What is needed is a system for computerized searching of data that provides a visual representation of "lead" textual objects and "lines" of textual objects, permitting a broad overview of the shape of the relevant legal "landscape."</p>
    <p>What is needed is a system for computerized searching of data that provides an easily-grasped picture or map of vast amounts of discrete information, permitting researchers (whether in law or other databases) to "zero in" on the most relevant material.</p>
    <p>What is needed is a system for computer searching of data that provides a high degree of virtual orientation and tracking, the vital sense of where one has been and where one is going, and that prevents researchers from becoming confused while assimilating a large amount of research materials.</p>
    <p>Accordingly, there is an unanswered need for a user friendly computerized research tool. There is a need for "intelligent" research technology that emulates human methods of research. There is a need in the marketplace for a more efficient and intelligent computerized research tool.</p>
    <p>The present invention is designed to address these needs.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>This invention is a system for computerized searching of data. Specifically, the present invention significantly aids a researcher in performing computerized research on a database. The invention simplifies the research task by improving upon methods of searching for data including textual objects and by implementing a user interface that significantly enhances the presentation of the data. Simplifying such research reduces the amount of human time that must be allocated to research.</p>
    <p>The invention begins with an existing database and indexes the data by creating a numerical representation of the data. This indexing technique called proximity indexing generates a quick-reference of the relations, patterns, and similarity found among the data in the database. Using this proximity index, an efficient search for pools of data having a particular relation, pattern or characteristic can be effectuated. This relationship can then be graphically displayed.</p>
    <p>There are three main components to the invention; a data indexing applications program, a Computer Search Program for Data Represented by Matrices ("CSPDM"), and a user interface. Various indexing application programs, CSPDMs, and user interface programs can be used in combination to achieve the desired results. The data indexing program indexes data into a more useful format. The CSPDM provides efficient computer search methods. The preferred CSPDM includes multiple search subroutines. The user interface provides a user friendly method of interacting with the indexing and CSPDM programs. The preferred user interface program allows for easy entry of commands and visual display of data via a graphical user interface.</p>
    <p>The method which the invention uses to index textual objects in a database is called Proximity Indexing. Proximity Indexing is a method of preparing data in a database for subsequent searching by advanced data searching programs. Proximity Indexing indexes the data by using statistical techniques and empirically developed algorithms. The resulting search by an advanced data searching program of the Proximity Indexed data is significantly more efficient and accurate than a simple Boolean search.</p>
    <p>The Proximity Indexing Application Program indexes the database into a more useful format to enable the Computer Search Program for Data Represented by Matrices (CSPDM) to efficiently search the database. The Proximity Indexing Application Program of the preferred embodiment has several subroutines, including the Extractor, the Patterner, and the Weaver. The Proximity Indexing Application Program indexes data in a locally located database or remotely located database. The database can contain any type of data including text, alphanumerics, or graphical information.</p>
    <p>In the preferred embodiment, the database is located remotely from the Computer Processor and contains data in the form of textual objects. The Proximity Indexing Application Program indexes the textual objects by determining how each full textual object (e.g., whole judicial opinion, statute, etc.) relates to every other full textual object by using empirical data and statistical techniques. Once each full textual object is related to each other full textual object, the Proximity Indexing Application Program compares each paragraph of each full textual object with every other full textual object as described above. The Proximity Indexing Application Program then clusters related contiguous paragraphs into sections. Subsequently, the Proximity Indexing Application Program indexes each section and the CSPDM evaluates the indexed sections to determine which sections to retrieve from the database. Such organization and classification of all of the textual objects in the database before any given search commences significantly limits the irrelevant textual objects that the CSPDM program retrieves during the subsequent search and allows retrieval of material based on its degree of relevancy.</p>
    <p>Legal research searches on systems like Westlaw™ and Lexis™ only use a series of interrelated Boolean searches of actual text to retrieve textual objects from databases. These searches unnecessarily consume valuable time and retrieve a significant number of irrelevant textual objects.</p>
    <p>Again, this method of computerized research can be used for nearly any database including those containing non-textual material, graphical material, newspaper material, data on personal identification, data concerning police records, etc.</p>
    <p>The remaining two programs in the present invention are the CSPDM and the GUI Program. The CSPDM has seven subroutines that each search for different pools of textual objects. The GUI Program also has seven subroutines. Each subroutine performs a different type of search. Each of the subroutines of the GUI uses the results of the corresponding subroutine of the CSPDM to create the proper display on the display.</p>
    <p>After the Proximity Indexing Application Program indexes a database, the CSPDM application program is used to search the indexed database. The CSPDM program can either be located in memory that is remote from the Computer Processor or local to the Computer Processor. In addition, the CSPDM program can either be remote or local in relation to the database.</p>
    <p>The subroutines of the CSPDM that utilize the matrix coefficients and other data created by the Proximity Indexing Application Program to facilitate its search. However, if the researcher does not have the particular textual object citation available, the researcher can perform a Boolean search to retrieve and organize a pool of textual objects. Alternatively, the researcher can subsequently search for related textual objects by using the Pool-Similarity Subroutine, the Pool-Paradigm Subroutine, the Pool-Importance Subroutine or the Pool-Paradigm-Similarity Subroutine as defined below.</p>
    <p>If the researcher already has the citation of a particular textual object available, the researcher can search for related textual objects by utilizing the Cases-In Subroutine, Cases-After Subroutine or Similar-Cases Subroutine. The Cases-In Subroutine retrieves all of the textual objects from the database to which a selected textual object refers. In addition, the subroutine determines the number of times the selected textual object refers to each retrieved textual object and other characteristics of each textual object, including its importance, and degree of relatedness to the selected textual object.</p>
    <p>The Cases-After Subroutine retrieves all of the textual objects from the database that refer to the selected textual object. Also, the subroutine determines the number of times each retrieved textual object refers to the selected textual object and other characteristics of each textual object, including its importance, and degree of relatedness to the particular textual object to which it refers.</p>
    <p>The Similar-Cases Subroutine determines the degree of similarity between the retrieved textual objects and the selected textual object. Similarity is defined, in the context of legal cases, as the extent to which the two textual objects lie in the same lines of precedent or discuss the same legal topic or concept.</p>
    <p>In addition, if the researcher does not know of a certain particular textual object on which to base his or her search, the researcher may execute a Boolean word search. After a standard Boolean word search has been run, the researcher may run the Pool-Similarity Subroutine to retrieve information containing the degree of similarity between each textual object in the pool and a particular textual object selected by the user. Similarly, the Pool-Importance Subroutine can be used to determine the degree of importance (i.e., whether a judicial opinion is a Supreme Court opinion or a District Court opinion) and other characteristics of each textual object retrieved using the Boolean word search.</p>
    <p>The Pool-Paradigm Subroutine calculates the geographic center in vector space of the pool of textual objects retrieved by the Boolean word search or other pool generating method. It then orders the retrieved textual objects by their degree of similarity to that center or "paradigm." The researcher can then evaluate this "typical textual object" and utilize it to help him or her find other relevant textual objects. In addition, the researcher can scan through neighboring "typical textual objects" to evaluate legal subjects that are closely related to the subject of the researcher's search.</p>
    <p>The Pool-Paradigm-Similarity Subroutine similarly creates a paradigm textual object from the retrieved textual objects. However, the subroutine calculates the similarity of all textual objects in the database to the paradigm textual object in addition to the similarity of the retrieved textual objects to the paradigm textual object.</p>
    <p>After the CSPDM has retrieved the desired textual objects, the Graphical User Interface (GUI) Program may be used to display the results of the search on the display. The GUI is a user interface program. The GUI Program contains three main subroutines: Cases-In Display Subroutine (CIDS), Cases-After Display Subroutine (CADS) and Similar-Cases Display Subroutine (SCDS). The main subroutines receive information from the corresponding subroutines Cases-In, Cases-After and Similar-Cases of the CSPDM. The GUI Program also contains four secondary subroutines: Pool-Similarity Display Subroutine ("PSDS"), Pool-Paradigm Display Subroutine ("PPDS"), Pool-Importance Display Subroutine ("PIDS"), and the Pool-Paradigm-Similarity Subroutine (PPSDS). The secondary subroutines also receive information from the corresponding subroutines Pool-Similarity Subroutine, Pool-Paradigm Subroutine, Pool-Importance Subroutine and the Pool-Paradigm Similarity Subroutine of the CSPDM.</p>
    <p>The CIDS subroutine receives information gathered from the Cases-In Subroutine of the CSPDM. The CIDS subroutine displays user friendly active boxes and windows on the display 38 which represent the textual objects retrieved from the database represented in Euclidean space. The display depicts the appropriate location of textual objects in Euclidean space on a coordinate means. The coordinate means may have one or more axis, but the present embodiment contains two axes. The horizontal axis of the coordinate means represents the time of textual object creation. The vertical axis represents a weighted combination of the number of sections in which that particular retrieved text is cited or discussed, its degree of importance, and its degree of similarity to the host textual object. The CIDS also enables the researcher to open up various active boxes on the display by entering a command into the computer processor with the input means. After entering the proper command, the active box transforms into a window displaying additional information about the selected textual object. These windows can be moved about the display and stacked on top or placed beside each other via the input means to facilitate viewing of multiple windows of information simultaneously. Since the number of textual objects retrieved in a single search may exceed the amount which could be displayed simultaneously, the GUI Program enables the researcher to "zoom in" or "zoom out" to different scales of measurement on both the horizontal and vertical axis.</p>
    <p>The CADS receives information gathered by the Cases-After Subroutine of the CSPDM. The CADS creates a display similar to the CIDS display. However, the active boxes representing the retrieved textual objects indicate which textual objects in the database refer to a selected textual object as opposed to which textual objects a selected textual object refers.</p>
    <p>The SCDS receives information gathered by the Similar-Cases Subroutine of the CSPDM. The SCDS causes a similar display on the display as the CIDS and the CADS except that the vertical axis indicates the degree of similarity between the retrieved textual objects and the selected textual object.</p>
    <p>The GUI Program contains four secondary subroutines: Pool-Search Display Subroutine (PSDS), Pool-Paradigm Display Subroutine (PPDS), Pool-Importance Display Subroutine (PIDS) and the Pool-Paradigm-Similarity Display Subroutine (PPSDS). The PSDS receives the results gathered by the Pool-Search Subroutine of the CSPDM. The PPDS receives the results gathered by the Pool-Paradigm Subroutine of the CSPDM. The PIDS receives the results gathered by the Pool-Importance Subroutine of the CSPDM. The PPSDS receives the results gathered by the Pool-Paradigm-Similarity Subroutine of the CSPDM. The results of the PSDS, PPDS, PIDS and PPSDS are then displayed in a user friendly graphical manner similar to the results of the CIDS, CADS and SCDS. A researcher can access the PSDS, PIDS, PSDS or PPSDS from any of the three main or four secondary subroutines of the GUI to gather information corresponding to the active boxes that represent the pool of textual objects retrieved by the corresponding subroutine of the CSPDM.</p>
    <p>By using the graphical display, the researcher can view immediately a visual representation of trends developing in the law and current and past legal doctrines. In addition, the researcher can immediately identify the important precedent and which textual object serving as the precedent is most important to the project on which the researcher is working. This visual representation is a vast improvement over the current computerized research tools. Furthermore, the researcher using the present invention does not have to rely on the interpretation of another person to categorize different textual objects because the researcher can immediately visualize the legal trends and categories of law. In addition, new topic areas can be recognized without direct human intervention. The current research programs require a researcher to read through the actual text of a number of textual objects in order to determine which textual objects are important, interrelated, or most closely related to the topic at hand and which ones are not.</p>
    <p>It is an object of this invention to create an efficient and intelligent system for computerized searching of data that is faster than available systems of research.</p>
    <p>It is an object of the invention to integrate the system of computerized searching into the techniques to which researchers are already accustomed.</p>
    <p>It is an object of the invention to utilize statistical techniques along with empirically generated algorithms to reorganize, re-index and reformat data in a database into a more efficient model for searching.</p>
    <p>It is an object of the invention to utilize statistical techniques along with empirically generated methods to increase the efficiency of a computerized research tool.</p>
    <p>It is an object of the invention to create a system of computerized searching of data that significantly reduces the number of irrelevant textual objects retrieved.</p>
    <p>It is an object of this invention to create a user friendly interface for computer search tools which can convey a significant amount of information quickly.</p>
    <p>It is an object of the invention to enable the researcher to easily and immediately classify retrieved textual objects according to the researcher's own judgment.</p>
    <p>It is an object of the invention to provide a visual representation of "lead" textual objects and "lines" of textual objects, permitting a broad overview of the shape of the relevant legal "landscape."</p>
    <p>It is an object of the invention to provide an easily-grasped picture or map of vast amounts of discrete information, permitting researchers (whether in law or other databases) to "zero in" on the most relevant material.</p>
    <p>It is an object of the invention to provide a high degree of virtual orientation and tracking that enables a researcher to keep track of exactly what information the researcher has already researched and what information the researcher needs to research.</p>
    <p>These and other objects and advantages of the invention will become obvious to those skilled in the art upon review of the description of a preferred embodiment, and the appended drawings and claims.</p>
    <heading>DESCRIPTION OF THE DRAWINGS</heading> <p>FIG. 1 is a high level diagram of the hardware for the system for computerized searching of data.</p>
    <p>FIG. 2 is high level diagram of the software for the system for computerized searching of data. The three main programs are the Proximity Indexing Application Program, the Computer Search Program for Data Represented by Matrices (CSPDM) Application Program and the Graphical User Interface (GUI) Program.</p>
    <p>FIG. 3A is a flow chart illustrating a possible sequence of procedures that are executed during the Proximity Indexing Application Program.</p>
    <p>FIG. 3B is a flow chart illustrating a possible sequence of the specific subroutines that are executed during one stage of the Proximity Indexing Application Program. The subroutines are the Initial Extractor Subroutine, Opinion Patterner Subroutine, the Opinion Weaver Subroutine, the Paragraph Patterner Subroutine (Optional), the Paragraph Weaver Subroutine and the Section Comparison Subroutine.</p>
    <p>FIG. 3C is flow chart illustrating a possible sequence of subroutines that are executed after the Section Comparison Subroutine. The Section Comparison Subroutine may comprise the Sectioner-Geographic Subroutine and the Section-Topical Subroutine (Optional). The sequence of subroutines executed after the Section Comparison Subroutine are the Section Extractor Subroutine, the Section Patterner Subroutine and the Section Weaver Subroutine.</p>
    <p>FIG. 3D is a high level flow chart illustrating a possible sequence of subroutines that comprise the Boolean Indexing Subroutine which are executed during another stage of the Proximity Indexing Application Program. The first two subroutines, Initialize Core English Words and Create p×w Boolean Matrix, are executed by the Initial Extractor Subroutine. The results are then run through the Pool-Patterner Subroutine, the Pool-Weaver Subroutine, the Pool-Sectioner Subroutine, the Section-Extractor Subroutine, the Section-Patterner Subroutine and the Section Weaver Subroutine.</p>
    <p>FIG. 4A is a high level diagram illustrating the flow of various search routines depending on the type of search initiated by the user by inputing commands to the Computer Processor via the input means. The diagram further illustrates the interaction between the CSPDM and the GUI Program.</p>
    <p>FIG. 4B is a high level flow chart illustrating the sequence of subroutines in the CSPDM program and user interactions with the subroutines.</p>
    <p>FIG. 4C is a high level flow chart for the Cases-In Subroutine.</p>
    <p>FIG. 4D is a high level flow chart for the Cases-After Subroutine.</p>
    <p>FIG. 4E is a high level flow chart for the Similar-Cases Subroutine.</p>
    <p>FIG. 4F is a high level flow chart for the Pool-Similarity Subroutine.</p>
    <p>FIG. 4G is a high level flow chart for the Pool-Paradigm Subroutine.</p>
    <p>FIG. 4H is a high level flow chart for the Pool-Importance Subroutine.</p>
    <p>FIG. 4I is a high level flow chart showing two possible alternate Pool-Paradigm-Similarity Subroutines.</p>
    <p>FIG. 5A is a high level diagram illustrating the interaction between respective subroutines of the CSPDM and of the GUI Program. The diagram further illustrates the interaction between the GUI Program and the display.</p>
    <p>FIG. 5B is an example of the display once the Cases-After Display Subroutine (CADS) is executed.</p>
    <p>FIG. 5C is an example of the display after a user selects an active box representing a textual object retrieved by the Cases-After Subroutine and chooses to open the "full text" window relating to the icon.</p>
    <p>FIG. 5D is an example of the display once the Cases-In Display Subroutine (CIDS) is executed.</p>
    <p>FIG. 5E is an example of the display once the Similar-Cases Display Subroutine (SCDS) is executed.</p>
    <p>FIG. 5F is an example of the display after a user chooses to execute the Similar Cases Subroutine for a textual object retrieved by the Similar-Cases Subroutine represented in FIG. 5E.</p>
    <p>FIG. 5G is an example of the display after a user chooses to execute the Similar Cases Subroutine for one of the cases retrieved by the Similar-Cases Subroutine represented in FIG. 5F.</p>
    <p>FIG. 5H depicts an Execute Search Window.</p>
    <p>FIG. 6 Schematic Representations of the Eighteen Primary Patterns.</p>
    <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT</heading> <p>Referring now to the drawings, the preferred embodiment of the present invention will be described.</p>
    <p>FIG. 1 is an overview of the preferred embodiment of the hardware system 26 for computerized searching of data. The hardware system 26 comprises a Computer Processor 30, a database 54 for storing data, input means, display 38, and RAM 34.</p>
    <p>The Computer Processor 30 can be a processor that is typically found in Macintosh computers, IBM computers, portable PCs, clones of such PC computers (e.g. Dell computers), any other type of PC, or a processor in a more advanced or more primitive computing device. Parallel processing techniques may also be utilized with this invention.</p>
    <p>The database 54 is connected to the Computer Processor 30 and can be any device which will hold data. For example, the database 54 can consist of any type of magnetic or optical storing device for a computer. The database 54 can be located either remotely from the Computer Processor 30 or locally to the Computer Processor 30. The preferred embodiment shows a database 54 located remotely from the Computer Processor 30 that communicates with the personal computer 28 via modem or leased line. In this manner, the database 54 is capable of supporting multiple remote computer processors 50. The preferred connection 48 between the database 54 and the Computer Processor 30 is a network type connection over a leased line. It is obvious to one skilled in the art that the database 54 and the Computer Processor 30 may be electronically connected in a variety of ways. In the preferred embodiment the database 54 provides the large storage capacity necessary to maintain the many records of textual objects.</p>
    <p>The input means is connected to the Computer Processor 30. The user enters input commands into the Computer Processor 30 through the input means. The input means could consist of a keyboard 46, a mouse 42, or both working in tandem. Alternatively, the input means could comprise any device used to transfer information or commands from the user to the Computer Processor 30.</p>
    <p>The display 38 is connected to the Computer Processor 30 and operates to display information to the user. The display 38 could consist of a computer monitor, television, LCD, LED, or any other means to convey information to the user.</p>
    <p>The Random Access Memory (RAM 34) is also connected to the Computer Processor 30. The software system 60 for computerized searching of data may reside in the RAM 34, which can be accessed by the Computer Processor 30 to retrieve information from the software routines. A Read Only Memory (ROM), Erasable Programmable Read Only Memory (EPROM), disk drives, or any other magnetic storage device could be used in place of the RAM 34. Furthermore, the RAM 34 may be located within the structure of the Computer Processor 30 or external to the structure.</p>
    <p>The hardware system 26 for computerized searching of data shown in FIG. 1 supports any one, or any combination, of the software programs contained in the software system 60 for computerized searching of data. The software system 60 for the computerized searching of data comprises one or more of the following programs: the Proximity Indexing Application Program 62, the Computer Search Program for Data Represented by Matrices (CSPDM 66) and the Graphical User Interface (GUI) Program. The Proximity Indexing Application Program 62 could reside in RAM 34 or in separate memory connected to the database 54. The Computer Processor 30 or a separate computer processor 50 attached to the database 54 could execute the Proximity Indexing Application Program 62. In the preferred embodiment the Proximity Indexing Application Program 62 resides in separate memory that is accessible to the database 54, and a separate computer processor 50 attached to the database 54 executes the Proximity Indexing Application Program 62.</p>
    <p>The CSPDM 66 could reside in the RAM 34 connected to the Computer Processor 30 or in the separate memory connected to the database 54. In the preferred embodiment, the CSPDM 66 is located in the RAM 34 connected to the Computer Processor 30. The CSPDM 66 may use the display 38 to depict input screens for user entry of information.</p>
    <p>The GUI Program 70 could likewise reside in the RAM 34 connected to the Computer Processor 30 or in separate memory connected to the database 54. In the preferred embodiment, the GUI Program 70 is located in the RAM 34 connected to the Computer Processor 30. The GUI Program 70 also communicates with the display 38 to enhance the manner in which the display 38 depicts information.</p>
    <p>FIG. 2 is an overview of the preferred embodiment of the software system 60 for computerized searching of data. The software system 60 for computerized searching of data comprises at least one or more of the following programs: the Proximity Indexing Application Program 62, the Computer Search Program for Data Represented by Matrices (CSPDM 66) and the Graphical User Interface (GUI) Program. Proximity Indexing is a method of identifying relevant data by using statistical techniques and empirically developed algorithms. The Proximity Indexing Application Program 62 is an application program which indexes the database 54 to a proper format to enable the Computer Search Program for Data Represented by Matrices (CSPDM 66) to properly search the database 54. The Proximity Indexing Application Program 62 can index data in a local database 54 or a remote database 54. The Proximity Indexing Application Program 62 is shown in more detail in FIGS. 3A to 3D.</p>
    <p>After the Proximity Indexing Application Program 62 indexes the database 54, the CSPDM 66 application program can adequately search the database 54. The CSPDM 66 program searches the database 54 for textual objects according to instructions that the user enters into the Computer Processor 30 via the input means. The CSPDM 66 then retrieves the requested textual objects. The CSPDM 66 either relays the textual objects and other information to the GUI program in order for the GUI program to display this information on the display 38, or the CSPDM 66 sends display commands directly to the Computer Processor 30 for display of this information. However, in the preferred embodiment, the CSPDM 66 relays the textual objects and other commands to the GUI Program 70. The CSPDM 66 is described in more detail in FIGS. 4A to 4I.</p>
    <p>After the CSPDM 66 has retrieved the textual objects, the Graphical User Interface (GUI) Program, which is a user interface program, causes the results of the search to be depicted on the display 38. The GUI Program 70 enhances the display of the results of the search conducted by the CSPDM 66. The GUI Program 70, its method and operation, can be applied to other computer systems besides a system for computerized searching of data. The GUI Program 70 is described in more detail in FIGS. 5A to 5H.</p>
    <p>FIGS. 3A to 3D depict examples of the preferred procedures and subroutines of a Proximity Indexing Application Program 62, and possible interactions among the subroutines. FIG. 3A depicts a sequence of procedures followed by the Proximity Indexing Application Program 62 to index textual objects for searching by the CSPDM 66. FIG. 3B depicts specific subroutines that the Proximity Indexing Application Program 62 executes to partition full textual objects into smaller sections. FIG. 3C depicts subroutines executed by the Section Comparison Routine of FIG. 3B and subsequent possible subroutines to format and index the sections. FIG. 3D depicts a sequence of subroutines of the Proximity Indexing Application Program 62 which first sections and then indexes these sections of "core english words" 140 contained in the database 54. "core english words" 140 are words that are uncommon enough to somewhat distinguish one textual object from another. The word searches of the CSPDM 66 search these sections of core English words to determine which textual objects to retrieve.</p>
    <p>Before describing the Proximity Indexing Application Program 62 in detail, a preliminary description of the Proximity indexing method would be helpful.</p>
    <p>"Proximity indexing" is a method of indexing that uses statistical techniques and empirically generated algorithms to organize and categorize data stored in databases. The Proximity Indexing Application Program 62 applies the Proximity indexing method to a database 54. The preferred embodiment of the present invention uses the Proximity Indexing Application Program 62 to Proximity index textual objects used for legal research by indexing objects based on their degree of relatedness--in terms of precedent and topic--to one another.</p>
    <p>Applying the method to legal research, the "Proximity indexing" system treats any discrete text as a "textual object." Textual objects may contain "citations," which are explicit references to other textual objects. Any legal textual object may have a number or different designations of labels. For example, 392 U.S. 1, 102 S.Ct 415, 58 U.S.L.W. 1103, etc. may all refer to the same textual object.</p>
    <p>Cases are full textual objects that are not subsets of other textual objects. Subsets of a full textual object include words, phrases, paragraphs, or portions of other full textual objects that are referred to in a certain full textual object. (The system does not treat textual objects as subsets of themselves.)</p>
    <p>Every case, or "full" textual object, is assigned a counting-number "name"--designated by a letter of the alphabet in this description--corresponding to its chronological order in the database 54. Obviously, textual objects may contain citations only to textual objects that precede them. In other words, for full textual objects, if "B cites A," (i.e. "A is an element of B" or "the set `B` contains the name `A`"), textual object A came before B, or symbolically, A&lt;B. Every textual object B contains a quantity of citations to full textual objects, expressed as Q(B), greater than or equal to zero, such that Q(B)&lt;B.</p>
    <p>Textual objects other than full textual objects may be subsets of full textual objects and of each other. For example, a section, page, or paragraph of text taken from a longer text may be treated as a textual object. Phrases and words are treated as a special kind of textual object, where Q(w)=0. Sections, pages, and paragraphs are generally subsets of only one full textual object, and may be organized chronologically under the numerical "name" of that full textual object. For purposes of chronology, phrases and words are treated as textual objects that precede every full textual object, and can generally be treated as members of a set with name "0," or be assigned arbitrary negative numbers.</p>
    <p>Any two textual objects may be related to each other through a myriad of "patterns." Empirical research demonstrates that eighteen patterns capture most of the useful relational information in a cross-referenced database 54. A list of these eighteen patterns, in order of importance, is attached as Appendix #1. (For a discussion on probability theory and statistics, see Wilkinson, Leland; SYSTAT: The System for Statistics; Evanston, Ill.: SYSTAT Inc., 1989 incorporated herein by reference.) Some patterns occur only between two full textual objects, and others between any two textual objects; this distinction is explained below. Semantical patterning is only run on patterns number one and number two of Appendix #1. For purposes of explaining how patterns are used to generate the Proximity Index, only the two simplest patterns are illustrated.</p>
    <p>The simplest, Pattern #1, is "B cites A." See Appendix No. 1. In the notation developed, this can be diagramed: a b c A d e f B g h i where the letters designate textual objects in chronological order, the most recent being on the right, arrows above the text designate citations to A or B, and arrows below the text designate all other citations. The next simplest pattern between A and B, Pattern #2, is "B cites c and A cites c," which can also be expressed as "there exists c, such that c is an element of (A intersect B)." See Appendix No. 1. This can be diagramed: a b c A d e f B g h i. For every textual object c from 0 to (A-1), the existence of Pattern #2 on A and B is signified by 1, its absence by 0. This function is represented as P#2AB(c)=1 or P#2AB(c)=0. The complete results of P#1AB and P#2AB can be represented by an (A)×(1) citation vector designated X.</p>
    <p>The functions of some Patterns require an (n)×(1) matrix, a pattern vector. Therefore it is simplest to conceive of every Pattern function generating an (n)×(1) vector for every ordered pair of full textual objects in the database 54, with "missing" arrays filled in by 0s. Pattern Vectors can be created for Pattern #1 through Pattern #4 by just using the relationships among textual object A and the other textual objects in the database 54 and among textual object B and the other textual objects in the database 54. Pattern Vectors for Patterns #5 through #18 can only be created if the relationship of every textual object to every other textual object is known. In other words, Pattern Vectors for Patterns #1 through #4, can be created from only the rows A and B to the Citation Matrix but Pattern Vectors for Patterns #5 through #18 can only be created from the whole Citation Matrix. (See Appendix #1):</p>
    <p>(total textual objects c) / (theoretical maximum textual objects c) [(x)(x)<sup>T</sup> /TMax],</p>
    <p>(total textual objects c) / (actual maximum textual objects c) [(x)(x)<sup>T</sup> /AMax] frequency of object c per year [f], and</p>
    <p>the derivative of the frequency [f].</p>
    <p>In pattern #2, given that A&lt;B, the theoretical maximum ("TMax") number Q(A intersect B)=A minus 1. The actual maximum possible ("AMax"), given A and B,is the lesser of Q(A) and Q(B). The ratios "X(X)<sup>T</sup> /TMax" and "X(X)<sup>T</sup> /AMax," as well as the frequency of occurrence of textual objects c per year, f2(A, B), and the first derivative f'2(A, B), which gives the instantaneous rate of change in the frequency of "hits," are all defined as "numerical factors" generated from patterns #1 and #2. These are the raw numbers that are used in the weighing algorithm.</p>
    <p>For Pattern #2, the total number of possible textual objects c subject to analysis, (i.e. TMax) is A-1 (one) and is only for the years at issue which are those up to the year in which A occurred. However, a relationship may remain "open," that is, it may require recalculation of f(x) and f'(x) as each new textual object is added to the database 54, (for a total of n cases subject to analysis).</p>
    <p>The "numerical factors" for all eighteen patterns are assigned various weights in a weighing algorithm used to generate a scalar F(A, B). The function F generates a scalar derived from a weighted combination of the factors from all eighteen patterns. The patterns are of course also weighted by "importance," allowing Supreme Court full textual objects to impose more influence on the final scalar than District Court full textual objects, for example. The weighing of the more than 100 factors is determined by empirical research to give results closest to what an expert human researcher would achieve. The weighing will vary depending upon the type of material that is being compared and the type of data in the database 54. (See Thurstone. The Vectors of Mind, Chicago, Ill.: University of Chicago Press, 1935, for a description of factor loading and manipulating empirical data incorporated herein by reference.) In a commercial "Proximity Indexer" it will be possible to reset the algorithm to suit various types of databases.</p>
    <p>A scalar F(A, B) is generated for every ordered pair of full cases in the database 54, from F(1, 2) to F(n-1, n). F(z,z) is defined as equal to 0.</p>
    <p>The full results of F(A,B) are arranged in an (n)×(n) matrix designated F. Note that F(B, A) is defined as equal to F(A, B), and arrays that remain empty are designated by 0. For every possible pairing of cases (A,B), a Euclidean distance D(A,B) is calculated by subtracting the Bth row of Matrix F from the Ath row of Matrix F. In other words:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">D(A,B)=[(F(1, A)-F(1, B))<sup>2</sup> +(F(2, A)-F(2, B)<sup>2</sup> + . . . +(F(n, A)-F(n, B))<sup>2</sup> ]<sup>1/2</sup>.</pre>
    
    <p>A function designated D(A,B) generates a scalar for every ordered pair (A,B), and hence for every ordered pair of textual objects (A,B) in the database 54. The calculations D(A,B) for every ordered pair from D(1,1) to D(n,n) are then arranged in an (n)×(n) "proximity matrix" D. Every column vector in D represents the relationship between a given case A and every other case in the database 54. Comparing the column vectors from column A (representing textual object A) and column B (representing textual object B) allows one to identify their comparative positions in n-dimensional vector space, and generate a coefficient of similarity, S(A,B), from 0-100%, which is more precise and sophisticated than F(A,B) or D(A,B) alone. A similarity subroutine can run directly on F(A,B). However, the real power of the Proximity Matrix D is that it allows one to identify "groups" or "clusters" of interrelated cases.</p>
    <p>Through factor loading algorithms, the relationships represented by D for "n" cases can be re-represented in a vector space containing fewer than "n" orthogonal vectors. This knowledge can be reflected in S(A,B).</p>
    <p>The Proximity Indexing Application Program 62 is an application program that applies the above techniques and algorithms to index and format data to be searched by the CSPDM 66. FIG. 3A describes the overall procedure of the Proximity Application Indexing Program 72. The first stage initializes the data 74 in the database 54. The second stage determines the relationships between full textual objects 78. The third stage determines the relationships between paragraphs of each textual object and each full textual object 80. The fourth stage clusters related paragraphs using factor loading and empirical data and then groups the paragraphs into sections based on such data 84. The fifth stage determines the relationships between the sections 88. In the final stage, the sectioned textual objects are not further processed until commands are received from the CSPDM Routine 92.</p>
    <p>The following description of FIG. 3B and FIG. 3C elaborates on this general procedure by describing specific subroutines of the preferred Proximity Application Indexing Program. The following is a step by step description of the operation of the Proximity Index Application Program.</p>
    <p>Section A Initial Extractor Subroutine 96:</p>
    <p>FIG. 3B describes subroutines for the first portion of the preferred Proximity Indexing Application Program 62. The first subroutine of the Proximity Indexing Applications Program is the Initial Extractor Subroutine 96. The Initial extractor subroutine 96 performs three primary functions: Creation of the Opinion Citation Matrix, creation of the Paragraph Citation Matrix, and creation of Boolean Word Index.</p>
    <p>The following steps are performed by the Initial extractor subroutine 96.</p>
    <p>1. Number all full textual objects chronologically with arabic numbers from 1 through n.</p>
    <p>2. Number all paragraphs in all the full textual objects using arabic numbers from 1 through p.</p>
    <p>3. Identify the page number upon which each paragraph numbered in step two above begins.</p>
    <p>4. Create Opinion Citation Vectors (X). By comparing each full textual object in the data base to every other full textual object in the data base that occurred earlier in time.</p>
    <p>5. Combine Opinion Citation Vectors to create the bottom left half portion of the n×n Opinion citation matrix.</p>
    <p>6. Create a mirror image of the bottom left half portion of the Opinion citation matrix in the top right half portion of the same matrix, to complete the matrix. In this manner only n<sup>2</sup> /2 comparisons need to be conducted. The other 1/2 of the comparisons are eliminated.</p>
    <p>7. Create the p×n Paragraph Citation Vectors by comparing each paragraph to each full textual object that occurred at an earlier time. This will require (n/2)p searches.</p>
    <p>8. Create a Paragraph Citation Matrix by combining Paragraph Citation Vectors to create the bottom left half portion of the matrix.</p>
    <p>9. Complete the creation of the Paragraph Citation Matrix by copying a mirror image of the bottom left half portion of the matrix into the top right half portion of the matrix.</p>
    <p>10. Initialize the Initial extractor subroutine 96 with a defined set of core English words.</p>
    <p>11. Assign identification numbers to the core English words. In the preferred embodiment 50,000 English words are used and they are assigned for identification the numbers from -50,000 to -1.</p>
    <p>12. Create a Boolean Index Matrix 144 with respect to the core English words by searching the database 54 for the particular word and assigning the paragraph number of each location of the particular word to each particular word. This procedure is described in greater detail in FIG. 3D.</p>
    <p>Section B Opinion Patterner Subroutine 100:</p>
    <p>The Opinion Patterner Subroutine 100 performs three primary functions: Pattern analysis on matrices, calculation of the numerical factors and weighing the numerical factors to reach resultant numbers.</p>
    <p>13. Process the Opinion Citation Matrix through each of the pattern algorithms contained in appendix #1 for each ordered pair of full textual objects to create opinion pattern vectors for each pattern and for each pair of full textual objects. The pattern algorithms determine relationships which exist between the ordered pair of textual objects. (See Appendix #1) The first four pattern algorithms can be run utilizing just the Opinion Citation Vector for the two subject full textual objects. Each pattern algorithm produces a opinion pattern vector as a result. The fifth through eighteenth pattern algorithms require the whole Opinion Citation Matrix to be run through the Opinion Patterner Subroutine 100.</p>
    <p>14. Calculate total hits (citation) for each pattern algorithm. This can be done by taking the resultant opinion pattern vector (OPV) and multiplying it by the transposed opinion pattern vector (OPV)<sup>T</sup> to obtain a scalar number representing the total hits.</p>
    <p>15. Calculate the theoretical maximum number of hits. For example, in the second pattern of Appendix #1, the theoretical maximum is all of the full textual objects that occur prior in time to case A (A-1).</p>
    <p>16. Calculate the actual maximum number of hits. For example, in the second pattern of Appendix #1, the actual maximum possible number of hits is the lesser of the number of citations in full textual object Q(A) or full textual object Q(B).</p>
    <p>17. Calculate the total number of hits (citations) per year. This is labeled f(A,B).</p>
    <p>18. Calculate the derivative of the total change in hits per year. This is the rate of change in total hits per year and is labeled f' (A,B).</p>
    <p>19. Calculate the ratio of total hits divided by theoretical max [(oPV)(oPV)<sup>t/TMAX</sup> ].</p>
    <p>20. Calculate the ratio of the total hits divided by the actual maximum [(oPV)(oPV)<sup>t</sup> <sub>/AMAX</sub> ].</p>
    <p>21. Calculate a weighted number F(A,B) which represents the relationship between full textual object A and full textual object B. The weighted number is calculated using the four raw data numbers, two ratios and one derivative calculated above in steps 14 through 20 for each of the 18 patterns. The weighing algorithm uses empirical data or loading factors to calculate the resulting weighted number.</p>
    <p>22. The Opinion Patterner Subroutine 100 sequence for the Opinion Citation Matrix is repeated n-1 times to compare each of the ordered pairs of full textual objects. Therefore, during the process, the program repeats steps 13 through 21, n-1 times.</p>
    <p>23. Compile the Opinion Pattern Matrix by entering the appropriate resulting numbers from the weighing algorithm into the appropriate cell locations to form an n×n Opinion Pattern Matrix.</p>
    <p>Section C The Opinion Weaver Subroutine 104:</p>
    <p>The Opinion Weaver Subroutine 104 performs two primary tasks: calculation of the Opinion Proximity Matrix and calculation of the Opinion Similarity Matrix. The Opinion Proximity Matrix D is generated by calculating the Euclidean Distance between each row A and B of the Opinion Pattern Matrix (D(A,B)) for each cell DC(A,B ). The Opinion Similarity Matrix is generated by calculating the similarity coefficient from 0 to 100 between each row A and B of the Opinion Proximity Matrix (S(A,B)) in each cell SC(A,B) in matrix S.</p>
    <p>24. Calculate the n×n Opinion Proximity Matrix. To calculate D(A,B) the program takes the absolute Euclidian distance between column A and column B of the n×n Opinion Pattern Matrix. The formula for calculating such a distance is the square root of the sum of the squares of the distances between the columns in each dimension, or:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">D(A,B)=[(F(1,A)-F(1,B))<sup>2</sup> +(F(2,A)-F(2,B))<sup>2</sup> + . . . +(F(N,A)F(N,B))<sup>2</sup> ]<sup>1/2</sup> </pre>
    
    <p>The Opinion Proximity Matrix created will be an n×n matrix. The smaller the numbers in the Opinion Proximity Matrix the closer the relationship between full textual object A and full textual object B.</p>
    <p>25. Create n×n Opinion Similarity Matrix. To calculate the Opinion Similarity Matrix each scalar number in the Opinion Proximity Matrix is processed through a coefficient of similarity subroutine which assigns it a number between 0 and 100. By taking the coefficient of similarity, the program is able to eliminate full textual objects which have Euclidian distances that are great. (For example, a Euclidean distance that is very large and is run through the coefficient of similarity would result in a very low coefficient of similarity. Euclidean distances resulting in similarities below four are eliminated in the preferred embodiment).</p>
    <p>Section D Paragraph Patterner Matrix Subroutine 108 (Optional):</p>
    <p>26. Obtain the p×n Paragraph Citation Matrix calculated by the Initial extractor subroutine 96.</p>
    <p>27. Run each ordered pair of rows of the p×n Paragraph Citation Matrix for an individual full textual object i through the pattern algorithms number one and two (see appendix #1) and determine the resultant Paragraph Pattern Vector.</p>
    <p>28. Calculate the various numerical factors (AMax, TMax, etc.) by evaluating the values in the Paragraph Pattern Vector.</p>
    <p>29. Run the Paragraph Pattern Vector and the numerical factors through the weighing algorithm to determine the appropriate value for each cell of the c<sub>i</sub> ×n Partial Paragraph Pattern Matrix where c<sub>i</sub> is the number of paragraphs in full textual object i.</p>
    <p>30. Repeat steps 27 through 29 for each full textual object i where i=1 to n, to create the p×n Paragraph Pattern Matrix.</p>
    <p>Section E Paragraph Weaver Subroutine 112:</p>
    <p>31. Calculate the Euclidean distance of each ordered pair of rows of either the p×n Paragraph Citation Matrix or the p×n Paragraph Pattern Matrix for a single full textual object i.</p>
    <p>32. Place the resultant Euclidean distance values in the appropriate cell of the c<sub>i</sub> ×c<sub>i</sub> Paragraph Proximity Matrix where c<sub>i</sub> is the number of paragraphs in full textual object i, where 0&lt;i&lt;n+1.</p>
    <p>33. Repeat steps 31 through 32 n times in order to calculate n different Paragraph Proximity Matrices (one for each full textual object i).</p>
    <p>34. The Section Comparison Subroutine 116 clusters all p paragraphs in the database 54 into sections. Then the sections are compared and indexed in the database 54. This procedure is described in greater detail in FIG. 3C.</p>
    <p>FIG. 3C depicts possible subroutines that the Section Comparison Subroutine 116 comprises. The subroutines are the Sectioner Geographical Subroutine 120, the Sectioner Topical Subroutine 124 (Optional), the Section Extractor Subroutine 128, the Section Patterner Subroutine 132 and the Section Weaver Subroutine 163.</p>
    <p>Section F Sectioner Geographical Subroutine 120:</p>
    <p>35. For each full texual object i, the Sectioner Geographical Subroutine 120 uses the corresponding c<sub>i</sub> ×c<sub>i</sub> Paragraph Proximity Matrix and a contiguity factor for each paragraph to determine which paragraphs may be clustered into sections. Sections are made up of continuous paragraphs that are combined based upon weighing their Euclidean distances and contiguity.</p>
    <p>36. Repeat step 35 for all n full textual objects until all p paragraphs are grouped into q sections.</p>
    <p>Section H Sectioner Topical Subroutine 124 (Optional):</p>
    <p>37. The Sectioner Topical Subroutine 124 provides additional assistance to the Sectioner Geographical Subroutine 120 by considering the factor of topical references to determine the q sections.</p>
    <p>38. For the total number of discrete references "z" to each full textual object in a particular full textual object, a z×z Citation Proximity Matrix is formed by comparing the Euclidean distances between each reference to a full textual object contained in each paragraph and calculating the topical weight given to each paragraph.</p>
    <p>Section I Section Extractor Subroutine 128:</p>
    <p>39. The Section Extractor Subroutine 128 numbers each section created by the Sectioner Subroutines from 1 to q.</p>
    <p>40. The Sectioner Extractor Subroutine 128 creates a q×q Section Citation Matrix by determining which sections refer to every other section.</p>
    <p>Section J Section Patterner Subroutine 132:</p>
    <p>41. The Section Patterner Subroutine 132 then calculates 18 Section Pattern Vectors corresponding to each row of the q×q Section Citation Matrix using the 18 pattern algorithms in appendix #1.</p>
    <p>42. From the Section Pattern Vectors, the numerical factors (AMax, TMax, etc.) are calculated.</p>
    <p>43. The weighing algorithm evaluates the numerical factors and the Section Pattern Vectors and determines the values for each cell of the q×q Section Pattern Matrix.</p>
    <p>Section K Section Weaver Subroutine 163:</p>
    <p>44. The Section Weaver Subroutine 163 calculates the Euclidean distances between each row of the q×q Section Pattern Matrix and creates a q×q Section Proximity Matrix.</p>
    <p>45. The Section Weaver Subroutine 163 then creates a q×q Section Similarity Matrix with coefficients 0 to 100 using the values of the Section Proximity Matrix and empirical data and factor loading.</p>
    <p>Section L Semantical Clustering of a Boolean Index Routine:</p>
    <p>FIG. 3D depicts a possible Semantical Clustering of a Boolean Index Routine. (See Hartigan, J. A. Clustering Algorithms. New York: John Wiley &amp; Sons, Inc., 1975, for detailed description of clustering algorithms incorporated herein by reference.) The Semantical Clustering routine indexes the textual objects according to the similarity of phrases and words contained within each textual object in a database 54. The routine comprises seven possible subroutines: the Opinion Extractor Subroutine, the Pool Patterner Subroutine 152, the Pool Weaver Subroutine, the Pool Sectioner Subroutine 160, the Section Extractor, the Section Patterner Subroutine 132 and the Section Weaver Subroutine 163. In fact, it is quite possible, using only semantical statistical techniques, to "Proximity-index" documents that do not refer to one another at all based on their Boolean indices.</p>
    <p>Section M Opinion Extractor Subroutine:</p>
    <p>46. As described in steps 10 and 11, the Opinion Extractor Subroutine initializes a set of core English words and assigns each word a number. The preferred embodiment uses 50,000 discrete core English words and assigns each discrete core English word a number from -50,000 to -1.</p>
    <p>47. The Opinion Extractor Subroutine then converts the core English words into a p×w matrix. The number of columns (w) represents the number of discrete core English words in the database 54 and the number of rows (p) represents the number of paragraphs in the database 54.</p>
    <p>48. The Opinion Extractor Subroutine fills the p×w matrix by inserting a "1" in the matrix cell where a certain paragraph contains a certain word.</p>
    <p>Section N Pool Patterner Subroutine 152:</p>
    <p>49. The Pool Patterner Subroutine 152 creates two pattern algorithm vectors for only the first two patterns in appendix #1 and determines values for the total number of hits, the theoretical maximum number of hits, the actual maximum number of hits, the total number of hits per year and the derivative of the total number of hits per year.</p>
    <p>50. The weighing algorithm of the Pool Patterner Subroutine 152 uses empirical data and factor loading to determine values to enter into a p×w Paragraph/Word Pattern Matrix.</p>
    <p>51. The Pool Patterner Subroutine 152 creates a p×w Paragraph/Word Pattern Matrix by filling the appropriate cell of the Matrix with the appropriate value calculated by the weighing algorithm.</p>
    <p>52. The Pool Patterner Subroutine 152 creates a p×w Paragraph/Word Proximity Matrix taking the Euclidean distance between the rows of the Paragraph/Word Pattern Matrix.</p>
    <p>Section O Pool Sectioner Subroutine 160:</p>
    <p>53. The Pool Sectioner Subroutine 160 evaluates the Euclidean distances in the Paragraph/Word Proximity Matrix and the contiguity factor of each paragraph to duster the paragraphs (p) into a group of (v) sections and create a v×w Preliminary Cluster Word Matrix.</p>
    <p>Section P Section Extractor Subroutine 128:</p>
    <p>54. The Section Extractor Subroutine 128 numbers each section chronologically and creates a v×v Section Word Citation Matrix.</p>
    <p>Section Q Section Patterner Subroutine 132:</p>
    <p>55. The Section Patterner Subroutine 132 evaluates the v×v Section Word Citation Matrix to create two word pattern vectors for only the first two patterns algorithms in appendix #1 and determines numerical factors for the total number of hits, the theoretical maximum number of hits, the actual maximum number of hits, the total number of hits per year and the derivative of the total number of hits per year.</p>
    <p>56. The Weighing algorithm uses empirical data and factor loading to weigh the numerical factors created from the word pattern vectors and uses the numerical factors and the word pattern vectors to determine values to enter into a v×v Section Word Pattern Matrix.</p>
    <p>Section R Section Weaver Subroutine 136:</p>
    <p>57. The Section Weaver Subroutine 136 creates a v×v Section Word Proximity Matrix by taking the Euclidean distance between the rows of the Section Word Pattern Matrix and placing the appropriate Euclidean distance value in the appropriate cell of the Section Word Proximity Matrix.</p>
    <p>58. The Section Weaver Subroutine 136 create a v×v Section Word Similarity Matrix by evaluating the Euclidean distances from the Section Word Proximity Matrix and empirical data, and calculating the similarity coefficient for each ordered pair of sections, and places the value in the appropriate cell of the Section Word Similarity Matrix.</p>
    <p>59. The Pool Searches of the CSPDM 66 evaluate the Section Word Similarity Matrix as well as other matrices to determine whether or not to retrieve a full textual object.</p>
    <p>FIGS. 4A and 4B are high level flow charts that illustrate the general flow of the subroutines of the CSPDM 66. FIG. 4A illustrates that the flow of various search routines depend on the type of search initiated by the researcher. The diagram further illustrates the interaction between the CSPDM 66 and the GUI Program 70. FIG. 4B illustrates the sequence of subroutines in the CSPDM 66 program and the user interactions with the subroutines. FIG. 4B further shows that the researcher can access the different search subroutines and use information that the researcher has already received to find new information.</p>
    <p>FIG. 4B provides a high level flow chart illustrating the sequence of subroutines in the CSPDM 66 program and the researcher's interactions with the subroutines. Assuming that the database 54 the researcher desires to access has been proximity indexed, the researcher must log on 260 to the database 54. By entering the appropriate information into the Computer Processor 30 via the input means, the researcher electronically accesses 264 the database 54 and enables the CSPDM 66 to search 200 the database 54.</p>
    <p>FIGS. 4A and 4B both show the preliminary options that the researcher can choose from before selecting one of the searching subroutines of the CSPDM 66. The CSPDM 66 questions the researcher on whether the researcher has identified a pool of textual objects 204. If the researcher has selected a pool of textual objects 204, then the researcher is able to choose one of the pool search 208 subroutines 212. If the researcher has not selected a pool of textual objects, the CSPDM 66 questions the researcher on whether the researcher has selected a single textual object 216. If the researcher has selected a single textual object 216, then the researcher is able to choose one 220 of the textual object searches 224. If the researcher has not selected either a pool of textual objects 204 or a single textual object 216, then the researcher must execute a Boolean Word Search or alternate Pool-Generation Method 228 to retrieve textual objects 268, 272.</p>
    <p>After CSPDM 66 subroutine has executed a particular search, the CSPDM 66 retrieves the appropriate data from the database 54, analyzes the data, and sends the data to the GUI Program 70 in order for the GUI Program 70 to display the results of the search on the display 38.</p>
    <p>FIG. 4B illustrates that after the CSPDM 66 has completed the above procedure, the researcher has the option to exit the CSPDM 66 by logging off, executing a search based on the results of a previous search, or executing a new search.</p>
    <p>FIGS. 4A and 4B also depict the seven subroutines of the CSPDM 66. There are three textual object search subroutines 224 and four pool search subroutines 212. The three textual object search subroutines 224 are: the Cases-In Subroutine 232, the Cases-After Subroutine 236 and the Cases-Similar Subroutine 240. The four pool search subroutines 212 are the Pool-Similarity Subroutine 244, the Pool-Paradigm Subroutine 248, the Pool-Importance Subroutine 252, and the Pool-Paradigm-Similarity Subroutine 256. Each of these subroutines are described in more detail in FIGS. 4C to 4I. The following is a step by step description of the subroutines 224, 212 of the CSPDM 66.</p>
    <p>Section A Cases-In Subroutine 232:</p>
    <p>FIG. 4C is a high level flow chart for the Cases-In Subroutine 232.</p>
    <p>1. The researcher must select a single textual object 400.</p>
    <p>2. The researcher selects the Cases-In Subroutine 232 option.</p>
    <p>3. The Cases-In Subroutine 232 examines the n×n Opinion Citation Matrix and other matrices 404 created by the Proximity Indexing Application Program 62 and retrieves the textual objects to which the selected textual object refers 408, data relating to the number of times the selected textual object refers to the retrieved textual objects, data relating to the importance of each textual object, and other relevant data.</p>
    <p>Section B Cases-After Subroutine 236:</p>
    <p>FIG. 4D is a high level flow chart for the Cases-After Subroutine 236.</p>
    <p>4. The researcher must select a single textual object 400.</p>
    <p>5. The researcher selects the Cases-After Subroutine 236 option.</p>
    <p>6. The Cases-After Subroutine 236 examines the n×n Opinion Citation Matrix and other matrices 412 created by the Proximity Indexing Application Program 62 and retrieves the textual objects that refer to the selected textual object 416, data relating to the number of times the retrieved textual objects refer to the selected textual object, data relating to the importance of each textual object, and other relevant data.</p>
    <p>Section C Similar-Cases Subroutine 240:</p>
    <p>FIG. 4E is a high level flow chart for the Similar-Cases Subroutine 240.</p>
    <p>7. The researcher must select a single textual object 400.</p>
    <p>8. The researcher selects the Similar-Cases Subroutine 240 option.</p>
    <p>9. The Similar-Cases Subroutine examines the q×q Section Similarity Matrix and other matrices 420 created by the Proximity Indexing Application Program 62 and retrieves the textual objects that are similar to the selected textual object 424, data relating to the degree of similarity between the selected textual object and the retrieved textual objects, data relating to the importance of each textual object, and other relevant data. In order to be retrieved, a textual object must have a similarity coefficient with respect to the selected textual object of at least a minimum value. The preferred embodiment sets the minimum similarity coefficient to four percent (4%).</p>
    <p>Section D Pool-Similarity Subroutine 244:</p>
    <p>FIG. 4F is a high level flow chart for the Pool-Similarity Subroutine 244.</p>
    <p>10. The researcher must select a pool of full textual objects 428.</p>
    <p>11. The researcher must then select a single full textual object 400 to compare with the pool of full textual objects. It should be noted that the researcher can select the single textual object from the selected pool of textual objects, or the researcher can select a textual object from outside of the pool 432.</p>
    <p>12. The Pool-Similarity Subroutine 244 examines the n×n Opinion Similarity Matrix and other matrices 436 and values created by the Proximity Indexing Application Program 62 for the selected full textual object and the pool of full textual objects.</p>
    <p>13. The Pool-Similarity Subroutine 244 determines the degree of similarity of other full textual objects in the pool to the selected full textual object 440.</p>
    <p>Section E Pool-Paradigm:</p>
    <p>FIG. 4G is a high level flow chart for the Pool-Paradigm Subroutine 248.</p>
    <p>14. The researcher must select a pool of full textual objects 428.</p>
    <p>15. The Pool-Paradigm Subroutine 248 examines the n×n Opinion Proximity Matrix, the n×n Opinion Similarity Matrix and other matrices and values created by the Proximity Indexing Application Program 62 for the pool of full textual objects 448.</p>
    <p>16. The Pool-Paradigm Subroutine 248 determines the Paradigm full textual object by calculating the mean of the Euclidean distances of all the textual objects in the pool 452.</p>
    <p>17. The Pool-Paradigm Subroutine 248 determines the similarity of the other full textual objects in the pool to the Paradigm full textual object 456.</p>
    <p>Section F Pool-Importance Subroutine 252:</p>
    <p>FIG. 4H is a high level flow chart for the Pool-Importance Subroutine 252.</p>
    <p>18. The researcher must select a pool of full textual objects.</p>
    <p>19. The Pool-Importance Subroutine 252 examines 448 the n×n Opinion Citation Matrix, the n×n Opinion Similarity Matrix, numerical factors and other matrices and values created by the Proximity Indexing Application Program 62 for the pool of full textual objects 460.</p>
    <p>20. The Pool-Importance Subroutine 252 then ranks the importance of each of the full textual objects in the pool 464.</p>
    <p>FIG. 4I is a high level flow chart showing two possible alternate Pool-Paradigm-Similarity Subroutines 256.</p>
    <p>Section G Pool-Paradigm-Similarity Subroutine 256 (Option 1) 256:</p>
    <p>21. The researcher must select a pool of k full textual objects where k equals the number of full textual objects in the pool 428.</p>
    <p>22. For each of the k full textual objects, the Pool-Paradigm-Similarity Subroutine 256 selects a n×1 vector from the corresponding column of the n×n matrix 468.</p>
    <p>23. The Pool-Paradigm-Similarity Subroutine 256 creates an n×k matrix by grouping the n×1 vector representing each of the k full textual objects beside each other.</p>
    <p>24. The Pool-Paradigm-Similarity Subroutine 256 calculates the mean of each row of the n×k matrix and enters the mean in the corresponding row of an n×1 Paradigm Proximity Vector 472.</p>
    <p>25. The Pool-Paradigm-Similarity Subroutine 256 combines the n×1 Paradigm Proximity Vector with the n×n Opinion Proximity Matrix to create an (n+1)×(n+1) Paradigm Proximity Matrix 476.</p>
    <p>26. From the (n+1)×(n+1) Paradigm Proximity Matrix, the Pool-Paradigm-Similarity Subroutine 256 evaluates the Euclidian distances and empirical data to create an (n+1)×(n+1) Paradigm Similarity Matrix 480.</p>
    <p>27. The Pool-Paradigm Similarity Subroutine searches the row in the (n+1)×(n+1) Paradigm Similarity Matrix that corresponds to the Paradigm full textual object and retrieves the full textual objects that have a maximum degree of similarity with the Paradigm lull textual object 500.</p>
    <p>Section H Pool-Paradigm-Similarity Subroutine 256 (Option 2):</p>
    <p>28. The researcher must select a pool of k full textual objects where k equals the number of full textual objects in the pool 428.</p>
    <p>29. For each of the k full textual objects, the Pool-Paradigm-Similarity Subroutine 256 selects an n×1 vector from the corresponding column of the n×n matrix 484.</p>
    <p>30. The Pool-Paradigm-Similarity Subroutine 256 creates an n×k matrix by grouping the n×1 vector for each of the k full textual objects beside each other.</p>
    <p>31. The Pool-Paradigm-Similarity Subroutine 256 calculates the mean of each row of the n×k matrix and enters the mean in the corresponding row of an n×1 Paradigm Pattern Vector PF 488.</p>
    <p>32. The Pool-Paradigm-Similarity Subroutine 256 combines the n×1 Paradigm Pattern Vector PF with the n×n Opinion Pattern Matrix to create a (n+1)×(n+1) Paradigm Pattern Matrix 492.</p>
    <p>33. From the (n+1)×(n+1) Paradigm Pattern Matrix, the Pool-Paradigm-Similarity Subroutine 256 evaluates the Euclidean distances between the rows of the Paradigm Pattern Matrix and creates an (n+1)×(n+1) Paradigm Proximity Matrix 496.</p>
    <p>34. From the (n+1)×(n+1) Proximity Matrix, the Pool-Paradigm-Similarity Subroutine 256 evaluates the Euclidean distances between the rows of the (n×1)×(n×1) Paradigm Proximity Matrix and empirical data to create an (n+1)×(n+1) Paradigm Similarity Matrix 480.</p>
    <p>35. The Pool-Paradigm Similarity Subroutine searches the row in the (n+1)×(n+1) Paradigm Similarity Matrix that corresponds to the Paradigm full textual object and retrieves the full textual objects that have a minimum degree of similarity with the Paradigm full textual object 500.</p>
    <p>Application of the Proximity Indexing Technique.</p>
    <p>The above Proximity Indexing Application Program 62 and CSPDM 66 have a number of different applications and versions. Three of the most useful applications are described below.</p>
    <p>The first type of Proximity Indexing Application Program is for use on very large databases. The matrices generated by this type of Proximity Indexer are "attached" to the database 54, along with certain clustering information, so that the database 54 can be searched and accessed using the Cases-In Subroutine 232, Cases-After Subroutine 236, Cases-Similarity Subroutine, Pool-Similarity Subroutine 244, Pool-Paradigm Subroutine 248, Pool-Importance Subroutine 252 and Pool-Paradigm-Similarity Subroutine 256 of the CSPDM 66.</p>
    <p>The second type of Proximity Indexing Application Program 62 is a Proximity Indexer that law firms, businesses, government agencies, etc. can use to Proximity Index their own documents in their own databases. The researcher can navigate through the small business's preexisting database 54 using the Cases-In Subroutine 232, Cases-After Subroutine 236, Cases-Similarity Subroutine, Pool-Similarity Subroutine 244, Pool-Paradigm Subroutine 248, Pool-Importance Subroutine 252 and Pool-Paradigm-Similarity Subroutine 256 of the CSPDM 66. In addition, this type of Proximity Indexer Application Program will be designed to be compatible with the commercial third-party databases which are Proximity Indexed using the first type of program. In other words, the researcher in a small business may "weave" in-house documents into a commercial database 54 provided by a third party, so that searches in the large database 54 will automatically bring up any relevant in-house documents, and vice versa.</p>
    <p>The third type of Proximity Indexing Applications Program involves the capacity to do Proximity indexing of shapes. Each image or diagram will be treated as a "textual object." The various matrix coefficients can be generated purely from topological analysis of the object itself, or from accompanying textual information about the object, or from a weighted combination of the two. The text is analyzed using the Proximity Indexing Application Program 62 as explained above. Shapes are analyzed according to a coordinate mapping procedure similar to that used in Optical Character Recognition ("OCR"). The numerical "maps" resulting from scanning the images are treated as "textual objects" that can be compared through an analogous weighing algorithm to generate a proximity matrix for every ordered pair of "textual objects" in the database 54. A similarity matrix can then be generated for each ordered pair, and the results organized analogous to a database 54 totally comprised of actual text.</p>
    <p>This third type of Proximity indexing applications program can provide "Proximity Indexed" organization access to many different types of objects. For example, it can be used to search patent diagrams, or compare line drawings of known pottery to a newly discovered archeological find. It can be used to scan through and compare police composite drawings, while simultaneously scanning for similar partial descriptions of suspects. It can be used to locate diagrams of molecular structures, appraise furniture by comparing a new item to a database 54 of past sales, identify biological specimens, etc., etc.</p>
    <p>FIG. 5A is a high level drawing that depicts the GUI Program 70 and its interaction with both the CSPDM 66 and the display 38. The GUI Program 70 has one or more display subroutines. The preferred embodiment contains seven display subroutines. The seven subroutines comprise three textual object display subroutines 504 and four pool display subroutines 508. The three textual object display subroutines 504 are the Cases-In Display Subroutine (CIDS) 512, the Cases-After Display Subroutine (CADS) 516 and the Similar-Cases Display Subroutine (SCDS) 520. The four pool display subroutines 508 are the Pool-Similarity Display Subroutine (PSDS) 524, the Pool-Paradigm Display Subroutine (PPDS) 528, the Pool-Importance Display Subroutine (PIDS) 532 and the Pool-Paradigm-Similarity Display Subroutine (PPSDS) 536. The three textual object display subroutines receive data from the corresponding textual object search subroutine of the CSPDM 66. Similarly, the four pool display subroutines 508 receive data from the corresponding pool search subroutine 212 of the CSPDM 66. Once the display subroutines have processed the data received by the search subroutines, the data is sent to the integrator 540. The integrator 540 prepares the data to be displayed in the proper format on the display 38.</p>
    <p>FIGS. 5B through 5H depict screens generated by the textual object display subroutines, CIDS 512, CADS 516 and SCDS 520. The three types of screens are the Cases In screen 1000, the Cases After screen 1004 and the Similarity Screen 1008, respectively. The Similarity Screen 1008 provides the most "intelligent" information, but all three screens generated by the textual object display subroutines work in tandem as a system. The other screens created by the pool display subroutines are variances of these three, and also work in tandem with each other and with the three textual object display screens.</p>
    <p>FIG. 5B depicts the "Cases After" 1004 Screen created by the CADS 516 for the textual object, Terry v. Ohio, 392 U.S. 1 (1968). The "Cases After" search produces all of the textual objects in the designated field (here D.C. Circuit criminal cases since 1990) that cite Terry. The number "12" in the upper left hand corner indicates that there are a total of 12 such textual objects. The vertical axis 1012 indicates the degree to which a given textual object relied upon Terry. The number "10" immediately below the 12 indicates that the textual object in the field which most relied upon Terry, namely U.S. v. Tavolacci, 895 F.2d 1423 (D.C. Cir. 1990), discusses or refers to Terry in ten of its paragraphs.</p>
    <p>The Tear-Off Window 1016 feature is illustrated in FIG. 5B by the Tear-Off Window 1016 for U.S. V. McCrory, 930 F.2d 63 (D.C. Cir. 1991). The four Tear-Off Window active boxes 1020 (displayed on the Tear-Off Window 1016): 1) open up the full text 1104 of McCrory to the first paragraph that cites Terry; 2) run any of the three searches, namely Cases-In Subroutine 232 Cases-After Subroutine 236 or Cases-Similar Subroutine 240 for McCrory itself (the default is to run the same type of search, namely Cases-After Subroutine 236 again); 3) hide the Terry execute search window 1024; and 4) bring the Terry Execute Search window 1024 to the foreground, respectively. The weight numeral 1028 indicates the number of paragraphs in McCrory that discusses or refers to Terry, in this textual object (in this example there is only one).</p>
    <p>The "Cases After" screen 1004 for a given Textual object B displays a Textual Object Active Box 1032 representing every subsequent textual object in the database 54 that refers explicitly to Textual object B. The analysis starts with the same pool of material as a Shepards™ list for Textual object B. As well as some additional material not gathered by Shepards. However, the "Cases After" screen 1004 conveys a wealth of information not conveyed by a Shepards™ list.</p>
    <p>The horizontal axis 1036 may represent time, importance or any other means of measurement to rank the textual objects. In the preferred embodiment, the horizontal axis 1036 represents time. The Shepards list itself contains no information as to when a case was decided. The vertical axis 1012 similarly may represent any means of measurement to rank the textual objects. In the preferred embodiment, the vertical axis 1012 represents the degree to which the subsequent Textual object C relied upon the original Textual object B. The display 38 makes it obvious when a textual object has received extensive discussion in another textual object, or provides key precedent for a subsequent textual object, or merely mentions the earlier textual object in passing. It also provides guidance as to possible gradations in between extensive, or merely citing.</p>
    <p>The "shape" of the overall pattern of active boxes on the "Cases After" screen 1004 provides a rich lode of information to be investigated. For example, a "dip" in citation frequency immediately after a particular textual object suggests that the particular textual object, while not formally overruling Textual object B, has largely superseded it. A sudden surge in citation frequency after a particular Supreme Court case may indicate that the Supreme Court has "picked up" and adopted the doctrine first enunciated in Textual object B. The researcher can instantly determine if the holding of Textual object B has been adopted in some circuits but not in others, if Textual object B is losing strength as a source of controlling precedent, etc. None of this information is now available to lawyers in graphical or any other form.</p>
    <p>As with the "Cases In" screen 1000, every Textual Object Active Box 1032 on the "Cases After" screen is active, and includes a Tear-Off Window 1016 that may be moved by dragging on the tear-off window 1016 with a mouse 42, and that tear-off window 1016 becomes a text Tear-Off Window 1040, visible even when one moves on to other searches and other screens. Thus one may "tear off" for later examination every relevant citation to Textual object B, or even for a group of textual objects. The text tear-off windows "tile"; that is, they can be stacked on top of one another to take up less room. There is also a "Select All" feature (not shown), that creates a file containing the citations of every textual object retrieved in a given search.</p>
    <p>In "Cases After" mode, clicking on the expanded-view button 1044 of the text tear-off window 1040 opens the text of the subsequent Textual object C to the first place where Textual object B is cited. A paragraph window 1048 displays a paragraph selection box 1052 indicating what paragraph in Textual object C the researcher is reading, and a total paragraph box 1056 indication how many paragraphs Textual object C contains in total. The user can view paragraphs sequentially simply by scrolling through them, or see any paragraph immediately by typing its number in the paragraph selection box 1052. Clicking on a Next paragraph active box 1060 immediately takes the researcher to the next paragraph in Textual object C where Textual object B is mentioned. Traditional Shepardizing allows the researcher to explore the subsequent application of a doctrine in a range of different factual situations, situations that help to define the outer contours of the applicability of a rule. Combining the expanded-view button functions 1044 and "Next Paragraph" active box 1060 functions allows the researcher to study how Textual object B has been used in all subsequent textual objects, in a fraction of the time the same task currently requires with available searching methods.</p>
    <p>Perhaps the most fundamental form of legal research is "Shepardizing." A researcher starts with a textual object known to be relevant, "Textual object B," and locates the "Shepards" for that textual object. The "Shepards" is a list of every subsequent textual object that explicitly refers to Textual object B. The researcher then looks at every single textual object on the list. Shepardizing is often painstaking work. Many subsequent references are made in passing and have almost no legal significance. Although Shepards includes some codes next to its long lists of citations, such as "f" for "followed" and "o" for "overruled," the experience of most lawyers is that such letters cannot be relied upon. For example, the researcher may be citing Textual object B for a different holding than that recognized by the anonymous Shepards reader, interpreting Textual object B differently, or interpreting the subsequent textual object differently. However, for really thorough research, checking a Shepards type of list is essential. The researcher must make absolutely sure that any textual object cited as legal authority in a brief, for instance, has not been superseded by later changes in the law.</p>
    <p>Very often, textual objects located on the Shepards list for Textual object B refer back to other important textual objects, some of which may predate Textual object B, all of which may be Shepardized in turn. This "zig-zag" method of research is widely recognized as the only way to be sure that one has considered the full line of textual objects developing and interpreting a doctrine. The real power of the "Cases After" screen 1004 emerges when it is used in conjunction with the "Cases In" screens 1000 and "Similarity" screens 1008. Using the preferred embodiment, the researcher may engage in the same kind of careful "zig-zag" study of a legal doctrine in a much more efficient manner.</p>
    <p>For example, consider the following hypothetical search. The researcher reads Textual object B, and makes a list of every Supreme Court textual object it substantially relies upon, perhaps six textual objects. The researcher then Shepardizes Textual object B and reads each of those textual objects, in order to find other Supreme Court textual objects that they relied upon, perhaps eight. One then Shepardizes those fourteen Supreme Court decisions, in order to find any Court of Appeals cases in a selected circuit 1096 within the last three years on the same basic topic. This process would take at least an hour, even using Shepards through an on-line service. The same search can be performed with the present invention using the "Cases In" screens 1000 and "Cases After" screens 1004 in under five minutes.</p>
    <p>In order to perform the same search, a researcher can pull up both the "Cases In" screens 1000 and "Cases After" screens 1004 for Textual object B simultaneously. The researcher can then "tear-off" all of the Supreme Court Cases on both lists, run Cases-After Subroutine 236 searches on every Supreme Court Case mentioned on either list, then examine the "Cases In" screens 1000 for all of the Supreme Court cases produced by these searches. The researcher can locate every recent Court of Appeals case from one's circuit 1096 mentioned in any of those Supreme Court cases. Use of the "Similarity" screen 1008 as well, allows the researcher to find the pool of relevant Court of Appeals full textual objects even faster.</p>
    <p>FIG. 5C depicts the "Cases After" Screen 1004 for U.S. v. Lam Kwong-Wah, 924 F.2d 298 (D.C. Cir. 1991). FIG. 5C shows a text Tear-Off Window 1040 on a "Cases After" Screen 1004, (in this textual object the Tear-Off Window 1016 for U.S. v. Barry, 938 F.2d 1327 (D.C. Cir. 1991), is opened using the full text active box 1064. A text Tear-Off Window 1040 containing the text of Barry opens, to the first cite of U.S. v. Lam Kwong-Wah at paragraph 15. Clicking on the "Next Paragraph" active box 1060 will open the text of Barry to the next paragraph that cites Lam Kwong-Wah.</p>
    <p>The number "34" in the lower-left corner of the total paragraph box 1056 indicates that Barry has a total of 34 paragraphs in the cite U.S. v. Lam Kwong-Wah. Dragging the small squares 1068 to the left and below the text allow the researcher to move within a paragraph, and from paragraph to paragraph, in the text of Barry, respectively. The empty space below the text 1072 would contain the text of any footnote in paragraph 15. The compress window active box now closes the window and replaces it with the corresponding active box.</p>
    <p>FIG. 5D depicts the "Cases In" Screen 1000 for U.S. v. North, 910 F.2d 843 (D.C. Cir. 1990). FIG. 5D contains a Textual Object Active Box 1032 representing every textual object with persuasive authority, cited in the text of North. The vertical axis 1012 represents the degree to which North relied upon a given textual object. In this example it is immediately apparent that Kastigar v. United States, 406 U.S. 441 (1972) is the most important precedent, and its Tear-Off Window 1016 has been activated. The weight numeral 1028 indicates that Kastigar is referred to in seventy-seven (77) paragraphs of North.</p>
    <p>A highlighted Textual Object Active Box 1076 can be created by clicking on it, as has been done with U.S. v. Mariana, 851 F.2d 595 (D.C. Cir. 1988). The number "212" in the case number box 1080 indicates that citations to two-hundred-twelve distinct texts appear in North. Fewer are visible because the textual object active boxes 1032 "tile" on top of one another; the "Zoom" feature is used to focus on a smaller area of the screen, and ultimately resolves down to a day-by-day level, making all the textual object active boxes 1032 visible.</p>
    <p>The unique "Cases In" screen 1000 provides a schematic representation of the precedent from which Textual object A is built. The "Cases In" screen 1000 contains a textual object "active box" 1032 representing every textual object which is relied upon, or even mentioned, in Textual object A. Any citation in textual object A to a textual object that possesses potential persuasive authority, whether a statute, constitutional provision, treatise, scholarly article, Rule of Procedure, etc., is treated as a "textual object." The textual object active boxes 1032 are color-coded to indicate the court or other source of each textual object. Supreme Court cases are red, Court of Appeals cases are green, District Court cases are blue, and statutes are purple, for example. Each Textual Object Active Box 1032 contains the full official citation 1084 of its textual object. Clicking on any Textual Object Active Box 1032 immediately pulls up a larger window, known as a "tear-off" window, also containing the full citation 1084 to the textual object (Tear-Off Window Citation 1088), its date 1092, its circuit 1096, and its weight numeral 1028 to the textual object being analyzed. The user may then drag the Tear-Off Window 1016 free of the Textual Object Active Box 1032 and release it.</p>
    <p>This creates a text Tear-Off Window 1040 that remains visible until the researcher chooses to close it, no matter how many subsequent screens the researcher examines. The text Tear-Off Window 1040 can be moved anywhere by dragging it with the mouse 42. The text Tear-Off Window 1040 contains small text active boxes 1100 allowing the researcher to access or "pull up" the full text 1104 of the textual object it represents with a single click of the mouse 42. This feature also allows the researcher to run Cases-In Subroutine 232 Cases-After Subroutine 236 and Cases-Similar Subroutine 240 searches on the textual object. (See below for a description of the "Similarity" screen).</p>
    <p>The organization of the boxes on the screen, including their position on the horizontal axis 1036 and vertical axis 1012, represents the real "intelligence" behind the Cases-In screen 1000. The horizontal axis 1036 in the preferred embodiment represents time, with the left margin 1108 corresponding to the present, i.e., the date 1092 when the search is run. The right margin 1112 represents the date 1092 of decision of the earliest textual object cited in Textual object A. (Certain special materials, such as treatises updated annually, and the U.S. Constitution, are located in a column 1116 to the left of the margin.)</p>
    <p>The vertical axis 1012 in the preferred embodiment represents the degree to which Textual object A relied upon each particular textual object it contains. For example, if the Cases In screen 1000 is run on a district court case (Textual object A) which happens to be a "stop and search" textual object that mainly relies upon Terry v. Ohio, 392 U.S. 1 (1968), Terry will be at the top of the screen, with all other textual object active box 1032 appearing far below. The researcher can thus access the text of Terry directly without ever reading the text of Textual object A. Of course, the full text 1104 of Textual object A is also instantly available if desired. If the researcher wants to see where Terry "came from," the researchers can instantly, by clicking on a textual active box 1100 within the Terry text Tear-Off Window 1040, run the Cases-In Subroutine 232 for Terry--and so on. There is no limit to the number of "levels" or "generations" the researchers may explore using this technique. It is therefore possible (assuming a sufficient database 54) to find, in a matter of seconds, without having to read through layers of texts, the possibly long-forgotten eighteenth-century precursors to a modern doctrine.</p>
    <p>The "Cases In" screen 1000 creates an instant visual summary or "blueprint" of a textual object. The blueprint can help a researcher make a preliminary judgment about whether a particular textual object is worth closer examination. Viewing the "Cases In" screens 1000 for a group of textual objects allows a researcher to recognize whether there are precedents common to that group. The blueprint tells the researcher whether Textual object A is primarily a statutory construction case, a textual object that relies on local Court of Appeals cases without Supreme Court support, a textual object relying on precedent outside the circuit 1096 as persuasive authority, etc.</p>
    <p>The initial "Cases In" screen 1000 presents every citation within a given textual object. In a textual object with an unusually large number of citations, the screen will be crowded with textual object active boxes 1032. The GUI therefore contains a "zoom" feature that allows the researcher to expand any small portion of the screen. To get back to the "big picture," the researcher simply selects the "Fit in Window" menu item, or else selects the "zoom out" feature. The same "zoom," "zoom out," and "Fit in Window" functions are present in the "Cases After" screen 1004 and "Similarity" screen 1008 as well.</p>
    <p>The routine that calculates "degree to which Textual object A relies upon the cited textual object" clearly ranks major textual objects at the top, textual objects mentioned only in passing at the bottom, and textual objects of potentially greater relevance in between. In addition, the routine can recognize when a highly relevant textual object is mentioned only in passing and give a higher weight to that textual object than it would otherwise receive in the ranking procedure.</p>
    <p>The "intelligence" behind the entire GUI is driven by the knowledge that the lawyers do not want the computer to do legal analysis or make judgments for them, but simply guide them through the great mass of irrelevant material to those texts where lawyerly analysis of a problem begins.</p>
    <p>The "Cases In" screen 10004 is designed with practical legal research in mind. It is common in legal research to locate a lower court textual object on the correct topic, call it "local Textual object A." However, the researcher desired to find the most persuasive authority available. The aim of this type of research is to find the "lead" textual object or textual objects on a particular topic. The researcher ultimately desires the first textual object, most famous textual object, and most recent textual objects of the Supreme Court (or state Supreme Court in state law issues) that stand for the same principle. ("Lead" textual objects also occur at the intermediate and trial court level.)</p>
    <p>The standard way to find lead textual objects is to read through the text of a local Textual object A until one finds references to "higher court textual objects," then look up each of those higher court textual objects in turn. The researcher then reads the text of those textual objects until the researcher determines the textual objects they have in common, the textual objects that appear many times. Very often, the lower court textual object from which the researcher started is of no real value in and of itself--it may well be from a different local jurisdiction--and the researcher reads through it only to find citations within it. Since the GUI quickly locates and schematically diagrams the textual objects, this process is accelerated dramatically using the GUI.</p>
    <p>FIGS. 5E through 5G depict multiple "Similarity" searches run in sequence. A "Similarity" Screen for U.S. v. Caballero, 936 F.2d 1292 (D.C. Cir. 1991), reveals via the case number box 1080, that 17 textual objects were retrieved by a "similarity" search. The vertical axis 1012 indicates that the textual objects retrieved had similarity coefficients 1120 between 4% and 15% with respect to U.S. v. Caballero. Cases with less than 4% similarity are not shown. The vertical axis 1012 represents degree of similarity, or topical relatedness, so that 100% would be two identical texts. The Tear-Off Window 1016 of U.S. v. Nurse, 916 F.2d 20 (D.C. Cir. 1990) shows that the textual object has a similarity of 9%.</p>
    <p>The "Similarity" screen for a given Textual object C is organized like the "Cases In" and "Cases After" screens, with the same color-coded textual object active boxes representing textual objects, and time on the horizontal axis 1036. However, the vertical axis 1012 represents the degree to which the represented textual object is related to Textual object C. The system is built on the principle that legal doctrines tend to emerge out of lines of textual objects developing a legal principle. Lines of textual objects contain "lead" textual objects that establish basic rules and subsequent textual objects that do not establish new rules, but apply and re-interpret the pre-existing rules in various circumstances. Some lead textual objects invent new doctrines, while others modify or redirect the law based on earlier precedent.</p>
    <p>The routine that operates behind the "Similarity" screen determines which line or lines of textual objects that Textual object C can be grouped. The routine then ranks the textual objects in that line depending on how closely they are related to Textual object C. For example, a typical similarity search starting with a Court of Appeals case in a certain circuit, Textual object D, will find the Supreme Court and Court of Appeals cases that have established the principles followed in Textual object D. The Supreme Court and Court of Appeals case will appear as textual object active boxes whether or not they are cited in Textual object D. Furthermore, the similarity search will find the textual objects decided subsequent to Textual object D that have applied, and possibly modified, those principles, whether or not those textual objects cite Textual object D.</p>
    <p>Similarity searches allow a researcher to find textual objects on the same topic that do not share common phrases and might be overlooked by a Boolean word search. Similarity searches also allow researchers, who only have an obscure district court case, to "tap in" to the lead textual objects in any area. By organizing all case law in "conceptual space," the Similarity screens allow one to locate emerging topics that have not been formally recognized by those assigning "key numbers" or otherwise manually classifying textual objects--or even by the authors of the textual objects themselves.</p>
    <p>The "shape" of a Similarity Screen 1008 may convey a great deal of information about a particular legal concept. For example, the screen conveys to the researcher whether a certain concept, which is essentially novel, is supported by Supreme Court case law or is an old doctrine that has been recently applied in a new context. The system as a whole gives lawyers the ability to assess what textual objects are "available" on their topic, and to zero in on the textual objects that are most useful. The researcher has the ability to track down every subsequent reference to any particular textual objects by utilizing multiple "Cases After" searches, identifying core precedents through "Cases In" searches, and by running new "Similarity" searches to obtain any textual objects that emerge in closely related topic areas. The "Similarity" algorithm is more "aggressive" then the others, since it contains built-in judgments as to what "relatedness" means. It also judges what is no longer sufficient to display on the screen. The bottom edge of the screen represents a minimum degree of similarity below which the connections are too tenuous to be worth pursuing. In the commercial product, this minimum level can be reset at the preference of the user.</p>
    <p>FIG. 5F:</p>
    <p>FIG. 5F is the "Similarity Screen 1008" for U.S. v. Nurse. Clicking on the "run search" Tear-Off Window active box 1020, which is on the Tear-Off Window 1016 for Nurse produces FIG. 5F. Clicking on the Textual Object Active Box 1032 for U.S. v. Jordan, 951 F.2d 1278 (D.C. Cir. 1991) long enough to pull up its Tear-Off Window 1016, and then clicking on Jordan's "run search" Tear-Off Window active box 1020 (not shown), produces the Similarity Screen 1008 shown in FIG. 5G.</p>
    <p>FIG. 5G:</p>
    <p>FIG. 5G shows how multiple tear-off windows can be shown at the same time, here the U.S. v. Jordan similarity Tear-Off Window 1016 depicts for the three textual objects most similar to Jordan. Note that U.S. v. Jordan, 958 F.2d 1085 (D.C. Cir. 1992), is very closely related, i.e., 41%, to U.S. v. Jordan, 951 F.2d 1278 (D.C. Cir. 1991), apparently as it is a subsequent full textual object decision of the same dispute as the first textual object.</p>
    <p>FIG. 5H:</p>
    <p>FIG. 5H depicts a close-up view of an Execute Search Window. The researcher can input a selected textual object that is either represented or not represented on a display 38 screen as a Textual Object Active Box 1032. The researcher can title his search by inputing the title in the Search Title box. The researcher can then input the reference to the selected textual object in the reference input boxes. The reference input boxes of the preferred embodiment allow the researcher to refer to the selected textual object by Volume, Category, Page and/or Section by inputing the appropriate values in the volume reference box, category reference box, page reference box, and/or section reference box, respectively.</p>
    <p>The researcher can also identify the type of search to be performed on the selected textual object by selecting the appropriate search in the Analysis box.</p>
    <p>Once the researcher has inputed all the appropriate values, the researcher executes the search by activating the execute search button.</p>
    <p>The PSDS 524, PPDS 528, PIDS 532 and PPSDS 536 of the GUI Program 70, also create similar displays to the CIDS 512, CADS 516, and SCDS 520 subroutines. The only major difference between the screens created by the three textual object display subroutines and the four pool display subroutines is the information contained in the Execute Search window and the options available in the analysis box.</p>
    <p>The options in the analysis box enable a researcher to select a textual object outside the pool of textual objects and compare how the selected textual object relates to the pool of textual objects by selecting to the Pool-Similarity Subroutine 44, the Pool-Paradigm Subroutine 248 or Pool-Importance Subroutine 252 of the CSPDM 66.</p>
    <p>The PSDS 524 creates a Pool-Similarity Screen 1008. The vertical axis 1012 ranks the similarity of the objects in a pool of textual objects with respect to a selected textual object. All of the other aspects of this display 38 are similar to the Similar Cases Screen.</p>
    <p>PPDS 528 creates a Pool-Paradigm Screen. The vertical axis 1012 ranks the similarity of the pool of textual objects on the screen with respect to the paradigm textual object. The paradigm textual object is calculated by averaging the mean of all the Euclidean distances of the pool of textual objects on the screen. All of the other aspects of this display 38 are similar to the Similar-Cases Screen.</p>
    <p>The PIDS 532 creates a Pool-Importance Screen. The vertical axis 1012 ranks the importance of the pool of textual objects on the screen. All other aspects of the PIDS 532 display 38 are similar to the Cases-In Screen 1000 and Cases-After Screen 1004.</p>
    <p>The PPSDS 536 creates a Pool-Paradigm Similarity Screen 1008. The vertical axis 1012 represents the similarity of all textual objects in the database 54 to the paradigm textual object created by a selected pool of textual objects. All other aspects of the PPSDS 536 display 38 are similar to Similar-Cases Screen.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4839853">US4839853</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 15, 1988</td><td class="patent-data-table-td patent-date-value">Jun 13, 1989</td><td class="patent-data-table-td ">Bell Communications Research, Inc.</td><td class="patent-data-table-td ">Computer information retrieval using latent semantic structure</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4945476">US4945476</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 26, 1988</td><td class="patent-data-table-td patent-date-value">Jul 31, 1990</td><td class="patent-data-table-td ">Elsevier Science Publishing Company, Inc.</td><td class="patent-data-table-td ">Interactive system and method for creating and editing a knowledge base for use as a computerized aid to the cognitive process of diagnosis</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5122951">US5122951</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 11, 1990</td><td class="patent-data-table-td patent-date-value">Jun 16, 1992</td><td class="patent-data-table-td ">Sharp Kabushiki Kaisha</td><td class="patent-data-table-td ">Subject and word associating devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5157783">US5157783</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 26, 1988</td><td class="patent-data-table-td patent-date-value">Oct 20, 1992</td><td class="patent-data-table-td ">Wang Laboratories, Inc.</td><td class="patent-data-table-td ">Data base system which maintains project query list, desktop list and status of multiple ongoing research projects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5206949">US5206949</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 7, 1989</td><td class="patent-data-table-td patent-date-value">Apr 27, 1993</td><td class="patent-data-table-td ">Nancy P. Cochran</td><td class="patent-data-table-td ">Database search and record retrieval system which continuously displays category names during scrolling and selection of individually displayed search terms</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5241671">US5241671</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 26, 1989</td><td class="patent-data-table-td patent-date-value">Aug 31, 1993</td><td class="patent-data-table-td ">Encyclopaedia Britannica, Inc.</td><td class="patent-data-table-td ">Multimedia search system using a plurality of entry path means which indicate interrelatedness of information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5243655">US5243655</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 16, 1992</td><td class="patent-data-table-td patent-date-value">Sep 7, 1993</td><td class="patent-data-table-td ">Symbol Technologies Inc.</td><td class="patent-data-table-td ">System for encoding and decoding data in machine readable graphic form</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5301109">US5301109</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 17, 1991</td><td class="patent-data-table-td patent-date-value">Apr 5, 1994</td><td class="patent-data-table-td ">Bell Communications Research, Inc.</td><td class="patent-data-table-td ">Computerized cross-language document retrieval using latent semantic indexing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5325298">US5325298</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 3, 1991</td><td class="patent-data-table-td patent-date-value">Jun 28, 1994</td><td class="patent-data-table-td ">Hnc, Inc.</td><td class="patent-data-table-td ">Methods for generating or revising context vectors for a plurality of word stems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5418948">US5418948</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 8, 1993</td><td class="patent-data-table-td patent-date-value">May 23, 1995</td><td class="patent-data-table-td ">West Publishing Company</td><td class="patent-data-table-td ">Concept matching of natural language queries with a database of document concepts</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Agosti, et al., "<a href='http://scholar.google.com/scholar?q="A+Two-Level+Hypertext+Retrieval+Model+for+Legal+Data%2C"'>A Two-Level Hypertext Retrieval Model for Legal Data,</a>" SIGIR '91 (1991).</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Agosti, et al., A Two Level Hypertext Retrieval Model for Legal Data, SIGIR 91 (1991).</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Belew, Richard, "<a href='http://scholar.google.com/scholar?q="A+Connectionist+Approach+to+Conceptual+Information+Retrieval%2C"'>A Connectionist Approach to Conceptual Information Retrieval,</a>" ICAIL '87 (1987).</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Belew, Richard, A Connectionist Approach to Conceptual Information Retrieval, ICAIL 87 (1987).</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Fowler, et al., "<a href='http://scholar.google.com/scholar?q="Integrating+Query%2C+Thesaurus+and+Documents+Through+a+Commn+Visual+Representation%2C"'>Integrating Query, Thesaurus and Documents Through a Commn Visual Representation,</a>" SIGIR '91 (1991).</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Fowler, et al., Integrating Query, Thesaurus and Documents Through a Commn Visual Representation, SIGIR 91 (1991).</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Gelbart &amp; Smith, "<a href='http://scholar.google.com/scholar?q="Beyond+Boolean+Search%3A+FLEXICON%2C+A+Legal+Text-Based+Intelligent+System%2C"'>Beyond Boolean Search: FLEXICON, A Legal Text-Based Intelligent System,</a>" ICAIL '91 (1991).</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Gelbart &amp; Smith, Beyond Boolean Search: FLEXICON, A Legal Text Based Intelligent System, ICAIL 91 (1991).</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Lin, "<a href='http://scholar.google.com/scholar?q="A+Self-Organizing+Semantic+Map+for+Information+Retrieval%2C"'>A Self-Organizing Semantic Map for Information Retrieval,</a>" SIGIR '91 (1991).</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Lin, A Self Organizing Semantic Map for Information Retrieval, SIGIR 91 (1991).</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Rose &amp; Belew, "<a href='http://scholar.google.com/scholar?q="Legal+Information+Retrieval%3A+a+Hybrid+Approach%2C"'>Legal Information Retrieval: a Hybrid Approach,</a>" ICAIL '89 (1989).</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Rose &amp; Belew, Legal Information Retrieval: a Hybrid Approach, ICAIL 89 (1989).</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Turtle &amp; Croft, "<a href='http://scholar.google.com/scholar?q="Inference+Networks+for+Document+Retrieval%2C"'>Inference Networks for Document Retrieval,</a>" SIGR '90 (1990).</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Turtle &amp; Croft, Inference Networks for Document Retrieval, SIGR 90 (1990).</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5642502">US5642502</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 6, 1994</td><td class="patent-data-table-td patent-date-value">Jun 24, 1997</td><td class="patent-data-table-td ">University Of Central Florida</td><td class="patent-data-table-td ">Method and system for searching for relevant documents from a text database collection, using statistical ranking, relevancy feedback and small pieces of text</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5659732">US5659732</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 17, 1995</td><td class="patent-data-table-td patent-date-value">Aug 19, 1997</td><td class="patent-data-table-td ">Infoseek Corporation</td><td class="patent-data-table-td ">Document retrieval over networks wherein ranking and relevance scores are computed at the client for multiple database documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5664174">US5664174</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 9, 1995</td><td class="patent-data-table-td patent-date-value">Sep 2, 1997</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and method for discovering similar time sequences in databases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5684506">US5684506</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 20, 1995</td><td class="patent-data-table-td patent-date-value">Nov 4, 1997</td><td class="patent-data-table-td ">Sharper Image Corporation</td><td class="patent-data-table-td ">Digital recorder apparatus with graphical display representation and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5715445">US5715445</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 7, 1995</td><td class="patent-data-table-td patent-date-value">Feb 3, 1998</td><td class="patent-data-table-td ">Wolfe; Mark A.</td><td class="patent-data-table-td ">System for retrieving information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5717913">US5717913</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 3, 1995</td><td class="patent-data-table-td patent-date-value">Feb 10, 1998</td><td class="patent-data-table-td ">University Of Central Florida</td><td class="patent-data-table-td ">Method for detecting and extracting text data using database schemas</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5717914">US5717914</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 15, 1995</td><td class="patent-data-table-td patent-date-value">Feb 10, 1998</td><td class="patent-data-table-td ">Infonautics Corporation</td><td class="patent-data-table-td ">Method for categorizing documents into subjects using relevance normalization for documents retrieved from an information retrieval system in response to a query</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5724571">US5724571</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 7, 1995</td><td class="patent-data-table-td patent-date-value">Mar 3, 1998</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Method and apparatus for generating query responses in a computer-based document retrieval system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5727199">US5727199</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 13, 1995</td><td class="patent-data-table-td patent-date-value">Mar 10, 1998</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Computer-implemented method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5729741">US5729741</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 10, 1995</td><td class="patent-data-table-td patent-date-value">Mar 17, 1998</td><td class="patent-data-table-td ">Golden Enterprises, Inc.</td><td class="patent-data-table-td ">System for storage and retrieval of diverse types of information obtained from different media sources which includes video, audio, and text transcriptions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5761497">US5761497</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 7, 1995</td><td class="patent-data-table-td patent-date-value">Jun 2, 1998</td><td class="patent-data-table-td ">Reed Elsevier, Inc.</td><td class="patent-data-table-td ">Associative text search and retrieval system that calculates ranking scores and window scores</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5765149">US5765149</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 9, 1996</td><td class="patent-data-table-td patent-date-value">Jun 9, 1998</td><td class="patent-data-table-td ">Digital Equipment Corporation</td><td class="patent-data-table-td ">Modified collection frequency ranking method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5765150">US5765150</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 9, 1996</td><td class="patent-data-table-td patent-date-value">Jun 9, 1998</td><td class="patent-data-table-td ">Digital Equipment Corporation</td><td class="patent-data-table-td ">Computer implemented method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5771378">US5771378</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 7, 1995</td><td class="patent-data-table-td patent-date-value">Jun 23, 1998</td><td class="patent-data-table-td ">Reed Elsevier, Inc.</td><td class="patent-data-table-td ">Associative text search and retrieval system having a table indicating word position in phrases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5778361">US5778361</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 29, 1995</td><td class="patent-data-table-td patent-date-value">Jul 7, 1998</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for fast indexing and searching of text in compound-word languages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5781899">US5781899</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 26, 1995</td><td class="patent-data-table-td patent-date-value">Jul 14, 1998</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Image index production method and image index production system for image storage and management system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5794237">US5794237</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 3, 1997</td><td class="patent-data-table-td patent-date-value">Aug 11, 1998</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and method for improving problem source identification in computer systems employing relevance feedback and statistical source ranking</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5812998">US5812998</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 1994</td><td class="patent-data-table-td patent-date-value">Sep 22, 1998</td><td class="patent-data-table-td ">Omron Corporation</td><td class="patent-data-table-td ">Similarity searching of sub-structured databases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5813002">US5813002</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 31, 1996</td><td class="patent-data-table-td patent-date-value">Sep 22, 1998</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and system for linearly detecting data deviations in a large database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5832476">US5832476</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 19, 1996</td><td class="patent-data-table-td patent-date-value">Nov 3, 1998</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Document searching method using forward and backward citation tables</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5832494">US5832494</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 17, 1996</td><td class="patent-data-table-td patent-date-value">Nov 3, 1998</td><td class="patent-data-table-td ">Libertech, Inc.</td><td class="patent-data-table-td ">Method and apparatus for indexing, searching and displaying data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5870770">US5870770</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 28, 1998</td><td class="patent-data-table-td patent-date-value">Feb 9, 1999</td><td class="patent-data-table-td ">Wolfe; Mark A.</td><td class="patent-data-table-td ">Document research system and method for displaying citing documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5893092">US5893092</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 23, 1997</td><td class="patent-data-table-td patent-date-value">Apr 6, 1999</td><td class="patent-data-table-td ">University Of Central Florida</td><td class="patent-data-table-td ">Relevancy ranking using statistical ranking, semantics, relevancy feedback and small pieces of text</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5913211">US5913211</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 26, 1996</td><td class="patent-data-table-td patent-date-value">Jun 15, 1999</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Database searching method and system using retrieval data set display screen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5930789">US5930789</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 28, 1997</td><td class="patent-data-table-td patent-date-value">Jul 27, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5940825">US5940825</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 4, 1996</td><td class="patent-data-table-td patent-date-value">Aug 17, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Adaptive similarity searching in sequence databases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5974413">US5974413</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 3, 1997</td><td class="patent-data-table-td patent-date-value">Oct 26, 1999</td><td class="patent-data-table-td ">Activeword Systems, Inc.</td><td class="patent-data-table-td ">Semantic user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5995962">US5995962</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 25, 1997</td><td class="patent-data-table-td patent-date-value">Nov 30, 1999</td><td class="patent-data-table-td ">Claritech Corporation</td><td class="patent-data-table-td ">Sort system for merging database entries</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6006252">US6006252</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 25, 1997</td><td class="patent-data-table-td patent-date-value">Dec 21, 1999</td><td class="patent-data-table-td ">Wolfe; Mark A.</td><td class="patent-data-table-td ">System and method for communicating information relating to a network resource</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6025843">US6025843</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 21, 1997</td><td class="patent-data-table-td patent-date-value">Feb 15, 2000</td><td class="patent-data-table-td ">Peter Sklar</td><td class="patent-data-table-td ">Clustering user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6041303">US6041303</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 6, 1997</td><td class="patent-data-table-td patent-date-value">Mar 21, 2000</td><td class="patent-data-table-td ">Mathews; Edward Henry</td><td class="patent-data-table-td ">Method of assisting the conducting of a research project</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6055526">US6055526</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 2, 1998</td><td class="patent-data-table-td patent-date-value">Apr 25, 2000</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Data indexing technique</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6070176">US6070176</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 30, 1997</td><td class="patent-data-table-td patent-date-value">May 30, 2000</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Method and apparatus for graphically representing portions of the world wide web</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6101491">US6101491</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 31, 1997</td><td class="patent-data-table-td patent-date-value">Aug 8, 2000</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Method and apparatus for distributed indexing and retrieval</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6128613">US6128613</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 29, 1998</td><td class="patent-data-table-td patent-date-value">Oct 3, 2000</td><td class="patent-data-table-td ">The Chinese University Of Hong Kong</td><td class="patent-data-table-td ">Method and apparatus for establishing topic word classes based on an entropy cost function to retrieve documents represented by the topic words</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6138114">US6138114</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 9, 1999</td><td class="patent-data-table-td patent-date-value">Oct 24, 2000</td><td class="patent-data-table-td ">Claritech Corporation</td><td class="patent-data-table-td ">Sort system for merging database entries</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6138116">US6138116</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 31, 1997</td><td class="patent-data-table-td patent-date-value">Oct 24, 2000</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Method and apparatus for retrieving data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6154737">US6154737</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 29, 1997</td><td class="patent-data-table-td patent-date-value">Nov 28, 2000</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Document retrieval system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6157923">US6157923</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 26, 1998</td><td class="patent-data-table-td patent-date-value">Dec 5, 2000</td><td class="patent-data-table-td ">Ensera, Inc.</td><td class="patent-data-table-td ">Query processing based on associated industry codes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6163782">US6163782</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 14, 1998</td><td class="patent-data-table-td patent-date-value">Dec 19, 2000</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Efficient and effective distributed information management</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6182063">US6182063</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 1997</td><td class="patent-data-table-td patent-date-value">Jan 30, 2001</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Method and apparatus for cascaded indexing and retrieval</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6233571">US6233571</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 4, 1998</td><td class="patent-data-table-td patent-date-value">May 15, 2001</td><td class="patent-data-table-td ">Daniel Egger</td><td class="patent-data-table-td ">Method and apparatus for indexing, searching and displaying data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6243094">US6243094</a></td><td class="patent-data-table-td patent-date-value">Oct 11, 1999</td><td class="patent-data-table-td patent-date-value">Jun 5, 2001</td><td class="patent-data-table-td ">Peter Sklar</td><td class="patent-data-table-td ">Clustering user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6292813">US6292813</a></td><td class="patent-data-table-td patent-date-value">Nov 17, 1998</td><td class="patent-data-table-td patent-date-value">Sep 18, 2001</td><td class="patent-data-table-td ">Mark A. Wolfe</td><td class="patent-data-table-td ">System and method for communicating information relating to a network resource</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6317741">US6317741</a></td><td class="patent-data-table-td patent-date-value">Aug 7, 2000</td><td class="patent-data-table-td patent-date-value">Nov 13, 2001</td><td class="patent-data-table-td ">Altavista Company</td><td class="patent-data-table-td ">Technique for ranking records of a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6336131">US6336131</a></td><td class="patent-data-table-td patent-date-value">Apr 5, 2000</td><td class="patent-data-table-td patent-date-value">Jan 1, 2002</td><td class="patent-data-table-td ">Mark A. Wolfe</td><td class="patent-data-table-td ">System and method for communicating information relating to a network resource</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6339767">US6339767</a></td><td class="patent-data-table-td patent-date-value">Aug 29, 1997</td><td class="patent-data-table-td patent-date-value">Jan 15, 2002</td><td class="patent-data-table-td ">Aurigin Systems, Inc.</td><td class="patent-data-table-td ">Using hyperbolic trees to visualize data generated by patent-centric and group-oriented data processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6347317">US6347317</a></td><td class="patent-data-table-td patent-date-value">Oct 2, 2000</td><td class="patent-data-table-td patent-date-value">Feb 12, 2002</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Efficient and effective distributed information management</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6366907">US6366907</a></td><td class="patent-data-table-td patent-date-value">Dec 15, 1999</td><td class="patent-data-table-td patent-date-value">Apr 2, 2002</td><td class="patent-data-table-td ">Napster, Inc.</td><td class="patent-data-table-td ">Real-time search engine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6415319">US6415319</a></td><td class="patent-data-table-td patent-date-value">Feb 7, 1997</td><td class="patent-data-table-td patent-date-value">Jul 2, 2002</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Intelligent network browser using incremental conceptual indexer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6438545">US6438545</a></td><td class="patent-data-table-td patent-date-value">Apr 15, 1999</td><td class="patent-data-table-td patent-date-value">Aug 20, 2002</td><td class="patent-data-table-td ">Value Capital Management</td><td class="patent-data-table-td ">Semantic user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6446065">US6446065</a></td><td class="patent-data-table-td patent-date-value">Feb 29, 2000</td><td class="patent-data-table-td patent-date-value">Sep 3, 2002</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Document retrieval assisting method and system for the same and document retrieval service using the same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6457004">US6457004</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 1, 1998</td><td class="patent-data-table-td patent-date-value">Sep 24, 2002</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Document retrieval assisting method, system and service using closely displayed areas for titles and topics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6496817">US6496817</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 27, 2000</td><td class="patent-data-table-td patent-date-value">Dec 17, 2002</td><td class="patent-data-table-td ">Korea Advanced Institute Of Science &amp; Technology</td><td class="patent-data-table-td ">Subsequence matching method using duality in constructing windows in time-series databases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6499026">US6499026</a></td><td class="patent-data-table-td patent-date-value">Sep 15, 2000</td><td class="patent-data-table-td patent-date-value">Dec 24, 2002</td><td class="patent-data-table-td ">Aurigin Systems, Inc.</td><td class="patent-data-table-td ">Using hyperbolic trees to visualize data generated by patent-centric and group-oriented data processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6523030">US6523030</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 24, 2000</td><td class="patent-data-table-td patent-date-value">Feb 18, 2003</td><td class="patent-data-table-td ">Claritech Corporation</td><td class="patent-data-table-td ">Sort system for merging database entries</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6529898">US6529898</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 22, 1999</td><td class="patent-data-table-td patent-date-value">Mar 4, 2003</td><td class="patent-data-table-td ">Matthew Shawn Fortner</td><td class="patent-data-table-td ">Method and system for electronically retrieving door hardware data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6567810">US6567810</a></td><td class="patent-data-table-td patent-date-value">Dec 21, 2001</td><td class="patent-data-table-td patent-date-value">May 20, 2003</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Efficient and effective distributed information management</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6604103">US6604103</a></td><td class="patent-data-table-td patent-date-value">Oct 9, 2001</td><td class="patent-data-table-td patent-date-value">Aug 5, 2003</td><td class="patent-data-table-td ">Mark A. Wolfe</td><td class="patent-data-table-td ">System and method for information retrieval employing a preloading procedure</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6654738">US6654738</a></td><td class="patent-data-table-td patent-date-value">Dec 17, 2001</td><td class="patent-data-table-td patent-date-value">Nov 25, 2003</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Computer program embodied on a computer-readable medium for a document retrieval service that retrieves documents with a retrieval service agent computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6711558">US6711558</a></td><td class="patent-data-table-td patent-date-value">Apr 7, 2000</td><td class="patent-data-table-td patent-date-value">Mar 23, 2004</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Associative database scanning and information retrieval</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6742023">US6742023</a></td><td class="patent-data-table-td patent-date-value">Apr 28, 2000</td><td class="patent-data-table-td patent-date-value">May 25, 2004</td><td class="patent-data-table-td ">Roxio, Inc.</td><td class="patent-data-table-td ">Use-sensitive distribution of data files between users</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6745183">US6745183</a></td><td class="patent-data-table-td patent-date-value">Apr 29, 2002</td><td class="patent-data-table-td patent-date-value">Jun 1, 2004</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Document retrieval assisting method and system for the same and document retrieval service using the same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6745194">US6745194</a></td><td class="patent-data-table-td patent-date-value">Aug 3, 2001</td><td class="patent-data-table-td patent-date-value">Jun 1, 2004</td><td class="patent-data-table-td ">Alta Vista Company</td><td class="patent-data-table-td ">Technique for deleting duplicate records referenced in an index of a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6772149">US6772149</a></td><td class="patent-data-table-td patent-date-value">Sep 23, 1999</td><td class="patent-data-table-td patent-date-value">Aug 3, 2004</td><td class="patent-data-table-td ">Lexis-Nexis Group</td><td class="patent-data-table-td ">System and method for identifying facts and legal discussion in court case law documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6834276">US6834276</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 25, 1999</td><td class="patent-data-table-td patent-date-value">Dec 21, 2004</td><td class="patent-data-table-td ">Integrated Data Control, Inc.</td><td class="patent-data-table-td ">Database system and method for data acquisition and perusal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6941317">US6941317</a></td><td class="patent-data-table-td patent-date-value">Sep 14, 1999</td><td class="patent-data-table-td patent-date-value">Sep 6, 2005</td><td class="patent-data-table-td ">Eragen Biosciences, Inc.</td><td class="patent-data-table-td ">Graphical user interface for display and analysis of biological sequence data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6957206">US6957206</a></td><td class="patent-data-table-td patent-date-value">Apr 19, 2001</td><td class="patent-data-table-td patent-date-value">Oct 18, 2005</td><td class="patent-data-table-td ">Quantum Dynamics, Inc.</td><td class="patent-data-table-td ">Computer system and method with adaptive N-level structures for automated generation of program solutions based on rules input by subject matter experts</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7003719">US7003719</a></td><td class="patent-data-table-td patent-date-value">Jan 25, 1999</td><td class="patent-data-table-td patent-date-value">Feb 21, 2006</td><td class="patent-data-table-td ">West Publishing Company, Dba West Group</td><td class="patent-data-table-td ">System, method, and software for inserting hyperlinks into documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7043526">US7043526</a></td><td class="patent-data-table-td patent-date-value">Dec 12, 2001</td><td class="patent-data-table-td patent-date-value">May 9, 2006</td><td class="patent-data-table-td ">Wolfe Mark A</td><td class="patent-data-table-td ">System and method for communicating information relating to a network resource</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7089301">US7089301</a></td><td class="patent-data-table-td patent-date-value">Aug 11, 2000</td><td class="patent-data-table-td patent-date-value">Aug 8, 2006</td><td class="patent-data-table-td ">Napster, Inc.</td><td class="patent-data-table-td ">System and method for searching peer-to-peer computer networks by selecting a computer based on at least a number of files shared by the computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7093023">US7093023</a></td><td class="patent-data-table-td patent-date-value">May 21, 2002</td><td class="patent-data-table-td patent-date-value">Aug 15, 2006</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Methods, systems, and devices using reprogrammable hardware for high-speed processing of streaming data to find a redefinable pattern and respond thereto</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7103594">US7103594</a></td><td class="patent-data-table-td patent-date-value">Jul 1, 2003</td><td class="patent-data-table-td patent-date-value">Sep 5, 2006</td><td class="patent-data-table-td ">Wolfe Mark A</td><td class="patent-data-table-td ">System and method for information retrieval employing a preloading procedure</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7127685">US7127685</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 2002</td><td class="patent-data-table-td patent-date-value">Oct 24, 2006</td><td class="patent-data-table-td ">America Online, Inc.</td><td class="patent-data-table-td ">Instant messaging interface having a tear-off element</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7137062">US7137062</a></td><td class="patent-data-table-td patent-date-value">Dec 28, 2001</td><td class="patent-data-table-td patent-date-value">Nov 14, 2006</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and method for hierarchical segmentation with latent semantic indexing in scale space</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7139743">US7139743</a></td><td class="patent-data-table-td patent-date-value">May 21, 2002</td><td class="patent-data-table-td patent-date-value">Nov 21, 2006</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Associative database scanning and information retrieval using FPGA devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7165071">US7165071</a></td><td class="patent-data-table-td patent-date-value">Dec 19, 2001</td><td class="patent-data-table-td patent-date-value">Jan 16, 2007</td><td class="patent-data-table-td ">Napster, Inc.</td><td class="patent-data-table-td ">Real-time search engine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7181437">US7181437</a></td><td class="patent-data-table-td patent-date-value">Nov 24, 2003</td><td class="patent-data-table-td patent-date-value">Feb 20, 2007</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Associative database scanning and information retrieval</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7233940">US7233940</a></td><td class="patent-data-table-td patent-date-value">Nov 5, 2001</td><td class="patent-data-table-td patent-date-value">Jun 19, 2007</td><td class="patent-data-table-td ">Answers Corporation</td><td class="patent-data-table-td ">System for processing at least partially structured data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7257604">US7257604</a></td><td class="patent-data-table-td patent-date-value">Aug 5, 2003</td><td class="patent-data-table-td patent-date-value">Aug 14, 2007</td><td class="patent-data-table-td ">Wolfe Mark A</td><td class="patent-data-table-td ">System and method for communicating information relating to a network resource</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7281215">US7281215</a></td><td class="patent-data-table-td patent-date-value">Jul 31, 2002</td><td class="patent-data-table-td patent-date-value">Oct 9, 2007</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">IM conversation counter and indicator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7284207">US7284207</a></td><td class="patent-data-table-td patent-date-value">Sep 6, 2006</td><td class="patent-data-table-td patent-date-value">Oct 16, 2007</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">Instant messaging interface having a tear-off element</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7302401">US7302401</a></td><td class="patent-data-table-td patent-date-value">Apr 4, 2000</td><td class="patent-data-table-td patent-date-value">Nov 27, 2007</td><td class="patent-data-table-td ">Single Source Oy</td><td class="patent-data-table-td ">Data management mechanism for project planning</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7302638">US7302638</a></td><td class="patent-data-table-td patent-date-value">Aug 29, 2003</td><td class="patent-data-table-td patent-date-value">Nov 27, 2007</td><td class="patent-data-table-td ">Wolfe Mark A</td><td class="patent-data-table-td ">Efficiently displaying and researching information about the interrelationships between documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7310629">US7310629</a></td><td class="patent-data-table-td patent-date-value">Apr 3, 2002</td><td class="patent-data-table-td patent-date-value">Dec 18, 2007</td><td class="patent-data-table-td ">Napster, Inc.</td><td class="patent-data-table-td ">Method and apparatus for controlling file sharing of multimedia files over a fluid, de-centralized network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7333966">US7333966</a></td><td class="patent-data-table-td patent-date-value">Jun 13, 2002</td><td class="patent-data-table-td patent-date-value">Feb 19, 2008</td><td class="patent-data-table-td ">Thomson Global Resources</td><td class="patent-data-table-td ">Systems, methods, and software for hyperlinking names</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7370277">US7370277</a></td><td class="patent-data-table-td patent-date-value">Dec 23, 2002</td><td class="patent-data-table-td patent-date-value">May 6, 2008</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">E-mail interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7406460">US7406460</a></td><td class="patent-data-table-td patent-date-value">Apr 20, 2004</td><td class="patent-data-table-td patent-date-value">Jul 29, 2008</td><td class="patent-data-table-td ">Overture Services, Inc.</td><td class="patent-data-table-td ">Technique for ranking records of a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7412463">US7412463</a></td><td class="patent-data-table-td patent-date-value">Jan 11, 2002</td><td class="patent-data-table-td patent-date-value">Aug 12, 2008</td><td class="patent-data-table-td ">Bloomberg Finance L.P.</td><td class="patent-data-table-td ">Dynamic legal database providing historical and current versions of bodies of law</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7421661">US7421661</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 2002</td><td class="patent-data-table-td patent-date-value">Sep 2, 2008</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">Instant messaging interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7523126">US7523126</a></td><td class="patent-data-table-td patent-date-value">Jun 22, 2002</td><td class="patent-data-table-td patent-date-value">Apr 21, 2009</td><td class="patent-data-table-td ">Rose Blush Software Llc</td><td class="patent-data-table-td ">Using hyperbolic trees to visualize data generated by patent-centric and group-oriented data processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7552107">US7552107</a></td><td class="patent-data-table-td patent-date-value">Jan 8, 2007</td><td class="patent-data-table-td patent-date-value">Jun 23, 2009</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Associative database scanning and information retrieval</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7571174">US7571174</a></td><td class="patent-data-table-td patent-date-value">Jan 3, 2005</td><td class="patent-data-table-td patent-date-value">Aug 4, 2009</td><td class="patent-data-table-td ">Thomson Reuters Global Resurces</td><td class="patent-data-table-td ">Systems, methods, interfaces and software for automated collection and integration of entity data into online databases and professional directories</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7602785">US7602785</a></td><td class="patent-data-table-td patent-date-value">Feb 9, 2005</td><td class="patent-data-table-td patent-date-value">Oct 13, 2009</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Method and system for performing longest prefix matching for network address lookup using bloom filters</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7636703">US7636703</a></td><td class="patent-data-table-td patent-date-value">May 2, 2006</td><td class="patent-data-table-td patent-date-value">Dec 22, 2009</td><td class="patent-data-table-td ">Exegy Incorporated</td><td class="patent-data-table-td ">Method and apparatus for approximate pattern matching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7660793">US7660793</a></td><td class="patent-data-table-td patent-date-value">Nov 12, 2007</td><td class="patent-data-table-td patent-date-value">Feb 9, 2010</td><td class="patent-data-table-td ">Exegy Incorporated</td><td class="patent-data-table-td ">Method and system for high performance integration, processing and searching of structured and unstructured data using coprocessors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7668853">US7668853</a></td><td class="patent-data-table-td patent-date-value">Nov 25, 2003</td><td class="patent-data-table-td patent-date-value">Feb 23, 2010</td><td class="patent-data-table-td ">Sony United Kingdom Limited</td><td class="patent-data-table-td ">Information storage and retrieval</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7672968">US7672968</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 12, 2005</td><td class="patent-data-table-td patent-date-value">Mar 2, 2010</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Displaying a tooltip associated with a concurrently displayed database object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7680790">US7680790</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 2007</td><td class="patent-data-table-td patent-date-value">Mar 16, 2010</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Method and apparatus for approximate matching of DNA sequences</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7702629">US7702629</a></td><td class="patent-data-table-td patent-date-value">Dec 2, 2005</td><td class="patent-data-table-td patent-date-value">Apr 20, 2010</td><td class="patent-data-table-td ">Exegy Incorporated</td><td class="patent-data-table-td ">Method and device for high performance regular expression pattern matching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7711622">US7711622</a></td><td class="patent-data-table-td patent-date-value">Mar 5, 2008</td><td class="patent-data-table-td patent-date-value">May 4, 2010</td><td class="patent-data-table-td ">Stephen M Marceau</td><td class="patent-data-table-td ">Financial statement and transaction image delivery and access system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7711844">US7711844</a></td><td class="patent-data-table-td patent-date-value">Aug 15, 2002</td><td class="patent-data-table-td patent-date-value">May 4, 2010</td><td class="patent-data-table-td ">Washington University Of St. Louis</td><td class="patent-data-table-td ">TCP-splitter: reliable packet monitoring methods and apparatus for high speed networks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7716060">US7716060</a></td><td class="patent-data-table-td patent-date-value">Feb 23, 2001</td><td class="patent-data-table-td patent-date-value">May 11, 2010</td><td class="patent-data-table-td ">Germeraad Paul B</td><td class="patent-data-table-td ">Patent-related tools and methodology for use in the merger and acquisition process</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7716330">US7716330</a></td><td class="patent-data-table-td patent-date-value">Oct 19, 2001</td><td class="patent-data-table-td patent-date-value">May 11, 2010</td><td class="patent-data-table-td ">Global Velocity, Inc.</td><td class="patent-data-table-td ">System and method for controlling transmission of data packets over an information network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7729990">US7729990</a></td><td class="patent-data-table-td patent-date-value">Apr 14, 2004</td><td class="patent-data-table-td patent-date-value">Jun 1, 2010</td><td class="patent-data-table-td ">Stephen Michael Marceau</td><td class="patent-data-table-td ">Check image access system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7783619">US7783619</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 27, 2005</td><td class="patent-data-table-td patent-date-value">Aug 24, 2010</td><td class="patent-data-table-td ">Elsevier B.V.</td><td class="patent-data-table-td ">Methods and software for analysis of research publications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7792811">US7792811</a></td><td class="patent-data-table-td patent-date-value">Dec 22, 2005</td><td class="patent-data-table-td patent-date-value">Sep 7, 2010</td><td class="patent-data-table-td ">Transaxtions Llc</td><td class="patent-data-table-td ">Intelligent search with guiding info</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7797336">US7797336</a></td><td class="patent-data-table-td patent-date-value">May 4, 2001</td><td class="patent-data-table-td patent-date-value">Sep 14, 2010</td><td class="patent-data-table-td ">Tim W Blair</td><td class="patent-data-table-td ">System, method, and computer program product for knowledge management</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7840482">US7840482</a></td><td class="patent-data-table-td patent-date-value">Jun 8, 2007</td><td class="patent-data-table-td patent-date-value">Nov 23, 2010</td><td class="patent-data-table-td ">Exegy Incorporated</td><td class="patent-data-table-td ">Method and system for high speed options pricing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7877697">US7877697</a></td><td class="patent-data-table-td patent-date-value">Oct 5, 2007</td><td class="patent-data-table-td patent-date-value">Jan 25, 2011</td><td class="patent-data-table-td ">Aol Inc.</td><td class="patent-data-table-td ">IM conversation counter and indicator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7900148">US7900148</a></td><td class="patent-data-table-td patent-date-value">May 5, 2008</td><td class="patent-data-table-td patent-date-value">Mar 1, 2011</td><td class="patent-data-table-td ">Aol Inc.</td><td class="patent-data-table-td ">E-mail interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7908277">US7908277</a></td><td class="patent-data-table-td patent-date-value">Feb 5, 2007</td><td class="patent-data-table-td patent-date-value">Mar 15, 2011</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Annotating links in a document based on the ranks of documents pointed to by the links</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7917299">US7917299</a></td><td class="patent-data-table-td patent-date-value">Feb 22, 2006</td><td class="patent-data-table-td patent-date-value">Mar 29, 2011</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Method and apparatus for performing similarity searching on a data stream with respect to a query string</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7930295">US7930295</a></td><td class="patent-data-table-td patent-date-value">Aug 23, 2010</td><td class="patent-data-table-td patent-date-value">Apr 19, 2011</td><td class="patent-data-table-td ">Elsevier, Inc.</td><td class="patent-data-table-td ">Methods and software for analysis of research publications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7945528">US7945528</a></td><td class="patent-data-table-td patent-date-value">Feb 10, 2010</td><td class="patent-data-table-td patent-date-value">May 17, 2011</td><td class="patent-data-table-td ">Exegy Incorporated</td><td class="patent-data-table-td ">Method and device for high performance regular expression pattern matching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7949650">US7949650</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 2007</td><td class="patent-data-table-td patent-date-value">May 24, 2011</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Associative database scanning and information retrieval</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7949728">US7949728</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 2006</td><td class="patent-data-table-td patent-date-value">May 24, 2011</td><td class="patent-data-table-td ">Rose Blush Software Llc</td><td class="patent-data-table-td ">System, method, and computer program product for managing and analyzing intellectual property (IP) related transactions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7953743">US7953743</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 2007</td><td class="patent-data-table-td patent-date-value">May 31, 2011</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Associative database scanning and information retrieval</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7954114">US7954114</a></td><td class="patent-data-table-td patent-date-value">Jan 26, 2006</td><td class="patent-data-table-td patent-date-value">May 31, 2011</td><td class="patent-data-table-td ">Exegy Incorporated</td><td class="patent-data-table-td ">Firmware socket module for FPGA-based pipeline processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7966328">US7966328</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 2006</td><td class="patent-data-table-td patent-date-value">Jun 21, 2011</td><td class="patent-data-table-td ">Rose Blush Software Llc</td><td class="patent-data-table-td ">Patent-related tools and methodology for use in research and development projects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8001129">US8001129</a></td><td class="patent-data-table-td patent-date-value">Jul 28, 2009</td><td class="patent-data-table-td patent-date-value">Aug 16, 2011</td><td class="patent-data-table-td ">Thomson Reuters Global Resources</td><td class="patent-data-table-td ">Systems, methods, interfaces and software for automated collection and integration of entity data into online databases and professional directories</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8001457">US8001457</a></td><td class="patent-data-table-td patent-date-value">Feb 6, 2006</td><td class="patent-data-table-td patent-date-value">Aug 16, 2011</td><td class="patent-data-table-td ">West Services, Inc.</td><td class="patent-data-table-td ">System, method, and software for inserting hyperlinks into documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8069102">US8069102</a></td><td class="patent-data-table-td patent-date-value">Nov 20, 2006</td><td class="patent-data-table-td patent-date-value">Nov 29, 2011</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Method and apparatus for processing financial information at hardware speeds using FPGA devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8095508">US8095508</a></td><td class="patent-data-table-td patent-date-value">May 21, 2004</td><td class="patent-data-table-td patent-date-value">Jan 10, 2012</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Intelligent data storage and processing using FPGA devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8126884">US8126884</a></td><td class="patent-data-table-td patent-date-value">Jan 28, 2010</td><td class="patent-data-table-td patent-date-value">Feb 28, 2012</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8131697">US8131697</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 2007</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Method and apparatus for approximate matching where programmable logic is used to process data being written to a mass storage medium and process data being read from a mass storage medium</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8131715">US8131715</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 2010</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8131717">US8131717</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 2010</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8135758">US8135758</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 2009</td><td class="patent-data-table-td patent-date-value">Mar 13, 2012</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Customizable, dynamic and on-demand database-informer for relational databases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8156101">US8156101</a></td><td class="patent-data-table-td patent-date-value">Dec 17, 2009</td><td class="patent-data-table-td patent-date-value">Apr 10, 2012</td><td class="patent-data-table-td ">Exegy Incorporated</td><td class="patent-data-table-td ">Method and system for high performance integration, processing and searching of structured and unstructured data using coprocessors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8195651">US8195651</a></td><td class="patent-data-table-td patent-date-value">Feb 2, 2010</td><td class="patent-data-table-td patent-date-value">Jun 5, 2012</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8285744">US8285744</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 2005</td><td class="patent-data-table-td patent-date-value">Oct 9, 2012</td><td class="patent-data-table-td ">Rockwell Automation Technologies, Inc.</td><td class="patent-data-table-td ">Indexing and searching manufacturing process related information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8326819">US8326819</a></td><td class="patent-data-table-td patent-date-value">Nov 12, 2007</td><td class="patent-data-table-td patent-date-value">Dec 4, 2012</td><td class="patent-data-table-td ">Exegy Incorporated</td><td class="patent-data-table-td ">Method and system for high performance data metatagging and data indexing using coprocessors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8326823">US8326823</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 31, 2008</td><td class="patent-data-table-td patent-date-value">Dec 4, 2012</td><td class="patent-data-table-td ">Ebay Inc.</td><td class="patent-data-table-td ">Navigation for large scale graphs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8374986">US8374986</a></td><td class="patent-data-table-td patent-date-value">May 15, 2008</td><td class="patent-data-table-td patent-date-value">Feb 12, 2013</td><td class="patent-data-table-td ">Exegy Incorporated</td><td class="patent-data-table-td ">Method and system for accelerated stream processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8396856">US8396856</a></td><td class="patent-data-table-td patent-date-value">Nov 12, 2010</td><td class="patent-data-table-td patent-date-value">Mar 12, 2013</td><td class="patent-data-table-td ">Robert Leland Jensen</td><td class="patent-data-table-td ">Database system and method for data acquisition and perusal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8407122">US8407122</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2011</td><td class="patent-data-table-td patent-date-value">Mar 26, 2013</td><td class="patent-data-table-td ">Exegy Incorporated</td><td class="patent-data-table-td ">High speed processing of financial information using FPGA devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8429543">US8429543</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 2011</td><td class="patent-data-table-td patent-date-value">Apr 23, 2013</td><td class="patent-data-table-td ">Facebook, Inc.</td><td class="patent-data-table-td ">E-mail interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8458081">US8458081</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2011</td><td class="patent-data-table-td patent-date-value">Jun 4, 2013</td><td class="patent-data-table-td ">Exegy Incorporated</td><td class="patent-data-table-td ">High speed processing of financial information using FPGA devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8478680">US8478680</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2011</td><td class="patent-data-table-td patent-date-value">Jul 2, 2013</td><td class="patent-data-table-td ">Exegy Incorporated</td><td class="patent-data-table-td ">High speed processing of financial information using FPGA devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8489630">US8489630</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2011</td><td class="patent-data-table-td patent-date-value">Jul 16, 2013</td><td class="patent-data-table-td ">Elsevier B.V.</td><td class="patent-data-table-td ">Methods and software for analysis of research publications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8515682">US8515682</a></td><td class="patent-data-table-td patent-date-value">Mar 11, 2011</td><td class="patent-data-table-td patent-date-value">Aug 20, 2013</td><td class="patent-data-table-td ">Washington University</td><td class="patent-data-table-td ">Method and apparatus for performing similarity searching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8521730">US8521730</a></td><td class="patent-data-table-td patent-date-value">May 30, 2012</td><td class="patent-data-table-td patent-date-value">Aug 27, 2013</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8549024">US8549024</a></td><td class="patent-data-table-td patent-date-value">Mar 2, 2012</td><td class="patent-data-table-td patent-date-value">Oct 1, 2013</td><td class="patent-data-table-td ">Ip Reservoir, Llc</td><td class="patent-data-table-td ">Method and apparatus for adjustable data matching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8595104">US8595104</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2011</td><td class="patent-data-table-td patent-date-value">Nov 26, 2013</td><td class="patent-data-table-td ">Ip Reservoir, Llc</td><td class="patent-data-table-td ">High speed processing of financial information using FPGA devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8600856">US8600856</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2011</td><td class="patent-data-table-td patent-date-value">Dec 3, 2013</td><td class="patent-data-table-td ">Ip Reservoir, Llc</td><td class="patent-data-table-td ">High speed processing of financial information using FPGA devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8620881">US8620881</a></td><td class="patent-data-table-td patent-date-value">Jun 21, 2011</td><td class="patent-data-table-td patent-date-value">Dec 31, 2013</td><td class="patent-data-table-td ">Ip Reservoir, Llc</td><td class="patent-data-table-td ">Intelligent data storage and processing using FPGA devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8626624">US8626624</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2011</td><td class="patent-data-table-td patent-date-value">Jan 7, 2014</td><td class="patent-data-table-td ">Ip Reservoir, Llc</td><td class="patent-data-table-td ">High speed processing of financial information using FPGA devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8626763">US8626763</a></td><td class="patent-data-table-td patent-date-value">Jun 4, 2012</td><td class="patent-data-table-td patent-date-value">Jan 7, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Server-side suggestion of preload operations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8639694">US8639694</a></td><td class="patent-data-table-td patent-date-value">May 23, 2012</td><td class="patent-data-table-td patent-date-value">Jan 28, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Client-side processing of preload operations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8655764">US8655764</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2011</td><td class="patent-data-table-td patent-date-value">Feb 18, 2014</td><td class="patent-data-table-td ">Ip Reservoir, Llc</td><td class="patent-data-table-td ">High speed processing of financial information using FPGA devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8725726">US8725726</a></td><td class="patent-data-table-td patent-date-value">Sep 14, 2012</td><td class="patent-data-table-td patent-date-value">May 13, 2014</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8751452">US8751452</a></td><td class="patent-data-table-td patent-date-value">Jan 6, 2012</td><td class="patent-data-table-td patent-date-value">Jun 10, 2014</td><td class="patent-data-table-td ">Ip Reservoir, Llc</td><td class="patent-data-table-td ">Intelligent data storage and processing using FPGA devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8762249">US8762249</a></td><td class="patent-data-table-td patent-date-value">Jun 7, 2011</td><td class="patent-data-table-td patent-date-value">Jun 24, 2014</td><td class="patent-data-table-td ">Ip Reservoir, Llc</td><td class="patent-data-table-td ">Method and apparatus for high-speed processing of financial market depth data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8768805">US8768805</a></td><td class="patent-data-table-td patent-date-value">Jun 7, 2011</td><td class="patent-data-table-td patent-date-value">Jul 1, 2014</td><td class="patent-data-table-td ">Ip Reservoir, Llc</td><td class="patent-data-table-td ">Method and apparatus for high-speed processing of financial market depth data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8768888">US8768888</a></td><td class="patent-data-table-td patent-date-value">Jan 6, 2012</td><td class="patent-data-table-td patent-date-value">Jul 1, 2014</td><td class="patent-data-table-td ">Ip Reservoir, Llc</td><td class="patent-data-table-td ">Intelligent data storage and processing using FPGA devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090204582">US20090204582</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 31, 2008</td><td class="patent-data-table-td patent-date-value">Aug 13, 2009</td><td class="patent-data-table-td ">Roopnath Grandhi</td><td class="patent-data-table-td ">Navigation for large scale graphs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110083585">US20110083585</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 10, 2009</td><td class="patent-data-table-td patent-date-value">Apr 14, 2011</td><td class="patent-data-table-td ">Lafarge</td><td class="patent-data-table-td ">Aqueous formulations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130097133">US20130097133</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 3, 2012</td><td class="patent-data-table-td patent-date-value">Apr 18, 2013</td><td class="patent-data-table-td ">Ebay Inc.</td><td class="patent-data-table-td ">Navigation for large scale graphs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/USRE39090">USRE39090</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 26, 2001</td><td class="patent-data-table-td patent-date-value">May 2, 2006</td><td class="patent-data-table-td ">Activeword Systems, Inc.</td><td class="patent-data-table-td ">Semantic user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/USRE41071">USRE41071</a></td><td class="patent-data-table-td patent-date-value">Jun 30, 2004</td><td class="patent-data-table-td patent-date-value">Jan 5, 2010</td><td class="patent-data-table-td ">AT&amp;T Intellectual Propeerty I, L.P.</td><td class="patent-data-table-td ">Arranging records in a search result to be provided in response to a data inquiry of a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO1997012334A1?cl=en">WO1997012334A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 25, 1996</td><td class="patent-data-table-td patent-date-value">Apr 3, 1997</td><td class="patent-data-table-td ">Int Compu Research Inc</td><td class="patent-data-table-td ">Matching and ranking legal citations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO1998036343A2?cl=en">WO1998036343A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 6, 1998</td><td class="patent-data-table-td patent-date-value">Aug 20, 1998</td><td class="patent-data-table-td ">Intel Corp</td><td class="patent-data-table-td ">Method and apparatus for graphically representing portions of the world wide web</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO1998055945A1?cl=en">WO1998055945A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 2, 1998</td><td class="patent-data-table-td patent-date-value">Dec 10, 1998</td><td class="patent-data-table-td ">Smartpatents Inc</td><td class="patent-data-table-td ">System, method and computer program product for patent-centric and group-oriented data processing, including using hyperbolic trees to visualize data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2000025229A1?cl=en">WO2000025229A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 22, 1999</td><td class="patent-data-table-td patent-date-value">May 4, 2000</td><td class="patent-data-table-td ">Johns Hopkins Singapore Pte Lt</td><td class="patent-data-table-td ">Informatics system weaves</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001090930A1?cl=en">WO2001090930A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 24, 2001</td><td class="patent-data-table-td patent-date-value">Nov 29, 2001</td><td class="patent-data-table-td ">Bertolus Phillip Andre</td><td class="patent-data-table-td ">Indexing and searching ideographic characters on a networked system of computers</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S715000">707/715</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707SE17108">707/E17.108</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707SE17089">707/E17.089</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999005">707/999.005</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0007000000">G06F7/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0017300000">G06F17/30</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99934">Y10S707/99934</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99945">Y10S707/99945</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99932">Y10S707/99932</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99935">Y10S707/99935</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99933">Y10S707/99933</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99943">Y10S707/99943</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30522">G06F17/30522</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30705">G06F17/30705</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30882">G06F17/30882</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30554">G06F17/30554</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30864">G06F17/30864</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YPs_BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30321">G06F17/30321</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06F17/30W5H</span>, <span class="nested-value">G06F17/30S4V</span>, <span class="nested-value">G06F17/30S4P7</span>, <span class="nested-value">G06F17/30T4</span>, <span class="nested-value">G06F17/30W1</span>, <span class="nested-value">G06F17/30S2P</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Sep 3, 2013</td><td class="patent-data-table-td ">IPR</td><td class="patent-data-table-td ">Aia trial proceeding filed before the patent and appeal board: inter partes review</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">TRIAL NO: IPR2013-00478</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20130730</span></div><div class="nested-key-value"><span class="nested-key">Opponent name: </span><span class="nested-value">FACEBOOK, INC., LINKEDIN CORP., AND TWITTER, INC.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 20, 2011</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 26-42 AND 44 IS CONFIRMED. CLAIM 45 IS CANCELLED. NEW CLAIMS 53-61 ARE ADDED AND DETERMINED TO BE PATENTABLE. CLAIMS 1-25, 43 AND 46-52 WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 10, 2010</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100524</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 4, 2008</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EGGER, DANIEL, NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SITE TECHNOLOGIES, INC.;SITE/TECHNOLOGIES/INC.;REEL/FRAME:021794/0648</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080813</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, LLC, NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:EGGER, DANIEL;REEL/FRAME:021794/0661</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080926</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, LLC,NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EGGER, DANIEL,NORTH CAROLINA</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 5, 2008</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 13, 2007</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, LLC, NEW YORK</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:SOFTWARE RIGHTS ARCHIVE, INC.;REEL/FRAME:019714/0723</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20070518</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, LLC,NEW YORK</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 13, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EGGER, DANIEL, NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">BILL OF SALE, ASSIGNMENT &amp; LICENSE AGREEMENT;ASSIGNOR:SITE TECHNOLOGIES, INC.;REEL/FRAME:018160/0500</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19980916</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 23, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, INC., NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:EGGER, MR. DANIEL;REEL/FRAME:015698/0357</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20050222</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, INC. 905 WEST MAIN STREET</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:EGGER, MR. DANIEL /AR;REEL/FRAME:015698/0357</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 16, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EGGER, MR. DANIEL, NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:SITE/TECHNOLOGIES/INC.;REEL/FRAME:015687/0186</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20050211</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EGGER, MR. DANIEL 2027 WEST CLUB BOULEVARDDURHAM,</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:SITE/TECHNOLOGIES/INC. /AR;REEL/FRAME:015687/0186</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 27, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SITE/TECHNOLOGIES/INC., CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:LIBERTECH INC.;REEL/FRAME:015612/0397</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19960822</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SITE/TECHNOLOGIES/INC. 1120 FOREST AVENUE, #301PAC</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:LIBERTECH INC. /AR;REEL/FRAME:015612/0397</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 5, 2004</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 13, 2000</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 13, 2000</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 29, 2000</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 15, 1993</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">LIBERTECH, INC., NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:EGGER, DANIEL;REEL/FRAME:006800/0165</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19931109</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U3M-LrHoHWMaE6yJACuVLKpnBTsPQ\u0026id=YPs_BAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U1q5dJvO2A29MS1midfMYcJcHR2iw\u0026id=YPs_BAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U2gPft7hFNeu3SvPhbzqMioMhitGQ","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Method_and_apparatus_for_indexing_search.pdf?id=YPs_BAABERAJ\u0026output=pdf\u0026sig=ACfU3U0LJmnrM9FTdtDAO1eopgdzQhyhwQ"},"sample_url":"http://www.google.com/patents/reader?id=YPs_BAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>