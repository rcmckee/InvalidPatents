<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6959112 - Method for finding a pattern which may fall partially outside an image - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Method for finding a pattern which may fall partially outside an image"><meta name="DC.contributor" content="Adam Wagman" scheme="inventor"><meta name="DC.contributor" content="Cognex Technology And Investment Corporation" scheme="assignee"><meta name="DC.date" content="2001-6-29" scheme="dateSubmitted"><meta name="DC.description" content="A method is provided for finding a whole pattern in an image, where at least a portion of the whole pattern falls outside the boundary of the image. The method includes, for each candidate pose of a search model of the whole pattern that results in a transformed search model that may extend beyond the boundary of the image, applying a match-quality metric to only a subset of search model features and corresponding image features, the subset being uniquely determined by the pose. The features which do not overlap the image at that pose of the model are completely excluded from the metric computation. All the relevant available information is used, using no arbitrarily hypothesized information. The higher-level strategy of the search procedure is free to consider poses where the model extends partially outside the image, and the results of each metric computation will be the most true value possible."><meta name="DC.date" content="2005-10-25" scheme="issued"><meta name="DC.relation" content="JP:H0737893" scheme="references"><meta name="DC.relation" content="US:3069654" scheme="references"><meta name="DC.relation" content="US:3816722" scheme="references"><meta name="DC.relation" content="US:3898617" scheme="references"><meta name="DC.relation" content="US:3936800" scheme="references"><meta name="DC.relation" content="US:4115762" scheme="references"><meta name="DC.relation" content="US:4200861" scheme="references"><meta name="DC.relation" content="US:4441205" scheme="references"><meta name="DC.relation" content="US:4672676" scheme="references"><meta name="DC.relation" content="US:4955062" scheme="references"><meta name="DC.relation" content="US:5003166" scheme="references"><meta name="DC.relation" content="US:5020006" scheme="references"><meta name="DC.relation" content="US:5040231" scheme="references"><meta name="DC.relation" content="US:5086478" scheme="references"><meta name="DC.relation" content="US:5168530" scheme="references"><meta name="DC.relation" content="US:5177559" scheme="references"><meta name="DC.relation" content="US:5226095" scheme="references"><meta name="DC.relation" content="US:5253306" scheme="references"><meta name="DC.relation" content="US:5313532" scheme="references"><meta name="DC.relation" content="US:5384711" scheme="references"><meta name="DC.relation" content="US:5513275" scheme="references"><meta name="DC.relation" content="US:5537669" scheme="references"><meta name="DC.relation" content="US:5550763" scheme="references"><meta name="DC.relation" content="US:5568563" scheme="references"><meta name="DC.relation" content="US:5586058" scheme="references"><meta name="DC.relation" content="US:5602937" scheme="references"><meta name="DC.relation" content="US:5613013" scheme="references"><meta name="DC.relation" content="US:5627915" scheme="references"><meta name="DC.relation" content="US:5708731" scheme="references"><meta name="DC.relation" content="US:5717785" scheme="references"><meta name="DC.relation" content="US:5825913" scheme="references"><meta name="DC.relation" content="US:5835622" scheme="references"><meta name="DC.relation" content="US:5848189" scheme="references"><meta name="DC.relation" content="US:6002793" scheme="references"><meta name="DC.relation" content="US:6005978" scheme="references"><meta name="DC.relation" content="US:6023530" scheme="references"><meta name="DC.relation" content="US:6035066" scheme="references"><meta name="DC.relation" content="US:6067379" scheme="references"><meta name="DC.relation" content="US:6154567" scheme="references"><meta name="DC.relation" content="US:6246478" scheme="references"><meta name="DC.relation" content="US:6385340" scheme="references"><meta name="DC.relation" content="US:6532301" scheme="references"><meta name="DC.relation" content="US:6625303" scheme="references"><meta name="citation_reference" content="Cognex MVS-8000 Series CVL Vision Tools Guide, Version 5.4, 2000, pp. 25-136."><meta name="citation_reference" content="Wallack, Aaron, &quot;Algorithms and Techniques for Manufacturing,&quot; University of California at Berkeley, 1995, pp. 97-335."><meta name="citation_patent_number" content="US:6959112"><meta name="citation_patent_application_number" content="US:09/895,369"><link rel="canonical" href="http://www.google.com/patents/US6959112"/><meta property="og:url" content="http://www.google.com/patents/US6959112"/><meta name="title" content="Patent US6959112 - Method for finding a pattern which may fall partially outside an image"/><meta name="description" content="A method is provided for finding a whole pattern in an image, where at least a portion of the whole pattern falls outside the boundary of the image. The method includes, for each candidate pose of a search model of the whole pattern that results in a transformed search model that may extend beyond the boundary of the image, applying a match-quality metric to only a subset of search model features and corresponding image features, the subset being uniquely determined by the pose. The features which do not overlap the image at that pose of the model are completely excluded from the metric computation. All the relevant available information is used, using no arbitrarily hypothesized information. The higher-level strategy of the search procedure is free to consider poses where the model extends partially outside the image, and the results of each metric computation will be the most true value possible."/><meta property="og:title" content="Patent US6959112 - Method for finding a pattern which may fall partially outside an image"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("LkXsU7HlDKTEsASR1IDwDQ"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("GBR"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("LkXsU7HlDKTEsASR1IDwDQ"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("GBR"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6959112?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6959112"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=7oRvBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6959112&amp;usg=AFQjCNFz-W0iqeAAQuiuAUjyOMehveKZhw" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6959112.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6959112.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6959112" style="display:none"><span itemprop="description">A method is provided for finding a whole pattern in an image, where at least a portion of the whole pattern falls outside the boundary of the image. The method includes, for each candidate pose of a search model of the whole pattern that results in a transformed search model that may extend beyond the...</span><span itemprop="url">http://www.google.com/patents/US6959112?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6959112 - Method for finding a pattern which may fall partially outside an image</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6959112 - Method for finding a pattern which may fall partially outside an image" title="Patent US6959112 - Method for finding a pattern which may fall partially outside an image"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6959112 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/895,369</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Oct 25, 2005</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Jun 29, 2001</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Jun 29, 2001</td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">09895369, </span><span class="patent-bibdata-value">895369, </span><span class="patent-bibdata-value">US 6959112 B1, </span><span class="patent-bibdata-value">US 6959112B1, </span><span class="patent-bibdata-value">US-B1-6959112, </span><span class="patent-bibdata-value">US6959112 B1, </span><span class="patent-bibdata-value">US6959112B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Adam+Wagman%22">Adam Wagman</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Cognex+Technology+And+Investment+Corporation%22">Cognex Technology And Investment Corporation</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6959112.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6959112.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6959112.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (43),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (2),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (4),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (13),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (5)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6959112&usg=AFQjCNF8bPI6g8cD7JYpuslUV85BCHVUaw">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6959112&usg=AFQjCNEH5n3IWvo8kv50AGi7DK1RbgUC2A">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6959112B1%26KC%3DB1%26FT%3DD&usg=AFQjCNFG-6VEib21j1AKlgAgLHYPXWMjfg">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55487153" lang="EN" load-source="patent-office">Method for finding a pattern which may fall partially outside an image</invention-title></span><br><span class="patent-number">US 6959112 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA50889841" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">A method is provided for finding a whole pattern in an image, where at least a portion of the whole pattern falls outside the boundary of the image. The method includes, for each candidate pose of a search model of the whole pattern that results in a transformed search model that may extend beyond the boundary of the image, applying a match-quality metric to only a subset of search model features and corresponding image features, the subset being uniquely determined by the pose. The features which do not overlap the image at that pose of the model are completely excluded from the metric computation. All the relevant available information is used, using no arbitrarily hypothesized information. The higher-level strategy of the search procedure is free to consider poses where the model extends partially outside the image, and the results of each metric computation will be the most true value possible.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(8)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6959112B1/US06959112-20051025-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6959112B1/US06959112-20051025-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6959112B1/US06959112-20051025-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6959112B1/US06959112-20051025-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6959112B1/US06959112-20051025-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6959112B1/US06959112-20051025-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6959112B1/US06959112-20051025-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6959112B1/US06959112-20051025-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6959112B1/US06959112-20051025-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6959112B1/US06959112-20051025-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6959112B1/US06959112-20051025-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6959112B1/US06959112-20051025-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6959112B1/US06959112-20051025-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6959112B1/US06959112-20051025-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6959112B1/US06959112-20051025-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6959112B1/US06959112-20051025-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(11)</span></span></div><div class="patent-text"><div mxw-id="PCLM8916566" lang="EN" load-source="patent-office" class="claims">
  <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
    <div class="claim-text">1. A method for finding a whole pattern, a portion of the whole pattern falling outside the boundary of an image, the method including:
<div class="claim-text">for each candidate pose of a search model of the whole pattern that results in a transformed search model that falls partially outside the boundary of the image, applying a match-quality metric to only a subset of search model features and corresponding image features, the subset being uniquely determined by the pose.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
    <div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the subset of model features is determined in accordance with at least a portion of the boundary of the image.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
    <div class="claim-text">3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the subset of model features falls within an overlap region of the transformed search model and the image.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
    <div class="claim-text">4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the model features are pixels.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
    <div class="claim-text">5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the model features include edge elements.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00006" num="00006" class="claim">
    <div class="claim-text">6. A method for finding a whole pattern, at least a portion of the whole pattern falling outside the boundary of an image, the method including:
<div class="claim-text">for each candidate pose of a search model of the whole pattern that results in a transformed search model that falls partially outside the boundary of the image, computing a match-quality score by considering only model features of the transformed search model that overlap the image, thereby ignoring model features of the transformed search model that fall outside the boundary of the image.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
    <div class="claim-text">7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein at least some of the model features of the transformed search model that overlap the image also abut at least a portion of the boundary of the image.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
    <div class="claim-text">8. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the model features are pixels.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
    <div class="claim-text">9. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the model features include edge elements.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00010" num="00010" class="claim">
    <div class="claim-text">10. A method for finding a pattern which may fall partially outside a boundary of an image, the method comprising:
<div class="claim-text">providing a search model, the search model having a model boundary;</div>
<div class="claim-text">providing an image, the image having an image boundary and a pattern only partially contained within the image boundary;</div>
<div class="claim-text">performing a search strategy for systematically generating a sequence of candidate poses of the search model, some of the candidate poses possibly resulting in a partial overlap of the search model and the image;</div>
<div class="claim-text">computing a match-quality metric for each candidate pose in the sequence of candidate poses to provide a plurality of metric results; and</div>
<div class="claim-text">using the plurality of metric results to determine the pose of the pattern in the image.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
    <div class="claim-text">11. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein computing the match-quality metric includes:
<div class="claim-text">computing an overlap region in the search model and an overlap region in the image, thereby ignoring model features of the search model that fall outside the boundary of the image, using the candidate pose of the model, the model boundary, and the image boundary; and</div>
<div class="claim-text">computing the metric using the model features within the overlap region in the model, and the image features within the overlap region in the image.</div>
</div>
  </div>
</div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES15939110" lang="EN" load-source="patent-office" class="description">
<heading>FIELD OF THE INVENTION</heading> <p num="p-0002">This invention relates to machine vision systems, and more particularly, to techniques for finding a pattern in an image.</p>
  <heading>BACKGROUND OF THE INVENTION</heading> <p num="p-0003">In many applications, it is necessary to determine the location or, more generally, the pose (multi-degree-of-freedom position) of an object in a scene using machine vision, and various techniques exist for performing such a search. Such techniques generally involve a method for systematically visiting a plurality of locations in an image, and at each location visited, performing a computation using a model and a match-quality metric. The match-quality metric represents the quality of the match between the model at that location (pose) in the image and the pattern in the image. For example, one commonly used match-quality metric is normalized (cross) correlation, which computes a score between equal-sized regions of two images using a particular formula, such as the formula on page 653 of Pratt, <i>Digital Image Processing, </i>2<sup>nd </sup>Ed., Wiley-Interscience. A simple high-level algorithm might compute the normalized correlation metric at all translations of the model image where the model image would fit entirely within the image of the scene, and the location would be the translation at which the score was greatest.</p>
  <p num="p-0004">A limitation that is typical of many pattern location algorithms, and of normalized-correlation-based pattern location algorithms in particular, is an inability to find the correct location if the object is not entirely contained within the field of view, and thus only part of the pattern occurs in the image.</p>
  <p num="p-0005">A fairly simple way to work around this limitation is to somehow extend the original image, where the amount of extension determines the range of additional candidate locations that could be compared. The image might be extended in a number of ways, e.g., by filling the extra region with pixels of a constant value or with pixels representing a periodic repetition of the image. However, these techniques suffer from the fundamental limitation that the match-quality metric will eventually be applied to pixel data which are not truly meaningful, thus leading to misleading match-quality metrics at these locations, and consequently to a possibly incorrect location found by the overall search algorithm.</p>
  <p num="p-0006">For example, consider the technique of extending the image by appending pixels of a constant value, and using normalized correlation as the metric. Even for a location where the model correctly matches the portion of a pattern that falls within the image, the normalized correlation match-quality metric will return a reduced value, because the region of the image that includes the pixels of a constant value usually does not match the portion of the model that extends beyond the image. It is similarly possible to report an abnormally high value if the model does happen to have a constant region at its border.</p>
  <p num="p-0007">In general, the metric is applied to the model at a given location (pose) in the image, where the pose may consist of just a translation, or may include additional degrees of freedom, such as rotation, scale, aspect, and shear, for example. In this general case, the search algorithm usually generates a set of possible poses of the model, and applies the metric at each pose. The usual limitation is that only poses of the model wherein the entire model overlaps the image can be considered. In the typical normalized correlation approach, the set of poses that would be considered would be all translations in integer pixel increments such that the model still falls within the image. Note that even when using normalized correlation as the metric, it is possible to consider more general poses, typically by digitally transforming the model (or image) pixels according to the pose (e.g., by rotating, scaling, and/or shearing), in addition to translations of the model.</p>
  <p num="p-0008">However, known approaches degrade or fail when the model extends beyond the boundary of the image, providing incorrect locations of the model.</p>
  <heading>SUMMARY OF THE INVENTION</heading> <p num="p-0009">A general aspect of the invention is a method for finding a whole pattern, where at least a portion of the whole pattern falls within the boundary of an image. The method includes, for each candidate pose of a search model of the whole pattern that results in a transformed search model that may extend beyond the boundary of the image, applying a match-quality metric to only a subset of search model features and corresponding image features, the subset being uniquely determined by the pose. In a preferred embodiment, the subset of model features is determined in accordance with at least a portion of the boundary of the image. In a further preferred embodiment, the subset of model features falls within an overlap region of the transformed search model and the image. The features can be pixels, edge elements, or higher level features, such as geometric features.</p>
  <p num="p-0010">In another general aspect, the invention is a method for finding a whole pattern, where at least a portion of the whole pattern falls within the boundary of an image. The method includes, for each candidate pose of a search model of the whole pattern that results in a transformed search model that may extend beyond the boundary of the image, computing a match-quality score by considering only features of the transformed search model that overlap the image. In a preferred embodiment, at least some of the features of the transformed search model that overlap the image also abut at least a portion of the boundary of the image. Again, the features can be pixels, edge elements, or higher level features, such as geometric features.</p>
  <p num="p-0011">In a further general aspect, the invention is a method for finding a pattern which may fall partially outside an image. The method includes providing a search model, the search model having a boundary. Then, providing an image, the image having a pattern at least partially contained within the boundary of the image. Next, performing a search strategy for systematically generating a sequence of poses of the search model, some of the poses possibly resulting in a partial overlap of the search model and the image. Then, computing a match-quality metric for each pose in the sequence of poses to provide a plurality of metric results, and then using the plurality of metric results to determine the pose of the pattern in the image. In a preferred embodiment, computing the match-quality metric includes computing the overlap region in the model and the overlap region in the image using the pose, the model boundary, and the image boundary. The metric is computed using the model features within the overlap region in the model, and the image features within the overlap region in the image.</p>
  <p num="p-0012">In accordance with the principles of the present invention, the set of model pixels, or more generally model features, which overlap the image given a specified model pose are determined, and the metric is computed using only the set of such overlapping model features. The features which do not overlap the image at that pose of the model are completely excluded from the metric computation. In this way, all of the relevant available information (where the model and image overlap) is used, and no arbitrarily hypothesized information (the supposed contents of the image outside of the actual given image) is used. Thus, the higher-level strategy of the search procedure is free to consider poses where the model extends partially outside the image, and the results of each metric computation will be the most true value possible.</p>
  <p num="p-0013">In a preferred embodiment, the match-quality metric is correlation (sometimes called “cross correlation”), and can be either normalized or not. In some preferred embodiments, the features are pixels, and the set of poses to be considered consist only of translations, the values of the translations being quantized, and measured as some integer value of a number of pixels. For these embodiments, there are a number of efficient techniques for computing the metric, especially if the set of poses is evaluated in a certain sequence, such as successive adjacent poses.</p>
  <p num="p-0014">In another embodiment, the metric used is correlation, whether normalized or not, the features used are the pixel values, and an arbitrary set of poses may be considered.</p>
  <p num="p-0015">In another embodiment, the metric used is the least-squares fitting error, the features used are higher-level features such as edges, and an arbitrary set of poses may be considered.</p>
<description-of-drawings> <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p num="p-0016">The invention will be more fully understood from the following detailed description, in conjunction with the following figures, wherein:</p>
    <p num="p-0017"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a flowchart illustrating the steps for determining a set of optimal model poses in accordance with the principles of the invention;</p>
    <p num="p-0018"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a diagram illustrating the overlapping image region for each of several translation-only poses of a model;</p>
    <p num="p-0019"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a diagram illustrating the overlapping image region for each of several model poses which include translation, rotation, and scale;</p>
    <p num="p-0020"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a diagram illustrating the rotation and scaling of a model; and</p>
    <p num="p-0021"> <figref idrefs="DRAWINGS">FIG. 5</figref> includes a sequence of FIGS. <i>a–d </i>illustrating a model with a repeated feature, and a set of images, each of which contain a partial instance of the model.</p>
  </description-of-drawings> <heading>DETAILED DESCRIPTION</heading> <p num="p-0022"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a flow diagram for the method of locating a pattern in an image using a model according to the present invention. The process begins with step <b>100</b>, wherein some information about the model may be precomputed, although not all metrics require such precomputation. This type of precomputation is sometimes referred to as “training”. For example, when using correlation as the metric, it is typical to precompute the sum of all model pixels and the sum of the squares of all model pixels. When correlation is to be used with rotation and scale as possible degrees of freedom in the search method that selects the poses to be evaluated using the metric, one might pre-generate rotated and scaled models. For other metrics, one might also perform some sort of feature extraction, such as performing edge detection. Some metrics may not require any precomputation. For example, one can do correlation with no precomputation, but usually doing the precomputation decreases execution time at runtime.</p>
  <p num="p-0023">The process then continues with step <b>110</b>, where a pose is generated for consideration. A pose is a mathematical transformation from coordinates in the model to coordinates in the image (and vice versa). If we represent coordinates in the model as (x, y) and coordinates in the image as (x′, y′), then a pose P is a function (or other mapping) where
<br>(<i>x′, y′</i>)=<i>P</i>(<i>x, y</i>)<br>
For example, if the pose represents a translation (x<sub>0</sub>, y<sub>0</sub>), then
<br>(<i>x′, y′</i>)=<i>P</i>(<i>x, y</i>)=(<i>x+x</i> <sub>0</sub> <i>, y+y</i> <sub>0</sub>)</p>
  <p num="p-0024">If the pose P represents a combination of a translation (x<sub>0</sub>, y<sub>0</sub>), rotation θ, scale S, aspect A, and shear K, then
<br>(<i>x′, y′</i>)=<i>P</i>(<i>x, y</i>)=<i>M</i>(θ, <i>S, A, K</i>)*(<i>x, y</i>)+(<i>x</i> <sub>0</sub> <i>, y</i> <sub>0</sub>)<br>
where M can be represented as a 2×2 matrix and the multiplication indicates multiplication of that matrix by the column vector (x, y), yielding
<br> <i>x′=</i>(<i>S </i>cos θ)<i>x+AS</i>(−sin θ−(cos θ)(tan <i>K</i>))<i>y </i> <br> <i>y′=</i>(<i>S </i>sin θ)<i>x+AS</i>(cos θ−(sin θ)(tan <i>K</i>))<i>y </i> </p>
  <p num="p-0025">There are many different possible pose generation strategies, and the present invention may be used with any of them. For example, a very simple strategy for the translation-only correlation case is to generate all poses at integer pixel translations such that the model at least partially overlaps the image. Without the present invention, note that one would typically be limited to considering the subset of such poses where the model completely overlaps the image. The goal of such “exhaustive” correlation is then to find the pose that generates the maximum correlation score, and that pose is reported as the location of the model. A more sophisticated strategy would use the match results for an intermediate set of poses to determine the next pose to be evaluated.</p>
  <p num="p-0026">When we speak of a model pixel or feature “overlapping” the image, we mean that the image coordinate (x′, y′), resulting from transforming the model coordinate (x, y) by the pose P as described above, lies within the boundaries of the image. Typically the boundaries of the image are those of a rectangle, so that the test for whether (x′, y′) lies within those boundaries is trivial for those skilled in the art. Note that an individual pixel actually occupies a range of coordinate values, but it usually suffices to represent the location of a pixel as the coordinate at its center, and to consider a pixel to be contained within a boundary if the center of the pixel is contained within the boundary.</p>
  <p num="p-0027">The present invention allows whatever pose generation strategy is used to consider poses where the model lies partially outside the image. Typically, some additional information will be used to determine reasonable limits for the possible poses. For example, a user might specify that only up to 25% of the model may lie outside the image; such inputs allow users to use application-specific knowledge to obtain optimal results. Usually some restriction, beyond the minimal restriction that the model at least partially overlap the image, is required in order to obtain good results. For example, the maximum possible range allows consideration of a pose where just a single corner pixel of the model overlapped the image. However, a correlation metric would be undefined for such a pose, and so the pose would typically not be considered.</p>
  <p num="p-0028">The precomputation phase of step <b>100</b> can also be used to analyze the spatial distribution of information in the model in order to determine the maximal range of poses that would allow for reliable application of the metric. For example, if a model consists of a uniform black square on a uniform white background, then it is clear that only poses for which at least part of the black square would overlap the image should be considered.</p>
  <p num="p-0029">The strategy may present the poses in any order, but some orders may allow for more computationally efficient processing. As will be discussed later, it can be advantageous in the translation-only correlation embodiments for the poses to be adjacent, i.e., each pose being shifted by one pixel in either the x-direction or the y-direction.</p>
  <p num="p-0030">In step <b>120</b>, the pose is used to determine the set of pixels or other features of the model that overlap the image. In general, one could compute the image coordinate (x′, y′) corresponding to a model feature at model coordinate (x, y) using the pose P as discussed above and then test whether (x′, y′) was contained within the image boundaries. Also as discussed above, one typically uses the coordinate of the center of a pixel for the test. However, because the model boundary is typically a rectangle (or possibly a parallelogram after transformation by the pose) and the image boundary is typically a rectangle, the overlap region can be computed by simple geometry, as is known to those skilled in the art. According to the present invention, it is this use of only overlapping features which allows for the most accurate metric to be computed, and consequently for the optimal pose to be determined.</p>
  <p num="p-0031">Also, it is typical to use the pose to determine the correspondence between features of the model and features of the image, by considering the model feature at location (x, y) to correspond to the image feature or features near (x′, y′), where various definitions of “near” are possible, as is known to those skilled in the art. For example, one might take the correspondence to use only the single image feature closest to (x′, y′) within some maximum distance. Note that if the pose is just an integer-pixel translation, and if the features are pixels, then the correspondence is trivial, namely that the single image pixel located exactly at (x′, y′) corresponds to the model pixel located at (x, y). Not all metrics require the correspondence to be determined, but most do. Correlation is an example which requires correspondence, and histogram matching, wherein a histogram is computed for the model image and for each pose searched in the image, is an example which does not require correspondence. Also note that some metrics may use the pose to generate an initial correspondence, which may then be refined before applying the metric proper. For example, when performing least-squares fitting to feature positions, the pose will be used to determine a correspondence, and the fitting will produce a refined pose as well as the fitting error, where the fitting error is the actual metric. The refined pose could then be used to generate a refined correspondence (possibly after recomputing the overlap for the refined pose) and thus a refined error; this procedure could be iterated until some convergence criterion was met, as is known to those skilled in the art.</p>
  <p num="p-0032"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a sketch showing a number of possible translation-only poses of a model <b>170</b> with respect to an image <b>180</b>. Each pose (<b>1</b>,<b>2</b>,<b>3</b>) is indicated in the diagram of <figref idrefs="DRAWINGS">FIG. 2A</figref> as a point in the image <b>180</b> at which the upper-left corner of the model should be placed.</p>
  <p num="p-0033">Pose <b>1</b> is an example of a pose for which the entire model lies within the image, as shown in <figref idrefs="DRAWINGS">FIG. 2B</figref>, and thus the set of overlapping features is the entire model, and the corresponding section of the image.</p>
  <p num="p-0034">Pose <b>2</b> is a pose for which the model lies partially outside the image to both the right and down, as shown in <figref idrefs="DRAWINGS">FIG. 2C</figref>, with the overlapping region <b>190</b> shaded and including a corner of the image. Note that the overlapping region in the image is near the lower right of the image, while the overlapping region in the model is near the upper left of the model.</p>
  <p num="p-0035">Pose <b>3</b> is a pose for which the model lies partially outside the image upwards, as shown in <figref idrefs="DRAWINGS">FIG. 2D</figref>, with the overlapping region <b>200</b> shaded and including just a linear boundary of the image.</p>
  <p num="p-0036">The match-quality metric is applied only to the overlapping region, such as one of the regions <b>190</b> or <b>200</b>, for example. Consider a metric such as correlation, for which each model pixel must correspond to a single image pixel. The model and image are typically rectangular sets of pixels; denote the model's width as w<sub>M </sub>and its height as h<sub>M</sub>, and denote the image's width and height as w<sub>I </sub>and h<sub>I</sub>, respectively. Denote each pixel in the model by M(x,y), where (x=0, y=0) is the upper-left corner of the model and (x=w<sub>M</sub>−1,y=h<sub>M</sub>−1) is the lower-right corner of the model; similarly, denote each pixel in the image by l(x,y), where (x=0, y=0) is the upper-left corner of the image and (x=w<sub>I</sub>−1,y=h<sub>I</sub>−1) is the lower-right corner of the image. For a given translation-only pose P=(p<sub>x</sub>, p<sub>y</sub>), the model pixel M(x,y) corresponds to the image pixel l(p<sub>x</sub>+x, p<sub>y</sub>+y). Assuming that the model is not larger than the image in either direction (i.e. w<sub>M</sub>≦w<sub>I </sub>and h<sub>M</sub>≦h<sub>I</sub>), then it is clear that the poses in the range (0, 0) to (w<sub>I</sub>−w<sub>M</sub>−1, h<sub>I</sub>−h<sub>M</sub>−1), inclusive, specify poses for which the model is entirely contained within the image; such poses will be referred to as total-overlap poses.</p>
  <p num="p-0037">Similarly, the complete set of all translation-only poses for which the model at least partially overlaps the image is the range (−w<sub>M</sub>+1, −h<sub>M</sub>+1) to (w<sub>I</sub>−1, h<sub>I</sub>−1), inclusive. Note that this range includes the set of all total-overlap poses. Poses in the larger set which are not in the total-overlap set will be referred to as partial-overlap poses. Note that the present invention also applies to cases where the model is larger than the image.</p>
  <p num="p-0038">For a total-overlap pose, the metric is applied to the entire model and the corresponding set of image features. For a partial-overlap pose, the metric is applied only over the region where the model and image overlap. For the translation-only case discussed above, this region is always a rectangle; a given pixel is in the overlap region if (x, y) is in the range [(0, 0), (w<sub>M</sub>−1,h<sub>M</sub>−1)] and if (P<sub>x</sub>+x, p<sub>y</sub>+y) is in the range [(0, 0), (w<sub>I</sub>−1,h<sub>1</sub>−1)].</p>
  <p num="p-0039">For clarity, let us consider two specific examples, where w<sub>M</sub>=150, h<sub>M</sub>=100, w<sub>I</sub>=600,h<sub>I</sub>=400. For the first example, let the pose be p<sub>x</sub>=50, p<sub>y</sub>=60, such as that illustrated for pose <b>1</b> in <figref idrefs="DRAWINGS">FIG. 2B</figref>. This is a total overlap case because the full rectangle of model pixels [(x=0, y=0), (x=149, y=99)] transforms to the rectangle of image pixels [(x′=50, y′=60), (x′=199, y′=159)], which is clearly contained within the full range of the image [(x′=0, y′=0), (x′=599, y′=399)]. For the second example, let the pose be p<sub>x</sub>=500, p<sub>y</sub>=340, such as that illustrated for pose <b>2</b> in <figref idrefs="DRAWINGS">FIG. 2C</figref>. In this case, the full rectangle of model pixels transforms to the rectangle of image pixels [(x′=500, y′=340), (x′=649, y′=439)], which is not completely contained by the full rectangle of the image. The overlap region in the image, which is the rectangle of image pixels contained within the boundaries of the image, is [(x′=500, y′=340), (x′=599, y′=399)], where the initial result has been clipped to the image boundaries, as will be clear to those skilled in the art. The overlap region in the model, which is the rectangle of model pixels corresponding to the overlap region in the image, is [(x=0, y=0), (x=99, y=59)].</p>
  <p num="p-0040">Note that in the above examples we simply used the integer pixel coordinates for the case of an integer-pixel translation; when using a more general pose, and thus using real-valued coordinates, one would consider the model boundary to be the rectangle [(0, 0), (150, 100)] and the image boundary to be the rectangle [(0, 0), (600, 400)], and the centers of the model pixels would be used as the (x, y) values.</p>
  <p num="p-0041">The present invention is not limited to translation-only poses. <figref idrefs="DRAWINGS">FIG. 3</figref> depicts three poses (pose <b>1</b>, pose <b>2</b>, pose <b>3</b>) which include rotation and scale in addition to translation. It will be clear to someone skilled in the art that computing the overlap region and correspondences, as shown in <figref idrefs="DRAWINGS">FIGS. 3A–3C</figref> in such a case is a matter of simple geometry. For correlation or other pixel-grid-based metrics, one would typically digitally transform either the model or the image, and then perform the overlap and correspondence calculations to the transformed model or image. For metrics which use features not so closely tied to the pixel grid, such transformation is typically fairly simple and computationally efficient.</p>
  <p num="p-0042"> <figref idrefs="DRAWINGS">FIG. 4</figref> includes an example of rotating and scaling a model from its initial pixel grid to a transformed one. <figref idrefs="DRAWINGS">FIG. 4A</figref> shows the original model. <figref idrefs="DRAWINGS">FIG. 4B</figref> shows the model after being scaled and rotated, with a grid <b>400</b> aligned with the model boundaries for clarity. <figref idrefs="DRAWINGS">FIG. 4C</figref> indicates that the resulting transformed model is stored in a new pixel grid which in general is no longer aligned with the model boundaries, and further indicates the enclosing pixel-grid-aligned rectangle that would typically be used to store the model. Note that it is common, although not necessary, to store a model as a rectangular array of pixels; because the transformed boundary of the model is also stored, only those pixels within the transformed boundary will ever be within a region of overlap with the image.</p>
  <p num="p-0043">Pixels within the enclosing rectangle but not within the model boundaries are not part of the model, and are therefore not included in the overlap set that is used to compute the metric. For example, one might keep track of the coordinate of the start and end pixel of the overlap pixel set for each row of the transformed model. Alternatively, one might simply keep track of the rotated and scaled rectangle and then determine the overlap pixels when needed.</p>
  <p num="p-0044">The pixels which are overlapping are those which lie within both the transformed model rectangle and the image rectangle; such an intersection is a convex polygon, and, as is known to those skilled in the art, testing the coordinate of a feature for containment within such a polygon is a simple matter. Note that, in general, one may transform either the model or the image, but that the model is typically transformed both because it is usually smaller, and because some information can be precomputed.</p>
  <p num="p-0045">For features not tied to the quantized pixel grid, the situation is usually much simpler. Such features are usually represented sparsely by their (x, y) coordinates, instead of by an exhaustive set or grid of feature values at all possible (x, y) locations, so that it is trivial to determine if the transformed (x, y) coordinate lies within the rectangular image. However, for such sparse features, the correspondence may be less clear, since one may want to allow nearby features to correspond even if they do not have the exact same position. For example, one might want the correspondence to use the closest feature within some maximum distance; considerably more complicated correspondence schemes are certainly possible. The present invention can be applied to any such scheme, since a central principle of the invention is to gain improvements in accuracy and computational efficiency by excluding all but the features in the overlap region from being considered in the correspondence determination.</p>
  <p num="p-0046">After the determination of the set of overlapping features and their correspondence in step <b>120</b>, the metric is applied to those features, and only those features, in step <b>130</b>. For correlation, this involves computing the sum of the model pixels, the sum of the squares of the model pixels, the sum of the image pixels, the sum of the square of the image pixels, the sum of the product of corresponding model and image pixels, and the number of model pixels, where the sum is performed only over the overlap region. Another metric that could be used is least-squares fitting error, where the fitting could allow various specified degrees of freedom; the possibilities include but are not limited to translation only, translation and rotation, translation and scale, or all three. Aspect and shear might also be included.</p>
  <p num="p-0047">Many other metrics are also possible and within the spirit and scope of the invention. For example, the sum of the absolute value of the difference between model pixels and their corresponding image pixels can be used as a metric; various types of correlation are also possible, including normalized, absolute, and relative.</p>
  <p num="p-0048">In step <b>140</b>, the strategy might choose to modify the result of the metric based on the nature of the match, e.g. the extent to which the match is a partial one. For example, the strategy might consider the true result to be the metric multiplied by the percentage of the model used in the overlap calculation, or to be some more complicated function of the overlap percentage. It might also use the particular pose, especially when the information in the model is not uniformly distributed. For example, if the model is a cross, then a partial-overlap pose in which only part of one arm of the cross is in the overlap region may produce a fairly unreliable result. Alternatively, the strategy might use the raw result of the metric. The choice of whether and how to modify the raw result of the metric is usually application specific, and possibly model specific.</p>
  <p num="p-0049"> <figref idrefs="DRAWINGS">FIG. 5</figref> includes a sequence of figures that illustrate an example of a model that presents a challenge to a search strategy. Consider, for example, the case where the model <b>500</b> consists of four identical evenly-spaced vertical bars, as shown in <figref idrefs="DRAWINGS">FIG. 5</figref> <i>a</i>. <figref idrefs="DRAWINGS">FIG. 5</figref> <i>b </i>shows an image in which only three evenly-spaced vertical bars are contained in the image <b>510</b>. P<b>1</b>, P<b>2</b>, and P<b>3</b> are partial-overlap poses of the model <b>500</b> which correspond to one, two, and three bars of the model <b>500</b> overlapping with the image <b>510</b>, respectively. If the bars in the image <b>510</b> are a perfect match to those in the model <b>500</b>, then it is likely that P<b>3</b> is the desired optimal pose, even though all three poses are a perfect match using the unmodified metric. In this case, i.e. when no image degradation is expected, the strategy might choose to weight the raw result by the overlap percentage. Thus, in <figref idrefs="DRAWINGS">FIG. 5</figref> <i>b</i>, P<b>3</b> would receive the most weight, since the overlap area is the greatest, and it would thus receive the heighest weighted metric result, since all three raw metric results are the same.</p>
  <p num="p-0050">Alternatively, consider <figref idrefs="DRAWINGS">FIG. 5</figref> <i>c</i>, in which the third bar <b>520</b> in the image <b>530</b> is no longer a perfect match to the model, in that it is an oval instead of a rectangle. If such a “bar” is actually a degraded instance of a bar in the model, then P<b>3</b> is probably still the desired optimal pose. If, however, such a bar is a confusing object, then P<b>2</b> is probably the desired optimal pose in that it excludes the confusing object <b>520</b>, and using the raw metric (or possibly weighting less heavily by the overlap percentage) would be preferable. However, if one uses the raw metric, P<b>1</b> and P<b>2</b> will seem equally good. There are a number of ways of dealing with this difficulty. If the user restricts the amount by which the model may lie outside the image, and if the restriction is chosen such that P<b>1</b> would exceed this amount, then using the raw metric suffices. Alternatively, the raw metric might be weighted by a power of the overlap percentage, where that power is less than 1. The choice of final metric would probably depend on the expected probabilities of degraded images versus confusing objects.</p>
  <p num="p-0051">Also consider <figref idrefs="DRAWINGS">FIG. 5</figref> <i>d</i>, where one group of three vertical bars and one group of two vertical bars appear in the image <b>540</b>. The group of three vertical bars is contained by the total-overlap region corresponding to total-overlap poses P<b>4</b> and P<b>5</b>, and the group of two vertical bars is contained by the partial-overlap region corresponding to the partial-overlap pose P<b>2</b>. P<b>4</b> and P<b>5</b> are equally good, but note that this situation is different from P<b>2</b>. P<b>2</b> is a perfect partial match because the contents of the overlap region in the image are identical to the contents of the overlap region in the model (two vertical bars in each), while P<b>4</b> and P<b>5</b> are imperfect total matches because the contents of the overlap region in the image contain only three bars, while the overlap region in the model [i.e. the entire model] contains four bars; it is likely that P<b>2</b> is the desired result. To handle such cases, the strategy should use the unweighted metric.</p>
  <p num="p-0052">Note that <figref idrefs="DRAWINGS">FIG. 5</figref> represents a fairly pathological case, used to emphasize some of the choices one may need to make when choosing an overall search strategy and modified metric. Models which do not have such degeneracies (e.g., repeated elements) tend to make the particular choice of modified metric less critical.</p>
  <p num="p-0053">In step <b>150</b>, the search strategy decides whether to continue generating and analyzing poses, or to quit. For example, the strategy might choose to quit as soon as the metric for a pose exceeded a certain value, or it might choose to quit only when the complete set of all possible poses had been evaluated. If it chooses not to quit, execution returns to step <b>110</b>, where the next pose is evaluated. If it chooses to quit, execution proceeds to step <b>160</b>, where the desired results are returned. The desired result might be, for example, just the one pose for which the metric was a maximum, or the set of all poses for which the metric exceeded some value, or the set of all poses and their metrics. Typically, the optimal pose is the one for which the metric is a maximum.</p>
  <p num="p-0054">It is worthwhile to consider how to efficiently implement the translation-only case for correlation. One will recall that the five values in the formula for normalized correlation are N (the number of model pixels in the overlap region [which is also the number of image pixels]), Sm (the sum of model pixels in the overlap region), Smm (the sum of squares of the model pixels in the overlap region), Si (the sum of image pixels in the overlap region), Sii (the sum of squares of image pixels in the overlap region), and Sim (the sum of the product of corresponding model and image pixels in the overlap region). For all total-overlap poses, N, Sm, and Smm are constant for a given model and are typically precomputed in step <b>100</b>, thus leaving only the calculation of Si, Sii, and Sim to be evaluated for each total-overlap pose.</p>
  <p num="p-0055">However, N, Sm, and Smm also vary for different overlap regions of the model and thus for different partial-overlap poses; note that it is possible for some different partial-overlap poses to correspond to the same overlap region in the model, as is illustrated by poses <b>4</b> and <b>5</b> in <figref idrefs="DRAWINGS">FIG. 2E</figref>. Thus a straightforward implementation will recompute all of these for each partial-overlap pose, causing execution time per pixel to increase substantially. N can be calculated trivially, since it is just the area of the overlap rectangle, but the per-pixel calculation will now include five additions and three multiplications instead of three additions and two multiplications. Note that the actual percentage increase in time can vary substantially depending on the computer chip instruction set and other factors, but it is clear that one can expect this straightforward minimal-precomputation approach to take approximately twice as long per pixel in the partial-overlap case compared to the total-overlap case. This minimal-precomputation approach is a reasonable one, but it is usually not the most efficient. Note that although one could use this approach to handle both partial-overlap and total-overlap cases, it is almost always desirable to treat the two separately for performance reasons. In the following discussion, it is assumed that the total-overlap case is always treated separately. [Thus we refer to this approach as “minimal-precomputation” instead of as “zero-precomputation”, since we typically still precompute the model sums for the total-overlap case.]</p>
  <p num="p-0056">There are a number of alternatives for more efficiently handling the partial-overlap cases. One could precompute Sm and Smm for all possible partial-overlap shifts. “Shift” is used to specify the set of model pixels that are in the overlap region, where the shift value is a unique label for each possible overlap region in the model; note that multiple poses may correspond to the same shift value. For example, all total-overlap poses correspond to a shift of (0, 0).</p>
  <p num="p-0057">The shift can be represented by the pair (s<sub>x</sub>, s<sub>y</sub>), where:
</p> <ul> <li id="ul0001-0001" num="0000"> <ul> <li id="ul0002-0001" num="0057">s<sub>x</sub>&lt;0 indicates that the overlap region in the model excludes the pixels for which x is in the range [0, −s<sub>x</sub>] (i.e. pixels at the left of the model are excluded).</li> <li id="ul0002-0002" num="0058">s<sub>x</sub>&gt;0 indicates that the overlap region in the model excludes the pixels for which x is in the range [w<sub>M</sub>−s<sub>x</sub>, w<sub>M</sub>−1] (i.e. pixels at the right of the model are excluded).</li> <li id="ul0002-0003" num="0059">s<sub>x</sub>=0 indicates that the overlap region in the model includes all values of x in the range [0, w<sub>M</sub>−1] (i.e. all values of x within the model are included).</li> <li id="ul0002-0004" num="0060">s<sub>y</sub>&lt;0 indicates that the overlap region in the model excludes the pixels for which y is in the range [0, −s<sub>y</sub>] (i.e. pixels at the top of the model).</li> <li id="ul0002-0005" num="0061">s<sub>y</sub>&gt;0 indicates that the overlap region in the model excludes the pixels for which y is in the range [h<sub>M</sub>−s<sub>y</sub>, h<sub>M</sub>−1] (i.e. pixels at the bottom of the model).</li> <li id="ul0002-0006" num="0062">s<sub>y</sub>=0 indicates that the overlap region in the model includes all values of y in the range [0, h<sub>M</sub>−1] (i.e. all values of y within the model are included).</li> </ul> </li> </ul> <p num="p-0058">As a specific example, consider again the second example discussed above for <figref idrefs="DRAWINGS">FIG. 2C</figref>, where we determined that the overlap region in the model was [(x=0, y=0), (x=99, y=59)], where the full region of the model was [(x=0, y=0), (x=149, y=99)]. The shift value used to represent this overlap region would be (s<sub>x</sub>=50, s<sub>y</sub>=40).</p>
  <p num="p-0059">In general, the shift value can be determined using the following procedure:
</p> <ul> <li id="ul0003-0001" num="0000"> <ul> <li id="ul0004-0001" num="0065">If p<sub>x</sub>&lt;0, then s<sub>x</sub>=p<sub>x</sub>,</li> <li id="ul0004-0002" num="0066">Else if p<sub>x</sub>&gt;w<sub>I</sub>−w<sub>M</sub>, then s<sub>x</sub>=p<sub>x</sub>−(w<sub>I</sub>−w<sub>M</sub>),</li> <li id="ul0004-0003" num="0067">Else s<sub>x</sub>=0</li> <li id="ul0004-0004" num="0068">If p<sub>y</sub>&lt;0, then s<sub>y</sub>=p<sub>y</sub>,</li> <li id="ul0004-0005" num="0069">Else if p<sub>y</sub>&gt;h<sub>I</sub>−h<sub>M</sub>, then s<sub>x</sub>=p<sub>y</sub>−(h<sub>I</sub>−h<sub>M</sub>) ,</li> <li id="ul0004-0006" num="0070">Else s<sub>y</sub>=0</li> </ul> </li> </ul> <p num="p-0060">This formulation uses coordinate systems where (x=0, y=0) is the upper-left corner of the model and (x′=0, y′=0) is the upper-left corner of the image; generalization to other coordinate systems is straightforward for one skilled in the art.</p>
  <p num="p-0061">The maximum set of shifts is [(−w<sub>M</sub>+1, −h<sub>M</sub>+1), (w<sub>M</sub>−1, h<sub>M</sub>−1)], where a shift of (0, 0) is the total-overlap case; the user might choose to further restrict this set, as discussed previously. In the maximal case, the number of possible shifts is (2w<sub>M</sub>−1) (2h<sub>M</sub>−1), or approximately (for simplicity) 4w<sub>M</sub>h<sub>M</sub>, i.e. four times the model area. If one considers the common case of an 8-bit [1 byte] model pixels and 32-bit [4 byte] values for Sm and Smm, then one would need approximately 4*(4+4)=32 times as much memory to store the precomputed values as are required to store the model itself. If one uses 64-bit values (either large integers or double-precision floating point) for Sm and Smm, one needs 64 times as much memory. For most applications, this is not a practical approach, although it may certainly be reasonable when memory is plentiful or when the set of possible shifts is restricted, e.g. if the user wishes to allow the model to be shifted at most one or two pixels off the image. Both the minimal-precomputation approach and this maximal-precomputation approach are within the spirit and scope of the invention.</p>
  <p num="p-0062">In a preferred embodiment, a balance is struck between these two extremes of precomputation by caching one or more sets of model sums for given shift values, representing the sums at poses previously requested by the strategy. In a typical case, the strategy usually generates successive poses at adjacent translations, or at least at nearby translations. For example, if one pose is (p<sub>x</sub>, p<sub>y</sub>), it is quite common for the next pose to be (p<sub>x</sub>+1, p<sub>y</sub>) or (p<sub>x</sub>, p<sub>y</sub>+1); this is certainly the pattern used by a search strategy when performing exhaustive correlation, and often occurs in other schemes as well. For two such successive adjacent poses, the set of model pixels differs by at most one row (or column), and may actually not differ at all (i.e. the shifts may be the same). Thus, if one caches the sums used at the previous pose, one need only compute the model sums for that row or column and add or subtract them from the cached values. Such a scheme causes a much smaller increase in time per pixel than does the minimal-precomputation approach, since only one row needs to be reevaluated, and it has a negligible increase in required memory relative to that needed in the maximal-precomputation approach.</p>
  <p num="p-0063">Note that this balanced approach generalizes to the case where the two successive poses differ by more than one pixel in one direction, or even if they differ in both directions simultaneously. One merely determines the set of pixels which differ between the model overlap regions implied by the two shift values, and then adds or subtracts the sums for that set from the cached sums. Note that if the two shifts differ substantially (especially if, e.g., s<sub>y </sub>is positive for one shift and negative for the other), it may be more efficient to simply recompute the sums from scratch at the new shift value.</p>
  <p num="p-0064">A more complex caching scheme can certainly be used to choose a different tradeoff in speed and memory. For example, one might cache the last several sets of sums, or one might precompute in step <b>100</b> the sums for a set of evenly-spaced shift values so that the difference between any given pose and the closest cached pose would be relatively small. All such schemes are within the spirit and scope of the invention.</p>
  <p num="p-0065">A software implementation of the above-described embodiment may comprise a series of computer instructions either fixed on a tangible medium, such as a computer readable medium, e.g. a diskette, a CD-ROM, a ROM memory, or a fixed disk, or transmissible to a computer system, via a modem or other interface device over a data communications medium. The medium either can be a tangible medium, including, but not limited to, optical or analog communications lines, or may be implemented with wireless techniques, including but not limited to microwave, infrared or other transmission techniques. It may also be the Internet. The series of computer instructions embodies all or part of the functionality previously described herein with respect to the invention. Those skilled in the art will appreciate that such computer instructions can be written in a number of programming languages for use with many computer architectures or operating systems. Further, such instructions may be stored using any memory technology, present or future, including, but not limited to, semiconductor, magnetic, optical or other memory devices, or transmitted using any communications technology, present or future, including but not limited to optical, infrared, microwave, or other transmission technologies. It is contemplated that such a computer program product may be distributed as removable media with accompanying printed or electronic documentation, e.g., shrink wrapped software, pre-loaded with a computer system, e.g., on system ROM or fixed disk, or distributed from a server or electronic bulletin board over a network, e.g., the Internet or World Wide Web.</p>
  <p num="p-0066">Although an exemplary embodiment of the invention has been disclosed, it will be apparent to those skilled in the art that various changes and modifications can be made which will achieve some of the advantages of the invention without departing from the spirit and scope of the invention. For example, although only a few strategies and metrics were explicitly discussed in the specification, those skilled in the art will appreciate that there are many other possible strategies and metrics that can be used for the purposes described herein. Use of any such embodiment for the purposes described herein is within the spirit and scope of the invention. Other aspects, such as the specific instructions utilized to achieve a particular function, as well as other modifications to the inventive concept are intended to be covered by the appended claims.</p>
</div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3069654">US3069654</a></td><td class="patent-data-table-td patent-date-value">Mar 25, 1960</td><td class="patent-data-table-td patent-date-value">Dec 18, 1962</td><td class="patent-data-table-td ">Hough Paul V C</td><td class="patent-data-table-td ">Method and means for recognizing complex patterns</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3816722">US3816722</a></td><td class="patent-data-table-td patent-date-value">Sep 28, 1971</td><td class="patent-data-table-td patent-date-value">Jun 11, 1974</td><td class="patent-data-table-td ">Nippon Electric Co</td><td class="patent-data-table-td ">Computer for calculating the similarity between patterns and pattern recognition system comprising the similarity computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3898617">US3898617</a></td><td class="patent-data-table-td patent-date-value">Feb 22, 1974</td><td class="patent-data-table-td patent-date-value">Aug 5, 1975</td><td class="patent-data-table-td ">Hitachi Ltd</td><td class="patent-data-table-td ">System for detecting position of pattern</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3936800">US3936800</a></td><td class="patent-data-table-td patent-date-value">Mar 27, 1974</td><td class="patent-data-table-td patent-date-value">Feb 3, 1976</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Pattern recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4115762">US4115762</a></td><td class="patent-data-table-td patent-date-value">Nov 30, 1977</td><td class="patent-data-table-td patent-date-value">Sep 19, 1978</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Alignment pattern detecting apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4200861">US4200861</a></td><td class="patent-data-table-td patent-date-value">Sep 1, 1978</td><td class="patent-data-table-td patent-date-value">Apr 29, 1980</td><td class="patent-data-table-td ">View Engineering, Inc.</td><td class="patent-data-table-td ">Pattern recognition apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4441205">US4441205</a></td><td class="patent-data-table-td patent-date-value">May 18, 1981</td><td class="patent-data-table-td patent-date-value">Apr 3, 1984</td><td class="patent-data-table-td ">Kulicke &amp; Soffa Industries, Inc.</td><td class="patent-data-table-td ">System for comparing groups of digitized data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4672676">US4672676</a></td><td class="patent-data-table-td patent-date-value">Dec 20, 1984</td><td class="patent-data-table-td patent-date-value">Jun 9, 1987</td><td class="patent-data-table-td ">International Business Machines Corp.</td><td class="patent-data-table-td ">Method and apparatus for automatically aligning an object with respect to a reference pattern</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4955062">US4955062</a></td><td class="patent-data-table-td patent-date-value">Jul 18, 1989</td><td class="patent-data-table-td patent-date-value">Sep 4, 1990</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Pattern detecting method and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5003166">US5003166</a></td><td class="patent-data-table-td patent-date-value">Nov 7, 1989</td><td class="patent-data-table-td patent-date-value">Mar 26, 1991</td><td class="patent-data-table-td ">Massachusetts Institute Of Technology</td><td class="patent-data-table-td ">Multidimensional range mapping with pattern projection and cross correlation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5020006">US5020006</a></td><td class="patent-data-table-td patent-date-value">May 3, 1989</td><td class="patent-data-table-td patent-date-value">May 28, 1991</td><td class="patent-data-table-td ">Hewlett-Packard Company</td><td class="patent-data-table-td ">Method for finding a reference point</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5040231">US5040231</a></td><td class="patent-data-table-td patent-date-value">Oct 17, 1988</td><td class="patent-data-table-td patent-date-value">Aug 13, 1991</td><td class="patent-data-table-td ">Raytheon Company</td><td class="patent-data-table-td ">Vertical vector pattern recognition algorithm</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5086478">US5086478</a></td><td class="patent-data-table-td patent-date-value">Dec 27, 1990</td><td class="patent-data-table-td patent-date-value">Feb 4, 1992</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Finding fiducials on printed circuit boards to sub pixel accuracy</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5168530">US5168530</a></td><td class="patent-data-table-td patent-date-value">Aug 29, 1988</td><td class="patent-data-table-td patent-date-value">Dec 1, 1992</td><td class="patent-data-table-td ">Raytheon Company</td><td class="patent-data-table-td ">Confirmed boundary pattern matching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5177559">US5177559</a></td><td class="patent-data-table-td patent-date-value">May 17, 1991</td><td class="patent-data-table-td patent-date-value">Jan 5, 1993</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Dark field imaging defect inspection system for repetitive pattern integrated circuits</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5226095">US5226095</a></td><td class="patent-data-table-td patent-date-value">May 26, 1992</td><td class="patent-data-table-td patent-date-value">Jul 6, 1993</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Method of detecting the position of an object pattern in an image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5253306">US5253306</a></td><td class="patent-data-table-td patent-date-value">Dec 31, 1990</td><td class="patent-data-table-td patent-date-value">Oct 12, 1993</td><td class="patent-data-table-td ">Futec Inc.</td><td class="patent-data-table-td ">Used for detecting a defect</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5313532">US5313532</a></td><td class="patent-data-table-td patent-date-value">Oct 28, 1991</td><td class="patent-data-table-td patent-date-value">May 17, 1994</td><td class="patent-data-table-td ">Massachusetts Institute Of Technology</td><td class="patent-data-table-td ">Recognition of patterns in images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5384711">US5384711</a></td><td class="patent-data-table-td patent-date-value">Nov 12, 1991</td><td class="patent-data-table-td patent-date-value">Jan 24, 1995</td><td class="patent-data-table-td ">Dainippon Screen Mfg. Co., Ltd.</td><td class="patent-data-table-td ">Method of and apparatus for inspecting pattern on printed board</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5513275">US5513275</a></td><td class="patent-data-table-td patent-date-value">Jun 9, 1994</td><td class="patent-data-table-td patent-date-value">Apr 30, 1996</td><td class="patent-data-table-td ">Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Automated direct patterned wafer inspection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5537669">US5537669</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 1993</td><td class="patent-data-table-td patent-date-value">Jul 16, 1996</td><td class="patent-data-table-td ">Kla Instruments Corporation</td><td class="patent-data-table-td ">Inspection method and apparatus for the inspection of either random or repeating patterns</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5550763">US5550763</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 2, 1994</td><td class="patent-data-table-td patent-date-value">Aug 27, 1996</td><td class="patent-data-table-td ">Michael; David J.</td><td class="patent-data-table-td ">Using cone shaped search models to locate ball bonds on wire bonded devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5568563">US5568563</a></td><td class="patent-data-table-td patent-date-value">May 3, 1994</td><td class="patent-data-table-td patent-date-value">Oct 22, 1996</td><td class="patent-data-table-td ">Mitsubishi Denki Kabushiki Kaisha</td><td class="patent-data-table-td ">Method and apparatus of pattern recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5586058">US5586058</a></td><td class="patent-data-table-td patent-date-value">Apr 21, 1992</td><td class="patent-data-table-td patent-date-value">Dec 17, 1996</td><td class="patent-data-table-td ">Orbot Instruments Ltd.</td><td class="patent-data-table-td ">Apparatus and method for inspection of a patterned object by comparison thereof to a reference</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5602937">US5602937</a></td><td class="patent-data-table-td patent-date-value">Jun 1, 1994</td><td class="patent-data-table-td patent-date-value">Feb 11, 1997</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Methods and apparatus for machine vision high accuracy searching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5613013">US5613013</a></td><td class="patent-data-table-td patent-date-value">May 13, 1994</td><td class="patent-data-table-td patent-date-value">Mar 18, 1997</td><td class="patent-data-table-td ">Reticula Corporation</td><td class="patent-data-table-td ">Glass patterns in image alignment and analysis</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5627915">US5627915</a></td><td class="patent-data-table-td patent-date-value">Jan 31, 1995</td><td class="patent-data-table-td patent-date-value">May 6, 1997</td><td class="patent-data-table-td ">Princeton Video Image, Inc.</td><td class="patent-data-table-td ">Pattern recognition system employing unlike templates to detect objects having distinctive features in a video field</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5708731">US5708731</a></td><td class="patent-data-table-td patent-date-value">Jul 2, 1996</td><td class="patent-data-table-td patent-date-value">Jan 13, 1998</td><td class="patent-data-table-td ">Nireco Corporation</td><td class="patent-data-table-td ">Pattern matching method with pixel vectors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5717785">US5717785</a></td><td class="patent-data-table-td patent-date-value">May 9, 1994</td><td class="patent-data-table-td patent-date-value">Feb 10, 1998</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and apparatus for locating patterns in an optical image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5825913">US5825913</a></td><td class="patent-data-table-td patent-date-value">Jul 18, 1995</td><td class="patent-data-table-td patent-date-value">Oct 20, 1998</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">System for finding the orientation of a wafer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5835622">US5835622</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 1995</td><td class="patent-data-table-td patent-date-value">Nov 10, 1998</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and apparatus to locate and measure capillary indentation marks on wire bonded leads</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5848189">US5848189</a></td><td class="patent-data-table-td patent-date-value">Mar 25, 1996</td><td class="patent-data-table-td patent-date-value">Dec 8, 1998</td><td class="patent-data-table-td ">Focus Automation Systems Inc.</td><td class="patent-data-table-td ">Method, apparatus and system for verification of patterns</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6002793">US6002793</a></td><td class="patent-data-table-td patent-date-value">Oct 26, 1994</td><td class="patent-data-table-td patent-date-value">Dec 14, 1999</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Machine vision method and apparatus for finding an object orientation angle of a rectilinear object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6005978">US6005978</a></td><td class="patent-data-table-td patent-date-value">Feb 7, 1996</td><td class="patent-data-table-td patent-date-value">Dec 21, 1999</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Robust search for image features across image sequences exhibiting non-uniform changes in brightness</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6023530">US6023530</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 13, 1995</td><td class="patent-data-table-td patent-date-value">Feb 8, 2000</td><td class="patent-data-table-td ">Applied Intelligent Systems, Inc.</td><td class="patent-data-table-td ">Vector correlation system for automatically locating patterns in an image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6035066">US6035066</a></td><td class="patent-data-table-td patent-date-value">Jun 2, 1995</td><td class="patent-data-table-td patent-date-value">Mar 7, 2000</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Boundary tracking method and apparatus to find leads</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6067379">US6067379</a></td><td class="patent-data-table-td patent-date-value">Nov 12, 1996</td><td class="patent-data-table-td patent-date-value">May 23, 2000</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and apparatus for locating patterns in an optical image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6154567">US6154567</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 1, 1998</td><td class="patent-data-table-td patent-date-value">Nov 28, 2000</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Pattern similarity metric for image search, registration, and comparison</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6246478">US6246478</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 25, 1999</td><td class="patent-data-table-td patent-date-value">Jun 12, 2001</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Reticle for an object measurement system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6385340">US6385340</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 4, 2000</td><td class="patent-data-table-td patent-date-value">May 7, 2002</td><td class="patent-data-table-td ">Applied Intelligent Systems, Inc.</td><td class="patent-data-table-td ">Vector correlation system for automatically locating patterns in an image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6532301">US6532301</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 18, 1999</td><td class="patent-data-table-td patent-date-value">Mar 11, 2003</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Object recognition with occurrence histograms</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6625303">US6625303</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 9, 2000</td><td class="patent-data-table-td patent-date-value">Sep 23, 2003</td><td class="patent-data-table-td ">Eastman Kodak Company</td><td class="patent-data-table-td ">Method for automatically locating an image pattern in digital images using eigenvector analysis</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="http://www.google.com/url?id=7oRvBAABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3DH0737893A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNEbRGrc5r_BH2OtBNoTEtjSbSNF3A">JPH0737893A</a></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">Title not available</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex MVS-8000 Series CVL Vision Tools Guide, Version 5.4, 2000, pp. 25-136.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Wallack, Aaron, "<a href='http://scholar.google.com/scholar?q="Algorithms+and+Techniques+for+Manufacturing%2C"'>Algorithms and Techniques for Manufacturing,</a>" University of California at Berkeley, 1995, pp. 97-335.</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7054492">US7054492</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 24, 2002</td><td class="patent-data-table-td patent-date-value">May 30, 2006</td><td class="patent-data-table-td ">Lee Shih-Jong J</td><td class="patent-data-table-td ">Fast regular shaped pattern searching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7822262">US7822262</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 17, 2004</td><td class="patent-data-table-td patent-date-value">Oct 26, 2010</td><td class="patent-data-table-td ">Kabushiki Kaisha Topcon</td><td class="patent-data-table-td ">Outer surface-inspecting method, master patterns used therefor, and outer surface-inspecting apparatus equipped with such a master pattern</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8345988">US8345988</a></td><td class="patent-data-table-td patent-date-value">Jun 22, 2005</td><td class="patent-data-table-td patent-date-value">Jan 1, 2013</td><td class="patent-data-table-td ">Sri International</td><td class="patent-data-table-td ">Method and apparatus for recognizing 3-D objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2006002299A2?cl=en">WO2006002299A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 22, 2005</td><td class="patent-data-table-td patent-date-value">Jan 5, 2006</td><td class="patent-data-table-td ">Rakesh Kumar</td><td class="patent-data-table-td ">Method and apparatus for recognizing 3-d objects</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S181000">382/181</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S199000">382/199</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S216000">382/216</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S154000">382/154</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009000000">G06K9/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009480000">G06K9/48</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009620000">G06K9/62</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009640000">G06K9/64</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0007000000">G06T7/00</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06T7/0046">G06T7/0046</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=7oRvBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/6203">G06K9/6203</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06T7/00P1M</span>, <span class="nested-value">G06K9/62A1A</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">May 14, 2013</td><td class="patent-data-table-td ">FPB1</td><td class="patent-data-table-td ">Expired due to reexamination which canceled all claims</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 23, 2013</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 23, 2009</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090424</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 16, 2009</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 5, 2001</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COGNEX TECHNOLOGY AND INVESTMENT CORPORATION, CALI</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:WAGMAN, ADAM;REEL/FRAME:012264/0529</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20011001</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U0jGxej2GRJ-DQHVjf0U9wUKjO2-A\u0026id=7oRvBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U3ARjyxyvKhL_9NKUVxf_xrsqqYFg\u0026id=7oRvBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U3la83G2tAuokrOYU_3H9rtiu5prQ","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Method_for_finding_a_pattern_which_may_f.pdf?id=7oRvBAABERAJ\u0026output=pdf\u0026sig=ACfU3U0zz_gt9CENC1dpbvx_0OtdAcT-ww"},"sample_url":"http://www.google.com/patents/reader?id=7oRvBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>