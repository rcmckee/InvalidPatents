<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US5832494 - Method and apparatus for indexing, searching and displaying data - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Method and apparatus for indexing, searching and displaying data"><meta name="DC.contributor" content="Daniel Egger" scheme="inventor"><meta name="DC.contributor" content="Shawn Cannon" scheme="inventor"><meta name="DC.contributor" content="Ronald D. Sauers" scheme="inventor"><meta name="DC.contributor" content="Libertech, Inc." scheme="assignee"><meta name="DC.date" content="1996-5-17" scheme="dateSubmitted"><meta name="DC.description" content="A computer research tool for indexing, searching and displaying data is disclosed. Specifically, a computer research tool for performing computerized research of data including textual objects in a database or a network and for providing a user interface that significantly enhances data presentation is described. Textual objects and other data in a database or network is indexed by creating a numerical representation of the data. The indexing technique called proximity indexing generates a quick-reference of the relations, patterns and similarity found among the data in the database. Proximity indexing indexes the data by using statistical techniques and empirically developed algorithms. Using this proximity index, an efficient search for pools of data having a particular relation, pattern or characteristic can be effectuated. The Computer Search program, called the Computer Search Program for Data represented in Matrices (CSPDM), provides efficient computer search methods. The CSPDM rank orders data in accordance with the data&#39;s relationship to time, a paradigm datum, or any similar reference. An alternative embodiment of the invention employs a cluster link generation algorithm which uses links and nodes to index and search a database or network. The algorithm searches for direct and indirect links to a search node and retrieves the nodes which are most closely related to the search node. The user interface program, called the Graphical User Interface (GUI), provides a user friendly method of interacting with the CSPDM program and prepares and presents a visual graphical display. The graphical display provides the user with a two or three dimensional spatial orientation of the data."><meta name="DC.date" content="1998-11-3" scheme="issued"><meta name="DC.relation" content="US:4868733" scheme="references"><meta name="DC.relation" content="US:5265065" scheme="references"><meta name="DC.relation" content="US:5287493" scheme="references"><meta name="DC.relation" content="US:5386556" scheme="references"><meta name="DC.relation" content="US:5471611" scheme="references"><meta name="DC.relation" content="US:5544352" scheme="references"><meta name="DC.relation" content="US:5546517" scheme="references"><meta name="DC.relation" content="US:5586311" scheme="references"><meta name="DC.relation" content="US:5630120" scheme="references"><meta name="DC.relation" content="US:5649193" scheme="references"><meta name="citation_reference" content="Agosti, et al., &quot;a Two-level Hypertext Retrieval Model for Legal Data,&quot; SIGIR &#39;91 (1991)."><meta name="citation_reference" content="Agosti, et al., a Two level Hypertext Retrieval Model for Legal Data, SIGIR 91 (1991)."><meta name="citation_reference" content="Belew, Richard, &quot;A Connectionist Approach to Conceptual Information Retrieval,&quot; ICAIL &#39;87 (1987)."><meta name="citation_reference" content="Belew, Richard, A Connectionist Approach to Conceptual Information Retrieval, ICAIL 87 (1987)."><meta name="citation_reference" content="Fowler, et al., &quot;Integrating Query, Thesaurus and Documents Through a Common Visual Representation, &quot; SIGIR &#39;91 (1991)."><meta name="citation_reference" content="Fowler, et al., Integrating Query, Thesaurus and Documents Through a Common Visual Representation, SIGIR 91 (1991)."><meta name="citation_reference" content="Gelbart &amp; Smith, &quot;Beyond Boolean Search: Flexicon, A Legal Text-Based Intelligent System,&quot; ICAIL &#39;91 (1991)."><meta name="citation_reference" content="Gelbart &amp; Smith, Beyond Boolean Search: Flexicon, A Legal Text Based Intelligent System, ICAIL 91 (1991)."><meta name="citation_reference" content="Lin, &quot;A Self-Organizing Semantic Map for Information Retrieval,&quot; SIGIR &#39;91 (1991)."><meta name="citation_reference" content="Lin, A Self Organizing Semantic Map for Information Retrieval, SIGIR 91 (1991)."><meta name="citation_reference" content="Rose &amp; Belew, &quot;Legal Information Retrieval: A Hybrid Approach,&quot; ICAIL &#39;89 (1989)."><meta name="citation_reference" content="Rose &amp; Belew, Legal Information Retrieval: A Hybrid Approach, ICAIL 89 (1989)."><meta name="citation_reference" content="Turtle &amp; Croft, &quot;Interference Networks for Document Retrieval,&quot; SIGR &#39;90 (1990)."><meta name="citation_reference" content="Turtle &amp; Croft, Interference Networks for Document Retrieval, SIGR 90 (1990)."><meta name="citation_reference" content="Uzzi, V Search Integration Toolkit for Folio Views, User s Manual, 6 Dec. 1995."><meta name="citation_reference" content="Uzzi, V Search Publisher s Toolkit, Beta Release 2.0, User s Manual, Dec. 8, 1995."><meta name="citation_reference" content="Uzzi, V-Search Integration Toolkit for Folio Views, User&#39;s Manual, 6 Dec. 1995."><meta name="citation_reference" content="Uzzi, V-Search Publisher&#39;s Toolkit, Beta Release 2.0, User&#39;s Manual, Dec. 8, 1995."><meta name="citation_patent_number" content="US:5832494"><meta name="citation_patent_application_number" content="US:08/649,304"><link rel="canonical" href="http://www.google.com/patents/US5832494"/><meta property="og:url" content="http://www.google.com/patents/US5832494"/><meta name="title" content="Patent US5832494 - Method and apparatus for indexing, searching and displaying data"/><meta name="description" content="A computer research tool for indexing, searching and displaying data is disclosed. Specifically, a computer research tool for performing computerized research of data including textual objects in a database or a network and for providing a user interface that significantly enhances data presentation is described. Textual objects and other data in a database or network is indexed by creating a numerical representation of the data. The indexing technique called proximity indexing generates a quick-reference of the relations, patterns and similarity found among the data in the database. Proximity indexing indexes the data by using statistical techniques and empirically developed algorithms. Using this proximity index, an efficient search for pools of data having a particular relation, pattern or characteristic can be effectuated. The Computer Search program, called the Computer Search Program for Data represented in Matrices (CSPDM), provides efficient computer search methods. The CSPDM rank orders data in accordance with the data&#39;s relationship to time, a paradigm datum, or any similar reference. An alternative embodiment of the invention employs a cluster link generation algorithm which uses links and nodes to index and search a database or network. The algorithm searches for direct and indirect links to a search node and retrieves the nodes which are most closely related to the search node. The user interface program, called the Graphical User Interface (GUI), provides a user friendly method of interacting with the CSPDM program and prepares and presents a visual graphical display. The graphical display provides the user with a two or three dimensional spatial orientation of the data."/><meta property="og:title" content="Patent US5832494 - Method and apparatus for indexing, searching and displaying data"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("lontU_ikKIvRsQTN54GwAQ"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("FRA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("lontU_ikKIvRsQTN54GwAQ"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("FRA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us5832494?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US5832494"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=8YhIBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS5832494&amp;usg=AFQjCNF8orLAX490A4Of3wMLh6gto2WdcQ" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US5832494.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US5832494.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US5832494" style="display:none"><span itemprop="description">A computer research tool for indexing, searching and displaying data is disclosed. Specifically, a computer research tool for performing computerized research of data including textual objects in a database or a network and for providing a user interface that significantly enhances data presentation...</span><span itemprop="url">http://www.google.com/patents/US5832494?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US5832494 - Method and apparatus for indexing, searching and displaying data</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US5832494 - Method and apparatus for indexing, searching and displaying data" title="Patent US5832494 - Method and apparatus for indexing, searching and displaying data"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US5832494 A</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 08/649,304</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Nov 3, 1998</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">May 17, 1996</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Jun 14, 1993</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/DE69431351D1">DE69431351D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69431351T2">DE69431351T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0704075A1">EP0704075A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0704075A4">EP0704075A4</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0704075B1">EP0704075B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5544352">US5544352</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6233571">US6233571</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7840524">US7840524</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8555196">US8555196</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20060242564">US20060242564</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1995000896A2">WO1995000896A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1995000896A3">WO1995000896A3</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">08649304, </span><span class="patent-bibdata-value">649304, </span><span class="patent-bibdata-value">US 5832494 A, </span><span class="patent-bibdata-value">US 5832494A, </span><span class="patent-bibdata-value">US-A-5832494, </span><span class="patent-bibdata-value">US5832494 A, </span><span class="patent-bibdata-value">US5832494A</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Daniel+Egger%22">Daniel Egger</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Shawn+Cannon%22">Shawn Cannon</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Ronald+D.+Sauers%22">Ronald D. Sauers</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Libertech,+Inc.%22">Libertech, Inc.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US5832494.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5832494.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5832494.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (10),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (18),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (189),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (28),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (21)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/5832494&usg=AFQjCNFRzcz3W3gvYJU7dJFV4kolAOWVpQ">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D5832494&usg=AFQjCNFL5kGgddzsxY1qpBdlB8pi66v3_Q">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D5832494A%26KC%3DA%26FT%3DD&usg=AFQjCNH2UzUYyyJbMsW8MKbeRSltZm2Gfg">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54365824" lang="EN" load-source="patent-office">Method and apparatus for indexing, searching and displaying data</invention-title></span><br><span class="patent-number">US 5832494 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA37836965" lang="EN" load-source="patent-office"> <div class="abstract">A computer research tool for indexing, searching and displaying data is disclosed. Specifically, a computer research tool for performing computerized research of data including textual objects in a database or a network and for providing a user interface that significantly enhances data presentation is described. Textual objects and other data in a database or network is indexed by creating a numerical representation of the data. The indexing technique called proximity indexing generates a quick-reference of the relations, patterns and similarity found among the data in the database. Proximity indexing indexes the data by using statistical techniques and empirically developed algorithms. Using this proximity index, an efficient search for pools of data having a particular relation, pattern or characteristic can be effectuated. The Computer Search program, called the Computer Search Program for Data represented in Matrices (CSPDM), provides efficient computer search methods. The CSPDM rank orders data in accordance with the data's relationship to time, a paradigm datum, or any similar reference. An alternative embodiment of the invention employs a cluster link generation algorithm which uses links and nodes to index and search a database or network. The algorithm searches for direct and indirect links to a search node and retrieves the nodes which are most closely related to the search node. The user interface program, called the Graphical User Interface (GUI), provides a user friendly method of interacting with the CSPDM program and prepares and presents a visual graphical display. The graphical display provides the user with a two or three dimensional spatial orientation of the data.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(56)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-1.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-1.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-2.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-2.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-3.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-3.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-4.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-4.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-5.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-5.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-6.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-6.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-7.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-7.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-8.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-8.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-9.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-9.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-10.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-10.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-11.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-11.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-12.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-12.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-13.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-13.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-14.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-14.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-15.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-15.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-16.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-16.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-17.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-17.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-18.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-18.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-19.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-19.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-20.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-20.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-21.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-21.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-22.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-22.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-23.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-23.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-24.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-24.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-25.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-25.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-26.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-26.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-27.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-27.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-28.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-28.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-29.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-29.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-30.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-30.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-31.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-31.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-32.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-32.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-33.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-33.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-34.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-34.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-35.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-35.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-36.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-36.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-37.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-37.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-38.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-38.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-39.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-39.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-40.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-40.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-41.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-41.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-42.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-42.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-43.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-43.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-44.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-44.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-45.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-45.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-46.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-46.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-47.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-47.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-48.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-48.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-49.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-49.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-50.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-50.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-51.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-51.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-52.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-52.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-53.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-53.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-54.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-54.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-55.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-55.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5832494-56.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5832494-56.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(33)</span></span></div><div class="patent-text"><div mxw-id="PCLM59494764" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is:</claim-statement> <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1. A method of analyzing a database with indirect relationships, using links and nodes, comprising the steps of:<div class="claim-text">selecting a node for analysis;</div> <div class="claim-text">generating candidate cluster links for the selected node, wherein the step of generating comprises an analysis of one or more indirect relationships in the database;</div> <div class="claim-text">deriving actual cluster links from the candidate cluster links;</div> <div class="claim-text">identifying one or more nodes for display; and</div> <div class="claim-text">displaying the identity of one or more nodes using the actual cluster links.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2. The method of claim 1 wherein each link is given a length, the step of generating the candidate cluster links comprises the steps of:<div class="claim-text">choosing a number as the maximum number of link lengths that will be examined; and</div> <div class="claim-text">examining only those links which are less than the maximum number of link lengths.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3. The method of claim 1 wherein the step of deriving actual cluster links comprises the step of:<div class="claim-text">selecting the top rated candidate cluster links, wherein the top rated candidate cluster links are those which are most closely linked to the node under analysis.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4. The method of claim 3 wherein the selecting step further comprises the step of:<div class="claim-text">calculating the top rated candidate links using the formula min(constant, 4*the number of direct links).</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5. The method of claim 1 wherein the step of generating the candidate cluster links comprises the step of:<div class="claim-text">eliminating candidate cluster links, wherein the number of candidate cluster links are limited and the closest candidate cluster links are chosen over the remaining links.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6. The method of claim 1 wherein the step of displaying further comprises the step of:<div class="claim-text">generating graphics to display the identity of the node, wherein a box is used to graphically represent the node.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7. The method of claim 1, wherein one or more nodes provide external connections to objects external to the database, the method further comprising the steps of:<div class="claim-text">activating the desired node; and</div> <div class="claim-text">accessing the external object linked to the node.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8. The method of claim 7, wherein the external object is an independent application which can be executed in background, the method further comprising the step of:<div class="claim-text">executing the independent application.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9. The method of claim 8, wherein one or more nodes provide links to more than one independent application which can be executed as an extension, the method further comprising the steps of:<div class="claim-text">displaying a list of independent applications linked to the node, wherein the step of accessing accesses an independent application.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" class="claim">
      <div class="claim-text">10. The method of claim 8, wherein the connection provides the independent application access to the information stored within the database.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" class="claim">
      <div class="claim-text">11. The method of claim 7, wherein the external connection is to another computer, wherein information is located that can be accessed, the step of accessing further comprising the step of:<div class="claim-text">accessing the information located within the computer.</div> </div>
    </div>
    </div> <div class="claim"> <div num="12" class="claim">
      <div class="claim-text">12. A method for determining the proximity of an object in a stored database to another object in the stored database using indirect relationships, links, and a display, comprising:<div class="claim-text">selecting an object to determine the proximity of other objects to the selected object;</div> <div class="claim-text">generating a candidate cluster link set for the selected object, wherein the generating step includes an analysis of one or more indirect relationships in the database;</div> <div class="claim-text">deriving an actual cluster link set for the selected object using the generated candidate cluster link set; and</div> <div class="claim-text">displaying one or more of the objects in the database, referred to in the actual cluster link set, on a display.</div> </div>
    </div>
    </div> <div class="claim"> <div num="13" class="claim">
      <div class="claim-text">13. The method of 12 wherein a set of direct links exists for the database, and wherein the step of generating a candidate cluster link set comprises:<div class="claim-text">recursively analyzing portions of the set of direct links for indirect links.</div> </div>
    </div>
    </div> <div class="claim"> <div num="14" class="claim">
      <div class="claim-text">14. A method for representing the relationship between nodes using stored direct links, paths, and candidate cluster links, comprising the steps of:<div class="claim-text">a) initializing a set of candidate cluster links;</div> <div class="claim-text">b) selecting the destination node of a path as the selected node to analyze;</div> <div class="claim-text">c) retrieving the set of direct links from the selected node to any other node in the database;</div> <div class="claim-text">d) determining the weight of the path using the retrieved direct links;</div> <div class="claim-text">repeating steps b through d for each path; and</div> <div class="claim-text">e) storing the determined weights as candidate cluster links.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" class="claim">
      <div class="claim-text">15. The method of claim 14 further comprising the step of deriving the actual cluster links wherein the actual cluster links are a subset of the candidate cluster links.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" class="claim">
      <div class="claim-text">16. The method of claim 15 wherein the step of deriving comprises the step of choosing the top rated candidate cluster links.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" class="claim">
      <div class="claim-text">17. The method of claim 14 wherein the stored direct links are length L, the paths are counted i=0 to N, the nodes are counted N<sub>0</sub> to N<sub>i+1</sub>, the weight's of the paths are stored as C<sub>i+1</sub>, and wherein the step of determining the weight of the path comprises the steps of:<div class="claim-text">i) creating a new path P' of length i+1 consisting of the path P plus the direct link L from the selected node to the node N<sub>i+1</sub>, for each direct link L;</div> <div class="claim-text">ii) calculating the stored weight of the path (C<sub>i+1</sub>) comprising the steps of:</div> <div class="claim-text">deciding whether there already is a path in the cluster link from Node<sub>0</sub> to Node<sub>i+1</sub> and a stored weight, wherein:<div class="claim-text">if there is a not already a path, the stored weight of the path (C<sub>i+1</sub>) is set equal to P';</div> <div class="claim-text">if there already is a path, the combined weight WC<sub>i+1</sub> is added to the already stored weight of the existing path (in C<sub>i+1</sub>);</div> <div class="claim-text">wherein the combined weight, WC<sub>i+1</sub>, is computed from the weight of the path P (WC<sub>i</sub>), a dampening factor (D<sub>i+1</sub>) and the weight of direct Link L (W<sub>i+1</sub>), and wherein the combined weight is computed using the following formula: WC<sub>i+1</sub> =min(WC<sub>i</sub>,D<sub>i+1</sub> *W<sub>i+1</sub>); and</div> </div> <div class="claim-text">iii) repeating steps i and ii for each direct link.</div> </div>
    </div>
    </div> <div class="claim"> <div num="18" class="claim">
      <div class="claim-text">18. A method of analyzing a database having objects and a first numerical representation of direct relationships in the database, comprising the steps of:<div class="claim-text">generating a second numerical representation using the first numerical representation, wherein the second numerical representation accounts for indirect relationships in the database;</div> <div class="claim-text">storing the second numerical representation;</div> <div class="claim-text">identifying at least one object in the database, wherein the stored numerical representation is used to identify objects; and</div> <div class="claim-text">displaying one or more identified objects from the database.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" class="claim">
      <div class="claim-text">19. The method of claim 18 wherein the step of generating a second numerical representation comprises:<div class="claim-text">selecting an object in the database for analysis;</div> <div class="claim-text">analyzing the direct relationships expressed by the first numerical representation for indirect relationships involving the selected object; and</div> <div class="claim-text">creating a second numerical representation of the direct and indirect relationships involving the selected object.</div> </div>
    </div>
    </div> <div class="claim"> <div num="20" class="claim">
      <div class="claim-text">20. The method of 18 wherein the step of identifying at least one object in the database comprises:<div class="claim-text">searching for objects in a database using the stored numerical representation, wherein direct and/or indirect relationships are searched.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" class="claim">
      <div class="claim-text">21. The method of claim 18 wherein the displaying step comprises:<div class="claim-text">generating a graphical display for representing an object in the database.</div> </div>
    </div>
    </div> <div class="claim"> <div num="22" class="claim">
      <div class="claim-text">22. A method of representing data in a computer database with relationships, wherein nodes or objects in a database are represented by boxes of a default box size, and wherein various information types may be assigned to node, node sub-types, links, and link sub-types to be placed within the box, and assigned information types contain information, comprising the steps of:<div class="claim-text">generating links, wherein each link represents a relationship between two nodes and is identified by the two nodes in which the relationship exists;</div> <div class="claim-text">allocating a weight to each link, wherein the weight signifies the strength of the relationship represented by the link relative to the strength of other relationships represented by other links;</div> <div class="claim-text">generating link sub-types;</div> <div class="claim-text">generating node sub-types;</div> <div class="claim-text">selecting anchor points within the boxes for each information type;</div> <div class="claim-text">placing each information type at their selected anchor point;</div> <div class="claim-text">determining whether the information of the placed information type overflows the default box size, comprising the step of:<div class="claim-text">adjusting the position of the anchor points; and</div> <div class="claim-text">adjusting the size of the box;</div> <div class="claim-text">determining whether a placed information type overlaps another placed information type within the same box comprising the steps of;</div> <div class="claim-text">adjusting the position of the anchor points; and</div> <div class="claim-text">adjusting the size of the box; and</div> <div class="claim-text">displaying the box.</div> </div> </div>
    </div>
    </div> <div class="claim"> <div num="23" class="claim">
      <div class="claim-text">23. A method of representing data in a computer database with relationships, comprising the steps of:<div class="claim-text">assigning nodes node identifications;</div> <div class="claim-text">generating links, wherein each link represents a relationship between two nodes and is identified by the two nodes in which the relationship exists;</div> <div class="claim-text">allocating a weight to each link, wherein the weight signifies the strength of the relationship represented by the link relative to the strength of other relationships represented by other links; and</div> <div class="claim-text">displaying a node identification.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="24" class="claim">
      <div class="claim-text">24. The method of claim 23, wherein the data in the database is objects, wherein the nodes represent objects and each object is assigned a node identification, and wherein the relationships that exist comprise direct relationships between objects, further comprising the step of:<div class="claim-text">searching generated links, wherein nodes are located by searching the generated links.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" class="claim">
      <div class="claim-text">25. The method of claim 23 further comprising the step of:<div class="claim-text">generating link sub-types, comprising the steps of:<div class="claim-text">identifying each link sub-type with a name; and</div> <div class="claim-text">providing a comment to one or more link sub-types.</div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="26" class="claim">
      <div class="claim-text">26. The method of claim 25 further comprising the step of:<div class="claim-text">specifying the place to display the comment using a comment place holder.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="27" class="claim">
      <div class="claim-text">27. The method of claim 26 wherein multiple comments are provided to a link sub-type, further comprising the step of:<div class="claim-text">specifying the order multiple comments appear in the comment place holder using a comment display order, comprising the steps of:<div class="claim-text">assigning each comment a value;</div> <div class="claim-text">ranking the comments in order of their assigned value; and</div> <div class="claim-text">displaying the comments in order of their rank.</div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="28" class="claim">
      <div class="claim-text">28. The method of claim 25 further comprising the step of:<div class="claim-text">determining whether the comment will appear in all displays using the always display command, comprising the steps of:<div class="claim-text">assigning each comment a binary value based on its importance;</div> <div class="claim-text">displaying comments which have been assigned the first binary value on all displays;</div> <div class="claim-text">suppressing comments which have been assigned the second binary value from all displays wherein only one node of the link subtype is displayed.</div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="29" class="claim">
      <div class="claim-text">29. The method of claim 26 wherein icon files are assigned to link sub-types.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="30" class="claim">
      <div class="claim-text">30. The method of claim 26 wherein visual styles are assigned to link sub-types.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="31" class="claim">
      <div class="claim-text">31. The method of claim 23 wherein attributes are assigned to nodes.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="32" class="claim">
      <div class="claim-text">32. The method of claim 31 further comprising the step of:<div class="claim-text">generating node sub-types wherein the node sub-types are assigned information.</div> </div>
    </div>
    </div> <div class="claim"> <div num="33" class="claim">
      <div class="claim-text">33. A method of representing data in a computer database and for computerized searching of the data, wherein relationships exist in the database, comprising:<div class="claim-text">assigning links to represent relationships in the database;</div> <div class="claim-text">generating node identifications based upon the assigned links, wherein node identifications are generated so that each link represents a relationship between two identified nodes;</div> <div class="claim-text">storing the links and node identifications, wherein the links and nodes may be retrieved;</div> <div class="claim-text">searching for node identifications using the stored links; and</div> <div class="claim-text">displaying node identifications, wherein the displayed node identifications are located in the searching step.</div> </div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES67199229" lang="EN" load-source="patent-office" class="description">
    <heading>RELATED APPLICATIONS</heading> <p>This application is a continuation-in-part of U.S. application Ser. No. 08/076,658, filed Jun. 14, 1993 with the same title, now U.S. Pat. No. 5,544,352.</p>
    <heading>TECHNICAL FIELD</heading> <p>This invention pertains to computerized research tools. More particularly, it relates to computerized research on databases. Specifically, the invention indexes data, searches data, and graphically displays search results with a user interface.</p>
    <heading>BACKGROUND</heading> <p>Two manuals containing background materials are hereby incorporated by reference: "V-Search Integration Tool Kit for Folio VIEWS", containing thirty-six (36) pages, and "V-Search Publisher's Tool Kit User's Manual", containing one hundred sixty (160) pages.</p>
    <p>Our society is in the information age. Computers maintaining databases of information have become an everyday part of our lives. The ability to efficiently perform computer research has become increasingly more important. Recent efforts in the art of computer research have been aimed at reducing the time required to accomplish research. Computer research on non-textual objects is very limited. Current computer search programs use a text-by-text analysis procedure (Boolean Search) to scan a database and retrieve items from a database. The user must input a string of text, and the computer evaluates this string of text. Then the computer retrieves items from the database that match the string of text. The two popular systems for computerized searching of data used in the legal profession are Westlaw™, a service sold by West Publishing Company, 50 W. Kellogg Blvd., P.O. Box 64526, St. Paul, Minn. 55164-0526, and Lexis™, a service sold by Mead Data Central, P.O. Box 933, Dayton, Ohio 45401.</p>
    <p>However, Boolean searches of textual material are not very efficient. Boolean searches only retrieve exactly what the computer interprets the attorney to have requested. If the attorney does not phrase his or her request in the exact manner in which the database represents the textual object, the Boolean search will not retrieve the desired textual object. Therefore, the researcher may effectively by denied access to significant textual objects that may be crucial to the project on which the researcher is working. A second problem encountered with Boolean searches is that the search retrieves a significant amount of irrelevant textual objects. (It should be noted that in the context of research, a textual object could be any type of written material. The term textual object is used to stress the fact that the present invention applies to all types of databases. The only requirement that a textual object must satisfy in order to be selected by a Boolean search program is that part of the textual object match the particular request of the researcher. Since the researcher cannot possibly know all of the groupings of text within all the textual objects in the database, the researcher is unable to phrase his request to only retrieve the textual objects that are relevant.</p>
    <p>Aside from the inefficiency of Boolean searches, the present systems for computerized searching of data are inadequate to serve the needs of a researcher for several other reasons. Even if one assumes that all the textual objects retrieved from a Boolean search are relevant, the listing of the textual objects as done by any currently available systems does not convey some important and necessary information to the researcher. The researcher does not know which textual objects are the most significant (i.e., which textual object is referred to the most by another textual object) or which textual objects are considered essential precedent (i.e., which textual objects describe an important doctrine).</p>
    <p>In the legal research field, both Westlaw™ and Lexis™ have a Shepardizing™ feature that enables the researcher to view a list of textual objects that mention a particular textual object. The shepardizing feature does not indicate how many times a listed textual object mentions the particular textual object. Although the shepardizing feature uses letter codes to indicate the importance of a listed textual object (e.g. an "f" beside a listed textual object indicates that the legal rule contained in particular textual object was followed in the listed textual object), data on whether a listed textual object followed the rule of a particular textual object is entered manually by employees of Shepard's™/McGraw Hill, Inc., Div. of McGraw-Hill Book Co., 420 N. Cascade Ave., Colorado Springs, Colo. 80901, toll free 1-800-525-2474. Such a process is subjective and is prone to error.</p>
    <p>Another legal research system that is available is the Westlaw™ key number system. The Westlaw™ key number system has problems similar to the shepardizing feature on the Lexis™ and Westlaw™ systems.</p>
    <p>The video displays of both the West™ and Lexis™ systems are difficult to use. The simple text displays of these systems do not provide a researcher with all the information that is available in the database.</p>
    <p>Computerized research tools for legal opinions and related documents are probably the most sophisticated computer research tools available and therefore form the background for this invention. However, the same or similar computer research tools are used in many other areas. For example, computer research tools are used for locating prior art for a patent application. The same problems of inefficiency discussed above exist for computer research tools in many areas of our society.</p>
    <p>What is needed is a system for computerized searching of data that is faster than the available systems of research.</p>
    <p>What is needed is a system for computerized searching of data that enables researchers to research in a manner in which they are familiar.</p>
    <p>What is needed is a computerized research tool that will reorganize, re-index or reformat the data into a more efficient format for searching.</p>
    <p>What is needed are more sophisticated methods to search data.</p>
    <p>What is needed is a system for computerized searching of data that will significantly reduce the number of irrelevant textual objects it retrieves.</p>
    <p>What is needed is a user friendly computerized research tool.</p>
    <p>What is needed is a visual user interface which can convey information to a user conveniently.</p>
    <p>What is needed is a system for computerized searching of data that easily enables the researcher to classify the object according to his or her own judgment.</p>
    <p>What is needed is a system for computerized searching of data that provides a visual representation of "lead" objects and "lines" of objects, permitting a broad overview of the shape of the relevant "landscape."</p>
    <p>What is needed is a system for computerized searching of data that provides an easily-grasped picture or map of vast amounts of discrete information, permitting researchers to "zero in" on the most relevant material.</p>
    <p>What is needed is a system for computer searching of data that provides a high degree of virtual orientation and tracking, the vital sense of where one has been and where one is going, and that prevent researchers from becoming confused while assimilating a large amount of research materials.</p>
    <p>Accordingly, there is an unanswered need for a user friendly computerized research tool. There is a need for "intelligent" research technology that emulates human methods of research. There is a need in the marketplace for a more efficient and intelligent computerized research tool.</p>
    <p>The present invention is designed to address these needs.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>This invention is a system for computerized searching of data. Specifically, the present invention significantly aids a researcher in performing computerized research on a database or a network. The invention simplifies the research task by improving upon methods of searching for data including textual objects and by implementing a user interface that significantly enhances the presentation of the data.</p>
    <p>The invention can be used with an existing database by indexing the data and creating a numerical representation of the data. This indexing technique called proximity indexing generates a quick-reference of the relations, patterns, and similarity found among the data in the database. Using this proximity index, an efficient search for pools of data having a particular relation, pattern or characteristic can be effectuated. This relationship can then be graphically displayed.</p>
    <p>There are three main components to the invention; a data indexing applications program, a Computer Search Program for Data Represented by Matrices ("CSPDM"), and a user interface. Each component may be used individually. Various indexing application programs, CSPDMs, and user interface programs can be used in combination to achieve the desired results. The data indexing program indexes data into a more useful format. The CSPDM provides efficient computer search methods. The preferred CSPDM includes multiple search subroutines. The user interface provides a user friendly method of interacting with the indexing and CSPDM programs. The preferred user interface program allows for easy entry of commands and visual display of data via a graphical user interface.</p>
    <p>The method which the invention uses to index textual objects in a database is called Proximity Indexing. This method can also be used to index objects located on a network. The application of this method to network domains is discussed in greater detail later in this specification. Proximity Indexing is a method of preparing data in a database for subsequent searching by advanced data searching programs. Proximity Indexing indexes the data by using statistical techniques and empirically developed algorithms. The resulting search by an advanced data searching program of the Proximity Indexed data is significantly more efficient and accurate than a simple Boolean search.</p>
    <p>The Proximity Indexing Application Program indexes (or represents) the database in a more useful format to enable the Computer Search Program for Data Represented by Matrices (CSPDM) to efficiently search the database. The Proximity Indexing Application Program may include one or more of the following subroutines, an Extractor, a Patterner, and a Weaver. The Proximity Indexing Application Program indexes (or represents) data in a locally located database or remotely located database. The database can contain any type of data including text, alphanumerics, or graphical information.</p>
    <p>In one embodiment, the database is located remotely from the Computer Processor and contains some data in the form of textual objects. The Proximity Indexing Application Program indexes the textual objects by determining how each full textual object (e.g., whole judicial opinion, statute, etc.) relates to every other full textual object by using empirical data and statistical techniques. Once each full textual object is related to each other full textual object, the Proximity Indexing Application Program compares each paragraph of each full textual object with every other full textual object as described above. The Proximity Indexing Application Program then clusters related contiguous paragraphs into sections. Subsequently, the Proximity Indexing Application Program indexes each section and the CSPDM evaluates the indexed sections to determine which sections to retrieve from the database. Such organization and classification of all of the textual objects in the database before any given search commences significantly limits the irrelevant textual objects that the CSPDM program retrieves during the subsequent search and allows retrieval of material based on its degree of relevancy.</p>
    <p>In a preferred embodiment, the Proximity Indexing Application Program includes a link generation subroutine wherein direct and indirect relationships between or among data is used to generate a representation of the data. Generally, direct and indirect relationships in the database are identified as links and placed in a table.</p>
    <p>Again, this method of computerized research can be used for nearly any database including those containing non-textual material, graphical material, newspapers material, data on personal identification, data concerning police records, etc.</p>
    <p>The remaining two programs in the present invention are the CSPDM and the GUI Program. The CSPDM has seven subroutines that each search for different pools of objects. The GUI Program also has seven subroutines. Each CSPDM subroutine performs a different type of search. Each of the subroutines of the GUI uses the results of the corresponding subroutine of the CSPDM to create the proper display on the display.</p>
    <p>After the Proximity Indexing Application Program indexes a database, the CSPDM application program is used to search the indexed database. For example, the CSPDM program can either be located in memory that is remote from the Computer Processor or local to the Computer Processor. In addition, the CSPDM program can either be remote or local in relation to the database.</p>
    <p>The subroutines of the CSPDM utilize the coefficients and other data created by the Proximity Indexing Application Program to facilitate its search. However, if the researcher does not have the particular object citation available, the researcher can perform a Boolean search to retrieve and organize a pool of objects. Alternatively, the researcher can subsequently search for related objects by using the Pool-Similarity Subroutine, the Pool-Paradigm Subroutine, the Pool-Importance Subroutine or the Pool-Paradigm-Similarity Subroutine as defined below.</p>
    <p>If the researcher already has the citation of a particular object available, the researcher can search for related objects by utilizing the Cases-In Subroutine, Cases-After Subroutine or Similar-Cases Subroutine. The Cases-In Subroutine retrieves all of the objects from the database to which a selected object refers. In addition, the subroutine determines the number of times the selected object refers to each retrieved object and other characteristics of each object, including its importance, and degree of relatedness to the selected object.</p>
    <p>The Cases-After Subroutine retrieves all of the objects from the database that refer to the selected object. Also, the subroutine determines the number of times each retrieved object refers to the selected object and other characteristics of each object, including its importance, and degree of relatedness to the particular object to which it refers.</p>
    <p>The Similar-Cases Subroutine determines the degree of similarity between the retrieved objects and the selected object. Similarity may be defined, in the context of legal cases, as the extent to which the two objects lie in the same lines of precedent or discuss the same legal topic or concept. Numerous other relationships may be used to define similarity.</p>
    <p>In addition, for a textual object, if the researcher does not know of a particular textual object on which to base his or her search, the researcher may execute a Boolean word search. After a standard Boolean word search has been run, the researcher may run the Pool-Similarity Subroutine to retrieve information containing the degree of similarity between each textual object in the pool and a particular textual object selected by the user. Similarly, the Pool-Importance Subroutine can be used to determine the degree of importance (i.e., whether a judicial opinion is a Supreme Court opinion or a District Court opinion) and other characteristics of each textual object retrieved using the Boolean word search.</p>
    <p>The Pool-Paradigm Subroutine calculates the geographic center in vector space of the pool of textual objects retrieved by the Boolean word search or other pool generating method. It then orders the retrieved textual objects by their degree of similarity to that center or "paradigm." The researcher can then evaluate this "typical textual object" and utilize it to help him or her find other relevant textual objects. In addition, the researcher can scan through neighboring "typical textual objects" to evaluate legal subjects that are closely related to the subject of the researcher's search.</p>
    <p>The Pool-Paradigm-Similarity Subroutine similarly creates a paradigm textual object from the retrieved textual objects. However, the subroutine calculates the similarity of all textual objects in the database to the paradigm textual object in addition to the similarity of the retrieved textual objects to the paradigm textual object.</p>
    <p>After the CSPDM has retrieved the desired objects, the Graphical User Interface (GUI) Program may be used to display the results of the search on the display. In one embodiment, the GUI is a user interface program. The GUI Program contains three main subroutines: Cases-In Display Subroutine (CIDS), Cases-After Display Subroutine (CADS) and Similar-Cases Display Subroutine (SCDS). The main subroutines receive information from the corresponding subroutines Cases-In, Cases-After and Similar-Case s of the CSPDM. The GUI Program also contains four secondary subroutines: Pool-Similarity Display Subroutine ("PSDS"), Pool-Paradigm Display Subroutine ("PPDS"), Pool-Importance Display Subroutine ("PIDS"), and the Pool-Paradigm-Similarity Subroutine (PPSDS). The secondary subroutines also receive information from the corresponding subroutines Pool-Similarity Subroutine, Pool-Paradigm Subroutine, Pool-Importance Subroutine and the Pool-Paradigm Similarity Subroutine of the CSPDM.</p>
    <p>The CIDS subroutine receives information gathered from the Cases-In Subroutine of the CSPDM. The CIDS subroutine displays user friendly active boxes and windows on the display which represent the textual objects retrieved from the database represented in Euclidean space. It can also use the boxes to represent objects retrieved from a network. Various active box formats and arranging of information within the boxes may be utilized. The display depicts the appropriate location of textual objects in Euclidean space on a coordinate means. An algorithm may be used to determine the appropriate location of the boxes. The coordinate means may have one or more axis. In one embodiment, the horizontal axis of the coordinate means may represent the time of textual object creation; the vertical axis could represent a weighted combination of the number of sections in which that particular retrieved text is cited or discussed, its degree of importance, and its degree of similarity to the host textual object and the depth axis (Z-axis) represents the existence of data and length of the textual data or object.</p>
    <p>The invention can also alter the background color of the window itself to communicate additional information graphically to the user. For example, if the horizontal axis represented time, then the invention could display the portion of the window containing objects occurring previous to the search object in one color and the portion containing the objects occurring after in another. Thus, the researcher can understand at a glance the relative position of his search target in relation to all the other objects related to it.</p>
    <p>CIDS also enables the researcher to open up various active boxes on the display by entering a command into the computer processor with the input means. After entering the proper command, the active box transforms into a window displaying additional information about the selected textual object. These windows can be moved about the display and stacked on top or placed beside each other via the input means to facilitate viewing of multiple windows of information simultaneously. In one embodiment, the windows are automatically arranged by the computer system. Since the number of textual objects retrieved in a single search may exceed the amount which could be displayed simultaneously, the GUI Program enables the researcher to "zoom in" or "zoom out" to different scales of measurement on both the horizontal and vertical axis.</p>
    <p>The CADS receives information gathered by the Cases-After Subroutine of the CSPDM. The CADS creates a display similar to the CIDS display. However, the active boxes representing the retrieved textual objects indicate which textual objects in the database refer to a selected textual object as opposed to which textual objects a selected textual object refers.</p>
    <p>The SCDS receives information gathered by the Similar-Cases Subroutine of the CSPDM. The SCDS causes a similar display on the display as the CIDS and the CADS except that the vertical axis indicates the degree of similarity between the retrieved textual objects and the selected textual object.</p>
    <p>The GUI Program contains four secondary subroutines: Pool-Search Display Subroutine (PSDS), Pool-Paradigm Display Subroutine (PPDS), Pool-Importance Display Subroutine (PIDS) and the Pool-Paradigm-Similarity Display Subroutine (PPSDS). The PSDS receives the results gathered by the Pool-Search Subroutine of the CSPDM. The PPDS receives the results gathered by the Pool-Paradigm Subroutine of the CSPDM. The PIDS receives the results gathered by the Pool-Importance Subroutine of the CSPDM. The PPSDS receives the results gathered by the Pool-Paradigm-Similarity Subroutine of the CSPDM. The results of the PSDS, PPDS, PIDS and PPSDS are then displayed in a user friendly graphical manner similar to the results of the CIDS, CADS and SCDS. A researcher can access the PSDS, PIDS, PSDS or PPSDS from any of the three main or four secondary subroutines of the GUI to gather information corresponding to the active boxes that represent the pool of textual objects retrieved by the corresponding subroutine of the CSPDM.</p>
    <p>By using the graphical display, the researcher can view immediately a visual representation of trends in the data (for example, trends developing in the law and current and past legal doctrines). In addition, the researcher can immediately identify important data or important precedent and which object serving as the precedent is most important to the project on which the researcher is working. This visual representation is a vast improvement over the current computerized research tools. Furthermore, the researcher using the present invention does not have to rely on the interpretation of another person to categorize different textual objects because the researcher can immediately visualize the legal trends and categories of law. In addition, new topic areas can be recognized without direct human intervention. The current research programs require a researcher to view objects in a database or to read through the actual text of a number of objects in order to determine which objects are important, interrelated, or most closely related to the topic at hand and which ones are not.</p>
    <p>It is an object of this invention to create an efficient and intelligent system for computerized searching of data that is faster than available systems of research.</p>
    <p>It is an object of the invention to integrate the system of computerized searching into the techniques to which researchers are already accustomed.</p>
    <p>It is an object of the invention to utilize statistical techniques along with empirically generated algorithms to reorganize, re-index and reformat data in a database into a more efficient model for searching.</p>
    <p>It is an object of the invention to utilize statistical techniques along with empirically generated methods to increase the efficiency of a computerized research tool.</p>
    <p>It is an object of the invention to create a system of computerized searching of data that significantly reduces the number of irrelevant objects retrieved.</p>
    <p>It is an object of this invention to create a user friendly interface for computer search tools which can convey a significant amount of information quickly.</p>
    <p>It is an object of the invention to enable the researcher to easily and immediately classify retrieved database objects according to the researcher's own judgment.</p>
    <p>It is an object of the invention to provide a visual representation of "lead" objects and "lines" of objects, permitting a broad overview of the shape of the relevant "landscape."</p>
    <p>It is an object of the invention to provide an easily-grasped picture or map of vast amounts of discrete information, permitting researchers to "zero in" on the most relevant material.</p>
    <p>It is an object of the invention to provide a high degree of virtual orientation and tracking that enables a researcher to keep track of exactly what information the researcher has already researched and what information the researcher needs to research.</p>
    <p>These and other objects and advantages of the invention will become obvious to those skilled in the art upon review of the description of a preferred embodiment, and the appended drawings and claims.</p>
    <heading>DESCRIPTION OF THE DRAWINGS</heading> <p>FIG. 1 is a high level diagram of the hardware for the system for computerized searching of data.</p>
    <p>FIG. 2 is high level diagram of the software for the system for computerized searching of data. The three main programs are the Proximity Indexing Application Program, the Computer Search Program for Data Represented by Matrices (CSPDM) Application Program and the Graphical User Interface (GUI) Program.</p>
    <p>FIG. 3A is a flow chart illustrating a possible sequence of procedures that are executed during the Proximity Indexing Application Program.</p>
    <p>FIG. 3B is a flow chart illustrating a possible sequence of the specific subroutines that are executed during one stage of the Proximity Indexing Application Program. The subroutines are the Initial Extractor Subroutine, Opinion Patterner Subroutine, the Opinion Weaver Subroutine, the Paragraph Patterner Subroutine (Optional), the Paragraph Weaver Subroutine and the Section Comparison Subroutine.</p>
    <p>FIG. 3C is flow chart illustrating a possible sequence of subroutines that are executed after the Section Comparison Subroutine. The Section Comparison Subroutine may comprise the Sectioner-Geographic Subroutine and the Section-Topical Subroutine (Optional). The sequence of subroutines executed after the Section Comparison Subroutine are the Section Extractor Subroutine, the Section Patterner Subroutine and the Section Weaver Subroutine.</p>
    <p>FIG. 3D is a high level flow chart illustrating a possible sequence of subroutines that comprise the Boolean Indexing Subroutine which are executed during another stage of the Proximity Indexing Application Program. The first two subroutines, Initialize Core English Words and Create p×w Boolean Matrix, are executed by the Initial Extractor Subroutine. The results are then run through the Pool-Patterner Subroutine, the Pool-Weaver Subroutine, the Pool-Sectioner Subroutine, the Section-Extractor Subroutine, the Section-Patterner Subroutine and the Section Weaver Subroutine.</p>
    <p>FIG. 3E is a chart illustrating the database format. The figure shows the types of structures contained within the database, links, link types, link subtypes, nodes, node types, node subtypes, and visual styles and also shows the various types of information that can be assigned to the links and nodes, including weights, identifications, names, comments, icons, and attributes.</p>
    <p>FIG. 3F is a high level diagram showing a sequence of nodes, N<sub>0</sub> -N<sub>3</sub>, connected by direct links which have weights W<sub>1</sub> -W<sub>3</sub> .</p>
    <p>FIG. 3G is a high level diagram showing a sequence of nodes, N<sub>1</sub> -N<sub>3</sub>,connected by direct and indirect links. The set of cluster links are also shown in the figure as functions of the weights associated with the direct links and the weight of the previous cluster link.</p>
    <p>FIG. 3H is a flow chart which depicts the Cluster Link Generation Algorithm.</p>
    <p>FIG. 4A is a high level diagram illustrating the flow of various search routines depending on the type of search initiated by the user by inputting commands to the Computer Processor via the input means. The diagram further illustrates the interaction between the CSPDM and the GUI Program.</p>
    <p>FIG. 4B is a high level flow chart illustrating the sequence of subroutines in the CSPDM program and user interactions with the subroutines.</p>
    <p>FIG. 4C is a high level flow chart for the Cases-In Subroutine.</p>
    <p>FIG. 4D is a high level flow chart for the Cases-After Subroutine.</p>
    <p>FIG. 4E is a high level flow chart for the Similar-Cases Subroutine.</p>
    <p>FIG. 4F is a high level flow chart for the Pool-Similarity Subroutine.</p>
    <p>FIG. 4G is a high level flow chart for the Pool-Paradigm Subroutine.</p>
    <p>FIG. 4H is a high level flow chart for the Pool-Importance Subroutine.</p>
    <p>FIG. 4I is a high level flow chart showing two possible alternate Pool-Paradigm-Similarity Subroutines.</p>
    <p>FIG. 5A is a high level diagram illustrating the interaction between respective subroutines of the CSPDM and of the GUI Program. The diagram further illustrates the interaction between the GUI Program and the display.</p>
    <p>FIG. 5B is an example of the display once the Cases-After Display Subroutine (CADS) is executed.</p>
    <p>FIG. 5C is an example of the display after a user selects an active box representing a textual object retrieved by the Cases-After Subroutine and chooses to open the "full text" window relating to the icon.</p>
    <p>FIG. 5D is an example of the display once the Cases-In Display Subroutine (CIDS) is executed.</p>
    <p>FIG. 5E is an example of the display once the Similar-Cases Display Subroutine (SCDS) is executed.</p>
    <p>FIG. 5F is an example of the display after a user chooses to execute the Similar Cases Subroutine for a textual object retrieved by the Similar-Cases Subroutine represented in FIG. 5E.</p>
    <p>FIG. 5G is an example of the display after a user chooses to execute the Similar Cases Subroutine for one of the cases retrieved by the Similar-Cases Subroutine represented in FIG. 5F.</p>
    <p>FIG. 5H depicts an Executive Search Window.</p>
    <p>FIG. 6 depicts a schematic representation of eighteen patterns.</p>
    <p>FIG. 7 is a high level diagram of the Layout of Boxes Algorithm.</p>
    <p>FIG. 8 is a diagram of a screen showing execution of a show usage command.</p>
    <p>FIG. 9 is a diagram of the Internal Box Layout Algorithm.</p>
    <p>FIG. 10A is a diagram of a screen showing an Influence Map, which is a screen used in one embodiment of this invention.</p>
    <p>FIG. 10B is a diagram of a screen showing a Source Map, which is a screen used in one embodiment of this invention.</p>
    <p>FIG. 10C is a diagram of a screen showing a Cluster Map, which is a screen used in one embodiment of the invention.</p>
    <p>FIG. 11 depicts a Look-Up Table for Bitmaps.</p>
    <p>FIG. 12 is a software flow chart for the auto arranging window feature.</p>
    <p>FIG. 13A is a depiction of a display with vertically tiled windows.</p>
    <p>FIG. 13B is a depiction of a display with horizontally tiled windows.</p>
    <p>FIG. 14A is a high level diagram of a method for searching, indexing, and displaying data stored in a network.</p>
    <p>FIG. 14B is a high level diagram of a method for searching, indexing, and displaying data stored in a network using the cluster generation algorithm.</p>
    <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT</heading> <p>Referring now to the drawings, the preferred embodiment of the present invention will be described.</p>
    <p>FIG. 1 is an overview of the preferred embodiment of the hardware system 26 for computerized searching of data. The hardware system 26 comprises a Computer Processor 30, a database 54 for storing data, input means, display 38, and RAM 34.</p>
    <p>The Computer Processor 30 can be a processor that is typically found in Macintosh computers, IBM computers, portable PCs, clones of such PC computers (e.g. Dell computers), any other type of PC, or a processor in a more advanced or more primitive computing device. Parallel processing techniques may also be utilized with this invention.</p>
    <p>The database 54 is connected to the Computer Processor 30 and can be any device which will hold data. For example, the database 54 can consist of any type of magnetic or optical storing device for a computer. The database 54 can be located either remotely from the Computer Processor 30 or locally to the Computer Processor 30. The preferred embodiment shows a database 54 located remotely from the Computer Processor 30 that communicates with the personal computer 28 via modem or leased line. In this manner, the database 54 is capable of supporting multiple remote computer processors 50. The preferred connection 48 between the database 54 and the Computer Processor 30 is a network type connection over a leased line. It is obvious to one skilled in the art that the database 54 and the Computer Processor 30 may be electronically connected in a variety of ways. In the preferred embodiment the database 54 provides the large storage capacity necessary to maintain the many records of textual objects.</p>
    <p>The input means is connected to the Computer Processor 30. The user enters input commands into the Computer Processor 30 through the input means. The input means could consist of a keyboard 46, a mouse 42, or both working in tandem. Alternatively, the input means could comprise any device used to transfer information or commands from the user to the Computer Processor 30.</p>
    <p>The display 38 is connected to the Computer Processor 30 and operates to display information to the user. The display 38 could consist of a computer monitor, television, LCD, LED, or any other means to convey information to the user.</p>
    <p>The Random Access Memory (RAM 34) is also connected to the Computer Processor 30. The software system 60 for computerized searching of data may reside in the RAM 34, which can be accessed by the Computer Processor 30 to retrieve information from the software routines. A Read Only Memory (ROM), Erasable Programmable Read Only Memory (EPROM), disk drives, or any other magnetic storage device could be used in place of the RAM 34. Furthermore, the RAM 34 may be located within the structure of the Computer Processor 30 or external to the structure.</p>
    <p>The hardware system 26 for computerized searching of data shown in FIG. 1 supports any one, or any combination, of the software programs contained in the software system 60 for computerized searching of data. The software system 60 for the computerized searching of data comprises one or more of the following programs: the Proximity Indexing Application Program 62, the Computer Search Program for Data Represented by Matrices (CSPDM 66) and the Graphical User Interface (GUI) Program. The Proximity Indexing Application Program 62 could reside in RAM 34 or in separate memory connected to the database 54. The Computer Processor 30 or a separate computer processor 50 attached to the database 54 could execute the Proximity Indexing Application Program 62. In the preferred embodiment the Proximity Indexing Application Program 62 resides in separate memory that is accessible to the database 54, and a separate computer processor 50 attached to the database 54 executes the Proximity Indexing Application Program 62.</p>
    <p>The CSPDM 66 could reside in the RAM 34 connected to the Computer Processor 30 or in the separate memory connected to the database 54. In the preferred embodiment, the CSPDM 66 is located in the RAM 34 connected to the Computer Processor 30. This is also the preferred embodiment for the application of this method to network searching. For network application, a separate database 54 storing information to be analyzed is remodeling connected to the computer processor 30. The CSPDM 66 may use the display 38 to depict input screens for user entry of information.</p>
    <p>The GUI Program 70 could likewise reside in the RAM 34 connected to the Computer Processor 30 or in separate memory connected to the database 54. In the preferred embodiment, the GUI Program 70 is located in the RAM 34 connected to the Computer Processor 30. The GUI Program 70 also communicates with the display 38 to enhance the manner in which the display 38 depicts information.</p>
    <p>FIG. 2 is an overview of the preferred embodiment of the software system 60 for computerized searching of data. The software system 60 for computerized searching of data comprises at least one or more of the following programs: the Proximity Indexing Application Program 62, the Computer Search Program for Data Represented by Matrices (CSPDM 66) and the Graphical User Interface (GUI) Program. Proximity Indexing is a method of identifying relevant data by using statistical techniques and empirically developed algorithms. (See Appendix # 2) The Proximity Indexing Application Program 62 is an application program which represents or indexes the database 54 to a proper format to enable the Computer Search Program for Data Represented by Matrices (CSPDM 66) to properly search the database 54. The Proximity Indexing Application Program 62 can index data in a local database 54 or a remote database 54. The Proximity Indexing Application Program 62 is shown in more detail in FIGS. 3A to 3H.</p>
    <p>After the Proximity Indexing Application Program 62 indexes the database 54, the CSPDM 66 application program can adequately search the database 54. The CSPDM 66 program searches the database 54 for objects according to instructions that the user enters into the Computer Processor 30 via the input means. The CSPDM 66 then retrieves the requested objects. The CSPDM 66 either relays the objects and other information to the GUI program in order for the GUI program to display this information on the display 38, or the CSPDM 66 sends display commands directly to the Computer Processor 30 for display of this information. However, in the preferred embodiment, the CSPDM 66 relays the objects and other commands to the GUI Program 70. The CSPDM 66 is described in more detail in FIGS. 4A to 4I.</p>
    <p>After the CSPDM 66 has retrieved the objects, the Graphical User Interface (GUI) Program, which is a user interface program, causes the results of the search to be depicted on the display 38. The GUI Program 70 enhances the display of the results of the search conducted by the CSPDM 66. The GUI Program 70, its method and operation, can be applied to other computer systems besides a system for computerized searching of data. The GUI Program 70 is described in more detail in FIGS. 5A to 5H.</p>
    <p>FIGS. 3A to 3D depict examples of the procedures and subroutines of a Proximity Indexing Application Program 62, and possible interactions among the subroutines. FIG. 3A depicts a sequence of procedures followed by the Proximity Indexing Application Program 62 to index textual objects for searching by the CSPDM 66. FIG. 3B depicts specific subroutines that the Proximity Indexing Application Program 62 executes to partition full textual objects into smaller sections. FIG. 3C depicts subroutines executed by the Section Comparison Routine of FIG. 3B and subsequent possible subroutines to format and index the sections. FIG. 3D depicts a sequence of subroutines of the Proximity Indexing Application Program 62 which first sections and then indexes these sections of "core english words" 140 contained in the database 54. "Core english words" 140 are words that are uncommon enough to somewhat distinguish one textual object from another. The word searches of the CSPDM 66 search these sections of core English words to determine which textual objects to retrieve.</p>
    <p>FIGS. 3E-3H show a preferred embodiment for representing the data in a database 54 or documents in a network in accordance with the present invention. The application of this method for representing documents on a network is described in greater detail later in this specification.</p>
    <p>FIG. 3E shows a method for representing the data using the present invention. Specifically, FIG. 3E shows a method in which links 2004 and nodes 2008 can be used along with link types 2012, link subtypes 2020, node types 2016 and node subtypes 2024 to represent the data.</p>
    <p>A node 2008 is any entity that can be represented by a box on a display 38 such as a GUI 70. A node 2008 might be for example, an object in a database 54, a portion of an object in a database 54, a document, a section of a document, a World Wide Web page, or an idea or concept, such as a topic name. A node 2008 need not represent any physical entity such as an actual document. It is preferred that a node 2008 have links 2004, specifically, it is preferred that a node 2008 have links to other nodes 2008 (for example source links (a source link is a link 2004, or influence links (an influence link is a link 2004)). A node 2008 can represent any idea or concept that has links to other ideas or concepts. For example, two nodes 2008 can exist such as a node 2008 called Modern Architecture (not shown) and a node 2008 called Classical Architecture (not shown) and the links would show that Classical Architecture is a source for Modern Architecture and that Modern Architecture is influenced by Classical Architecture. In this example, a source link 2004 and an influence link 2004 would exist between the two nodes 2008. (Many times, links 2004 represent inverse relationships such as source links 2004 and influence links 2004, and one type of link may be derived or generated from analysis of another link.)</p>
    <p>More specifically, in the preferred embodiment, the software defines a node 2008 as something that has a unique node 2008 identification, a node type 2016, a node subtype 2024, and an associated date (or plot date). Node types 2016 or subtypes may have names 2021 or identifications, title descriptors 2026 and external attributes. A node 2008 may have a corresponding numerical representation assigned, a vector, a matrix, or a table. In the preferred embodiment a table format is used for the nodes.</p>
    <p>Referring to FIG. 3E, 3F, and 3G a link 2004 is another name or identification for a relationship between two nodes 2008. The relationship may be semantical, non-semantical, stated, implied, direct 2032, indirect 2036, actual, statistical and/or theoretical. A link 2004 can be represented by a vector or an entry on a table and contain information for example, a from-node identification 2010 (ID), a to-node ID 2010, a link type 2012, and a weight 2034. A group of links 2004 may be represented by a series of vectors or entries in a table, a link table. Link subtypes 2020 may be used, named and assigned comments.</p>
    <p>In addition, to better integrate the GUI 70 and the data representation, visual styles 2028 may be assigned for example to nodes 2008, links 2004, link types 2012, and link subtypes 2020 to assist in the visual displays 38.</p>
    <p>In the preferred embodiment, three types of links 2004 are used: source links 2004, influence links 2004 and cluster links 2004. Source links 2004 generally link a first node 2008 to second node 2008 that represents information or documentation specifically cited or referred to by the first node 2008. Influence links 2004 are generally the inverse of a source link 2004. The relationships represented by these links 2004 may be explicit or implied.</p>
    <p>Links 2004 and nodes 2008 may be manually entered by a user or automatically generated by a computer 30. It is preferred that cluster links 2004 be generated automatically by a processor. A cluster link 2004 is a relationship between two nodes 2008, for example, two nodes 2008 both directly linked to the same intermediate nodes 2008, may be indirectly linked through many paths and therefore have a cluster link 2004 between them. The cluster links 2004 may be determined using the specific or general methods described later for finding relationships in a database 54. However, the preferred method is through using a Proximity Indexing Application Program.</p>
    <p>"Proximity indexing" is a method of indexing that uses statistical techniques and empirically generated algorithms to organize and categorize data stored in databases or on a network. The Proximity Indexing Application Program 62 applies the Proximity indexing method to a database 54. One embodiment of the present invention uses the Proximity Indexing Application Program 62 to Proximity index textual objects used for legal research by indexing objects based on their degree of relatedness--in terms of precedent and topic--to one another.</p>
    <p>Applying the method to legal research, the "Proximity indexing" system treats any discrete text as a "textual object." Textual objects may contain "citations," which are explicit references to other textual objects. Any legal textual object may have a number of different designations of labels. For example, 392 U.S. 1, 102 S.Ct 415, 58 U.S.L.W. 1103, etc. may all refer to the same textual object.</p>
    <p>Cases are full textual objects that are not subsets of other textual objects. Subjects of a full textual object include words, phrases, paragraphs, or portions of other full textual objects that are referred to in a certain full textual object. (The system does not treat textual objects as subsets of themselves.)</p>
    <p>Every case, or "full" textual object, is assigned a counting-number "name"--designated by a letter of the alphabet in this description--corresponding to its chronological order in the database 54. Obviously, textual objects may contain citations only to textual objects that precede them. In other words, for full textual objects, if "B cites A," (i.e. "A is an element of B" or "the set `B` contains the name `A`"), textual object A came before B, or symbolically, A&lt;B. Every textual object B contains a quantity of citations to full textual objects, expressed as Q(B), greater than or equal to zero, such that Q(B)&lt;B.</p>
    <p>Textual objects other than full textual objects may be subsets of full textual objects and of each other. For example, a section, page, or paragraph of text taken from a longer text may be treated as a textual object. Phrases and words are treated as a special kind of textual object, where Q(w)=0. Sections, pages, and paragraphs are generally subsets of only one full textual object, and may be organized chronologically under the numerical "name" of that full textual object. For purposes of chronology, phrases and words are treated as textual objects that precede every full textual object, and can generally be treated as members of a set with name "0," or be assigned arbitrary negative numbers.</p>
    <p>Any two textual objects may be related to each other through a myriad of "patterns." Empirical research demonstrates that eighteen patterns capture most of the useful relational information in a cross-referenced database 54. A list of these eighteen patterns, in order of importance, follows:</p>
    <p>Given that:</p>
    <p>a,b,c&lt;A;</p>
    <p>A&lt;d, e, f&lt;B; and</p>
    <p>B&lt;g, h, i.</p>
    <p>Patterns Between A and B Include</p>
    <p>1. B cites A.</p>
    <p>2. A cites c, and B cites c.</p>
    <p>3. g cites A, and g cites B.</p>
    <p>4. B cites f, and f cites A.</p>
    <p>5. B cites f, f cites e, and e cites A.</p>
    <p>6. B cites f, f cites e, e cites d, and d cites A.</p>
    <p>7. g cites A, h cites B, g cites a, and h cites a.</p>
    <p>8. i cites B, i cites f  or g!, and f  or g! cites A.</p>
    <p>9. i cites g, i cites A, and g cites B.</p>
    <p>10. i cites g  or d!, i cites h, g  or d! cites A, and h cites B.</p>
    <p>11. i cites a, i cites B, and A cites a.</p>
    <p>12. i cites A, i cites e, B cites e.</p>
    <p>13. g cites A, g cites a, A cites a, h cites B, and h cites a.</p>
    <p>14. A cites a, B cites d, i cites a, and i cites d.</p>
    <p>15. i cites B, i cites d, A cites a, and d cites a.</p>
    <p>16. A cites b, B cites d   or c!, and d  or c! cites b.</p>
    <p>17. A cites b, B cites d, b cites a, and d cites a.</p>
    <p>18. A cites a, B cites b, d  or c! cites a, and d  or c! cites b.</p>
    <p>These 18 patterns are shown schematically in FIG. 6. (For a discussion on probability theory and statistics, see Wilkinson, Leland; SYSTAT: The System for Statistics; Evanston, Ill.: SYSTAT Inc., 1989 incorporated herein by reference.) Some patterns occur only between two full textual objects, and others between any two textual objects; this distinction is explained below. Semantical patterning is only run on patterns number one and number two, shown above.</p>
    <p>For purposes of explaining how patterns are used to generate the Proximity Index, only the two simplest patterns are illustrated.</p>
    <p>The simplest, Pattern #1, is "B cites A." See FIG. 6. In the notation developed, this can be diagramed: a b c A d e f B g h i where the letters designate textual objects in chronological order, the most recent being on the right, arrows above the text designate citations to A or B, and arrows below the text designate all other citations. The next simplest pattern between A and B, Pattern #2, is "B cites c and A cites c," which can also be expressed as "there exists c, such that c is an element of (A intersect B)." See Appendix #1. This can be diagramed: a b c A d e f B g h i. For every textual object c from 0 to (A-1), the existence of Pattern #2 on A and B is signified by 1, its absence by 0. This function is represented as P#2AB(c)=1 or P#2AB(c)=0. The complete results of P#1AB and P#2AB can be represented by an (A)x(1) citation vector designated X.</p>
    <p>The functions of some Patterns require an (n)×(1) matrix, a pattern vector. Therefore it is simplest to conceive of every Pattern function generating an (n)×(1) vector for every ordered pair of full textual objects in the database 54, with "missing" arrays filled in by 0s. Pattern Vectors can be created for Pattern #1 through Pattern #4 by just using the relationships among textual object A and the other textual objects in the database 54 and among textual object B and the other textual objects in the database 54. Pattern Vectors for Patterns #5 through #18 can only be created if the relationship of every textual object to every other textual object is known. In other words, Pattern Vectors for Patterns #1 through #4, can be created from only the rows A and B to the Citation Matrix but Pattern Vectors for Patterns #5 through #18 can only be created from the whole Citation Matrix.</p>
    <p>(total textual objects c)/(theoretical maximum textual objects c)  (x)(x)<sup>T</sup> /TMax!,</p>
    <p>(total textual objects c)/(actual maximum textual objects c)  (x)(x)<sup>T</sup> /AMax!</p>
    <p>frequency of object c per year  f!, and</p>
    <p>the derivative of the frequency  f!.</p>
    <p>In pattern #2, given that A&lt;B, the theoretical maximum ("TMax") number Q(A intersect B)=A minus 1. The actual maximum possible ("AMax"), given A and B, is the lesser of Q(A) and Q(B). The ratios "X(X)<sup>T</sup> /TMax" and "X(X)<sup>T</sup> /AMax," as well as the frequency of occurrence of textual objects c per year, f2(A, B), and the first derivative f'2(A, B), which gives the instantaneous rate of change in the frequency of "hits," are all defined as "numerical factors" generated from patterns #1 and #2. These are the raw numbers that are used in the weighing algorithm.</p>
    <p>For Pattern #2, the total number of possible textual objects c subject to analysis, i.e. TMax, is A-1, one only for the years at issue which are those up to the year in which A occurred. However, a relationship may remain "open," that is, it may require recalculation of f(x) and f'(x) as each new textual object is added to the database 54, (for a total of n cases subject to analysis).</p>
    <p>The "numerical factors" for all eighteen patterns are assigned various weights in a weighing algorithm used to generate a scalar F(A, B). The function F generates a scalar derived from a weighted combination of the factors from all eighteen patterns. The patterns are of course also weighted by "importance," allowing Supreme Court full textual objects to impose more influence on the final scalar than District Court full textual objects, for example. The weighing of the more than 100 factors is determined by empirical research to give results closest to what an expert human researcher would achieve. The weighing will vary depending upon the type of material that is being compared and the type of data in the database 54. (See Thurstone, The Vectors of Mind, Chicago, Ill.: University of Chicago Press, 1935, for a description of factor loading and manipulating empirical data incorporated herein by reference.) In a commercial "Proximity Indexer" it will be possible to reset the algorithm to suit various types of databases.</p>
    <p>A scalar F(A, B) is generated for every ordered pair of full cases in the database 54, from F(1, 2) to F(n-1, n). F(z,z) is defined as equal to 0.</p>
    <p>The full results of F(A,B) are arranged in an (n)×(n) matrix designated F. Note that F(B, A) is defined as equal to F(A, B), and arrays that remain empty are designated by 0. For every possible pairing of cases (A,B), a Euclidean distance D(A,B) is calculated by subtracting the Bth row of Matrix F from the Ath row of Matrix F. In other words:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">D(A,B)= (F(1, A)-F(1, B))<sup>2</sup> +(F(2, A)-F(2, B))<sup>2</sup> +. . .+(F(n, A)-F(n, B))<sup>2</sup> !<sup>1/2</sup>.</pre>
    
    <p>A function designated D(A,B) generates a scalar for every ordered pair (A,B), and hence for every ordered pair of textual objects (A,B) in the database 54. The calculations D(A,B) for every ordered pair from D(1,1) to D(n,n) are then arranged in an (n)×(n) "proximity matrix" D. Every column vector in D represents the relationship between a given case A and every other case in the database 54. Comparing the column vectors from column A (representing textual object A) and column B (representing textual object B) allows one to identify their comparative positions in n-dimensional vector space, and generate a coefficient of similarity, S(A,B), from 0-100%, which is more precise and sophisticated than F(A,B) or D(A,B) alone. A similarity subroutine can run directly on F(A,B). However, the real power of the Proximity Matrix D is that it allows one to identify "groups" or "clusters" of interrelated cases.</p>
    <p>Through factor loading algorithms, the relationships represented by D for "n" cases can be re-represented in a vector space containing fewer than "n" orthogonal vectors. This knowledge can be reflected in S(A,B).</p>
    <p>The Proximity Indexing Application Program 62 is an application program that applies the above techniques and algorithms to index and format data to be searched by the CSPDM 66.</p>
    <p>FIG. 3A describes the overall procedure of the Proximity Application Indexing Program 72. The first stage initializes the data 74 in the database 54. The second stage determines the relationships between full textual objects 78. The third stage determines the relationships between paragraphs of each textual object and each full textual object 80. The fourth stage clusters related paragraphs using factor loading and empirical data and then groups the paragraphs into sections based on such data 84. The fifth stage determines the relationships between the sections 88. In the final stage, the sectioned textual objects are not further processed until commands are received from the CSPDM Routine 92.</p>
    <p>The following description of FIG. 3B and FIG. 3C elaborates on this general procedure by describing specific subroutines of a Proximity Indexing Application Program 62. The following is a step by step description of the operation of the Proximity Indexing Application Program 62.</p>
    <p>Section A Initial Extractor Subroutine 96</p>
    <p>FIG. 3B describes subroutines for the first portion of the preferred Proximity Indexing Application Program 62. The first subroutine of the Proximity Indexing Applications Program is the Initial Extractor Subroutine 96. The Initial extractor subroutine 96 performs three primary functions: Creation of the Opinion Citation Matrix, creation of the Paragraph Citation Matrix, and creation of Boolean Word Index.</p>
    <p>The following steps are performed by the Initial extractor subroutine 96.</p>
    <p>1. Number all full textual objects chronologically with arabic numbers from 1 through n.</p>
    <p>2. Number all paragraphs in all the full textual objects using arabic numbers from 1 through p.</p>
    <p>3. Identify the page number upon which each paragraph numbered in step two above begins.</p>
    <p>4. Create Opinion Citation Vectors (X). By comparing each full textual object in the data base to every other full textual object in the data base that occurred earlier in time.</p>
    <p>5. Combine Opinion Citation Vectors to create the bottom left half portion of the n×n Opinion citation matrix.</p>
    <p>6. Create a mirror image of the bottom left half portion of the Opinion citation matrix in the top right half portion of the same matrix, to complete the matrix. In this manner only n<sup>2</sup> /2 comparisons need to be conducted. The other 1/2 of the comparisons are eliminated.</p>
    <p>7. Create the p×n Paragraph Citation Vectors by comparing each paragraph to each full textual object that occurred at an earlier time. This will require (n/2)p searches.</p>
    <p>8. Create a Paragraph Citation Matrix by combining Paragraph Citation Vectors to create the bottom left half portion of the matrix.</p>
    <p>9. Complete the creation of the Paragraph Citation Matrix by copying a mirror image of the bottom left half portion of the matrix into the top right half portion of the matrix.</p>
    <p>10. Initialize the Initial extractor subroutine 96 with a defined set of core English words 140.</p>
    <p>11. Assign identification numbers to the core English words 140. In the preferred embodiment 50,000 English words are used and they are assigned for identification the numbers from -50,000 to -1.</p>
    <p>12. Create a Boolean Index Matrix 144 with respect to the core English words by searching the database 54 for the particular word and assigning the paragraph number of each location of the particular word to each particular word. This procedure is described in greater detail in FIG. 3D.</p>
    <p>Section B Opinion Patterner Subroutine 100</p>
    <p>The Opinion Patterner Subroutine 100 performs three primary functions: Pattern analysis on matrices, calculation of the numerical factors and weighing the numerical factors to reach resultant numbers.</p>
    <p>13. Process the Opinion Citation Matrix through each of the pattern algorithms described above and in FIG. 6 for each ordered pair of full textual objects to create opinion pattern vectors for each pattern and for each pair of full textual objects. The pattern algorithms determine relationships which exist between the ordered pair of textual objects. The first four pattern algorithms can be run utilizing just the Opinion Citation Vector for the two subject full textual objects. Each pattern algorithm produces a opinion pattern vector as a result. The fifth through eighteenth pattern algorithms require the whole Opinion Citation Matrix to be run through the Opinion Patterner Subroutine 100.</p>
    <p>14. Calculate total hits (citation) for each pattern algorithm. This can be done by taking the resultant opinion pattern vector (OPV) and multiplying it by the transposed opinion pattern vector (OPV)T to obtain a scalar number representing the total hits.</p>
    <p>15. Calculate the theoretical maximum number of hits. For example, in the second pattern, the theoretical maximum is all of the full textual objects that occur prior in time to case A(A--).</p>
    <p>16. Calculate the actual maximum number of hits. For example, in the second pattern, the actual maximum possible number of hits is the lesser of the number of citations in full textual object Q(A) or full textual object Q(B).</p>
    <p>17. Calculate the total number of hits (citations) per year. This is labeled f(A,B).</p>
    <p>18. Calculate the derivative of the total change in hits per year. This is the rate of change in total hits per year and is labeled f'(A,B).</p>
    <p>19. Calculate the ratio of total hits divided by theoretical max  ((OPV)(OPV)<sup>t/TMAX</sup> !.</p>
    <p>20. Calculate the ratio of the total hits divided by the actual maximum  (OPV(OPV)<sup>t</sup> /AMAX!.</p>
    <p>21. Calculate a weighted number F(A,B) which represents the relationship between full textual object A and full textual object B. The weighted number is calculated using the four raw data numbers, two ratios and one derivative calculated above in steps 14 through 20 for each of the 18 patterns. The weighing algorithm uses empirical data or loading factors to calculate the resulting weighted number.</p>
    <p>22. The Opinion Patterner Subroutine 100 sequence for the Opinion Citation Matrix is repeated n-1 times to compare each of the ordered pairs of full textual objects. Therefore, during the process, the program repeats steps 13 through 21, n-1 times.</p>
    <p>23. Compile the Opinion Pattern Matrix by entering the appropriate resulting numbers from the weighing algorithm into the appropriate cell locations to form an n×n Opinion Pattern Matrix.</p>
    <p>Section C The Opinion Weaver Subroutine 104</p>
    <p>The Opinion Weaver Subroutine 104 shown in FIG. 3B, performs two primary tasks: calculation of the Opinion Proximity Matrix and calculation of the Opinion Similarity Matrix. The Opinion Proximity Matrix D is generated by calculating the Euclidean Distance between each row A and B of the Opinion Pattern Matrix (D(A,B)) for each cell DC(A,B). The Opinion Similarity Matrix is generated by calculating the similarity coefficient from 0 to 100 between each row A and B of the Opinion Proximity Matrix (S(A,B)) in each cell SC(A,B) in matrix S.</p>
    <p>24. Calculate the n×n Opinion Proximity Matrix. To calculate D(A,B) the program takes the absolute Euclidian distance between column A and column B of the n×n Opinion Pattern Matrix. The formula for calculating such a distance is the square root of the sum of the squares of the distances between the columns in each dimension, or:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">D(A,B)= (F(1,A)-F(1,B))<sup>2</sup> +(F(2,A)-F(2,B))<sup>2</sup> +. . .+(F(N,A)F(N,B))<sup>2</sup> !<sup>1/2</sup> </pre>
    
    <p>The Opinion Proximity Matrix created will be an n×n matrix. The smaller the numbers in the Opinion Proximity Matrix the closer the relationship between full textual object A and full textual object B.</p>
    <p>25. Create n×n Opinion Similarity Matrix. To calculate the Opinion Similarity Matrix each scalar number in the Opinion Proximity Matrix is processed through a coefficient of similarity subroutine which assigns it a number between 0 and 100. By taking the coefficient of similarity, the program is able to eliminate full textual objects which have Euclidian distances that are great. (For example, a Euclidean distance that is very large and is run through the coefficient of similarity would result in a very low coefficient of similarity. Euclidean distances resulting in similarities below four are eliminated in the preferred embodiment).</p>
    <p>Section D Paragraph Patterner Subroutine 108 (Optional)</p>
    <p>26. Obtain the p×n Paragraph Citation Matrix calculated by the Initial extractor subroutine 96.</p>
    <p>27. Run each ordered pair of rows of the p×n Paragraph Citation Matrix for an individual full textual object i through the pattern algorithms number one and two and determine the resultant Paragraph Pattern Vector.</p>
    <p>28. Calculate the various numerical factors (AMax, TMax, etc.) by evaluating the values in the Paragraph Pattern Vector.</p>
    <p>29. Run the Paragraph Pattern Vector and the numerical factors through the weighing algorithm to determine the appropriate value for each cell of the c<sub>i</sub> ×n Partial Paragraph Pattern Matrix where c<sub>i</sub> is the number of paragraphs in full textual object i.</p>
    <p>30. Repeat steps 27 through 29 for each full textual object i where i=1 to n, to create the p×n Paragraph Pattern Matrix.</p>
    <p>Section E Paragraph Weaver Subroutine 112</p>
    <p>31. Calculate the Euclidean distance of each ordered pair of rows of either the p×n Paragraph Citation Matrix or the p×n Paragraph Pattern Matrix for a single full textual object i.</p>
    <p>32. Place the resultant Euclidean distance values in the appropriate cell of the c<sub>i</sub> ×c<sub>i</sub> Paragraph Proximity Matrix where c<sub>i</sub> is the number of paragraphs in full textual object i, where 0&lt;i&lt;n+1.</p>
    <p>33. Repeat steps 31 through 32 n times in order to calculate n different Paragraph Proximity Matrices (one for each full textual object i).</p>
    <p>34. The Section Comparison Subroutine 116 clusters all p paragraphs in the database 54 into sections. Then the sections are compared and indexed in the database 54. This procedure is described in greater detail in FIG. 3C.</p>
    <p>FIG. 3C depicts possible subroutines that the Section Comparison Subroutine 116 comprises. The subroutines are the Sectioner Geographical Subroutine 120, the Sectioner Topical Subroutine 124 (Optional), the Section Extractor Subroutine 128, the Section Patterner Subroutine 132 and the Section Weaver Subroutine 136.</p>
    <p>Section F Sectioner Geographical Subroutine 120</p>
    <p>35. For each full textual object i, the Sectioner Geographical Subroutine 120 uses the corresponding c<sub>i</sub> ×c<sub>i</sub> Paragraph Proximity Matrix and a contiguity factor for each paragraph to determine which paragraphs may be clustered into sections. Sections are made up of continuous paragraphs that are combined based upon weighing their Euclidean distances and contiguity.</p>
    <p>36. Repeat step 35 for all n full textual objects until all p paragraphs are grouped into q sections.</p>
    <p>Section H Sectioner Topical Subroutine 124 (Optional)</p>
    <p>37. The Sectioner Topical Subroutine 124 provides additional assistance to the Sectioner Geographical Subroutine 120 by considering the factor of topical references to determine the q sections.</p>
    <p>38. For the total number of discrete references "z" to each full textual object in a particular full textual object, a z×z Citation Proximity Matrix is formed by comparing the Euclidean distances between each reference to a full textual object contained in each paragraph and calculating the topical weight given to each paragraph.</p>
    <p>Section I Section Extractor Subroutine 128</p>
    <p>39. The Section Extractor Subroutine 128 numbers each section created by the Sectioner Geographical Subroutine 120 and Sectioner Topical Subroutine 124 Subroutines from 1 to q.</p>
    <p>40. The Sectioner Extractor Subroutine 128 creates a q×q Section Citation Matrix by determining which sections refer to every other section.</p>
    <p>Section J Section Patterner Subroutine 132 (shown in FIG. 3C)</p>
    <p>41. The Section Patterner Subroutine 132 then calculates 18 Section Pattern Vectors corresponding to each row of the q×q Section Citation Matrix using the 18 pattern algorithms.</p>
    <p>42. From the Section Pattern Vectors, the numerical factors (AMax, TMax, etc.) are calculated.</p>
    <p>43. The weighing algorithm evaluates the numerical factors and the Section Pattern Vectors and determines the values for each cell of the q×q Section Pattern Matrix.</p>
    <p>Section K Section Weaver Subroutine 136</p>
    <p>44. The Section Weaver Subroutine 136 calculates the Euclidean distances between each row of the q×q Section Pattern Matrix and creates a q×q Section Proximity Matrix.</p>
    <p>45. The Section Weaver Subroutine 136 then creates a q×q Section Similarity Matrix with coefficients 0 to 100 using the values of the Section Proximity Matrix and empirical data and factor loading.</p>
    <p>Section L Semantical Clustering of a Boolean Index Routine 138</p>
    <p>FIG. 3D depicts a possible Semantical Clustering of a Boolean Index Routine 138. (See Hartigan, J. A. Clustering Algorithms. New York: John Wiley &amp; Sons, Inc., 1975, for detailed description of clustering algorithms incorporated herein by reference.) The Semantical Clustering routine of a Boolean Index 138 indexes the textual objects according to the similarity of phrases and words contained within each textual object in a database 54. The routine comprises seven possible subroutines: the Initial Opinion Extractor Subroutine 96, the Pool Patterner Subroutine 152, the Pool Weaver Subroutine, the Pool Sectioner Subroutine 160, the Section Extractor Subroutine 128, the Section Patterner Subroutine 132 and the Section Weaver Subroutine 136. In fact, it is quite possible, using only semantical statistical techniques, to "Proximity-index" documents that do not refer to one another at all based on there Boolean indices.</p>
    <p>Section M Initial Extractor Subroutine 96</p>
    <p>46. As described in steps 10 and 11, the Initial Extractor Subroutine initializes a set of core English words 140 and assigns each word a number. The preferred embodiment uses 50,000 discrete core English words 140 and assigns each discrete core English word 140 a number from -50,000 to -1.</p>
    <p>47. The Initial Extractor Subroutine 96 then converts the core English words 140 into a p×w matrix. The number of columns (w) represents the number of discrete core English words 140 in the database 54 and the number of rows (p) represents the number of paragraphs in the database 54.</p>
    <p>48. The Initial Extractor Subroutine 96 fills the p×w matrix by inserting a "1" in the matrix cell where a certain paragraph contains a certain word.</p>
    <p>Section N Pool Patterner Subroutine 152</p>
    <p>49. The Pool Patterner Subroutine 152 creates two pattern algorithm vectors for only the first two patterns and determines values for the total number of hits, the theoretical maximum number of hits, the actual maximum number of hits, the total number of hits per year and the derivative of the total number of hits per year.</p>
    <p>50. The weighing algorithm of the Pool Patterner Subroutine 152 uses empirical data and factor loading to determine values to enter into a p×w Paragraph/Word Pattern Matrix.</p>
    <p>51. The Pool Weaver Subroutine 156 creates a p×w Paragraph/Word Pattern Matrix by filling the appropriate cell of the Matrix with the appropriate value calculated by the weighing algorithm.</p>
    <p>52. The Pool Patterner Subroutine 152 creates a p×w Paragraph/Word Proximity Matrix taking the Euclidean distance between the rows of the Paragraph/Word Pattern Matrix.</p>
    <p>Section O Pool Sectioner Subroutine 160</p>
    <p>53. The Pool Sectioner Subroutine 160 evaluates the Euclidean distances in the Paragraph/Word Proximity Matrix and the contiguity factor of each paragraph to cluster the paragraphs (p) into a group of (v) sections and create a v×w Preliminary Cluster Word Matrix.</p>
    <p>Section P Section Extractor Subroutine 128</p>
    <p>54. The Section Extractor Subroutine 128 numbers each section chronologically and creates a v×v Section Word Citation Matrix.</p>
    <p>Section Q Section Patterner Subroutine 132</p>
    <p>55. The Section Patterner Subroutine 132 evaluates the v×v Section Word Citation Matrix to create two word pattern vectors for only the first two patterns algorithms (described above and shown in FIG. 6) and determines numerical factors for the total number of hits, the theoretical maximum number of hits, the actual maximum number of hits, the total number of hits per year and the derivative of the total number of hits per year.</p>
    <p>56. The Weighing algorithm uses empirical data and factor loading to weigh the numerical factors created from the word pattern vectors and uses the numerical factors and the word pattern vectors to determine values to enter into a v×v Section Word Pattern Matrix.</p>
    <p>Section R Section Weaver Subroutine 136</p>
    <p>57. The Section Weaver Subroutine 136 creates a v×v Section Word Proximity Matrix by taking the Euclidean distance between the rows of the Section Word Pattern Matrix and placing the appropriate Euclidean distance value in the appropriate cell of the Section Word Proximity Matrix.</p>
    <p>58. The Section Weaver Subroutine 136 create a v×v Section Word Similarity Matrix by evaluating the Euclidean distances from the Section Word Proximity Matrix and empirical data, and calculating the similarity coefficient for each ordered pair of sections, and places the value in the appropriate cell of the Section Word Similarity Matrix.</p>
    <p>59. The Pool Searches of the CSPDM 66 evaluate the Section Word Similarity Matrix as well as other matrices to determine whether or not to retrieve a full textual object.</p>
    <p>The following describes a preferred cluster link generator 2040 which implements a specific type of patterner or clustering system for use alone or in conjunction with other proximity indexing subroutines, and prior to searching. The cluster link generator 2040 analyzes a set of numerical representations of a database 54 and generates a second set of numerical representations of the database 54. This second set is stored in the RAM. This second set of numerical data can represent indirect 2036, direct 2032, or direct 2032 and indirect 2036 relationships in the database 54. Preferably, the second set of numerical representations accounts for indirect 2036 relationships in the database 54. It is preferred that the first and second set of numerical data be in a table format and that the first set represent direct 2032 relationships or links and the second set represent cluster links 2004.</p>
    <p>Referring to FIG. 3H, the cluster link generation algorithm 2044 analyzes links to generate a set of cluster links 2004. More specifically, the cluster link generation algorithm 2044 generates a set of cluster links 2004 by analyzing direct 2032 and/or indirect links 2004 between nodes 2008 or between objects in a database 54 and generates a set of cluster links 2004. The set of cluster links 2004 is generated based upon direct 2032 and indirect 2036 paths or links existing in the database 54.</p>
    <p>In the preferred embodiment, the cluster link generator 2040 analyzes direct links 2004 (for example source links 2004 and influence links 2004). These direct links 2004 may be represented by a table or series of vectors. The generator then locates indirect 2036 paths between nodes 2008 or objects in a database 54. The indirect 2036 paths are preferably made up of direct links 2004. The cluster link generator 2040 then generates a set of cluster links 2004 based upon both the direct links 2004 and on the indirect 2036 paths. The cluster links 2004 may be represented by a table or a series of vectors. Another embodiment of this invention uses candidate cluster links 2004 to provide a more efficient search. Candidate cluster links are the set of all possible cluster links 2004 between a search node 2008 and a target node 2004. In this embodiment, only a subset of the candidate cluster links 2004, the actual cluster links 2004, which meet a certain criteria are used to locate nodes 2008 for display.</p>
    <p>Consider a set of nodes 2008 N<sub>0</sub> . . . N<sub>3</sub> connected by a sequence of direct links 2004 whose weights 2034 are given by W<sub>1</sub> . . . W<sub>3</sub>, as shown in FIGS. 3F.</p>
    <p>Node 2008 N<sub>1</sub> is reachable from N<sub>0</sub> through a path of length 1 (that is, N<sub>0</sub> →N<sub>1</sub>); node 2008 N<sub>2</sub> is reachable through a path of length 2 (N<sub>0</sub> →N<sub>1</sub> →N<sub>2</sub>); and so on.</p>
    <p>Each path provides some evidence that the start node 2008 (N<sub>0</sub>) and destination node 2008 (N<sub>1</sub>, N<sub>2</sub>, or N<sub>3</sub>) are related to some extent. The strength of the implied relationship depends on the length of the path, and on the weights 2034 of the individual direct links 2004 along that path.</p>
    <p>In FIG. 3G, the implied relationships from N<sub>0</sub> to N<sub>1</sub>, N<sub>2</sub>, and N<sub>3</sub> are shown as arcs.</p>
    <p>The weight 2034 of each implied link, C<sub>1</sub> . . . C<sub>3</sub>, is a function of the weight 2034 of the path to the previous node 2008 and the weight 2034 of the last link.</p>
    <p>The individual functions F1 . . . F3 describe how to combine the weights 2034 of the direct links 2004 to determine the weight 2034 of an implied link. Selecting appropriate functions is the key to making cluster link generation work well. A preferred definition of F<sub>N</sub> is as follows:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">C<sub>N</sub> =F<sub>N</sub> (C<sub>N-1</sub>, W<sub>N</sub>)=min(C<sub>N-1</sub>, D<sub>N</sub> * W<sub>N</sub>),</pre>
    
    <p>where D<sub>N</sub> is a damping factor that decreases rapidly as N increases.</p>
    <p>The cluster link algorithm 2044 determines the set of all paths P from a given node 2008 N<sub>0</sub> that have a length less than or equal to a given length L. Each path is rated using the method described above. The paths are then grouped by destination node 2008; the candidate cluster link 2004 C(N<sub>0</sub>, N<sub>N</sub>) between N<sub>0</sub> and a given destination node 2008 N<sub>N</sub> has a weight 2034 equal to the sum of the weights 2034 of all paths P<sub>N</sub> leading to N<sub>N</sub>.</p>
    <p>The set of all candidate cluster links 2004 is then sorted by weight 2034. A subset of the candidate links 2004 is chosen as actual cluster links 2004. The number of cluster links 2004 chosen may vary, depending on the number of direct links 2004 from N<sub>0</sub>, and on the total number of candidate cluster links 2004 available to choose from.</p>
    <p>Performance considerations and efficiency are more important than for small databases. For large databases, finding the set of all paths P from a given node 2008 N<sub>0</sub> that have a length less than or equal to a given length L may be impractical, since the number of unique paths may number in the tens of millions.</p>
    <p>One embodiment of this invention uses candidate cluster links 2004 to provide a more efficient search. Candidate cluster links 2004 are the set of all possible cluster links 2004 between a search node (2008) and a target node. (2008) In this embodiment, only a subset of the candidate cluster links 2004, the actual cluster links 2004, which meet a specified criteria are used to identify nodes (2008) for display 38.</p>
    <p>Clearly, it is not necessary to examine millions of paths when the goal is to select the top or strongest cluster links 2004 for each N<sub>0</sub> (for example, the top 20 to 25 cluster links 2004). The great majority of paths have an insignificant effect on the final results. What is needed is an implementation of the cluster link algorithm 2044 where the total number of paths examined is bounded, independent of the size of the database 54, without a loss in effectiveness. To this end, we have an implementation of the algorithm 2044 such that a cluster link 2004 is defined recursively.</p>
    <p>We define C<sub>L</sub> (N<sub>0</sub>, N<sub>N</sub>), the order-L cluster link 2004 from N<sub>0</sub> to N<sub>N</sub>, as the cluster link 2004 between N<sub>0</sub> and N<sub>N</sub>, considering only paths of length less than or equal to L. Then, we can derive C<sub>L+1</sub> (N<sub>0</sub>, N<sub>N</sub>) from C<sub>L</sub> (N<sub>0</sub>, N<sub>N</sub>) and C<sub>1</sub> (N<sub>0</sub>, N<sub>N</sub>).</p>
    <p>The assumption is that most of the paths P<sub>L</sub> (N<sub>0</sub>, N<sub>N</sub>) of length L (or greater) from N<sub>0</sub> to N<sub>N</sub> will not have a significant impact on cluster link generation. Therefore, we an use a set of candidate cluster links 2004 C<sub>L</sub> (N<sub>0</sub>, N<sub>N</sub>) as a summary of that path information for the purpose of determining C<sub>L+1</sub> (N<sub>0</sub>, N<sub>N</sub>). This assumption has a significant impact on the performance of the algorithm 2044 in this implementation, since the search space is significantly reduced at each step. The computer processing "cost" of generating cluster links 2004 is bounded by the size of the candidate cluster link 2004 sets generated at the intermediate steps, rather than by the total number of relevant paths in the database 54.</p>
    <p>The size of the candidate cluster link 2004 set generated at each intermediate step affects the speed of the algorithm 2044 in this implementation. If too many candidate cluster links 2004 are generated at each intermediate step, the algorithm 2044 is too slow. On the other hand, if too few candidate cluster links 2004 are generated, and too many paths are pruned, then C<sub>L</sub> (N<sub>0</sub>, N<sub>N</sub>) is no longer an accurate summary of P<sub>L</sub> (N<sub>0</sub>, N<sub>N</sub>).</p>
    <p>Finally, since the weights 2034 of the individual candidate cluster links 2004 in C<sub>L</sub> (N<sub>0</sub>, N<sub>N</sub>) are generally much greater than the weights 2034 of the individual paths in P<sub>L</sub> (N<sub>0</sub>, N<sub>N</sub>), the damping factors D<sub>N</sub> used to derive the combined weights 2034 at each step must be decreased accordingly in this implementation.</p>
    <p>The specifics for the basic algorithm 2044 of this implementation, for determining the set of order N cluster links 2004 from a given start node 2008 N<sub>0</sub>, are shown in FIG. 3H. The general algorithm 2044 works for any value of N greater than zero. If N=1, the set of candidate cluster links 2004 generated is simple. The processing cost of determining the candidate cluster links 2004 increases with N. In practice, N=3 appears to yield the best results.</p>
    <p>The algorithm 2044 starts by initializing the candidate cluster link 2004 set 2048 and creating a loop for i=0 to N 2052. The algorithm 2044 then performs a series of steps for each path P 2056. First, it selects the destination node 2008 as the node to analyze and retrieves the set of direct links 2004 (L) from the selected node 2008 to any other node 2008 in the database 54, N<sub>i+1</sub>. Second, for each direct link 2004 L the algorithm 2044 performs a series of steps:</p>
    <p>The algorithm 2044 creates a new path P' of length i+1 consisting of the path P plus the direct link 2004 L from the selected node 2008 to the node 2008 N<sub>i+1</sub> 2056. The algorithm 2044 then determines the combined weight 2034 WC<sub>i+1</sub> from WC<sub>i</sub>, the weight 2034 of the path P, and W<sub>i+1</sub>, the weight 2034 of Link 2004 L, using the following preferred formula:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">WC<sub>i+1</sub> =min(WC<sub>i</sub>, D<sub>i+1</sub> *W<sub>i+1</sub>)2064.</pre>
    
    <p>Following these computations, the algorithm 2044 decides whether there already is a path in the cluster link 2004 from N<sub>0</sub> to N<sub>i+1</sub> 2068. If there is a not already a path, the algorithm 2044 adds P' to C<sub>i+1</sub> 2072. If there already is a path, the algorithm 2044 adds WC<sub>i+1</sub> to the weight 2034 of the existing path in C<sub>i+1</sub> 2076. These steps are then repeated as necessary.</p>
    <p>Once the candidate cluster link 2004 set has been generated, deriving the actual cluster links 2004 is a simple matter of selecting or choosing the T top rated candidate links 2004, and eliminating the rest. In practice, the following formula has yielded good results:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">T=min(constant, 4*d),</pre>
    
    <p>where d is the number of direct links 2004 from No. Setting the constant equal to twenty has yielded good results. More than T cluster links 2004 may be generated if there are ties in the ratings. After each iteration, the candidate cluster link 2004 set C<sub>i</sub> may be pruned so that it contains only the top candidate cluster links 2004 (for example, the top 200).</p>
    <p>FIGS. 4A and 4B are high level flow charts that illustrate the general flow of the subroutines of the CSPDM 66. FIG. 4A illustrates that the flow of various search routines depend on the type of search initiated by the researcher. The diagram further illustrates the interaction between the CSPDM 66 and the GUI Program 70. FIG. 4B illustrates the sequence of subroutines in the CSPDM 66 program and the user interactions with the subroutines. FIG. 4B further shows that the researcher can access the different search subroutines and use information that the researcher has already received to find new information.</p>
    <p>FIG. 4B provides a high level flow chart illustrating the sequence of subroutines in the CSPDM 66 program and the researcher's interactions with the subroutines. Assuming that the database 54 the researcher desires to access has been proximity indexed, the researcher must log on 260 to the database 54. By entering the appropriate information into the Computer Processor 30 via the input means, the researcher electronically access 264 the database 54 and enables the CSPDM 66 to search 200 the database 54.</p>
    <p>FIGS. 4A and 4B both show the preliminary options that the researcher can choose from before selecting one of the searching subroutines of the CSPDM 66. The CSPDM 66 questions the researcher on whether the researcher has identified a pool of textual objects 204. If the researcher has selected a pool of textual objects 204, then the researcher is able to choose one of the pool search 208 subroutines 212. If the researcher has not selected a pool of textual objects, the CSPDM 66 questions the researcher on whether the researcher has selected a single textual object 216. If the researcher has selected a single textual object 216, then the researcher is able to choose one 220 of the textual object searches 224. If the researcher has not selected either a pool of textual objects 204 or a single textual object 216, then the researcher must execute a Boolean Word Search or alternate Pool-Generation Method 228 to retrieve textual objects 268, 272.</p>
    <p>After CSPDM 66 subroutine has executed a particular search, the CSPDM 66 retrieves the appropriate data from the database 54, analyzes the data, and sends the data to the GUI Program 70 in order for the GUI Program 70 to display the results of the search on the display 38.</p>
    <p>FIG. 4B illustrates that after the CSPDM 66 has completed the above procedure, the researcher has the option to exit the CSPDM 66 by logging off 300, executing a search based on the results of a previous search, or executing a new search.</p>
    <p>FIGS. 4A and 4B also depict the seven subroutines of the CSPDM 66. There are three textual object search subroutines 224 and four pool search subroutines 212. The three textual object search subroutines 224 are: the Cases-In Subroutine 232, the Cases-After Subroutine 236 and the Similar Cases Subroutine 240. The four pool search subroutines 212 are the Pool-Similarity Subroutine 244, the Pool-Paradigm Subroutine 248, the Pool-Importance Subroutine 252, and the Pool-Paradigm-Similarity Subroutine 256. Each of these subroutines are described in more detail in FIGS. 4C to 4I. The following is a step by step description of the subroutines 224, 212 of the CSPDM 66.</p>
    <p>Section A Cases-In Subroutine 232</p>
    <p>FIG. 4C is a high level flow chart for the Cases-In Subroutine 232.</p>
    <p>1. The researcher must select a single textual object 400.</p>
    <p>2. The researcher selects the Cases-In Subroutine 232 option.</p>
    <p>3. The Cases-In Subroutine 232 examines the n×n Opinion Citation Matrix and other matrices 404 created by the Proximity Indexing Application Program 62 and retrieves the textual objects to which the selected textual object refers 408, data relating to the number of times the selected textual object refers to the retrieved textual objects, data relating to the importance of each textual object, and other relevant data.</p>
    <p>Section B Cases-After Subroutine 236</p>
    <p>FIG. 4D is a high level flow chart for the Cases-After Subroutine 236.</p>
    <p>4. The researcher must select a single textual object 400.</p>
    <p>5. The researcher selects the Cases-After Subroutine 236 option.</p>
    <p>6. The Cases-After Subroutine 236 examines the n×n Opinion Citation Matrix and other matrices 412 created by the Proximity Indexing Application Program 62 and retrieves the textual objects that refer to the selected textual object 416, data relating to the number of times the retrieved textual objects refer to the selected textual object, data relating to the importance of each textual object, and other relevant data.</p>
    <p>Section C Similar-Cases Subroutine 240</p>
    <p>FIG. 4E is a high level flow chart for the Similar-Cases Subroutine 240.</p>
    <p>7. The researcher must select a single textual object 400.</p>
    <p>8. The researcher selects the Similar-Cases Subroutine 240 option.</p>
    <p>9. The Similar-Cases Subroutine examines the q×q Section Similarity Matrix and other matrices 420 created by the Proximity Indexing Application Program 62 and retrieves the textual objects that are similar to the selected textual object 424, data relating to the degree of similarity between the selected textual object and the retrieved textual objects, data relating to the importance of each textual object, and other relevant data. In order to be retrieved, a textual object must have a similarity coefficient with respect to the selected textual object of at least a minimum value. The preferred embodiment sets the minimum similarity coefficient of four percent (4%).</p>
    <p>Section D Pool-Similarity Subroutine 244</p>
    <p>FIG. 4F is a high level flow chart for the Pool-Similarity Subroutine 244.</p>
    <p>10. The researcher must select a pool of full textual objects 428.</p>
    <p>11. The researcher must then select a single full textual object 400 to which in compare the pool of full textual objects. It should be noted that the researcher can select the single textual object from the selected pool of textual objects, or the researcher can select a textual object from outside of the pool 432.</p>
    <p>12. The Pool-Similarity Subroutine 244 examines the n×n Opinion Similarity Matrix and other matrices 436 and values created by the Proximity Indexing Application Program 62 for the selected full textual object and the pool of full textual objects.</p>
    <p>13. The Pool-Similarity Subroutine 244 determines the degree of similarity of other full textual objects in the pool to the selected full textual object 440.</p>
    <p>Section E Pool-Paradigm</p>
    <p>FIG. 4G is a high level flow chart for the Pool-Paradigm Subroutine 248.</p>
    <p>14. The researcher must select a pool of full textual objects 428.</p>
    <p>15. The Pool-Paradigm Subroutine 248 examines the n×n Opinion Proximity Matrix, the n×n Opinion Similarity Matrix and other matrices and values created by the Proximity Indexing Application Program 62 for the pool of full textual objects 448.</p>
    <p>16. The Pool-Paradigm Subroutine 248 determines the Paradigm full textual object by calculating the mean of the Euclidean distances of all the textual objects in the pool 452.</p>
    <p>17. The Pool-Paradigm Subroutine 248 determines the similarity of the other full textual objects in the pool to the Paradigm full textual object 456.</p>
    <p>Section F Pool-Importance Subroutine 252</p>
    <p>FIG. 4H is a high level flow chart for the Pool-Importance Subroutine 252.</p>
    <p>18. The researcher must select a pool of full textual objects 428.</p>
    <p>19. The Pool-Importance Subroutine 252 examines 448 the n×n Opinion Citation Matrix, the n×n Opinion Similarity Matrix, numerical factors and other matrices and values created by the Proximity Indexing Application Program 62 for the pool of full textual objects 460.</p>
    <p>20. The Pool-Importance Subroutine 252 then ranks the importance of each of the full textual objects in the pool 464.</p>
    <p>FIG. 41 is a high level flow chart showing two possible alternate Pool-Paradigm-Similarity Subroutines 256.</p>
    <p>Section G Pool-Paradigm-Similarity Subroutine 256 (Option 1) 256</p>
    <p>21. The researcher must select a pool of k full textual objects where k equals the number of full textual objects in the pool 428.</p>
    <p>22. For each of the k full textual objects, the Pool-Paradigm-Similarity Subroutine 256 selects a n×1 vector from the corresponding column of the n×n.</p>
    <p>23. The Pool-Paradigm-Similarity Subroutine 256 creates an n×k matrix by grouping the n×1 vector representing each of the k full textual objects beside each other.</p>
    <p>24. The Pool-Paradigm-Similarity Subroutine 256 calculates the mean of each row of the n×k matrix and enters the mean in the corresponding row of an n×1 Paradigm Proximity Vector 472.</p>
    <p>25. The Pool-Paradigm-Similarity Subroutine 256 combines the n×1 Paradigm Proximity Vector with the n×n Opinion Proximity Matrix to create an (n+1)×(n+1) Paradigm Proximity Matrix 476.</p>
    <p>26. From the (n+1)×(n+1) Paradigm Proximity Matrix, the Pool-Paradigm-Similarity Subroutine 256 evaluates the Euclidian distances and empirical data to create an (n+1)×(n+1) Paradigm Similarity Matrix 480.</p>
    <p>27. The Pool-Paradigm Similarity Subroutine searches the row in the (n+1)×(n+1) Paradigm Similarity Matrix that corresponds to the Paradigm full textual object and retrieves the full textual objects that have a maximum degree of similarity with the Paradigm full textual object 500.</p>
    <p>Section H Pool-Paradigm-Similarity Subroutine 256 (Option 2)</p>
    <p>28. The researcher must select a pool of k full textual objects where k equals the number of full textual objects in the pool 428.</p>
    <p>29. For each of the k full textual objects, the Pool-Paradigm-Similarity Subroutine 256 selects an n×1 vector from the corresponding column of the n×n.</p>
    <p>30. The Pool-Paradigm-Similarity Subroutine 256 creates an n×k matrix by grouping the n×1 vector for each of the k full textual objects beside each other.</p>
    <p>31. The Pool-Paradigm-Similarity Subroutine 256 calculates the mean of each row of the n×k matrix and enters the mean in the corresponding row of an n×1 Paradigm Pattern Vector PF 488.</p>
    <p>32. The Pool-Paradigm-Similarity Subroutine 256 combines the n×1 Paradigm Pattern Vector PF with the n×n Opinion Pattern Matrix to create a (n+1)×(n+1) Paradigm Pattern Matrix 492.</p>
    <p>33. From the (n+1)×(n+1) Paradigm Pattern Matrix, the Pool-Paradigm-Similarity Subroutine 256 evaluates the Euclidean distances between the rows of the Paradigm Pattern Matrix and creates an (n+1)×(n+1) Paradigm Proximity Matrix 496.</p>
    <p>34. From the (n+1)×(n+1) Proximity Matrix, the Pool-Paradigm-Similarity Subroutine 256 evaluates the Euclidean distances between the rows of the (n×1)×(n×1) Paradigm Proximity Matrix and empirical data to create an (n+1)×(n+1) Paradigm Similarity Matrix 480.</p>
    <p>35. The Pool-Paradigm Similarity Subroutine searches the row in the (n+1)×(n+1) Paradigm Similarity Matrix that corresponds to the Paradigm full textual object and retrieves the full textual objects that have a minimum degree of similarity with the Paradigm full textual object 500.</p>
    <heading>Application of the Proximity Indexing Technique</heading> <p>The above Proximity Indexing Application Program 62 and CSPDM 66 have a number of different applications and versions. Three of the most useful applications are described below.</p>
    <p>The first type of Proximity Indexing Application Programs 62 is for use on very large databases. The matrices generated by this type of Proximity Indexer are "attached" to the database 54, along with certain clustering information, so that the database 54 can be searched and accessed using the Cases-In Subroutine 232, Cases-After Subroutine 236, Cases-Similarity Subroutine, Pool-Similarity Subroutine 244, Pool-Paradigm Subroutine 248, Pool-Importance Subroutine 252 and Pool-Paradigm-Similarity Subroutine 256 of the CSPDM 66.</p>
    <p>The second type of Proximity Indexing Application Program 62 is a Proximity Indexer that law firms, businesses, government agencies, etc. can use to Proximity Index their own documents in their own databases 54. The researcher can navigate through the small business's preexisting database 54 using the Cases-In Subroutine 232, Cases-After Subroutine 236, Cases-Similarity Subroutine, Pool-Similarity Subroutine 244, Pool-Paradigm Subroutine 248, Pool-Importance Subroutine 252 and Pool-Paradigm-Similarity Subroutine 256 of the CSPDM 66. In addition, this type of Proximity Indexer Application Program will be designed to be compatible with the commercial third-party databases 54 which are Proximity Indexed using the first type of program. In other words, the researcher in a small business may "weave" in-house documents into a commercial database 54 provided by a thirdparty, so that searches in the large database 54 will automatically bring up any relevant in-house documents, and vice versa.</p>
    <p>The third type of Proximity Indexing Application Program 62 involves the capacity to do Proximity indexing of shapes. Each image or diagram will be treated as a "textual object." The various matrix coefficients can be generated purely from topological analysis of the object itself, or from accompanying textual information about the object, or from a weighted combination of the two. The text is analyzed using the Proximity Indexing Application Program 62 as explained above. Shapes are analyzed according to a coordinate mapping procedure similar to that used in Optical Character Recognition ("OCR"). The numerical "maps" resulting from scanning the images are treated as "textual objects" that can be compared through an analogous weighing algorithm to generate a proximity matrix for every ordered pair of "textual objects" in the database 54. A similarity matrix can then be generated for each ordered pair, and the results organized analogous to a database 54 totally comprised of actual text.</p>
    <p>This third type of Proximity indexing applications program can provide "Proximity Indexed" organization access to many different types of objects. For example, it can be used to search patent diagrams, or compare line drawings of known pottery to a newly discovered archeological find. It can be used to scan through and compare police composite drawings, while simultaneously scanning for similar partial descriptions of suspects. It can be used to locate diagrams of molecular structures, appraise furniture by comparing a new item to a database 54 of past sales, identify biological specimens, etc., etc.</p>
    <p>FIG. 5A is a high level drawing that depicts one embodiment of the GUI Program 70 and its interaction with both the CSPDM 66 and the display 38. The GUI Program 70 has one or more display subroutines. One embodiment contains seven display subroutines. The seven subroutines comprise three textual object display subroutines 504 and four pool display subroutines 508. The three textual object display subroutines 504 are the Cases-In Display Subroutine (CIDS) 512, the Cases-After Display Subroutine (CADS) 516 and the Similar-Cases Display Subroutine (SCDS) 520. The four pool display subroutines 508 are the Pool-Similarity Display Subroutine (PSDS) 524, the Pool-Paradigm Display Subroutine (PPDS) 528, the Pool-Importance Display Subroutine (PIDS) 532 and the Pool-Paradigm-Similarity Display Subroutine (PPSDS) 536. The three textual object display subroutines receive data from the corresponding textual object search subroutine 224 of the CSPDM 66. Similarly, the four pool display subroutines 508 receive data from the corresponding pool search subroutine 212 of the CSPDM 66. Once the display subroutines have processed the data received by the search subroutines, the data is sent to the integrator 540. The integrator 540 prepares the data to be displayed in the proper format on the display 38.</p>
    <p>FIGS. 5B through 5H depict screens generated by the textual object display subroutines, CIDS 512, CADS 516 and SCDS 520. The three types of screens are the Cases In screen 1000, the Cases After screen 1004 and the Similarity Screen 1008, respectively. The Similarity Screen 1008 provides the most "intelligent" information, but all three screens generated by the textual object display subroutineswork in tandem as a system. The other screens created by the pool display subroutines are variances of these three, and also work in tandem with each other and with the three textual object display screens.</p>
    <p>FIG. 5B depicts the "Cases After" 1004 Screen created by the CADS 516 for the textual object, Terry v. Ohio, 392 U.S. 1 (1968). The Cases-After subroutine 236 search produces all of the textual objects in the designated field (here D.C. Circuit criminal cases since 1990) that cite Terry. The number "12" in the upper left hand comer indicates that there are a total of 12 such textual objects. The vertical axis 1012 indicates the degree to which a given textual object relied upon Terry. The number "10" immediately below the 12 indicates that the textual object in the field which most relied upon Terry, namely U.S. v. Tavolacci, 895 F.2d 1423 (D.C. Cir. 1990), discusses or refers to Terry in ten of its paragraphs.</p>
    <p>The Tear-Off Window 1016 feature is illustrated in FIG. 5B by the Tear-Off Window 1016 for U.S. V. McCrory 930 F.2d 63 (D.C. Cir. 1991). The four Tear-Off Window active boxes 1020 (displayed on the Tear-Off Window 1016): 1) open up the full text 1104 of McCrory to the first paragraph that cites Terry; 2) run any of the three searches, namely Cases-In Subroutine 232 Cases-After Subroutine 236 or Cases-Similar Subroutine 240 for McCrory itself (the default is to run the same type of search, namely Cases-After Subroutine 236 again); 3) hide the Terry execute search window 1024; and 4) bring the Terry Execute Search window to the foreground, respectively. The weight numeral 1028 indicates the number of paragraphs in McCrory that discusses or refers to Terry, in this textual object (in this example there is only one).</p>
    <p>The Cases After screen 1004 for a given Textual object B displays a Textual Object Active Box 1032 representing every subsequent textual object in the database 54 that refers explicitly to Textual object B. The analysis starts with the same pool of material as a Shepards™ list for Textual object B. As well as some additional material not gathered by Shepards. However, the Cases After screen 1004 conveys a wealth of information not conveyed by a Shepards™ list.</p>
    <p>The horizontal axis 1036 may represent time, importance or any other means of measurement to rank the textual objects. The Shepards list itself contains no information as to when a case was decided. The vertical axis 1012 similarly may represent any means of measurement to rank the textual objects. In the preferred embodiment, the vertical axis 1012 represents the degree to which the subsequent Textual object C relied upon the original Textual object B. The display 38 makes it obvious when a textual object has received extensive discussion in another textual object, or provides key precedent for a subsequent textual object, or merely mentions the earlier textual object in passing. It also provides guidance as to possible gradations in between extensive, or merely citing.</p>
    <p>The "shape" of the overall pattern of active boxes on the Cases After screen 1004 provides a rich lode of information to be investigated. For example, a "dip" in citation frequency immediately after a particular textual object suggests that the particular textual object, while not formally overruling Textual object B, has largely superseded it. A sudden surge in citation frequency after a particular Supreme Court case may indicate that the Supreme Court has "picked up" and adopted the doctrine first enunciated in Textual object B. The researcher can instantly determine if the holding of Textual object B has been adopted in some circuits but not in others, if Textual object B is losing strength as a source of controlling precedent, etc. None of this information is now available to lawyers in graphical or any other form.</p>
    <p>As with the Cases In screen 1000, every Textual Object Active Box 1032 on the Cases After screen 1004 is active, and includes a Tear-Off Window 1016 that may be moved by dragging on the tear-off window 1016 with a mouse 42, and that tear-off window 1016 becomes a text Tear-Off Window 1040, visible even when one moves on to other searches and other screens. Thus one may "tear off" for later examination every relevant citation to Textual object B, or even for a group of textual objects. The text tear-off windows 1040 "tile"; that is, they can be stacked on top of one another to take up less room. There is also a "Select All" feature (not shown), that creates a file containing the citations of every textual object retrieved in a given search.</p>
    <p>In Cases After screen 1004 mode, clicking on the expanded-view button 1044 of the text tear-off window 1040 opens the text of the subsequent Textual object C to the first place where Textual object B is cited. A paragraph window 1048 displays a paragraph selection box 1052 indicating what paragraph in Textual object C the researcher is reading, and a total paragraph box 1056 indication how many paragraphs Textual object C contains in total. The user can view paragraphs sequentially simply by scrolling through them, or see any paragraph immediately by typing its number in the paragraph selection box 1052. Clicking on a Next paragraph active box 1060 immediately takes the researcher to the next paragraph in Textual object C where Textual object B is mentioned. Traditional Shepardizing allows the researcher to explore the subsequent application of a doctrine in a range of different factual situations, situations that help to define the outer contours of the applicability of a rule. Combining the expanded-view button 1044 functions and "Next Paragraph" active box 1060 functions allows the researcher to study howTextual object B has been used in all subsequent textual objects, in a fraction of the time the same task currently requires with available searching methods.</p>
    <p>Perhaps the most fundamental form of legal research is "Shepardizing." A researcher starts with a textual object known to be relevant, "Textual object B," and locates the "Shepards" for that textual object. The "Shepards" is a list of every subsequent textual object that explicitly refers to Textual object B. The researcher then looks at every single textual object on the list. Shepardizing is often painstaking work. Many subsequent references are made in passing and have almost no legal significance. Although Shepards includes some codes next to its long lists of citations, such as "f" for "followed" and "o" for "overruled," the experience of most lawyers is that such letters cannot be relied upon. For example, the researcher may be citing Textual object B for a different holding than that recognized by the anonymous Shepards reader, interpreting Textual object B differently, or interpreting the subsequent textual object differently. However, for really thorough research, checking a Shepards type of list is essential. The researcher must make absolutely sure that any textual object cited as legal authority in a brief, for instance, has not been superseded by later changes in the law.</p>
    <p>Very often, textual objects located on the Shepards list for Textual object B refer back to other important textual objects, some of which may predate Textual object B, all of which may be Shepardized in turn. This "zig-zag" method of research is widely recognized as the only way to be sure that one has considered the full line of textual objects developing and interpreting a doctrine. The real power of the Cases After screen 1004 emerges when it is used in conjunction with the Cases In screens 1000 and Similarity screens 1008. Using the preferred embodiment, the researcher may engage in the same kind of careful "zig-zag" study of a legal doctrine in a much more efficient manner.</p>
    <p>For example, consider the following hypothetical search. The researcher reads Textual object B, and makes a list of every Supreme Court textual object it substantially relies upon, perhaps six textual objects. The researcher then Shepardizes Textual object B and reads each of those textual objects, in order to find other Supreme Court textual objects that the relied upon, perhaps eight. One then Shepardizes those fourteen Supreme Court decisions, in order to find any Court of Appeals cases in a selected circuit within the last three years on the same basic topic. This process would take at least an hour, even using Shepards through an on-line service. The same search can be performed with the present invention using the Cases In screens 1000 and Cases After screens 1004 in under five minutes.</p>
    <p>In order to perform the same search, a researcher can pull up both the Cases In screens 1000 and Cases After screens 1004 for Textual object B simultaneously. The researcher can then "tear-off" all of the Supreme Court Cases on both lists, run Cases-After Subroutine 236 searches on every Supreme Court Case mentioned on either list, then examine the Cases In screens 1000 for all of the Supreme Court cases produced by these searches. The researcher can locate every recent Court of Appeals case from a selected circuit mentioned in any of those Supreme Court cases. Use of the Similarity screen 1008 as well, allows the researcher to find the pool of relevant Court of Appeals full textual objects even faster.</p>
    <p>FIG. 5C depicts the Cases After Screen 1004 for U.S. v. Lam Kwong-Wah, 924 F.2d 298 (D.C. Cir. 1991). FIG. 5C shows a text Tear-Off Window 1040 on a Cases After Screen 1004, (in this textual object the Tear-Off Window 1016 for U.S. v. Barry, 938 F.2d 1327 (D.C. Cir. 1991), is opened using the full text active box 1064. A text Tear-Off Window 1040 containing the text of Barry opens, to the first cite of U.S. v. Lam Kwong-Wah at paragraph 15. Clicking on the Next Paragraph active box 1060 will open the text of Barry to the next paragraph that cites Lam Kwong-Wah.</p>
    <p>The number "34" in the lower-left corner of the total paragraph box 1056 indicates that Barry has a total of 34 paragraphs in the cite U.S. v. Lam Kwong-Wah. Dragging the small squares 1068 to the left and below the text allow the researcher to move within a paragraph, and from paragraph to paragraph, in the text of Barry, respectively. The empty space below the text 1072 would contain the text of any footnote in paragraph 15. The compress window active box 1074 now closes the window and replaces it with the corresponding active textual object box 1032.</p>
    <p>FIG. 5D depicts the Cases In Screen 1000 for U.S. v. North, 910 F.2d 843 (D.C. Cir. 1990). FIG. 5D contains a Textual Object Active Box 1032 representing every textual object or node with persuasive authority, cited in the text of North. The vertical axis 1012 represents the degree to which North relied upon a given textual object. In this example it is immediately apparent that Kastigar v. United States, 406 U.S. 441 (1972) is the most important precedent, and its Tear-Off Window 1016 have been activated. The weight numeral 1028 indicates that Kastigar is referred to in 77 paragraphs of North.</p>
    <p>A highlighted Textual Object Active Box 1076 can be created by clicking on it, as has been done with U.S. v. Mariana, 851 F.2d 595 (D.C. Cir. 1988). The number "212" in the case number box 1080 indicates that citations to two-hundred-twelvedistinct texts appear in North. Fewer are visible because the textual object activeboxes 1032 "tile" on top of one another; the "Zoom" feature is used to focus on a smaller area of the screen, and ultimately resolves down to a day-by-day level, making all the textual object active boxes 1032 visible.</p>
    <p>The unique Cases In screen 1000 provides a schematic representation of the precedent from which Textual object A is built. The Cases In screen 1000 contains a textual object active box 1032 representing every textual object which is relied upon, or even mentioned, in Textual object A. Any citation in textual object A to a textual object that possesses potential persuasive authority, whether a statute, constitutional provision, treatise, scholarly article, Rule of Procedure, etc., is treated as a "textual object." The textual object active boxes 1032 are color-coded to indicate the court or other source of each textual object. Supreme Court cases are red, Court of Appeals cases are green, District Court cases are blue, and statutes are purple, for example. Each Textual Object Active Box 1032 contains the full official citation 1084 of its textual object. Clicking on any Textual Object Active Box 1032 immediately pulls up a larger window, known as a tear-off window 1016, also containing the full citation 1084 to the textual object (Tear-Off Window Citation 1088), its date 1092, its circuit 1096, and its weight numeral 1028 to the textual object being analyzed. The user may then drag the Tear-Off Window 1016 free of the Textual Object Active Box 1032 and release it.</p>
    <p>This creates a text Tear-Off Window 1040 that remains visible until the researcher chooses to close it, no matter how many subsequent screens the researcher examines. The text Tear-Off Window 1040 can be moved anywhere by dragging it with the mouse 42. The text Tear-Off Window 1040 contains small text active boxes 1100 allowing the researcher to access or "pull up" the full text 1104 of the textual object it represents with a single click of the mouse 42. This feature also allows the researcher to run Cases-In Subroutine 232 Cases-After Subroutine 236 and Cases-Similar Subroutine 240 searches on the textual object. (See below for a description of the Similarity screen 1008).</p>
    <p>The organization of the boxes on the screen, including their position on the horizontal axis 1036 and vertical axis 1012, represents the real "intelligence" behind the Cases-In screen 1000. The horizontal axis 1036 in the preferred embodiment represents time, with the left margin 1108 corresponding to the present, i.e., the date 1092 when the search is run. The right margin 1112 represents the date of decision of the earliest textual object cited in Textual object A. (Certain special materials, such as treatises updated annually, and the U.S. Constitution, are located in a column 1116 to the left of the margin.)</p>
    <p>The vertical axis 1012 in the preferred embodiment represents the degree to which Textual object A relied upon each particular textual object it contains. For example, if the Cases In screen 1000 is run on a district court case (Textual object A) which happens to be a "stop and search" textual object that mainly relies upon Terry v. Ohio 392 U.S. 1 (1968), Terry will be at the top of the screen, with all other textual object active boxes 1032 appearing far below. The researcher can thus access the text of Terry directly without ever reading the text of Textual object A. Of course, the full text 1104 of Textual object A is also instantly available if desired. If the researcher wants to see where Terry "came from," the researchers can instantly, by clicking on a text active box 1100 within the Terry text Tear-Off Window 1040, run the Cases-In Subroutine 232 for Terry--and so on. There is no limit to the number of "levels" or "generations" the researchers may explore using this technique. It is therefore possible (assuming a sufficient database 54) to find, in a matter of seconds, without having to read through layers of texts, the possibly long-forgotten eighteenth-century precursors to a modern doctrine.</p>
    <p>The Cases In screen 1000 creates an instant visual summary or "blueprint" of a textual object. The blueprint can help a researcher make a preliminary judgment about whether a particular textual object is worth closer examination. Viewing the Cases In screens 1000 for a group of textual objects allows a researcher to recognize whether there are precedents common to that group. The blueprint tells the researcher whether Textual object A is primarily a statutory construction case, a textual object that relies on local Court of Appeals cases without Supreme Court support, a textual object relying on precedent outside the circuit 1096 as persuasive authority, etc.</p>
    <p>The initial Cases In screen 1000 presents every citation within a given textual object. In a textual object with an unusually large number of citations, the screen will be crowded with textual object active boxes 1032. The GUI therefore contains a "zoom" feature that allows the researcher to expand any small portion of the screen. To get back to the "big picture," the researcher simply selects the "Fit in Window" menu item, or else selects the "zoom out" feature. The same "zoom," "zoom out," and "Fit in Window" functions are present in the Cases After screen 1004 and Similarity screen 1008 as well.</p>
    <p>The routine that calculates "degree to which Textual object A relies upon the cited textual object" clearly ranks major textual objects at the top, textual objects mentioned only in passing at the bottom, and textual objects of potentially greater relevance in between via display the appropriate textual object active boxes 1032 in the appropriate place. In addition, the routine can recognize when a highly relevant textual object is mentioned only in passing and give a higher weight to that textual object than it would otherwise receive in the ranking procedure.</p>
    <p>The "intelligence" behind the entire GUI is driven by the knowledge that the lawyers do not want the computer to do legal analysis or make judgments for them, but simply guide them through the great mass of irrelevant material to those texts where lawyerly analysis of a problem begins.</p>
    <p>The Cases In screen 1000 is designed with practical legal research in mind. It is common in legal research to locate a lower court textual object on the correct topic, call it "local Textual object A." However, the researcher desired to find the most persuasive authority available. The aim of this type of research is to find the "lead" textual object or textual objects on a particular topic. The researcher ultimatelydesires the first textual object, most famous textual object, and most recent textualobjects of the Supreme Court (or state Supreme Court in state law issues) that stand for the same principle. ("Lead" textual objects also occur at the intermediate and trial court level.)</p>
    <p>The standard way to find lead textual objects is to read through the text of a local Textual object A until one finds references to "higher court textual objects," then look up each of those higher court textual objects in turn. The researcher thenreads the text of those textual objects until the researcher determines the textual objects they have in common, the textual objects that appear many times. Very often, the lower court textual object from which the researcher started is of no real value in and of itself--it may well be from a different local jurisdiction--and the researcher reads through it only to find citations within it. Since the GUI quickly locates and schematically diagrams the textual objects, this process is accelerated dramatically using the GUI.</p>
    <p>FIGS. 5E through 5G depict multiple Similar Case Subroutine 240 searches run in sequence. A Similarity Screen 1008 for U.S. v. Caballero, 936 F.2d 1292 (D.C. Cir. 1991), reveals via the case number box 1080, that 17 textual objects were retrieved by Similar Cases Subroutine 240 search. The vertical axis 1012 indicates that the textual objects retrieved had similarity coefficients 1120 between 4% and 15% with respect to U.S. v. Caballero. Textual objects with less than 4% similarity are not shown. The vertical axis 1012 represents degree of similarity, or topical relatedness, so that 100% would be two identical texts. The Tear-Off Window 1016 of U.S. v. Nurse 916 F.2d 20 (D.C. Cir. 1990) shows that the textual object has a similarity of 9%.</p>
    <p>The Similarity screen for a given Textual object C is organized like the Cases In screen 1000 and Cases After screen 1004, with the same color-coded textual object active boxes representing textual objects, and time on the horizontal axis 1036. However, the vertical axis 1012 represents the degree to which the represented textual object is related to Textual object C. The system is built on the principle that legal doctrines tend to emerge out of lines of textual objects developing a legal principle. Lines of textual objects contain "lead" textual objects that establish basic rules and subsequent textual objects that do not establish new rules, but apply and re-interpret the pre-existing rules in various circumstances. Some lead textual objects invent new doctrines, while others modify or redirect the law based on earlier precedent.</p>
    <p>The routine that operates behind the Similarity screen 1008 determines which line or lines of textual objects that Textual object C can be grouped. The routine then ranks the textual objects in that line depending on how closely they are related to Textual object C. For example, a typical similarity search starting with a Court of Appeals case in a certain circuit, Textual object D, will find the Supreme Court and Court of Appeals cases that have established the principles followed in Textual object D. The Supreme Court and Court of Appeals case will appear as textual object active boxes whether or not they are cited in Textual object D. Furthermore, the Similar Cases Subroutine 240 search will find the textual objects decided subsequent to Textual object D that have applied, and possibly modified, those principles, whether or not those textual objects cite Textual object D.</p>
    <p>Similarity searches allow a researcher to find textual objects on the same topic that do not share common phrases and might be overlooked by a Boolean word search. Similarity searches also allow researchers, who only have an obscure district court case, to "tap in" to the lead textual objects in any area. By organizing all case law in "conceptual space," the Similarity screens 1008 allow one to locate emerging topics that have not been formally recognized by those assigning "key numbers" or otherwise manually classifying textual objects--or even by the authors of the textual objects themselves.</p>
    <p>The "shape" of a Similarity Screen 1008 may convey a great deal of information about a particular legal concept. For example, the screen conveys to the researcher whether a certain concept, which is essentially novel, is supported by Supreme Court case law. Or is an old doctrine that has been recently applied in a new context. The system as a whole gives lawyers the ability to assess what textual objects are "available" on their topic, and to zero in on the textual objects that are most useful. The researcher has the ability to track down every subsequent reference to any particular textual objects by utilizing multiple "Cases After" searches, identifying core precedents through "Cases In" searches, and by running new "Similarity" searches to obtain any textual objects that emerge in closely related topic areas. The "Similarity" algorithm is more "aggressive" then the others, since it contains built-in judgments as to what "relatedness" means. It also judges what is no longer sufficient to display on the screen. The bottom edge of the screen represents a minimum degree of similarity below which the connections are too tenuous to be worth pursuing. In the commercial product, this minimum level can be reset at the preference of the user.</p>
    <p>FIG. 5F is the Similarity Screen 1008 for U.S. v. Nurse. Clicking on the run search Tear-Off Window active box 1128, which is on the Tear-Off Window 1016 for Nurse produces FIG. 5F. Clicking on the Textual Object Active Box 1032 for U.S. v. Jordan. 951 F.2d 1278 (D.C. Cir. 1991) long enough to pull up its Tear-Off Window 1016, and then clicking on Jordan's run search Tear-Off Window active box 1020 (not shown), produces the Similarity Screen 1008 shown in FIG. 5G.</p>
    <p>FIG. 5G shows how multiple tear-off windows 1016 can be shown at the same time, here the U. S. v. Jordan similarity Tear-Off Window 1016 depicts for the three textual objects most similar to Jordan. Note that U.S. v. Jordan, 958 F.2d 1085 (D.C. Cir. 1992), is very closely related, i.e., 41%, to U.S. v. Jordan, 951 F.2d 1278 (D.C. Cir. 1991), apparently as it is a subsequent full textual object decision of the same dispute as the first textual object.</p>
    <p>FIG. 5H depicts a close-up view of an Execute Search Window 1024. The researcher can input a selected textual object that is either represented or not represented on a display 38 screen as a Textual Object Active Box 1032. The researcher can title his search by inputting the title in the Title Search box 1132. The researcher can then input the reference to the selected textual object in the reference input boxes 1136. The reference input boxes of the preferred embodiment allow the researcher to refer to the selected textual object by Volume, Category, Page and/or Section by inputting the appropriate values in the volume reference box, category reference box, page reference box, and/or section reference box, respectively.</p>
    <p>The researcher can also identify the type of search to be performed on the selected textual object by selecting the appropriate search in the Analysis box.</p>
    <p>Once the researcher has inputted all the appropriate values, the researcher executes the search by activating the execute search button.</p>
    <p>Referring generally to FIGS. 5A through 5H, the PSDS 524, PPDS 528, PIDS 532 and PPSDS 536 of the GUI Program 70, also create similar displays to the CIDS 512, CADS 516, and SCDS 520 subroutines. The only major difference between the screens created by the three textual object display subroutines and the four pool display subroutines is the information contained in the Execute Search window and the options available in the analysis box.</p>
    <p>The options in the analysis box enable a researcher to select a textual object outside the pool of textual objects and compare how the selected textual object relates to the pool of textual objects by selecting to the Pool-Similarity Subroutine 244, the Pool-Paradigm Subroutine 248 or Pool-Importance Subroutine 252 of the CSPDM 66.</p>
    <p>The PSDS 524 creates a Pool-Similarity Screen 1008. The vertical axis 1012 ranks the similarity of the objects in a pool of textual objects with respect to a selected textual object. All of the other aspects of this display 38 are similar to the Similar Cases Screen.</p>
    <p>PPDS 528 creates a Pool-Paradigm Screen. The vertical axis 1012 ranks the similarity of the pool of textual objects on the screen with respect to the paradigm textual object. The paradigm textual object is calculated by averaging the mean of all the Euclidean distances of the pool of textual objects on the screen. All of the other aspects of this display 38 are similar to the Similar-Cases Screen.</p>
    <p>The PIDS 532 creates a Pool-Importance Screen. The vertical axis 1012 ranks the importance of the pool of textual objects on the screen. All other aspects of the PIDS 532 display 38 are similar to the Cases-In Screen 1000 and Cases-After Screen 1004.</p>
    <p>The PPSDS 536 creates a Pool-Paradigm Similarity Screen 1008. The vertical axis 1012 represents the similarity of all textual objects in the database 54 to the paradigm textual object created by a selected pool of textual objects. All other aspects of the PPSDS 536 display 38 are similar to Similar-Cases Screen 1008.</p>
    <p>Before displaying the text boxes 1032 representing result nodes 2104 on the screen to the user, the graphical user interface program 70 optimally organizes and arranges the location 1032 of boxes on the X and Y axis. In the preferred embodiment, the GUI Program 70 uses a layout of boxes algorithm to optimally place boxes within a window.</p>
    <p>Referring to FIG. 7, generally, a layout algorithm plots text boxes 1032 on a Cartesian axis as determined by their X and Y values 1200. The algorithm compares the locations of boxes 1032 within a display window to determine if there are any overlapping boxes 1204. In order to perform this comparison, the preferred algorithm initializes a first loop, for i=0 to N, and chooses box<sub>i</sub> to begin the comparison. The algorithm next creates a second loop, for j=1 to N, and chooses box<sub>j</sub> to compare with box<sub>i</sub>. For both loops, N is the number of result nodes obtained. Performing N<sup>2</sup> comparisons provides the optimal number of comparisons needed to determine the existence of any overlaps. If the boxes 1032 are a known size, certain steps may be eliminated from the method.</p>
    <p>More particularly, a preferred algorithm compares the X and Y values of a first and second box 1032 to determine if the boxes 1032 are occupying the same Cartesian space 1204. This comparison is accomplished by identifying the X and Y coordinate pairs of the corners of the two boxes 1032, and then choosing one of the coordinate pairs of a corner of the second box 1032 to be compared. The X value of that pair is compared to the X values of all of the coordinate pairs of the corners of the first box 1032. If the X value of the chosen coordinate pair is a value less than all of the first box corner X values or a value greater than all of the first box corner X values, then the algorithm compares the Y value of the chosen pair to all of the first box corner Y values. If the Y value of the chosen pair is either a value less than all of the first box corner Y values or a value greater than all of the first box corner Y values of the first box 1032, then the algorithm determines that the boxes 1032 do not overlap. The algorithm adds 1 to counter j and then repeats the routine. The routine is repeated until j reaches N and then 1 is added to the i value, and the entire process is repeated again. This particular method ensures that every box is compared with every other box 1032.</p>
    <p>If during the comparison of the X values the algorithm finds that the X value of the chosen pair is greater than one of the first box corner X values, but is less than one of the first box corner X values, then the algorithm determines that the boxes 1032 overlap. If during the comparison of the Y values the algorithm finds that the Y value of the chosen pair is greater than one of the first box corner Y values but is less than one of the first box corner Y values, then the algorithm determines that the boxes 1032 overlap.</p>
    <p>If the preferred algorithm has determined that two boxes 1032 overlap, then the algorithm moves 1208 one of the boxes 1032. Preferably, this is accomplished by adjusting the Y values of the second box 1032 by increasing or adding a predetermined value (to its Y values.) The algorithm performs the above comparison routine 1204 again to see if there is an overlap. If there is an overlap, it moves 1208 the box again. Preferably, it adjusts 1208 the second box's Y value again, and compares 1204 again. If there is no overlap, then the algorithm adds 1 to the j counter and repeats the comparison routine with another box 1032 until N<sup>2</sup> comparisons have been completed.</p>
    <p>When the preferred layout algorithm has ensured that no boxes 1032 overlaps, the algorithm determines whether the results of the search will fit on one screen 1212. The algorithm compares the Y values of each of the boxes 1032 with the highest Y value represented on a single screen display. If the Y value of one or more boxes 1032 exceeds the highest Y value represented on the screen display, then the preferred algorithm increases the length of the X axis and rescales the Y axis to match (e.g., doubling the length of the X axis) 1216. The algorithm again compares the Y values of each of the boxes 1032 to the highest Y value on the screen to determine if the search results will fit on one screen 1212. If they will not, then the algorithm adjusts the X axis 1216 again and compares the Y values 1212 again until the search results fit on one screen.</p>
    <p>Once the search results fit on one screen, the algorithm replots 1200 all of the boxes 1032 to their coordinate positions, and then performs the overlap comparison check 1204 again to see if any boxes 1032 are overlapping. If boxes 1032 are overlapping, the algorithm performs its adjustment step 1208 and the axis resizing step 1216 until the window displays 1220 all of the result nodes on a single screen without any of the boxes overlapping.</p>
    <p>At the option of the user, the algorithm can allow the display 1220 to scroll off the screen in the Y direction or the X direction without resizing 1216 the axes. This option enhances the information content of the map by keeping the scale of the axes small.</p>
    <p>The preferred algorithm can perform this routine by adjusting 1208 the X axis, the Y axis, or both axes. The algorithm has the additional capability of graphically breaking an axis, if one or a few result nodes 2104 are so far away graphically from the main body of result nodes 2104 that representing the far away result nodes 2104 would unnecessarily encumber the graphical display of the main nodes. This graphical break may be represented by a squiggly line at the break point in the axis. Using this axis break allows all of the result nodes 2104 to be displayed on one window, and still maximizes the informational content that the relative spacing on the X or Y axis provides for the result nodes 2104 which are positioned closer together.</p>
    <p>Various other specific methods of optimally organizing and locating boxes 1032 on a graphical computer display 38 may be used with the GUI Program 70.</p>
    <p>In the preferred embodiment, the graphical user interface 70 maximizes the types and quantity of information about particular boxes 1032, nodes 2008, objects in the database 54 that can be displayed without visually overloading the user. The preferred embodiments ergonomically and efficiently represent complex data sets. Each embodiment must strike a balance between that which is technically and intellectually possible to be displayed on a screen, and that which can be visually understood and comprehended by the typical user of the database 54 (on a screen). This method can also be used to display objects retrieved from a network, but it is not the preferred method at the present time.</p>
    <p>An important feature of the invention is its use of a three-dimensional box to communicate information to the user, in addition to the information provided by the location of the box in the X and Y coordinates. Referring to FIG. 8, in the preferred embodiment a three coordinate view or map is displayed on a two dimensional CRT screen. The variables represented by the X, Y, and Z coordinate planes may be interchanged from one coordinate to another. In other words, the X, Y or Z coordinate plane may represent, for example, the variable time. For use of the Z coordinate, it is preferred that a six-sided box 1033 be used and appear to be "floating" at its appropriate location in the Z direction. More importantly, theinvention also can use the depth of the box 1032 (size of the box 1032 in the Zdirection) to convey additional information to the user (in addition to the information provided by the location of the box 1032 in the X, Y, Z coordinate).</p>
    <p>First, using box depth, a bit of binary information is passed along to the user by the fact that the box 1033 has no depth (little or nominal depth) or the box 1033 has a significant depth in the Z direction. In the preferred embodiment, this binary piece of information informs the user of whether or not there is available (hidden) data associated with that box 1033. For example, a box 1033 or node which represents an object in the database 54 may have associated graphics, maps, menus, or text which is not shown. If the box 1033 is shown on the screen as having a significant depth then additional data associated with that box 1033 is available for viewing by the user. If the box 1033 has nominal or no depth then there is no additional data available to the user.</p>
    <p>In addition to the binary information of whether or not additional data is available to the user, in the preferred embodiment the magnitude of the depth of the box 1033 corresponds to the amount of additional or hidden data available to the user. For example, if the box 1033 represents an object in the database 54 which has an extensive amount of associated data the magnitude of the depth of the box 1033 would be large in comparison to other boxes 1033 on the same screen. In this manner, a box 1033 which represents, for example, a textual object of great length would have a larger depth than a box 1033 representing a textual object with little or no text associated with that object in the data base. In this manner, important information is visually passed to the user easily and on the same screen on which other information about the database 54 is being presented.</p>
    <p>Also, in advanced embodiments, the depth of a box 1033 or the fact that a box 1033 has depth may be used to represent to the user that the box 1033 enables the user to tie-in or access another application, program, menu, extension, and/or another database 54. In this way, active boxes 1033 with depth can allow a user great flexibility to move around within the database 54 or within associated database 54s or even to access other applications. This can be particularly useful when the underlying data supporting a node or box 1032 is not located locally at the user's location and requires the user to access communication links or a second database 54 in order to obtain the underlying data. With this invention, the user is able to access the underlying data from the graphical user interface screen.</p>
    <p>Also in advanced embodiments, an axis may represent a variable such as cost data associated with a node 2008 or the cost of accessing the underlying data. In one example, if the data is available only through a separate application which may impose a cost, the box's depth would increase in proportion to that cost. Or, if the application itself imposes a cost for accessing data, then the depth of each box 1033 would represent the cost of accessing that box's data.</p>
    <p>In summary, the depth of a box 1033 provides two types of information. First, binary-type information regarding the presence of additional data or information, or lack thereof, and second, based on the relative measure of the depth of the box, 1033 the amount or size of the underlying data or information which is available and associated with that box 1033. Thus, a box 1033 can be activated and brought to life so that there is an extension that points to either data or another entity independent of the original database 54 which assist the GUI 70 user.</p>
    <p>Additional information concerning the database is presented by this invention by the intelligent use of comments. Comments attached to the textual object boxes provide the user with easy access to vital information contained in the database. FIG. 3E shows the various information types which can be added to the database 54. For example, FIG. 3E shows that links 2004 are assigned weights 2032, that nodes 2008 are assigned node identifications (IDs) 2010 and plot dates 2011 (creation date or the like), that link sub-types 2020 can be assigned names 2021, comment descriptors 2022, comment display orders 2023, comment place holders 2027 and always display comment commands 2030, that node sub-types 2024 can be assigned names 2021 and title descriptors 2026, that node types 2016 can also be assigned names 2021 as well as extra attributes in an extra-attributes table 2016, that link types 2012 can be assigned names 2021 and icon files 2014 for icon graphics and various visual styles 2028 can be assigned to nodes 2008 and links 2004. In addition to those items specifically described, various attributes can be assigned to links 2004, nodes 2008 and link sub-types 2020 and node sub-types 2024. The various additional information which is stored in the database 54 can be shown on maps or on menus when using the database 54. These identifications can be used as part of the searching algorithms discussed previously.</p>
    <p>A unique feature of the graphical user interface program 70 is its ability to optimally space the information within displayed objects. More particularly, the GUI program 70 arranges text and graphics within boxes 1032 or the like on a computer display 38 screen. The preferred GUI Program achieves this by using a box spacing algorithm as shown in FIG. 9. A preferred box spacing algorithm is described below.</p>
    <p>The boxes 1032 used by the preferred GUI Program 70 generally include different types of information or data such as box titles, textual information, and graphical information within the box 1032, as discussed previously. The information types may be assigned to nodes 2008, node sub-types 2024, links 2004, or link sub-types 2020. Preferably, the GUI 70 defines and/or selects points in the box 1032 to serve as anchor points 2200 for each type of information. For example, the GUI 70 may designate a point 2200 near the upper right hand corner of a box 1032 as the anchor point for the graphical information, the lower left hand corner as the anchor for the textual information, and the upper left hand corner as the anchor point for the box title. In the preferred embodiment, the algorithm finds an arrangement which keeps the size of the boxes as small as possible while preventing overlaps between the different types of information.</p>
    <p>Also, the preferred embodiment adjusts the positioning 2212 of the information or data within the box 1032 to make the box 1032 aesthetically pleasing. Preferably, the anchor points are moved or adjusted to arrange or rearrange the content within the box.</p>
    <p>Referring to FIG. 9, generally, the box spacing algorithm plots 2204 the information types at their designated anchor points and determines whether the plotted information fits within the default box size 2208. If necessary, the box 1032 is resized 2212. Following, the algorithm checks for any overlap of information types within the box 2216 and adjusts the location of anchor points 2212, if necessary.</p>
    <p>The overlap checking function performed by the preferred box spacing algorithm is similar to the overlap checking function performed by the preferred layout algorithm discussed above. Various tolerances or thresholds may be set to ensure that the information or data within the box 1032 not only does not overlap, but is sufficiently spaced so that it can be easily understood by a user. If the information overlaps, the anchor points are moved and/or, the boxes are reshaped. Finally, the processed boxes are displayed 2220.</p>
    <p>More particularly, to perform the function of arranging anchor points and reshaping boxes, the preferred box spacing algorithm initializes a loop, for i=0 to N, where N is the number of information types to be displayed on the box; chooses information type<sub>i</sub> and then initialize a second loop, for j=1 to N; and chooses information type<sub>j</sub> to compare to information type<sub>i</sub>.</p>
    <p>The preferred algorithm plots 2204 the first information type on the box 1032 at its designated anchor point. The algorithm plots 2204 the information beginning at the anchor point and fills out horizontally or vertically from there until the information is plotted. After plotting, the algorithm determines if the information fits within a normal or default box size 2208. If the box is too small, the algorithm may adjust 2212 the box 1032 dimensions horizontally or vertically to accommodate the size of the information.</p>
    <p>The algorithm then plots 2204 the second type information in the box 1032. After this information is plotted, the algorithm determines X and Y values of the first information type (using the left and lower edges of the main box as coordinate axes) and compares 2216 them with the X and Y values of the second information type. The box spacing algorithm performs this function in the same manner as the layout algorithm performs its comparison function.</p>
    <p>If there is an overlap, then the algorithm preferably attempts to adjust the location of anchor points 2212 to eliminate overlap. If this is not possible, the algorithm adjusts the size 2212 of the box by a set value in either the X or Y direction. The algorithm re-plots the information types 2204 at their anchor points. Preferably, the anchor points generally remain in the same relative position in the box, but as the box increases in size, the anchor points are in an absolute sense farther away from each other. After adjustments, the algorithm runs the comparison routine 2216 again to determine if the two information types overlap or are aesthetically displeasing. A box 1032 may be aesthetically displeasing if the data within the box 1032 is not evenly or symmetrically distributed, or if data is too close.</p>
    <p>If the information types are appropriately spaced within the box, the algorithm adds 1 to the j counter, and compares 2216 the first information type to the j+1 information type. The algorithm continues to compare 2216 information types until the first information type has been compared 2216 with all of the information types in the box 1032. Then, the algorithm routine returns to the i loop, adds 1 to i, and then compares 2216 the second information type to the other information types until the second has been compared 2216 to all of the information types. If the algorithm ever finds an overlap, the algorithm adjusts 2212 the location of the anchor points and/or the size of the box as described earlier to fit in all of the information. Once the algorithm has compared the information types 2216, found no overlap, and found that the information fits 2208 within the box 1032, it displays 2220 the box 1032. In this way, the graphical user interface program 70 ensures that the information types displayed by the boxes 1032 do not overlap and are aesthetically pleasing, while keeping the size of the box 1032 to a minimum.</p>
    <p>Many box spacing algorithms may be used with the GUI 70. Many variations of the described algorithm are possible which will perform the function of spacing text and/or graphics within a circumscribed space on a display 38. In the preferred embodiment, as shown in FIGS. 10A and 10B, comments 2112 are used extensively on graphical displays to assist the user in understanding the data and relationships of the data the user is viewing. In this manner, a great deal of information about a node 2008 or a link 2004 can be placed in or around a graphical box 1032 display for the node 2008. With this information a user has a better understanding of the relationship between data and the database 54 and the graphical box 1032 represents more than just a location in the X, Y and/or Z coordinate plane.</p>
    <p>The comment descriptor shown on FIG. 3E allows comments 2112 to be assigned to a particular link sub-type and for these comments 2112 to be displayed on the node box 1032 of a linked node 2008. It is preferred that this comment descriptor assigned to a link sub-type be placed on the to-node 2008 of the link 2004. Some examples of possible comment descriptors are "overruled by," "criticizes," "distinguishes." When a node box 1032 is displayed, these comment descriptorsmay be shown in any portion of the node box 1032. In a preferred embodiment, thenode box 1032 is subdivided into three parts: (1) a title place holder part; (2) a graphics place holder part; and (3) an indicator part.</p>
    <p>To specify the specific place within the node box 1032 that the comment will be displayed, a comment place holder 2027, which is a more specific type of the anchor point discussed previously. In the preferred embodiment, a comment place holder 2027, may specify three different place holder 2027 areas (title area, indicator area, or graphics area) in the node box 1032 in which the comment is to be displayed. Various other place holder 2027 options within or in the vicinity of a box are possible.</p>
    <p>Also, using the commands available through the comment display order 2023 or the always display comment commands 2030, the user or designer of the database 54 may specify when particular comments 2112 will or will not be displayed and in what position the comments 2112 will be displayed. In the preferred embodiment, the always display comment 2030 is used to make a comment always available or globally available at any time it is relevant. In other words, the comment will be displayed whenever the to-node box 1032 is drawn on any map. It is preferred that this global comment be used whenever a comment is so important that it should be shown whenever relevant.</p>
    <p>The comment display order 2023 specifies the order or preference in which to display multiple comments 2116 in one comment place holder 2027. In the preferred embodiment, a number in the range of zero (0) to two hundred fifty-five (255) is assigned as the priority of any specific comment 2112. Wherein zero (0) signifies that the comment 2112 has high priority and should be displayed at the top of the title or indicator place holder or on the left in the graphics place holder while a value of two hundred fifty-five (255) means that the comment 2112 has very low priority and should be located at the bottom of the title or indicator place holder 2027 or on the right in the graphics place holder.</p>
    <p>The always display comment 2030 can be simply a binary value of zero (0) or one (1) wherein if the value is zero (0) the comment 2112 is only displayed on the to-node 2008 when a link 2004 of the specified link type 2012 is represented on a map and the from-node 2008 is also on the map. A one (1) means that the comment 2112 is displayed on the to-node 2008 at all times whether or not the from-node 2008 appears on the map.</p>
    <p>Comments 2112 may be active or inactive. Active comments 2112 provide another means for a user to navigate in the database 54 in a customized and flexible manner. Active comments 2112 allow a user to jump or to access a menu, a map or an extension by selecting the comment 2112. The active comments 2112 may also allow a user to jump into a particular object in the database 54. In the preferred embodiment, comments 2112 which are always displayed or are global comments are preferably active comments. Comments 2112 which are assigned low priorities and/or are not global are preferably not active comments 2112. Referring to FIG. 10C, the comment 2112 may be an icon or graphics such as the red flag 2020 shown in the node boxes 1032.</p>
    <p>Coloring, shading, texture and background can be useful and very effective tools for visually passing information to a user. Shading, texture or coloring can be used both within boxes 1032 on the screen and in the background area of the maps or screen displays. The coloring or background inside a box 1032 can represent a particular data type. In one embodiment, the user chooses a color to assign to all the distinct data types used in the database 54. When the user subsequent uses the invention, the invention will display those data types in the color chosen by the user. For example, in a medical database where boxes 1032 represent patients, patients admitted through an emergency room can be assigned a different color box 1032 than patients admitted through a normal process, or patients that survive a procedure may have a different color box 1032 than patients who die. This allows a user to see at a glance what type of data he or she is looking at. Changing the color between boxes 1032 is particularly useful and is discussed in further detail later.</p>
    <p>Some of the preferred uses for passing additional information through the background are changing the background type of a map at a particular point on the X, Y, or Z coordinate. A specific example would be changing the background coloring on a map at a particular point on an axis where that point on the axis represents an average, a median, or an important date. Another example is creating a background coloring band between two points on the same axis representing an acceptable or ideal range for a variable. Either the computer or the user can choose what value to change the background type around. The background can change on more than one axis creating "panels" or areas within a map or screen.</p>
    <p>Finally, for purposes of consistency within a particular application of the graphical user interface 70, the coloring of the background of maps of the same type are preferably the same or similar. For example, source maps showing the source for a particular searched object may all have yellow background while influence maps which show objects that have been influenced from an identified object may all have a blue background for the map. In this way, background, coloring and texture can play an important role in visually providing information to the user on a map or screen with the present invention.</p>
    <p>In order to present the most aesthetically pleasing display or output, the preferred GUI Program 70 chooses an optimal bit map 2300 or swatch to create a graphical display. In particular, the GUI 70 determines the color, resolution, and style supported by a display 38, output from a printing device or any other computer output. The GUI Program 70 preferably accomplishes this by categorizing general types of displays 38 and output devices and assigning bit maps 2300 or swatches for use with those general types. These general types may include types of printing devices such as color printers, laser printers, inkjet printers, dot matrix printers and types of displays such 38 as black and white monitors and color monitors with differing resolution capabilities. This feature of the GUI 70 chooses the optimal bit maps 2300 or swatches to use as fill-in on boxes 1032 and the like used in the graphical display.</p>
    <p>To achieve this capability the GUI Program 70 preferably uses an algorithm to determine what type of display 38 or printer or other output device is being used by the user. The algorithm then matches that type with one of the general types of categories stored in a look up table 2304, as shown in FIG. 11. If the type of display 38 or output device is an exact match with one of the stored types, then the algorithm instructs the GUI Program to use the bit map 2300 indicated by the table. If the type of display or printer does not match with one of the stored types, then the algorithm determines the optimal bitmap 2300 for this display 38 or printer.</p>
    <p>The preferred bit map fill algorithm determines an optimal fill by determining the category the display 38 or printer being used is closest to, and then picking a bitmap 2300 according to certain weighted factors. The algorithm preferably chooses a bitmap 2300 or swatch that will optimize the color depth and resolution of the display 38 or printer. If both color depth and resolution cannot be optimized by one bitmap 2300, the algorithm preferably chooses a category of bitmaps which will optimize the display 38 or printer's color depth, and then looks in that sub-category for bitmaps 2300 which will optimize its resolution. The use of this algorithm results in graphical outputs that take advantage of the user's hardware capabilities.</p>
    <p>The GUI Program 70 also preferably uses the look up table to determine the best bit map 2300 to be used as a background for the windows 2300. The GUI Program 70 executes an algorithm which determine what type of display 38 is being used and accesses the look up table 2304 to determine the preferred bitmap 2300. If the type of display 38 being used is not in the table, the algorithm preferably selects the bitmap 2300 that is the best fit, again weighing factors such as color depth and resolution in determining the best bitmap 2300 for display as a window background 2308.</p>
    <p>It is preferred that the graphical user interface (GUI 70) use a windows approach or a Windows® type application. The preferred GUI 70 for a database 54 is unusual in that during the normal course of operation it is common (in fact preferred) for many search map windows (1000, 1004, 1008) to be visible at any given time. The preferred GUI 70 embodiments utilize various mechanisms to help manage these windows (1000, 1004, 1008) and avoid confusing the user with too many "open" or active windows (1000, 1004, 1008).</p>
    <p>An example of the type of hardware which may be used to implement a preferred window management system is shown in FIG. 1. Specifically, it is preferred that a processor 30, display 38, memory 34, 58, and a input device such as a mouse 42 or keyboard 46 are used. Although the GUI 70 is described primarily for use with a database management system, the GUI 70 may be used with many other software applications and in many other hardware configurations.</p>
    <p>For the preferred GUI 70 window management system embodiments, a parent window (or parent frame window) is used with multiple active child windows. Various mechanisms or commands may be utilized to help manage a plurality of active windows (1000, 1004, 1008). For example, cascading may be used to arrange the currently displayed windows (1000, 1004, 1008) in an orderly, consistently-overlapping fashion. The windows (1000, 1004, 1008) are arranged such that each newly activated window (1000, 1004, 1008) is a fixed size, and the title bars of previous windows (1000, 1004, 1008) are still visible. Tiling may also be used to arrange the currently displayed windows (1000, 1004, 1008) in an orderly, non-overlapping fashion. When using tiling, the child windows are drawn as large as possible within the parent frame window, covering the entire frame window area. There are two preferred methods of tiling, Tile Vertical and Tile Horizontal. Vertical tiling generally involves the side by side display of child windows (e.g., two windows (1000, 1004, 1008) side by side), shown in FIG. 13A, while horizontal is above and below (e.g. two windows (1000, 1004, 1008), one above and one below), shown in FIG. 13B. Minimizing may be used to display or represent a particular child window in a very small space, examples of representative displays include graphics, icons and/or a text titles. The minimized child window may be displayed at various places in the parent window (e.g. at the bottom of the parent window, taskbar, or titlebar). Maximizing may also be used to display a particular child window as large as possible within a parent windows area. A maximized child window covers or obscures all other active child windows. Restoring may be used to restore a minimized or maximized child window to its previous state. Icon arranging may be performed to arrange all child windows being represented as icons in an orderly fashion.</p>
    <p>In addition, in the preferred GUI 70 embodiment an auto arrange feature is utilized for enhanced window management. The auto arrange feature solves many of the problems inherent in an interface which creates a large number of child windows. When the number of child windows is large, no arrangement that tries to display all windows at the same time works very well. The child windows either become too small or too cluttered. Forcing the user to manually select a subset of the child windows in which the user is most interested, manually arranging those windows (1000, 1004, 1008) to be viewed in a primary format and minimizing the rest of the windows (1000, 1004, 1008) for viewing in a secondary format (or ignored). The user must perform this window management each time a new arrangement is desired which often means each time a new window (1000, 1004, 1008) is activated or displayed. The auto arrange feature automates this process for the user and intelligently arranges the windows (1000, 1004, 1008) for the user's screen.</p>
    <p>With the auto arrange feature a limit is placed on the number of windows (1000, 1004, 1008) to be displayed in the primary format at any one time, a desired number of activated windows (1000, 1004, 1008). This limit may be set by default, by the user, or by an intelligent process which analyzes for example, the amount of data to be visually represented, screen size, and other variables to determine an optimum number of windows (1000, 1004, 1008) and a layout for those windows (1000, 1004, 1008).</p>
    <p>Referring generally to FIG. 12, one version of the auto arrange process involves the following general steps: (1) Based on a default value or through an intelligent process, identify the most recently activated windows (1000, 1004, 1008) which will be allocated the greatest amount of screen space 2080; (2) Using one of several methods, minimize the screen size of the remaining windows (1000, 1004, 1008) so that their identities may be recognized by the user but only need a small amount of screen space (e.g. icons, text) 2084; (3) Arrange the identified windows (1000, 1004, 1008) in a useful and space efficient manner (e.g. vertically, horizontally, cubes etc. 2088) (4) Arrange the minimized but recognizable windows (1000, 1004, 1008) in an orderly but non-obtrusive manner on the screen (e.g. arrange icons in lower corner of screen 2092). Using this automated process, the windows (1000, 1004, 1008) can be automatically rearranged each time a new window (1000, 1004, 1008) is activated (by repeating the above steps) or whenever the user initiates the process. The windows (1000, 1004, 1008) are kept in an organized and useable fashion with little effort on the part of the user. The auto arrange can use different formats (primary, secondary, tertiary, etc) for different levels of interest in the window (1000, 1004, 1008). The auto arrange feature can also be turned on or off at the will of the user.</p>
    <p>Instead of recognizing and minimizing the windows (1000, 1004, 1008) which are beyond the desired number of active windows (1000, 1004, 1008), the system may simply ignore these windows (1000, 1004, 1008), or some combination of minimizing and ignoring may be used 2084. For example, if the desired number of active windows (1000, 1004, 1008) for display is two, the last two activated windows (1000, 1004, 1008) may by arranged on the screen side by side in a full format and an additional three windows (1000, 1004, 1008) may be recognized and minimized to icons for display on a small portion of the screen. Any windows (1000, 1004, 1008) beyond the last five activated windows (1000, 1004, 1008) are ignored by the GUI 70 window management system.</p>
    <p>By providing some options, preferences options, for the auto arrange feature, the feature can be customized to the particular taste of a user. The desired arrangement of the windows (1000, 1004, 1008), or target arrangement can be explicitly chosen by the user and changed at will by the user (or chosen from a list of available formats). For example, a user can specify the number of windows (1000, 1004, 1008) to display, the particular format each window (1000, 1004, 1008) will appear on the screen, and the layout of the screen. There possible screen layouts are nearly limitless. Various formats are possible for each window (1000, 1004, 1008), for example as 1/2, 3/4, full, vertically stretched, horizontally stretched or enlarged format. The user can chose the number of windows (1000, 1004, 1008) to be displayed in each format for example two, three, or four windows (1000, 1004, 1008) in the 1/2 format. And therefore, a target arrangement can be chosen such as full format, two windows (1000, 1004, 1008), vertically side by side. Thus, when step three of the auto arrange process is preformed, the system will identify the two most recently activated windows (1000, 1004, 1008) and arrange the two windows (1000, 1004, 1008) in the target arrangement, side by side, rather than in some other manner. In the preferred embodiment, a menu is provided to the user permitting the user to chose a target arrangement including number of windows (1000, 1004, 1008), format of windows (1000, 1004, 1008), and screen arrangement.</p>
    <p>For "high-end" power users, the window management system can be modified to allow the user to custom build nearly any arbitrary layout for the screen. The user creates any number of arbitrary layouts, each of which is given a name that is inserted into a window menu and is stored in a database. After a layout has been named, it is then treated as a new window management command which can be executed. In the most sophisticated embodiments, through the use of pointer and/or a mouse 42 the user selects anchor points, such as center points, or upper left, upper right, lower left, lower right, and various shapes for the windows (1000, 1004, 1008) such as hand sketched, rectangular, triangular, rhomboids, octagons etc. In this manner displays can be generated which are suited for specific uses. Also, through this medium, the artistry and creativity of the user may be expressed in aesthetically pleasing displays.</p>
    <p>An innovative feature of the preferred embodiment is the ability to call up a search screen or map while viewing the data of a particular object in the database 54. This feature is implemented through the use of embedded active links 2004. By using embedded icons that are active within the data of an object being viewed or by using embedded text which is active within the data of an object in the database 54, this feature allows the user to jump from viewing data to a search screen, menu, map or the like. The search screen or map can be one which has been previously generated or can be generated at the time of selecting the embedded active icon or active text.</p>
    <p>The preferred method of using this feature is with text documents. Active icons or active text are embedded within the text documents and the user is alerted to these active icons or text through the use of highlighting or different coloring of the active icon or text. When the user sees an active icon or active text while viewing an object in the database 54, the user may choose to jump out of the object and into a map, search screen, or the like.</p>
    <p>The system may be configured so that upon selection of an active icon or active portion of text, a menu is displayed to the user wherein the user may select the generation of a particular map or the return to an existing map that was previously generated.</p>
    <p>Although these active links 2004 within an object in the database 54 have been described for use in jumping from an object in the database 54 to a map, the active links 2004 may be used to jump to other objects in the database 54 or extensions to other databases 54, other applications, or communication programs. Providing active links 2004 within objects being viewed in a database 54 allows great flexibility for the user to navigate through data in any manner he chooses.</p>
    <p>To allow access to extensions to other databases, the preferred embodiment is set up in a modular fashion in order to be able to modularly add extensions or add on links to connect to other applications or programs which can be called up from the present invention.</p>
    <p>In the preferred embodiment, the invention is set up in a modular fashion to accept one or more extensions. An extension can be another application or can be a communications link to connect to another computer or application. Use of these connections is particularly well suited for the invention in that the underlying data need not be stored locally with the user, but instead, through the use of extensions, the underlying data can be accessed by the user through an extension, another application and/or through a communications link.</p>
    <p>Multiple extensions are possible and it is possible for the same underlying data to be available through one or more extensions, this allows the user to choose which extension or communication link it will use to access underlying data. In the preferred implementation, a box 1032 is given depth to signify that an extension associated with a particular box 1032 is available to the user. By activating the box 1032, the user is given the opportunity to use the extension. In the preferred embodiment with modular implementations of extensions, a user can add on or plug in further extensions or eliminate extensions.</p>
    <p>Another feature of the preferred embodiment is the "show usage" command. FIG. 8 is a screen display 38 depicting the use of the "show usage" command. The preferred embodiment includes this command to allow the user to see a portion of an object in the database 54 which uses cites or refers to the node 2008 from which the show usage command is requested. More specifically, the show usage command allows the user to see the text or data of a portion of the document that is represented by the node 2008 being searched. In the preferred embodiment, the show usage command is only available from a result node 2104. When a map or graphics display is shown the search node 2100 is the node 2008 upon which the search being displayed is based. The result nodes 2104 are the nodes 2008 which are graphically displayed as a result of the search conducted upon the search node 2100. Referring to FIG. 8, the search node 2100 is Alves v. Commissioner and the result node 2104 is 26 U.S.C. §83.</p>
    <p>Through the use of the show usage command, a user may immediately access that portion of the search node object 2108 or document 2108 which refers to a specific result node 2104. This is accomplished by breaking up the data connected to the search node 2100 into groups of records with header identifiers. The data attached to the result node 2104 will also have an identifier, a header identifier, which particularly identifies the data attached to it. When the user executes the show usage command after activating the result node 2104, the record or records in the data attached to the source node 2100 which match the result node 2104 identifier will be displayed and highlighted. For example, in FIG. 8, the Alves v. Commissioner document 2108 is shown highlighted at the appropriate location identifying 26 U.S.C. §83 2104. The show usage command is accessed through the use of a pull-down menu from the result node 26 U.S.C. §83 2104. Using the earlier example of modern an classical architecture, if the search node 2100 was modern architecture and the search requested items influencing modern architecture an influence map or graphic display would be generated which would include the classical architecture node. By selecting the classical architecture node and using the "show usage" command on the classical architecture node (for example through a pull-down menu or directly in the node box) the invention will immediately bring the user to the first location in the modern architectural data where classical architecture is referred to as influencing modern architecture. Thus, in effect, the show usage command allows the user to jump from a result node 2104 to the specific location 2108 in the search node 2100 where the result node 2104 is referenced or identified.</p>
    <p>Finally, one of the most important features of the invention is its method of integrating itself with third party software applications. Although useful with many third party software applications, it is particularly useful to integrate the present invention with third party database applications which operate in a windows type environment. Nearly all of the database management functions and graphical user interface 70 features can be used in an integrated scheme with third party database management software.</p>
    <p>The preferred method of integrating the present invention with third party software is through the use of a subclassing technique in a windows multiple document interface (MDI) environment. Specifically, the present invention can take advantage of the common behavior exhibited by MDI applications to integrate with third party software operating in a windows environment.</p>
    <p>When the preferred embodiment of the invention is loaded to be used in conjunction with third party software application, the invention immediately subclasses the third party software applications frame window. Through this subclassing technique, the present invention receives (intercepts) every message or command originally intended for the third party software. Since the invention is the first to receive each window message, it acts as a message arbiter. The message arbiter has the ability to recognize the message or command and decide how each message should be processed. For example, the arbiter decides whether any given message should be processed by the master program (the invention) or by the subclassed third party software.</p>
    <p>The precise processing that is appropriate for a given message is somewhat message dependent. However, the general message scheme dictates that messages intended for one of the child MDI windows (or that depend in some way on the content of a child window) are dispatched to the application that is the "real" owner or creator of that child window. Thus, most messages are dispatched to the software application to which the child window belongs (to which the window is a native), if it the window belongs to a subclassed application, the message is directed or forwarded to the subclassed application. The subclassed application then processes the forwarded message and changes a child window display if necessary. Using this technique, the subclassed software application acts as if it alone owns the main frame window. Thus, operation of the master program has little affect on the performance of subclassed program. Further, the operation of the subclassed program is transparent to the user. To the casual user, the master program operates all the windows and is the only user interface used.</p>
    <p>Using this technique, more than one software application may be subclassed with the present invention. Also, each subclassed application may have multiple child window displays. And finally, the master application may generate its own native windows which may be displayed simultaneously with the child windows of a subclassed application.</p>
    <p>This computerized system for researching data is also effective with any type of internal or global network application (see generally FIGS. 14A and 14B). As long as a network stores data and provides links 2004 between that data, this system can provide an effective and efficient system for indexing, searching, and displaying that data. For example, this system can be applied to the Internet and the World Wide Web. The World Wide Web is made up of numerous web sites which contain documents and internet or web pages. Documents are usually defined in the art as unique pieces of data, which includes text files, graphic files, audio files, and video files. A web page is usually a document with its own Universal Resource Locator (URL). URLs are the standardized addresses commonly used for web pages. Generally, web sites are a collection of web pages and documents. Web sites are usually identified by a home page, which may contain an overall starting point for the web site and a summary of what is to be found at the web site. Hyperjump links, or hyperlinks, is the name commonly given to the links which connect web pages, web sites, and documents on the web. Hyperlinks are electronic links which allow end users to jump to the specified web page or web site. The software code commonly used to create the majority of web pages containing text files is HyperText Markup Language (HTML). Other pages containing graphics, audio, video, and other resources may not be coded in HTML, but still will be connected by hyperlinks.</p>
    <p>The Internet can be viewed as an immense collection of linked documents providing varied information to the public via an elaborate electronic distribution channel. In the past, the end user's ability to search, find, index, and navigate through relevant documents of interest has been primarily limited to word based queries which primarily rely on the target document's text indexing. Instead of relying on textual searching, this method and apparatus for indexing, searching, and displaying data analyzes hyperlinks which connect web pages to other web pages in order to help the end user to search, find, and navigate through the relevant documents of interest. This system analyzes hyperlinks using proximity indexing or clustering technology discussed previously. Once identified, the system displays the results in a variety of ways and end users are able to navigate directly to the documents identified by this system's analyzation technology.</p>
    <p>In the preferred embodiment, this system uses the cluster link generation algorithm described in FIG. 3H to search and identify closely associated documents located on the Internet in the same manner as described above. The system treats hyperlinks 2004 on the Web in the same manner as it treats links 2004 in a database, and it treats web pages on the Web in the same manner as it treats nodes 2008 in a database 54. Source links 2004 on the Web link a source node 2008 (or source web page) to a second node (or second web page). Influence links 2004 perform the same function in reverse. Direct links 2032 (as described above) are the same as hyperlinks 2004, which use URLs, in the World Wide Web, and they directly link one web page (or node) to another. Indirect links 2036 link two web pages or nodes 2008 through more than one path. A cluster link, for purposes of the Web, is any relationship between two web pages.</p>
    <p>To begin the process, as shown generally in FIG. 14A, a node 2008 is chosen 3000 for analysis. Next, the system accesses link data 3004 or "crawls" the source web page (or source node 2008) looking for URLs which directly link the source web page to other web pages. Web crawling is a known technique in the art, performed by most World Wide Web search services, such as Yahoo (located at www.yahoo.com) or Alta Vista. Crawling is accomplished by the use of automated programs called robots or spiders, which analyze a web page for objects which provide URL links to other web pages or documents. The source node 2008, whether it is a web page, the home page of a web site, or a document with no links 2004, is a data document which may have been encoded in HTML or some other language. The encoded data document includes commands such as "insert picture here" or "begin a new paragraph" or "place a link here to another document" along with the normal text of the document. These coded commands are generally invisible to the end user, although many Web documents reveal text containing coded links 2004 to other documents in different colors. The system reads the coded HTML instructions to identify 3008 the coded links, which are direct links 2032. There are many publicly known methods of identifying links 2004 from a coded document that one skilled in the art could employ to perform this function.</p>
    <p>FIG. 14B describes the embodiment of the invention which executes 3020 the cluster link generator algorithm 2044 to generate direct and indirect links 2004 to find the set of candidate cluster links. After identifying 3008 all of the URLs referenced in the source web page, in the preferred embodiment, the cluster link generation algorithm 2044 retrieves 2056 a list of URLs and classifies them as the direct links 2032 to be analyzed. The cluster link generator 2044 traces the links 2032 to their destination nodes 2008 (a web site or web page) and performs a web crawl to retrieve 2056 a list of URLs referenced by the source nodes 2008. The generator 2044 classifies the second set of nodes 2008 as being indirectly linked to the source node 2004, and the links 2036 to these nodes 2008 are added 2072 to the list of candidate cluster links. In order to find the set of candidate cluster links, the cluster link generator 2044 repeats the above steps 2052. In the more general method described in FIG. 14A, the system identifies 3012 the links 2036 which have an indirect relationship and then displays 3020 the direct 2032 and indirect 2036 links.</p>
    <p>Once a candidate cluster link set is identified, the generator 2044 assigns 2064, 2076 weights 2034 to the candidate cluster links 2004. The weight 2034 of each individual path or link 2004 is a function of the weight 2034 of the path to the previous node 2008 and the weight 2034 of the last link 2004. In order to determine the weight 2034 of an implied link 2004, the preferred formula, WC<sub>i+1</sub> =min(WC<sub>i</sub>,D<sub>i+1</sub> *W<sub>i+1</sub>) 2064, as previously discussed, is used. Following weighting, the generator 2044 sorts the set of candidate cluster links 2004 by weight, and a subset of these links 2004 (those links 2004 above a specified cut-off weight) are retained for display 3020 to the end user. In the preferred embodiment, the formula T=min(constant, 4*d), discussed before determines the optimal cut-off weight.</p>
    <p>In another embodiment, the Proximity Indexing Application Program (Program) 62 organizes and categorizes the crawled links 2004 using the statistical techniques and empirically generated algorithms described earlier in this application. The Program 62 treats URL addresses as citations and web pages as textual objects. The Program 62 applies some or all of the eighteen pattern list to determine the relatedness of the web pages (or nodes) which are linked to the source web page (or node). The Program 62 weighs the patterns by importance, giving one type of data document more importance than another type. For example, it may give more importance to a web site than to a single document which has no other links. The Program 62 may use other factors to weigh the data documents, such as the number of "hits" (visits by other end users to the site, a number which is available to web users) a data document receives in a specific time frame or the number of hyperlinks within a page. The Program 62 then forms a matrix based on ordered pairs of documents, and the matrix calculations discussed before of this specification can be carried out. The Program 62 generates a coefficient of similarity which will determine the relatedness of web pages to each other and to the source web page. The Program 62 displays the most similar web pages to the user.</p>
    <p>The preferred embodiment of the network application of this system uses the graphical user interface program 70 to display the results of the algorithm as a list showing the selected links 2004 and the various data associated with the links 2004. The links 2004 shown on the screen to the end user are active links 2004, similar to the active comments used in the text boxes 1032 described previously in this application. The end user may instantaneously link to the destination node 2008 that the user selects. The list format provides link information in a style familiar to user of the Internet. However, this system is also capable of displaying the results in the user-friendly graphical format as described above. The graphical user interface program 70 described previously uses box coloring and sizing to communicate large amounts of information quickly and intelligibly to the user. In a preferred embodiment, different colors for boxes 1032 are assigned depending on what type of node 2008 they represent (e.g., a web page, web site, a document, a file transfer protocol (FTP) (a common internet designation for news sites)). Preferably, the box 1032 is given depth. The amount of URL links a node 2008 contains may determine the amount of depth.</p>
    <p>The graphical user interface program 70 displays a list of the most related web pages to the source web page. This list includes documents, web sites, and pages which are directly or indirectly linked to the subject document or the subject topic. The links 2004 can be source links 2004 or influence links 2004, so the end user may monitor the sites to which his site (the source web page) is referring, and the end user may view the sites which are referring to his site. The system can parse the URL of the destination nodes 2008 for a variety of information. Thus, the end user may monitor whether the connections to which his web site refers are still open, the end user may view the date and time a destination node 2008 was modified, and the end user may view the identification of the organization or author of the destination node that directly or indirectly links to the source node 2008 . The GUI program 70 displays all of this information either in the list format or in the text box 1032 used in the graphical format. Graphical comments may be placed in the text box to communicate information quickly, such as showing a happy face for a connected application, and so forth. Hyperlinks can appear as active comments in a text box in order to allow the user to instantaneously jump to the web page represented by the text box.</p>
    <p>Although this computerized system for researching data is described as functioning in the World Wide Web environment, it can function equally well in any network system. A network that utilizes any type of hyperjump 2004 to connect documents together can serve as the links 2004 analyzed by this invention. This system therefore can be modified to navigate and search through internal company networks, and provide the same features as described above for the Web application. Additionally, the comment boxes can be tailored to display critical information about company files, thus enhancing its usefulness for the company employee who is attempting to sort through company documents stored on a network.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4868733">US4868733</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 26, 1986</td><td class="patent-data-table-td patent-date-value">Sep 19, 1989</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Document filing system with knowledge-base network of concept interconnected by generic, subsumption, and superclass relations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5265065">US5265065</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 8, 1991</td><td class="patent-data-table-td patent-date-value">Nov 23, 1993</td><td class="patent-data-table-td ">West Publishing Company</td><td class="patent-data-table-td ">Method and apparatus for information retrieval from a database by replacing domain specific stemmed phases in a natural language to create a search query</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5287493">US5287493</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 31, 1990</td><td class="patent-data-table-td patent-date-value">Feb 15, 1994</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Database interactive prompted query system having named database tables linked together by a user through join statements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5386556">US5386556</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 23, 1992</td><td class="patent-data-table-td patent-date-value">Jan 31, 1995</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Natural language analyzing apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5471611">US5471611</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 12, 1992</td><td class="patent-data-table-td patent-date-value">Nov 28, 1995</td><td class="patent-data-table-td ">University Of Strathclyde</td><td class="patent-data-table-td ">Computerised information-retrieval database systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5544352">US5544352</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 14, 1993</td><td class="patent-data-table-td patent-date-value">Aug 6, 1996</td><td class="patent-data-table-td ">Libertech, Inc.</td><td class="patent-data-table-td ">Method and apparatus for indexing, searching and displaying data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5546517">US5546517</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 7, 1994</td><td class="patent-data-table-td patent-date-value">Aug 13, 1996</td><td class="patent-data-table-td ">Mitsubishi Electric Information Technology Center America, Inc.</td><td class="patent-data-table-td ">Apparatus for determining the structure of a hypermedia document using graph partitioning</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5586311">US5586311</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 14, 1994</td><td class="patent-data-table-td patent-date-value">Dec 17, 1996</td><td class="patent-data-table-td ">American Airlines, Inc.</td><td class="patent-data-table-td ">Object oriented data access and analysis system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5630120">US5630120</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 8, 1995</td><td class="patent-data-table-td patent-date-value">May 13, 1997</td><td class="patent-data-table-td ">Bull, S.A.</td><td class="patent-data-table-td ">Method to help in optimizing a query from a relational data base management system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5649193">US5649193</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 17, 1995</td><td class="patent-data-table-td patent-date-value">Jul 15, 1997</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Document detection system using detection result presentation for facilitating user&#39;s comprehension</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Agosti, et al., "<a href='http://scholar.google.com/scholar?q="a+Two-level+Hypertext+Retrieval+Model+for+Legal+Data%2C"'>a Two-level Hypertext Retrieval Model for Legal Data,</a>" SIGIR '91 (1991).</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Agosti, et al., a Two level Hypertext Retrieval Model for Legal Data, SIGIR 91 (1991).</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Belew, Richard, "<a href='http://scholar.google.com/scholar?q="A+Connectionist+Approach+to+Conceptual+Information+Retrieval%2C"'>A Connectionist Approach to Conceptual Information Retrieval,</a>" ICAIL '87 (1987).</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Belew, Richard, A Connectionist Approach to Conceptual Information Retrieval, ICAIL 87 (1987).</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Fowler, et al., "<a href='http://scholar.google.com/scholar?q="Integrating+Query%2C+Thesaurus+and+Documents+Through+a+Common+Visual+Representation%2C+"'>Integrating Query, Thesaurus and Documents Through a Common Visual Representation, </a>" SIGIR '91 (1991).</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Fowler, et al., Integrating Query, Thesaurus and Documents Through a Common Visual Representation, SIGIR 91 (1991).</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Gelbart &amp; Smith, "<a href='http://scholar.google.com/scholar?q="Beyond+Boolean+Search%3A+Flexicon%2C+A+Legal+Text-Based+Intelligent+System%2C"'>Beyond Boolean Search: Flexicon, A Legal Text-Based Intelligent System,</a>" ICAIL '91 (1991).</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Gelbart &amp; Smith, Beyond Boolean Search: Flexicon, A Legal Text Based Intelligent System, ICAIL 91 (1991).</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Lin, "<a href='http://scholar.google.com/scholar?q="A+Self-Organizing+Semantic+Map+for+Information+Retrieval%2C"'>A Self-Organizing Semantic Map for Information Retrieval,</a>" SIGIR '91 (1991).</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Lin, A Self Organizing Semantic Map for Information Retrieval, SIGIR 91 (1991).</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Rose &amp; Belew, "<a href='http://scholar.google.com/scholar?q="Legal+Information+Retrieval%3A+A+Hybrid+Approach%2C"'>Legal Information Retrieval: A Hybrid Approach,</a>" ICAIL '89 (1989).</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Rose &amp; Belew, Legal Information Retrieval: A Hybrid Approach, ICAIL 89 (1989).</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Turtle &amp; Croft, "<a href='http://scholar.google.com/scholar?q="Interference+Networks+for+Document+Retrieval%2C"'>Interference Networks for Document Retrieval,</a>" SIGR '90 (1990).</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Turtle &amp; Croft, Interference Networks for Document Retrieval, SIGR 90 (1990).</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Uzzi, V Search Integration Toolkit for Folio Views, User s Manual, 6 Dec. 1995.</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Uzzi, V Search Publisher s Toolkit, Beta Release 2.0, User s Manual, Dec. 8, 1995.</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Uzzi, V-Search Integration Toolkit for Folio Views, User's Manual, 6 Dec. 1995.</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Uzzi, V-Search Publisher's Toolkit, Beta Release 2.0, User's Manual, Dec. 8, 1995.</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5915251">US5915251</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 3, 1998</td><td class="patent-data-table-td patent-date-value">Jun 22, 1999</td><td class="patent-data-table-td ">Digital Equipment Corporation</td><td class="patent-data-table-td ">Method and apparatus for generating and searching range-based index of word locations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5940825">US5940825</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 4, 1996</td><td class="patent-data-table-td patent-date-value">Aug 17, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Adaptive similarity searching in sequence databases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5995982">US5995982</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 28, 1998</td><td class="patent-data-table-td patent-date-value">Nov 30, 1999</td><td class="patent-data-table-td ">Micron Technology, Inc.</td><td class="patent-data-table-td ">Method and device for file transfer by cascade release</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6018738">US6018738</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 22, 1998</td><td class="patent-data-table-td patent-date-value">Jan 25, 2000</td><td class="patent-data-table-td ">Microsft Corporation</td><td class="patent-data-table-td ">Methods and apparatus for matching entities and for predicting an attribute of an entity based on an attribute frequency value</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6021409">US6021409</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 8, 1998</td><td class="patent-data-table-td patent-date-value">Feb 1, 2000</td><td class="patent-data-table-td ">Digital Equipment Corporation</td><td class="patent-data-table-td ">Method for parsing, indexing and searching world-wide-web pages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6073135">US6073135</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 10, 1998</td><td class="patent-data-table-td patent-date-value">Jun 6, 2000</td><td class="patent-data-table-td ">Alta Vista Company</td><td class="patent-data-table-td ">Connectivity server for locating linkage information between Web pages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6138128">US6138128</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 2, 1997</td><td class="patent-data-table-td patent-date-value">Oct 24, 2000</td><td class="patent-data-table-td ">Microsoft Corp.</td><td class="patent-data-table-td ">Sharing and organizing world wide web references using distinctive characters</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6144973">US6144973</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 5, 1997</td><td class="patent-data-table-td patent-date-value">Nov 7, 2000</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Document requesting system and method of receiving related document in advance</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6154213">US6154213</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 29, 1998</td><td class="patent-data-table-td patent-date-value">Nov 28, 2000</td><td class="patent-data-table-td ">Rennison; Earl F.</td><td class="patent-data-table-td ">Immersive movement-based interaction with large complex information structures</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6182127">US6182127</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 12, 1998</td><td class="patent-data-table-td patent-date-value">Jan 30, 2001</td><td class="patent-data-table-td ">Digital Paper, Llc</td><td class="patent-data-table-td ">Network image view server using efficent client-server tilting and caching architecture</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6189019">US6189019</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 14, 1996</td><td class="patent-data-table-td patent-date-value">Feb 13, 2001</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Computer system and computer-implemented process for presenting document connectivity</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6202068">US6202068</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 2, 1998</td><td class="patent-data-table-td patent-date-value">Mar 13, 2001</td><td class="patent-data-table-td ">Thomas A. Kraay</td><td class="patent-data-table-td ">Database display and search method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6226620">US6226620</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 11, 1997</td><td class="patent-data-table-td patent-date-value">May 1, 2001</td><td class="patent-data-table-td ">Yeong Kuang Oon</td><td class="patent-data-table-td ">Iterative problem solving technique</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6233571">US6233571</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 4, 1998</td><td class="patent-data-table-td patent-date-value">May 15, 2001</td><td class="patent-data-table-td ">Daniel Egger</td><td class="patent-data-table-td ">Method and apparatus for indexing, searching and displaying data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6246803">US6246803</a></td><td class="patent-data-table-td patent-date-value">Jun 21, 1999</td><td class="patent-data-table-td patent-date-value">Jun 12, 2001</td><td class="patent-data-table-td ">The University Of Kansas</td><td class="patent-data-table-td ">Real-time feature-based video stream validation and distortion analysis system using color moments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6278993">US6278993</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 3, 2000</td><td class="patent-data-table-td patent-date-value">Aug 21, 2001</td><td class="patent-data-table-td ">Yodlee.Com, Inc.</td><td class="patent-data-table-td ">Method and apparatus for extending an on-line internet search beyond pre-referenced sources and returning data over a data-packet-network (DPN) using private search engines as proxy-engines</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6282540">US6282540</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 26, 1999</td><td class="patent-data-table-td patent-date-value">Aug 28, 2001</td><td class="patent-data-table-td ">Vicinity Corporation</td><td class="patent-data-table-td ">Method and apparatus for efficient proximity searching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6285999">US6285999</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 9, 1998</td><td class="patent-data-table-td patent-date-value">Sep 4, 2001</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Method for node ranking in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6292894">US6292894</a></td><td class="patent-data-table-td patent-date-value">Nov 23, 1999</td><td class="patent-data-table-td patent-date-value">Sep 18, 2001</td><td class="patent-data-table-td ">Science Applications International Corporation</td><td class="patent-data-table-td ">System, method, and medium for retrieving, organizing, and utilizing networked data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6295514">US6295514</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 4, 1997</td><td class="patent-data-table-td patent-date-value">Sep 25, 2001</td><td class="patent-data-table-td ">3-Dimensional Pharmaceuticals, Inc.</td><td class="patent-data-table-td ">Method, system, and computer program product for representing similarity/dissimilarity between chemical compounds</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6351755">US6351755</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 2, 1999</td><td class="patent-data-table-td patent-date-value">Feb 26, 2002</td><td class="patent-data-table-td ">Alta Vista Company</td><td class="patent-data-table-td ">System and method for associating an extensible set of data with documents downloaded by a web crawler</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6397224">US6397224</a></td><td class="patent-data-table-td patent-date-value">Dec 10, 1999</td><td class="patent-data-table-td patent-date-value">May 28, 2002</td><td class="patent-data-table-td ">Gordon W. Romney</td><td class="patent-data-table-td ">Anonymously linking a plurality of data records</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6418433">US6418433</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 28, 1999</td><td class="patent-data-table-td patent-date-value">Jul 9, 2002</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and method for focussed web crawling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6421612">US6421612</a></td><td class="patent-data-table-td patent-date-value">Nov 4, 1997</td><td class="patent-data-table-td patent-date-value">Jul 16, 2002</td><td class="patent-data-table-td ">3-Dimensional Pharmaceuticals Inc.</td><td class="patent-data-table-td ">System, method and computer program product for identifying chemical compounds having desired properties</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6434490">US6434490</a></td><td class="patent-data-table-td patent-date-value">Dec 17, 1998</td><td class="patent-data-table-td patent-date-value">Aug 13, 2002</td><td class="patent-data-table-td ">3-Dimensional Pharmaceuticals, Inc.</td><td class="patent-data-table-td ">Method of generating chemical compounds having desired properties</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6446065">US6446065</a></td><td class="patent-data-table-td patent-date-value">Feb 29, 2000</td><td class="patent-data-table-td patent-date-value">Sep 3, 2002</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Document retrieval assisting method and system for the same and document retrieval service using the same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6453246">US6453246</a></td><td class="patent-data-table-td patent-date-value">May 7, 1998</td><td class="patent-data-table-td patent-date-value">Sep 17, 2002</td><td class="patent-data-table-td ">3-Dimensional Pharmaceuticals, Inc.</td><td class="patent-data-table-td ">System, method, and computer program product for representing proximity data in a multi-dimensional space</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6457004">US6457004</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 1, 1998</td><td class="patent-data-table-td patent-date-value">Sep 24, 2002</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Document retrieval assisting method, system and service using closely displayed areas for titles and topics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6466940">US6466940</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 11, 1998</td><td class="patent-data-table-td patent-date-value">Oct 15, 2002</td><td class="patent-data-table-td ">Dudley John Mills</td><td class="patent-data-table-td ">Building a database of CCG values of web pages from extracted attributes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6505194">US6505194</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 29, 2000</td><td class="patent-data-table-td patent-date-value">Jan 7, 2003</td><td class="patent-data-table-td ">Koninklijke Philips Electronics N.V.</td><td class="patent-data-table-td ">Search user interface with enhanced accessibility and ease-of-use features based on visual metaphors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6510459">US6510459</a></td><td class="patent-data-table-td patent-date-value">Jan 29, 2001</td><td class="patent-data-table-td patent-date-value">Jan 21, 2003</td><td class="patent-data-table-td ">Digital Paper Corporation</td><td class="patent-data-table-td ">Network image view server using efficient client-server, tiling and caching architecture</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6513027">US6513027</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 16, 1999</td><td class="patent-data-table-td patent-date-value">Jan 28, 2003</td><td class="patent-data-table-td ">Oracle Corporation</td><td class="patent-data-table-td ">Automated category discovery for a terminological knowledge base</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6529898">US6529898</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 22, 1999</td><td class="patent-data-table-td patent-date-value">Mar 4, 2003</td><td class="patent-data-table-td ">Matthew Shawn Fortner</td><td class="patent-data-table-td ">Method and system for electronically retrieving door hardware data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6567850">US6567850</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 1999</td><td class="patent-data-table-td patent-date-value">May 20, 2003</td><td class="patent-data-table-td ">Yodlee, Inc.</td><td class="patent-data-table-td ">System and method for determining revenue from an intermediary derived from servicing data requests</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6571227">US6571227</a></td><td class="patent-data-table-td patent-date-value">May 3, 1999</td><td class="patent-data-table-td patent-date-value">May 27, 2003</td><td class="patent-data-table-td ">3-Dimensional Pharmaceuticals, Inc.</td><td class="patent-data-table-td ">Method, system and computer program product for non-linear mapping of multi-dimensional data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6629097">US6629097</a></td><td class="patent-data-table-td patent-date-value">Apr 14, 2000</td><td class="patent-data-table-td patent-date-value">Sep 30, 2003</td><td class="patent-data-table-td ">Douglas K. Keith</td><td class="patent-data-table-td ">Displaying implicit associations among items in loosely-structured data sets</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6654738">US6654738</a></td><td class="patent-data-table-td patent-date-value">Dec 17, 2001</td><td class="patent-data-table-td patent-date-value">Nov 25, 2003</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Computer program embodied on a computer-readable medium for a document retrieval service that retrieves documents with a retrieval service agent computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6671627">US6671627</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 2001</td><td class="patent-data-table-td patent-date-value">Dec 30, 2003</td><td class="patent-data-table-td ">3-D Pharmaceuticals, Inc.</td><td class="patent-data-table-td ">Method and computer program product for designing combinatorial arrays</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6711586">US6711586</a></td><td class="patent-data-table-td patent-date-value">Jul 17, 2000</td><td class="patent-data-table-td patent-date-value">Mar 23, 2004</td><td class="patent-data-table-td ">William Mitchell Wells</td><td class="patent-data-table-td ">Methods and systems for providing information based on similarity</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6714941">US6714941</a></td><td class="patent-data-table-td patent-date-value">Jul 19, 2000</td><td class="patent-data-table-td patent-date-value">Mar 30, 2004</td><td class="patent-data-table-td ">University Of Southern California</td><td class="patent-data-table-td ">Learning data prototypes for information extraction</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6718324">US6718324</a></td><td class="patent-data-table-td patent-date-value">Jan 30, 2003</td><td class="patent-data-table-td patent-date-value">Apr 6, 2004</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Metadata search results ranking system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6745183">US6745183</a></td><td class="patent-data-table-td patent-date-value">Apr 29, 2002</td><td class="patent-data-table-td patent-date-value">Jun 1, 2004</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Document retrieval assisting method and system for the same and document retrieval service using the same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6760638">US6760638</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 16, 2000</td><td class="patent-data-table-td patent-date-value">Jul 6, 2004</td><td class="patent-data-table-td ">Esko Graphics, Nv</td><td class="patent-data-table-td ">Method and apparatus for resolving overlaps in a layout containing possibly overlapping designs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6763148">US6763148</a></td><td class="patent-data-table-td patent-date-value">Nov 13, 2000</td><td class="patent-data-table-td patent-date-value">Jul 13, 2004</td><td class="patent-data-table-td ">Visual Key, Inc.</td><td class="patent-data-table-td ">Image recognition methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6772149">US6772149</a></td><td class="patent-data-table-td patent-date-value">Sep 23, 1999</td><td class="patent-data-table-td patent-date-value">Aug 3, 2004</td><td class="patent-data-table-td ">Lexis-Nexis Group</td><td class="patent-data-table-td ">System and method for identifying facts and legal discussion in court case law documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6799174">US6799174</a></td><td class="patent-data-table-td patent-date-value">Sep 17, 2001</td><td class="patent-data-table-td patent-date-value">Sep 28, 2004</td><td class="patent-data-table-td ">Science Applications International Corporation</td><td class="patent-data-table-td ">Retrieving, organizing, and utilizing networked data using databases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6799176">US6799176</a></td><td class="patent-data-table-td patent-date-value">Jul 6, 2001</td><td class="patent-data-table-td patent-date-value">Sep 28, 2004</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Method for scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6834239">US6834239</a></td><td class="patent-data-table-td patent-date-value">Aug 22, 2001</td><td class="patent-data-table-td patent-date-value">Dec 21, 2004</td><td class="patent-data-table-td ">Victor S. Lobanov</td><td class="patent-data-table-td ">Method, system, and computer program product for determining properties of combinatorial library products from features of library building blocks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6834276">US6834276</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 25, 1999</td><td class="patent-data-table-td patent-date-value">Dec 21, 2004</td><td class="patent-data-table-td ">Integrated Data Control, Inc.</td><td class="patent-data-table-td ">Database system and method for data acquisition and perusal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6938046">US6938046</a></td><td class="patent-data-table-td patent-date-value">Mar 2, 2001</td><td class="patent-data-table-td patent-date-value">Aug 30, 2005</td><td class="patent-data-table-td ">Dow Jones Reuters Business Interactive, Llp</td><td class="patent-data-table-td ">Polyarchical data indexing and automatically generated hierarchical data indexing paths</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6961731">US6961731</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 15, 2001</td><td class="patent-data-table-td patent-date-value">Nov 1, 2005</td><td class="patent-data-table-td ">Kooltorch, L.L.C.</td><td class="patent-data-table-td ">Apparatus and method for organizing and/or presenting data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6996572">US6996572</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 8, 1997</td><td class="patent-data-table-td patent-date-value">Feb 7, 2006</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and system for filtering of information entities</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7020847">US7020847</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 6, 1999</td><td class="patent-data-table-td patent-date-value">Mar 28, 2006</td><td class="patent-data-table-td ">Siemens Aktiengesellschaft</td><td class="patent-data-table-td ">Search and navigation device for hypertext documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7039621">US7039621</a></td><td class="patent-data-table-td patent-date-value">Mar 22, 2001</td><td class="patent-data-table-td patent-date-value">May 2, 2006</td><td class="patent-data-table-td ">Johnson &amp; Johnson Pharmaceutical Research &amp; Development, L.L.C.</td><td class="patent-data-table-td ">System, method, and computer program product for representing object relationships in a multidimensional space</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7054757">US7054757</a></td><td class="patent-data-table-td patent-date-value">Jan 29, 2002</td><td class="patent-data-table-td patent-date-value">May 30, 2006</td><td class="patent-data-table-td ">Johnson &amp; Johnson Pharmaceutical Research &amp; Development, L.L.C.</td><td class="patent-data-table-td ">for generating mapping coordinates of combinatorial library products from features of library building blocks; pattern analysis, information representation, information cartography and data mining</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7058628">US7058628</a></td><td class="patent-data-table-td patent-date-value">Jul 2, 2001</td><td class="patent-data-table-td patent-date-value">Jun 6, 2006</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Method for node ranking in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7058647">US7058647</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 31, 2000</td><td class="patent-data-table-td patent-date-value">Jun 6, 2006</td><td class="patent-data-table-td ">Charles E. Hill &amp; Associates</td><td class="patent-data-table-td ">Electronic presentation generation system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7062572">US7062572</a></td><td class="patent-data-table-td patent-date-value">Mar 19, 2001</td><td class="patent-data-table-td patent-date-value">Jun 13, 2006</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system to determine the geographic location of a network user</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7089233">US7089233</a></td><td class="patent-data-table-td patent-date-value">Sep 6, 2001</td><td class="patent-data-table-td patent-date-value">Aug 8, 2006</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and system for searching for web content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7117187">US7117187</a></td><td class="patent-data-table-td patent-date-value">May 2, 2003</td><td class="patent-data-table-td patent-date-value">Oct 3, 2006</td><td class="patent-data-table-td ">Johnson &amp; Johnson Pharmaceutical Reseach &amp; Develpment, L.L.C.</td><td class="patent-data-table-td ">Method, system and computer program product for non-linear mapping of multi-dimensional data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7127685">US7127685</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 2002</td><td class="patent-data-table-td patent-date-value">Oct 24, 2006</td><td class="patent-data-table-td ">America Online, Inc.</td><td class="patent-data-table-td ">Instant messaging interface having a tear-off element</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7139739">US7139739</a></td><td class="patent-data-table-td patent-date-value">Apr 3, 2001</td><td class="patent-data-table-td patent-date-value">Nov 21, 2006</td><td class="patent-data-table-td ">Johnson &amp; Johnson Pharmaceutical Research &amp; Development, L.L.C.</td><td class="patent-data-table-td ">Method, system, and computer program product for representing object relationships in a multidimensional space</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7159023">US7159023</a></td><td class="patent-data-table-td patent-date-value">Dec 16, 2003</td><td class="patent-data-table-td patent-date-value">Jan 2, 2007</td><td class="patent-data-table-td ">Alexa Internet</td><td class="patent-data-table-td ">Use of web usage trail data to identify relationships between browsable items</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7159207">US7159207</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 9, 2001</td><td class="patent-data-table-td patent-date-value">Jan 2, 2007</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Method and apparatus for accessing related computer objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7174339">US7174339</a></td><td class="patent-data-table-td patent-date-value">Mar 7, 2000</td><td class="patent-data-table-td patent-date-value">Feb 6, 2007</td><td class="patent-data-table-td ">Tririga Llc</td><td class="patent-data-table-td ">Integrated business system for the design, execution, and management of projects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7188055">US7188055</a></td><td class="patent-data-table-td patent-date-value">Mar 12, 2001</td><td class="patent-data-table-td patent-date-value">Mar 6, 2007</td><td class="patent-data-table-td ">Johnson &amp; Johnson Pharmaceutical Research, &amp; Development, L.L.C.</td><td class="patent-data-table-td ">Method, system, and computer program for displaying chemical data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7197697">US7197697</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 15, 2000</td><td class="patent-data-table-td patent-date-value">Mar 27, 2007</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Apparatus for retrieving information using reference reason of document</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7269587">US7269587</a></td><td class="patent-data-table-td patent-date-value">Dec 1, 2004</td><td class="patent-data-table-td patent-date-value">Sep 11, 2007</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7281215">US7281215</a></td><td class="patent-data-table-td patent-date-value">Jul 31, 2002</td><td class="patent-data-table-td patent-date-value">Oct 9, 2007</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">IM conversation counter and indicator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7284207">US7284207</a></td><td class="patent-data-table-td patent-date-value">Sep 6, 2006</td><td class="patent-data-table-td patent-date-value">Oct 16, 2007</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">Instant messaging interface having a tear-off element</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7328136">US7328136</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 15, 2004</td><td class="patent-data-table-td patent-date-value">Feb 5, 2008</td><td class="patent-data-table-td ">Council Of Scientific &amp; Industrial Research</td><td class="patent-data-table-td ">Computer based method for finding the effect of an element in a domain of N-dimensional function with a provision for N+1 dimensions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7330856">US7330856</a></td><td class="patent-data-table-td patent-date-value">Oct 30, 2001</td><td class="patent-data-table-td patent-date-value">Feb 12, 2008</td><td class="patent-data-table-td ">Tririga Llc</td><td class="patent-data-table-td ">Item specification object management system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7363291">US7363291</a></td><td class="patent-data-table-td patent-date-value">Mar 29, 2002</td><td class="patent-data-table-td patent-date-value">Apr 22, 2008</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Methods and apparatus for increasing efficiency of electronic document delivery to users</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7370277">US7370277</a></td><td class="patent-data-table-td patent-date-value">Dec 23, 2002</td><td class="patent-data-table-td patent-date-value">May 6, 2008</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">E-mail interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7383282">US7383282</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 17, 2001</td><td class="patent-data-table-td patent-date-value">Jun 3, 2008</td><td class="patent-data-table-td ">Anthony David Whitehead</td><td class="patent-data-table-td ">Method and device for classifying internet objects and objects stored on computer-readable media</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7386834">US7386834</a></td><td class="patent-data-table-td patent-date-value">Jun 28, 2002</td><td class="patent-data-table-td patent-date-value">Jun 10, 2008</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Undo/redo technique for token-oriented representation of program code</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7416524">US7416524</a></td><td class="patent-data-table-td patent-date-value">Feb 18, 2000</td><td class="patent-data-table-td patent-date-value">Aug 26, 2008</td><td class="patent-data-table-td ">Johnson &amp; Johnson Pharmaceutical Research &amp; Development, L.L.C.</td><td class="patent-data-table-td ">System, method and computer program product for fast and efficient searching of large chemical libraries</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7421661">US7421661</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 2002</td><td class="patent-data-table-td patent-date-value">Sep 2, 2008</td><td class="patent-data-table-td ">Aol Llc</td><td class="patent-data-table-td ">Instant messaging interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7437351">US7437351</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 2003</td><td class="patent-data-table-td patent-date-value">Oct 14, 2008</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Method for searching media</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7457818">US7457818</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 14, 2006</td><td class="patent-data-table-td patent-date-value">Nov 25, 2008</td><td class="patent-data-table-td ">Accenture Global Services Gmbh</td><td class="patent-data-table-td ">Context-based display technique with hierarchical display format</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7474759">US7474759</a></td><td class="patent-data-table-td patent-date-value">Nov 13, 2001</td><td class="patent-data-table-td patent-date-value">Jan 6, 2009</td><td class="patent-data-table-td ">Pixel Velocity, Inc.</td><td class="patent-data-table-td ">Digital media recognition apparatus and methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7475083">US7475083</a></td><td class="patent-data-table-td patent-date-value">May 17, 2005</td><td class="patent-data-table-td patent-date-value">Jan 6, 2009</td><td class="patent-data-table-td ">Factiva, Inc.</td><td class="patent-data-table-td ">Polyarchical data indexing and automatically generated hierarchical data indexing paths</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7487443">US7487443</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 30, 2004</td><td class="patent-data-table-td patent-date-value">Feb 3, 2009</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Portal page view layout based on weights</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7496597">US7496597</a></td><td class="patent-data-table-td patent-date-value">Dec 11, 2003</td><td class="patent-data-table-td patent-date-value">Feb 24, 2009</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Techniques for spatial representation of data and browsing based on similarity</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7529756">US7529756</a></td><td class="patent-data-table-td patent-date-value">Dec 22, 2000</td><td class="patent-data-table-td patent-date-value">May 5, 2009</td><td class="patent-data-table-td ">West Services, Inc.</td><td class="patent-data-table-td ">System and method for processing formatted text documents in a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7552190">US7552190</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 1999</td><td class="patent-data-table-td patent-date-value">Jun 23, 2009</td><td class="patent-data-table-td ">Verticalone Corporation</td><td class="patent-data-table-td ">System and method for automated electronic notification and transaction execution</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7593863">US7593863</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 9, 2001</td><td class="patent-data-table-td patent-date-value">Sep 22, 2009</td><td class="patent-data-table-td ">Smiths Detection Inc.</td><td class="patent-data-table-td ">System for measuring and testing a product using artificial olfactometry and analytical data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7593920">US7593920</a></td><td class="patent-data-table-td patent-date-value">Apr 4, 2002</td><td class="patent-data-table-td patent-date-value">Sep 22, 2009</td><td class="patent-data-table-td ">West Services, Inc.</td><td class="patent-data-table-td ">System, method, and software for identifying historically related legal opinions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7620626">US7620626</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 30, 2006</td><td class="patent-data-table-td patent-date-value">Nov 17, 2009</td><td class="patent-data-table-td ">West Services, Inc.</td><td class="patent-data-table-td ">System, method, and software for identifying historically related legal opinions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7657655">US7657655</a></td><td class="patent-data-table-td patent-date-value">May 24, 2006</td><td class="patent-data-table-td patent-date-value">Feb 2, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system to determine the geographic location of a network user</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7668822">US7668822</a></td><td class="patent-data-table-td patent-date-value">Sep 18, 2006</td><td class="patent-data-table-td patent-date-value">Feb 23, 2010</td><td class="patent-data-table-td ">Become, Inc.</td><td class="patent-data-table-td ">Method for assigning quality scores to documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7672879">US7672879</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 2000</td><td class="patent-data-table-td patent-date-value">Mar 2, 2010</td><td class="patent-data-table-td ">Yodlee.Com, Inc.</td><td class="patent-data-table-td ">Interactive activity interface for managing personal data and performing transactions over a data packet network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7672950">US7672950</a></td><td class="patent-data-table-td patent-date-value">May 3, 2005</td><td class="patent-data-table-td patent-date-value">Mar 2, 2010</td><td class="patent-data-table-td ">The Boston Consulting Group, Inc.</td><td class="patent-data-table-td ">Method and apparatus for selecting, analyzing, and visualizing related database records as a network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7680812">US7680812</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 16, 2005</td><td class="patent-data-table-td patent-date-value">Mar 16, 2010</td><td class="patent-data-table-td ">Telenor Asa</td><td class="patent-data-table-td ">Method, system, and computer program product for searching for, navigating among, and ranking of documents in a personal web</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7711622">US7711622</a></td><td class="patent-data-table-td patent-date-value">Mar 5, 2008</td><td class="patent-data-table-td patent-date-value">May 4, 2010</td><td class="patent-data-table-td ">Stephen M Marceau</td><td class="patent-data-table-td ">Financial statement and transaction image delivery and access system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7711738">US7711738</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 15, 2000</td><td class="patent-data-table-td patent-date-value">May 4, 2010</td><td class="patent-data-table-td ">West Services, Inc.</td><td class="patent-data-table-td ">Method, system and computer-readable medium for accessing and retrieving court records, items and documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7729990">US7729990</a></td><td class="patent-data-table-td patent-date-value">Apr 14, 2004</td><td class="patent-data-table-td patent-date-value">Jun 1, 2010</td><td class="patent-data-table-td ">Stephen Michael Marceau</td><td class="patent-data-table-td ">Check image access system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7752198">US7752198</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 24, 2007</td><td class="patent-data-table-td patent-date-value">Jul 6, 2010</td><td class="patent-data-table-td ">Telenor Asa</td><td class="patent-data-table-td ">Method and device for efficiently ranking documents in a similarity graph</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7752535">US7752535</a></td><td class="patent-data-table-td patent-date-value">Dec 1, 2005</td><td class="patent-data-table-td patent-date-value">Jul 6, 2010</td><td class="patent-data-table-td ">Yodlec.com, Inc.</td><td class="patent-data-table-td ">Categorization of summarized information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7765279">US7765279</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 1999</td><td class="patent-data-table-td patent-date-value">Jul 27, 2010</td><td class="patent-data-table-td ">Verticalone Corporation</td><td class="patent-data-table-td ">System and method for scheduling harvesting of personal information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7778954">US7778954</a></td><td class="patent-data-table-td patent-date-value">Mar 6, 2006</td><td class="patent-data-table-td patent-date-value">Aug 17, 2010</td><td class="patent-data-table-td ">West Publishing Corporation</td><td class="patent-data-table-td ">Systems, methods, and software for presenting legal case histories</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7797344">US7797344</a></td><td class="patent-data-table-td patent-date-value">Dec 23, 2005</td><td class="patent-data-table-td patent-date-value">Sep 14, 2010</td><td class="patent-data-table-td ">Become, Inc.</td><td class="patent-data-table-td ">Method for assigning relative quality scores to a collection of linked documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7814089">US7814089</a></td><td class="patent-data-table-td patent-date-value">Sep 28, 2007</td><td class="patent-data-table-td patent-date-value">Oct 12, 2010</td><td class="patent-data-table-td ">Topix Llc</td><td class="patent-data-table-td ">System and method for presenting categorized content on a site using programmatic and manual selection of content items</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7836043">US7836043</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 8, 2004</td><td class="patent-data-table-td patent-date-value">Nov 16, 2010</td><td class="patent-data-table-td ">Robert Leland Jensen</td><td class="patent-data-table-td ">Database system and method for data acquisition and perusal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7853883">US7853883</a></td><td class="patent-data-table-td patent-date-value">Jun 26, 2007</td><td class="patent-data-table-td patent-date-value">Dec 14, 2010</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Content aggregation view layout based on weights</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7856386">US7856386</a></td><td class="patent-data-table-td patent-date-value">Sep 17, 2009</td><td class="patent-data-table-td patent-date-value">Dec 21, 2010</td><td class="patent-data-table-td ">Yodlee, Inc.</td><td class="patent-data-table-td ">Host exchange in bill paying services</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7865498">US7865498</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 23, 2002</td><td class="patent-data-table-td patent-date-value">Jan 4, 2011</td><td class="patent-data-table-td ">Worldwide Broadcast Network, Inc.</td><td class="patent-data-table-td ">Broadcast network platform system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7877697">US7877697</a></td><td class="patent-data-table-td patent-date-value">Oct 5, 2007</td><td class="patent-data-table-td patent-date-value">Jan 25, 2011</td><td class="patent-data-table-td ">Aol Inc.</td><td class="patent-data-table-td ">IM conversation counter and indicator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7900148">US7900148</a></td><td class="patent-data-table-td patent-date-value">May 5, 2008</td><td class="patent-data-table-td patent-date-value">Mar 1, 2011</td><td class="patent-data-table-td ">Aol Inc.</td><td class="patent-data-table-td ">E-mail interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7908253">US7908253</a></td><td class="patent-data-table-td patent-date-value">Aug 7, 2008</td><td class="patent-data-table-td patent-date-value">Mar 15, 2011</td><td class="patent-data-table-td ">Factiva, Inc.</td><td class="patent-data-table-td ">Polyarchical data indexing and automatically generated hierarchical data indexing paths</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7908277">US7908277</a></td><td class="patent-data-table-td patent-date-value">Feb 5, 2007</td><td class="patent-data-table-td patent-date-value">Mar 15, 2011</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Annotating links in a document based on the ranks of documents pointed to by the links</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7930647">US7930647</a></td><td class="patent-data-table-td patent-date-value">Dec 11, 2005</td><td class="patent-data-table-td patent-date-value">Apr 19, 2011</td><td class="patent-data-table-td ">Topix Llc</td><td class="patent-data-table-td ">System and method for selecting pictures for presentation with text content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7971154">US7971154</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 16, 2006</td><td class="patent-data-table-td patent-date-value">Jun 28, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Text box numbering and linking visual aids</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7984053">US7984053</a></td><td class="patent-data-table-td patent-date-value">Nov 16, 2009</td><td class="patent-data-table-td patent-date-value">Jul 19, 2011</td><td class="patent-data-table-td ">West Services, Inc.</td><td class="patent-data-table-td ">System, method, and software for identifying historically related legal cases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7987172">US7987172</a></td><td class="patent-data-table-td patent-date-value">Aug 30, 2004</td><td class="patent-data-table-td patent-date-value">Jul 26, 2011</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Minimizing visibility of stale content in web searching including revising web crawl intervals of documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8042112">US8042112</a></td><td class="patent-data-table-td patent-date-value">Jun 30, 2004</td><td class="patent-data-table-td patent-date-value">Oct 18, 2011</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Scheduler for search engine crawler</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8065275">US8065275</a></td><td class="patent-data-table-td patent-date-value">Feb 15, 2007</td><td class="patent-data-table-td patent-date-value">Nov 22, 2011</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Systems and methods for cache optimization</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8069407">US8069407</a></td><td class="patent-data-table-td patent-date-value">Sep 7, 2000</td><td class="patent-data-table-td patent-date-value">Nov 29, 2011</td><td class="patent-data-table-td ">Yodlee.Com, Inc.</td><td class="patent-data-table-td ">Method and apparatus for detecting changes in websites and reporting results to web developers for navigation template repair purposes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8108393">US8108393</a></td><td class="patent-data-table-td patent-date-value">Jan 9, 2009</td><td class="patent-data-table-td patent-date-value">Jan 31, 2012</td><td class="patent-data-table-td ">Hulu Llc</td><td class="patent-data-table-td ">Method and apparatus for searching media program databases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8126818">US8126818</a></td><td class="patent-data-table-td patent-date-value">Dec 30, 2003</td><td class="patent-data-table-td patent-date-value">Feb 28, 2012</td><td class="patent-data-table-td ">West Publishing Company</td><td class="patent-data-table-td ">Knowledge-management systems for law firms</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8126884">US8126884</a></td><td class="patent-data-table-td patent-date-value">Jan 28, 2010</td><td class="patent-data-table-td patent-date-value">Feb 28, 2012</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8131701">US8131701</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 29, 2010</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">Patentratings, Llc</td><td class="patent-data-table-td ">Method and system for probabilistically quantifying and visualizing relevance between two or more citationally or contextually related data objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8131715">US8131715</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 2010</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8131717">US8131717</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 2010</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8161033">US8161033</a></td><td class="patent-data-table-td patent-date-value">May 25, 2010</td><td class="patent-data-table-td patent-date-value">Apr 17, 2012</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Scheduler for search engine crawler</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8185513">US8185513</a></td><td class="patent-data-table-td patent-date-value">Dec 31, 2008</td><td class="patent-data-table-td patent-date-value">May 22, 2012</td><td class="patent-data-table-td ">Hulu Llc</td><td class="patent-data-table-td ">Method and apparatus for generating merged media program metadata</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8190629">US8190629</a></td><td class="patent-data-table-td patent-date-value">Jul 13, 2006</td><td class="patent-data-table-td patent-date-value">May 29, 2012</td><td class="patent-data-table-td ">Yodlee.Com, Inc.</td><td class="patent-data-table-td ">Network-based bookmark management and web-summary system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8195651">US8195651</a></td><td class="patent-data-table-td patent-date-value">Feb 2, 2010</td><td class="patent-data-table-td patent-date-value">Jun 5, 2012</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8214295">US8214295</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 2010</td><td class="patent-data-table-td patent-date-value">Jul 3, 2012</td><td class="patent-data-table-td ">Icopyright, Inc.</td><td class="patent-data-table-td ">Internet system for facilitating human user advisement and licensing of copyrighted works of authorship</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8224684">US8224684</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 4, 2009</td><td class="patent-data-table-td patent-date-value">Jul 17, 2012</td><td class="patent-data-table-td ">Accenture Global Services Limited</td><td class="patent-data-table-td ">Behavior mapped influence analysis tool</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8224964">US8224964</a></td><td class="patent-data-table-td patent-date-value">Jun 30, 2004</td><td class="patent-data-table-td patent-date-value">Jul 17, 2012</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">System and method of accessing a document efficiently through multi-tier web caching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8239889">US8239889</a></td><td class="patent-data-table-td patent-date-value">Nov 30, 2009</td><td class="patent-data-table-td patent-date-value">Aug 7, 2012</td><td class="patent-data-table-td ">Hulu, LLC</td><td class="patent-data-table-td ">Method and apparatus for collecting viewer survey data and for providing compensation for same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8250118">US8250118</a></td><td class="patent-data-table-td patent-date-value">Dec 29, 2011</td><td class="patent-data-table-td patent-date-value">Aug 21, 2012</td><td class="patent-data-table-td ">West Services, Inc.</td><td class="patent-data-table-td ">Systems, methods, and software for presenting legal case histories</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8261334">US8261334</a></td><td class="patent-data-table-td patent-date-value">Apr 25, 2008</td><td class="patent-data-table-td patent-date-value">Sep 4, 2012</td><td class="patent-data-table-td ">Yodlee Inc.</td><td class="patent-data-table-td ">System for performing web authentication of a user by proxy</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8271495">US8271495</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 9, 2004</td><td class="patent-data-table-td patent-date-value">Sep 18, 2012</td><td class="patent-data-table-td ">Topix Llc</td><td class="patent-data-table-td ">System and method for automating categorization and aggregation of content from network sites</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8275790">US8275790</a></td><td class="patent-data-table-td patent-date-value">Oct 14, 2008</td><td class="patent-data-table-td patent-date-value">Sep 25, 2012</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">System and method of accessing a document efficiently through multi-tier web caching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8286171">US8286171</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 21, 2008</td><td class="patent-data-table-td patent-date-value">Oct 9, 2012</td><td class="patent-data-table-td ">Workshare Technology, Inc.</td><td class="patent-data-table-td ">Methods and systems to fingerprint textual information using word runs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8296666">US8296666</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 30, 2005</td><td class="patent-data-table-td patent-date-value">Oct 23, 2012</td><td class="patent-data-table-td ">Oculus Info. Inc.</td><td class="patent-data-table-td ">System and method for interactive visual representation of information content and relationships using layout and gestures</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8332257">US8332257</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 12, 2010</td><td class="patent-data-table-td patent-date-value">Dec 11, 2012</td><td class="patent-data-table-td ">Accenture Global Services Limited</td><td class="patent-data-table-td ">Behavior mapped influence analysis tool with coaching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8364707">US8364707</a></td><td class="patent-data-table-td patent-date-value">Jan 11, 2012</td><td class="patent-data-table-td patent-date-value">Jan 29, 2013</td><td class="patent-data-table-td ">Hulu, LLC</td><td class="patent-data-table-td ">Method and apparatus for searching media program databases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8386912">US8386912</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 28, 1997</td><td class="patent-data-table-td patent-date-value">Feb 26, 2013</td><td class="patent-data-table-td ">Ca, Inc.</td><td class="patent-data-table-td ">Hypermedia document publishing including hypermedia document parsing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8396856">US8396856</a></td><td class="patent-data-table-td patent-date-value">Nov 12, 2010</td><td class="patent-data-table-td patent-date-value">Mar 12, 2013</td><td class="patent-data-table-td ">Robert Leland Jensen</td><td class="patent-data-table-td ">Database system and method for data acquisition and perusal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8407204">US8407204</a></td><td class="patent-data-table-td patent-date-value">Jun 22, 2011</td><td class="patent-data-table-td patent-date-value">Mar 26, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Minimizing visibility of stale content in web searching including revising web crawl intervals of documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8417700">US8417700</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 1, 2005</td><td class="patent-data-table-td patent-date-value">Apr 9, 2013</td><td class="patent-data-table-td ">Northrop Grumman Systems Corporation</td><td class="patent-data-table-td ">Interactive tool for constructing and editing process diagrams</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8429543">US8429543</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 2011</td><td class="patent-data-table-td patent-date-value">Apr 23, 2013</td><td class="patent-data-table-td ">Facebook, Inc.</td><td class="patent-data-table-td ">E-mail interface having an informational tool tip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8473847">US8473847</a></td><td class="patent-data-table-td patent-date-value">Jul 27, 2010</td><td class="patent-data-table-td patent-date-value">Jun 25, 2013</td><td class="patent-data-table-td ">Workshare Technology, Inc.</td><td class="patent-data-table-td ">Methods and systems for comparing presentation slide decks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8504560">US8504560</a></td><td class="patent-data-table-td patent-date-value">Mar 2, 2012</td><td class="patent-data-table-td patent-date-value">Aug 6, 2013</td><td class="patent-data-table-td ">Patentratings, Llc</td><td class="patent-data-table-td ">Method and system for probabilistically quantifying and visualizing relevance between two or more citationally or contextually related data objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8521730">US8521730</a></td><td class="patent-data-table-td patent-date-value">May 30, 2012</td><td class="patent-data-table-td patent-date-value">Aug 27, 2013</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8527537">US8527537</a></td><td class="patent-data-table-td patent-date-value">Sep 3, 2010</td><td class="patent-data-table-td patent-date-value">Sep 3, 2013</td><td class="patent-data-table-td ">Hulu, LLC</td><td class="patent-data-table-td ">Method and apparatus for providing community-based metadata</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8555080">US8555080</a></td><td class="patent-data-table-td patent-date-value">Sep 11, 2008</td><td class="patent-data-table-td patent-date-value">Oct 8, 2013</td><td class="patent-data-table-td ">Workshare Technology, Inc.</td><td class="patent-data-table-td ">Methods and systems for protect agents using distributed lightweight fingerprints</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8555359">US8555359</a></td><td class="patent-data-table-td patent-date-value">Feb 26, 2009</td><td class="patent-data-table-td patent-date-value">Oct 8, 2013</td><td class="patent-data-table-td ">Yodlee, Inc.</td><td class="patent-data-table-td ">System and methods for automatically accessing a web site on behalf of a client</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8578408">US8578408</a></td><td class="patent-data-table-td patent-date-value">Mar 10, 2009</td><td class="patent-data-table-td patent-date-value">Nov 5, 2013</td><td class="patent-data-table-td ">Hulu, LLC</td><td class="patent-data-table-td ">Method and apparatus for providing directed advertising based on user preferences</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8600974">US8600974</a></td><td class="patent-data-table-td patent-date-value">Apr 29, 2009</td><td class="patent-data-table-td patent-date-value">Dec 3, 2013</td><td class="patent-data-table-td ">West Services Inc.</td><td class="patent-data-table-td ">System and method for processing formatted text documents in a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8620020">US8620020</a></td><td class="patent-data-table-td patent-date-value">Oct 24, 2012</td><td class="patent-data-table-td patent-date-value">Dec 31, 2013</td><td class="patent-data-table-td ">Workshare Technology, Inc.</td><td class="patent-data-table-td ">Methods and systems for preventing unauthorized disclosure of secure information using image fingerprinting</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8639742">US8639742</a></td><td class="patent-data-table-td patent-date-value">Jul 16, 2012</td><td class="patent-data-table-td patent-date-value">Jan 28, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Refreshing cached documents and storing differential document content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8661017">US8661017</a></td><td class="patent-data-table-td patent-date-value">Feb 29, 2012</td><td class="patent-data-table-td patent-date-value">Feb 25, 2014</td><td class="patent-data-table-td ">Hulu, LLC</td><td class="patent-data-table-td ">Method and apparatus for generating merged media program metadata</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8661066">US8661066</a></td><td class="patent-data-table-td patent-date-value">Jun 13, 2012</td><td class="patent-data-table-td patent-date-value">Feb 25, 2014</td><td class="patent-data-table-td ">West Service, Inc.</td><td class="patent-data-table-td ">Systems, methods, and software for presenting legal case histories</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8670600">US8670600</a></td><td class="patent-data-table-td patent-date-value">Oct 24, 2012</td><td class="patent-data-table-td patent-date-value">Mar 11, 2014</td><td class="patent-data-table-td ">Workshare Technology, Inc.</td><td class="patent-data-table-td ">Methods and systems for image fingerprinting</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8676922">US8676922</a></td><td class="patent-data-table-td patent-date-value">Jun 30, 2004</td><td class="patent-data-table-td patent-date-value">Mar 18, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Automatic proxy setting modification</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8694419">US8694419</a></td><td class="patent-data-table-td patent-date-value">Mar 5, 2009</td><td class="patent-data-table-td patent-date-value">Apr 8, 2014</td><td class="patent-data-table-td ">Ocean Tomo, Llc</td><td class="patent-data-table-td ">Methods and systems for utilizing intellectual property assets and rights</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8707312">US8707312</a></td><td class="patent-data-table-td patent-date-value">Jun 30, 2004</td><td class="patent-data-table-td patent-date-value">Apr 22, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Document reuse in a search engine crawler</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8707313">US8707313</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 18, 2011</td><td class="patent-data-table-td patent-date-value">Apr 22, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Scheduler for search engine crawler</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8707343">US8707343</a></td><td class="patent-data-table-td patent-date-value">Jul 31, 2012</td><td class="patent-data-table-td patent-date-value">Apr 22, 2014</td><td class="patent-data-table-td ">Hulu, LLC</td><td class="patent-data-table-td ">Method and apparatus for collecting viewer survey data and for providing compensation for same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8725726">US8725726</a></td><td class="patent-data-table-td patent-date-value">Sep 14, 2012</td><td class="patent-data-table-td patent-date-value">May 13, 2014</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Scoring documents in a linked database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8769392">US8769392</a></td><td class="patent-data-table-td patent-date-value">May 26, 2010</td><td class="patent-data-table-td patent-date-value">Jul 1, 2014</td><td class="patent-data-table-td ">Content Catalyst Limited</td><td class="patent-data-table-td ">Searching and selecting content from multiple source documents having a plurality of native formats, indexing and aggregating the selected content into customized reports</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8775403">US8775403</a></td><td class="patent-data-table-td patent-date-value">Apr 17, 2012</td><td class="patent-data-table-td patent-date-value">Jul 8, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Scheduler for search engine crawler</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8782032">US8782032</a></td><td class="patent-data-table-td patent-date-value">Mar 22, 2013</td><td class="patent-data-table-td patent-date-value">Jul 15, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Minimizing visibility of stale content in web searching including revising web crawl intervals of documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8782709">US8782709</a></td><td class="patent-data-table-td patent-date-value">Feb 19, 2009</td><td class="patent-data-table-td patent-date-value">Jul 15, 2014</td><td class="patent-data-table-td ">Hulu, LLC</td><td class="patent-data-table-td ">Method and apparatus for providing a program guide having search parameter aware thumbnails</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8788475">US8788475</a></td><td class="patent-data-table-td patent-date-value">Jun 28, 2012</td><td class="patent-data-table-td patent-date-value">Jul 22, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">System and method of accessing a document efficiently through multi-tier web caching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20070130193">US20070130193</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 1, 2005</td><td class="patent-data-table-td patent-date-value">Jun 7, 2007</td><td class="patent-data-table-td ">Northrop Grumman Corporation</td><td class="patent-data-table-td ">Interactive tool for constructing and editing process diagrams</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090276724">US20090276724</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 7, 2009</td><td class="patent-data-table-td patent-date-value">Nov 5, 2009</td><td class="patent-data-table-td ">Rosenthal Philip J</td><td class="patent-data-table-td ">Interface Including Graphic Representation of Relationships Between Search Results</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100017850">US20100017850</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 21, 2008</td><td class="patent-data-table-td patent-date-value">Jan 21, 2010</td><td class="patent-data-table-td ">Workshare Technology, Inc.</td><td class="patent-data-table-td ">Methods and systems to fingerprint textual information using word runs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100185712">US20100185712</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 4, 2009</td><td class="patent-data-table-td patent-date-value">Jul 22, 2010</td><td class="patent-data-table-td ">Accenture Global Services Gmbh</td><td class="patent-data-table-td ">Behavior Mapped Influence Analysis Tool</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100223109">US20100223109</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 12, 2010</td><td class="patent-data-table-td patent-date-value">Sep 2, 2010</td><td class="patent-data-table-td ">Hawn Mark K</td><td class="patent-data-table-td ">Behavior mapped influence analysis tool with coaching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110270893">US20110270893</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 30, 2010</td><td class="patent-data-table-td patent-date-value">Nov 3, 2011</td><td class="patent-data-table-td ">Selventa, Inc. (F/K/A Genstruct, Inc.)</td><td class="patent-data-table-td ">System, method and apparatus for assembling and mining life science data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120233151">US20120233151</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 11, 2011</td><td class="patent-data-table-td patent-date-value">Sep 13, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Generating visual summaries of research documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120233152">US20120233152</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 11, 2011</td><td class="patent-data-table-td patent-date-value">Sep 13, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Generation of context-informative co-citation graphs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120290571">US20120290571</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 15, 2012</td><td class="patent-data-table-td patent-date-value">Nov 15, 2012</td><td class="patent-data-table-td ">IP Street</td><td class="patent-data-table-td ">Evaluating Intellectual Property</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101384983B?cl=en">CN101384983B</a></td><td class="patent-data-table-td patent-date-value">Jan 10, 2007</td><td class="patent-data-table-td patent-date-value">Apr 25, 2012</td><td class="patent-data-table-td ">微软公司</td><td class="patent-data-table-td ">文本框编号和链接视觉帮助</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2000039712A1?cl=en">WO2000039712A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 20, 1999</td><td class="patent-data-table-td patent-date-value">Jul 6, 2000</td><td class="patent-data-table-td ">Sony Electronics Inc</td><td class="patent-data-table-td ">Improved techniques for spatial representation of data and browsing based on similarity</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2000051026A1?cl=en">WO2000051026A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 21, 2000</td><td class="patent-data-table-td patent-date-value">Aug 31, 2000</td><td class="patent-data-table-td ">Vicinity Corp</td><td class="patent-data-table-td ">Method and apparatus for efficient proximity searching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001024049A1?cl=en">WO2001024049A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 27, 2000</td><td class="patent-data-table-td patent-date-value">Apr 5, 2001</td><td class="patent-data-table-td ">Smashing Concepts Ltd</td><td class="patent-data-table-td ">Method and system for organizing information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001031507A1?cl=en">WO2001031507A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 23, 2000</td><td class="patent-data-table-td patent-date-value">May 3, 2001</td><td class="patent-data-table-td ">Stead Robert R</td><td class="patent-data-table-td ">Searchable database of informational materials such as research memoranda and the like</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001033428A1?cl=en">WO2001033428A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 26, 2000</td><td class="patent-data-table-td patent-date-value">May 10, 2001</td><td class="patent-data-table-td ">Alta Vista Company</td><td class="patent-data-table-td ">System and method for associating an extensible set of data with documents downloaded by a web crawler</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001042883A2?cl=en">WO2001042883A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 7, 2000</td><td class="patent-data-table-td patent-date-value">Jun 14, 2001</td><td class="patent-data-table-td ">Arcanvs Inc</td><td class="patent-data-table-td ">Anonymously linking a plurality of data records</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2002082224A2?cl=en">WO2002082224A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 4, 2002</td><td class="patent-data-table-td patent-date-value">Oct 17, 2002</td><td class="patent-data-table-td ">West Publishing Co</td><td class="patent-data-table-td ">System, method, and software for identifying historically related legal opinions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2004044708A2?cl=en">WO2004044708A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 12, 2003</td><td class="patent-data-table-td patent-date-value">May 27, 2004</td><td class="patent-data-table-td ">Clientdynamics Inc</td><td class="patent-data-table-td ">Enhanced client relationship management systems and methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2007095997A1?cl=en">WO2007095997A1</a></td><td class="patent-data-table-td patent-date-value">Feb 23, 2006</td><td class="patent-data-table-td patent-date-value">Aug 30, 2007</td><td class="patent-data-table-td ">Netbreeze Gmbh</td><td class="patent-data-table-td ">System and method for user-controlled, multi-dimensional navigation and/or subject-based aggregation and/or monitoring of multimedia data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2007097833A2?cl=en">WO2007097833A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 10, 2007</td><td class="patent-data-table-td patent-date-value">Aug 30, 2007</td><td class="patent-data-table-td ">Microsoft Corp</td><td class="patent-data-table-td ">Text box numbering and linking visual aids</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S737000">707/737</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707SE17089">707/E17.089</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707SE17108">707/E17.108</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999102">707/999.102</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999104">707/999.104</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999005">707/999.005</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S748000">707/748</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S716000">707/716</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0007000000">G06F7/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0017300000">G06F17/30</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99932">Y10S707/99932</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99945">Y10S707/99945</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99934">Y10S707/99934</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99933">Y10S707/99933</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99943">Y10S707/99943</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99935">Y10S707/99935</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30321">G06F17/30321</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30705">G06F17/30705</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30522">G06F17/30522</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30554">G06F17/30554</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30864">G06F17/30864</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=8YhIBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30882">G06F17/30882</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06F17/30W5H</span>, <span class="nested-value">G06F17/30S4V</span>, <span class="nested-value">G06F17/30T4</span>, <span class="nested-value">G06F17/30S4P7</span>, <span class="nested-value">G06F17/30W1</span>, <span class="nested-value">G06F17/30S2P</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Sep 3, 2013</td><td class="patent-data-table-td ">IPR</td><td class="patent-data-table-td ">Aia trial proceeding filed before the patent and appeal board: inter partes review</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Opponent name: </span><span class="nested-value">FACEBOOK, INC., LINKEDIN CORP., AND TWITTER, INC.</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">TRIAL NO: IPR2013-00479</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20130730</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">TRIAL NO: IPR2013-00480</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 1, 2012</td><td class="patent-data-table-td ">CC</td><td class="patent-data-table-td ">Certificate of correction</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 27, 2011</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 1-3, 5, 7-16 AND 18-21 IS CONFIRMED. CLAIMS 23-25 AND 31-33 ARE CANCELLED. NEW CLAIMS 34-54 ARE ADDED AND DETERMINED TO BE PATENTABLE. CLAIMS 4, 6, 17, 22 AND 26-30 WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 17, 2010</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100526</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 11, 2009</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 4, 2008</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EGGER, DANIEL, NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SITE TECHNOLOGIES, INC.;SITE/TECHNOLOGIES/INC.;REEL/FRAME:021794/0648</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080813</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, LLC, NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:EGGER, DANIEL;REEL/FRAME:021794/0661</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080926</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EGGER, DANIEL,NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, LLC,NORTH CAROLINA</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 29, 2007</td><td class="patent-data-table-td ">PRDP</td><td class="patent-data-table-td ">Patent reinstated due to the acceptance of a late maintenance fee</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20071102</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 13, 2007</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, LLC, NEW YORK</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:SOFTWARE RIGHTS ARCHIVE, INC.;REEL/FRAME:019714/0723</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20070518</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, LLC,NEW YORK</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 25, 2007</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 25, 2007</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 2, 2007</td><td class="patent-data-table-td ">FP</td><td class="patent-data-table-td ">Expired due to failure to pay maintenance fee</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20061103</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 3, 2006</td><td class="patent-data-table-td ">REIN</td><td class="patent-data-table-td ">Reinstatement after maintenance fee payment confirmed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 13, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EGGER, DANIEL, NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">BILL OF SALE, ASSIGNMENT &amp; LICENSE AGREEMENT;ASSIGNOR:SITE TECHNOLOGIES, INC.;REEL/FRAME:018160/0500</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19980916</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 24, 2006</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 23, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, INC., NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:EGGER, MR. DANIEL;REEL/FRAME:015698/0357</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20050222</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SOFTWARE RIGHTS ARCHIVE, INC. 905 WEST MAIN STREET</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:EGGER, MR. DANIEL /AR;REEL/FRAME:015698/0357</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 16, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EGGER, MR. DANIEL, NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:SITE/TECHNOLOGIES/INC.;REEL/FRAME:015687/0186</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20050211</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EGGER, MR. DANIEL 2027 WEST CLUB BOULEVARDDURHAM,</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:SITE/TECHNOLOGIES/INC. /AR;REEL/FRAME:015687/0186</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 27, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SITE/TECHNOLOGIES/INC., CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:LIBERTECH INC.;REEL/FRAME:015612/0397</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19960822</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SITE/TECHNOLOGIES/INC. 1120 FOREST AVENUE, #301PAC</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:LIBERTECH INC. /AR;REEL/FRAME:015612/0397</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 14, 2002</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 14, 2002</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 21, 2002</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 16, 1996</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">LIBERTECH, INC., NORTH CAROLINA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:EGGER, DANIEL;CANNON, SHAWN;SAUERS, RONALD D.;REEL/FRAME:008035/0470</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19960618</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U3Tx32xlwQ85uQ0b5cpUn5q7i7b0A\u0026id=8YhIBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U3VrhhpLYiyCJnVcvmEOUPNfNDw9w\u0026id=8YhIBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U0ewiGYs5BzQkc9FON-Ks96RPeX9w","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Method_and_apparatus_for_indexing_search.pdf?id=8YhIBAABERAJ\u0026output=pdf\u0026sig=ACfU3U1t9sihOYjjnrMDkwiXfUFaA21r0Q"},"sample_url":"http://www.google.com/patents/reader?id=8YhIBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>