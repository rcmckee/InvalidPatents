<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US5956681 - Apparatus for generating text data on the basis of speech data input from ... - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Apparatus for generating text data on the basis of speech data input from terminal"><meta name="DC.contributor" content="Tooru Yamakita" scheme="inventor"><meta name="DC.contributor" content="Casio Computer Co., Ltd." scheme="assignee"><meta name="DC.date" content="1997-11-6" scheme="dateSubmitted"><meta name="DC.description" content="A speech signal input from the microphone of a mobile terminal having a PHS function in a communication or off-line state is sent from a PHS network to a speech control host unit connected to a LAN in a specific speech service provider through the Internet and recognized. The contents of the recognition result are automatically determined and shaped into text data of a format type designated from the mobile terminal, and more particularly, into E-mail text data or FAX text data. The formatted text data is returned to the mobile terminal in real time and edited on the mobile terminal as needed. Thereafter, the E-mail text data or FAX text data is transferred to the speech control host unit and transmitted. In this system, the mobile terminal does not require any advanced speech recognition environment and can have a speech recognition function having a practical accuracy at a low cost. The mobile terminal can also be equipped with an E-mail/FAX generation/transmission function based on the speech recognition result."><meta name="DC.date" content="1999-9-21" scheme="issued"><meta name="DC.relation" content="EP:0405029:A1" scheme="references"><meta name="DC.relation" content="JP:H07222248" scheme="references"><meta name="DC.relation" content="JP:S6046647" scheme="references"><meta name="DC.relation" content="US:4712243" scheme="references"><meta name="DC.relation" content="US:5128985" scheme="references"><meta name="DC.relation" content="US:5163111" scheme="references"><meta name="DC.relation" content="US:5182765" scheme="references"><meta name="DC.relation" content="US:5280520" scheme="references"><meta name="DC.relation" content="US:5465326" scheme="references"><meta name="DC.relation" content="US:5577165" scheme="references"><meta name="DC.relation" content="US:5625675" scheme="references"><meta name="DC.relation" content="US:5632002" scheme="references"><meta name="DC.relation" content="US:5758332" scheme="references"><meta name="citation_reference" content="Furui, &quot;Introduction to Electronics/Information Engineering 2, Acoustic/Phonetic Engineering&quot;, Chapter 14, Kindaikagaku-sha, pp. 175-211."><meta name="citation_reference" content="Furui, Introduction to Electronics/Information Engineering 2, Acoustic/Phonetic Engineering , Chapter 14, Kindaikagaku sha, pp. 175 211."><meta name="citation_reference" content="Patent Abstracts Of Japan, vol. 009, No. 173 (E 329), Jul. 18, 1995 &amp; JP 60 046647 A (Fujitsu KK), Mar. 13, 1985."><meta name="citation_reference" content="Patent Abstracts Of Japan, vol. 009, No. 173 (E-329), Jul. 18, 1995 &amp; JP 60 046647 A (Fujitsu KK), Mar. 13, 1985."><meta name="citation_reference" content="Patent Abstracts of Japan, vol. 095, No. 011, Dec. 26, 1995 &amp; JP 07 222248 (Hitachi Ltd.), Aug. 18, 1995."><meta name="citation_patent_number" content="US:5956681"><meta name="citation_patent_application_number" content="US:08/966,912"><link rel="canonical" href="http://www.google.com/patents/US5956681"/><meta property="og:url" content="http://www.google.com/patents/US5956681"/><meta name="title" content="Patent US5956681 - Apparatus for generating text data on the basis of speech data input from terminal"/><meta name="description" content="A speech signal input from the microphone of a mobile terminal having a PHS function in a communication or off-line state is sent from a PHS network to a speech control host unit connected to a LAN in a specific speech service provider through the Internet and recognized. The contents of the recognition result are automatically determined and shaped into text data of a format type designated from the mobile terminal, and more particularly, into E-mail text data or FAX text data. The formatted text data is returned to the mobile terminal in real time and edited on the mobile terminal as needed. Thereafter, the E-mail text data or FAX text data is transferred to the speech control host unit and transmitted. In this system, the mobile terminal does not require any advanced speech recognition environment and can have a speech recognition function having a practical accuracy at a low cost. The mobile terminal can also be equipped with an E-mail/FAX generation/transmission function based on the speech recognition result."/><meta property="og:title" content="Patent US5956681 - Apparatus for generating text data on the basis of speech data input from terminal"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("CZDtU-jNK6_UsASTvYGICQ"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("USA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("CZDtU-jNK6_UsASTvYGICQ"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("USA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us5956681?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US5956681"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=CExMBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS5956681&amp;usg=AFQjCNHZ3TUSFZUekDB1Rj0DzmBPGvaaIw" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US5956681.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US5956681.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US5956681" style="display:none"><span itemprop="description">A speech signal input from the microphone of a mobile terminal having a PHS function in a communication or off-line state is sent from a PHS network to a speech control host unit connected to a LAN in a specific speech service provider through the Internet and recognized. The contents of the recognition...</span><span itemprop="url">http://www.google.com/patents/US5956681?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US5956681 - Apparatus for generating text data on the basis of speech data input from terminal</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US5956681 - Apparatus for generating text data on the basis of speech data input from terminal" title="Patent US5956681 - Apparatus for generating text data on the basis of speech data input from terminal"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US5956681 A</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 08/966,912</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Sep 21, 1999</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Nov 6, 1997</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Dec 27, 1996</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/DE69728744D1">DE69728744D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69728744T2">DE69728744T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0851403A2">EP0851403A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0851403A3">EP0851403A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0851403B1">EP0851403B1</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">08966912, </span><span class="patent-bibdata-value">966912, </span><span class="patent-bibdata-value">US 5956681 A, </span><span class="patent-bibdata-value">US 5956681A, </span><span class="patent-bibdata-value">US-A-5956681, </span><span class="patent-bibdata-value">US5956681 A, </span><span class="patent-bibdata-value">US5956681A</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Tooru+Yamakita%22">Tooru Yamakita</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Casio+Computer+Co.,+Ltd.%22">Casio Computer Co., Ltd.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US5956681.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5956681.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5956681.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (13),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (5),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (127),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (21),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (13)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/5956681&usg=AFQjCNHSWpR7Lb46CPtyK4VkGucr8QEchg">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D5956681&usg=AFQjCNHsxyzHb4jh7CxwhiYTeVhsPQoQ5w">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D5956681A%26KC%3DA%26FT%3DD&usg=AFQjCNEsZPMJBybfbNya-n0ZRJnXIEt2kA">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54492479" lang="EN" load-source="patent-office">Apparatus for generating text data on the basis of speech data input from terminal</invention-title></span><br><span class="patent-number">US 5956681 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA37961202" lang="EN" load-source="patent-office"> <div class="abstract">A speech signal input from the microphone of a mobile terminal having a PHS function in a communication or off-line state is sent from a PHS network to a speech control host unit connected to a LAN in a specific speech service provider through the Internet and recognized. The contents of the recognition result are automatically determined and shaped into text data of a format type designated from the mobile terminal, and more particularly, into E-mail text data or FAX text data. The formatted text data is returned to the mobile terminal in real time and edited on the mobile terminal as needed. Thereafter, the E-mail text data or FAX text data is transferred to the speech control host unit and transmitted. In this system, the mobile terminal does not require any advanced speech recognition environment and can have a speech recognition function having a practical accuracy at a low cost. The mobile terminal can also be equipped with an E-mail/FAX generation/transmission function based on the speech recognition result.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(18)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-1.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-1.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-2.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-2.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-3.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-3.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-4.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-4.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-5.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-5.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-6.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-6.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-7.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-7.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-8.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-8.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-9.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-9.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-10.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-10.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-11.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-11.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-12.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-12.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-13.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-13.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-14.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-14.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-15.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-15.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-16.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-16.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-17.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-17.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5956681-18.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5956681-18.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(12)</span></span></div><div class="patent-text"><div mxw-id="PCLM5445135" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>I claim:</claim-statement> <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1. A speech control apparatus connected to a terminal through a communication network, comprising:<div class="claim-text">means for receiving speech data transmitted from said terminal;</div> <div class="claim-text">processing means for recognizing the received speech data, converting the recognized speech data into document data, extracting a specific word from the converted document data, and generating formatted document data having a predetermined format by inserting the extracted word into a specified field of the converted document data; and</div> <div class="claim-text">transmitting means for transmitting the generated formatted document data through said communication network.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2. An apparatus according to claim 1, wherein said processing means comprises means for extracting a word associated with a destination from the converted document data and inserting the extracted word into a field designating a destination of the formatted document data.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3. An apparatus according to claim 2, wherein said processing means specifies an E-mail destination as the destination and generates formatted E-mail text data as the formatted document data, and wherein the transmitting means transmits the formatted E-mail text data to the specified destination.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4. An apparatus according to claim 2, wherein said processing means specifies a FAX destination as the destination and generates formatted FAX text data as the formatted document data, and wherein the transmitting means transmits the formatted FAX text data to the specified destination.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5. An apparatus according to claim 2, wherein said terminal comprises means for receiving the formatted document data generated by said apparatus, means for editing the formatted document data, and means for transmitting the formatted document data to the destination.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6. An apparatus according to claim 1, wherein said terminal comprises means for designating a type of formatted document data, and said apparatus receives data representing the designated type, and extracts a word corresponding to the formatted document data of the designated type, thereby generating the formatted document data.</div>
    </div>
    </div> <div class="claim"> <div num="7" class="claim">
      <div class="claim-text">7. A speech control apparatus connected to a terminal through a communication network, comprising:<div class="claim-text">means for receiving speech data transmitted from said terminal;</div> <div class="claim-text">means for recognizing the received speech data and converting the speech data into document data;</div> <div class="claim-text">means for extracting a word relating to a destination from the converted document data to specify a destination; and</div> <div class="claim-text">means for transmitting the converted document data to the specified destination.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8. An apparatus according to claim 7, further comprising an address database storing a correspondence between names and destinations, and wherein said means for extracting the word relating to the destination from the converted document data to specify the destination refers to the address database and specifies the destination from a name extracted as the word.</div>
    </div>
    </div> <div class="claim"> <div num="9" class="claim">
      <div class="claim-text">9. A portable terminal unit for obtaining text data from speech data through a network, comprising:<div class="claim-text">means for inputting speech data;</div> <div class="claim-text">transmit control means for appending an identification code of the portable terminal unit to the input speech data and for transmitting the speech data to a speech control unit connected to the portable terminal unit through the network;</div> <div class="claim-text">receive control means for receiving text data as a result of conversion of the speech data transmitted from the speech control unit to the portable terminal unit corresponding to the identification code; and</div> <div class="claim-text">display means for displaying the received text data.</div> </div>
    </div>
    </div> <div class="claim"> <div num="10" class="claim">
      <div class="claim-text">10. A speech control apparatus to which a plurality of terminal units are connected through a network, comprising:<div class="claim-text">means for receiving speech data and data identifying a format type transmitted from each of the terminal units;</div> <div class="claim-text">document generating means for recognizing the received speech data, converting the recognized speech data into document data, and generating formatted document data having a format corresponding to the format type for each terminal unit; and</div> <div class="claim-text">means for transmitting the generated formatted document data to the specified terminal unit through the network.</div> </div>
    </div>
    </div> <div class="claim"> <div num="11" class="claim">
      <div class="claim-text">11. An article of manufacture comprising a computer usable medium having computer readable program code means embodied therein for causing speech data to be converted into formatted document data in a speech control apparatus to which a terminal unit is connected through a network, the computer readable program code means comprising:<div class="claim-text">means for causing a computer to receive speech data transmitted from the terminal unit;</div> <div class="claim-text">means for causing the computer to recognize the received speech data, convert the recognized speech data into document data, extract a specific word from the converted document data, and insert the extracted word into a specific field of the converted document data to generate formatted document data having a predetermined format; and</div> <div class="claim-text">means for causing the computer to transmit the generated formatted document data through the network.</div> </div>
    </div>
    </div> <div class="claim"> <div num="12" class="claim">
      <div class="claim-text">12. An article of manufacture comprising a computer usable medium having computer readable program code means embodied therein for causing speech data to be converted into formatted document data in a speech control apparatus to which a plurality of terminal units are connected through a network, the computer readable program code means comprising:<div class="claim-text">means for causing a computer to receive speech data and data identifying a format type transmitted from each of the terminal units;</div> <div class="claim-text">means for causing the computer to recognize the received speech data, convert the recognized speech data into document data, and generate document data having a format corresponding to the format type for each terminal unit; and</div> <div class="claim-text">means for causing the computer to transmit the generated formatted document data to a specified terminal unit through the network.</div> </div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES67337091" lang="EN" load-source="patent-office" class="description">
    <p>This application is a continuation of application Ser. No. 08/708,133, filed Aug. 30, 1996, now U.S. Pat. No. 5,734,933, which is a continuation of Ser. No. 08/321,916, filed Oct. 12, 1994 abandoned; which is a division of Ser. No. 08/053,961, filed Apr. 26, 1993 (U.S. Pat. No. 5,386,264); which is a continuation of Ser. No. 07/970,652, filed Oct. 30, 1992, abandoned, which is a continuation of Ser. No. 07/621,294, filed Jan. 23, 1991, abandoned, which is a division of Ser. No. 07/319,658, filed Mar. 6, 1989 (U.S. Pat. No. 5,012,270).</p>
    <heading>BACKGROUND OF THE INVENTION</heading> <p>The present invention relates to a technique of recognizing speech data such as communication speech data input from a mobile (portable) terminal and generating an E-mail document or a FAX document, i.e., text data formatted on the basis of the recognition result and, more particularly, to a technique of transmitting the generated document.</p>
    <p>A speech recognition technique of recognizing a speech signal, converting the speech signal into character data, and storing the character data or using the recognition result for various services is conventionally demanded in various industrial fields.</p>
    <p>In recent years, along with the advance of the speech recognition algorithm, speech recognition systems using main frame computers or workstation computers have been developed.</p>
    <p>These systems, such as by a bank balance inquiry system for receiving telephone speech data, a seat reservation system, and a goods sorting system for automatically delivering goods upon recognizing the operator voice, are being introduced to various industrial fields.</p>
    <p>However, such speech recognition systems have just reached a practical recognition accuracy in the environment of the above-described large-scale computer system. In the environment of a small computer system such as a personal computer, no inexpensive speech recognition systems having a practical recognition accuracy has been realized yet.</p>
    <p>Together with the above-described information processing technology, mobile terminals including such as mobile phones, portable telephones, and PHSs (Personal Handyphone Systems) are rapidly becoming popular.</p>
    <p>In particular, the PHS is compact and more inexpensive with respect to telephone charges than a mobile phone or portable telephone, and it is explosively being popularized because of its characteristic feature, i.e., the capability of high-quality communication "with anybody anytime anywhere". In addition, the PHS uses a public network having an ISDN (Integrated Services Digital Network) as a backbone and therefore allows high-speed digital communication at a transfer rate of 32 kbits/sec, so that future applications to multimedia communication fields are also increasingly expected.</p>
    <p>The PHS is also expected as a multimedia information management/communication terminal which can be used not only as a portable telephone but also as a portable information management device while exploiting the convenience of the mobile terminal. More specifically, such a mobile terminal is expected to have a home page access function and an E-mail communication function as functions of accessing the Internet or an intra-office network as well as a speech communication/FAX function. An information management function such as address management, schedule management, memo management, or database searching is also expected to be arranged.</p>
    <p>Such a mobile terminal is required to have a user interface as user-friendly and natural as possible such that the user can readily use it. User interfaces currently put into practice include finger operation input from a keyboard or a mouse and handwriting input using an electronic pen. It is ideal that the user interface can also cope with speech input or the like. More specifically, when not only address input, schedule input, and memo input but also E-mail generation/transmission and FAX generation/transmission are enabled using a speech signal representing the speech contents as data while using the speech communication function as the basic function, the convenience of the mobile terminal can be largely increased. This is the advantage of the application of the speech recognition function as a user interface to the mobile terminal.</p>
    <p>However, the mobile terminal is compact and has only a limited information processing capability. In addition, in current speech recognition processing, the practical recognition accuracy can be realized only in the environment of a main frame computer or workstation computer. Therefore, the speech recognition function as the user interface of a mobile terminal has not yet been realized.</p>
    <heading>BRIEF SUMMARY OF THE INVENTION</heading> <p>It is an object of the present invention to realize, in a communication environment using a mobile terminal, a speech recognition function as a user interface of the mobile terminal at a practical accuracy and cost and to enable generation/transmission of an E-mail or FAX document as formatted text data on the basis of the recognition result.</p>
    <p>To achieve the above object, there is provided a speech control apparatus connected to a terminal through a communication network, comprising: means for receiving speech data transmitted from the terminal; means for recognizing the received speech data and converting the speech data into document data; means for extracting a word from the converted document data and generating formatted text data on the basis of the extracted word; and means for transmitting the generated formatted text data through the communication network.</p>
    <p>According to the present invention, since speech recognition processing need not be performed on the terminal side, simplification of processing and size reduction of the terminal can be realized. Only by inputting speech data from the terminal, another text format data such as E-mail data or FAX data can be obtained. Therefore, the interface is easy to use as compared to the conventional text data input in a key operation. In addition, an E-mail or FAX function can be added even when the terminal side has no special function.</p>
    <p>Additional objects and advantages of the invention will be set forth in the description which follows, and in part will be obvious from the description, or may be learned by practice of the invention. The objects and advantages of the invention may be realized and obtained by means of the instrumentalities and combinations particularly pointed out in the appended claims.</p>
    <heading>BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWING</heading> <p>The accompanying drawings, which are incorporated in and constitute a part of the specification, illustrate presently preferred embodiments of the invention, and together with the general description given above and the detailed description of the preferred embodiments given below, serve to explain the principles of the invention.</p>
    <p>FIG. 1 is a block diagram showing the entire system configuration;</p>
    <p>FIG. 2 is a perspective view showing the outer appearance of a mobile terminal;</p>
    <p>FIG. 3 is a functional block diagram of the mobile terminal;</p>
    <p>FIG. 4 is a flow chart of the entire processing of the mobile terminal;</p>
    <p>FIG. 5 is a flow chart of transmission processing;</p>
    <p>FIGS. 6A, 6B, and 6C are views showing the format of communication data;</p>
    <p>FIGS. 7A and 7B are views showing the formats of an IP header and a TCP header, respectively;</p>
    <p>FIG. 8 is a flow chart of call origination processing using PPP;</p>
    <p>FIGS. 9A, 9B, and 9C are flow charts of the operation of a mobile terminal communication control section;</p>
    <p>FIG. 10 is a view showing the data structure of a processing terminal registration table;</p>
    <p>FIG. 11 is a block diagram of a text speech recognition section;</p>
    <p>FIG. 12 is a flow chart of the operation of an input/output control section in the speech recognition section;</p>
    <p>FIG. 13 is a flow chart of the operation of a formatted text generation section;</p>
    <p>FIG. 14 is a flow chart of the operation of an input/output control section in the formatted text generation section;</p>
    <p>FIG. 15 is a flow chart of the operation of a mail transmission/reception section; and</p>
    <p>FIG. 16 is a flow chart of the operation of a FAX transmission/reception section.</p>
    <heading>DETAILED DESCRIPTION OF THE INVENTION</heading> <p>An embodiment of the present invention will be described below in detail with reference to the accompanying drawing.</p>
    <p>System Configuration</p>
    <p>FIG. 1 is a block diagram showing the entire system configuration of the embodiment of the present invention.</p>
    <p>A mobile terminal 101 has a PHS terminal function and is connected to a PHS network 103 via a radio base station 102 in radio communication. The radio base station 102 is a public radio base station provided on a public telephone booth on a street, a utility pole, a building rooftop, or an underpass, or an extension telephone in a subscriber's house. When the mobile terminal 101 is connected to the extension telephone, it is directly connected to the public telephone network without interposing the PHS network. The mobile terminal 101 may be connected to the PHS network 103 or the public telephone network in wire communication via a wire connection unit in place of the radio base station 102.</p>
    <p>The PHS network 103 is mutually connected to the public telephone network or an ISDN network, and these networks are connected to a mobile terminal control host unit 104 connected to the Internet 105 through a dedicated high-speed digital line or the like.</p>
    <p>When the mobile terminal 101 automatically originates a dial-up call, through the radio base station 102 or the PHS network 103, to the mobile terminal control host unit 104 connected to the public telephone network or ISDN network, the mobile terminal 101 can be connected to the Internet 105.</p>
    <p>A router unit 106 connected to a LAN 107 of a predetermined speech service provider through a high-speed digital leased line or the like is connected to the Internet 105. The LAN 107 is a local area network based on Ethernet, ATM (Asynchronous Transfer Mode), or FDDI. A speech control host unit 108 is also connected to the LAN 107.</p>
    <p>After the mobile terminal 101 automatically originates a dial-up call to the mobile terminal control host unit 104, the mobile terminal 101 can communicate with the speech control host unit 108 through the Internet 105, the router unit 106, and the LAN 107.</p>
    <p>When the user instructs communication with the speech control host unit 108 from the touch panel of an input section 109 in the mobile terminal 101, a control section 110 requests a communication section 111 to start communication with the speech control host unit 108.</p>
    <p>If the mobile terminal 101 is not currently connected to the mobile terminal control host unit 104, the communication section 111 originates a call to the radio base station 102 by radio (or by wire) to connect the mobile terminal 101 to the PHS network 103 upon receiving the request for starting the communication from the control section 110, and thereafter, designates the access telephone number of the mobile terminal control host unit 104 and originates a dial-up call.</p>
    <p>When the call terminates at the mobile terminal control host unit 104, the communication section 111 in the mobile terminal 101 communicates with a connection establishment section 113 in the mobile terminal control host unit 104 first to negotiate for establishment of connection based on TCP/IP and PPP as a standard communication protocol on the Internet 105. As a result, the mobile terminal control host unit 104 assigns an IP address as an identification address on the Internet 105 to the communication section 111 in the mobile terminal 101, thereby allowing the mobile terminal 101 to access the Internet 105.</p>
    <p>If the mobile terminal 101 is connected to the mobile terminal control host unit 104, the communication section 111 in the mobile terminal 101 omits the dial-up call origination.</p>
    <p>The communication section 111 in the mobile terminal 101 sends a TCP/IP packet which stores a "destination IP address" serving as a predetermined IP address of the speech control host unit 108, a "transmission source IP address" serving as the IP address assigned by the mobile terminal control host unit 104, a "terminal identification code" (e.g., a PHS telephone number) for identifying the mobile terminal 101, and a text speech recognition/formatting start request command and a format type data based on an instruction from the user or a text speech recognition/formatting end command to the Internet 105.</p>
    <p>This TCP/IP packet is transferred to the router unit 106 in the speech service provider by a routing section 114 in the mobile terminal control host unit 104 and a relay host unit (not shown) in the Internet 105 on the basis of the "destination IP address" stored in the TCP/IP packet, and then transferred to a packet transmission/reception section 115 in the speech control host unit 108 through the LAN 107.</p>
    <p>The packet transmission/reception section 115 extracts, from the received TCP/IP packet, the "transmission source IP address", the "terminal identification code", and the text speech recognition/formatting start request command and the format type data, or the text speech recognition/formatting end request command, and transfers these data to a mobile terminal communication control section 116 in the speech control host unit 108.</p>
    <p>The mobile terminal communication control section 116 registers, in a processing terminal registration table (FIG. 10) to be described later, information associated with the transferred "transmission source IP address", "terminal identification code", and text speech recognition/formatting start request command and format type data, or text speech recognition/formatting end request command. Thereafter, the mobile terminal communication control section 116 requests the packet transmission/reception section 115 to return a TCP/IP packet storing transmission enable data to the mobile terminal 101.</p>
    <p>The packet transmission/reception section 115 transmits the corresponding TCP/IP packet to the IP address corresponding to the mobile terminal 101.</p>
    <p>In this way, the speech control host unit 108 can execute text speech recognition/formatting of speech data transferred from the mobile terminal 101. Upon receiving the TCP/IP packet storing the transmission enable data from the speech control host unit 108, the communication section 111 in the mobile terminal 101 transfers the transmission enable data stored in the TCP/IP packet to the control section 110.</p>
    <p>Upon receiving the transmission enable data, the control section 110 in the mobile terminal 101 requests the communication section 111 to transmit, to the speech control host unit 108, speech data input from a microphone by a speech communication operation or a speech input operation in an off-line state.</p>
    <p>The communication section 111 transmits the TCP/IP packet storing the speech data to the IP address corresponding to the speech control host unit 108.</p>
    <p>This TCP/IP packet is transferred to the packet transmission/reception section 115 in the speech control host unit 108 through the routing section 114 in the mobile terminal control host unit 104, the relay host unit (not shown) in the Internet 105, the router unit 106 in the speech service provider, and the LAN 107 on the basis of the "destination IP address" stored in the TCP/IP packet.</p>
    <p>The packet transmission/reception section 115 extracts speech data stored in the received TCP/IP packet and transfers the speech data to the mobile terminal communication control section 116 in the speech control host unit 108.</p>
    <p>The mobile terminal communication control section 116 transfers the transferred speech data to a text speech recognition section 117. The text speech recognition section 117 executes text speech recognition processing for the transferred speech data and transfers the recognition result, i.e., recognized speech text data to a formatted text generation section 118. The formatted text generation section 118 determines the field of the recognized speech text data output from the text speech recognition section 117 using the format type data which is designated from the mobile terminal 101 together with the text speech recognition/formatting start request command, and a format type field dictionary. The formatted text generation section 118 also deletes unnecessary words using an unnecessary word dictionary 1505 (FIG. 13), generates formatted text data, and transfers the formatted text data to the mobile terminal communication control section 116.</p>
    <p>To generate E-mail text data, the user of the mobile terminal 101 designates "E-mail" as format type data together with a text speech recognition/formatting start request command. Next, the user sequentially pronounces, e.g., "the destination is taro@casio.co.jp", "the carbon copy is hanako@osuga.co.jp", or "the text is . . . " To generate FAX text data, the user sequentially pronounces, e.g., "the destination number is 0425-79-7735", or "the text is . . . " These pronounced contents are recognized as recognized speech text data by the text speech recognition section 117 in the speech control host unit 108. The formatted text generation section 118 determines the recognized speech text data as text data in, e.g., the "To" field, "Cc" field, or "text" field of E-mail text data. The formatted text generation section 118 deletes unnecessary words and generates formatted text data such as "To: taro@casio.co.jp", "Cc: hanako@osuga.co.jp", or "text: . . . " Alternatively, the formatted text generation section 118 determines the recognized speech text data as text data in, e.g., the "destination number" field, or "text" field of FAX text data. The formatted text generation section 118 deletes unnecessary words and generates formatted text data such as "destination number: 0425-79-7735", or "text: . . . "</p>
    <p>The mobile terminal communication control section 116 requests to return a TCP/IP packet storing the formatted text data to the mobile terminal 101.</p>
    <p>The packet transmission/reception section 115 transmits the corresponding TCP/IP packet to the IP address corresponding to the mobile terminal 101.</p>
    <p>Upon receiving the TCP/IP packet storing the formatted text data from the speech control host unit 108, the communication section 111 in the mobile terminal 101 transfers the formatted text data stored in the TCP/IP packet to the control section 110.</p>
    <p>The control section 110 in the mobile terminal 101 inserts the formatted text data into text template data of a format type corresponding to the format type data designated by the user in advance and outputs the formatted text data to an output section 112. The output section 112 displays a text corresponding to the formatted text data on an LCD display section. The user can arbitrarily edit this text data.</p>
    <p>When the user of the mobile terminal 101 instructs, from the touch panel of the input section 109, transmission of the E-mail text data or FAX text data which has undergone edit processing, the control section 110 requests the communication section 111 to transmit the E-mail text data or FAX text data to the speech control host unit 108. In this case, a "From" field representing the transmission source address is automatically added to the E-mail text data, or transmission source information is automatically added to the FAX text data.</p>
    <p>The communication section 111 transmits a TCP/IP packet storing the E-mail text data or FAX text data to the IP address corresponding to the speech control host unit 108.</p>
    <p>This TCP/IP packet is transferred to the packet transmission/reception section 115 in the speech control host unit 108 through the routing section 114 in the mobile terminal control host unit 104, the relay host unit (not shown) in the Internet 105, the router unit 106 in the speech service provider, and the LAN 107 on the basis of the "destination IP address" stored in the TCP/IP packet.</p>
    <p>The packet transmission/reception section 115 extracts the E-mail text data or FAX text data stored in the received TCP/IP packet and transfers the data to a mail transmission/reception section 119 or a FAX transmission/reception section 120 in the speech control host unit 108.</p>
    <p>The mail transmission/reception section 119 inquires of a name solution server (not shown) to convert an E-mail address set in the "To" field and "Cc" field of the E-mail text data into an IP address, and requests the packet transmission/reception section 115 to transmit the E-mail text data to the IP address. The packet transmission/reception section 115 generates a TCP/IP packet storing the E-mail address and transmits the TCP/IP packet to the Internet 105.</p>
    <p>The FAX transmission/reception section 120 dials, on a telephone line 121 (FIG. 1), the destination number set in the "destination number" field of the FAX text data, thereby transmitting the FAX text data to a partner FAX apparatus where the call has terminated.</p>
    <p>Upon receiving the E-mail text data for the mobile terminal 101 from the Internet 105 through the packet transmission/reception section 115, the mail transmission/reception section 119 spools the data.</p>
    <p>Similarly, upon receiving the FAX text data for the mobile terminal 101 from the telephone line 121, the FAX transmission/reception section 120 spools the data.</p>
    <p>When the user of the mobile terminal 101 instructs to receive E-mail text data or FAX text data from the touch panel at an arbitrary timing, the control section 110 requests the communication section 111 to transmit a mail reception request command or a FAX reception request command to the speech control host unit 108.</p>
    <p>The communication section 111 transmits a TCP/IP packet storing the mail reception request command or FAX reception request command to the IP address corresponding to the speech control host unit 108.</p>
    <p>This TCP/IP packet is transferred to the packet transmission/reception section 115 in the speech control host unit 108 through the routing section 114 in the mobile terminal control host unit 104, the relay host unit (not shown) in the Internet 105, the router unit 106 in the speech service provider, and the LAN 107 on the basis of a "destination IP address" stored in the TCP/IP packet.</p>
    <p>The packet transmission/reception section 115 extracts the mail reception request command or the FAX reception request command stored in the received TCP/IP packet and transfers the command to the mail transmission/reception section 119 or the FAX transmission/reception section 120 in the speech control host unit 108.</p>
    <p>Upon fetching the mail reception request command, the mail transmission/reception section 119 requests the packet transmission/reception section 115 to extract the E-mail text data which has been received for the mobile terminal 101 from a spool file corresponding to the "terminal identification code" transferred from the mobile terminal 101 together with the mail reception request command and transmit the data to the mobile terminal 101.</p>
    <p>Similarly, upon fetching the FAX reception request command, the FAX transmission/reception section 120 requests the packet transmission/reception section 115 to extract FAX text data which has been received for the mobile terminal 101 from a spool file corresponding to the "terminal identification code" transferred from the mobile terminal 101 together with the FAX reception request command and transmit the data to the mobile terminal 101.</p>
    <p>The packet transmission/reception section 115 generates a TCP/IP packet storing the E-mail text data or the FAX text data and transmits the TCP/IP packet to the IP address corresponding to the mobile terminal 101.</p>
    <p>Upon receiving the TCP/IP packet storing the E-mail text data or the FAX text data from the speech control host unit 108, the communication section 111 in the mobile terminal 101 transfers the E-mail text data or the FAX text data to the control section 110.</p>
    <p>The control section 110 in the mobile terminal 101 displays the received E-mail text or FAX text on the LCD display section.</p>
    <p>In addition to the communication with the speech control host unit 108, the mobile terminal 101 can also freely access a desired resource on the Internet 105 by originating a dial-up call to the mobile terminal control host unit 104 using a home page browser tool of the mobile terminal 101.</p>
    <p>Outer Appearance of Mobile Terminal 101</p>
    <p>FIG. 2 is a perspective view showing the outer appearance of the mobile terminal 101 shown in FIG. 1.</p>
    <p>The mobile terminal 101 has the outer appearance of a compact portable information management device comprising a microphone 201 also serving as a transmitter for inputting speech data, a camera 202 for inputting image data, an LCD display section 203 which displays various kinds of information and has a touch panel function for receiving touch inputs or pen inputs, and a loudspeaker 204 also serving as a receiver for outputting speech data.</p>
    <p>The mobile terminal 101 also has a radio antenna 205 for originating a call to the radio base station 102 shown in FIG. 1, and a socket 206 for connecting the mobile terminal 101 to a wire connection unit in place of the radio base station 102.</p>
    <p>The mobile terminal 101 also has an IC card slot 207 for receiving various IC cards, and an optical transceiver 208 for performing infrared optical communication with another mobile terminal 101 or a personal computer.</p>
    <p>A switch 209 is a power switch.</p>
    <p>Functional Block Diagram of Mobile Terminal 101</p>
    <p>FIG. 3 is a functional block diagram of the mobile terminal 101.</p>
    <p>As shown in FIG. 1, the mobile terminal 101 comprises the input section 109, the control section 110, the communication section 111, and the output section 112, which are connected to each other via a bus 326.</p>
    <p>The input section 109 is constituted by a speech input section, an image input section, and a touch panel mechanism (to be described later in association with the operation of the output section 112).</p>
    <p>The speech input section comprises a microphone 301, an A/D conversion section 302, and a microphone control section 303.</p>
    <p>The microphone 301 (the microphone 301 corresponds to the microphone 201 shown in FIG. 2) also serves as the transmitter of the PHS and is used to input the user's voice.</p>
    <p>The A/D conversion section 302 converts an analog speech signal input from the microphone 301 into digital speech data and codes the digital speech data using ADPCM (Adaptive Differential Pulse Code Modulation) as the standard speech coding method of the PHS. This section has already been put into practice as an LSI circuit constituting a PHS terminal.</p>
    <p>In speech communication, the microphone control section 303 transfers the coded speech data to a communication control section 321 in the communication section 111 and sends it to a speech channel. In text speech recognition/formatting, the microphone control section 303 transfers the coded speech data to a RAM 317 in the control section 110.</p>
    <p>The image input section is constituted by a CCD (Charge Coupled Device) camera 304, an A/D conversion section 305, a memory 306, and a camera control section 307.</p>
    <p>The CCD camera 304 picks up an arbitrary image on the basis of the operation of the user.</p>
    <p>The A/D conversion section 305 converts an analog image signal picked up by the CCD camera 304 into digital image data.</p>
    <p>The memory 306 stores the digital image data in units of frames.</p>
    <p>The camera control section 307 controls the operations of the CCD camera 304, the A/D conversion section 305, and the memory 306.</p>
    <p>The output section 112 is constituted by a speech output section and an image output section.</p>
    <p>The speech output section is constituted by a loudspeaker 308, a D/A conversion section 309, and a loudspeaker control section 310.</p>
    <p>The loudspeaker control section 310 transfers PHS speech data received from the communication control section 321 in the communication section 111 or synthesized speech data received from the RAM 317 in the control section 110 to the D/A conversion section 309.</p>
    <p>The D/A conversion section 309 decodes the received speech data, converts the data into an analog speech signal, and causes the loudspeaker 308 (the loudspeaker 308 corresponds to the loudspeaker 204 in FIG. 2) to output the speech signal as speech data.</p>
    <p>The image output section is constituted by the LCD display section 203, an LCD driver 312, a memory 313, and an LCD control section 314.</p>
    <p>The LCD control section 314 causes the memory 313 to hold various image data such as character data, image data, and command button data from the RAM 317 in the control section 110 in units of frames and starts the LCD driver 312.</p>
    <p>The LCD driver 312 displays image data read out from the memory 313 in units of frames on an LCD display section 311 (the LCD display section 311 corresponds to the LCD display section 203 in FIG. 2).</p>
    <p>A transparent touch panel is arranged on the surface of the LCD display section 311 (203 in FIG. 2). The user can touch the touch panel with a finger or a pen in accordance with, e.g., command button data displayed on the LCD display section 311 to input a command. This input signal is transferred to the RAM 317 in the control section 110 by a touch panel control section 315.</p>
    <p>The control section 110 comprises a CPU 316, the RAM 317, a ROM 318, an IC card interface section 319, and an IC card 320 inserted into the IC card slot 207 (FIG. 2) as needed. The IC card interface section 319 controls input/output of data to/from the IC card 320.</p>
    <p>The CPU 316 controls the entire operation of the mobile terminal 101 using the RAM 317 as a work area in accordance with a control program stored in the ROM 318.</p>
    <p>The communication section 111 comprises the communication control section 321, a radio driver 322, a radio antenna 323, a wire driver 324, and a socket 325.</p>
    <p>The communication control section 321 executes PHS speech communication processing or TCP/IP communication processing (to be described later) with the Internet 105 and controls the radio driver 322 or the wire driver 324.</p>
    <p>The radio driver 322 performs conversion between communication data and a PHS radio signal transmitted/received through the radio antenna 323 (the radio antenna 323 corresponds to the radio antenna 205 shown in FIG. 2) in the radio communication mode. The PHS radio signal is based on a radio frequency of 1.9 GHz, a carrier frequency interval of 300 kHz, a four-channel/carrier TDMA-TDD radio access scheme, a π/4-shift QPSK modulation scheme, and a radio transfer rate of 384 kbits/sec.</p>
    <p>The wire driver 324 performs conversion between communication data and a wire signal transmitted/received through the socket 325 (the socket 325 corresponds to the socket 206 shown in FIG. 2). This wire signal is a general telephone band modem modulated signal.</p>
    <p>The operation of the embodiment of the present invention having the above arrangement will be described below in detail.</p>
    <p>Processing in Mobile Terminal 101</p>
    <p>Processing in the mobile terminal 101 will be described first.</p>
    <p>FIG. 4 is a flow chart showing the entire control operation realized as an operation of the CPU 316 in the control section 110 shown in FIG. 3, which executes a control program stored in the ROM 318 in the control section 110 after power-ON.</p>
    <p>The control program for realizing functions shown in the flow charts of FIGS. 4, 5, and 8 and data necessary for the program may be stored in the IC card 320 detachably attached to the IC card slot 207 shown in FIG. 2 in the form of program codes which can be read by the CPU 316. The program codes may be directly executed by the CPU 316, or loaded in the RAM 317 or the programmable ROM 318, as needed, and executed by the CPU 316. Alternatively, the control program and data necessary for the program may be received from another device via a radio or wire communication line or from the optical transceiver 208 (FIG. 2) through the communication section 111, loaded in the RAM 317 or the programmable ROM 318, and executed by the CPU 316.</p>
    <p>In the repetitive loop of steps 401→411→413→402→403→404→401, determination processing (401) of determining whether the touch panel control section 315 has notified of detection of a touch panel input, determination processing (411) of determining whether E-mail text data has been received from the speech control host unit 108 (FIG. 1), determination processing (413) of determining whether FAX text data has been received, determination processing (402) of determining whether formatted text data has been received, other reception/display processing (403), and transmission processing (404) of transmitting necessary data are executed.</p>
    <p>If the touch panel control section 315 has notified of detection of a touch panel input, i.e., YES in step 401, it is determined in step 405 or 406 whether the touch panel input is an input instruction for the CCD camera 304 (202 in FIG. 2) shown in FIG. 3 or an input instruction for the microphone 301 (201 in FIG. 2) shown in FIG. 3.</p>
    <p>If the touch panel input is an input instruction for the CCD camera 304 (202 in FIG. 2) shown in FIG. 3, i.e., YES in step 405, the camera control section 307 in the input section 109 shown in FIG. 3 is instructed to start image input processing in step 407. The flow advances to transmission processing in step 404. In step 404, if data to be transmitted is present, transmission is executed. Otherwise, the flow returns to step 401.</p>
    <p>If the touch panel input is an input instruction for the microphone 301 (201 in FIG. 2) shown in FIG. 3, i.e., YES in step 406, the microphone control section 303 in the input section 109 shown in FIG. 3 is instructed to start speech input processing in step 408. This speech input processing start instruction corresponds to, e.g., a PHS speech communication processing start instruction or an off-line speech input processing start instruction for executing text speech recognition/formatting.</p>
    <p>The microphone control section 303 instructs the microphone 301 (201 in FIG. 2) and the A/D conversion section 302 to start speech input processing in accordance with the instruction from the CPU 316. As a result, speech data input from the microphone 301 (201 in FIG. 2) is output from the A/D conversion section 302.</p>
    <p>When the speech input processing start instruction is a PHS speech communication processing start instruction, the speech data is sent to a predetermined speech channel in transmission processing (not shown) by the communication control section 321 and transmitted to the communication partner.</p>
    <p>When the speech input processing start instruction contains a speech input processing start instruction for text speech recognition/formatting, speech data input from the microphone 301 (201 in FIG. 2) and output from the microphone control section 303 is transmitted to the speech control host unit 108 in transmission processing in step 404 (to be described later).</p>
    <p>If the touch panel input is neither an input instruction for the CCD camera 304 (202 in FIG. 2) shown in FIG. 3 nor an input instruction for the microphone 301 (201 in FIG. 2) shown in FIG. 3, i.e., NO in steps 405 and 406, another key input processing is executed in step 409. Thereafter, the flow advances to transmission processing in step 404.</p>
    <p>If the RAM 317 in the control section 110 has received formatted text data from the speech control host unit 108 (FIG. 1) through the communication section 111, i.e., YES in step 402, the formatted text data received by the RAM 317 is inserted into text template data of a format type corresponding to the format type data designated by the user in advance, and transferred from the RAM 317 to the memory 313 in the output section 112, and the LCD control section 314 is instructed to display the data in step 410. The formatted text data output from the memory 313 through the LCD driver 312 is displayed on the LCD display section 311 (203 in FIG. 2) under the control of the LCD control section 314.</p>
    <p>If the RAM 317 in the control section 110 has received E-mail text data from the speech control host unit 108 (FIG. 1) through the communication section 111, i.e., YES in step 411, the E-mail text data received by the RAM 317 is transferred from the RAM 317 to the memory 313 in the output section 112, and the LCD control section 314 is instructed to display the data in step 412. The received E-mail text data output from the memory 313 through the LCD driver 312 is displayed on the LCD display section 311 (203 in FIG. 2) under the control of the LCD control section 314.</p>
    <p>If the RAM 317 in the control section 110 has received FAX text data from the speech control host unit 108 (FIG. 1) through the communication section 111, i.e., YES in step 413, the FAX text data received by the RAM 317 is transferred from the RAM 317 to the memory 313 in the output section 112, and the LCD control section 314 is instructed to display the data in step 414. The received FAX text data output from the memory 313 through the LCD driver 312 is displayed on the LCD display section 311 (203 in FIG. 2) under the control of the LCD control section 314.</p>
    <p>Transmission processing in step 404 will be described next.</p>
    <p>FIG. 5 is a flow chart showing details of transmission processing.</p>
    <p>It is determined in step 501 whether key inputs from the touch panel, which have been processed by another key input processing in step 409 in FIG. 4, have a transmission instruction. If NO in step 501, the flow advances to step 505.</p>
    <p>If YES in step 501, it is determined in step 502 whether the mobile terminal 101 is currently being connected to the mobile terminal control host unit 104 shown in FIG. 1.</p>
    <p>If the mobile terminal 101 is being connected to the mobile terminal control host unit 104 in FIG. 1, i.e., YES in step 502, the CPU 316 in the control section 110 shown in FIG. 3 requests the communication control section 321 in the communication section 111 shown in FIG. 3 to transmit the "terminal identification code" of the mobile terminal 101 and a command corresponding to the key input processing in step 504. The communication control section 321 generates a TCP/IP packet storing the "terminal identification code" and the command and transmits the TCP/IP packet to a predetermined host unit (e.g., the speech control host unit 108 shown in FIG. 1) connected to the Internet 105.</p>
    <p>If the mobile terminal 101 is not being connected to the mobile terminal control host unit 104 shown in FIG. 1, i.e., NO in step 502, the CPU 316 in the control section 110 shown in FIG. 3 requests the communication control section 321 in the communication section 111 shown in FIG. 3 to originate a call in step 503 and then executes processing in step 504.</p>
    <p>As will be described later, a transmission instruction for transmitting a text speech recognition/formatting start request command and format type data based on the instruction of the user, a text speech recognition/formatting end request command transmission instruction, a mail reception request command transmission instruction, and a FAX reception request command transmission instruction are issued in step 504.</p>
    <p>As described above, if NO in step 501, processing in step 504 is performed, and it is determined in step 505 whether an instruction for transmitting speech data to the speech control host unit 108 (FIG. 1) has been issued.</p>
    <p>If NO in step 505, the flow advances to step 510.</p>
    <p>If YES in step 505, it is determined in step 506 whether transmission enable data as a response to the text speech recognition/formatting start request command has already been returned from the speech control host unit 108.</p>
    <p>If NO in step 506, i.e., the speech control host unit 108 has not completed preparation for the text speech recognition/formatting start request command from the mobile terminal 101 yet, the flow advances to step 510.</p>
    <p>If transmission enable data as a response to the text speech recognition/formatting start request command has already been returned from the speech control host unit 108, i.e., YES in step 506, it is determined in step 507 whether the mobile terminal 101 is currently being connected to the mobile terminal control host unit 104 shown in FIG. 1.</p>
    <p>If the mobile terminal 101 is being connected to the mobile terminal control host unit 104 shown in FIG. 1, i.e., YES in step 507, the CPU 316 in the control section 110 shown in FIG. 3 requests the communication control section 321 in the communication section 111 to transmit speech data which has been transferred from the microphone control section 303 in the input section 109 shown in FIG. 3 to the RAM 317 in the control section 110 in step 509. The communication control section 321 generates a TCP/IP packet storing the speech data and transmits the TCP/IP packet to the speech control host unit 108 connected to the Internet 105 shown in FIG. 1.</p>
    <p>If the mobile terminal 101 is not being connected to the mobile terminal control host unit 104 shown in FIG. 1, i.e., NO in step 507, the CPU 316 in the control section 110 shown in FIG. 3 requests the communication control section 321 in the communication section 111 shown in FIG. 3 to originate a call in step 508 and then executes processing in step 509.</p>
    <p>As will be described later, a speech data transmission instruction for text speech recognition/formatting is issued in step 509.</p>
    <p>As described above, if NO in step 505 or 506, processing in step 509 is performed, and it is determined in step 510 whether an image input processing start instruction has been executed, and an instruction for transmitting image data to an image control host unit (not shown) connected to the Internet 105 shown in FIG. 1 has been issued in step 407 in FIG. 4.</p>
    <p>If NO in step 510, the flow advances to step 514.</p>
    <p>If YES in step 510, it is determined in step 511 whether the mobile terminal 101 is currently being connected to the mobile terminal control host unit 104 shown in FIG. 1.</p>
    <p>If the mobile terminal 101 is being connected to the mobile terminal control host unit 104 shown in FIG. 1, i.e., YES in step 511, the CPU 316 in the control section 110 shown in FIG. 3 requests the communication control section 321 in the communication section 111 to transmit image data which has been stored in the memory 306 in the input section 109 shown in FIG. 3 in step 513. The communication control section 321 generates a TCP/IP packet storing the image data and transmits the TCP/IP packet to the speech control host unit (not shown) 108 connected to the Internet 105.</p>
    <p>If the mobile terminal 101 is not being connected to the mobile terminal control host unit 104 shown in FIG. 1, i.e., NO in step 511, the CPU 316 in the control section 110 shown in FIG. 3 requests the communication control section 321 in the communication section 111 shown in FIG. 3 to originate a call in step 512, and then executes processing in step 513.</p>
    <p>As described above, if NO in step 510, or after processing in step 513, it is determined in step 514 whether the key inputs from the touch panel which have been processed by another key input processing in step 409 shown in FIG. 4 have an E-mail text data transmission instruction.</p>
    <p>If NO in step 514, the flow advances to step 518.</p>
    <p>If YES in step 514, it is determined in step 515 whether the mobile terminal 101 is currently being connected to the mobile terminal control host unit 104 shown in FIG. 1.</p>
    <p>If the mobile terminal 101 is being connected to the mobile terminal control host unit 104 shown in FIG. 1, i.e., YES in step 515, the CPU 316 in the control section 110 shown in FIG. 3 requests the communication control section 321 in the communication section 111 shown in FIG. 3 to transmit E-mail text data corresponding to the key input processing in step 517. In this case, a "From" field representing the transmission source address is automatically added to the E-mail text data. The communication control section 321 generates a TCP/IP packet storing the E-mail text data and transmits the TCP/IP packet to a predetermined host unit (e.g., the speech control host unit 108 shown in FIG. 1) connected to the Internet 105.</p>
    <p>If the mobile terminal 101 is not being connected to the mobile terminal control host unit 104 shown in FIG. 1, i.e., NO in step 515, the CPU 316 in the control section 110 shown in FIG. 3 requests the communication control section 321 in the communication section 111 shown in FIG. 3 to originate a call in step 516, and then executes processing in step 517.</p>
    <p>As described above, if NO in step 514, or after processing in step 517, it is determined in step 518 whether the key inputs from the touch panel which have been processed by another key input processing in step 409 shown in FIG. 4 have a FAX text data transmission instruction.</p>
    <p>If NO in step 518, transmission processing in step 404 shown in FIG. 4 is ended.</p>
    <p>If YES in step 518, it is determined in step 519 whether the mobile terminal 101 is currently being connected to the mobile terminal control host unit 104 shown in FIG. 1.</p>
    <p>If the mobile terminal 101 is being connected to the mobile terminal control host unit 104 shown in FIG. 1, i.e., YES in step 519, the CPU 316 in the control section 110 shown in FIG. 3 requests the communication control section 321 in the communication section 111 shown in FIG. 3 to transmit FAX text data corresponding to the key input processing in step 521. In this case, transmission source information is automatically added to the FAX text data. The communication control section 321 generates a TCP/IP packet storing the FAX text data and transmits the TCP/IP packet to a predetermined host unit (e.g., the speech control host unit 108 shown in FIG. 1) connected to the Internet 105.</p>
    <p>If the mobile terminal 101 is not being connected to the mobile terminal control host unit 104 shown in FIG. 1, i.e., NO in step 519, the CPU 316 in the control section 110 shown in FIG. 3 requests the communication control section 321 in the communication section 111 shown in FIG. 3 to originate a call in step 520, and then executes processing in step 521.</p>
    <p>As described above, if NO in step 518, or after processing in step 521, transmission processing in step 404 shown in FIG. 4 is ended.</p>
    <p>Format of Communication Data</p>
    <p>FIGS. 6A, 6B, and 6C are views showing the format of communication data transmitted among the mobile terminal 101, the mobile terminal control host unit 104, and the Internet 105 (speech control host unit 108).</p>
    <p>Between the mobile terminal 101 and the mobile terminal control host unit 104, communication data is transferred on a digital communication channel having a PHS standard transfer rate of 32 kbits/sec on the basis of a communication protocol called PPP (Point-to-Point Protocol) using a PPP frame (transferred from the left to the right in FIG. 6A) shown in FIG. 6A.</p>
    <p>Fixed bit strings shown in FIG. 6A are set for "flag", "address", and "control" fields constituting the PPP frame, respectively. The "FCS" field having a data length of 2 octets is called a frame check sequence and stores an error detection/correction data for the PPP frame data. The "information" field (this field has a variable length) of the PPP frame transferred after a PPP link is established between the mobile terminal 101 and the mobile terminal control host unit 104 stores an IP datagram as a fundamental data transfer unit on the Internet 105 (FIG. 1). In this case, the "protocol" field having a data length of 2 octets stores a hexadecimal value of "0021" representing that the IP datagram is stored in the "information" field.</p>
    <p>The IP datagram is stored in the "information" field of the PPP frame, as described above. This IP datagram is the fundamental data transfer unit on the Internet 105, as described above. The IP datagram is defined in accordance with the Internet Protocol (IP) and has a function of uniquely transferring data stored in the "data" field to a destination host unit on the Internet 105, a function of specifying the address on the Internet 105, a function of transferring the IP datagram itself to the host unit designated with a "destination IP address" through a predetermined path on the Internet 105, and a function of fragmenting (dividing) the IP datagram itself and reconstructing the IP datagram.</p>
    <p>As shown in FIG. 6B, the IP datagram is constituted by an IP header field and a data field. All pieces of information necessary for transmitting the IP datagram itself which contains the IP header field are stored in the IP header field. FIG. 7A is a view of the format of the IP header.</p>
    <p>The IP header has a data length of 5 or 6 words each consisting of 32 bits. This data length is stored in the "header length" field of the first word. The total data length of the IP datagram is stored in the "total IP datagram length" of the first word.</p>
    <p>The version of the Internet Protocol (IP) for defining an IP datagram transfer method is set in the "version" field of the first word. The current version is "4".</p>
    <p>Information representing the transmission priority or the like is stored in the "service type" field of the first word, although this field is not particularly related to the present invention.</p>
    <p>Fields of the second word define control information used when the IP datagram is fragmented (divided) because of a restriction on transfer on the Internet 105. A unique integer for identifying the IP datagram before division to which the IP datagram as a divided fragment belongs is set in the "identification number" field. Offset information representing a portion of the IP datagram before division, which corresponds to the IP datagram as a divided fragment is set in the "fragment offset" field. Whether other fragments constituting the IP datagram before division to which the IP datagram as a divided fragment belongs follow this IP datagram is set in the "flag string" field. Even when the IP datagram is fragmented in a relay host unit on the Internet 105, the IP datagram before division can be properly reconstructed on the reception side on the basis of these information.</p>
    <p>Time information in units of seconds which represents the time when the IP datagram is allowed to be present on the Internet 105 is set in the "time to live" (TTL) field of the third word. The relay host unit on the Internet 105 decrements this field value every time an IP datagram is processed. When this value becomes zero or less, the IP datagram is discarded from the Internet 105. With this processing, excess traffic on the Internet 105 can be prevented. Retransmission control for the discarded IP datagram is executed in control processing for TCP segment data stored in the IP datagram.</p>
    <p>An integer value for defining the format of data stored in the "data" field of the IP datagram is set in the "protocol" field of the third word. In this embodiment, since TCP segment data is stored in the "data" field of the IP datagram, as shown in FIG. 6C, an integer value of "6" is set to define the format of the data.</p>
    <p>Checksum data for detecting an error in the IP header data is set in the "header checksum" field of the third word.</p>
    <p>A 32-bit "transmission source IP address" is set in the fourth word. When the IP datagram is to be transferred from the mobile terminal 101 to the speech control host unit 108, an IP address assigned to the mobile terminal 101 by the mobile terminal control host unit 104 in call origination processing (to be described later) is set as a "transmission source IP address". The speech control host unit 108 shown in FIG. 1 stores the "transmission source IP address", so that the speech control host unit 108 can return formatted text data or the like to the mobile terminal 101 through the Internet 105.</p>
    <p>A 32-bit "destination IP address" is set in the fifth word. When the IP datagram is to be transferred from the mobile terminal 101 to the speech control host unit 108, an IP address permanently assigned to the speech control host unit 108 is set as a "destination IP address". The routing section 114 in the mobile terminal control host unit 104, relay host units on the Internet 105, and the router unit 106 in the speech service provider identify the "destination IP address" stored in the received IP datagram. With this operation, the IP datagram transmission path can be determined in accordance with path control table information of these units, and finally, the IP datagram can be transferred to the speech control host unit 108 in the speech service provider.</p>
    <p>The "IP option" field of the sixth word is optionally arranged to set information for testing or debugging networks constituting the Internet 105 or control information for controlling or monitoring the transmission path on the Internet 105, although the "IP option" field is not particularly related to the present invention.</p>
    <p>Padding data for matching the data length is set in the "padding" field of the sixth word.</p>
    <p>TCP segment data is stored in the "data" field of the IP datagram. This TCP segment is defined in accordance with a transmission control protocol (TCP) and has a function for transmitting data stored in the "data" field to the destination host unit on the Internet 105 properly in an appropriate order. The IP datagram provides only the function of uniquely transferring data on the Internet 105 and no function of ensuring the reliability of the data (e.g., retransmission control function) while the TCP segment provides a function of ensuring the reliability of the data.</p>
    <p>Communication data has a hierarchical structure of a (PPP frame), an IP datagram, and a TCP segment to efficiently cope with different requirements that efficient data transmission under a minimum processing load is necessary on the Internet 105, and end-to-end data transmission must be as reliable as possible. With this arrangement, the relay host unit on the Internet 105 can efficiently transmit information (TCP segment) stored in the "data" field of the IP datagram to the destination host unit as fast as possible by referring to only the IP header of the IP datagram. In end-to-end transmission (between the transmission source host unit and the destination host unit), highly reliable data communication such as retransmission control can be realized by referring to only the TCP header of the TCP segment.</p>
    <p>As shown in FIG. 6C, the TCP segment is constituted by a TCP header field and a data field. FIG. 7B is a view of the format of the TCP header.</p>
    <p>Like the IP header, the TCP header has a data length of 5 or 6 words each consisting of 32 bits. This data length is stored in the "header length" field of the fourth word. The total data length of the IP datagram is stored in the "total IP datagram length" of the first word.</p>
    <p>A 16-bit integer value for specifying a communication protocol for text speech recognition/formatting, a 16-bit integer value for specifying a mail transmission protocol (e.g., SMTP: Simple Mail Transfer Protocol), a 16-bit integer value for specifying a mail reception protocol (e.g., POP3), or a 16-bit integer value for specifying a FAX communication protocol is set in the "transmission source port number" field and the "destination port number" field of the first word.</p>
    <p>The packet transmission/reception section 115 (FIG. 1) in the speech control host unit 108 recognizes the value set in the "destination port number" field of the TCP header of the received TCP segment, thereby determining an application executed by the speech control host unit 108 as a transfer destination of data stored in the "data" field of the TCP segment.</p>
    <p>When the value set in the "destination port number" field of the TCP header of the received TCP segment corresponds to the communication protocol for text speech recognition/formatting, the packet transmission/reception section 115 can transfer speech data stored in the "data" field of the TCP segment to the mobile terminal communication control section 116. When the value corresponds to the above-described mail transmission protocol or mail reception protocol, the packet transmission/reception section 115 can transfer E-mail text data or a mail reception request command stored in the "data" field of the TCP segment to the mail transmission/reception section 119. When the value corresponds to the above-described FAX communication protocol, the packet transmission/reception section 115 can transfer FAX text data or a FAX reception request command stored in the "data" field of the TCP segment to the FAX transmission/reception section 120.</p>
    <p>Similarly, the communication control section 321 (FIG. 3) in the communication section 111 of the mobile terminal 101 recognizes the value set in the "destination port number" field of the TCP header of the received TCP segment, thereby determining an application executed by the movable terminal 101 as a transfer destination of data stored in the "data" field of the TCP segment.</p>
    <p>When the value set in the "destination port number" field of the TCP header of the received TCP segment corresponds to the communication protocol for text speech recognition/formatting, the communication control section 321 can notify the control section 110 (FIG. 1 or 3) of reception of data for text speech recognition/formatting and transfer formatted text data stored in the "data" field of the TCP segment. When the value corresponds to the above-described mail transmission protocol or mail reception protocol, the communication control section 321 can notify the control section 110 (FIG. 1 or 3) of reception of data for E-mail transmission/reception processing and transfer E-mail text data stored in the "data" field of the TCP segment. When the value corresponds to the FAX communication protocol, the communication control section 321 can notify the control section 110 (FIG. 1 or 3) of reception of data for FAX transmission/reception processing and transfer FAX text data stored in the "data" field of the TCP segment.</p>
    <p>The packet transmission/reception section 115 in the speech control host unit 108 and the communication control section 321 in the communication section 111 of the mobile terminal 101 confirm the "transmission source port number" set in the TCP header of the received TCP segment, thereby confirming the protocol of the application of the transmission source.</p>
    <p>The "sequence number" field of the second word of the TCP header shown in FIG. 7B is a field for notifying, from the transmission side, the reception side of the byte position of the start of the data stored in the "data" field of the TCP segment in the entire byte stream transmitted from the transmission side to the reception side in the current TCP connection. Inversely, the "confirmation response number" field of the third word is a field for notifying, from the reception side, the transmission side of the byte position of the data which has received without any error in the entire byte stream transmitted from the transmission side to the reception side in the current TCP connection. With this arrangement, speech data, E-mail text data, or FAX text data can be reliably transferred in the proper order from, e.g., the mobile terminal 101 to the speech control host unit 108.</p>
    <p>A value representing the type of the TCP segment is set in the "flag string" field of the fourth word. In TCP communication, various control data for confirmation response are transmitted at, e.g., the start or end of connection. The type of control data is set in the "flag string" field.</p>
    <p>The "window" field of the fourth word is a field for notifying, from the reception side, the transmission side of window data representing the number of bytes which can be currently continuously received on the reception side. With this arrangement, data flow control from the reception side to the transmission side is enabled, so that fine control for, e.g., suppressing transmission of speech data, E-mail text data, or FAX text data to the mobile terminal 101 when the load on the speech control host unit 108 is large.</p>
    <p>The "reserved" field of the fourth word is a field for reservation.</p>
    <p>Checksum data for detecting errors in the TCP header and data stored in the "data" field is set in the "checksum" field of the fifth word. With this arrangement, e.g., the speech control host unit 108 can properly receive speech data from the mobile terminal 101.</p>
    <p>The "emergency pointer" field of the fifth word stores control data for transmitting emergency data (e.g., interrupt data or abort data), although this field is not particularly related to the present invention.</p>
    <p>The "option" field of the sixth word is used to, e.g., designate the maximum segment length which can be transmitted between the transmission and reception units, although this field is not particularly related to the present invention.</p>
    <p>Padding data for matching the data length is set in the "padding" field of the sixth word.</p>
    <p>In the mobile terminal 101, the TCP segment communication (terminating) processing function having the above arrangement is realized by the communication control section 321 (FIG. 3) in the communication section 111. In the speech control host unit 108, this function is realized by the packet transmission/reception section 115 (FIG. 1). The control program executed by the CPU 316 in the mobile terminal 101 may realize the above processing function.</p>
    <p>Call Origination Processing</p>
    <p>As described above, in transmission processing shown in FIG. 5, if the mobile terminal 101 is not being connected to the mobile terminal control host unit 104, i.e., NO in step 502, 507, 511, 515, or 519, the CPU 316 (FIG. 3) in the control section 110 of the mobile terminal 101 requests the communication control section 321 in the communication section 111 shown in FIG. 3 to originate a call in step 503, 508, 512, 516, or 520. FIG. 8 is a flow chart showing call origination processing executed by the communication control section 321 in response to this request.</p>
    <p>In step 801, a link establishment phase is executed. In this phase, a dial-up call is automatically originated for the access telephone number of the mobile terminal control host unit 104. After the call has terminated at the mobile terminal control host unit 104, negotiation associated with determination of the maximum data length of a PPP frame (FIG. 6A) used for communication, determination of nontransmission characters which are to be escaped, determination of the presence/absence of compression of data length of the "protocol" field (FIG. 6A) of the PPP frame from 2 octets to 1 octet, determination of the presence/absence of omission (compression) of the "address" field (FIG. 6A) having a fixed value of "11111111" from the PPP frame, and the like is executed between the communication control section 321 and the connection establishment section 113 (FIG. 1) in the mobile terminal control host unit 104 using a protocol called a link control protocol (LCP). In this case, communication between the communication control section 321 in the communication section 111 of the mobile terminal 101 and the connection establishment section 113 in the mobile terminal control host unit 104 is executed using a PPP frame having the format shown in FIG. 6A while setting a hexadecimal value of "C021" for specifying the LCP in the "protocol" field of the PPP frame and necessary control data in the "information" field of the PPP frame.</p>
    <p>An authentication phase is executed in step 802. In this phase, the user who is using the mobile terminal 101 is authenticated by the connection establishment section 113 (FIG. 1) in the mobile terminal control host unit 104 for the mobile terminal 101 using an authentication protocol called PAP (Password Authentication Protocol) or CHAP (Challenge Handshake Authentication Protocol). With this processing, the Internet provider operating the mobile terminal control host unit 104 can determine whether the user who is using the mobile terminal 101 is a user as a subscriber. In this case, communication between the communication control section 321 in the communication section 111 of the mobile terminal 101 and the connection establishment section 113 in the mobile terminal control host unit 104 is executed using a PPP frame having the format shown in FIG. 6A while setting a hexadecimal value of "C023" for specifying PAP of a hexadecimal value of "C223" for specifying CHAP in the "protocol" field of the PPP frame and necessary authentication data in the "information" field of the PPP frame.</p>
    <p>Finally, a network layer protocol phase is executed in step 803. In this embodiment, in this network layer protocol phase, the presence/absence of compression of the TCP header (FIG. 7B) is determined using a protocol called IP control protocol (IPCP). In addition, one of free (unused) IP addresses which can be assigned by the mobile terminal control host unit 104 is assigned to the mobile terminal 101, and necessary path information is set in the communication control section 321 (FIG. 3) in the communication section 111 of the mobile terminal 101 and the routing section 114 (FIG. 1) in the mobile terminal control host unit 104. Thereafter, the mobile terminal 101 can access the speech control host unit 108 connected to the Internet 105 and an arbitrary resource desired by the user on the Internet 105. In this case, communication between the communication control section 321 in the communication section 111 of the mobile terminal 101 and the connection establishment section 113 in the mobile terminal control host unit 104 is executed using a PPP frame having the format shown in FIG. 6A while setting a hexadecimal value of "8021" for specifying IPCP in the "protocol" field of the PPP frame and necessary data for IP address negotiation in the "information" field of the PPP frame.</p>
    <p>With the above series of operations, the mobile terminal 101 can transmit/receive a PPP frame storing a TCP/IP packet for communication to/from the routing section 114 in the mobile terminal control host unit 104, so that the mobile terminal 101 can freely access resources on the Internet 105.</p>
    <p>To enable access to the speech control host unit 108 or the like in PHS speech communication as well, the mobile terminal 101 may have, e.g., a two-channel simultaneous communication function.</p>
    <p>When no transmitted/received data is detected for a predetermined time (e.g., 10 minutes), the communication control section 321 (FIG. 3) in the communication section 111 of the mobile terminal 101 may automatically disconnect the PPP link from the mobile terminal control host unit 104.</p>
    <p>Details of Transmission/reception Processing of Mobile Terminal 101 Associated with Text Speech Recognition/Formatting</p>
    <p>Details of transmission/reception processing executed by the mobile terminal 101 when and after the user operates the touch panel of the mobile terminal 101 to designate a format type and the start of text speech recognition/formatting will be described.</p>
    <p>In the control operation corresponding to the above-described flow chart shown in FIG. 4, in which the touch panel operation is detected by the touch panel control section 315 shown in FIG. 3 and executed by the CPU 316 (FIG. 3) in the control section 110, the above-described touch panel operation is detected when YES in step 401 and NO in steps 405 and 406, and another key input processing is executed in step 409. In transmission processing in step 404, if YES in step 501 shown in FIG. 5, and call origination processing is executed in step 503 as needed, the communication control section 321 in the communication section 111 shown in FIG. 3 is requested to transmit the "terminal identification code" of the mobile terminal 101 and a command and data corresponding to the key input processing for instructing to start text speech recognition/formatting in step 504.</p>
    <p>Consequently, the communication control section 321 generates a TCP segment having the format shown in FIG. 6C. In this case, a 16-bit integer value for specifying a communication protocol for text speech recognition/formatting is set in the "transmission source port number" field and the "destination port number" field of the TCP header having the format shown in FIGS. 6C and 7B. A "terminal identification code" (e.g., PHS telephone number) for specifying the mobile terminal 101, a text speech recognition/formatting start request command based on the instruction of the user, and format type data based on the instruction of the user are stored in the "data" field of the TCP segment.</p>
    <p>Next, the communication control section 321 generates an IP datagram having the format shown in FIG. 6B in which the TCP segment is stored in the "data" field. In this case, an integer value of "6" for defining the format of the TCP segment data stored in the "data" field is set in the "protocol" field of the IP header having the format shown in FIGS. 6B and 7A. An IP address assigned to the communication control section 321 in the communication section 111 of the mobile terminal 101 by the connection establishment section 113 in the mobile terminal control host unit 104 in call origination processing (see the description about step 803 in FIG. 8) which has already been executed is set in the "transmission source IP address" field. An IP address assigned to the speech control host unit 108 is set in the "destination IP address" field.</p>
    <p>The communication control section 321 generates a PPP frame having the format shown in FIG. 6A, in which the IP datagram is stored in the "information" field, and a hexadecimal value of "0021" representing that the IP datagram is stored in the "information" field is stored in the "protocol" field, and transmits the PPP frame to the mobile terminal control host unit 104 in accordance with path information (see the description about step 803 in FIG. 8) set in the communication control section 321. A data unit constituted by the TCP segment, the IP datagram, and the PPP frame and transferred in the Internet 105 will be simply referred to as a TCP/IP packet hereinafter.</p>
    <p>This TCP/IP packet is transferred to the router unit 106 in the speech service provider by the routing section 114 in the mobile terminal control host unit 104 and the relay host unit (not shown) in the Internet 105 on the basis of the "destination IP address" stored in the IP header of the IP datagram constituting the TCP/IP packet, and then transferred to the packet transmission/reception section 115 in the speech control host unit 108 through the LAN 107.</p>
    <p>The packet transmission/reception section 115 identifies that the IP address of the speech control host unit 108, i.e., the packet transmission/reception section 115 itself is set in the "destination IP address" field of the IP header of the IP datagram constituting the transferred TCP/IP packet, thereby receiving the TCP/IP packet.</p>
    <p>The packet transmission/reception section 115 confirms that the 16-bit integer value for specifying the communication protocol for text speech recognition/formatting is set in the "destination port number" field and the "transmission source port number" field of the TCP segment constituting the received TCP/IP packet, thereby notifying the mobile terminal communication control section 116 (FIG. 1) of the reception.</p>
    <p>Upon this notification, the packet transmission/reception section 115 extracts the "transmission source IP address" from the IP header of the IP datagram constituting the received TCP/IP packet and also extracts the "terminal identification code", the text speech recognition/formatting start request command, and the format type data from the "data" field of the TCP segment constituting the TCP/IP packet, and transfers these data to the mobile terminal communication control section 116.</p>
    <p>As a result, a TCP/IP packet storing transmission enable data is returned from the speech control host unit 108 to the mobile terminal 101 in a way to be described later.</p>
    <p>This TCP/IP packet is transferred to the routing section 114 in the mobile terminal control host unit 104 by the router unit 106 in the speech service provider and the relay host unit (not shown) in the Internet 105 on the basis of the "destination IP address" stored in the IP header of the IP datagram constituting the TCP/IP packet, and then transferred to the communication control section 321 (FIG. 3) in the communication section 111 of the mobile terminal 101 through the PHS network 103 (FIG. 1).</p>
    <p>The communication control section 321 in the communication section 111 of the mobile terminal 101 identifies that the IP address (temporarily or dynamically) assigned to the mobile terminal 101, i.e., the communication control section 321 itself is set in the "destination IP address" field of the IP header of the IP datagram constituting the transferred TCP/IP packet, thereby receiving the TCP/IP packet.</p>
    <p>The communication control section 321 confirms that the 16-bit integer value for specifying the communication protocol for text speech recognition/formatting is set in the "destination port number" field and the "transmission source port number" field of the TCP segment constituting the received TCP/IP packet, thereby notifying the CPU 316 in the control section 110 of the mobile terminal 101 of the reception.</p>
    <p>Upon this notification, the communication control section 321 extracts the transmission enable data from the "data" field of the TCP segment constituting the received TCP/IP packet and transfers the data to the CPU 316.</p>
    <p>The CPU 316 processes the reception notification and transmission enable data in step 403 shown in FIG. 4 and stores the transmission enable data in the RAM 317.</p>
    <p>When the user operates the touch panel of the mobile terminal 101 to instruct to start text speech recognition/formatting, the CPU 316 instructs the microphone control section 303 in the input section 109 shown in FIG. 3 to start PHS speech communication processing or off-line speech input processing for executing text speech recognition/formatting. With this processing, the user starts to input speech data from the microphone 301 by the speech communication operation or the off-line speech input operation.</p>
    <p>Thereafter, in transmission processing in step 404 executed by the CPU 316 as part of the repetitive loop of steps 401→402→403→404→401 in FIG. 4, when YES in steps 505 and 506 shown in FIG. 5, and call origination processing is executed again in step 508 as needed, the communication control section 321 in the communication section 111 is requested to transmit the speech data transferred from the microphone control section 303 in the input section 109 shown in FIG. 3 to the RAM 317 in the control section 110 in step 509.</p>
    <p>Consequently, the communication control section 321 generates a TCP segment having the format shown in FIG. 6C. In this case, a 16-bit integer value for specifying a communication protocol for text speech recognition/formatting is set in the "transmission source port number" field and the "destination port number" field of the TCP header having the format shown in FIGS. 6C and 7B. The speech data transferred from the microphone control section 303 in the input section 109 shown in FIG. 3 to the RAM 317 in the control section 110 is stored in the "data" field of the TCP segment.</p>
    <p>Next, the communication control section 321 generates an IP datagram having the format shown in FIG. 6B in which the TCP segment is stored in the "data" field. In this case, an integer value of 6 for defining the format of the TCP segment data stored in the "data" field is set in the "protocol" field of the IP header having the format shown in FIGS. 6B and 7A. An IP address assigned to the communication control section 321 in the communication section 111 of the mobile terminal 101 by the connection establishment section 113 in the mobile terminal control host unit 104 in call origination processing (see the description about step 803 in FIG. 8) which has already been executed is set in the "transmission source IP address" field. An IP address assigned to the speech control host unit 108 is set in the "destination IP address" field.</p>
    <p>The communication control section 321 generates a PPP frame having the format shown in FIG. 6A, in which the IP datagram is stored in the "information" field, and a hexadecimal value of "0021" representing that the IP datagram is stored in the "information" field is stored in the "protocol" field, and transmits the PPP frame to the mobile terminal control host unit 104 in accordance with path information (see the description about step 803 in FIG. 8) set in the communication control section 321.</p>
    <p>This TCP/IP packet is transferred to the router unit 106 in the speech service provider by the routing section 114 in the mobile terminal control host unit 104 and the relay host unit (not shown) in the Internet 105 on the basis of the "destination IP address" stored in the IP header of the IP datagram constituting the TCP/IP packet, and then transferred to the packet transmission/reception section 115 in the speech control host unit 108 through the LAN 107.</p>
    <p>The packet transmission/reception section 115 identifies that the IP address of the speech control host unit 108, i.e., the packet transmission/reception section 115 itself is set in the "destination IP address" field of the IP header of the IP datagram constituting the transferred TCP/IP packet, thereby receiving the TCP/IP packet.</p>
    <p>The packet transmission/reception section 115 confirms that the 16-bit integer value for specifying the communication protocol for text speech recognition/formatting is set in the "destination port number" field and the "transmission source port number" field of the TCP segment constituting the received TCP/IP packet, thereby notifying the mobile terminal communication control section 116 (FIG. 1) of the reception.</p>
    <p>Upon this notification, the packet transmission/reception section 115 extracts the "transmission source IP address" from the IP header of the IP datagram constituting the received TCP/IP packet and also extracts the speech data from the "data" field of the TCP segment constituting the TCP/IP packet, and transfers these data to the mobile terminal communication control section 116.</p>
    <p>As a result, the mobile terminal communication control section 116 controls text speech recognition/formatting in a manner to be described later, causes the text speech recognition section 117 to recognize the received speech data, and causes the formatted text generation section 118 to format resultant recognized speech text data. The mobile terminal communication control section 116 returns a TCP/IP packet storing formatted text data obtained from the formatted text generation section 118 to the mobile terminal 101 in a way to be described later.</p>
    <p>This TCP/IP packet is transferred to the routing section 114 in the mobile terminal control host unit 104 by the router unit 106 in the speech service provider and the relay host unit (not shown) in the Internet 105 on the basis of the "destination IP address" stored in the IP header of the IP datagram constituting the TCP/IP packet, and then transferred to the communication control section 321 (FIG. 3) in the communication section 111 of the mobile terminal 101 through the PHS network 103 (FIG. 1).</p>
    <p>The communication control section 321 in the communication section 111 of the mobile terminal 101 identifies that the IP address (temporarily or dynamically) assigned to the mobile terminal 101, i.e., the communication control section 321 itself is set in the "destination IP address" field of the IP header of the IP datagram constituting the transferred TCP/IP packet, thereby receiving the TCP/IP packet.</p>
    <p>The communication control section 321 confirms that the 16-bit integer value for specifying the communication protocol for text speech recognition/formatting is set in the "destination port number" field and the "transmission source port number" field of the TCP segment constituting the received TCP/IP packet, thereby notifying the CPU 316 in the control a section 110 of the mobile terminal 101 of the reception.</p>
    <p>Upon this notification, the communication control section 321 extracts the transmission enable data from the "data" field of the TCP segment constituting the received TCP/IP packet and transfers the data to the CPU 316.</p>
    <p>The CPU 316 processes the reception notification and formatted text data in step 402 shown in FIG. 4 and displays the formatted text data on the LCD display section 311 (203 in FIG. 2).</p>
    <p>The user can operate the touch panel of the mobile terminal 101 to instruct the speech control host unit 108 to execute a text speech recognition/formatting end request command for ending text speech recognition/formatting.</p>
    <p>In the control operation corresponding to the above-described flow chart shown in FIG. 4, in which the touch panel operation is detected by the touch panel control section 315 shown in FIG. 3 and executed by the CPU 316 (FIG. 3) in the control section 110, the above-described touch panel operation is detected when YES in step 401 and NO in steps 405 and 406, and another key input processing is executed in step 409. In transmission processing in step 404, if YES in step 501 shown in FIG. 5, and call origination processing is executed in step 503 as needed, the communication control section 321 in the communication section 111 shown in FIG. 3 is requested to transmit the "terminal identification code" of the mobile terminal 101 and a text speech recognition/formatting end request command in step 504.</p>
    <p>Consequently, the communication control section 321 generates a TCP segment having the format shown in FIG. 6C in which the "terminal identification code" for specifying the mobile terminal 101 and the text speech recognition/formatting end request command are stored in the "data" field. Next, the communication control section 321 generates an IP datagram having the format shown in FIG. 6B in which the TCP segment is stored in the "data" field. The communication control section 321 also generates a PPP frame having the format shown in FIG. 6A in which the IP datagram is stored in the "information" field. The communication control section 321 transmits a TCP/IP packet constituted by the TCP segment, the IP datagram, and the PPP frame. In this case, information set in the TCP header (FIGS. 6C and 7B), the IP header (FIGS. 6B and 7A), and the "protocol" field (FIG. 6A) are the same as those set in transmission of the text speech recognition/formatting start request command.</p>
    <p>As a result, the TCP/IP packet is transferred to the packet transmission/reception section 115 in the speech control host unit 108 through the Internet 105, like the TCP/IP packet storing the text speech recognition/formatting start request command.</p>
    <p>The packet transmission/reception section 115 receives the transferred TCP/IP packet and notifies the mobile terminal communication control section 116 (FIG. 1) of the reception, as in transfer of the TCP/IP packet storing the text speech recognition/formatting start request command.</p>
    <p>Upon this notification, the packet transmission/reception section 115 extracts the "terminal identification code and the text speech recognition/formatting end request command from the "data" field of the TCP segment constituting the received TCP/IP packet and transfers these data to the mobile terminal communication control section 116.</p>
    <p>As a result, the mobile terminal communication control section 116 ends text speech recognition/formatting for the mobile terminal 101 in a way to be described later.</p>
    <p>Details of E-mail Text Data or FAX Text Data Transmission/Reception Processing of Mobile Terminal 101</p>
    <p>Details of an operation of the mobile terminal 101 which is performed when the user operates the touch panel of the mobile terminal 101 to instruct to transmit E-mail text data or FAX text data which has already been edited will be described next.</p>
    <p>In the control operation corresponding to the above-described flow chart shown in FIG. 4, in which the touch panel operation is detected by the touch panel control section 315 shown in FIG. 3 and executed by the CPU 316 (FIG. 3) in the control section 110, the above-described touch panel operation is detected when YES in step 401 and NO in steps 405 and 406, and another key input processing is executed in step 409. In transmission processing in step 404, if YES in step 514 (in case of E-mail text data) shown in FIG. 5 or step 518 (in case of FAX text data), and call origination processing is executed in step 516 or 520 as needed, the communication control section 321 in the communication section 111 shown in FIG. 3 is requested to transmit E-mail text data or FAX text data in step 517 or 521. As described above, a "From" field representing the transmission source address is automatically added to the E-mail text data, or transmission source information is automatically added to the FAX text data.</p>
    <p>Consequently, the communication control section 321 generates a TCP segment having the format shown in FIG. 6C. In this case, a 16-bit integer value for specifying a mail transmission protocol (e.g., SMTP) or a 16-bit integer value for specifying a FAX communication protocol is set in the "transmission source port number" field and the "destination port number" field of the TCP header having the format shown in FIGS. 6C and 7B. E-mail text data or FAX text data is set in the "data" field of the TCP segment.</p>
    <p>Next, the communication control section 321 generates an IP datagram having the format shown in FIG. 6B in which the TCP segment is stored in the "data" field. The communication control section 321 also generates a PPP frame having the format shown in FIG. 6A in which the IP datagram is stored in the "information" field. The communication control section 321 transmits a TCP/IP packet constituted by the TCP segment, the IP datagram, and the PPP frame. In this case, the pieces of information set in the IP header (FIGS. 6B and 7A) and the "protocol" field (FIG. 6A) are the same as those set in transmission of speech data in text speech recognition/formatting.</p>
    <p>As a result, the TCP/IP packet is transferred to the packet transmission/reception section 115 in the speech control host unit 108 through the Internet 105, like the TCP/IP packet storing speech data in text speech recognition/formatting.</p>
    <p>The packet transmission/reception section 115 identifies that the IP address of the speech control host unit 108, i.e., the packet transmission/reception section 115 itself is set in the "destination IP address" field of the IP header of the IP datagram constituting the transferred TCP/IP packet, thereby receiving the TCP/IP packet.</p>
    <p>The packet transmission/reception section 115 confirms that the 16-bit integer value for specifying the mail transmission protocol or the 16-bit integer value for specifying the FAX communication protocol is set in the "transmission source port number" field and the "destination port number" field of the TCP segment constituting the received TCP/IP packet, thereby notifying the mail transmission/reception section 119 or the FAX transmission/reception section 120 of the reception.</p>
    <p>Upon this notification, the packet transmission/reception section 115 extracts the "transmission source IP address" from the IP header of the IP datagram constituting the received TCP/IP packet and E-mail text data or FAX text data from the "data" field of the TCP segment constituting the TCP/IP packet and transfers these data to the mail transmission/reception section 119 or the FAX transmission/reception section 120.</p>
    <p>As a result, the mail transmission/reception section 119 or the FAX transmission/reception section 120 executes transmission processing (to be described later) for the E-mail text data or the FAX text data.</p>
    <p>Details of an operation of the mobile terminal 101 which is performed when the user operates the touch panel of the mobile terminal 101 to instruct to receive E-mail text data or FAX text data will be described next.</p>
    <p>In the control operation corresponding to the above-described flow chart shown in FIG. 4, in which the touch panel operation is detected by the touch panel control section 315 shown in FIG. 3 and executed by the CPU 316 (FIG. 3) in the control section 110, the above-described touch panel operation is detected when YES in step 401 and NO in steps 405 and 406, and another key input processing is executed in step 409. In transmission processing in step 404, if YES in step 501 shown in FIG. 5, and call origination processing is executed in step 503 as needed, the communication control section 321 in the communication section 111 shown in FIG. 3 is requested to transmit a mail reception request command or a FAX reception request command in step 504.</p>
    <p>Consequently, the communication control section 321 generates a TCP segment having the format shown in FIG. 6C in which a "terminal identification code" for specifying the mobile terminal 101 and a mail reception request command or a FAX reception request command are stored in the "data" field. Next, the communication control section 321 generates an IP datagram having the format shown in FIG. 6B in which the TCP segment is stored in the "data" field, generates a PPP frame having the format shown in FIG. 6A in which the IP datagram is stored in the "information" field, and transmits a TCP/IP packet constituted by the TCP segment, the IP datagram, and the PPP frame. In this case, information set in the TCP header (FIGS. 6C and 7B), the IP header (FIGS. 6B and 7A), and the "protocol" field (FIG. 6A) are the same as those set in transmission of E-mail text data or FAX text data.</p>
    <p>As a result, the TCP/IP packet is transferred to the packet transmission/reception section 115 in the speech control host unit 108 through the Internet 105, as in transmission of E-mail text data or FAX text data.</p>
    <p>The packet transmission/reception section 115 identifies that the IP address of the speech control host unit 108, i.e., the packet transmission/reception section 115 itself is set in the "destination IP address" field of the IP header of the IP datagram constituting the transferred TCP/IP packet, thereby receiving the TCP/IP packet.</p>
    <p>The packet transmission/reception section 115 confirms that the 16-bit integer value for specifying the mail reception protocol or the 16-bit integer value for specifying the FAX communication protocol is set in the "destination port number" field and the "transmission source port number" field of the TCP segment constituting the received TCP/IP packet, thereby notifying the mail transmission/reception section 119 or the FAX transmission/reception section 120 of the reception.</p>
    <p>Upon this notification, the packet transmission/reception section 115 extracts the "transmission source IP address" from the IP header of the IP datagram constituting the received TCP/IP packet and the "terminal identification code" and the mail reception request command or the FAX reception request command from the "data" field of the TCP segment constituting the TCP/IP packet, and transfers these data to the mail transmission/reception section 119 or the FAX transmission/reception section 120.</p>
    <p>Upon fetching the mail reception request command or the FAX reception request command, the mail transmission/reception section 119 or the FAX transmission/reception section 120 extracts the E-mail text data or the FAX text data received for the mobile terminal 101 from a spool file corresponding to the "terminal identification code" transferred from the mobile terminal 101 together with the command, and transmits the E-mail text data or the FAX text data to the mobile terminal 101 through the packet transmission/reception section 115 in a way to be described later.</p>
    <p>This TCP/IP packet is transferred to the routing section 114 in the mobile terminal control host unit 104 by the router unit 106 in the speech service provider and the relay host unit (not shown) in the Internet 105 on the basis of the "destination IP address" stored in the IP header of the IP datagram constituting the TCP/IP packet, and then transferred to the communication control section 321 (FIG. 3) in the communication section 111 of the mobile terminal 101 through the PHS network 103 (FIG. 1).</p>
    <p>The communication control section 321 in the communication section 111 of the mobile terminal 101 identifies that the IP address (temporarily or dynamically) assigned to the mobile terminal 101, i.e., the communication control section 321 itself is set in the "destination IP address" field of the IP header of the IP datagram constituting the transferred TCP/IP packet, thereby receiving the TCP/IP packet.</p>
    <p>The communication control section 321 confirms that the 16-bit integer value for specifying the mail reception protocol or the 16-bit value for specifying the FAX communication protocol is set in the "destination port number" field and the "transmission source port number" field of the TCP segment constituting the received TCP/IP packet, thereby notifying the CPU 316 in the control section 110 of the mobile terminal 101 of the reception.</p>
    <p>Upon this notification, the communication control section 321 extracts the E-mail text data or the FAX text data from the "data" field of the TCP segment constituting the received TCP/IP packet and transfers the E-mail text data or the FAX text data to the CPU 316.</p>
    <p>The CPU 316 processes the reception notification and the E-mail text data or the FAX text data in step 412 or 414 executed on the basis of determination processing in step 411 or 413 shown in FIG. 4 and displays the E-mail text data or the FAX text data on the LCD display section 311 (203 in FIG. 2).</p>
    <p>General Operations of Mobile Terminal Communication Control Section 116, Text Speech Recognition Section 117, and Formatted Text Generation Section 118</p>
    <p>General operations of the mobile terminal communication control section 116, the text speech recognition section 117, and the formatted text generation section 118 in the speech control host unit 108 will be described next.</p>
    <p>The mobile terminal communication control section 116 registers an entry in a processing terminal registration table having a data structure shown in FIG. 10 in correspondence with the "terminal identification code" (the "terminal identification code" is stored in the TCP segment for transferring a command) assigned to the mobile terminal 101 which has transmitted a text speech recognition/formatting start request command. The mobile terminal communication control section 116 also generates a format type based on format type data, a buffer file (speech buffer file) for receiving speech data, a buffer file (text buffer file) for temporarily storing recognized speech text data, and a buffer file (formatted text buffer file) for transmitting formatted text data on a file system managed by the speech control host unit 108. In the processing terminal registration table shown in FIG. 10, the file names of generated files are stored in correspondence with the terminal identification code, the transmission source IP address, the format type, and the final access time. Upon successfully registering the entry and files, the mobile terminal communication control section 116 returns transmission enable data to the mobile terminal 101 corresponding to the "transmission source IP address" stored in the IP datagram which has transferred it.</p>
    <p>Thereafter, the mobile terminal communication control section 116 additionally writes speech data received from the mobile terminal 101 in a speech buffer file specified from the entry of the processing terminal registration table corresponding to the "transmission source IP address" (the "transmission source IP address" is stored in the IP datagram which has transferred it).</p>
    <p>If speech data has been received in the speech buffer file specified from the entry, the text speech recognition section 117 executes text speech recognition processing in units of entries of the processing terminal registration table shown in FIG. 10, and additionally writes resultant recognized speech text data in a text buffer file corresponding to the entry.</p>
    <p>When recognized speech text data has been obtained in the text buffer file specified from the entry, the formatted text generation section 118 (FIG. 1) formats the recognized speech text data in units of entries of the processing terminal registration table shown in FIG. 10, and additionally writes the resultant formatted text data in a formatted text buffer file corresponding to the entry.</p>
    <p>When formatted text data has been obtained in the formatted text buffer file specified from the entry, the mobile terminal communication control section 116 returns the formatted text data to the mobile terminal 101 corresponding to the "transmission source IP address" registered in the entry in units of entries of the processing terminal registration table.</p>
    <p>The mobile terminal communication control section 116 deletes the contents of an entry of the processing terminal registration table for which a text speech recognition/formatting end request command is received, or the final access time is earlier than the current time by a predetermined time or more, and deletes buffer files specified from the entry.</p>
    <p>Details of Operation of Mobile Terminal Communication Control Section 116</p>
    <p>FIGS. 9A through 9C are flow charts showing the control operation executed by the mobile terminal communication control section 116 to realize the above function. The mobile terminal communication control section 116 has a processor and a control program. The operation flow is realized as an operation performed by the processor to execute the control program.</p>
    <p>It is determined in step 901 whether the packet transmission/reception section 115 (FIG. 1) in the speech control host unit 108 has notified the mobile terminal communication control section 116 of reception. As described above, the packet transmission/reception section 115 identifies that the IP address of the speech control host unit 108, i.e., the packet transmission/reception section 115 itself is set in the "destination IP address" of the IP header of the IP datagram constituting the TCP/IP packet transferred from the Internet 105, thereby receiving the TCP/IP packet. The packet transmission/reception section 115 also confirms that the 16-bit integer value for specifying the communication protocol for text speech recognition/formatting is set in the "destination port number" field and the "transmission source port number" field of the TCP segment constituting the TCP/IP packet, thereby notifying the mobile terminal communication control section 116 of the reception. This reception of notification is associated with a text speech recognition/formatting start request command and format type data, speech data as a target of text speech recognition/formatting, or a text speech recognition/formatting end request command.</p>
    <p>If the packet transmission/reception section 115 has notified the mobile terminal communication control section 116 of the reception, i.e., YES In step 901, data transferred from the packet transmission/reception section 115 together with the reception notification is fetched in step 902. When the reception notification is associated with a text speech recognition/formatting start request command, the "transmission source IP address", the "terminal identification code", the command, and the format type data are fetched. When the reception notification is associated with speech data, the "transmission source IP address" and the speech data are fetched. When the reception notification is associated with a text speech recognition/formatting end request command, the "terminal identification code" and the command are fetched.</p>
    <p>After processing in step 902, step 903 in FIG. 9A and steps 907 and 909 in FIG. 9B are sequentially checked, and one determination result becomes YES. More specifically, if the data transferred from the packet transmission/reception section 115 in step 902 is associated with a text speech recognition/formatting start request command, i.e., YES in step 903, processing in steps 904 through 906 is executed. If the data is associated with speech data, i.e., YES in step 907 in FIG. 9B, processing in step 908 is executed. If the data is associated with a text speech recognition/formatting end request command, i.e., YES in step 909 in FIG. 9B, processing in steps 910 and 911 is executed.</p>
    <p>If the packet transmission/reception section 115 has not notified the mobile terminal communication control section 116 of the reception, i.e., NO in step 901, processing corresponding to reception of the command or speech data is performed, and thereafter, formatted text data transmission processing is executed in steps 912 and 913 in FIG. 9C. Processing for ending communication with the mobile terminal 101 for which the final access time is earlier by a predetermined time or more is performed in steps 914 and 915, and the flow returns to determination processing in step 901.</p>
    <p>Processing executed in steps 904 and 906 when YES in step 901, and the data transferred from the packet transmission/reception section 115 in step 902 is associated with a text speech recognition/formatting start request command, i.e., YES in step 903 will be described.</p>
    <p>In step 904, a speech buffer file for receiving speech data, a text buffer file for temporarily storing recognized speech text data, and a formatted text buffer file for transmitting formatted text data are generated on the file system managed by the speech control host unit 108.</p>
    <p>In step 905, one entry (data set of one row) is ensured on the processing terminal registration table having the data structure shown in FIG. 10, which is stored in a memory (not shown) in the mobile terminal communication control section 116. A "terminal identification code", a "transmission source IP address", a format type based on format type data, a final access time, a speech buffer file name, a text buffer file name, and a formatted text buffer file name are registered in the entry. The "terminal identification code" is data transferred from the packet transmission/reception section 115 in step 902, which has been stored in the "data" field of the TCP segment constituting the TCP/IP packet transferred from the mobile terminal 101 (FIG. 6C). The "transmission source IP address" is data transferred from the packet transmission/reception section 115 in step 902, which has been stored in the IP header of the IP datagram constituting the TCP/IP packet transferred from the mobile terminal 101 (FIGS. 6B and 7A). The current time is set in the final access time. The buffer file names represent the respective files generated in step 904.</p>
    <p>After processing in step 905, transmission enable data is returned in step 906 to the "transmission source IP address" transferred from the packet transmission/reception section 115 in step 902 and registered in the entry of the processing terminal registration table.</p>
    <p>More specifically, the mobile terminal communication control section 116 requests the packet transmission/reception section 115 (FIG. 1) to return transmission enable data to the "transmission source IP address".</p>
    <p>Consequently, the packet transmission/reception section 115 generates a TCP segment having the format shown in FIG. 6C. In this case, a 16-bit integer value for specifying a communication protocol for text speech recognition/formatting is set in the "transmission source port number" field and the "destination port number" field of the TCP header having the format shown in FIGS. 6C and 7B. The transmission enable data is stored in the "data" field of the TCP segment.</p>
    <p>Next, the packet transmission/reception section 115 generates an IP datagram having the format shown in FIG. 6B in which the TCP segment is stored in the "data" field. In this case, a 16-bit integer value for defining the format of the TCP segment data stored in the "data" field is set in the "protocol" field of the IP header having the format shown in FIGS. 6B and 7A. The IP address assigned to the speech control host unit 108 is set in the "transmission source IP address" field. The "transmission source IP address" transferred from the packet transmission/reception section 115 in step 902 of FIG. 9A is set in the "destination IP address" field.</p>
    <p>The packet transmission/reception section 115 generates a frame according to the protocol on the LAN 107 and storing the IP datagram and sends the frame to the LAN 107. For example, if the LAN 107 is a local area network based on Ethernet, the frame is an Ethernet frame.</p>
    <p>The TCP/IP packet constituted by the frame, the IP datagram, and the TCP segment is transferred to the mobile terminal control host unit 104 through the router unit 106 and the Internet 105 on the basis of the "destination IP address" stored in the IP header of the IP datagram constituting the TCP/IP packet, and then transferred to the communication control section 321 (FIG. 3) in the communication section 111 of the mobile terminal 101 through the PHS network 103 and the radio base station (or wire connection unit) 102.</p>
    <p>Thereafter, speech data is transferred from the mobile terminal 101 to the speech control host unit 108, as described above.</p>
    <p>After processing in step 906, formatted text data transmission processing is executed in steps 912 and 913 in FIG. 9C. Processing for ending communication with the mobile terminal 101 for which the final access time is earlier by a predetermined time or more is performed in steps 914 and 915, and the flow returns to determination processing in step 901 in FIG. 9A.</p>
    <p>Processing executed in step 908 when YES in step 901 in FIG. 9A, and the data transferred from the packet transmission/reception section 115 in step 902 is speech data, i.e., YES in step 907 in FIG. 9B will be described next.</p>
    <p>In step 908, an entry of the processing terminal registration table (FIG. 10) which stores the same "transmission source IP address" as that transferred from the packet transmission/reception section 115 in step 902 in FIG. 9A is searched for, and the speech data transferred from the packet transmission/reception section 115 in step 902 in FIG. 9A is additionally written in the speech buffer file (step 904 in FIG. 9A) corresponding to the speech buffer file name stored in the corresponding entry. The size of the speech buffer file in additional writing is automatically adjusted by the file system managed by the speech control host unit 108.</p>
    <p>In addition, the final access time stored in the corresponding entry is updated to the current time in step 908.</p>
    <p>In this manner, the speech data is transferred from the mobile terminal communication control section 116 to the text speech recognition section 117 (FIG. 1) through the speech buffer file for each mobile terminal 101 (for each "terminal identification code"). As will be described later, when speech data has been received in the speech buffer file specified from the entry, the text speech recognition section 117 executes text speech recognition processing for the speech data in units of entries of the processing terminal registration table, and additionally writes the resultant recognized speech text data in the text buffer file corresponding to the entry. As will be described later, when recognized speech text data has been obtained in the text buffer file specified from the entry, the formatted text generation section 118 (FIG. 1) formats the recognized speech text data in units of entries of the processing terminal registration table shown in FIG. 10, and additionally writes resultant formatted text data in the formatted text buffer file corresponding to the entry.</p>
    <p>After processing in step 908, formatted text data transmission processing is executed in steps 912 and 913 in FIG. 9C. Processing for ending communication with the mobile terminal 101 for which the final access time is earlier by a predetermined time or more is performed in steps 914 and 915, and the flow returns to determination processing in step 901 in FIG. 9A.</p>
    <p>Processing executed in steps 910 and 911 when YES in step 901 in FIG. 9A, and the data transferred from the packet transmission/reception section 115 in step 902 is associated with a text speech recognition/formatting end request command, i.e., YES in step 909 in FIG. 9B will be described next.</p>
    <p>In step 910, the contents of an entry of the processing terminal registration table (FIG. 10) which stores the same "terminal identification code" as that transferred from the packet transmission/reception section 115 in step 902 in FIG. 9A are deleted.</p>
    <p>In step 911, buffer files corresponding to the speech buffer file name, the text buffer file name, and formatted text buffer file name stored in the entry are deleted from the file system managed by the speech control host unit 108.</p>
    <p>After processing in step 911, formatted text data transmission processing is executed in steps 912 and 913 in FIG. 9C. Processing for ending communication with the mobile terminal 101 for which the final access time is earlier by a predetermined time or more is performed in steps 914 and 915, and the flow returns to determination processing in step 901 in FIG. 9A.</p>
    <p>Processing in steps 912 and 913 and subsequent processing in steps 914 and 915 in FIG. 9C performed when the packet transmission/reception section 115 has not notified of reception, i.e., NO in step 901 in FIG. 9A or after processing corresponding to reception of the command or speech data will be described.</p>
    <p>In these processing operations, formatted text data obtained from the formatted text generation section 118 is transmitted.</p>
    <p>It is determined in step 912 whether the processing terminal registration table (FIG. 10) has an entry in which formatted text data is present in a formatted text buffer file corresponding to the formatted text buffer file name.</p>
    <p>If such an entry is not present, i.e., NO in step 912, formatted text data transmission processing in step 913 is not executed, and the flow advances to processing in steps 914 and 915.</p>
    <p>If one or more entries as described above are present, i.e., YES in step 912, formatted text data in the formatted text buffer files corresponding to the formatted text buffer file names stored in these entries are transmitted to "transmission source IP addresses" stored in the entries in units of entries, and the transmitted formatted text data are deleted from the formatted text buffer files. The size of the formatted text buffer file in deletion is automatically adjusted by the file system managed by the speech control host unit 108.</p>
    <p>After processing in step 913 or if NO in step 912, processing in step 914 is executed. Of entries of the processing terminal registration table (FIG. 10), an entry for which the final access time is earlier than the current time by a predetermined time or more is detected, and all the contents of the entry are deleted.</p>
    <p>In step 915, buffer files corresponding to the speech buffer file name, the text buffer file name, and the formatted text buffer file name stored in the entry are deleted from the file system managed by the speech control host unit 108.</p>
    <p>After processing in step 915, the flow returns to determination processing in step 901 in FIG. 9A.</p>
    <p>Details of Operation of Text Speech Recognition Section 117</p>
    <p>FIG. 11 is a functional block diagram of the text speech recognition section 117.</p>
    <p>As described above, when speech data has been received in the speech buffer file specified from the entry, the text speech recognition section 117 executes text speech recognition for the speech data in units of entries of the processing terminal registration table shown in FIG. 10, and additionally writes resultant recognized speech text data in the text buffer file corresponding to the entry.</p>
    <p>Reading of speech data from the speech buffer file and writing of recognized speech text data in the text buffer file in units of entries are controlled by an input/output control section 1309 shown in FIG. 11. The control operation of the input/output control section 1309 will be described first. FIG. 12 is a flow chart showing the control operation executed by the input/output control section 1309. The input/output control section 1309 has a processor and a control program, and the operation flow is realized as an operation performed by the processor to execute the control program.</p>
    <p>It is determined in step 1401 whether the processing terminal registration table (FIG. 10) has an entry in which speech data is stored in the speech buffer file corresponding to the speech buffer file name.</p>
    <p>If such an entry is present, i.e., YES in step 1401, the "terminal identification code" stored in the entry and the speech data corresponding to the speech buffer file name stored in the entry are written in an input buffer queue 1301 shown in FIG. 11 in units of entries, and the speech data is deleted from the speech buffer file in step 1402.</p>
    <p>The input buffer queue 1301 has a function of sequentially supplying speech data which is being queued by the input buffer queue 1301 to a speech interval detection section 1302. A speech analysis section 1303, a phoneme recognition section 1304, a word recognition section 1306, and a text recognition section 1307 connected to the output of the speech interval detection section 1302 form a data processing pipeline and have a function of independently processing input data. The sections 1302 through 1307 can recognize the "terminal identification code" (the "terminal identification code" is input from the input buffer queue 1301) corresponding to the speech data which is currently being processed. Finally, a set of the "terminal identification code" and recognized speech text data is output from the text recognition section 1307 to an output buffer queue 1308.</p>
    <p>After processing in step 1402 or if NO in step 1401, it is determined in step 1403 whether the output buffer queue 1308 shown in FIG. 11 has obtained the set of the "terminal identification code" and the recognized speech text data.</p>
    <p>If such a set has been obtained, i.e., YES in step 1403, the recognized speech text data of the set in the output buffer queue 1308 is additionally written in the text buffer file corresponding to the text buffer file name stored in the entry of the processing terminal registration table, which corresponds to the "terminal identification code", in units of sets in the output buffer queue 1308 in step 1404.</p>
    <p>After processing in step 1404, or if NO in step 1403, determination processing in step 1401 is executed again.</p>
    <p>In the above-described way, the text speech recognition section 117 can efficiently execute text speech recognition processing for the speech data, which is requested from a plurality of mobile terminals 101, as an assembly line operation.</p>
    <p>The functions of the sections 1302 through 1307 for realizing text speech recognition processing will be described below. Each scheme to be described below can be realized by referring to, e.g., Furui, "Introduction to Electronics/information Engineering 2, Acoustic/phonetic Engineering", Chapter 14, Kindaikagaku-sha.</p>
    <p>The speech interval detection section 1302 detects the interval where speech data is present from the sample time series of speech data input from the input buffer queue 1301. More specifically, the speech interval detection section 1302 calculates the average power of predetermined samples (e.g., 32 to 256 samples of 8-kHz sampling data) and detects, as a speech interval, an interval where a state wherein the average power exceeds a predetermined threshold value continues a predetermined number of cycles or more. With this processing, erroneous recognition of text speech data in an interval where no speech data is present can be prevented.</p>
    <p>The speech analysis section 1303 analyzes the characteristic feature of the speech data output from the speech interval detection section 1302, thereby detecting a feature amount parameter vector. One of the following known analysis methods can be employed as a speech analysis method.</p>
    <p>(1) Each output from a band filter bank for receiving the speech data time series is smoothed, and each smoothed output is used as an element of the feature amount parameter vector.</p>
    <p>(2) Each short-time spectral component calculated by fast Fourier transform (FFT) is smoothed while receiving the speech data time series of predetermined continuous samples, and each smoothed component value is used as an element of the feature amount parameter vector.</p>
    <p>(3) A cepstrum coefficient group is calculated using cepstrum analysis while receiving the speech data time series of predetermined continuous samples, and the cepstrum coefficient group is used as an element of the feature amount parameter vector.</p>
    <p>(4) Not only the cepstrum coefficient group in (3) but also a Δ cepstrum (cepstrum differential coefficient) group for the cepstrum coefficient group is calculated and added as an element of the feature amount parameter vector.</p>
    <p>(5) An LPC (LSP) coefficient group is calculated by linear prediction analysis (LPC analysis, and more specifically, a line spectrum pair analysis: LSP analysis) while receiving the speech data time series of predetermined continuous samples and used as an element of the feature amount parameter vector.</p>
    <p>(6) An autocorrelation function is calculated by autocorrelation analysis while receiving the speech data time series of predetermined continuous samples, and a speech pitch fundamental frequency pattern detected on the basis of the autocorrelation function is added as an element of the feature amount parameter vector.</p>
    <p>The phoneme recognition section 1304 calculates the similarity (distance) between the feature amount parameter vector output from the speech analysis section 1303 at a predetermined frame period (in units of predetermined samples) and the standard pattern of the feature amount parameter vector of each phoneme stored in the phoneme standard pattern dictionary 1303, and outputs, as phoneme lattice data, a set of phonemes having high similarities obtained at a predetermined frame period together with the similarities. To prevent erroneous phoneme recognition, the phoneme recognition section 1304 outputs the resultant data in the form of phoneme lattice data in which phoneme candidates are listed in a table instead of determining a final phoneme at a predetermined frame period.</p>
    <p>The word recognition section 1306 receives the phoneme lattice data output from the phoneme recognition section 1304 at a predetermined frame period and outputs word lattice data in which word candidates are listed in a table at a predetermined frame period. One of the following known analysis methods can be employed as a word recognition method.</p>
    <p>(1) The word recognition section 1306 executes time normalization (DP matching or DTW: Dynamic Time Warping) for a phoneme lattice data time series across a plurality of frame periods, which is output from the phoneme recognition section 1304, and the total phoneme standard pattern series stored in the word dictionary, and outputs word lattice data. In this case as well, to prevent erroneous word recognition, the word recognition section 1306 outputs the resultant data in the form of word lattice data in which word candidates are listed in a table instead of determining a final word at a predetermined frame period.</p>
    <p>(2) The word recognition section 1306 models all words using HMM (Hidden Markov Model), inputs a phoneme lattice data time series across a plurality of frame periods, which is output from the phoneme recognition section 1304, to an HMM analysis section, and outputs words corresponding to a plurality of models as word lattice data containing word candidates in a descending order of the frequency of occurrence.</p>
    <p>Finally, as the first-stage processing, the text recognition section 1307 sequentially inputs word lattice data output from the word recognition section 1306 and calculates various clause likelihoods as clause lattice data in accordance with an intraclause grammar (word order rule) associated with the clause a structure of Japanese (or English). As the second-stage processing, the text recognition section 1307 analyzes the semantic modification between clauses in accordance with the intraclause grammar, determines recognized speech text data, and writes the recognized speech text data in the output buffer queue 1308 to be paired with the "terminal identification code" sequentially transmitted from the input buffer queue 1301.</p>
    <p>Details of Operation of Formatted Text Generation Section 118</p>
    <p>FIG. 13 is a functional block diagram of the formatted text generation section 118.</p>
    <p>As described above, when recognized speech text data has been received, from the text speech recognition section 117, in the text buffer file specified from an entry, the formatted text generation section 118 formats the recognized speech text data in units of entries of the processing terminal registration table shown in FIG. 10, and additionally writes resultant formatted text data in the formatted text buffer file corresponding to the entry.</p>
    <p>Reading of recognized speech text data from the text buffer file and writing of formatted text data in the formatted text buffer file in units of entries are controlled by an input/output control section 1508 shown in FIG. 13. The control operation of the input/output control section 1508 will be described first. FIG. 14 is a flow chart showing the control operation executed by the input/output control section 1508. The input/output control section 1508 has a processor and a control program, and the flow is realized as an operation performed by the processor to execute the control program. The same control operation as that of the input/output control section 1309 in the text speech recognition section 117, which is shown in FIG. 11, is realized.</p>
    <p>It is determined in step 1601 whether the processing terminal registration table (FIG. 10) has an entry in which recognized speech text data is stored in the text buffer file corresponding to the text buffer file name.</p>
    <p>If such an entry is present, i.e., YES in step 1601, the "terminal identification code" stored in the entry and recognized speech text data on the text buffer file corresponding to the text buffer file name stored in the entry are written in an input buffer queue 1501 shown in FIG. 13 in units of entries, and the recognized speech text data is deleted from the text buffer file in step 1602.</p>
    <p>The input buffer queue 1501 has a function of sequentially supplying recognized speech text data which is being queued by the input buffer queue 1501 to a field recognition section 1502. An unnecessary word deletion section 1504 and a formatted text data generation section 1506 connected to the output of the field recognition section 1502 form a data processing pipeline, as in the text speech recognition section 117 shown in FIG. 11, and have a function of independently processing input data. The sections 1502 through 1506 can recognize the "terminal identification code" (the "terminal identification code" is input from the input buffer queue 1501) corresponding to the recognized speech text data which is currently being processed. Finally, a set of the "terminal identification code" and formatted text data is output from the formatted text data generation section 1506 to an output buffer queue 1507.</p>
    <p>After processing in step 1602 or if NO in step 1601, it is determined in step 1603 whether the output buffer queue 1507 shown in FIG. 13 has obtained the set of the "terminal identification code" and the formatted text data.</p>
    <p>If such a set has been obtained, i.e., YES in step 1603, the formatted text data of the set in the output buffer queue 1507 is additionally written in the formatted text buffer file corresponding to the formatted text buffer file name stored in the entry of the processing terminal registration table, which corresponds to the "terminal identification code", in units of sets in the output buffer queue 1507 in step 1604.</p>
    <p>After processing in step 1604, or if NO in step 1603, determination processing in step 1601 is executed again.</p>
    <p>In the above-described way, like the text speech recognition section 117, the formatted text generation section 118 can efficiently format the recognized speech text data obtained by the text speech recognition section 117 on the basis of a request from a plurality of mobile terminals 101, as an assembly line operation.</p>
    <p>The functions of the sections 1502 through 1505 for realizing formatting will be described below.</p>
    <p>The field recognition section 1502 determines the format type stored in the entry of the processing terminal registration table in correspondence with the "terminal identification code" of the set for each set of the "terminal identification code" and the recognized speech text data sequentially input from the input buffer queue 1501, determines the field of the recognized speech text data of the set with reference to a format type field dictionary 1503, and outputs a set of field information, the "terminal identification code", and the recognized speech text data to the unnecessary word deletion section 1504.</p>
    <p>More specifically, the format type field dictionary 1503 stores a field name and a keyword corresponding to the field name in units of format types. The field recognition section 1502 designates a searching range to be referred to on the format type field dictionary 1503 in accordance with the format type obtained from the processing terminal registration table, searches for a field name for which a word contained in the recognized speech text data is registered as a keyword, and determines it as the field of the recognized speech text data.</p>
    <p>When the user of the mobile terminal 101 is to generate an E-mail, the user designates "E-mail" as format type data together with a text speech recognition/formatting start request command. Thereafter, the user sequentially pronounces, e.g., "the destination is taro@casio.co.jp", "the carbon copy is hanako@osuga.co.jp", or "the text is . . . " These pronounced contents are recognized as recognized speech text data by the text speech recognition section 117 in the speech control host unit 108. To generate FAX data, the user sequentially pronounces, e.g., "the destination number is 0425-79-7735, or "the text is . . . "</p>
    <p>Upon receiving, e.g., recognized speech text data "the destination is taro@casio.co.jp", the formatted text generation section 118 designates a searching range corresponding to the "E-mail" format on the format type field dictionary 1503 in accordance with format type data "E-mail". The formatted text generation section 118 searches for a field name "destination" for which a word "destination" contained in the recognized speech text data is registered as a keyword from the searching range, and determines it as the field of the recognized speech text data. Not only the keyword "destination (a Chinese character)" but also "destination (the cursive kana letters)", "destination address (a Chinese character+the Japanese syllabary)", "destination address (the cursive letters+the Japanese syllabary)", "partner (a Chinese character)", "destination (the cursive kana letters)", "partner address (a Chinese character+the Japanese syllabary)", "partner address (the cursive letters+the Japanese syllabary)" and the like are registered as keywords for the field name "destination" in the searching range of the format type field dictionary 1503. This arrangement can cope with various schema designated by the user for the "destination" field.</p>
    <p>This also applies to a case wherein the recognized speech text data is "the carbon copy is hanako@osuga.co.jp", "the text is . . . ", or "the destination number is 0425-79-7735".</p>
    <p>The same processing can be performed even when the format type is "address book", "schedule book", or "memo pad". For example, a keyword "address", "name", or "telephone number" is searched for from recognized speech text data.</p>
    <p>The unnecessary word deletion section 1504 refers to the unnecessary word dictionary 1505 for the set of the field information, the "terminal identification code", and the recognized speech text data output from the field recognition section 1502, thereby deleting unnecessary words "is" and the like. The resultant recognized speech text data is output to the formatted text data generation section 1506 together with the field information and the "terminal identification code".</p>
    <p>Consequently, the formatted text data generation section 1506 generates formatted text data on the basis of the received field information and recognized speech text data, and writes the formatted text data in the output buffer queue 1507 together with the received "terminal identification code". For example, when the format type is "E-mail", the field recognition section 1502 detects "destination" from the recognized speech text data "the destination is taro@casio.co.jp", the unnecessary word deletion section 1504 deletes unnecessary words, and it is determined that a field corresponding to the "destination" field is "taro@casio.co.jp". With this processing, a field such as "To: taro@casio.co.jp", "Cc: hanako@osuga.co.jp", or "text: . . . " is generated. When the format type is "FAX", a field such as "destination number: 0425-79-7735, or "text: . . . " is generated. When the format type is "address book", "schedule book", or "memo pad", a field such as "address: Shinjuku-ku Tokyo", "name: Yamada . . . " or "telephone: 03-123-4567" is generated. The generated field is inserted into a predetermined field of a predetermined text format such as "E-mail", "FAX", "address book", "schedule book", or "memo pad" to generate formatted text data.</p>
    <p>Operation of Mail Transmission/Reception Section 119</p>
    <p>FIG. 15 is a flow chart of the control operation executed by the mail transmission/reception section 119 in the speech control host unit 108. This flow chart is realized as an operation performed by a processor for controlling the mail transmission/reception section 119 (not shown) to execute a control program (not shown).</p>
    <p>It is determined in step 1701 whether the packet transmission/reception section 115 (FIG. 1) in the speech control host unit 108 has notified the mail transmission/reception section 119 of reception. As described above, the packet transmission/reception section 115 identifies that the IP address of the speech control host unit 108, i.e., the packet transmission/reception section 115 itself is set in the "destination IP address" field of the IP header of the IP datagram constituting the TCP/IP packet transferred from the Internet 105, thereby receiving the TCP/IP packet. The packet transmission/reception section 115 also confirms that the 16-bit integer value for specifying the mail transmission protocol or mail reception protocol is set in the "destination port number" field and the "transmission source port number" field of the TCP segment constituting the TCP/IP packet, thereby notifying the mail transmission/reception section 119 of the reception. This reception notification is associated with E-mail text data to be transmitted or a mail reception request command for a reception request.</p>
    <p>If the packet transmission/reception section 115 has notified the mail transmission/reception section 119 of reception, i.e., YES in step 1701, data transferred from the packet transmission/reception section 115 together with the reception notification are fetched in step 1702. When the reception notification is associated with E-mail text data to be transmitted, the "transmission source IP address" and the E-mail text data are fetched. When the reception notification is associated with a mail reception request command, the "transmission source IP address", the "terminal identification code", and the command are fetched.</p>
    <p>After processing in step 1702, steps 1703 and 1705 are sequentially checked, and one determination result becomes YES. More specifically, if the data transferred from the packet transmission/reception section 115 in step 1702 is associated with E-mail text data to be transmitted, i.e., YES in step 1703, mail transmission processing in step 1704 is executed. If the data is associated with a mail reception request command, i.e., YES in step 1705, received mail transfer processing in step 1706 is executed.</p>
    <p>If the packet transmission/reception section 115 has not notified the mail transmission/reception section 119 of reception, i.e., NO in step 1701, a wait state is set.</p>
    <p>Transmission processing in step 1704 performed when YES in step 1701, and data transferred from the packet transmission/reception section 115 in step 1702 is associated with E-mail text data to be transmitted, i.e., YES in step 1703 will be described.</p>
    <p>In step 1704, the mail transmission/reception section 119 inquires of a name solution server (host unit) (not shown) on the speech control host unit 108, the LAN 107, or the Internet 105 through the packet transmission/reception section 115 to convert the E-mail address set in the "To field" and "Cc field" of the E-mail text data fetched from the mobile terminal 101 through the packet transmission/reception section 115 into an IP address, and thereafter, requests the packet transmission/reception section 115 to transmit the E-mail text data to the IP address.</p>
    <p>The packet transmission/reception section 115 generates a TCP segment having the format shown in FIG. 6C. In this case, a 16-bit integer value for specifying a mail transmission protocol (e.g., SMTP) is set in the "transmission source port number" field and the "destination port number" field of the TCP header having the format shown in FIGS. 6C and 7B. A mail transmission command based on the mail transmission command and/or E-mail text data are stored in the "data" field of the TCP segment.</p>
    <p>Next, the packet transmission/reception section 115 generates an IP datagram having the format shown in FIG. 6B in which the TCP segment is stored in the "data" field. In this case, a 16-bit integer value for defining the format of the TCP segment data stored in the "data" field is set in the "protocol" field of the IP header having the format shown in FIGS. 6B and 7A. An IP address assigned to the speech control host unit 108 is set in the "transmission source IP address" field. An IP address corresponding to the "To field" and "Cc field" of the E-mail text data is set in the "destination IP address" field. When a plurality of "destination IP addresses" are present, a plurality of TCP/IP packets are copied and transmitted.</p>
    <p>The packet transmission/reception section 115 generates a frame according to the protocol on the LAN 107 and storing the IP datagram and sends the frame to the LAN 107. For example, if the LAN 107 is a local area network based on Ethernet, the frame is an Ethernet frame.</p>
    <p>The TCP/IP packet constituted by the frame, the IP datagram, and the TCP segment is transferred to the destination host unit on the basis of the "destination IP address" stored in the IP header of the IP datagram constituting the TCP/IP packet.</p>
    <p>Received mail transfer processing executed in step 1706 when YES in step 1701, and the data transferred from the packet transmission/reception section 115 in step 1702 is associated with a mail reception request command, i.e., YES in step 1705 will be described next.</p>
    <p>In step 1706, the mail transmission/reception section 119 requests the packet transmission/reception section 115 to extract E-mail text data which has been received for the mobile terminal 101 from a spool file corresponding to the "terminal identification code" fetched from the packet transmission/reception section 115 in step 1702 and transmit the E-mail text data to the mobile terminal 101.</p>
    <p>The packet transmission/reception section 115 generates a TCP segment having the format shown in FIG. 6C. In this case, a 16-bit integer value for specifying a mail reception protocol (e.g., POP3) is set in the "transmission source port number" field and the "destination port number" field of the TCP header having the format shown in FIGS. 6C and 7B. The E-mail text data extracted from the spool is stored in the "data" field of the TCP segment. Whether the contents of the spool are to be deleted is determined by user setting from the mobile terminal 101.</p>
    <p>The packet transmission/reception section 115 generates an IP datagram having the format shown in FIG. 6B in which the TCP segment is stored in the "data" field. In this case, an integer value of "6" for defining the format of the TCP segment data to be stored in the "data" field is set in the "protocol" field of the IP header having the format shown in FIGS. 6B and 7A. An IP address assigned to the speech control host unit 108 is set in the "transmission source IP address" field. A "transmission source IP address" fetched from the packet transmission/reception section 115 in step 1702 is set in the "destination IP address" field. This "transmission source IP address" is an address set in the TCP/IP packet which stores the mail reception request command and corresponding to the mobile terminal 101 which has transmitted the command.</p>
    <p>The packet transmission/reception section 115 generates a frame according to the protocol on the LAN 107 and storing the IP datagram and sends the frame to the LAN 107. For example, if the LAN 107 is a local area network based on Ethernet, the frame is an Ethernet frame.</p>
    <p>The TCP/IP packet constituted by the frame, the IP datagram, and the TCP segment is transferred to the mobile terminal control host unit 104 through the router unit 106 and the Internet 105 on the basis of the "destination IP address" stored in the IP header of the IP datagram constituting the TCP/IP packet, and then transferred to the communication control section 321 (FIG. 3) in the communication section 111 of the mobile terminal 101 through the PHS network 103 and the radio base station (or a wire connection unit) 102.</p>
    <p>Operation of FAX Transmission/Reception Section 120</p>
    <p>FIG. 16 is a flow chart showing the control operation executed by the FAX transmission/reception section 120 in the speech control host unit 108. This flow chart is realized as an operation performed by a processor (not shown) for controlling the FAX transmission/reception section 120 to execute a control program (not shown). This flow chart has the same function as that of the flow chart corresponding to the mail transmission/reception section 119 shown in FIG. 15 except in that not the Internet 105 but the telephone line 121 (FIG. 1) is used as a FAX text data transfer medium.</p>
    <p>It is determined in step 1801 whether the packet transmission/reception section 115 (FIG. 1) in the speech control host unit 108 has notified the FAX transmission/reception section 120 of reception. As described above, the packet transmission/reception section 115 identifies that the IP address of the speech control host unit 108, i.e., the packet transmission/reception section 115 itself is set in the "destination IP address" field of the IP header of the IP datagram constituting the TCP/IP packet transferred from the Internet 105, thereby receiving the TCP/IP packet. The packet transmission/reception section 115 also confirms that the 16-bit integer value for specifying the FAX communication protocol is set in the "destination port number" field and the "transmission source port number" field of the TCP segment constituting the TCP/IP packet, thereby notifying the FAX transmission/reception section 120 of reception. This reception notification is associated with FAX text data to be transmitted, or a FAX reception request command for a reception request.</p>
    <p>If the packet transmission/reception section 115 has notified the FAX transmission/reception section 120 of reception, i.e., YES in step 1801, data transferred from the packet transmission/reception section 115 together with the reception notification is fetched in step 1802. When the reception notification is associated with FAX text data to be transmitted, the "transmission source IP address" and the FAX text data are fetched. When the reception notification is associated with a FAX reception request command, the "transmission source IP address", the "terminal identification code", and the command are fetched.</p>
    <p>After processing in step 1802, steps 1803 and 1805 are sequentially checked, and one determination result becomes YES. More specifically, if the data A transferred from the packet transmission/reception section 115 in step 1802 is associated with FAX text data to be transmitted, i.e., YES in step 1803, mail transmission processing in step 1804 is executed. If the data is associated with a FAX reception request command, i.e., YES in step 1805, received mail transfer processing in step 1806 is executed.</p>
    <p>If the packet transmission/reception section 115 has not notified the FAX transmission/reception section 120 of reception, i.e., NO in step 1801, a wait state is set.</p>
    <p>Transmission processing in step 1804 which is performed when YES in step 1801, and data transferred from the packet transmission/reception section 115 in step 1802 is associated with FAX text data to be transmitted, i.e., YES in step 1803 will be described.</p>
    <p>In step 1804, the FAX transmission/reception section 120 dials, on the telephone line 121 (FIG. 1), the destination number set in the "destination number" field of the FAX text data fetched from the mobile terminal 101 through the packet transmission/reception section 115, thereby transmitting the FAX text data to the partner FAX apparaWhen a re the call has terminated. When a plurality of destination numbers are set in the destination number field, a plurality of FAX text data are copied and transmitted to the FAX apparatuses corresponding to the respective destination numbers.</p>
    <p>Received mail transfer processing executed in step 1806 when YES in step 1801, and the data transferred from the packet transmission/reception section 115 in step 1802 is associated with a FAX reception request command, i.e., YES in step 1805 will be described next.</p>
    <p>In step 1806, the FAX transmission/reception section 120 requests the packet transmission/reception section 115 to extract FAX text data which has been received for the mobile terminal 101 from a spool file corresponding to the "terminal identification code" fetched from the packet transmission/reception section 115 in step 1802 and transmit the FAX text data to the mobile terminal 101.</p>
    <p>The packet transmission/reception section 115 generates a TCP segment having the format shown in FIG. 6C. In this case, a 16-bit integer value for specifying the FAX communication protocol is set in the "transmission source port number" field and the "destination port number" field of the TCP header having the format shown in FIGS. 6C and 7B. The FAX text data extracted from the spool is stored in the "data" field of the TCP segment. Whether the contents of the spool are to be deleted is determined by user setting from the mobile terminal 101.</p>
    <p>Next, the packet transmission/reception section 115 generates an IP datagram having the format shown in FIG. 6B in which the TCP segment is stored in the "data" field. In this case, an integer value of "6" for defining the format of the TCP segment data to be stored in the "data" field is set in the "protocol" field of the IP header having the format shown in FIGS. 6B and 7A. An IP address assigned to the speech control host unit 108 is set in the "transmission source IP address" field. A "transmission source IP address" fetched from the packet transmission/reception section 115 in step 1802 is set in the "destination IP address" field. This "transmission source IP address" is an address set in the TCP/IP packet which stores the mail reception request command and corresponding to the mobile terminal 101 which has transmitted the command.</p>
    <p>The packet transmission/reception section 115 generates a frame according to the protocol on the LAN 107 and storing the IP datagram and sends the frame to the LAN 107. For example, if the LAN 107 is a local area network based on Ethernet, the frame is an Ethernet frame.</p>
    <p>The TCP/IP packet constituted by the frame, the IP datagram, and the TCP segment is transferred to the mobile terminal control host unit 104 through the router unit 106 and the Internet 105 on the basis of the "destination IP address" stored in the IP header of the IP datagram constituting the TCP/IP packet, and then transferred to the communication control section 321 (FIG. 3) in the communication section 111 of the mobile terminal 101 through the PHS network 103 and the radio base station (or a wire connection unit) 102.</p>
    <p>In case of "address book", "schedule book", or "memo pad", generated formatted text data is transmitted to the mobile terminal 101.</p>
    <p>Other Embodiments</p>
    <p>In the above-described embodiments, the mobile terminal 101 is a PHS terminal, and the mobile terminal 101 and the speech control host unit 108 are connected through the PHS network 103 and the Internet 105. However, the present invention is not limited to this embodiment. As far as the mobile terminal 101 is indirectly or directly connected to the speech control host unit 108 by radio or wire, the present invention can be applied.</p>
    <p>In inputting, e.g., an E-mail address or a FAX destination number, an address database may be formed in the formatted text generation section 118 of the speech control host unit 108 in advance. When a name or the like is pronounced on the mobile terminal 101 side, the name or the like may be confirmed, and the address database may be referred to, thereby converting the name or the like into an E-mail address or a FAX destination number and generating E-mail text data or FAX text data.</p>
    <p>In the above embodiment, E-mail text data or FAX text data generated by the speech control host unit 108 is transmitted to the mobile terminal 101, edited on the mobile terminal 101 side, and transmitted to the mail or FAX destination. However, the E-mail text data or FAX text data may be transmitted to the mail or FAX destination immediately after it is generated by the speech control host unit 108.</p>
    <p>In the above embodiment, the speech control host unit 108 generates formatted text data. However, a keyword may be searched for at least from recognized speech text data and transmitted to the mobile terminal 101.</p>
    <p>Additional advantages and modifications will readily occur to those skilled in the art. Therefore, the invention in its broader aspects is not limited to the specific details and representative embodiments shown and described herein. Accordingly, various modifications may be made without departing from the spirit or scope of the general inventive concept as defined by the appended claims and their equivalents.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4712243">US4712243</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 8, 1984</td><td class="patent-data-table-td patent-date-value">Dec 8, 1987</td><td class="patent-data-table-td ">Casio Computer Co., Ltd.</td><td class="patent-data-table-td ">Speech recognition apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5128985">US5128985</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 30, 1990</td><td class="patent-data-table-td patent-date-value">Jul 7, 1992</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Communication system having a plurality of voice communication terminals and a data communication terminal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5163111">US5163111</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 14, 1990</td><td class="patent-data-table-td patent-date-value">Nov 10, 1992</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Customized personal terminal device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5182765">US5182765</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 24, 1992</td><td class="patent-data-table-td patent-date-value">Jan 26, 1993</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Speech recognition system with an accurate recognition function</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5280520">US5280520</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 30, 1992</td><td class="patent-data-table-td patent-date-value">Jan 18, 1994</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Apparatus for speed and data communication</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5465326">US5465326</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 22, 1994</td><td class="patent-data-table-td patent-date-value">Nov 7, 1995</td><td class="patent-data-table-td ">Ricoh Company, Ltd.</td><td class="patent-data-table-td ">Mixed-mode transmission control apparatus for adding an identification block to mixed-mode data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5577165">US5577165</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 26, 1994</td><td class="patent-data-table-td patent-date-value">Nov 19, 1996</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Speech dialogue system for facilitating improved human-computer interaction</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5625675">US5625675</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 11, 1995</td><td class="patent-data-table-td patent-date-value">Apr 29, 1997</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Voice mail communication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5632002">US5632002</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 28, 1993</td><td class="patent-data-table-td patent-date-value">May 20, 1997</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Speech recognition interface system suitable for window systems and speech mail systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5758332">US5758332</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 2, 1995</td><td class="patent-data-table-td patent-date-value">May 26, 1998</td><td class="patent-data-table-td ">Casio Computer Co., Ltd.</td><td class="patent-data-table-td ">Information service providing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0405029A1?cl=en">EP0405029A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 29, 1989</td><td class="patent-data-table-td patent-date-value">Jan 2, 1991</td><td class="patent-data-table-td ">Jerome Hal Lemelson</td><td class="patent-data-table-td ">Speech communication system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="http://www.google.com/url?id=CExMBAABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3DH07222248A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNGLDIxRfGazez7sg5brARxki2vjCQ">JPH07222248A</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">Title not available</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="http://www.google.com/url?id=CExMBAABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3DS6046647A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNE6Kt3sPr1aMbMdQP0eOMpr4Xtn9A">JPS6046647A</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">Title not available</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Furui, "<a href='http://scholar.google.com/scholar?q="Introduction+to+Electronics%2FInformation+Engineering+2%2C+Acoustic%2FPhonetic+Engineering"'>Introduction to Electronics/Information Engineering 2, Acoustic/Phonetic Engineering</a>", Chapter 14, Kindaikagaku-sha, pp. 175-211.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Furui, Introduction to Electronics/Information Engineering 2, Acoustic/Phonetic Engineering , Chapter 14, Kindaikagaku sha, pp. 175 211.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Patent Abstracts Of Japan, vol. 009, No. 173 (E 329), Jul. 18, 1995 &amp; JP 60 046647 A (Fujitsu KK), Mar. 13, 1985.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Patent Abstracts Of Japan, vol. 009, No. 173 (E-329), Jul. 18, 1995 &amp; JP 60 046647 A (Fujitsu KK), Mar. 13, 1985.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Patent Abstracts of Japan, vol. 095, No. 011, Dec. 26, 1995 &amp; JP 07 222248 (Hitachi Ltd.), Aug. 18, 1995.</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6173259">US6173259</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 27, 1998</td><td class="patent-data-table-td patent-date-value">Jan 9, 2001</td><td class="patent-data-table-td ">Speech Machines Plc</td><td class="patent-data-table-td ">Speech to text conversion</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6389463">US6389463</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 16, 1999</td><td class="patent-data-table-td patent-date-value">May 14, 2002</td><td class="patent-data-table-td ">Im Networks, Inc.</td><td class="patent-data-table-td ">Internet radio receiver having a rotary knob for selecting audio content provider designations and negotiating internet access to URLS associated with the designations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6415256">US6415256</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 27, 2000</td><td class="patent-data-table-td patent-date-value">Jul 2, 2002</td><td class="patent-data-table-td ">Richard Joseph Ditzik</td><td class="patent-data-table-td ">Integrated handwriting and speed recognition systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6453294">US6453294</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 31, 2000</td><td class="patent-data-table-td patent-date-value">Sep 17, 2002</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Dynamic destination-determined multimedia avatars for interactive on-line communications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6490550">US6490550</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 30, 1998</td><td class="patent-data-table-td patent-date-value">Dec 3, 2002</td><td class="patent-data-table-td ">Ericsson Inc.</td><td class="patent-data-table-td ">System and method for IP-based communication transmitting speech and speech-generated text</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6510453">US6510453</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 26, 1999</td><td class="patent-data-table-td patent-date-value">Jan 21, 2003</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">System and method for creating and inserting multiple data fragments into an electronic mail message</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6519051">US6519051</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 6, 2000</td><td class="patent-data-table-td patent-date-value">Feb 11, 2003</td><td class="patent-data-table-td ">Shinestar Llc</td><td class="patent-data-table-td ">Fax through data network and remote access network appliance control apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6545978">US6545978</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 7, 1998</td><td class="patent-data-table-td patent-date-value">Apr 8, 2003</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Network managing method and system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6584179">US6584179</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 24, 1997</td><td class="patent-data-table-td patent-date-value">Jun 24, 2003</td><td class="patent-data-table-td ">Bell Canada</td><td class="patent-data-table-td ">Method and apparatus for improving the utility of speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6590877">US6590877</a></td><td class="patent-data-table-td patent-date-value">Dec 23, 1998</td><td class="patent-data-table-td patent-date-value">Jul 8, 2003</td><td class="patent-data-table-td ">Casio Computer Co., Ltd.</td><td class="patent-data-table-td ">Data transmission device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6600497">US6600497</a></td><td class="patent-data-table-td patent-date-value">Nov 15, 1999</td><td class="patent-data-table-td patent-date-value">Jul 29, 2003</td><td class="patent-data-table-td ">Elliot A. Gottfurcht</td><td class="patent-data-table-td ">Apparatus and method to navigate interactive television using unique inputs with a remote control</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6618749">US6618749</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 18, 1998</td><td class="patent-data-table-td patent-date-value">Sep 9, 2003</td><td class="patent-data-table-td ">Panasonic Communications Co., Ltd.</td><td class="patent-data-table-td ">Internet facsimile and e-mail reception method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6629107">US6629107</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 22, 2000</td><td class="patent-data-table-td patent-date-value">Sep 30, 2003</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Multimedia information collection control apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6651042">US6651042</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 2000</td><td class="patent-data-table-td patent-date-value">Nov 18, 2003</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and method for automatic voice message processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6681208">US6681208</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 25, 2001</td><td class="patent-data-table-td patent-date-value">Jan 20, 2004</td><td class="patent-data-table-td ">Motorola, Inc.</td><td class="patent-data-table-td ">Text-to-speech native coding in a communication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6684249">US6684249</a></td><td class="patent-data-table-td patent-date-value">May 26, 2000</td><td class="patent-data-table-td patent-date-value">Jan 27, 2004</td><td class="patent-data-table-td ">Sonicbox, Inc.</td><td class="patent-data-table-td ">Method and system for adding advertisements over streaming audio based upon a user profile over a world wide area network of computers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6687671">US6687671</a></td><td class="patent-data-table-td patent-date-value">Mar 13, 2001</td><td class="patent-data-table-td patent-date-value">Feb 3, 2004</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Method and apparatus for automatic collection and summarization of meeting information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6751589">US6751589</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 18, 2000</td><td class="patent-data-table-td patent-date-value">Jun 15, 2004</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">Voice-actuated generation of documents containing photographic identification</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6769028">US6769028</a></td><td class="patent-data-table-td patent-date-value">May 26, 2000</td><td class="patent-data-table-td patent-date-value">Jul 27, 2004</td><td class="patent-data-table-td ">Sonicbox, Inc.</td><td class="patent-data-table-td ">Method and apparatus for sharing streaming media links</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6813601">US6813601</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 11, 1998</td><td class="patent-data-table-td patent-date-value">Nov 2, 2004</td><td class="patent-data-table-td ">Loral Spacecom Corp.</td><td class="patent-data-table-td ">Highly compressed voice and data transmission system and method for mobile communications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6823225">US6823225</a></td><td class="patent-data-table-td patent-date-value">Dec 4, 1997</td><td class="patent-data-table-td patent-date-value">Nov 23, 2004</td><td class="patent-data-table-td ">Im Networks, Inc.</td><td class="patent-data-table-td ">Apparatus for distributing and playing audio information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6865187">US6865187</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 30, 1999</td><td class="patent-data-table-td patent-date-value">Mar 8, 2005</td><td class="patent-data-table-td ">Cisco Technology, Inc.</td><td class="patent-data-table-td ">Method and apparatus using transition state protocol signaling for fax transport manner capability exchange</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6907006">US6907006</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 1, 1999</td><td class="patent-data-table-td patent-date-value">Jun 14, 2005</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Method and apparatus for detecting faults in IP packet communication</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6917917">US6917917</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 30, 2000</td><td class="patent-data-table-td patent-date-value">Jul 12, 2005</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd</td><td class="patent-data-table-td ">Apparatus and method for voice recognition and displaying of characters in mobile telecommunication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6920479">US6920479</a></td><td class="patent-data-table-td patent-date-value">May 17, 2001</td><td class="patent-data-table-td patent-date-value">Jul 19, 2005</td><td class="patent-data-table-td ">Im Networks, Inc.</td><td class="patent-data-table-td ">Internet radio receiver with linear tuning interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6931463">US6931463</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 11, 2001</td><td class="patent-data-table-td patent-date-value">Aug 16, 2005</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Portable companion device only functioning when a wireless link established between the companion device and an electronic device and providing processed data to the electronic device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6950987">US6950987</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 9, 2002</td><td class="patent-data-table-td patent-date-value">Sep 27, 2005</td><td class="patent-data-table-td ">Simdesk Technologies, Inc.</td><td class="patent-data-table-td ">Remote document management system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6956662">US6956662</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 4, 1999</td><td class="patent-data-table-td patent-date-value">Oct 18, 2005</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Image workflow system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6970535">US6970535</a></td><td class="patent-data-table-td patent-date-value">Apr 25, 2002</td><td class="patent-data-table-td patent-date-value">Nov 29, 2005</td><td class="patent-data-table-td ">Envoy Worldwide, Inc.</td><td class="patent-data-table-td ">Wireless messaging system to multiple recipients</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6975993">US6975993</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 2, 2000</td><td class="patent-data-table-td patent-date-value">Dec 13, 2005</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">System, a server for a system and a machine for use in a system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6999565">US6999565</a></td><td class="patent-data-table-td patent-date-value">Feb 1, 2000</td><td class="patent-data-table-td patent-date-value">Feb 14, 2006</td><td class="patent-data-table-td ">Envoyworldwide, Inc.</td><td class="patent-data-table-td ">Multi-mode message routing and management</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7020845">US7020845</a></td><td class="patent-data-table-td patent-date-value">Mar 3, 2000</td><td class="patent-data-table-td patent-date-value">Mar 28, 2006</td><td class="patent-data-table-td ">Gottfurcht Elliot A</td><td class="patent-data-table-td ">Navigating internet content on a television using a simplified interface and a remote control</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7047195">US7047195</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 26, 2005</td><td class="patent-data-table-td patent-date-value">May 16, 2006</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Speech translation device and computer readable medium</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7050974">US7050974</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 13, 2000</td><td class="patent-data-table-td patent-date-value">May 23, 2006</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Environment adaptation for speech recognition in a speech communication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7069302">US7069302</a></td><td class="patent-data-table-td patent-date-value">Jul 14, 2003</td><td class="patent-data-table-td patent-date-value">Jun 27, 2006</td><td class="patent-data-table-td ">Panasonic Communications Co., Ltd.</td><td class="patent-data-table-td ">Internet facsimile and E-mail reception method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7099894">US7099894</a></td><td class="patent-data-table-td patent-date-value">Aug 6, 2003</td><td class="patent-data-table-td patent-date-value">Aug 29, 2006</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Multimedia information collection control apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7162414">US7162414</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 7, 2001</td><td class="patent-data-table-td patent-date-value">Jan 9, 2007</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Method and apparatus to perform speech recognition over a data channel</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7178102">US7178102</a></td><td class="patent-data-table-td patent-date-value">Dec 9, 2003</td><td class="patent-data-table-td patent-date-value">Feb 13, 2007</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Representing latent data in an extensible markup language document</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7184418">US7184418</a></td><td class="patent-data-table-td patent-date-value">Oct 20, 2000</td><td class="patent-data-table-td patent-date-value">Feb 27, 2007</td><td class="patent-data-table-td ">Telcordia Technologies, Inc.</td><td class="patent-data-table-td ">Method and system for host mobility management protocol</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7281245">US7281245</a></td><td class="patent-data-table-td patent-date-value">Jun 5, 2002</td><td class="patent-data-table-td patent-date-value">Oct 9, 2007</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Mechanism for downloading software components from a remote source for use by a local software application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7325194">US7325194</a></td><td class="patent-data-table-td patent-date-value">May 7, 2002</td><td class="patent-data-table-td patent-date-value">Jan 29, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method, system, and apparatus for converting numbers between measurement systems based upon semantically labeled strings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7346496">US7346496</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 8, 2007</td><td class="patent-data-table-td patent-date-value">Mar 18, 2008</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Method and apparatus to perform speech recognition over a data channel</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7346509">US7346509</a></td><td class="patent-data-table-td patent-date-value">Sep 26, 2003</td><td class="patent-data-table-td patent-date-value">Mar 18, 2008</td><td class="patent-data-table-td ">Callminer, Inc.</td><td class="patent-data-table-td ">Software for statistical analysis of speech</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7356470">US7356470</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 18, 2005</td><td class="patent-data-table-td patent-date-value">Apr 8, 2008</td><td class="patent-data-table-td ">Adam Roth</td><td class="patent-data-table-td ">Text-to-speech and image generation of multimedia attachments to e-mail</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7356537">US7356537</a></td><td class="patent-data-table-td patent-date-value">Jun 6, 2002</td><td class="patent-data-table-td patent-date-value">Apr 8, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Providing contextually sensitive tools and help content in computer-generated documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7392479">US7392479</a></td><td class="patent-data-table-td patent-date-value">Jun 27, 2002</td><td class="patent-data-table-td patent-date-value">Jun 24, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">System and method for providing namespace related information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7400618">US7400618</a></td><td class="patent-data-table-td patent-date-value">Dec 28, 2004</td><td class="patent-data-table-td patent-date-value">Jul 15, 2008</td><td class="patent-data-table-td ">Cisco Technology, Inc.</td><td class="patent-data-table-td ">Method and apparatus using transition state protocol signaling for fax transport manner capability exchange</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7404195">US7404195</a></td><td class="patent-data-table-td patent-date-value">Dec 9, 2003</td><td class="patent-data-table-td patent-date-value">Jul 22, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Programmable object model for extensible markup language markup in an application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7409229">US7409229</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 9, 2004</td><td class="patent-data-table-td patent-date-value">Aug 5, 2008</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd</td><td class="patent-data-table-td ">Mobile communication terminal and method for inputting characters by speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7412392">US7412392</a></td><td class="patent-data-table-td patent-date-value">Apr 14, 2003</td><td class="patent-data-table-td patent-date-value">Aug 12, 2008</td><td class="patent-data-table-td ">Sprint Communications Company L.P.</td><td class="patent-data-table-td ">Conference multi-tasking system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7421645">US7421645</a></td><td class="patent-data-table-td patent-date-value">Apr 24, 2001</td><td class="patent-data-table-td patent-date-value">Sep 2, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for providing electronic commerce actions based on semantically labeled strings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7434157">US7434157</a></td><td class="patent-data-table-td patent-date-value">Dec 9, 2003</td><td class="patent-data-table-td patent-date-value">Oct 7, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Programmable object model for namespace or schema library support in a software application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7437287">US7437287</a></td><td class="patent-data-table-td patent-date-value">Nov 1, 2004</td><td class="patent-data-table-td patent-date-value">Oct 14, 2008</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd</td><td class="patent-data-table-td ">Apparatus and method for voice recognition and displaying of characters in mobile telecommunication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7441196">US7441196</a></td><td class="patent-data-table-td patent-date-value">Mar 13, 2006</td><td class="patent-data-table-td patent-date-value">Oct 21, 2008</td><td class="patent-data-table-td ">Elliot Gottfurcht</td><td class="patent-data-table-td ">Apparatus and method of manipulating a region on a wireless device screen for viewing, zooming and scrolling internet content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7487515">US7487515</a></td><td class="patent-data-table-td patent-date-value">Dec 9, 2003</td><td class="patent-data-table-td patent-date-value">Feb 3, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Programmable object model for extensible markup language schema validation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7509573">US7509573</a></td><td class="patent-data-table-td patent-date-value">Feb 17, 2004</td><td class="patent-data-table-td patent-date-value">Mar 24, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Anti-virus security information in an extensible markup language document</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7518998">US7518998</a></td><td class="patent-data-table-td patent-date-value">Apr 27, 2005</td><td class="patent-data-table-td patent-date-value">Apr 14, 2009</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">IP packet communication apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7558841">US7558841</a></td><td class="patent-data-table-td patent-date-value">May 14, 2003</td><td class="patent-data-table-td patent-date-value">Jul 7, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method, system, and computer-readable medium for communicating results to a data query in a computer network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7584291">US7584291</a></td><td class="patent-data-table-td patent-date-value">Jan 21, 2005</td><td class="patent-data-table-td patent-date-value">Sep 1, 2009</td><td class="patent-data-table-td ">Mosi Media, Llc</td><td class="patent-data-table-td ">System and method for limiting dead air time in internet streaming media delivery</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7609821">US7609821</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 2006</td><td class="patent-data-table-td patent-date-value">Oct 27, 2009</td><td class="patent-data-table-td ">Varolii Corporation</td><td class="patent-data-table-td ">Multi-mode message routing and management</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7631088">US7631088</a></td><td class="patent-data-table-td patent-date-value">Feb 27, 2001</td><td class="patent-data-table-td patent-date-value">Dec 8, 2009</td><td class="patent-data-table-td ">Jonathan Logan</td><td class="patent-data-table-td ">System and method for minimizing perceived dead air time in internet streaming media delivery</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7644155">US7644155</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 21, 2006</td><td class="patent-data-table-td patent-date-value">Jan 5, 2010</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Data communication system and data communication method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7676372">US7676372</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 16, 2000</td><td class="patent-data-table-td patent-date-value">Mar 9, 2010</td><td class="patent-data-table-td ">Yugen Kaisha Gm&amp;M</td><td class="patent-data-table-td ">Prosthetic hearing device that transforms a detected speech into a speech of a speech form assistive in understanding the semantic meaning in the detected speech</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7693508">US7693508</a></td><td class="patent-data-table-td patent-date-value">Aug 20, 2001</td><td class="patent-data-table-td patent-date-value">Apr 6, 2010</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Method and apparatus for broadcast signaling in a wireless communication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7707024">US7707024</a></td><td class="patent-data-table-td patent-date-value">May 23, 2002</td><td class="patent-data-table-td patent-date-value">Apr 27, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method, system, and apparatus for converting currency values based upon semantically labeled strings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7707496">US7707496</a></td><td class="patent-data-table-td patent-date-value">May 9, 2002</td><td class="patent-data-table-td patent-date-value">Apr 27, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method, system, and apparatus for converting dates between calendars and languages based upon semantically labeled strings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7711550">US7711550</a></td><td class="patent-data-table-td patent-date-value">Apr 29, 2003</td><td class="patent-data-table-td patent-date-value">May 4, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Methods and system for recognizing names in a computer-generated document and for providing helpful actions associated with recognized names</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7712024">US7712024</a></td><td class="patent-data-table-td patent-date-value">Jul 16, 2001</td><td class="patent-data-table-td patent-date-value">May 4, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Application program interfaces for semantically labeling strings and providing actions based on semantically labeled strings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7716163">US7716163</a></td><td class="patent-data-table-td patent-date-value">Jul 17, 2001</td><td class="patent-data-table-td patent-date-value">May 11, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for defining semantic categories and actions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7716676">US7716676</a></td><td class="patent-data-table-td patent-date-value">Jun 25, 2002</td><td class="patent-data-table-td patent-date-value">May 11, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">System and method for issuing a message to a program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7739588">US7739588</a></td><td class="patent-data-table-td patent-date-value">Jun 27, 2003</td><td class="patent-data-table-td patent-date-value">Jun 15, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Leveraging markup language data for semantically labeling text strings and data and for providing actions based on semantically labeled text strings and data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7742048">US7742048</a></td><td class="patent-data-table-td patent-date-value">May 23, 2002</td><td class="patent-data-table-td patent-date-value">Jun 22, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method, system, and apparatus for converting numbers based upon semantically labeled strings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7770102">US7770102</a></td><td class="patent-data-table-td patent-date-value">Jun 6, 2000</td><td class="patent-data-table-td patent-date-value">Aug 3, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for semantically labeling strings and providing actions based on semantically labeled strings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7778816">US7778816</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 24, 2001</td><td class="patent-data-table-td patent-date-value">Aug 17, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for applying input mode bias</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7783614">US7783614</a></td><td class="patent-data-table-td patent-date-value">Feb 13, 2003</td><td class="patent-data-table-td patent-date-value">Aug 24, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Linking elements of a document to corresponding fields, queries and/or procedures in a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7788590">US7788590</a></td><td class="patent-data-table-td patent-date-value">Sep 26, 2005</td><td class="patent-data-table-td patent-date-value">Aug 31, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Lightweight reference user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7788602">US7788602</a></td><td class="patent-data-table-td patent-date-value">Jul 16, 2001</td><td class="patent-data-table-td patent-date-value">Aug 31, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for providing restricted actions for recognized semantic categories</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7827546">US7827546</a></td><td class="patent-data-table-td patent-date-value">Dec 9, 2003</td><td class="patent-data-table-td patent-date-value">Nov 2, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Mechanism for downloading software components from a remote source for use by a local software application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7847964">US7847964</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 21, 2005</td><td class="patent-data-table-td patent-date-value">Dec 7, 2010</td><td class="patent-data-table-td ">Celery, Llc</td><td class="patent-data-table-td ">Document delivery system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7877254">US7877254</a></td><td class="patent-data-table-td patent-date-value">Mar 28, 2007</td><td class="patent-data-table-td patent-date-value">Jan 25, 2011</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Method and apparatus for enrollment and verification of speaker authentication</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7949707">US7949707</a></td><td class="patent-data-table-td patent-date-value">Jul 12, 2005</td><td class="patent-data-table-td patent-date-value">May 24, 2011</td><td class="patent-data-table-td ">Mosi Media, Llc</td><td class="patent-data-table-td ">Internet radio receiver with linear tuning interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7992085">US7992085</a></td><td class="patent-data-table-td patent-date-value">May 15, 2007</td><td class="patent-data-table-td patent-date-value">Aug 2, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Lightweight reference user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7996412">US7996412</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 3, 2007</td><td class="patent-data-table-td patent-date-value">Aug 9, 2011</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Schedule information management method and system using digital living network alliance network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7996475">US7996475</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 3, 2008</td><td class="patent-data-table-td patent-date-value">Aug 9, 2011</td><td class="patent-data-table-td ">Barracuda Networks Inc</td><td class="patent-data-table-td ">Facilitating transmission of email by checking email parameters with a database of well behaved senders</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8077679">US8077679</a></td><td class="patent-data-table-td patent-date-value">Oct 24, 2001</td><td class="patent-data-table-td patent-date-value">Dec 13, 2011</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Method and apparatus for providing protocol options in a wireless communication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8098818">US8098818</a></td><td class="patent-data-table-td patent-date-value">Jul 7, 2003</td><td class="patent-data-table-td patent-date-value">Jan 17, 2012</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Secure registration for a multicast-broadcast-multimedia system (MBMS)</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8102856">US8102856</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 10, 2010</td><td class="patent-data-table-td patent-date-value">Jan 24, 2012</td><td class="patent-data-table-td ">Huawei Technologies Co., Ltd.</td><td class="patent-data-table-td ">Method of implementing traversal of multimedia protocols through network address translation device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8121296">US8121296</a></td><td class="patent-data-table-td patent-date-value">Aug 20, 2001</td><td class="patent-data-table-td patent-date-value">Feb 21, 2012</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Method and apparatus for security in a data processing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8144837">US8144837</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 28, 2001</td><td class="patent-data-table-td patent-date-value">Mar 27, 2012</td><td class="patent-data-table-td ">Dialogic Corporation</td><td class="patent-data-table-td ">Method and system for enhanced user experience of audio</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8180644">US8180644</a></td><td class="patent-data-table-td patent-date-value">Sep 17, 2008</td><td class="patent-data-table-td patent-date-value">May 15, 2012</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Method and apparatus for scrolling text display of voice call or message during video display session</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8213460">US8213460</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 7, 2000</td><td class="patent-data-table-td patent-date-value">Jul 3, 2012</td><td class="patent-data-table-td ">Cisco Technology, Inc.</td><td class="patent-data-table-td ">Method and system for processing traffic in an access network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8280015">US8280015</a></td><td class="patent-data-table-td patent-date-value">Apr 6, 2006</td><td class="patent-data-table-td patent-date-value">Oct 2, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Providing contextual information with a voicemail message</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8370141">US8370141</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 3, 2006</td><td class="patent-data-table-td patent-date-value">Feb 5, 2013</td><td class="patent-data-table-td ">Reagan Inventions, Llc</td><td class="patent-data-table-td ">Device, system and method for enabling speech recognition on a portable data device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8380515">US8380515</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 13, 2012</td><td class="patent-data-table-td patent-date-value">Feb 19, 2013</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Method and apparatus for scrolling text display of voice call or message during video display session</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8406794">US8406794</a></td><td class="patent-data-table-td patent-date-value">Apr 25, 2007</td><td class="patent-data-table-td patent-date-value">Mar 26, 2013</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Methods and apparatuses of initiating communication in wireless networks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8407052">US8407052</a></td><td class="patent-data-table-td patent-date-value">Apr 17, 2007</td><td class="patent-data-table-td patent-date-value">Mar 26, 2013</td><td class="patent-data-table-td ">Vovision, Llc</td><td class="patent-data-table-td ">Methods and systems for correcting transcribed audio files</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8452653">US8452653</a></td><td class="patent-data-table-td patent-date-value">Jun 30, 2003</td><td class="patent-data-table-td patent-date-value">May 28, 2013</td><td class="patent-data-table-td ">Personal Data Network Corporation</td><td class="patent-data-table-td ">Method for providing information and recommendations based on user activity</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8472430">US8472430</a></td><td class="patent-data-table-td patent-date-value">Apr 3, 2006</td><td class="patent-data-table-td patent-date-value">Jun 25, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">VoIP packet prioritization</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8483368">US8483368</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 2012</td><td class="patent-data-table-td patent-date-value">Jul 9, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Providing contextual information with a voicemail message</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8515728">US8515728</a></td><td class="patent-data-table-td patent-date-value">Mar 29, 2007</td><td class="patent-data-table-td patent-date-value">Aug 20, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Language translation of visual and audio input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8516047">US8516047</a></td><td class="patent-data-table-td patent-date-value">Jan 16, 2007</td><td class="patent-data-table-td patent-date-value">Aug 20, 2013</td><td class="patent-data-table-td ">Rick Castanho</td><td class="patent-data-table-td ">System and method for service specific notification</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8583434">US8583434</a></td><td class="patent-data-table-td patent-date-value">Jan 29, 2008</td><td class="patent-data-table-td patent-date-value">Nov 12, 2013</td><td class="patent-data-table-td ">Callminer, Inc.</td><td class="patent-data-table-td ">Methods for statistical analysis of speech</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8600373">US8600373</a></td><td class="patent-data-table-td patent-date-value">Apr 26, 2007</td><td class="patent-data-table-td patent-date-value">Dec 3, 2013</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Dynamic distribution of device functionality and resource management</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8605728">US8605728</a></td><td class="patent-data-table-td patent-date-value">Aug 18, 2011</td><td class="patent-data-table-td patent-date-value">Dec 10, 2013</td><td class="patent-data-table-td ">Huawei Technologies Co., Ltd.</td><td class="patent-data-table-td ">Method of implementing traversal of multimedia protocols through network address translation device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8620938">US8620938</a></td><td class="patent-data-table-td patent-date-value">Feb 23, 2007</td><td class="patent-data-table-td patent-date-value">Dec 31, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method, system, and apparatus for routing a query to one or more providers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8644396">US8644396</a></td><td class="patent-data-table-td patent-date-value">Apr 17, 2007</td><td class="patent-data-table-td patent-date-value">Feb 4, 2014</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Waveform encoding for wireless applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8645121">US8645121</a></td><td class="patent-data-table-td patent-date-value">Dec 28, 2012</td><td class="patent-data-table-td patent-date-value">Feb 4, 2014</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Language translation of visual and audio input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8654868">US8654868</a></td><td class="patent-data-table-td patent-date-value">Apr 17, 2007</td><td class="patent-data-table-td patent-date-value">Feb 18, 2014</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Offloaded processing for wireless applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8706708">US8706708</a></td><td class="patent-data-table-td patent-date-value">Oct 26, 2007</td><td class="patent-data-table-td patent-date-value">Apr 22, 2014</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Providing contextually sensitive tools and help content in computer-generated documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8713400">US8713400</a></td><td class="patent-data-table-td patent-date-value">Jan 6, 2010</td><td class="patent-data-table-td patent-date-value">Apr 29, 2014</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Method and system for reduction of decoding complexity in a communication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8718279">US8718279</a></td><td class="patent-data-table-td patent-date-value">Jun 16, 2004</td><td class="patent-data-table-td patent-date-value">May 6, 2014</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Apparatus and method for a secure broadcast system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8724803">US8724803</a></td><td class="patent-data-table-td patent-date-value">Sep 1, 2004</td><td class="patent-data-table-td patent-date-value">May 13, 2014</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Method and apparatus for providing authenticated challenges for broadcast-multicast communications in a communication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8730999">US8730999</a></td><td class="patent-data-table-td patent-date-value">Jun 18, 2010</td><td class="patent-data-table-td patent-date-value">May 20, 2014</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Method and system for reduction of decoding complexity in a communication system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020097842">US20020097842</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 28, 2001</td><td class="patent-data-table-td patent-date-value">Jul 25, 2002</td><td class="patent-data-table-td ">David Guedalia</td><td class="patent-data-table-td ">Method and system for enhanced user experience of audio</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20070280433">US20070280433</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 31, 2006</td><td class="patent-data-table-td patent-date-value">Dec 6, 2007</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Voicemail message controls</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110258272">US20110258272</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 3, 2011</td><td class="patent-data-table-td patent-date-value">Oct 20, 2011</td><td class="patent-data-table-td ">Barracuda Networks Inc.</td><td class="patent-data-table-td ">Facilitating transmission of an email of a well behaved sender by extracting email parameters and querying a database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120209607">US20120209607</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 13, 2012</td><td class="patent-data-table-td patent-date-value">Aug 16, 2012</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Method and apparatus for scrolling text display of voice call or message during video display session</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/USRE44298">USRE44298</a></td><td class="patent-data-table-td patent-date-value">Jul 27, 2006</td><td class="patent-data-table-td patent-date-value">Jun 11, 2013</td><td class="patent-data-table-td ">Mosi Media, Inc.</td><td class="patent-data-table-td ">Method and apparatus for sharing streaming media links</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN1801322B?cl=en">CN1801322B</a></td><td class="patent-data-table-td patent-date-value">Nov 17, 2005</td><td class="patent-data-table-td patent-date-value">Jun 9, 2010</td><td class="patent-data-table-td ">纽昂斯通讯公司</td><td class="patent-data-table-td ">Method and system for transcribing speech on demand using a transcription portlet</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN100555175C?cl=en">CN100555175C</a></td><td class="patent-data-table-td patent-date-value">May 8, 2001</td><td class="patent-data-table-td patent-date-value">Oct 28, 2009</td><td class="patent-data-table-td ">国际商业机器公司</td><td class="patent-data-table-td ">Method and system for data input into equipment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE102008046431A1?cl=en">DE102008046431A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 9, 2008</td><td class="patent-data-table-td patent-date-value">Mar 11, 2010</td><td class="patent-data-table-td ">Deutsche Telekom Ag</td><td class="patent-data-table-td ">Sprachdialogsystem mit Reject-Vermeidungsverfahren</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001004790A1?cl=en">WO2001004790A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 6, 2000</td><td class="patent-data-table-td patent-date-value">Jan 18, 2001</td><td class="patent-data-table-td ">Gil Jacob</td><td class="patent-data-table-td ">Sign translator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001031472A1?cl=en">WO2001031472A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 20, 2000</td><td class="patent-data-table-td patent-date-value">May 3, 2001</td><td class="patent-data-table-td ">Telcordia Tech Inc</td><td class="patent-data-table-td ">Method and system for host mobility management protocol</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001060048A1?cl=en">WO2001060048A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 8, 2001</td><td class="patent-data-table-td patent-date-value">Aug 16, 2001</td><td class="patent-data-table-td ">Docuport Inc</td><td class="patent-data-table-td ">Receiving and sending electronic messages through a portable device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001067739A1?cl=en">WO2001067739A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 6, 2001</td><td class="patent-data-table-td patent-date-value">Sep 13, 2001</td><td class="patent-data-table-td ">Pei Sern</td><td class="patent-data-table-td ">Fax-through data network and remote access system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2002023389A1?cl=en">WO2002023389A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 15, 2000</td><td class="patent-data-table-td patent-date-value">Mar 21, 2002</td><td class="patent-data-table-td ">Robert Fish</td><td class="patent-data-table-td ">Systems and methods for translating an item of information using a distal computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2004029773A2?cl=en">WO2004029773A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 26, 2003</td><td class="patent-data-table-td patent-date-value">Apr 8, 2004</td><td class="patent-data-table-td ">Callminer Inc</td><td class="patent-data-table-td ">Software for statistical analysis of speech</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S260000">704/260</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S231000">704/231</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S235000">704/235</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S270100">704/270.1</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S270000">704/270</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704SE15045">704/E15.045</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0003160000">G06F3/16</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04Q0007380000">H04Q7/38</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04N0001320000">H04N1/32</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0019000000">G10L19/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0015260000">G10L15/26</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04M0011000000">H04M11/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04N0001387000">H04N1/387</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0013000000">G06F13/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0013000000">G10L13/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04N0001000000">H04N1/00</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M2250/74">H04M2250/74</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L15/265">G10L15/265</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=CExMBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M1/72552">H04M1/72552</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G10L15/26A</span>, <span class="nested-value">H04M1/725F1M4</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Dec 12, 2011</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20111201</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:VLINGO CORPORATION;REEL/FRAME:027362/0898</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">RESEARCH IN MOTION LIMITED, CANADA</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 18, 2011</td><td class="patent-data-table-td ">PRDP</td><td class="patent-data-table-td ">Patent reinstated due to the acceptance of a late maintenance fee</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20110721</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 1, 2011</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 1, 2011</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">11</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 25, 2011</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 19, 2011</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 1-12 IS CONFIRMED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 1, 2011</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20101013</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 8, 2010</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100721</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:INTELLECTUAL VENTURES HOLDING 56 LLC;REEL/FRAME:025330/0404</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">VLINGO CORPORATION, MASSACHUSETTS</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 20, 2008</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">INTELLECTUAL VENTURES HOLDING 56 LLC, NEVADA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:CASIO COMPUTER CO., LTD.;REEL/FRAME:021701/0814</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080804</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 26, 2007</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 28, 2003</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 7, 1997</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">CASIO COMPUTER CO., LTD., JAPAN</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:YAMAKITA, TOORU;REEL/FRAME:008884/0931</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19971027</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 6, 1997</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">CASIO COMPUTER CO., LTD., JAPAN</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNOR S INTEREST RE-RECORD TO CORRECT THE RECORDATION DATE OF 11/07/97 TO 11/06/97PREVIOUSLY RECORDED AT REEL 8884 FRAME 0931;ASSIGNOR:YAMAKITA, TOORU;REEL/FRAME:009228/0005</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19971027</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U0uA2FCJgdz4Cc3p13ZDqq5MN8pPQ\u0026id=CExMBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U27VPEoXuc1kEuPcyVxHWAOONoyTQ\u0026id=CExMBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U2wPtjn_AtswERlqbkh5iVEyzNFWw","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Apparatus_for_generating_text_data_on_th.pdf?id=CExMBAABERAJ\u0026output=pdf\u0026sig=ACfU3U1CSAECto_e2kq9WiRvWTpv2vi2Dw"},"sample_url":"http://www.google.com/patents/reader?id=CExMBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>