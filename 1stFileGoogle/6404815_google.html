<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6404815 - Image encoder, image decoder, image encoding method, image decoding method ... - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Image encoder, image decoder, image encoding method, image decoding method and image encoding/decoding system"><meta name="DC.contributor" content="Shunichi Sekiguchi" scheme="inventor"><meta name="DC.contributor" content="Kohtaro Asai" scheme="inventor"><meta name="DC.contributor" content="Tokumichi Murakami" scheme="inventor"><meta name="DC.contributor" content="Hirofumi Nishikawa" scheme="inventor"><meta name="DC.contributor" content="Shinichi Kuroda" scheme="inventor"><meta name="DC.contributor" content="Yoshimi Isu" scheme="inventor"><meta name="DC.contributor" content="Yuri Hasegawa" scheme="inventor"><meta name="DC.contributor" content="Mitsubishi Denki Kabushiki Kaisha" scheme="assignee"><meta name="DC.date" content="1997-10-23" scheme="dateSubmitted"><meta name="DC.description" content="An efficient video encoder and a compression encoding of an input picture includes motion compensated prediction means for performing interframe motion detection. The motion compensated prediction means has a motion detector and a motion compensator. The motion detector includes an image transforming unit that transforms a reference picture portion into a picture portion having a predefined shape. The motion compensator includes a pel determinator for determining integer pels of real sample points of the transformed reference picture portion that corresponds to an integer pel of the input picture block according to the motion parameters supplied from the motion detector as a result of the comparison so as to supply prediction picture data. The video encoder and a decoder having a motion compensator that performs operations similar to the video encoder are combined to form a video encoding and decoding system."><meta name="DC.date" content="2002-6-11" scheme="issued"><meta name="DC.relation" content="JP:H048585" scheme="references"><meta name="DC.relation" content="JP:H05219498" scheme="references"><meta name="DC.relation" content="JP:H05244585" scheme="references"><meta name="DC.relation" content="JP:H06153185" scheme="references"><meta name="DC.relation" content="JP:H0698314" scheme="references"><meta name="DC.relation" content="JP:H0750773" scheme="references"><meta name="DC.relation" content="US:5311310" scheme="references"><meta name="DC.relation" content="US:5376971" scheme="references"><meta name="DC.relation" content="US:5574504" scheme="references"><meta name="DC.relation" content="US:5598215" scheme="references"><meta name="DC.relation" content="US:5684538" scheme="references"><meta name="DC.relation" content="US:5963259" scheme="references"><meta name="DC.relation" content="US:5978030" scheme="references"><meta name="DC.relation" content="US:6144701" scheme="references"><meta name="DC.relation" content="US:6208690" scheme="references"><meta name="DC.relation" content="US:6249318" scheme="references"><meta name="DC.relation" content="US:6249613" scheme="references"><meta name="citation_reference" content="ISO/IEC 11172-2 (14 pages)."><meta name="citation_reference" content="Motion Compensated Prediction Using An Affine Motion Model (Technical Report of the Institute of Electronics, Information and Communication Engineers, IE 94-36)."><meta name="citation_patent_number" content="US:6404815"><meta name="citation_patent_application_number" content="US:09/180,188"><link rel="canonical" href="http://www.google.com/patents/US6404815"/><meta property="og:url" content="http://www.google.com/patents/US6404815"/><meta name="title" content="Patent US6404815 - Image encoder, image decoder, image encoding method, image decoding method and image encoding/decoding system"/><meta name="description" content="An efficient video encoder and a compression encoding of an input picture includes motion compensated prediction means for performing interframe motion detection. The motion compensated prediction means has a motion detector and a motion compensator. The motion detector includes an image transforming unit that transforms a reference picture portion into a picture portion having a predefined shape. The motion compensator includes a pel determinator for determining integer pels of real sample points of the transformed reference picture portion that corresponds to an integer pel of the input picture block according to the motion parameters supplied from the motion detector as a result of the comparison so as to supply prediction picture data. The video encoder and a decoder having a motion compensator that performs operations similar to the video encoder are combined to form a video encoding and decoding system."/><meta property="og:title" content="Patent US6404815 - Image encoder, image decoder, image encoding method, image decoding method and image encoding/decoding system"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("1ybuU9f5H8q8sQSNp4KgAQ"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("POL"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("1ybuU9f5H8q8sQSNp4KgAQ"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("POL"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6404815?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6404815"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=GmpaBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6404815&amp;usg=AFQjCNEXT-PbmKPEd-NZ_rnm30pnVY5L0w" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6404815.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6404815.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6404815" style="display:none"><span itemprop="description">An efficient video encoder and a compression encoding of an input picture includes motion compensated prediction means for performing interframe motion detection. The motion compensated prediction means has a motion detector and a motion compensator. The motion detector includes an image transforming...</span><span itemprop="url">http://www.google.com/patents/US6404815?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6404815 - Image encoder, image decoder, image encoding method, image decoding method and image encoding/decoding system</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6404815 - Image encoder, image decoder, image encoding method, image decoding method and image encoding/decoding system" title="Patent US6404815 - Image encoder, image decoder, image encoding method, image decoding method and image encoding/decoding system"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6404815 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/180,188</td></tr><tr><td class="patent-bibdata-heading">PCT number</td><td class="single-patent-bibdata">PCT/JP1997/003825</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Jun 11, 2002</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Oct 23, 1997</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Mar 17, 1997</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/DE69738792D1">DE69738792D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69739999D1">DE69739999D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69740004D1">DE69740004D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69740005D1">DE69740005D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0905980A1">EP0905980A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0905980A4">EP0905980A4</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0905980B1">EP0905980B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1725045A2">EP1725045A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1725045A3">EP1725045A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1725045B1">EP1725045B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1725046A2">EP1725046A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1725046A3">EP1725046A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1725046B1">EP1725046B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1725047A2">EP1725047A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1725047A3">EP1725047A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1725047B1">EP1725047B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2252065A2">EP2252065A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2252065A3">EP2252065A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2252066A2">EP2252066A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2252066A3">EP2252066A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2252067A2">EP2252067A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2252067A3">EP2252067A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2252068A2">EP2252068A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP2252068A3">EP2252068A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7110456">US7110456</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8098734">US8098734</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8170105">US8170105</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8194742">US8194742</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20020118758">US20020118758</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20060133508">US20060133508</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20060159180">US20060159180</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20100172415">US20100172415</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1998042134A1">WO1998042134A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1998042135A1">WO1998042135A1</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">09180188, </span><span class="patent-bibdata-value">180188, </span><span class="patent-bibdata-value">PCT/1997/3825, </span><span class="patent-bibdata-value">PCT/JP/1997/003825, </span><span class="patent-bibdata-value">PCT/JP/1997/03825, </span><span class="patent-bibdata-value">PCT/JP/97/003825, </span><span class="patent-bibdata-value">PCT/JP/97/03825, </span><span class="patent-bibdata-value">PCT/JP1997/003825, </span><span class="patent-bibdata-value">PCT/JP1997/03825, </span><span class="patent-bibdata-value">PCT/JP1997003825, </span><span class="patent-bibdata-value">PCT/JP199703825, </span><span class="patent-bibdata-value">PCT/JP97/003825, </span><span class="patent-bibdata-value">PCT/JP97/03825, </span><span class="patent-bibdata-value">PCT/JP97003825, </span><span class="patent-bibdata-value">PCT/JP9703825, </span><span class="patent-bibdata-value">US 6404815 B1, </span><span class="patent-bibdata-value">US 6404815B1, </span><span class="patent-bibdata-value">US-B1-6404815, </span><span class="patent-bibdata-value">US6404815 B1, </span><span class="patent-bibdata-value">US6404815B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Shunichi+Sekiguchi%22">Shunichi Sekiguchi</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Kohtaro+Asai%22">Kohtaro Asai</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Tokumichi+Murakami%22">Tokumichi Murakami</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Hirofumi+Nishikawa%22">Hirofumi Nishikawa</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Shinichi+Kuroda%22">Shinichi Kuroda</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Yoshimi+Isu%22">Yoshimi Isu</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Yuri+Hasegawa%22">Yuri Hasegawa</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Mitsubishi+Denki+Kabushiki+Kaisha%22">Mitsubishi Denki Kabushiki Kaisha</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6404815.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6404815.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6404815.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (17),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (2),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (12),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (16),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (6)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6404815&usg=AFQjCNFRQNLcKvcmRzPEkl5KKMtPQUPMjg">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6404815&usg=AFQjCNHgQwye5adl1DzG6YJnPvAvL5FXew">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6404815B1%26KC%3DB1%26FT%3DD&usg=AFQjCNElX6Sr_uGcBy_lkbSGj-siupf1zg">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54929190" lang="EN" load-source="patent-office">Image encoder, image decoder, image encoding method, image decoding method and image encoding/decoding system</invention-title></span><br><span class="patent-number">US 6404815 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA50330977" lang="EN" load-source="patent-office"> <div class="abstract">An efficient video encoder and a compression encoding of an input picture includes motion compensated prediction means for performing interframe motion detection. The motion compensated prediction means has a motion detector and a motion compensator. The motion detector includes an image transforming unit that transforms a reference picture portion into a picture portion having a predefined shape. The motion compensator includes a pel determinator for determining integer pels of real sample points of the transformed reference picture portion that corresponds to an integer pel of the input picture block according to the motion parameters supplied from the motion detector as a result of the comparison so as to supply prediction picture data. The video encoder and a decoder having a motion compensator that performs operations similar to the video encoder are combined to form a video encoding and decoding system.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(52)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00018.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00018.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00019.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00019.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00020.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00020.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00021.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00021.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00022.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00022.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00023.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00023.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00024.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00024.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00025.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00025.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00026.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00026.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00027.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00027.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00028.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00028.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00029.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00029.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00030.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00030.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00031.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00031.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00032.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00032.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00033.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00033.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00034.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00034.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00035.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00035.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00036.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00036.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00037.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00037.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00038.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00038.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00039.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00039.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00040.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00040.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00041.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00041.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00042.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00042.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00043.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00043.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00044.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00044.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00045.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00045.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00046.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00046.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00047.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00047.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00048.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00048.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00049.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00049.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00050.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00050.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-D00051.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6404815B1/US06404815-20020611-D00051.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(15)</span></span></div><div class="patent-text"><div mxw-id="PCLM8305462" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is: </claim-statement> <div class="claim"> <div num="1" id="US-6404815-B1-CLM-00001" class="claim">
      <div class="claim-text">1. A video encoder for performing motion compensated prediction encoding of block-divided input image, comprising:</div>
      <div class="claim-text">a motion detector for detecting at least one motion parameter providing minimum prediction error, including an image transformation unit for transforming a reference image portion into a transformed image portion using a given transformation, and for matching the transformed reference image portion with an input image portion; </div>
      <div class="claim-text">a motion compensator for outputting a motion compensated prediction image, including a corresponding pel determinator for determining pels for transforming the reference image portion based on the motion parameters; </div>
      <div class="claim-text">a multiplexer for multiplexing said motion parameters together with the information which indicates a given transformation to the encoded bitstream; and </div>
      <div class="claim-text">wherein the image transformation unit transforms a reference image portion comprised of half-pels as well as integer pels into the transformed image portion. </div>
    </div>
    </div> <div class="claim"> <div num="2" id="US-6404815-B1-CLM-00002" class="claim">
      <div class="claim-text">2. A video encoder for performing motion compensated prediction encoding of block-divided input image, comprising:</div>
      <div class="claim-text">a motion detector for detecting at least one motion parameter providing minimum prediction error, including an image transformation unit for transforming a reference image portion into a transformed image portion using a given transformation, and for matching the transformed reference image portion with an input image portion; </div>
      <div class="claim-text">a motion compensator for outputting a motion compensated prediction image, including a corresponding pel determinator for determining pels for transforming the reference image portion based on the motion parameters; </div>
      <div class="claim-text">a multiplexer for multiplexing said motion parameters together with the information which indicates a given transformation to the encoded bitstream; and </div>
      <div class="claim-text">a preprocessor for separating an input video into video objects each of which is an coding unit, the video encoder dividing the video object into blocks, detecting motion parameters, and compensating the motion of a divided video object. </div>
    </div>
    </div> <div class="claim"> <div num="3" id="US-6404815-B1-CLM-00003" class="claim">
      <div class="claim-text">3. A video encoder for performing motion compensated prediction encoding of block-divided input image, comprising:</div>
      <div class="claim-text">a motion detector for detecting at least one motion parameter providing minimum prediction error, including an image transformation unit for transforming a reference image portion into a transformed image portion using a given transformation, and for matching the transformed reference image portion with an input image portion; </div>
      <div class="claim-text">a motion compensator for outputting a motion compensated prediction image, including a corresponding pel determinator for determining pels for transforming the reference image portion based on the motion parameters; </div>
      <div class="claim-text">a multiplexer for multiplexing said motion parameters together with the information which indicates a given transformation to the encoded bitstream; and </div>
      <div class="claim-text">wherein the image transformation unit transforms the reference image portion comprised of half-pels as well as integer pels by indicating addresses of one of an image portion scaled down at a specific ratio and an image portion scaled up at a specific ratio, and matches the transformed reference image portion with the input image portion. </div>
    </div>
    </div> <div class="claim"> <div num="4" id="US-6404815-B1-CLM-00004" class="claim">
      <div class="claim-text">4. A video encoder for performing motion compensated prediction encoding of block-divided input image, comprising:</div>
      <div class="claim-text">a motion detector for detecting at least one motion parameter providing minimum prediction error, including an image transformation unit for transforming a reference image portion into a transformed image portion using a given transformation, and for matching the transformed reference image portion with an input image portion; </div>
      <div class="claim-text">a motion compensator for outputting a motion compensated prediction image, including a corresponding pel determinator for determining pels for transforming the reference image portion based on the motion parameters; </div>
      <div class="claim-text">a multiplexer for multiplexing said motion parameters together with the information which indicates a given transformation to the encoded bitstream; and </div>
      <div class="claim-text">wherein the image transformation unit transforms a reference image portion comprised of half-pels as well as integer pels by indicating addresses of an image portion rotated by a predefined angle. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" id="US-6404815-B1-CLM-00005" class="claim">
      <div class="claim-text">5. The video encoder according to <claim-ref idref="US-6404815-B1-CLM-00004">claim 4</claim-ref>, wherein the image transformation unit indicates addresses of an image portion rotated by one of angles of ±45°, ±90°, ±135°, ±180°.</div>
    </div>
    </div> <div class="claim"> <div num="6" id="US-6404815-B1-CLM-00006" class="claim">
      <div class="claim-text">6. A video encoder for performing motion compensated prediction encoding of block-divided input image, comprising:</div>
      <div class="claim-text">a motion detector for detecting a final motion vector providing minimum prediction error, the motion detector including: </div>
      <div class="claim-text">a block matching unit for matching the reference image portion, which has been translated, with the input image portion to detect a first motion vector and produce a first minimum prediction error signal; </div>
      <div class="claim-text">a transformed block matching unit for transforming a reference image portion consisting of half-pels as well as integer pels by indicating addresses of the half or integer pels into a transformed image portion having a predefined shape within a search range for a second motion vector around the location indicated by the first motion vector, and for matching the transformed reference image portion with an input image portion to detect a second motion vector and produce a second minimum prediction error signal; </div>
      <div class="claim-text">a motion compensated prediction mode determinator for determining a final motion vector which provides the minimum prediction error based on the first and the second minimum prediction error signals; </div>
      <div class="claim-text">a motion compensator for outputting a motion compensated prediction image, including a corresponding pel determinator for determining pels for transforming the reference image portion based on the motion vector; and </div>
      <div class="claim-text">a multiplexer for multiplexing the final motion vector together with the information which indicates the predefined shape to the encoded bitstream. </div>
    </div>
    </div> <div class="claim"> <div num="7" id="US-6404815-B1-CLM-00007" class="claim">
      <div class="claim-text">7. A video encoder for performing motion compensated prediction encoding of block-divided input image, comprising:</div>
      <div class="claim-text">a motion detector for detecting at least one motion parameter providing minimum prediction error, including an image transformation unit for transforming a reference image portion into a transformed image portion using a given transformation, and for matching the transformed reference image portion with an input image portion; </div>
      <div class="claim-text">a motion compensator for outputting a motion compensated prediction image, including a corresponding pel determinator for determining pels for transforming the reference image portion based on the motion parameters; </div>
      <div class="claim-text">a multiplexer for multiplexing said motion parameters together with the information which indicates a given transformation to the encoded bitstream; and </div>
      <div class="claim-text">wherein the image transformation unit has a transformation pattern table for specifying addresses for transforming the reference image portion, and transforms the reference image portion by accessing the transformation pattern table. </div>
    </div>
    </div> <div class="claim"> <div num="8" id="US-6404815-B1-CLM-00008" class="claim">
      <div class="claim-text">8. A video encoder for performing motion compensated prediction encoding of block-divided input image, comprising:</div>
      <div class="claim-text">a motion detector for detecting at least one motion parameter providing minimum prediction error, including an image transformation unit for transforming a reference image portion into a transformed image portion using a given transformation, and for matching the transformed reference image portion with an input image portion; </div>
      <div class="claim-text">a motion compensator for outputting a motion compensated prediction image, including a corresponding pel determinator for determining pels for transforming the reference image portion based on the motion parameters; </div>
      <div class="claim-text">a multiplexer for multiplexing said motion parameters together with the information which indicates a given transformation to the encoded bitstream; and </div>
      <div class="claim-text">wherein the image transformation unit selectively filters specific pels of the reference image portion extracted for matching, and matches the transformed reference image portion with the input image portion. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" id="US-6404815-B1-CLM-00009" class="claim">
      <div class="claim-text">9. The video encoder according to <claim-ref idref="US-6404815-B1-CLM-00001">claim 1</claim-ref>, wherein the reference image portion is one of the following candidates: 1) a portion which is contained in the frame which is forward in time with respect to the frame which contains the input image portion, 2) a portion which is contained in the frame which is backward in time with respect to the frame which contains the input image portion, and 3) a portion which can be obtained by averaging 1) and 2).</div>
    </div>
    </div> <div class="claim"> <div num="10" id="US-6404815-B1-CLM-00010" class="claim">
      <div class="claim-text">10. A video decoder for decoding an encoded bitstream of video data, comprising:</div>
      <div class="claim-text">a motion compensation unit including an image transformation unit for transforming a reference image portion into a transformed image portion using the transformation specified by at least one motion parameter decoded from an input bitstream; </div>
      <div class="claim-text">an image reconstruction unit for reconstructing a decoded image portion using a decoded data and said transformed image portion; and </div>
      <div class="claim-text">wherein the at least one motion parameter consists of a motion vector and a transformation pattern information, and the motion compensation unit includes a mechanism for determining pels of the corresponding reference image portion comprised of half-pels as well as integer-pels by transforming the corresponding reference image portion into one of an image portion scaled up, an image portion scaled down, and an image portion rotated according to the motion parameters. </div>
    </div>
    </div> <div class="claim"> <div num="11" id="US-6404815-B1-CLM-00011" class="claim">
      <div class="claim-text">11. A video decoder for decoding an encoded bitstream of video data, comprising:</div>
      <div class="claim-text">a motion compensation unit including an image transformation unit for transforming a reference image portion into a transformed image portion using the transformation specified by at least one motion parameter decoded from an input bitstream; </div>
      <div class="claim-text">an image reconstruction unit for reconstructing a decoded image portion using a decoded data and said transformed image portion; and </div>
      <div class="claim-text">wherein the at least one motion parameter consists of plural motion vectors and a transformation pattern information, and the image transformation unit transforms the reference image portion comprised of half-pels as well as integer pels into the transformed image portion by indicating addresses calculated with plural motion vectors according to the transformation pattern information. </div>
    </div>
    </div> <div class="claim"> <div num="12" id="US-6404815-B1-CLM-00012" class="claim">
      <div class="claim-text">12. A video decoder for decoding an encoded bitstream of video data, comprising:</div>
      <div class="claim-text">a motion compensation unit including an image transformation unit for transforming a reference image portion into a transformed image portion using the transformation specified by at least one motion parameter decoded from an input bitstream; </div>
      <div class="claim-text">an image reconstruction unit for reconstructing a decoded image portion using a decoded data and said transformed image portion; and </div>
      <div class="claim-text">wherein the at least one motion parameter consists of plural motion vectors, a transformation pattern information and a round-up information, and the image transformation unit transforms the reference image portion comprised of half-pels as well as integer pels into the transformed image portion by indicating addresses according to the following process; </div>
      <div class="claim-text">1) calculate addresses with plural motion vectors according to the transformation pattern information, </div>
      <div class="claim-text">2) round up calculated addresses with the round-up information. </div>
    </div>
    </div> <div class="claim"> <div num="13" id="US-6404815-B1-CLM-00013" class="claim">
      <div class="claim-text">13. A video decoder for decoding an encoded bitstream of video data, comprising:</div>
      <div class="claim-text">a motion compensation unit including an image transformation unit for transforming a reference image portion into a transformed image portion using the transformation specified by at least one motion parameter decoded from an input bitstream; </div>
      <div class="claim-text">an image reconstruction unit for reconstructing a decoded image portion using a decoded data and said transformed image portion; and </div>
      <div class="claim-text">wherein the number of pels of the width and the height of the reference image portion is selected from a power of 2. </div>
    </div>
    </div> <div class="claim"> <div num="14" id="US-6404815-B1-CLM-00014" class="claim">
      <div class="claim-text">14. A video encoding method for performing motion compensated prediction encoding of block-divided input image, comprising:</div>
      <div class="claim-text">a motion detecting step for detecting motion parameters providing minimum prediction error, including an image transformation step for transforming a reference image portion consisting of half-pels as well as integer pels into a transformed image portion using a given transformation, and for matching the transformed reference image portion with an input image portion; </div>
      <div class="claim-text">a motion compensating step for outputting a motion compensated prediction image, including a corresponding pel determining step for determining pels for transforming the reference image portion based on the motion vector; and </div>
      <div class="claim-text">a multiplexing step for multiplexing said motion parameters together with the information which indicates a given transformation to the encoded bitstream. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" id="US-6404815-B1-CLM-00015" class="claim">
      <div class="claim-text">15. The video encoding method according to <claim-ref idref="US-6404815-B1-CLM-00014">claim 14</claim-ref>, wherein the image transformation step has a transformation pattern table for specifying addresses for transforming the reference image portion, and transforms the reference image portion by accessing the transformation pattern table.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES53579150" lang="EN" load-source="patent-office" class="description">
    <p>This application is the national phase under 35 U.S.C. §371 of prior PCT International Application No. PCT/JP97/03825 which has an International filing date of Oct. 23, 1997 which designated the United States of America, the entire contents of which are hereby incorporated by reference.</p>
    <p>1. Technical Field</p>
    <p>The present invention relates to a highly efficient picture (image) encoding and decoding system for performing motion compensated prediction of a picture (image) to be encoded or decoded for encoding a prediction error, and for decoding reference picture (image) data together with the prediction error by referring to an encoded picture (image).</p>
    <p>2. Background of the Invention</p>
    <p>Conventional motion compensated prediction methods for performing highly efficient encodation of a picture are described below.</p>
    <p>The first example of a conventional motion compensated prediction method which will be discussed is a motion compensated prediction method using block matching that compensates for translational motion of an object. For example, in ISO/IEC 11172-2 (also known as the MPEG 1 video standard), a forward/backward/interpolative motion compensated prediction method using block matching is described. The second example of a conventional motion compensated prediction method which will be discussed is a motion compensated prediction method using an affine motion model. For example, “Motion Compensated Prediction Using An Affine Motion Model” (the technical report of IE94-36, by the Institute of Electronics, Information and Communication Engineers of Japan) describes the motion compensated prediction method in which the displacement of an object in each arbitrarily shaped segment is modeled and expressed using affine motion parameters, and in which the affine motion parameters are detected so as to perform motion compensated prediction.</p>
    <p>Now, the conventional motion compensation method using block matching by a translational motion and the conventional motion compensation method using the affine motion model will be described in more detail below.</p>
    <p>FIG. 42 shows a known motion compensated prediction which utilizes block matching. In FIG. 42, i represents a position of a block on a display as a unit used for motion compensated prediction; fi(x, y, t) represents the pel value (x, y) in the block i at time t on the display; R represents a motion vector search range; and v represents a motion vector (εR). Block matching is a process for detecting, within the search range R of a reference picture <b>201</b>, a block whose pel value is most approximate to the pel value fi(x, y, t) of the block i in an input picture <b>202</b>, or for detecting a pel value fi+v (x, y, t−1) which will minimize a prediction error power Dv which may be expressed in one of the following equations (1) <maths> <math> <mtable> <mtr> <mtd> <mtable> <mtr> <mtd> <mrow> <mrow> <msub> <mi>D</mi> <mi>v</mi> </msub> <mo>=</mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <munder> <mo>∑</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> </mrow> </munder> <mo></mo> <msup> <mrow> <mo>{</mo> <mrow> <mrow> <msub> <mi>f</mi> <mrow> <mi>i</mi> <mo>+</mo> <mi>v</mi> </mrow> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mrow> <mi>t</mi> <mo>-</mo> <mn>1</mn> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> <mrow> <msub> <mi>f</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>t</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>}</mo> </mrow> <mn>2</mn> </msup> </mrow> </mrow> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>or</mi> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <munder> <mo>∑</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> </mrow> </munder> <mo></mo> <mrow> <mo>|</mo> <mrow> <mrow> <msub> <mi>f</mi> <mrow> <mi>i</mi> <mo>+</mo> <mi>v</mi> </mrow> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mrow> <mi>t</mi> <mo>-</mo> <mn>1</mn> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> <mrow> <msub> <mi>f</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>t</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>|</mo> </mrow> </mrow> </mrow> </mtd> </mtr> </mtable> </mtd> <mtd> <mrow> <mo>(</mo> <mn>1</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00001.png"> <img id="EMI-M00001" file="US06404815-20020611-M00001.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00001" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00001.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00001" attachment-type="nb" file="US06404815-20020611-M00001.NB"> </attachment> </attachments> </maths> </p>
    <p>The value v which minimizes Dv will be the motion vector. In FIG. 42, a block matching search method using real sample point integer pels in a reference picture is referred to as an integer pel precision search, and a block matching search method using half-pels (interposed midway between the integer pels) in addition to integer pels is referred to as a half-pel precision search. Generally, under the same block matching search range, more search pel points can be obtained in the half-pel precision search than in the integer pel precision search. Consequently, increased prediction accuracy will be obtained with the half-pel precision search.</p>
    <p>FIG. 43 is a block diagram showing a configuration of a motion compensated predictor (also referred to as a block matching section) using a motion compensated prediction method in accordance with, for example, the MPEGI video standard.</p>
    <p>In the figure, reference numeral <b>207</b> is a horizontal displacement counter, <b>208</b> is a vertical displacement counter, <b>211</b> is a memory readout-address generator, <b>213</b> is a pattern matching unit, and reference numeral <b>216</b> is a minimum prediction error power determinator. Reference numeral <b>203</b> is a horizontal displacement search range indication signal, <b>204</b> is a vertical displacement search range indication signal, <b>205</b> is input picture block data, <b>206</b> is an input picture block position indication signal, <b>209</b> is horizontal displacement search point data, <b>210</b> is vertical displacement search point data, <b>212</b> is a readout address, <b>214</b> is readout picture data, <b>215</b> is a prediction error power signal, <b>217</b> is a motion vector, <b>218</b> is a minimum prediction error power signal, and <b>219</b> is a frame memory for storing reference picture data.</p>
    <p>FIG. 44 is a flow chart showing the operations of the conventional motion compensated predictor having the above-mentioned configuration of FIG. <b>43</b>.</p>
    <p>In FIG. 44, dx represents a horizontal displacement search pel point;</p>
    <p>dy represents a vertical displacement search pel point;</p>
    <p>range_h_min represents a lower limit in a horizontal displacement search range;</p>
    <p>range_h_max represents an upper limit in the horizontal displacement search range;</p>
    <p>range_v_min represents a lower limit in a vertical displacement search range;</p>
    <p>range_v_max represents an upper limit in the vertical displacement search range;</p>
    <p>D_min represents the minimum prediction error power;</p>
    <p>(x, y) are coordinates representing the position of a pel in a macroblock;</p>
    <p>D(dx, dy) represents prediction error power produced when dx and dy are searched;</p>
    <p>f(x, y) is the value of a pel (x, y) in an input picture macroblock;</p>
    <p>fr(x, y) is the value of a pel (x, y) in a reference picture;</p>
    <p>D(x, y) is a prediction error for the pel (x, y) when dx and dy are searched;</p>
    <p>MV_h is a horizontal component of a motion vector (indicating horizontal displacement); and</p>
    <p>MV_v is a vertical component of a motion vector (indicating vertical displacement).</p>
    <p>The block matching operation will be described in more detail, by referring to FIGS. 43 and 44.</p>
    <p>1) Motion Vector Search Range Setting</p>
    <p>Range_h_min and range_h_max are set through the horizontal displacement counter <b>207</b> according to the horizontal displacement search range indication signal <b>203</b>. Range_v_min and range_v_max are set through the vertical displacement counter <b>208</b> according to the vertical displacement search range indication signal <b>204</b>. In addition, the initial values of dx for the horizontal displacement counter <b>207</b> and dy for the vertical displacement counter <b>208</b> are set to range_h_min and range_v_min, respectively. In the minimum prediction error power determinator <b>216</b>, the minimum prediction error power D_min is set to a maximum integer value MAXINT (for example, OxFFFFFFFF). These operations correspond to step S<b>201</b> in FIG. <b>44</b>.</p>
    <p>2) Possible Prediction Picture Readout Operation</p>
    <p>Data on the pel (x+dx, y+dy) in a reference picture, which are distant from the pel (x, y) in the input picture macroblock by dx and dy are fetched from the frame memory. The memory readout address generator <b>211</b> illustrated in FIG. 43 receives the value of dx from the horizontal displacement counter <b>207</b> and the value of dy from the vertical displacement counter <b>208</b>, and generates the address for the pel (x+dx, y+dy) in the frame memory.</p>
    <p>3) Prediction Error Power Calculation</p>
    <p>First, the prediction error power D(dx, dy) for the motion vector representing (dx, dy) is initialized to zero. This corresponds to step S<b>202</b> in FIG. <b>44</b>. The absolute value for the difference between the pel value readout in 2) and the value of the pel (x, y) in the input picture macroblock is accumulated into D(dx, dy). This operation is repeated until the value of x and the value of y become x=y=16. Then, the prediction error power D(dx, dy) produced when (dx, dy) is searched, or Dv given by numeral equations (1) is obtained. This operation is executed by the pattern matching unit <b>213</b> illustrated in FIG. <b>43</b>. Then, the pattern matching unit <b>213</b> supplies D(dx, dy) to the minimum prediction error power determinator <b>216</b> through the prediction error power signal <b>215</b>. These operations correspond to steps S<b>203</b> through S<b>209</b> in FIG. <b>44</b>.</p>
    <p>4) Minimum Prediction Error Power Updating</p>
    <p>It is then determined whether the resultant D(dx, dy) obtained in 3) has given the minimum prediction error power among the searched results which have been obtained so far. This determination is made by the minimum prediction error power determinator <b>216</b> illustrated in FIG. <b>43</b>. This corresponds to step S<b>210</b> in FIG. <b>44</b>. The minimum prediction error power determinator <b>216</b> compares the value of the minimum prediction error power D_min therein with D(dx, dy) supplied through the prediction error power signal <b>215</b>. If D(dx, dy) is smaller than D_min, the minimum prediction error power determinator <b>216</b> updates the value of D_min to D(dx, dy). In addition, the minimum prediction error power determinator <b>216</b> retains the values of dx and dy at that time as the possible motion vector (MV_h, MV_v). This updating operation corresponds to step S<b>211</b> in FIG. <b>44</b>.</p>
    <p>5) Motion Vector Value Determination</p>
    <p>The above-mentioned 2) through 4) operations are repeated for all (dx, dy) within the motion vector search range R. (These operations correspond to steps S<b>212</b> through S<b>215</b> in FIG. 44.) The final values (MV_h, MV_v) retained by the minimum prediction error power determinator <b>216</b> are output as the motion vector <b>217</b>.</p>
    <p>FIG. 45 schematically shows a motion compensated prediction system in accordance with the MPEG1 video standard.</p>
    <p>Under the MPEG1 video standard, a motion picture frame is typically referred to as a picture. One picture is divided into macroblocks, each of which includes 16×16 pels (color difference signal includes 8×8 pels). For each macroblock, motion compensated prediction using block matching is performed. The resultant motion vector value and a prediction error are then encoded.</p>
    <p>Under the MPEG1 video standard, different motion compensation methods can be applied to different individual pictures. Referring to the figure, I-pictures are encoded, without being subjected to motion compensated prediction and without reference to other pictures. P-pictures are encoded using forward motion compensated prediction from a past encoded picture. B-pictures may be encoded using forward motion compensated prediction from a future picture to be encoded, backward motion compensated prediction from a past picture, and interpolative prediction from the mean value between the past encoded picture and the future picture to be encoded. However, forward/backward/interpolative motion compensated predictions are basically all motion compensated prediction using block matching which utilize different reference pictures for implementing the prediction.</p>
    <p>As described above, block matching has been established as a main method for implementing motion compensated prediction for current video encoding systems. Block matching, however, is an operation which determines the translational displacement of an object for each square block such as a macroblock. Block matching is based on the assumption that “a picture portion having the same luminance belongs to the same object”. Consequently, in principle, it is impossible to detect motions of an object other than square-block-based motions. For portions in which the object does not move according to a simple translational motion such as rotation, scaling up and down, zooming, or three-dimensional motion prediction accuracy will be reduced.</p>
    <p>In order to solve the above-mentioned motion detecting problems that are associated with the conventional block matching method described above, motion compensated prediction using the affine motion model have been proposed and aim at more accurately detecting the displacement of an object including rotation and scaling of the object as well as translational motion. This known solution is based on the assumption that (x, y), the value of a pel in a picture segment to be predicted is converted to a reference picture pel value (x′, y′) using the affine motion model as shown in the following equation (2). Under this assumption, all the affine parameters are searched and detected as affine motion parameters. Motion compensated prediction performed on each arbitrarily shaped prediction picture segment after the detection of affine motion parameters is proposed and described in “Motion Compensated Prediction Using An Affine Motion Model” (a technical report of IE94-36 by the Institute of Electronics, Information and Communication Engineers of Japan). <maths> <math> <mtable> <mtr> <mtd> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msup> <mi>x</mi> <mi>′</mi> </msup> </mtd> </mtr> <mtr> <mtd> <msup> <mi>y</mi> <mi>′</mi> </msup> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo>=</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mrow> <mi>cos</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>θ</mi> </mrow> </mtd> <mtd> <mrow> <mi>sin</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>θ</mi> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mo>-</mo> <mi>sin</mi> </mrow> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>θ</mi> </mrow> </mtd> <mtd> <mrow> <mi>cos</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>θ</mi> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo></mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>C</mi> <mi>x</mi> </msub> </mtd> <mtd> <mn>0</mn> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <msub> <mi>C</mi> <mi>y</mi> </msub> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo></mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mi>x</mi> </mtd> </mtr> <mtr> <mtd> <mi>y</mi> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> <mo>+</mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>t</mi> <mi>x</mi> </msub> </mtd> </mtr> <mtr> <mtd> <msub> <mi>t</mi> <mi>y</mi> </msub> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>2</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00002.png"> <img id="EMI-M00002" file="US06404815-20020611-M00002.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00002" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00002.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00002" attachment-type="nb" file="US06404815-20020611-M00002.NB"> </attachment> </attachments> </maths> </p>
    <p>The definition of θ, (Cx, Cy), (tx, ty) will be described later.</p>
    <p>FIG. 46 shows a concept of the motion compensated prediction process using the affine motion model.</p>
    <p>In the figure,</p>
    <p>i represents the position of a segment on a display used as a unit for motion compensated prediction;</p>
    <p>fi(x, y, t) represents a pel (x, y) in the segment position i and at time t;</p>
    <p>Rv represents a translational displacement search range;</p>
    <p>Rrot, scale represents a search range for a rotated angle/scaled amount;</p>
    <p>v represents translational motion vector including translational motion parameters (=(tx, ty));</p>
    <p>rot is a rotation parameter (=a rotated angle θ); and</p>
    <p>scale is scaled amount parameters (=(Cx, Cy)).</p>
    <p>In the motion compensated prediction using the affine model, five affine motion parameters including the rotated angle θ, scaled amount parameters (Cx, Cy) as well as the translational motion parameters (tx, ty) representing the motion vector must be detected. The optimum affine motion parameters can be calculated by searching through all parameters. In order to find the optimum affine motion parameters, however, the number of the arithmetic operations required is enormous. Thus, based on an assumption that the translational displacement is predominant, two stages of affine motion parameter search algorithms are used. In the first stage, the translational displacement parameters (tx, ty) are searched. Then, in the second stage, the rotated angle θ and the scaled amount parameters (Cx, Cy) are searched around the area which the translational displacement parameters (tx, ty) determined in the first stage represents. Further, minute adjustment for the translational displacement is implemented. Among the possible parameters, a combination of affine motion parameters representing the segment which has produced the minimum prediction error power is determined to be the combination of parameters for a prediction picture segment. Then, a difference between the prediction picture segment and the current picture segment is calculated and regarded as a prediction error. And the prediction error is encoded. The prediction error power in accordance with motion compensated prediction using the affine motion model is given by the following equation (3): <maths> <math> <mtable> <mtr> <mtd> <mrow> <mtable> <mtr> <mtd> <mrow> <msub> <mi>D</mi> <mrow> <mi>v</mi> <mo>,</mo> <mi>rot</mi> <mo>,</mo> <mi>scale</mi> </mrow> </msub> <mo>=</mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <munder> <mo>∑</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> </mrow> </munder> <mo></mo> <msup> <mrow> <mo>{</mo> <mrow> <mrow> <msub> <mi>M</mi> <mi>rot</mi> </msub> <mo>·</mo> <msub> <mi>M</mi> <mi>scale</mi> </msub> <mo>·</mo> <mrow> <msub> <mi>f</mi> <mrow> <mi>i</mi> <mo>+</mo> <mi>v</mi> </mrow> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mrow> <mi>t</mi> <mo>-</mo> <mn>1</mn> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>-</mo> <mrow> <msub> <mi>f</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>t</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>}</mo> </mrow> <mn>2</mn> </msup> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>or</mi> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <munder> <mo>∑</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> </mrow> </munder> <mo></mo> <mrow> <mo>|</mo> <mrow> <mrow> <msub> <mi>M</mi> <mi>rot</mi> </msub> <mo>·</mo> <msub> <mi>M</mi> <mi>scale</mi> </msub> <mo>·</mo> <mrow> <msub> <mi>f</mi> <mrow> <mi>i</mi> <mo>+</mo> <mi>v</mi> </mrow> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mrow> <mi>t</mi> <mo>-</mo> <mn>1</mn> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>-</mo> <mrow> <msub> <mi>f</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>,</mo> <mi>t</mi> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>|</mo> </mrow> </mrow> </mrow> </mtd> </mtr> </mtable> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mi>where</mi> <mo>,</mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <msub> <mi>M</mi> <mi>rot</mi> </msub> <mo>=</mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <mrow> <mi>cos</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>θ</mi> </mrow> </mtd> <mtd> <mrow> <mi>sin</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>θ</mi> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mo>-</mo> <mi>sin</mi> </mrow> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>θ</mi> </mrow> </mtd> <mtd> <mrow> <mi>cos</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>θ</mi> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </mrow> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <msub> <mi>M</mi> <mi>scale</mi> </msub> <mo>=</mo> <mrow> <mo>(</mo> <mtable> <mtr> <mtd> <msub> <mi>C</mi> <mi>x</mi> </msub> </mtd> <mtd> <mn>0</mn> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <msub> <mi>C</mi> <mi>y</mi> </msub> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>3</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00003.png"> <img id="EMI-M00003" file="US06404815-20020611-M00003.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00003" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00003.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00003" attachment-type="nb" file="US06404815-20020611-M00003.NB"> </attachment> </attachments> </maths> </p>
    <p>FIG. 47 shows an example of a configuration of a conventional motion compensated predictor for performing motion compensated prediction using the affine motion model.</p>
    <p>In the figure, reference numeral <b>220</b> is a translational displacement minute adjusted amount search range indication signal, reference numeral <b>221</b> is a rotated angle search range indication signal, <b>222</b> is a scaled amount search range indication signal, <b>223</b> is a translational displacement search range indication signal, <b>224</b> is a signal indicating a position of an input picture segment on a display, and <b>225</b> is input picture segment data. Furthermore, reference numeral <b>226</b> is a horizontal displacement counter, <b>227</b> is a vertical displacement counter, <b>228</b> is a translational displacement adder, <b>229</b> is a first minimum prediction error power determinator, <b>230</b> is a merry readout address generator, <b>231</b> is an interpolator, <b>232</b> is a half-pel interpolator, <b>233</b> is a rotated angle counter, <b>234</b> is a scaled amount counter, <b>235</b> is a translational displacement/rotated angle/scaled amount adder, <b>236</b> is a second minimum prediction error power determinator, <b>237</b> is a translational displacement minute adjusted amount counter, <b>238</b> is a translational displacement minute adjusted amount adder, and <b>239</b> is a final minimum prediction error power determinator.</p>
    <p>FIG. 48 is a flow chart showing the conventional operations of the above-mentioned motion compensated predictor. FIG. 49 is a flow chart showing the details of the affine motion parameters detection step illustrated at S<b>224</b> of FIG. <b>48</b>.</p>
    <p>In these flow charts,</p>
    <p>MV_h[4] represents horizontal motion vector components (four possible components);</p>
    <p>MV_v[4] represents vertical motion vector components (four possible components);</p>
    <p>D_min represents the minimum prediction error power;</p>
    <p>θ represents a rotated angle [radian];</p>
    <p>Cx and Cy represent scaled amount parameters; and</p>
    <p>tx and ty are motion vector minute adjusted amount parameters.</p>
    <p>Furthermore, D(θ[i], Cx[i], Cy[i], tx[i], ty[i]) represent the minimum prediction error power obtained after the detection of the affine motion parameters when MV_h[i] and MV_v[i] have been selected;</p>
    <p>dθ represents a rotated angle search pel point;</p>
    <p>dCx represents a horizontal scaled amount search pel point;</p>
    <p>dCy represents a vertical scaled amount search pel point;</p>
    <p>dtx represents a horizontal displacement minute adjusted amount search pel point;</p>
    <p>dty represents a vertical displacement minute adjusted amount search pel point;</p>
    <p>range_radian_min represents a lower limit within a rotated angle search range;</p>
    <p>range_radian_max represents an upper limit within the rotated angle search range;</p>
    <p>range_scale_min represents a lower limit within a scaled amount search range;</p>
    <p>range_scale_max represents an upper limit within the scaled amounts search range;</p>
    <p>range_t_h_min represents a lower limit within a horizontal displacement minute adjusted amount search range;</p>
    <p>range_t_h_max represents an upper limit within the horizontal displacement minute adjusted amount search range;</p>
    <p>range_t_v min represents a lower limit within a vertical displacement minute adjusted amount search range;</p>
    <p>range_t_v_max represents an upper limit within the vertical displacement minute adjusted amount search range;</p>
    <p>D_min represents the minimum prediction error power;</p>
    <p>(x, y) represents a position of a pel in an input picture segment to be predicted;</p>
    <p>f(x, y) represents the value of the pel (x, y) in the input picture to be predicted;</p>
    <p>fr(x, y) represents the value of a pel (x, y) in a reference picture;</p>
    <p>ax represents a value representing horizontal displacement obtained by using the affine motion model;</p>
    <p>ay represents a value representing vertical displacement obtained by using the affine motion model;</p>
    <p>D(ax, ay) represents a prediction error power produced when ax and ay are searched; and</p>
    <p>D(x, y) is a prediction error for the pel (x, y) when ax and ay are searched.</p>
    <p>Referring to FIG. <b>47</b> through FIG. 49, an operation of the conventional motion compensated prediction process using the affine motion model will be described in more detail.</p>
    <p>It is assumed in these figures that like elements or like steps which are given like reference numerals and signs represent the same elements or represent the same processes.</p>
    <p>1) First Stage</p>
    <p>In the first stage of the conventional operation, detection of translational motion parameters (=the motion vector) obtained by the process similar to the above-mentioned block matching process is performed within a picture segment search range.</p>
    <p>Referring to FIG. 47, the picture segment search range is set through the horizontal displacement counter <b>226</b> and the vertical displacement counter <b>227</b> by using the translational displacement search range indication signal <b>223</b>. Then, the search pel points are moved. Through the translational displacement adder <b>228</b>, the value indicating the position of a pel in an input picture segment is added to the counter values. Then, the added result is supplied to the memory readout address generator <b>230</b>, and the pel value in a possible prediction picture portion is read out from the frame memory <b>219</b>. The readout pel value is supplied to the pattern matching unit <b>213</b>, and an error calculation operation similar to that used in the block matching method is performed. This matched result is supplied to the first minimum prediction error power determinator <b>229</b> so as to obtain four possible translational motion parameters representing prediction errors in the reverse order of magnitude. These four possible translational motion parameters are expressed as MV_h[4] (horizontal components) and MV_v[4] (vertical components). The operation of the first minimum prediction error power determinator <b>229</b> is similar to that of the minimum prediction error power determinator <b>216</b>. These process steps correspond to steps S<b>221</b> and S<b>222</b> in FIG. <b>48</b>.</p>
    <p>2) Second Stage</p>
    <p>2-1) Preparations (Picture Segment Search Range Setting and Initialization of the Minimum Prediction Error Power)</p>
    <p>For each MV_h[i]/MV_v[i] (0≦i≦3), the rotated angle/the scaled amount are searched around the minute space conterminous therewith. This operation corresponds to step S<b>224</b> in FIG. <b>48</b> and the detailed process steps thereof are illustrated in FIG. 49, and will be described in conjunction with the operation of the motion compensated predictor shown in FIG. <b>47</b>.</p>
    <p>First, through the rotated angle search range indication signal <b>221</b> and the scaled amount search range indication signal <b>222</b>, the rotated angle search range and the scaled amount Search range are set in the rotated angle counter <b>233</b> and the scaled amount counter <b>234</b>, respectively. Through the translational displacement minute adjusted amount search range indication signal <b>220</b>, the translational displacement search range is also set in the translational displacement minute adjusted amount counter <b>237</b>. The second minimum prediction error power determinator <b>236</b> sets the value of the minimum prediction error power D_min retained therein to MAXINT. These operations correspond to step S<b>229</b> in FIG. <b>49</b>.</p>
    <p>2-2) Rotated Angle Search</p>
    <p>The same operation is repeated for each of MV_h[i]/MV_v[i] (0≦i≦3). Thus, a description about the rotated angle search will be directed to the case of MV_h[0]/MV_v[0] alone, and descriptions about other cases will be omitted. The affine motion models ax and ay expressed in the following equations and obtained by changing the rotated angle θ within the rotated angle search range while keeping the scaled amount parameters Cx and Cy and the translational motion minute adjusted amounts tx and ty unchanged: <maths> <math> <mtable> <mtr> <mtd> <mrow> <mo>(</mo> <mrow> <mo></mo> <mtable> <mtr> <mtd> <mrow> <mi>ax</mi> <mo>=</mo> <mrow> <mrow> <mi>d</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>Cx</mi> <mo>*</mo> <mrow> <mi>cos</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>d</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>θ</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>*</mo> <mi>x</mi> </mrow> <mo>+</mo> <mrow> <mi>dCy</mi> <mo>*</mo> <mi>sin</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>d</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>θ</mi> </mrow> <mo>)</mo> </mrow> <mo>*</mo> <mi>y</mi> </mrow> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo>+</mo> <mrow> <mi>MV_h</mi> <mo></mo> <mrow> <mo>[</mo> <mi>i</mi> <mo>]</mo> </mrow> </mrow> <mo>+</mo> <mi>dtx</mi> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>ay</mi> <mo>=</mo> <mrow> <mrow> <mrow> <mo>-</mo> <mi>d</mi> </mrow> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>Cx</mi> <mo>*</mo> <mrow> <mi>sin</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>d</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>θ</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>*</mo> <mi>x</mi> </mrow> <mo>+</mo> <mrow> <mi>dCy</mi> <mo>*</mo> <mi>cos</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>d</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>θ</mi> </mrow> <mo>)</mo> </mrow> <mo>*</mo> <mi>y</mi> </mrow> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo>+</mo> <mrow> <mi>MV_v</mi> <mo></mo> <mrow> <mo>[</mo> <mi>i</mi> <mo>]</mo> </mrow> </mrow> <mo>+</mo> <mi>dty</mi> </mrow> </mrow> </mtd> </mtr> </mtable> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>4</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00004.png"> <img id="EMI-M00004" file="US06404815-20020611-M00004.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00004" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00004.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00004" attachment-type="nb" file="US06404815-20020611-M00004.NB"> </attachment> </attachments> </maths> </p>
    <p>The absolute value for a difference between the pel value fr(ax, ay) in a reference picture segment and the pel value f(x, y) in an input picture segment is determined and accumulated to D(ax, ay).</p>
    <p>Referring to FIG. 47, the above-mentioned operation is executed by fixing the counted values of the scaled amount counter <b>234</b> and the translational displacement minute adjusted amount counter <b>237</b>, determining ax and ay given by equations (4) through the translational displacement/rotated angle/scaled amount adder <b>235</b> based on the counted value of the rotated angle counter <b>233</b>, reading out the pels necessary for calculating fr(ax, ay) from the frame memory <b>219</b> through the memory readout address generator <b>230</b>, calculating fr(ax, ay) from these pels through the interpolator <b>231</b>, and determining the absolute value for the difference between the pel value f(x, y) in the input picture segment and the pel value fr(ax, ay) in the reference picture segment through the pattern matching unit <b>213</b>. Referring to FIG. 49, these operations correspond to steps S<b>231</b> through S<b>234</b>.</p>
    <p>The above-mentioned operations are performed all around the rotated angle search range. Then, the rotated angle θ which has produced the minimum prediction error within the rotated angle search range is determined through the second-stage minimum prediction error determinator <b>236</b>.</p>
    <p>2-3) Scaled Amount Search The affine motion models ax and ay given by numeral equation (4) are also obtained by fixing the counted value of the translational displacement minute adjusted amount counter <b>237</b> as in the rotated angle search, substituting the rotated angle θ determined in 2-2) into numeral equation (4), and changing the scaled amount parameters Cx and Cy within the scaled amount search range.</p>
    <p>The scaled amount parameters Cx and Cy which have minimized D(ax, ay) are obtained by performing the operations similar to those in the rotated angle search. The scaled amount counter <b>234</b> counts scaled amount search pel points.</p>
    <p>2-4) Translational Displacement Minute Adjusted Amount Search</p>
    <p>The affine motion models ax and ay given by numeral equation (4) are also obtained by using the rotated angle θ determined in 2-2) and the scaled amount parameters Cx and Cy determined in 2-3) and changing the value of the translational displacement minute adjusted amounts tx and ty within the translational displacement minute adjusted amount search range.</p>
    <p>Then, operations similar to those in the rotated angle search or the scaled amount search are performed. The translational displacement minute adjusted amount counter <b>237</b> counts translational displacement minute adjusted amount search pel points. In this case, tx and ty are searched with a half-pel precision. Then, the half-pel values for tx and ty are calculated through the half-pel interpolator <b>232</b>, if necessary, before the half-pel value data for tx and ty are supplied to the pattern matching unit <b>213</b>. The half-pel values are calculated as shown in FIG. <b>50</b> and as follows in the following equation (5), based on the spatial position relationship between half-pels and integer pels: <maths> <math> <mtable> <mtr> <mtd> <mtable> <mtr> <mtd> <mrow> <mrow> <mover> <mi>I</mi> <mo>^</mo> </mover> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mo>[</mo> <mrow> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mi>p</mi> </msub> <mo>,</mo> <msub> <mi>y</mi> <mi>p</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>+</mo> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>x</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> <mo>,</mo> <msub> <mi>y</mi> <mi>p</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>+</mo> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mi>p</mi> </msub> <mo>,</mo> <mrow> <msub> <mi>y</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mrow> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>x</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> <mo>,</mo> <mrow> <msub> <mi>y</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> </mrow> <mo>)</mo> </mrow> <mo></mo> <mrow> <mo>]</mo> <mo>/</mo> <mn>4</mn> </mrow> </mrow> <mo>;</mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>x</mi> </mrow> <mo>,</mo> <mrow> <mi>y</mi> <mo></mo> <mstyle> <mtext>:</mtext> </mstyle> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>ODD</mi> </mrow> </mrow> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mrow> <mrow> <mrow> <mo>[</mo> <mrow> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mi>p</mi> </msub> <mo>,</mo> <msub> <mi>y</mi> <mi>p</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>+</mo> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>x</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> <mo>,</mo> <msub> <mi>y</mi> <mi>p</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>]</mo> </mrow> <mo>/</mo> <mn>2</mn> </mrow> <mo>;</mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mi>x</mi> <mo></mo> <mstyle> <mtext>:</mtext> </mstyle> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>ODD</mi> </mrow> </mrow> <mo>,</mo> <mrow> <mi>y</mi> <mo></mo> <mstyle> <mtext>:</mtext> </mstyle> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>EVEN</mi> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mrow> <mrow> <mrow> <mo>[</mo> <mrow> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mi>p</mi> </msub> <mo>,</mo> <msub> <mi>y</mi> <mi>p</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>+</mo> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mi>p</mi> </msub> <mo>,</mo> <mrow> <msub> <mi>y</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>]</mo> </mrow> <mo>/</mo> <mn>2</mn> </mrow> <mo>;</mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mi>x</mi> <mo></mo> <mstyle> <mtext>:</mtext> </mstyle> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>EVEN</mi> </mrow> </mrow> <mo>,</mo> <mrow> <mi>y</mi> <mo></mo> <mstyle> <mtext>:</mtext> </mstyle> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>ODD</mi> </mrow> </mrow> </mrow> </mtd> </mtr> </mtable> </mtd> <mtd> <mrow> <mo>(</mo> <mn>5</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00005.png"> <img id="EMI-M00005" file="US06404815-20020611-M00005.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00005" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00005.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00005" attachment-type="nb" file="US06404815-20020611-M00005.NB"> </attachment> </attachments> </maths> </p>
    <p>in which, both x and y are integers equal to or greater than zero. When x and y are both even numbers, half-pels having such coordinates of x and y will become integer pels.</p>
    <p>The process flow of the operations illustrated in FIG. 49 will be completed as described above.</p>
    <p>2-5) Final Affine Motion Parameters Determination</p>
    <p>A prediction error between a prediction picture segment and an input picture segment is then determined. This prediction error can be obtained by using θ[i], Cx[i], Cy[i], tx[i], and ty[i] given by the above-mentioned affine motion parameters search from 2-2) through 2-4) for all of MV_h[i] and MV_v[i]. The picture segment position i and the set of affine motion parameters therefor which has given the smallest prediction error are regarded as the final search result. These operations correspond to steps S<b>225</b> through S<b>228</b> in FIG. <b>48</b>.</p>
    <p>As described above, the affine motion parameters search requires an enormous calculational burden as well as a great many process steps.</p>
    <p>FIG. 51 is a diagram showing a method of calculating a non-integer pel value produced when the rotated angle and the scaled amount are searched. In other words, the figure is a diagram showing a method of calculating fr(ax, ay) through the interpolator <b>231</b>.</p>
    <p>In the figure, ∘ represents a real sample point in a picture, while  represents a virtual pel value obtained by performing the above-mentioned calculation method. fr(ax, ay) are represented by ↑(x, y) calculated in a reference picture and given by the following equation (6) (in which x=ax, y=ay): <maths> <math> <mtable> <mtr> <mtd> <mrow> <mo></mo> <mtable> <mtr> <mtd> <mrow> <mrow> <mover> <mi>I</mi> <mo>^</mo> </mover> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mrow> <msub> <mi>w</mi> <mi>x1</mi> </msub> <mo></mo> <msub> <mi>w</mi> <mi>y1</mi> </msub> <mo></mo> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mi>p</mi> </msub> <mo>,</mo> <msub> <mi>y</mi> <mi>p</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>+</mo> <mrow> <msub> <mi>w</mi> <mi>x2</mi> </msub> <mo></mo> <msub> <mi>w</mi> <mi>y1</mi> </msub> <mo></mo> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>x</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> <mo>,</mo> <msub> <mi>y</mi> <mi>p</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>+</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mrow> <msub> <mi>w</mi> <mi>x1</mi> </msub> <mo></mo> <msub> <mi>w</mi> <mi>y2</mi> </msub> <mo></mo> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mi>p</mi> </msub> <mo>,</mo> <mrow> <msub> <mi>y</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>+</mo> <mrow> <msub> <mi>w</mi> <mi>x2</mi> </msub> <mo></mo> <msub> <mi>w</mi> <mi>y2</mi> </msub> <mo></mo> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>x</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> <mo>,</mo> <mrow> <msub> <mi>y</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>w</mi> <mi>x2</mi> </msub> <mo>=</mo> <mrow> <msup> <mi>x</mi> <mrow> <mi>′</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> </mrow> </msup> <mo>-</mo> <msub> <mi>x</mi> <mi>p</mi> </msub> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>w</mi> <mi>x1</mi> </msub> <mo>=</mo> <mrow> <mn>1.0</mn> <mo>-</mo> <msub> <mi>w</mi> <mi>x2</mi> </msub> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>w</mi> <mi>y2</mi> </msub> <mo>=</mo> <mrow> <msup> <mi>y</mi> <mi>′</mi> </msup> <mo>-</mo> <msub> <mi>y</mi> <mi>p</mi> </msub> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>w</mi> <mi>y1</mi> </msub> <mo>=</mo> <mrow> <mn>1.0</mn> <mo>-</mo> <msub> <mi>w</mi> <mi>y2</mi> </msub> </mrow> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>6</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00006.png"> <img id="EMI-M00006" file="US06404815-20020611-M00006.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00006" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00006.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00006" attachment-type="nb" file="US06404815-20020611-M00006.NB"> </attachment> </attachments> </maths> </p>
    <p>During the affine motion parameters search, pel matching is performed and the segment which has produced the minimum prediction error power is selected. Consequently, each time any of the above-mentioned five affine motion parameters is changed, the possible prediction picture segment should be formed again. In addition, rotation and scaling of an object produces non-integer pel values. Thus, the operations expressed in equation (6) are repeated over and over again during the affine motion parameters search. Consequently, the affine motion parameters search is very tedious and time-consuming.</p>
    <p>As another motion compensation method using block matching for applying a simple enlarged or reduced picture, Japanese Unexamined Patent Publication No. HEI6-153185 discloses a motion compensator and an encoder utilizing the above motion compensation method. In this method, a reference picture portion included in a frame memory is reduced or enlarged by a thin-out circuit or interpolator, and then a motion vector indicating the above reduction or enlargement is detected. In this configuration, a fixed block is extracted from the reference picture portion to perform an interpolation or a thin-out operation, instead of a complex arithmetic operation such as required by a motion compensation method using affine motion model. Namely, after implementing a predetermined process on an extracted fixed picture portion, the extracted picture portion is compared with an input picture. The process is a simple and fixed one, so that this method can be applied only to a motion prediction of a picture such as simple reduction or enlargement of an input picture.</p>
    <p>The conventional motion compensated prediction methods are constituted and implemented as described above.</p>
    <p>In the first conventional motion compensated prediction method using block matching, formation of a prediction picture portion is implemented by translational motion of a macroblock from a reference picture. Thus, the process itself is simple. However, in this process, only the translational displacement of an object can be predicted, and prediction performance deteriorates when rotation, scaling up and down, or zooming of the object are involved in the motion.</p>
    <p>On the other hand, in the second conventional motion compensated prediction method using the affine motion model, a prediction picture segment is formed using the affine motion model. Thus, when the motion of an object involves the more complicated types of motion such as rotation, this method can be applied. However, the operations needed for implementing the process according to this method are very complex and such a motion compensated predictor must be provided with a complex circuit having a large number of units.</p>
    <p>In general, as the motion compensated prediction process becomes more simplified, prediction often becomes less accurate. In contrast, the motion compensated prediction using the affine motion model increases prediction accuracy at the expense of more complex and tedious operations.</p>
    <p>As for a decoder, any concrete method performing a complex process with a conventional configuration has not been proposed.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>The present invention has been made to solve the above-mentioned problems. It is, therefore, an object of the present invention to obtain an encoder using a relatively simple motion compensated prediction method capable of performing motion compensated prediction for various types of object motion which vary with time. In order to achieve the above-mentioned objects, the real sample point pels on a reference picture, or the pels obtained by simple filtering, are used to form a prediction picture portion which having a different shape and size with respect to the picture portion to be predicted.</p>
    <p>It is a further object of the invention to obtain a decoder performing decoding corresponding to a relatively simple encoding. Further, the invention aims to obtain a video decoder reproducing a more precise and smoother motion with a similar configuration.</p>
    <p>According to the present invention, a video encoder for performing compression encoding of input video divided into predefined blocks, including a motion compensated prediction means for implementing a motion compensation by detecting a motion between frames of blocks of an input video, the video encoder comprises:</p>
    <p>a motion detector, including a transformed block matching unit one example of an image transformation unit for transforming a reference picture portion comprised of only integer pels, which consist of real sampling points, into a picture portion having a predefined shape, for determining addresses of pels of a transformed reference picture portion, and for matching the transformed reference picture portion with the block of input video comprised of integer pels, and the motion detector outputting a motion vector providing a minimum prediction error; and</p>
    <p>a motion compensator, including a corresponding pel determinator for transforming the reference picture portion based on a motion parameter obtained from a comparison result of the transformed block matching unit, and for determining pels corresponding to pels of the reference picture portion, and outputting a prediction picture portion.</p>
    <p>Further, in the video encoder of the invention, the transformed block matching unit transforms a reference picture portion comprised of half-pels as well as integer pels into a picture portion having a predefined shape.</p>
    <p>The video encoder of the invention further comprises a preprocessor for separating an input video into video objects each of which is an encoding unit, the video encoder dividing the video object into blocks, detecting a motion, and compensating a motion of a divided video object.</p>
    <p>Further, in the video encoder of the invention, the transformed block matching unit transforms the reference picture portion comprised of half-pels as well as integer pels into one of a picture portion scaled down at a specific ratio and a picture portion scaled up at a specific ratio, determines the addresses of pels of a transformed reference picture portion, and matches the transformed reference picture portion with the input picture portion;</p>
    <p>the corresponding pel determinator transforms the reference picture portion based on the motion parameter, determines addresses of pels of a transformed reference picture portion, and outputs a motion prediction picture.</p>
    <p>Further, in the video encoder of the invention, the transformed block matching unit transforms the reference picture portion comprised of integer pels and half-pels into a picture portion rotated by a predefined angle, determines addresses of pels of a transformed reference picture portion, and matches the transformed reference picture portion with the input picture; and</p>
    <p>the corresponding per determinator transforms the reference picture portion based on the motion parameter, determines addresses of pels of a transformed reference picture portion, and outputs a motion prediction picture.</p>
    <p>In the video encoder of the invention, the transformed block matching unit one example of the image transformation unit, transforms a reference picture portion comprised of integer pels and half-pels into a picture portion rotated by one of angles of ±45°, ±90°, ±135°, and ±180°.</p>
    <p>Further, in the video encoder of the invention, the transformed block matching unit transforms the reference picture portion, which has been translated within a search range of the reference picture, into one of a picture portion scaled up, a picture portion scaled down, a picture portion rotated by a predefined angle, determines addresses of pels of a transformed reference picture portion, and matches the transformed reference picture portion with the input picture, and</p>
    <p>the corresponding pel determinator transforms the reference picture portion based on the motion parameter, determines addresses of pels of a transformed reference picture portion, and outputs a motion prediction picture.</p>
    <p>Further, in the video encoder of the invention, the transformed block matching unit has a transformation pattern table for transforming the reference picture portion, and matches the reference picture portion transformed according to the transformed value obtained from the transformation pattern table with the input picture comprised of integer pels and half-pels, and</p>
    <p>the corresponding pel determinator transforms the reference picture portion based on the motion parameter, determines addresses of pels of a transformed reference picture portion, and outputs a motion prediction picture.</p>
    <p>Further, in the video encoder of the invention, the transformed block matching unit selectively filters specific pels of the reference picture portion extracted for matching, and matches the transformed reference picture portion with the input picture portion.</p>
    <p>In the video encoder cf the invention, the reference picture used for detecting the motion is the frame which is forward or backward in time and stored in a frame memory for reference, and wherein the transformed block matching unit matches the transformed reference picture portion with the input picture using the frame.</p>
    <p>According to the present invention, a video decoder for decoding a compressed picture code of an input video data includes:</p>
    <p>an entropy decoder for receiving a motion vector extracting motion parameter from the input video data and representing a direction and an amount of the motion and transformation pattern data showing transformation indication; and</p>
    <p>a motion compensator for transforming the integer pels of the reference picture portion corresponding to the frame into a predefined shape based on the motion parameter output from the entropy decoder according to the transformation pattern data included in the input video data and for generating a picture to be added to the input video data.</p>
    <p>In the video decoder of the invention, the motion compensator further computes pels of the corresponding reference picture portion comprised of half-pels as well as integer pels and transforms the corresponding reference picture portion into a picture portion having a predefined shape.</p>
    <p>According to the present invention, a video encoding method for performing compression encoding of an input digital video, comprises:</p>
    <p>a motion compensated prediction means for storing a reference picture, for dividing the reference picture into predefined blocks, and for detecting a motion between frames;</p>
    <p>transformed block matching step for transforming a reference picture portion comprised of integer pels into a picture portion having predefined shape, for determining addresses of pels of a transformed reference picture portion to generate a prediction picture portion, and for matching the transformed reference picture portion with the block of the input digital video;</p>
    <p>corresponding pel determining step for determining pels of the reference picture portion by using the address of transformed reference picture portion based on a motion vector providing a minimum error selected by the transformed block matching step, and for supplying a motion compensated output.</p>
    <p>In the video encoding method of the invention, the transformed block matching step transforms a reference picture portion comprised of half-pels as well as integer pels into a picture portion having a predefined shape as a reference, determines addresses of pels of a transformed reference picture portion to generate a prediction picture portion, and matches the transformed reference picture portion with the input digital video.</p>
    <p>In the video encoding method of the invention, the transformed block matching step further comprises a transformation pattern table, transforms the reference picture portion based on transformation value corresponding to an address from the transformation pattern table, and matches the transformed reference picture portion with the input digital video.</p>
    <p>According to the present invention, a video decoding method for decoding a compressed picture code of an input video data comprises:</p>
    <p>an entropy decoding step for receiving a motion vector extracting motion parameter from the input video data and representing a direction and an amount of the motion and transformation pattern data showing transformation indication; and</p>
    <p>a motion compensating step for transforming the reference picture comprised of integer pels corresponding to the frame into a predefined shape based on the motion parameter output from the entropy decoding step according to the transformation pattern data included in the input video data and for generating a picture to be added to the input video data.</p>
    <p>In the video decoding method of the invention, the motion compensating step transforms the reference picture portion comprised of half-pels as well as integer pels into a picture portion having a predefined shape.</p>
    <p>A video encoding/decoding system comprises:</p>
    <p>a video encoder for performing compression encoding of input video divided into predefined blocks, including a motion compensated prediction means for implementing a motion compensation by detecting a motion between frames of blocks of an input video, the video encoder comprising:</p>
    <p>a motion detector, including a transformed block matching unit for transforming a reference picture portion comprised of only integer pels, which consist of real sampling points, into a picture portion having a predefined shape, for determining addresses of pels of a transformed reference picture portion, and for matching the transformed reference picture portion with the block of input video comprised of integer pels, and the motion detector outputting a motion vector providing a minimum prediction error; and</p>
    <p>a motion compensator, including a corresponding pel determinator for transforming the reference picture portion based on a motion parameter obtained from a comparison result of the transformed block matching unit, and for determining pels corresponding to pels of the reference picture portion, and outputting a prediction picture portion; and</p>
    <p>a video decoder for decoding an input compressed video data, the video decoder including motion compensated prediction means for implementing motion compensation by detecting a motion between frames,</p>
    <p>the motion compensated prediction means has a mechanism for determining pels of a corresponding picture portion comprised of predefined integer pels by transforming the corresponding reference picture portion into a predefined shape according to motion parameters included in the input compressed video data, for extracting the integer pels determined, and for adding the transformed reference picture portion and the decoded video signal so as to produce a decoded video data.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p>FIG. 1 is a block diagram showing a basic configuration of a video encoder according to the present invention.</p>
    <p>FIG. 2 is a block diagram showing an internal configuration of a motion detector <b>8</b> illustrated in FIG. <b>1</b>.</p>
    <p>FIG. 3 is a flow chart showing operations of the motion detector <b>8</b> configured as shown in FIG. <b>2</b>.</p>
    <p>FIG. 4 is a combination of explanatory drawings which describe an operation outline of a transformed block matching unit <b>21</b> according to a first embodiment of the invention.</p>
    <p>FIG. 5 is a block diagram showing an internal configuration of the transformed block matching unit <b>21</b>.</p>
    <p>FIG. 6 is a flow chart showing operations of the transformed block matching unit <b>21</b>.</p>
    <p>FIG. 7 is a block diagram showing an internal configuration of a motion compensator <b>9</b> illustrated in FIG. <b>1</b>.</p>
    <p>FIG. 8 is a flow chart showing operations of the motion compensator <b>9</b>.</p>
    <p>FIG. 9 is a explanatory drawing showing a picture object separating operation of a preprocessor <b>2</b>.</p>
    <p>FIG. 10 is a block diagram showing an internal configuration of a motion detector <b>8</b> <i>b </i>according to a second embodiment of the invention.</p>
    <p>FIG. 11 is a block diagram showing an internal configuration of a motion detector <b>8</b> <i>c </i>according to a third embodiment of the invention.</p>
    <p>FIG. 12 is a combination of explanatory drawings which describe an operation outline of a transformed block matching unit <b>42</b>.</p>
    <p>FIG. 13 is a block diagram showing an internal configuration of the transformed block matching unit <b>42</b>.</p>
    <p>FIG. 14 is a flow chart showing operations of the transformed block matching unit <b>42</b>.</p>
    <p>FIG. 15 is a combination of explanatory drawings which describe an operation outline of a transformed block matching unit <b>42</b> <i>b </i>according to a fourth embodiment of the invention.</p>
    <p>FIG. 16 is a block diagram showing an internal configuration of the transformed block matching unit <b>42</b> <i>b. </i> </p>
    <p>FIG. 17 is a flow chart showing operations of the transformed block matching Unit <b>42</b> <i>b. </i> </p>
    <p>FIG. 18 is a combination of explanatory drawings which describe other form of transformed block matching according to the fourth embodiment.</p>
    <p>FIG. 19 is a combination of explanatory drawings which describe other form of transformed block matching according to the fourth embodiment.</p>
    <p>FIG. 20 is a block diagram showing an internal configuration of a corresponding pel determinator <b>34</b> according to a fifth embodiment of the invention.</p>
    <p>FIG. 21 is a combination of explanatory drawings which describe transformed block matching according to a sixth embodiment of the invention.</p>
    <p>FIG. 22 is a explanatory drawing showing a filtering operation of integer pels constituting a prediction picture portion according to the sixth embodiment.</p>
    <p>FIG. 23 is a combination of explanatory drawings which describe an operation outline of a transformed block matching unit <b>42</b> <i>c. </i> </p>
    <p>FIG. 24 is a block diagram showing an internal configuration of the transformed block matching unit <b>42</b> <i>c. </i> </p>
    <p>FIG. 25 is a flow chart showing operations of the transformed block matching unit <b>42</b> <i>c. </i> </p>
    <p>FIG. 26 is a block diagram showing an internal configuration of a motion compensator <b>9</b> <i>b </i>according to the sixth embodiment.</p>
    <p>FIG. 27 is a flow chart showing operations of the motion compensator <b>9</b> <i>b </i>according to the sixth embodiment.</p>
    <p>FIG. 28 is a block diagram showing a configuration of a video decoder according to a seventh embodiment of the invention.</p>
    <p>FIG. 29 shows an internal configuration of the motion compensator <b>9</b> of the seventh embodiment.</p>
    <p>FIG. 30 is a flowchart showing an operation of the motion compensator <b>9</b> of FIG. <b>29</b>.</p>
    <p>FIG. 31 explains that the pel position is transferred by the motion compensator <b>9</b> of FIG. <b>29</b>.</p>
    <p>FIG. 32 explains an example of transformation performed by the motion compensator <b>9</b> of FIG. <b>29</b>.</p>
    <p>FIG. 33 shows a half-pel interpolation for computing pel position.</p>
    <p>FIG. 34 explains an operation in case that transformation is performed by rotation and enlargement of the block.</p>
    <p>FIG. 35 shows a configuration of a video decoder according to an eighth embodiment of the invention.</p>
    <p>FIG. 36 shows an internal configuration of the motion compensator <b>90</b> of the eighth embodiment.</p>
    <p>FIG. 37 is a flowchart showing an operation of the motion compensator <b>90</b> of FIG. <b>36</b>.</p>
    <p>FIG. 38 explains an example of transformation performed by the motion compensator <b>90</b> of FIG. <b>36</b>.</p>
    <p>FIG. 39 shows an example of computation of pel position implemented by the motion compensator <b>90</b> of FIG.</p>
    <p>FIG. 40 is a flowchart showing an operation of the corresponding pel determinator <b>37</b> <i>c </i>in the motion, compensator of a ninth embodiment.</p>
    <p>FIG. 41 explains an example of transformation performed by the motion compensator of the ninth embodiment.</p>
    <p>FIG. 42 is a combination of explanatory drawings which describe a concept of motion compensated prediction using block matching according to a first conventional related art.</p>
    <p>FIG. 43 is a block diagram showing a configuration of a motion compensated predictor (block matching section) of a video encoder according to the first conventional related art.</p>
    <p>FIG. 44 is a flow chart showing operations of the motion compensator according to the first conventional related art.</p>
    <p>FIG. 45 is a combination of explanatory drawings which describe a motion compensated prediction method used in the MPEG1 video standard.</p>
    <p>FIG. 46 is a combination of explanatory drawings which describe a concept of motion compensated prediction using an affine motion model according to a second conventional related art.</p>
    <p>FIG. 47 is a block diagram showing a configuration of a motion compensator for performing motion compensated prediction using the affine motion model according to the second conventional related art.</p>
    <p>FIG. 48 is a flow chart showing operations of the motion compensated predictor according to the second conventional related art.</p>
    <p>FIG. 49 is a flow chart showing details of an affine motion parameters detection step illustrated in FIG. <b>48</b>.</p>
    <p>FIG. 50 is a explanatory drawing showing half-pel interpolation implemented by a half-pel interpolater <b>232</b>.</p>
    <p>FIG. 51 is a explanatory drawing which describes a non-integer pel interpolation method at a rotated angle/scaled amount search step executed by an interpolator <b>231</b>.</p>
    <heading>DETAILED DESCRIPTION OF THE INVENTION</heading> <heading>Embodiment 1</heading> <p>A video encoder and a video decoder according to the present invention may be used in, for example, a digital video transmitting system, a digital video recording apparatus, a digital video storage data base, or a digital video retrieval and reading system using a satellite, ground wave, or priority communication network.</p>
    <p>FIG. 1 shows a basic configuration of a video encoder.</p>
    <p>Referring to the figure, reference numeral <b>1</b> is an input digital video signal, reference numeral <b>2</b> is a preprocessor, reference numerals <b>3</b> and <b>13</b> are intra-/interframe (inside of the frame/between the frames) encoding selectors, <b>4</b> is an orthogonal transformer, <b>5</b> is a quantizer, <b>6</b> is a dequantizer, <b>7</b> is an inverse orthogonal transformer, <b>8</b> is a motion detector, <b>9</b> is a motion compensator, <b>10</b> is a frame memory (reference picture (image)), <b>11</b> indicates motion parameters including the motion vector, <b>12</b> is prediction picture (image) data, <b>14</b> is an encoding controller, <b>15</b> is a mode selection prompting flag, <b>16</b> is an intra-/interframe encoding indication flag, <b>17</b> is a quantizing step parameter, and reference numeral <b>18</b> is an entropy encoder, <b>19</b> is a compression video data. The essential elements of the present invention are the motion detector <b>8</b> and the motion compensator <b>9</b>.</p>
    <p>The operation of a video encoder according to the first embodiment of the present invention will now be described.</p>
    <p>The video encoder receives a video signal <b>1</b> representing a frame which is a component of a color video sequence. The input video signal <b>1</b> is digitized, preprocessed in the preprocessor <b>2</b> where it is subjected to format conversion, and separated into block data. It is assumed herein that the separated block data includes a pair of the luminance signal component and color difference signal components spatially correspond thereto. From now on, the luminance signal component will be referred to as a luminance block, while the color difference signal component will be referred to as a color difference block.</p>
    <p>Next, the intra-/interframe encoding selector <b>3</b> determines whether each block data is subject to intraframe encoding or interframe encoding. When intraframe (inside of the frame) encoding has been selected, the block data representing input original picture (image) data supplied from the preprocessor <b>2</b> is supplied to the orthogonal transformer <b>4</b>. On the other hand, when interframe (between the frames) encoding has been selected, prediction error block data, which is a difference between the input original picture data <b>1</b> supplied from the preprocessor <b>2</b> and the prediction picture data <b>12</b> supplied from the motion compensator <b>9</b>, is supplied to the orthogonal transformer <b>4</b>. This mode selection between the intraframe encoding and the interframe encoding may be implemented by the mode selection prompting flag <b>15</b> conditioned by the command of the encoding controller <b>14</b>. The selected encoding mode is transmitted to the entropy encoder <b>18</b> in the form of the intra-/interframe encoding indication flag <b>16</b> and multiplexed onto an encoded bitstream <b>19</b>.</p>
    <p>The orthogonal transformer <b>4</b> uses an orthogonal transform such as Discrete Cosine transform (DCT). The orthogonal transforming coefficient is quantized by the quantizer <b>5</b> by using the quantizing step parameter <b>17</b> prepared by the encoding controller <b>14</b>. Then, the redundancy of the quantized orthogonal transforming coefficient is reduced through the entropy encoder <b>18</b> and multiplexed onto the encoded bitstream <b>19</b>. At the same time, the quantized coefficient is dequantized by dequantizer <b>6</b> and is further subjected to the inverse orthogonal transformation through the inverse orthogonal transformer <b>7</b> such that the prediction error signal is restored. A local, decoded picture is generated by adding the restored prediction error signal to the prediction picture data <b>12</b> supplied from the motion compensator <b>9</b>. When the intra-/interframe encoding indication flag <b>16</b> indicates the intraframe encoding mode, the zero signal will be selected via the encoding selector <b>13</b>, and the prediction error signal will not be added to the prediction picture data. The local, decoded picture is used as a reference picture in motion compensated prediction for subsequent frames. Consequently, the local, decoded picture data are written into the frame memory <b>10</b>.</p>
    <p>Now, the motion compensated prediction method and apparatus, which is one of the important features of this embodiment, will be described.</p>
    <p>In this embodiment, it is assumed that block data separated by the preprocessor <b>2</b> is subjected to motion compensated prediction. The motion compensated prediction process is implemented by the motion detector <b>8</b> and the motion compensator <b>9</b>. Then, in the motion detector <b>8</b>, the motion parameters <b>11</b> including the motion vector for the block subject to the motion compensated prediction are detected. The motion compensator <b>9</b> uses the motion parameters <b>11</b> and fetches the prediction picture data <b>12</b> from the frame memory <b>10</b>. The motion detection process is implemented by using luminance blocks. The motion compensated prediction for color difference blocks uses the motion detected result of the luminance blocks. Now, a description will be directed to the motion compensated prediction for luminance blocks alone.</p>
    <p>First, the motion detection process will be described.</p>
    <p>The motion detector <b>8</b> implements the motion detection process. The motion detector <b>8</b> searches within a predetermined reference picture range for a picture portion that is most similar to an input picture luminance block. Then, the parameters representing the change of the input picture block on the display are detected. In the conventional block matching method which has been described above in the conventional related art, a block which is most similar to an input picture luminance block is searched. Then, the translational displacement of the input picture block on the display is detected in the form of the motion vector.</p>
    <p>The motion detector <b>8</b> according to this embodiment implements both conventional block matching using square blocks and block matching using transformed blocks which will be described later. The motion detector <b>8</b> selects data exhibiting the higher prediction accuracy obtained by using conventional block matching or transformed block matching.</p>
    <p>Now, the operation of the motion detector <b>8</b> according to this embodiment will be described.</p>
    <p>FIG. 2 is a block diagram showing a detailed configuration of the motion detector <b>8</b> illustrated in FIG. <b>1</b>. FIG. 3 is a flow chart showing the operations of the motion detector <b>8</b>.</p>
    <p>Referring to FIG. 2, reference numeral <b>20</b> is a block matching unit, reference numeral <b>21</b> is a transformed block matching unit, <b>22</b> is a motion compensated prediction mode determinator, <b>23</b> is a motion vector obtained by implementing transformed block matching, <b>24</b> is the minimum prediction error produced by implementing transformed block matching, <b>25</b> is a final motion vector, and reference numeral <b>26</b> is a motion compensated prediction mode indication signal. It is assumed herein that the final motion vector <b>25</b> and the motion compensated prediction mode indication signal <b>26</b> are both included within the motion parameters <b>11</b>.</p>
    <p>The internal configuration of the block matching unit <b>20</b> and the flow chart showing the operations of the block matching unit <b>20</b> are as illustrated in FIGS. 43 and 44 showing the conventional block matching unit. In FIG. 3, D_BM represents the minimum prediction error produced by implementing block matching, and D_DEF represents the minimum prediction error produced by implementing transformed block matching.</p>
    <p>FIG. 4 is a combination of explanatory drawings which describe an operational outline of the transformed block matching unit <b>21</b> which is one of the most essential structural elements of the present invention. FIG. 5 is a block diagram showing a detailed internal configuration of the transformed block matching unit <b>21</b>. FIG. 6 is a flow chart showing the operations of the transformed block matching unit <b>21</b>.</p>
    <p>Referring to FIG. 5, reference numeral <b>29</b> is a horizontal displacement search range indication signal, reference numeral <b>30</b> is a vertical displacement search range indication signal, <b>31</b> is a horizontal displacement counter, <b>32</b> is a vertical displacement counter, <b>33</b> is a rotated angle counter, <b>34</b> is a corresponding pel determinator, and reference numeral <b>35</b> is a memory readout address generator. The pattern matching unit <b>213</b> and the minimum prediction error power determinator <b>216</b> perform the same operations as those of the corresponding structural elements illustrated in FIG. <b>47</b>.</p>
    <p>Referring to FIG. 6,</p>
    <p>dx represents a horizontal displacement search pel point;</p>
    <p>dy represents a vertical displacement search pel point;</p>
    <p>range_h_min represents a lower limit within a horizontal search range;</p>
    <p>range_h_max represents an upper limit within the horizontal search range;</p>
    <p>range_v_min represents a lower limit within a vertical search range;</p>
    <p>range_v_max represents an upper limit within the vertical search range;</p>
    <p>D_min represents the minimum prediction error power;</p>
    <p>D(dx, dy) is prediction error power produced when dx and dy are searched;</p>
    <p>(x, y) represents the position of a pel in an input picture block;</p>
    <p>(rx, ry) represents a pel in a reference picture corresponding to the pel (x, y);</p>
    <p>(rdx, rdy) are rotated angle parameters;</p>
    <p>D(dx, dy) represents a prediction error for the pel (x, y) when dx and dy are searched;</p>
    <p>f(x, y) represents the value of the pel (x, y) in the input picture;</p>
    <p>fr(x, y) represents the value of the pel (rx, ry) in the reference picture;</p>
    <p>MV_h represents a motion vector horizontal component;</p>
    <p>MV_v represents a motion vector vertical component;</p>
    <p>ix represents an offset value for horizontal displacement (constant);</p>
    <p>iy represents an offset value for vertical displacement (constant); and</p>
    <p>block_size represents the size of the input picture block.</p>
    <p>1) Motion Vector Detection By Using Block Matching</p>
    <p>The motion vector for the input picture block is determined through the block matching unit <b>20</b> by using the procedures and the operations described hereinbefore in relation to the conventional art. Consequently, the motion vector <b>217</b> and the minimum prediction error power D_BM <b>218</b> to be supplied from the block matching unit <b>20</b> are obtained. These operations correspond to step S<b>1</b> in FIG. <b>3</b>.</p>
    <p>2) Motion Vector Detection By Using Transformed Block Matching</p>
    <p>Then, the transformed block matching process is implemented through the transformed block matching unit <b>21</b> (step S<b>2</b> in FIG. <b>3</b>).</p>
    <p>Now, the transformed block matching process will be described in more detail. It is assumed herein that an input picture block which includes 8×8 integer pels is used as a unit for implementing the transformed block matching process.</p>
    <p>2-1) Operation Outline</p>
    <p>FIG. 4 shows an outline of the transformed block matching process implemented by the transformed block matching unit <b>21</b>.</p>
    <p>Referring to the figure, an input picture <b>27</b> is encoded by using motion compensated prediction. For example, a frame (picture) within the preprocessor <b>2</b>, that is, a reference picture <b>28</b> is a local decoded frame (picture) which had been encoded prior to the input picture <b>27</b> and stored in the frame memory <b>10</b>. ∘ represents a real sample integer pel in a frame portion represented by the luminance signal, X represents a half-pel interposed midway between the real sample integer pels. Herein, (the luminance block of) the input picture block is a picture portion including 8×8 (integer pels) in the input picture <b>27</b>, while the transformed block is a picture portion including □ pels in the reference picture <b>28</b> of possible prediction picture. The output of the frame memory <b>10</b> and the output of the preprocessor <b>2</b>, both of which are indicated by parenthesized reference numerals in FIGS. 1 and 2, are extracted and supplied to the transformed block matching unit <b>21</b> in the motion detector <b>8</b> and compared.</p>
    <p>In this embodiment, a transformed block is defined to be a picture portion rotated 45 degrees clockwise or counterclockwise relative to the luminance block of the reference picture and having four sides {square root over (2+L )} times scaled up from the sides of the input picture block. That is, a length of the reference picture block becomes 1/{square root over (2)} times of the length of the input picture. Intervals of the pels within the reference picture block becomes identical to horizontal/vertical length of intervals of the sample points of the input digital picture <b>1</b> of the frame. The transformed block in this embodiment includes only integer pels from the reference picture <b>28</b>. Namely, transformed block matching according to this embodiment is a process for finding, within a given search range, a transformed block that is most similar to the luminance block of the input picture block including 8×8 integer pels in the reference picture <b>28</b> as shown in FIG. <b>4</b>.</p>
    <p>2-2) Initial Settings (Transformed Block Search Range Setting, Initial Value Setting)</p>
    <p>Because an input picture block has a different shape than a possible prediction picture portion, it is necessary to specify the starting point of the motion vector to be detected. Namely, a one-to-one correspondence is established in advance between the integer pels constituting the input picture luminance block and the integer pels constituting a transformed block within a prediction picture portion.</p>
    <p>Herein, as shown in the dotted arrows in FIG. 4, the integer pel at the upper left corner of an input picture block is made to correspond to the integer pel at the left top of a transformed block. In other words, the possible prediction picture portion is a picture portion rotated 45 degrees clockwise relative to the transformed block within the reference picture <b>28</b> and having four sides 1/{square root over (2)} scaled down with respect to the sides of the transformed block. A change in this correspondence will lead to a change in the rotated direction of the possible prediction picture portion. In addition, because this one-to-one correspondence is also established between the integer pels of the input picture block and the integer pels of the possible prediction picture portion, motion detection similar to that implemented by using block matching can be performed.</p>
    <p>Namely, a shape for extracting picture portion of the reference picture <b>28</b> in FIG. 4 is patternized for block matching and indicated by a predetermined address (pel position) comprised of only integer pels. The difference between the pel of the address and the pel included in the input picture <b>27</b> corresponding to the pels of an original picture data is accumulated to determine a minimum prediction error power. Consequently, block matching is implemented only by indicating the address without complex operation, which enables a high-speed block matching. Furthermore, by providing a various addressing (indicating pel position), an indication can be flexible to extract the reference picture portion such as rotating, combination of rotating and enlarging or reducing other than simply enlarging, reducing.</p>
    <p>More specifically, a transformed block search range for implementing transformed block matching is set in the horizontal displacement counter <b>31</b> and the vertical displacement counter <b>32</b> through the horizontal displacement search range indication signal <b>29</b> and the vertical displacement search range indication signal <b>30</b>. Then, in the minimum prediction error power determinator <b>216</b>, the minimum prediction error power D_min is set to a maximum integer value MAXINT (OxFFFFFFFF, for example). These operations correspond to step S<b>4</b> in FIG. <b>6</b>.</p>
    <p>2-3) Block Transformation Parameters Setting</p>
    <p>In this embodiment, rdx and rdy shown in steps S<b>6</b> and S<b>8</b> in FIG. 6 will be used as block transformation parameters. The setting of these parameters is implemented by the rotated angle counter <b>33</b> such that the block to be transformed within the reference picture illustrated in FIG. 4 is rotated 45 degrees clockwise. The value of y is regarded as the initial value of rdx or rdy. Then, each time x is incremented, rdx is incremented, and rdy is decremented. These operations correspond to steps S<b>6</b> through S<b>8</b> in FIG. <b>6</b>. The setting in this way is implemented such that the block to be transformed is rotated clockwise. If the setting is done as rdy=−y at step S<b>6</b>, and ry=iy+(rdy++) at step S<b>8</b>, the block to be transformed will be rotated counterclockwise. The operation of step S<b>8</b> can be also represented as rx=ix+(rdx+1) and ry=iy+(rdy−1).</p>
    <p>Namely, an addressing of pels to be extracted from the reference picture <b>28</b> is indicated at step S<b>8</b>. An address of integer pels rotated 45 degrees clockwise is indicated as next pels rx, ry. At step S<b>12</b>, this operation is repeated until addressing of the pel equals to block size of x, and is also repeated until addressing of the pel equals to block size of y at step S<b>14</b>. In this way, a prediction error of the pel extracted by addressing at step S<b>8</b> is detected at step S<b>9</b>, and the error is accumulated at step S<b>10</b>. As shown in the operation flow of FIG. 6, no computation is needed, and high-speed processing can be performed. The operation of step S<b>10</b> can be also represented as D(dx, dy)=D(dx, dy)+D(x, y). Similarly, the operations of steps S<b>11</b>, S<b>13</b>, S<b>17</b> and S<b>19</b> are represented as x=x+1, y=y+1. This can be also said in flowcharts which will be described hereinafter.</p>
    <p>2-4) Possible Prediction picture Portion Readout Operation</p>
    <p>First, the reference picture portion pel (rx, ry) corresponding to the input picture block pel (x, y) in the input picture luminance block is determined. In other words, the initial positioning correspondence between the integer pels of the input picture block and the integer pels of the possible prediction picture portion shown in FIG. 4 is established. The corresponding pel determinator <b>34</b> implements this operation. As shown at step S<b>8</b> in FIG. 6, rx and ry can be obtained by adding rdx and rdy obtained in 2-3) to the predetermined offset value ix and iy, respectively. Then, data on a reference picture portion pel (rx+dx, ry+dy) is fetched from the frame memory. The memory readout address generator <b>35</b>, shown in FIG. 5, receives the value of dx from the horizontal displacement counter <b>31</b>, the value of dy from the vertical displacement counter <b>32</b>, and the value of rx and ry from the corresponding pel determinator <b>34</b> so as to generate the address for the pel (rx+dx, ry+dy) in the frame memory.</p>
    <p>2-5) Prediction Error Power Calculation</p>
    <p>First, the prediction error power D(dx, dy), produced from the motion vector indicating the pel position (dx, dy), is initialized to zero. This operation corresponds to step S<b>5</b> in FIG. <b>6</b>. The absolute value for a difference between the pel value read out in 2-4) and the value of the pel corresponding thereto in the input picture luminance block is accumulated to D(dx, dy). This operation is repeated until the condition x=y=block_size holds (in this case, the operation is repeated until block_size=8 holds). Then, the prediction error power D(dx, dy) produced from the motion vector indicating the pel position (dx, dy) can be obtained. The pattern matching unit <b>213</b> illustrated in FIG. 5 implements this operation. The pattern matching unit <b>213</b> supplies D(dx, dy) to the minimum prediction error power determinator <b>216</b> through the prediction error power signal <b>215</b>.</p>
    <p>These operations correspond to steps S<b>9</b> through S<b>14</b> in FIG. <b>6</b>.</p>
    <p>2-6) Minimum Prediction Error Power Updating</p>
    <p>It is then determined whether D(dx, dy) obtained in 2-5) has produced the minimum prediction error power among all the searched results that have been obtained so far. The minimum prediction error power determinator <b>216</b> illustrated in FIG. 5 makes this determination. This operation corresponds to step S<b>15</b> in FIG. <b>6</b>. The minimum prediction error power determinator <b>216</b> compares the value of the minimum prediction error power D_min retained therein with the value of D(dx, dy) received through the prediction error power signal <b>215</b>. If only the value of D(dx, dy) is smaller than D_min, D_min is updated to the value of D(dx, dy). Further, the minimum prediction error power determinator <b>216</b> retains the value of (dx, dy) at that time as the possible motion vector (MV_h, MV_v). This updating operation corresponds to step S<b>16</b> in FIG. <b>6</b>.</p>
    <p>2-7) Motion Vector Value Determination</p>
    <p>The above-mentioned operations from 2-2) through 2-6) are repeated for all the search pel points (dx, dy) within the transformed block search rage (steps S<b>17</b> through S<b>20</b> in FIG. <b>6</b>). The finally retained (MV_h, MV_v) within the minimum prediction error power determinator <b>216</b> are output as the motion vector <b>23</b>.</p>
    <p>As described above, a prediction picture portion which has produced the minimum prediction error power and therefore is most similar to an input picture block is searched. The displacement from the predetermined starting pel point of the selected prediction picture portion as a result of the transformed block search is obtained and expressed in terms of the motion vector <b>23</b>. The prediction error power D_DEF <b>24</b> produced at that time is also retained.</p>
    <p>3) Final Motion Compensated Prediction Mode Determination</p>
    <p>Next, the minimum prediction error power D_BM <b>218</b> supplied from the block matching unit <b>20</b> and the minimum prediction error power D_DEF <b>24</b> supplied from the transformed block matching unit <b>21</b> are compared by the motion compensated prediction mode determinator <b>22</b>. The smaller of the prediction error powers is selected to determine the final motion compensated mode between block matching and transformed block matching. This operation corresponds to step S<b>3</b> in FIG. <b>3</b>.</p>
    <p>The motion compensated prediction mode determinator <b>22</b> supplies the final, selected motion compensated prediction mode Indication signal <b>26</b> and the final motion vector <b>25</b> to the motion compensator <b>9</b> and the entropy encoder <b>18</b> in the form of motion parameters <b>11</b>.</p>
    <p>Next, the motion compensation process will be described.</p>
    <p>The motion compensator <b>9</b> implements the motion compensation process. The motion compensator <b>9</b> extracts prediction picture portion data from reference picture data according to the motion parameters <b>11</b> supplied from the motion detector <b>8</b>. The motion compensator <b>9</b> according to this embodiment supports both the conventional square-block-based block matching and transformed block matching using a specific transformed block as disclosed herein. According to the motion compensated prediction mode represented by the motion parameters <b>11</b>, the motion compensator <b>9</b> switches the operation between the conventional block matching and the transformed block matching to provide optimal performance.</p>
    <p>Now, the operation of the motion compensator <b>9</b> according to this embodiment will be described.</p>
    <p>FIG. 7 is a block diagram showing a configuration of the motion compensator <b>9</b> of FIG. <b>1</b>. FIG. 8 is a flow chart showing the operations of the motion compensator <b>9</b>.</p>
    <p>Referring to FIG. 7, reference numeral <b>37</b> is a corresponding pel determinator and reference numeral <b>38</b> is a memory readout address generator.</p>
    <p>1) Corresponding Pel Determination</p>
    <p>By using an input picture block position indication signal <b>206</b> and the motion parameters <b>11</b> supplied from the motion detector <b>8</b>, the sample points of the input picture block corresponding to those of the prediction picture portion in the reference picture <b>28</b> are determined. This operation corresponds to step S<b>21</b> in FIG. <b>8</b>. The corresponding pel determinator <b>37</b> illustrated in FIG. 7 implements this process. When the motion compensated prediction mode, represented by the motion parameters <b>11</b>, indicates block matching, the corresponding pels will become the sample points of a reference picture translated from the position indicated by the input picture block position indication signal <b>206</b> to the area specified by the motion vector. This operation corresponds to step S<b>204</b> in FIG. 44, and is an operation for determining the position of the pel (x+dx, y+dy) within the reference picture <b>28</b> when dx and dy are expressed in terms of the motion vector. When the motion compensated prediction mode represented by the motion parameters <b>11</b> indicates transformed block matching, the corresponding pels will become the sample points of a reference picture block rotated by a specified angle indicated by the input picture block position indication signal <b>206</b>, as described in the explanation of the motion detector <b>8</b> in the above 2-4), and then translated to the area specified by the motion vector. This operation corresponds to step S<b>9</b> in FIG. 6, and is an operation for determining the position of the pel (rx+dx, ry+dy) within the reference picture <b>28</b> when dx and dy are expressed in terms of the motion vector.</p>
    <p>2) Prediction picture Data Readout Operation</p>
    <p>The following operations correspond to steps S<b>22</b> through S<b>25</b> in FIG. <b>8</b>. The memory readout address generator <b>38</b> receives the output of the corresponding pel determinator <b>37</b> and generates the memory address specifying the position of the prediction picture portion within the reference picture <b>28</b> to be stored in the frame memory <b>10</b>.</p>
    <p>When the prediction picture portion includes half-pels, half-pel values are interpolated by the half-pel interpolator <b>232</b>. This operation is implemented before the prediction picture data is supplied from the motion compensator <b>9</b>, and corresponds to steps S<b>23</b> and S<b>24</b> in FIG. <b>8</b>. Whether or not the prediction picture portion includes half-pels is determined by the corresponding pel determinator <b>37</b> according to the motion vector value included in the motion parameters <b>11</b>. Then, the determined result is supplied to a selective switch <b>36</b>.</p>
    <p>The transformed block matching unit <b>21</b> configured as illustrated in FIG. 5 generates the corresponding pel points for a prediction picture portion that includes only real sample points as shown in FIG. <b>4</b>. However, when the prediction picture portion includes half-pels as well, the transformed block matching unit <b>42</b> in FIG. 13 will utilize configurations including the half-pel interpolator <b>232</b> as will be described later.</p>
    <p>After aforementioned processes, the final prediction picture data <b>12</b> is supplied. In this embodiment, a description is directed to the case where a transformed block has been formed by rotating an input picture block by 45 degrees. The rotated angle may be specified arbitrarily, such as 90 degrees, 135 degrees, 180 degrees, and the like. In addition, depending on the determination of the values of dx and dy, other forms of rotation can be implemented.</p>
    <p>Further, in this embodiment, a frame-based video encoder is described. When an input digital video sequence is separated into a plurality of picture objects through the preprocessor <b>2</b> and each of the picture objects (a picture portion having a common feature such as a motion feature, a pattern feature, etc. or a camera subject, and the like) is regarded as or is included in a block-based picture portion, the present invention may be applied to this object-based video encoder.</p>
    <p>Object-based encoding, for example, may be implemented by regarding a person with a still background as a single picture object as shown in FIG. 9, dividing the portion surrounding the person into a plurality of subblocks, and encoding data on the subblocks including the person as effective block data. Processes similar to those required for performing the above-described transformed block matching and motion compensation may also be applied to this case. This understanding also applies to other embodiments which will be described hereinafter.</p>
    <p>In this embodiment, an encoder using an orthogonal transform encoding method is described. It is to be understood that the present invention may be applied to encoders using other encoding methods for encoding a motion compensated prediction error. This understanding also applies to other embodiments which will be described hereinafter.</p>
    <heading>Embodiment 2</heading> <p>Approximate displacement of a picture portion which will be subject to the transformed block matching process is determined by the value of the motion vector representing the translational motion of an object. Consequently, when the picture portion which will be subjected to the transformed block matching process is defined to be the portion represented by the motion vector <b>217</b> supplied from the block matching unit <b>20</b> and when that picture portion is transformed and compared, the number of processing steps and the amount of processing time can be reduced. In this embodiment, a configuration which allows the above-mentioned determination of approximate displacement will be described. This method can be applied to other embodiments which will be described hereinafter.</p>
    <p>This embodiment will describe another configuration for motion detector <b>8</b>.</p>
    <p>FIG. 10 is a block diagram showing an internal configuration of the motion detector <b>8</b> <i>b </i>according to this embodiment. Reference numeral <b>39</b> is a transformed block matching unit, reference numeral <b>40</b> is an adder, and reference numeral <b>41</b> is an initial search pel position indication signal. The transformed block matching unit <b>39</b> uses the initial search pel position indication signal <b>41</b> instead of the input <b>206</b>. Other operations of the transformed block matching unit are the same as those of the transformed block matching unit <b>21</b> described in the first embodiment.</p>
    <p>FIG. 10 shows a concrete circuit of the motion detector for obtaining approximate values.</p>
    <p>Referring to FIG. 10, the motion vector <b>217</b> supplied from the block matching unit <b>20</b> is added to the input picture block data <b>205</b> by the adder <b>40</b>. Instead of the input picture block position indication signal, the added result of the adder <b>40</b> is supplied to the transformed block matching unit <b>39</b> as the initial search pel position indication signal <b>41</b>. The transformed block search range set through the horizontal displacement search range indication signal <b>29</b> and the vertical displacement search range indication signal <b>30</b> is set so as to be smaller than those set in the first embodiment. Thus, the time required for implementing the repetitive operations from steps S<b>17</b> through S<b>20</b> in FIG. 6 can be reduced.</p>
    <heading>Embodiment 3</heading> <p>In the first and second embodiments, the transformed block portion in the reference picture <b>28</b> includes only integer pels. In this embodiment, the transformed block in the reference picture <b>28</b> includes half-pels as well as integer pels.</p>
    <p>This embodiment is different from the first embodiment in that the motion detector <b>8</b> and the motion compensator <b>9</b> illustrated in FIG. 1 have different internal configurations, respectively. The operation of the transformed block matching unit in a motion detector and the operation of the corresponding pel determinator of a motion compensator according to this embodiment are different from those in the first embodiment. The operations of other elements are the same as those in the first embodiment. Consequently, the following description will be directed to the operation of the transformed block matching unit and the operation of the motion compensator. As in the first embodiment, the operation of the motion detector <b>8</b> <i>c </i>and the operation of the motion compensator <b>9</b> will be described separately.</p>
    <p>FIG. 11 is a block diagram showing an internal configuration of the motion detector <b>8</b> <i>c </i>according to this embodiment. FIG. 12 is a combination of explanatory drawings which describe an operation outline of the transformed block matching unit <b>42</b> which is one of important features of the present invention. FIG. 13 is a block diagram showing a detailed internal configuration of the transformed block matching unit <b>42</b>. FIG. 14 is a flow chart showing the operations of the transformed block matching unit <b>42</b>.</p>
    <p>Referring to these figures, like structural elements and like steps which are given like reference numerals and signs represent the same elements or perform the same processes.</p>
    <p>First, the operation of the transformed block matching unit <b>42</b> will be described.</p>
    <p>1) Operation Outline</p>
    <p>FIG. 12 is a combination of explanatory drawings which describe an operation outline of the transformed block matching unit <b>42</b>.</p>
    <p>Referring to the figure, as in the first embodiment, reference numerals <b>27</b> and <b>28</b> refer to an input picture to be predicted and a reference picture, respectively. Reference sign ∘ means a real sample point (integer pel) in a frame represented by luminance signals, and reference sign X means an interposed pel (half-pel) interposed midway between the integer pels. It is assumed herein that the input picture <b>27</b> including 8×8 (integer pels) is (a luminance block of) an input picture block. A picture portion comprised of □ pels within the reference picture <b>28</b> is a transformed block in a possible prediction picture portion herein.</p>
    <p>In this embodiment, a transformed block is defined to be a picture portion rotated 45 degrees clockwise or counterclockwise relative to the luminance block and having four sides 1/{square root over (2)} times scaled down from the sides of the input picture block. That is, a length of the reference picture block becomes {square root over (2+L )} times of the length of the input picture. Intervals of the pels within the reference picture block becomes identical to horizontal/vertical length of intervals of the sample points of the input digital picture <b>1</b> of the frame. This block includes half-pels in addition to integer pels within the reference picture <b>28</b>. Transformed block matching according to this embodiment is a process for finding, within the reference picture <b>28</b>, a transformed block portion which is most similar to the input picture luminance block including 8×8 samples (hereinafter, samples mean integer pels or half-pels) as illustrated in FIG. <b>12</b>.</p>
    <p>2) Initial Settings (Transformed Block Search Range and Initial Value Setting)</p>
    <p>As in the first embodiment, a one-to-one correspondence is established in advance between the integer or half-pels constituting the input picture luminance block and the integer or half-pels constituting a transformed block within a possible prediction picture portion. Herein, as shown in the dotted arrows in FIG. 12, the pel at the upper left corner of the input picture block is made to correspond to the pel at the left top of the transformed block. Referring to the figure, the pel at the left top of the transformed block is a half-pel which indicates that the motion vector represents a picture portion comprised of half-pels in addition to integer pels. A possible prediction picture portion herein means a picture portion rotated 45 degrees clockwise relative to the transformed block within the reference picture <b>28</b> and having four sides {square root over (2+L )} times scaled up with respect to the sides of the transformed block in the reference picture. A change in this correspondence will lead to a change in the rotated direction of the possible prediction picture portion. In addition, establishment of this one-to-one correspondence between the pels will lead to other one-to-one correspondences between the pels of the transformed block and the pels of the input picture block. This one-to-one correspondence allows motion detection similar to that implemented by using block matching. The operations needed for setting the transformed block search range by using the transformed block matching unit are the same as those described in the first embodiment. The structural elements needed for performing transformed block search range setting are illustrated in FIG. <b>13</b>. This operation corresponds to step S<b>26</b> in FIG. <b>14</b>.</p>
    <p>3) Block Transformation Parameters Setting</p>
    <p>As in the first embodiment, rdx and rdy shown in FIG. 14 will be used as block transformation parameters. The setting of these parameters is implemented by the rotated angle counter <b>45</b>. The value of y is regarded as the initial value of rdx or rdy. Then, each time x is incremented, rdx is incremented by 0.5, and rdy is decremented by 0.5. These operations correspond to steps S<b>28</b> through S<b>30</b> in FIG. <b>14</b>. The setting of the parameters rdx and rdy in this way is implemented such that the block to be transformed is rotated clockwise. If the setting is performed as rdy=−y at step S<b>28</b> and ry=iy+(rdy+=0.5) at step S<b>30</b>, the block to be transformed will be rotated counterclockwise.</p>
    <p>4) Possible Prediction picture Portion Readout Operation</p>
    <p>First, the reference picture portion pel (rx, ry) corresponding to the input picture block pel (x, y) in the input picture luminance block is determined. The corresponding pel determinator <b>46</b> implements this operation. As shown in step S<b>30</b> in FIG. 14, rx and ry can be obtained by adding rdx and rdy obtained in 3) to the predetermined offset values ix and iy, respectively. Then, data on the reference picture portion pel (rx+dx, ry+dy) is fetched from the frame memory.</p>
    <p>Next, pel (rx+dx, ry+dy) is fetched from the frame memory. The memory readout address generator <b>47</b> illustrated in FIG. 13 receives the value of dx from the horizontal displacement counter <b>31</b>, the value of dy from the vertical displacement counter <b>32</b>, and the value of rx and ry from the corresponding pel determinator <b>46</b> so as to generate the address for the pel (rx+dx, ry+dy) to be stored in the frame memory. Readout data is used to interpolate the value of a half-pel through the half-pel interpolator <b>232</b>, if necessary, as shown in step S<b>31</b> in FIG. <b>14</b>.</p>
    <p>5) Prediction Error Power Calculation</p>
    <p>First, the prediction error power D(dx, dy) produced when dx and dy are expressed in terms of the motion vector is initialized to zero. This operation corresponds to step S<b>27</b> in FIG. <b>14</b>. The absolute value for a difference between the pel value read out in 4) and the corresponding pel value of the input picture luminance block is accumulated to D(dx, dy). This operation is repeated until the condition x=y=block_size (in this case, block_size=8) holds. Then, the prediction error power D(dx, dy) produced when the motion vector indicates the pel position (dx, dy) can be obtained. The pattern matching unit <b>213</b> illustrated in FIG. 13 implements this operation. The pattern matching unit <b>213</b> supplies D(dx, dy) to the minimum prediction error power determinator <b>216</b> using the prediction error power signal <b>215</b>. These operations correspond to steps S<b>32</b> through S<b>37</b> in FIG. <b>14</b>.</p>
    <p>6) Minimum Prediction Error Power Updating</p>
    <p>It is determined whether D(dx, dy) obtained in 5) has produced the minimum prediction error power among all the searched results that have been obtained so far. The minimum prediction error power determinator <b>216</b> illustrated in FIG. 13 makes this determination. This determination process corresponds to step S<b>38</b> in FIG. <b>14</b>. The determination process is the same as that in the first embodiment. The value of (dx, dy) at that time is retained as the possible motion vector (MV_h, MV_v) This updating process corresponds to step S<b>39</b> in FIG. <b>14</b>.</p>
    <p>7) Motion Vector Value Determination</p>
    <p>The above-mentioned operations from 2) through 6) are repeated for all the search pel points (dx, dy) within the transformed block search range (these operations correspond to steps S<b>40</b> through S<b>43</b> in FIG. <b>14</b>). The finally retained value (MV_h, MV_v) within the minimum prediction error power determinator <b>216</b> are output as the motion vector <b>43</b>.</p>
    <p>As described above, a prediction picture portion which has produced the minimum prediction error power and, therefore, is most similar to an input picture block is searched by using transformed block matching. As the result of the prediction picture portion search, the displacement from the predetermined starting pel point of the selected prediction picture portion is obtained and expressed in terms of the motion vector <b>43</b>. The prediction error power D_DEF <b>44</b> at that time is also retained.</p>
    <p>The above-mentioned motion vector <b>43</b> and the prediction error power D_DEF <b>44</b> are used to determine the final motion compensated mode. The final motion compensated mode is thus determined. This determination method is the same as that in the first embodiment.</p>
    <p>Next, the motion compensation process will be described.</p>
    <p>The motion compensator <b>9</b> implements the motion compensation process. In this embodiment, the operation of the corresponding pel determinator <b>37</b> is different from that in the first embodiment. Consequently, the following description will be directed only to the operation of the corresponding pel determinator <b>37</b>. The overall motion compensation operational flow is illustrated in the form of the flow chart in FIG. <b>8</b>.</p>
    <p>In this embodiment, the corresponding pel determination is performed as follows:</p>
    <p>When the motion compensated prediction mode represented by the motion parameters <b>11</b> indicates block matching, the corresponding pels will become the sample points of a reference picture portion translated from the position indicated by the input picture block position indication signal <b>206</b> to the area specified by the motion vector. This process corresponds to step S<b>204</b> in FIG. 44, and represents an operation for determining the position of the pel (x+dx, y+dy) within the reference picture <b>28</b> when dx and dy are expressed in terms of the motion vector.</p>
    <p>When the motion compensated prediction mode represented by motion parameters <b>11</b> indicates transformed block matching, the corresponding pels are expressed with the sample points of a reference picture portion rotated by a specified angle indicated by the input picture block position indication signal <b>206</b> and then translated to the area specified by the motion vector as described in 4) of the explanation of the motion detector <b>8</b>. This operation corresponds to step S<b>32</b> in FIG. 14, and is an operation for determining the position of the pel (rx+dx, ry+dy) within the reference picture <b>28</b> when dx and dy are expressed in terms of the motion vector.</p>
    <p>The prediction picture data readout operation and prediction picture generation are the same as those described in the first embodiment.</p>
    <heading>Embodiment 4</heading> <p>In this embodiment, a description will be directed to the case where a transformed block is a reduced input picture block. With regard to the case where a transformed block is enlarged relative to an input picture block, the operation will be similar and the description will be omitted. Thus, simplified transformed block matching and motion compensation will be described.</p>
    <p>Now, the operation of the transformed block matching unit <b>42</b> <i>b </i>in the motion detector and the operation of the corresponding pel determinator in the motion compensator will be described in detail with reference to FIG. <b>16</b>. For simplifying the description, it is assumed herein that the transformed block matching unit <b>42</b> <i>b </i>is a variation of the transformed block matching unit <b>42</b> illustrated in FIG. <b>13</b>. The transformed block matching unit <b>42</b> <i>b </i>receives the same inputs as those with the transformed block matching unit <b>42</b>, and supplies variations of the motion vector <b>43</b> and the prediction error power <b>44</b>. The corresponding pel determinator in the motion compensator <b>9</b> is also a variation of the corresponding pel determinator <b>37</b> illustrated in FIG. <b>7</b>. Consequently, this embodiment uses the transformed block matching unit <b>42</b> <i>b </i>and the corresponding pel determinator <b>37</b>.</p>
    <p>FIG. 15 is a combination of explanatory drawings showing the operation outline of the transformed block matching unit <b>42</b> <i>b </i>according to this embodiment. FIG. 16 is a block diagram showing a detailed internal configuration of the transformed block matching unit <b>42</b> <i>b</i>. FIG. 17 is a flow chart showing the operations of the transformed block matching unit <b>42</b> <i>b. </i> </p>
    <p>In these figures, the elements and steps assigned the same reference numerals with the above-mentioned drawings will mean the same elements and operations.</p>
    <p>First, the operation of the transformed block matching unit <b>42</b> <i>b </i>will be described.</p>
    <p>1) Operation Outline</p>
    <p>FIG. 15 shows the operation outline of the transformed block matching unit <b>42</b> <i>b</i>. An input picture <b>27</b>, a reference picture <b>28</b> and signs used within the pictures are the same as described above. In this embodiment, a transformed block is defined to be a picture portion having four sides which are half as large as those of an input picture luminance block. Transformed block matching according to this embodiment is a process for finding, within the given search range, a transformed reference picture block portion most similar to the input picture luminance block consisting of 8×8 samples as illustrated in FIG. <b>15</b>.</p>
    <p>2) Initial Settings (Transformed Block Search Range Setting and Initial Value Setting)</p>
    <p>As in the first embodiment, a one-to-one correspondence is established in advance between the pels constituting an input picture luminance block and the pels constituting a transformed block within a possible prediction picture portion. Herein, as shown in the dotted arrows in FIG. 15, the pel at the upper left corner of the input picture block is made to correspond to the pel at the left top of the transformed block. Because a one-to-one correspondence is established between the pels of the input picture block and the pels of the possible prediction picture portion, motion detection similar to that implemented by block matching can be performed. The operation needed for setting the transformed block search range by using the transformed block matching unit is the same as that in the first embodiment. FIG. 16 illustrates the structural elements needed for performing transformed block search range setting. This operation corresponds to step S<b>44</b> in FIG. <b>17</b>.</p>
    <p>3) Possible Prediction picture Portion Readout Operation</p>
    <p>In this embodiment, specific block transformation parameters will not be used. As shown in step S<b>47</b> in FIG. 17, the reference picture portion pel (sx, sy) corresponding to the input picture luminance block pel (x, y) is obtained by adding x/2 and y/2 to the horizontal offset constant ix and the vertical offset constant iy, respectively. This corresponding pel determination is made by the corresponding pel determinator <b>48</b>. Then, the reference picture portion pel (sx+dx, sy+dy) is fetched from the frame memory <b>10</b>. The memory readout address generator <b>49</b> illustrated in FIG. 16 receives the value of dx from the horizontal displacement counter <b>31</b>, the value of dy from the vertical displacement counter <b>32</b>, and the value of sx and sy from the corresponding pel determinator <b>48</b> so as to generate the address for the pel (sx+dx, sy+dy) to be stored in the frame memory. Readout data is used to generate the value of a half-pel through the half-pel interpolator <b>232</b>, if necessary, as shown in step S<b>48</b> in FIG. <b>17</b>.</p>
    <p>4) Prediction Error Power Calculation</p>
    <p>First, the prediction error power D(dx, dy) produced when dx and dy are expressed in terms of the motion vector is initialized to zero. This operation corresponds to step S<b>45</b> in FIG. <b>17</b>. The absolute value for a difference between the pel value read out in 3) and the value of the corresponding pel in the input picture luminance block is accumulated to D(dx, dy) in step S<b>50</b>. This operation is repeated until the condition x=y=block_size (in this example, block_size=8) holds in steps S<b>52</b> and S<b>54</b>. Then, the prediction error power D(dx, dy) produced when the motion vector indicates the pel position (dx, dy) can be obtained. The pattern matching unit <b>213</b> illustrated in FIG. 16 implements this operation. The pattern matching unit <b>213</b> supplies D(dx, dy) to the minimum prediction error power determinator <b>216</b> through the prediction error power signal <b>215</b>. These operations correspond to steps S<b>49</b> through S<b>54</b> in FIG. <b>17</b>.</p>
    <p>5) Minimum Prediction Error Power Updating</p>
    <p>Then, it is determined whether D(dx, dy) obtained in 4) has produced the minimum prediction error power among all the searched results that have been obtained so far. The minimum prediction error power determinator <b>216</b> illustrated in FIG. 16 makes this determination. This operation corresponds to step S<b>55</b> in FIG. <b>17</b>. The determination process is the same as that in the first embodiment. The value of (dx, dy) at that time is retained as the possible motion vector. This updating process corresponds to step S<b>56</b> in FIG. <b>17</b>.</p>
    <p>6) Motion Vector Value Determination</p>
    <p>The above-mentioned operations from 2) through 5) are repeated for all the search pel points (dx, dy) within the transformed block search range as indicated by steps from S<b>57</b> through S<b>60</b> in FIG. <b>17</b>. The finally retained dx and dy within the minimum prediction error power determinator <b>216</b> are output as the motion vector <b>43</b>.</p>
    <p>As described above, a prediction picture portion most similar to an input picture block with the minimum prediction error power produced is searched by using transformed block matching. As the result of the prediction picture portion search, the displacement from the predetermined starting pel of the selected prediction picture portion is obtained and expressed in terms of the motion vector <b>43</b>. The prediction error power D_DEF <b>44</b> at that time is also retained.</p>
    <p>The above-mentioned motion vector <b>43</b> and the prediction error power D_DEF <b>44</b> are used to determine the final motion compensated mode. This determination process is the same as that in the first embodiment.</p>
    <p>Next, the motion compensation process will be described.</p>
    <p>The motion compensator <b>9</b> implements the motion compensation process. In this embodiment, the operation of the corresponding pel determinator <b>37</b> is different from that in the first embodiment. Consequently, a description will be directed only to the operation of the corresponding pel determinator <b>37</b>. The overall motion compensation operational flow is illustrated in the form of the flow chart in FIG. <b>8</b>.</p>
    <p>In this embodiment, the corresponding pel determination is made as follows:</p>
    <p>When the motion compensated prediction mode represented by the motion parameters <b>11</b> indicates block matching, the corresponding pels are expressed with the sample points of a picture portion to which the displacement indicated by the input picture block position indication signal <b>206</b> is added and then translated to the area specified by the motion vector. This process corresponds to step S<b>204</b> in FIG. 44, and represents an operation for determining the position of the pel (x+dx, y+dy) within the reference picture <b>28</b> when dx and dy are expressed in terms of the motion vector.</p>
    <p>When the motion compensated prediction mode represented by motion parameters <b>11</b> indicates transformed block matching, the corresponding pels are expressed with the sample points of a picture portion to which the displacement indicated by the input picture block position indication signal <b>206</b> is added and then translated to the area specified by the motion vector. This operation corresponds to step S<b>47</b> in FIG. <b>17</b> and represents an operation for determining the position of the pel (sx+dx, sy+dy) within the reference picture <b>28</b> when dx and dy are expressed in terms of the motion vector. The prediction picture portion data readout operation and prediction picture portion generation are performed as in the first embodiment.</p>
    <p>A transformed block according to any one of the above-mentioned embodiments may have any arbitrary shape according to the following two assumptions:</p>
    <p>1) One-to-one correspondence is established between the pels of an input picture block and the pels of a prediction picture portion.</p>
    <p>2) Corresponding pels of the prediction picture portion in the reference picture are integer pels. A transformed block may have such a shape as that shown in FIG. 18 or FIG. <b>19</b>. In addition, prediction picture portions may be reduced or enlarged at an arbitrary ratio so as to be transformed into various shapes to allow block matching. By defining various shapes of the prediction picture portion in advance in this way, transformed block matching which gives the best match can be selected. In this case, the selected transformed block data is represented by the motion parameters <b>11</b> and is supplied to the entropy encoder <b>18</b>.</p>
    <p>According to the above-mentioned embodiments, if only half-pels are interpolated, motion compensation for various types of motion including rotation and scaling down of an object can be performed. Furthermore, complex arithmetic operations which are required when using the affine motion model are not needed for this invention. Thus, appropriate motion compensated prediction can be implemented even for the picture portion where the prediction error cannot be minimized merely by using the motion vector representing a translational amount, or for which correct prediction cannot be performed.</p>
    <p>In the embodiments described hereinbefore, a description has been directed to the case where integer pels or half-pels are used as the predetermined fixed pels of a transformed block. Other pels interposed between the integer pels at a 1:3 ratio, for example, may also be used as the fixed pels. In this case as well, pel interpolation is not needed in the transformed block matching process, which is different from the conventional compensated prediction using the affine motion model. Thus, the number of steps needed for the process can be reduced, and high-speed processing can be performed.</p>
    <heading>Embodiment 5</heading> <p>In the embodiments described hereinbefore, determination of block transformation parameters for each pel or the process for expressing the position of each pel in terms of the coordinates therefor is performed. Alternatively, corresponding pel determination may be made by preparing in advance a transformation pattern table such as a ROM storing the corresponding coordinates for the respective pels and determining the respective pels of a prediction picture portion based on the coordinates extracted from the transformation pattern table. Thus, transformed block matching and motion compensation having an arbitrary correspondence relationship between the pels of an input picture block and the pels of a prediction picture portion can be efficiently performed.</p>
    <p>Now, the above-mentioned operation will be described in conjunction with the first embodiment.</p>
    <p>FIG. 20 is a block diagram showing another internal configuration (corresponding pel determinator <b>34</b> <i>b</i>) of the corresponding pel determinator <b>34</b> illustrated in FIG. 5 which realizes another embodiment of the present invention. In this embodiment, corresponding pel determination can be made by storing rdx and rdy in the ROM and determining the pel (rx, ry) corresponding to the pel (x, y) using rdx and rdy fetched from the ROM. Consequently, incrementation or decrementation of the transformation parameters rdx and rdy as shown in step S<b>8</b> in FIG. 6 are not needed. For making corresponding pel determinations, the rotated angle counter <b>33</b> illustrated in FIG. 5 is also not needed in this embodiment. Instead, corresponding pel determination can be made by providing a ROM table (transformation pattern table <b>100</b>) inside the corresponding pel determinator <b>34</b> <i>b</i>, as shown in FIG. <b>20</b>. Through the corresponding pel determinator <b>34</b> <i>b</i>, the transformation parameters rdx and rdy are fetched from the transformation pattern table <b>100</b> based on the respective values x and y of the pel within an input picture block. Then, the transformation parameters rdx and rdy are supplied to the adder <b>110</b> and added to the motion vector data so as to determine the corresponding pel values. Then, the determined corresponding pel value data is supplied to the memory readout address generator <b>35</b>. This method can also be applied to other embodiments described hereinbefore. Thus, if only a few ROM (transformation pattern table <b>100</b>) memories are added to the corresponding pel determinator circuit <b>34</b> <i>b</i>, the structural element for performing arithmetic operations for determining corresponding pels is not necessary and need not be provided. In this way, the corresponding pel determinator circuit <b>34</b> can be simplified and the number of the corresponding pel determination processing steps can be reduced. In addition, corresponding pel determinations can be made for a transformed block as shown in FIG. 21 which cannot be simply expressed in terms of transformation parameters. Thus, a transformation pattern library storing more abundant transformation patterns can be conceived.</p>
    <heading>Embodiment 6</heading> <p>In this embodiment, an encoder is disclosed which can decrease variations in the frequency characteristic of a prediction picture portion that is separated from the reference picture as a transformed block and which can reduce mismatches when implementing prediction for an input picture block.</p>
    <p>When a prediction picture portion includes half-pels as well as integer pels, a variation in the spatial frequency characteristic occurs between the integer pels and the half-pels. An input picture block includes only integer pels. Consequently, this variation in the spatial frequency characteristic can become a factor which may cause a prediction mismatch. In this embodiment, after a transformed block has been defined similarly as in the other embodiments described hereinbefore, filtering of integer pels will be performed.</p>
    <p>Interpolation of half-pels can be performed by filtering integer pels thereabout at a [1/2,1/2] ratio. In order to do so, a low-pass filter filtering only equal to or lower frequency component than the cos(ωt/2) is provided. Prediction picture portions defined in the embodiments described hereinbefore include both unfiltered integer pels and filtered half-pels generated by the above-mentioned filtering. Consequently, a variation in the spatial frequency characteristic will occur in the prediction picture portions. When prediction accuracy has been reduced due to this variation, the unfiltered integer pels should also be filtered by the filter having a filter characteristic that is similar to the filter characteristic described above. Thus, the prediction accuracy can be enhanced.</p>
    <p>FIG. 22 is a explanatory drawing showing an example of the above-mentioned filtering operation. FIG. 22 shows the case where a low-pass filter F is used to filter the integer pels at a [1/8,6/8,1/8] ratio as shown in the following equations (7).</p>
    <p>
      <maths> <formula-text>↑(x,y)=F<sub>y</sub>[F<sub>x</sub>[I(x, y)]]</formula-text> </maths> </p>
    <p>
      <maths> <formula-text>F[I(n)]=(I(n−1)+6*I(n)+I(n+1))/8  (7)</formula-text> </maths> </p>
    <p>This filter filters equal to or lower frequency component than {cos(ωt/2)}2 so as to reduce a deviation in the spatial frequency characteristic in a prediction picture portion. After the low-pass filtering, a one-to-one correspondence is established between the pels of an input picture block and a prediction picture portion. Then, transformed block matching, determination of the motion vector, and motion compensated prediction mode determination are made, as in the embodiments described hereinbefore.</p>
    <p>A filtering operation and associated configurations of the transformed block matching unit and the motion compensator for implementing the filtering operation will now be described.</p>
    <p>In this embodiment, the transformed block matching unit and the motion compensator have configurations which are different from those described in the above-mentioned embodiments. A transformed block according to this embodiment is simply a reduced transformed block described in the fourth embodiment. In this embodiment, the transformed block matching unit is regarded as a variation of the transformed block matching unit <b>42</b> within the motion detector <b>8</b> <i>c </i>and is indicated by reference numeral <b>42</b> <i>c</i>. Furthermore, the motion compensator is also regarded as a variation of the motion compensator <b>9</b> and is indicated by reference numeral <b>9</b> <i>b. </i> </p>
    <p>FIG. 23 is a combination of explanatory drawings which describe an operation outline of the transformed block matching unit <b>42</b> <i>c </i>according to this embodiment. FIG. 24 is a block diagram showing a detailed internal configuration of the transformed block matching unit <b>42</b> <i>c</i>. FIG. 25 is a flow chart showing the operations of the transformed-block matching unit <b>42</b> <i>c </i>according to this embodiment.</p>
    <p>In the drawings, elements or steps assigned the same reference numerals as those in the aforementioned drawings mean the same elements or operations.</p>
    <p>First, the operation of the transformed block matching unit <b>42</b> <i>c </i>will be described. With regard to operations similar to those described in the fourth embodiment, the description about these operation will be omitted.</p>
    <p>1) Operation Outline</p>
    <p>A transformed block can be defined in quite the same way as in the fourth embodiment. This embodiment is different from the fourth embodiment in that the integer pels of a prediction picture portion are filtered. As shown in FIG. 23, Δ pels are provided in the reference picture so as to be filtered. A transformed block is comprised of Δ and □ pels.</p>
    <p>2) Initial Settings (Transformed Block Search Range and Initial Value Setting)</p>
    <p>These operations are performed in the same way as those described in the fourth embodiment.</p>
    <p>3) Possible Prediction picture Portion Readout Operation</p>
    <p>First, the pel (sx, sy) corresponding to the pel (x, y) in an input picture block is determined in the same way as that described in the fourth embodiment. Next, the reference picture pel (sx+dx, sy+dy) is fetched from the frame memory. Then, it is determined whether the pel (sx+dx, sy+dy) is an integer pel or a half-pel. In other words, it is determined whether sx+dx and sy+dy are both half-pel components. The corresponding pel determinator <b>48</b> illustrated in FIG. 24 makes this determination. This operation corresponds to step S<b>61</b> in FIG. <b>25</b>. When the reference picture pel (sx+dx, sy+dy) has been determined to be a half-pel, half-pel is interpolated by the half-pel interpolater <b>232</b>. When the reference picture pel (sx+dx, sy+dy) has been determined to be an integer pel, a filter <b>50</b> implements integer-pel filtering as shown in FIG. <b>22</b>. This operation corresponds to step S<b>62</b> in FIG. <b>25</b>.</p>
    <p>With regard to the following:</p>
    <p>4) Prediction Error Power Calculation;</p>
    <p>5) Minimum Prediction Error Power Updating; and</p>
    <p>6) Motion Vector Value Determination,</p>
    <p>these operations are implemented in the same way as the operations described in the fourth embodiment.</p>
    <p>Next, the motion compensation process will be described.</p>
    <p>The motion compensator <b>9</b> <i>b </i>implements the motion compensation process.</p>
    <p>FIG. 26 is a block diagram showing an internal configuration of the motion compensator <b>9</b> <i>b</i>. FIG. 27 is a flow chart showing the operations of the motion compensator <b>9</b> <i>b </i>according to this embodiment.</p>
    <p>In this embodiment, the filter <b>50</b> is provided within the motion compensator <b>9</b> illustrated in FIG. <b>7</b>. The corresponding pel determinator <b>37</b> implements the same operations as described in the fourth embodiment. When the motion compensated prediction mode represented by the motion parameters <b>11</b> indicates block matching, the corresponding pel is expressed with a sample point of a reference picture portion translated from the input picture block position indication signal <b>206</b> to an area specified by the motion vector. This operation corresponds to step S<b>204</b> in FIG. <b>44</b>. The reference picture pel (x+dx, y+dy) when dx and dy are expressed in terms of the motion vector is thus determined.</p>
    <p>When the motion compensated prediction mode represented by the motion parameters <b>11</b> indicates transformed block matching, the reference picture pel (x+dx, y+dy) is expressed with a sample point included in a reference picture portion to which a displacement indicated by the input picture block position indication signal <b>206</b> is added and then translated by an area specified by the motion vector. This operation corresponds to step S<b>47</b> in FIG. <b>17</b>. The reference picture pel (sx+dx, sy+dy) within the reference picture <b>28</b> when dx and dy are expressed in terms of the motion vector is thus determined. In either case, it is determined whether the reference picture pel is an integer pel or a half-pel. When the reference picture pel has been determined to be an integer pel, the filtering of the pel as shown in FIG. 22 will be implemented in the same way as that implemented by the transformed block matching unit for generating a prediction picture portion. This filtering is implemented by the filter. The prediction picture portion data readout operation and prediction picture portion generation will be implemented in the same way as those described in the first embodiment.</p>
    <p>The transformed block matching unit <b>42</b> <i>c </i>may implement the search for an unfiltered prediction picture portion or a prediction picture portion filtered by the filter F and supply the searched result to the motion compensated prediction mode determinator <b>22</b>. Alternatively, the transformed block matching unit <b>42</b> <i>c </i>may implement the search for an unfiltered prediction picture portion alone and then filter the unfiltered prediction picture portion by using the filter F so as to determine the prediction picture portion exhibiting the greatest prediction accuracy.</p>
    <p>When the transformed block matching unit <b>42</b> <i>c </i>has a facility for turning the filter F ON or OFF, the filter ON/OFF information should be included in the motion parameters <b>11</b>.</p>
    <p>According to this embodiment, only the filtering of integer pels can reduce a deviation in the spatial frequency characteristic of a prediction picture portion. Thus, appropriate motion compensated prediction can be performed even for an input picture block where only the motion vector indicating a translational displacement cannot minimize a prediction error, or for which correct prediction cannot be performed.</p>
    <heading>Embodiment 7</heading> <p>FIG. 28 is a block diagram showing a configuration of a video decoder for decoding compression encoded digital picture data, decompressing the compression encoded digital picture data, and reproducing a picture using the motion compensated prediction method according to this embodiment. Herein, a video decoder for receiving compression encoded data (which will be referred to as a bitstream) <b>19</b> generated by the video encoder described in the first embodiment, decompressing the bitstream, and reproducing a complete picture will be described.</p>
    <p>Referring to FIG. 28, reference numeral <b>51</b> indicates an entropy decoder, reference numeral <b>6</b> is a dequantizer, reference numeral <b>7</b> is an inverse orthogonal transformer, <b>53</b> is a decoding adder, <b>54</b> is a frame memory, and <b>56</b> indicates a display controller.</p>
    <p>The decoder according to this embodiment is characterized in the configuration and operation of the motion compensator <b>9</b>. The configuration and operation of each element other than the motion compensator <b>9</b> has been described, and a detailed explanation will be omitted. The motion compensator <b>9</b> is the same as that illustrated in FIG. <b>1</b>. Consequently, an internal configuration of the motion compensator <b>9</b> is as illustrated in FIG. 7, and an operation flow chart thereof is as illustrated in FIG. <b>8</b>.</p>
    <p>The operation of the motion compensator <b>9</b> having the above-mentioned configuration will be described.</p>
    <p>First, the bitstream is analyzed through the entropy decoder <b>51</b> and separated into a plurality of encoded data. A quantized orthogonal transforming coefficient <b>52</b> is supplied to the dequantizer <b>6</b> and dequantized using a dequantizing step parameter <b>17</b>. The dequantized result is subjected to an inverse orthogonal transformation through the inverse orthogonal transformer <b>7</b> and supplied to the decoding adder <b>53</b>. The inverse orthogonal transformer is the same as that within the encoder, e.g., DCT, described hereinbefore.</p>
    <p>The following three kinds of data are supplied to the motion compensator <b>9</b> as the motion parameter <b>11</b>: the motion vector <b>25</b> decoded from the bitstream by the entropy decoder <b>51</b>; the transformation pattern data <b>26</b> <i>a</i>; and the input picture portion position data <b>27</b> <i>a </i>showing the position of the input picture portion (in this embodiment, a fixed size of block) in the display. In this case, the motion vector <b>25</b> and the input picture portion position data is fixed values for each input picture portion. The transformation pattern data <b>26</b> <i>a </i>can be fixed value for each input picture portion, or can be encoded for using the same transformation pattern data for all input picture portions included in larger picture consisting of a plurality of input picture portions (e.g., a picture frame, and VOP disclosed in ISO/IEC JTC1/SC29/WG11). The motion compensator <b>9</b> fetches the prediction picture data <b>12</b> from the reference picture data stored in the frame memory <b>54</b> based on these three kinds of data. The process for the prediction picture generation will be described in the explanation of the operation of the motion compensator <b>9</b>.</p>
    <p>The motion parameters <b>11</b> decoded by the entropy decoder <b>51</b> are supplied to the motion compensator <b>9</b>.</p>
    <p>The motion compensator <b>9</b> fetches the prediction picture data <b>12</b> from the reference picture data stored in the frame memory <b>54</b> according to the motion parameters <b>11</b>. According to the motion compensated prediction method of the present invention, a one-to-one correspondence is established between the pels of an input picture block and the pels of a prediction picture portion. Consequently, as in the motion compensated prediction using the conventional block matching method, a single prediction picture portion is determined by using the motion parameters <b>11</b>.</p>
    <p>Based on the value of the intra-/interframe encoding indication flag <b>16</b>, intraframe encoding or interframe encoding is selected. Then, when intraframe encoding has been selected, the decoding adder <b>53</b> outputs the output of the inverse orthogonal transformer as the decoded picture data <b>55</b>. While on the other hand, when interframe encoding has been selected, the decoding adder <b>53</b> adds the prediction picture data <b>12</b> to the output of the inverse orthogonal transformer and outputs the added result as the decoded picture data <b>55</b>. The decoded picture data <b>55</b> is supplied to the display controller <b>56</b> to be displayed on a display device (not shown). In addition, the decoded picture data <b>55</b> is also written into the frame memory <b>54</b> to be used as reference picture data for the subsequent frame decoding process.</p>
    <p>Next, the prediction picture generation by the motion compensator <b>9</b> is explained hereinafter.</p>
    <p>In the picture prediction method according to this embodiment, the correspondence between the pels consisting the input picture portion and the pels consisting the prediction picture is predetermined by the transformation pattern data <b>26</b> <i>a</i>. Accordingly, the prediction picture can be generated by a simple address computation based on displacement by the motion vector <b>25</b> and adjustment by the transformation pattern data <b>26</b> <i>a </i>and interpolation.</p>
    <p>FIG. 29 shows an internal configuration of the motion compensator <b>9</b>.</p>
    <p>In the figure, a reference numeral <b>37</b> indicates the corresponding pel determinator and <b>38</b> indicates a memory readout address generator.</p>
    <p>FIG. 30 is a flowchart showing an operation of the motion compensator.</p>
    <p>FIG. 31 explains that a block extracted from the reference picture is transferred by an amount indicated by the motion vector to the pel position of the input picture. FIG. 32 explains that addressing is done to the transferred block by the predetermined transformation pattern.</p>
    <p>In the above drawings, ∘ shows integer pel and X shows half-pel.</p>
    <p>In the following, an operation of the motion compensator <b>9</b> of this embodiment will be explained referring to FIGS. 29 and 30.</p>
    <p>1) Corresponding Pel Determination</p>
    <p>First, the corresponding pel determinator <b>37</b> computes a sample position of the prediction picture corresponding to each pel of the input picture based on the received motion vector <b>25</b> and the transformation pattern data <b>26</b> <i>a</i>. A reference position of the prediction picture corresponding to the present position of the input picture portion is determined based on the motion vector <b>25</b>. This process (step S<b>71</b> of FIG. 30) corresponds to determining (i′, j′)=(i+dx, j+dy) when the input picture portion position <b>27</b> <i>a </i>is assumed (i, j) and the motion vector <b>25</b> is assumed (dx, dy) as shown in FIG. <b>31</b>.</p>
    <p>Then, the pel (i′, j′) is adjusted based on the transformation pattern data <b>26</b> <i>a </i>and the sample position of the prediction picture is finally obtained. FIG. 32 shows an example where the transformation pattern data <b>26</b> <i>a </i>indicates “horizontal &amp; vertical ½ reduction”. By this transformation pattern data, an effective area of the prediction picture portion becomes ¼ of an effective area of the input picture portion positioned in the display. That is, the prediction picture is reduced from the input picture portion, thus the motion prediction accompanying enlargement and so on becomes efficient. Concrete position adjustment is implemented by obtaining the adjusted pel (i″, J″) corresponding to the pel (i′, j′) of the reference picture. This can be performed by the following operation (step S<b>72</b> of FIG. <b>30</b>). <maths> <math> <mtable> <mtr> <mtd> <mrow> <mtable> <mtr> <mtd> <mrow> <mi>for</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <mi>y</mi> <mo>=</mo> <mn>0</mn> </mrow> <mo>;</mo> <mrow> <mi>y</mi> <mo>&lt;</mo> <mi>block_height</mi> </mrow> <mo>;</mo> <mrow> <mi>y</mi> <mo>++</mo> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msup> <mi>j</mi> <mi>″</mi> </msup> <mo>=</mo> <mrow> <msup> <mi>j</mi> <mi>′</mi> </msup> <mo>+</mo> <mrow> <mi>y</mi> <mo>/</mo> <mn>2</mn> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>for</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <mi>x</mi> <mo>=</mo> <mn>0</mn> </mrow> <mo>;</mo> <mrow> <mi>x</mi> <mo>&lt;</mo> <mi>block_width</mi> </mrow> <mo>;</mo> <mrow> <mi>x</mi> <mo>++</mo> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msup> <mi>i</mi> <mi>″</mi> </msup> <mo>=</mo> <mrow> <msup> <mi>i</mi> <mi>′</mi> </msup> <mo>+</mo> <mrow> <mi>x</mi> <mo>/</mo> <mn>2</mn> </mrow> </mrow> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>7</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00007.png"> <img id="EMI-M00007" file="US06404815-20020611-M00007.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00007" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00007.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00007" attachment-type="nb" file="US06404815-20020611-M00007.NB"> </attachment> </attachments> </maths> </p>
    <p>In FIG. 32, it is assumed that block_width=block_height=4, namely, a block consists of 4×4 pels. In this case, a block can consist of an arbitrary number of positive integer pels for height and width of the block.</p>
    <p>The pel position (i″, j″) obtained above is output as a prediction picture sample pel position corresponding to the pel (i, j).</p>
    <p>2) Prediction Picture Generating Data Readout</p>
    <p>The memory readout address generator <b>38</b> generates a memory address indicating picture data position required for generating the prediction picture within the reference picture stored in the frame memory <b>54</b> based on the prediction picture sample pel position supplied from the corresponding pel determinator <b>37</b>. The memory readout address generator <b>38</b> then reads out the prediction picture generating data.</p>
    <p>3) Prediction Picture Generation</p>
    <p>On addressing only integer pel position within the pels for generating the prediction picture, the prediction picture generating data becomes pel positions constituting the prediction picture. On the contrary, on addressing half-pel positions, the half-pel interpolator <b>232</b> operates interpolation of the prediction picture generating data to generate half-pels. FIG. 33 shows a concrete operation of half-pel interpolation. By a method shown in FIG. 33, half-pel is simply generated by addition and division by 2. This process corresponds to step S<b>24</b> of FIG. 8 showing a flow chart of the half-pel interpolator <b>232</b> described in the first embodiment.</p>
    <p>The operation of the motion compensator <b>9</b> has been described above referring to FIG. <b>32</b>. When the transformation pattern data includes information different from FIG. 32, transforming operation becomes different.</p>
    <p>An example of another transformation pattern is shown in FIG. <b>34</b>. In this case, the corresponding pel (i″, j″) of the transformed block is obtained by the following. <maths> <math> <mrow> <mo></mo> <mtable> <mtr> <mtd> <mrow> <mtable> <mtr> <mtd> <mrow> <mrow> <mi>ix</mi> <mo>=</mo> <msup> <mi>i</mi> <mi>′</mi> </msup> </mrow> <mo>,</mo> <mrow> <mi>iy</mi> <mo>=</mo> <mrow> <msup> <mi>j</mi> <mi>′</mi> </msup> <mo>+</mo> <mn>2</mn> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>for</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <mi>y</mi> <mo>=</mo> <mn>0</mn> </mrow> <mo>;</mo> <mrow> <mi>y</mi> <mo>&lt;</mo> <mi>block_height</mi> </mrow> <mo>;</mo> <mrow> <mi>y</mi> <mo>++</mo> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mi>rdx</mi> <mo>=</mo> <mrow> <mi>rdx</mi> <mo>=</mo> <mrow> <mi>y</mi> <mo>/</mo> <mn>2</mn> </mrow> </mrow> </mrow> <mo>;</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>for</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <mi>x</mi> <mo>=</mo> <mn>0</mn> </mrow> <mo>;</mo> <mrow> <mi>x</mi> <mo>&lt;</mo> <mi>block_width</mi> </mrow> <mo>;</mo> <mrow> <mi>x</mi> <mo>++</mo> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mi>rdx</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo>+=</mo> <mn>0.5</mn> </mrow> <mo>;</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>rdy</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo>-=</mo> <mn>0.5</mn> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <msup> <mi>i</mi> <mi>″</mi> </msup> <mo>=</mo> <mrow> <mi>ix</mi> <mo>+</mo> <mi>rdx</mi> </mrow> </mrow> <mo>;</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <msup> <mi>j</mi> <mi>″</mi> </msup> <mo>=</mo> <mrow> <mi>iy</mi> <mo>+</mo> <mi>rdy</mi> </mrow> </mrow> <mo>;</mo> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>8</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </mrow> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00008.png"> <img id="EMI-M00008" file="US06404815-20020611-M00008.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00008" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00008.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00008" attachment-type="nb" file="US06404815-20020611-M00008.NB"> </attachment> </attachments> </maths> </p>
    <p>In this way, it is predetermined and specified as the transformation pattern data showing how the transformed block is extracted. Decoding can be implemented by motion compensation of the block transformed by simple addressing based on the transformation pattern data.</p>
    <p>As described above, according to the video decoder of the present embodiment, decoded picture can be obtained by simply computing sample pel positions from the encoded bitstream by efficiently predicting a complex motion, which cannot be implemented simply by the translation. This can be implemented by providing the transformation patterns previously.</p>
    <p>Further, in this embodiment, decoded picture can be also obtained from the bitstream, which is obtained by the prediction error signal encoded in an encoding method other than the orthogonal transformation encoding method, by changing element except the motion compensator <b>9</b> for decoding the prediction error signal.</p>
    <p>Further, the above example of this embodiment, where decoding is implemented by a unit of fixed size of block, can be applied to a decoder, where decoding is implemented by a unit of picture object having an arbitrary shape consisting of fixed size of blocks (e.g., Video Object Plane disclosed in ISO/IEC JTCI/SC29/WG11/N1796). For example, in a scene including a person in front of static background as shown in FIG. 9 of the first embodiment, a bitstream having a picture object of a person and small blocks divided from circumscribing rectangle surrounding the picture object of the person. This bitstream of effective blocks including the picture object is encoded and to be decoded. In this case, the similar decoding process can be applied to these effective blocks of the bitstream.</p>
    <heading>Embodiment 8</heading> <p>In the above seventh embodiment, the predetermined transformation and the motion compensation is implemented only by addressing (indicating pel position) using only integer pels or half-pels corresponding to the video decoder of the first through the sixth embodiments. In this embodiment, another video decoder implementing more precise motion compensation by computing other than half-pel interpolation on addressing will be described.</p>
    <p>FIG. 35 shows a configuration of the video decoder of the embodiment for extending compressed encoded digital picture and reproducing the picture.</p>
    <p>In the figure, reference numeral <b>90</b> indicates a motion compensator, <b>25</b> <i>b </i>indicates 0-4 motion vectors, and 60 indicates an interpolation accuracy indicating data.</p>
    <p>FIG. 36 shows an internal configuration of the motion compensator <b>90</b>.</p>
    <p>In the figure, reference numeral <b>37</b> <i>b </i>is a corresponding pel determinator which inputs the motion vector <b>25</b> <i>b</i>, transformation pattern data <b>26</b> <i>a</i>, input picture portion position <b>27</b> <i>a </i>and the interpolation accuracy indicating data <b>60</b> shown in FIG. <b>35</b> and determines corresponding pel position. An interpolation processor <b>232</b> <i>b </i>computes pel position interposed between the integer pels. In this case, the input picture portion position <b>27</b> <i>a </i>is a fixed value for each input picture portion. The motion vector <b>25</b> <i>b </i>and the transformation pattern data <b>26</b> <i>a </i>can be fixed values for each input picture portion, or can be encoded for using the same motion vector and the same transformation pattern data for all input picture portions included in larger picture consisting of a plurality of input picture portions (e.g., a picture frame, and VOP disclosed in ISO/IEC JTC1/SC29/WG11)</p>
    <p>FIG. 37 is a flowchart showing an operation of the motion compensator of FIG. 36, and FIG. 38 explains the same operation.</p>
    <p>In the following, an operation of the apparatus of the above configuration.</p>
    <p>Conventionally, corresponding pel is determined by only one motion vector representing the corresponding block. In this embodiment, pel positions are determined by computation for determining corresponding pels described below from four pieces of input corresponding to four corners within the reference picture block. Then, the obtained pel position is rounded to the accuracy indicated by the interpolation accuracy indicating data to finally determine pel position.</p>
    <p>The operation of this embodiment is identical to the seventh embodiment except the motion compensator <b>90</b>. Namely, the entropy decoder <b>51</b> analyzes the bitstream to divide into each encoded data. The quantized orthogonal transforming coefficient <b>52</b> is decoded through the dequantizer <b>6</b> and the inverse orthogonal transformer <b>7</b> using the quantizing step parameter <b>17</b> and is supplied to the decoding adder <b>53</b>. The decoding adder <b>53</b> outputs the prediction picture data <b>12</b> as the decoded picture <b>55</b> with/without addition according to intra-/interframe encoding block based or the intra-/interframe encoding indication flag <b>16</b>. The decoded picture <b>55</b> is transmitted to the display controller <b>56</b> to output on the display device and stored in the frame memory <b>54</b> as the reference picture.</p>
    <p>The prediction picture generation of the motion compensator <b>90</b> will be explained hereinafter.</p>
    <p>In this embodiment, transformation equation is obtained using a necessary number of motion vectors <b>25</b> <i>b </i>based on the transformation pattern data <b>26</b> <i>a</i>. After determining sample pel positions of the prediction picture portion corresponding to each pel of the input picture portion by the transformation equation, the prediction picture can be generated by performing simple interpolation to the precision indicated by the interpolation precision indicating data.</p>
    <p>In the following, the operation of the motion compensator <b>90</b> of this embodiment will be explained by referring to FIGS. 36 through 38.</p>
    <p>1) Corresponding Pel Determination</p>
    <p>The corresponding pel determinator <b>37</b> <i>b </i>computes sample pel position corresponding to each pel within the input picture portion based on the input motion vector <b>25</b> <i>b </i>and the transformation pattern data <b>26</b> <i>a</i>. As shown in FIG. 38, the motion vectors <b>25</b> <i>b </i>mean four motion vectors indicating respective motions of four corners of the circumscribing rectangle within the input picture portion. A required transformation equation is obtained based on the transformation pattern data <b>26</b> <i>a</i>. For example, the following transformation equations are used:</p>
    <p>1-1) Without Motion, Static State (required number of motion vectors: 0)</p>
    <p>
      <maths> <formula-text>(i′, j′)=(i, j)  (9)</formula-text> </maths> </p>
    <p>1-2) Translation (required number of motion vectors: 1)</p>
    <p>
      <maths> <formula-text>(i′, j′)=(i+dx<b>0</b>, j+dy<b>0</b>)  (10)</formula-text> </maths> </p>
    <p>1-3) Isotropic Transformation (required number of motion vectors: 2) <maths> <math> <mtable> <mtr> <mtd> <mrow> <mrow> <mtable> <mtr> <mtd> <mrow> <mi>i</mi> <mo>=</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mi>W</mi> </mrow> <mo>)</mo> </mrow> <mo>*</mo> <mi>i</mi> </mrow> <mo>+</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>y0</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y1</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mi>W</mi> </mrow> <mo>*</mo> <mi>j</mi> </mrow> <mo>+</mo> </mrow> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>x0</mi> <mi>′</mi> </msup> <mo>-</mo> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mi>x0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>+</mo> <mrow> <mi>y0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y0</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y1</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mi>W</mi> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>j</mi> <mo>=</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>y1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mi>W</mi> </mrow> <mo>)</mo> </mrow> <mo>*</mo> <mi>i</mi> </mrow> <mo>+</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mi>W</mi> </mrow> <mo>*</mo> <mi>j</mi> </mrow> <mo>+</mo> </mrow> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y0</mi> <mi>′</mi> </msup> <mo>-</mo> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mi>y0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>+</mo> <mrow> <mi>x0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y0</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y1</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mi>W</mi> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo></mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>11</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00009.png"> <img id="EMI-M00009" file="US06404815-20020611-M00009.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00009" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00009.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00009" attachment-type="nb" file="US06404815-20020611-M00009.NB"> </attachment> </attachments> </maths> </p>
    <p>where,</p>
    <p>(x<b>0</b>, y<b>0</b>): pel position of the left upper corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>1</b>, y<b>1</b>): pel position of the right upper corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>0</b>′, y<b>0</b>′): displaced pel position of (x<b>0</b>, y<b>0</b>) by the first motion vector (dx<b>0</b>, dy<b>0</b>)</p>
    <p>(x<b>1</b>′, y<b>1</b>′): displaced pel position of (x<b>1</b>, y<b>1</b>) by the second motion vector (dx<b>1</b>, dy<b>1</b>)</p>
    <p>W: x<b>1</b>−x<b>0</b> </p>
    <p>1-4) Affine Transformation (required number of motion vectors: 3) <maths> <math> <mtable> <mtr> <mtd> <mrow> <mtable> <mtr> <mtd> <mrow> <msup> <mi>i</mi> <mi>′</mi> </msup> <mo>=</mo> </mrow> </mtd> <mtd> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mi>W</mi> </mrow> <mo>)</mo> </mrow> <mo>*</mo> <mi>i</mi> </mrow> <mo>+</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x2</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mi>H</mi> </mrow> <mo>*</mo> <mi>j</mi> </mrow> <mo>+</mo> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mstyle> <mtext> </mtext> </mstyle> </mtd> <mtd> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x0</mi> <mi>′</mi> </msup> <mo>-</mo> <mrow> <mrow> <mi>x0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>/</mo> <mi>W</mi> </mrow> <mo>+</mo> <mrow> <mi>y0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>x2</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mi>H</mi> </mrow> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msup> <mi>j</mi> <mi>′</mi> </msup> <mo>=</mo> </mrow> </mtd> <mtd> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>y1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mi>W</mi> </mrow> <mo>)</mo> </mrow> <mo>*</mo> <mi>i</mi> </mrow> <mo>+</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>y2</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mi>H</mi> </mrow> <mo>*</mo> <mi>j</mi> </mrow> <mo>+</mo> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mstyle> <mtext> </mtext> </mstyle> </mtd> <mtd> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>y0</mi> <mi>′</mi> </msup> <mo>-</mo> <mrow> <mrow> <mi>x0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>/</mo> <mi>W</mi> </mrow> <mo>+</mo> <mrow> <mi>y0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y2</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mi>H</mi> </mrow> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>12</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00010.png"> <img id="EMI-M00010" file="US06404815-20020611-M00010.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00010" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00010.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00010" attachment-type="nb" file="US06404815-20020611-M00010.NB"> </attachment> </attachments> </maths> </p>
    <p>where,</p>
    <p>(x<b>0</b>, y<b>0</b>): pel position of the left upper corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>1</b>, y<b>1</b>): pel position of the right upper corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>2</b>, y<b>2</b>): pel position of the left lower corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>0</b>′, y<b>0</b>′): displaced pel position of (x<b>0</b>, y<b>0</b>) by the first motion vector (dx<b>0</b>, dy<b>0</b>)</p>
    <p>(x<b>1</b>′, y<b>1</b>′): displaced pel position of (x<b>1</b>, y<b>1</b>) by the second motion vector (dx<b>1</b>, dy<b>1</b>)</p>
    <p>(x<b>2</b>′, y<b>2</b>′): displaced pel position of (x<b>2</b>, y<b>2</b>) by the third motion vector (dx<b>2</b>, dy<b>2</b>)</p>
    <p>W: x<b>1</b>−x<b>0</b> </p>
    <p>H: y<b>2</b>−y<b>0</b> </p>
    <p>1-5) Perspective Transformation (required number of motion vectors: 4) <maths> <math> <mtable> <mtr> <mtd> <mrow> <mtable> <mtr> <mtd> <mrow> <msup> <mi>i</mi> <mi>′</mi> </msup> <mo>=</mo> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mi>A</mi> <mo>*</mo> <mi>i</mi> </mrow> <mo>+</mo> <mrow> <mi>B</mi> <mo>*</mo> <mi>j</mi> </mrow> <mo>+</mo> <mi>C</mi> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mi>P</mi> <mo>*</mo> <mi>i</mi> </mrow> <mo>+</mo> <mrow> <mi>Q</mi> <mo>*</mo> <mi>j</mi> </mrow> <mo>+</mo> <mrow> <mi>Z</mi> <mo>*</mo> <mi>W</mi> <mo>*</mo> <mi>H</mi> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msup> <mi>j</mi> <mi>′</mi> </msup> <mo>=</mo> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mi>D</mi> <mo>*</mo> <mi>i</mi> </mrow> <mo>+</mo> <mrow> <mi>E</mi> <mo>*</mo> <mi>j</mi> </mrow> <mo>+</mo> <mi>F</mi> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mi>P</mi> <mo>*</mo> <mi>i</mi> </mrow> <mo>+</mo> <mrow> <mi>Q</mi> <mo>*</mo> <mi>j</mi> </mrow> <mo>+</mo> <mrow> <mi>Z</mi> <mo>*</mo> <mi>W</mi> <mo>*</mo> <mi>H</mi> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>13</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00011.png"> <img id="EMI-M00011" file="US06404815-20020611-M00011.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00011" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00011.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00011" attachment-type="nb" file="US06404815-20020611-M00011.NB"> </attachment> </attachments> </maths> </p>
    <p>where, <maths> <math> <mtable> <mtr> <mtd> <mrow> <mtable> <mtr> <mtd> <mrow> <mi>A</mi> <mo>=</mo> <mrow> <mrow> <mrow> <mi>Z</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>*</mo> <mi>H</mi> </mrow> <mo>+</mo> <mrow> <mi>P</mi> <mo>*</mo> <msup> <mi>x1</mi> <mi>′</mi> </msup> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>B</mi> <mo>=</mo> <mrow> <mrow> <mrow> <mi>Z</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>x2</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>*</mo> <mi>W</mi> </mrow> <mo>+</mo> <mrow> <mi>Q</mi> <mo>*</mo> <msup> <mi>x2</mi> <mi>′</mi> </msup> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>C</mi> <mo>=</mo> <mrow> <mi>Z</mi> <mo>*</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> <mo>*</mo> <mi>W</mi> <mo>*</mo> <mi>H</mi> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>D</mi> <mo>=</mo> <mrow> <mrow> <mrow> <mi>Z</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>*</mo> <mi>H</mi> </mrow> <mo>+</mo> <mrow> <mi>P</mi> <mo>*</mo> <msup> <mi>y1</mi> <mi>′</mi> </msup> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>E</mi> <mo>=</mo> <mrow> <mrow> <mrow> <mi>Z</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y2</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>*</mo> <mi>W</mi> </mrow> <mo>+</mo> <mrow> <mi>Q</mi> <mo>*</mo> <msup> <mi>y2</mi> <mi>′</mi> </msup> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>F</mi> <mo>=</mo> <mi>Z</mi> <mo></mo> <mrow> <mi>(*</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <msup> <mi>y0</mi> <mi>′</mi> </msup> <mo>*</mo> <mi>W</mi> <mo>*</mo> <mi>H</mi> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>P</mi> <mo>=</mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x0</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x2</mi> <mi>′</mi> </msup> <mo>+</mo> <msup> <mi>x3</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y2</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y3</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x2</mi> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>′</mi> </mrow> </msup> <mo>-</mo> <msup> <mi>x3</mi> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>′</mi> </mrow> </msup> </mrow> <mo>)</mo> </mrow> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y0</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y2</mi> <mi>′</mi> </msup> <mo>+</mo> <msup> <mi>y3</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> <mo>*</mo> <mi>H</mi> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>Q</mi> <mo>=</mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mi>x1</mi> <mo>-</mo> <mmultiscripts> <mi>x3</mi> <none> </none> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>′</mi> </mrow> <mprescripts> </mprescripts> <none> </none> <mi>′</mi> </mmultiscripts> </mrow> <mo>)</mo> </mrow> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y0</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y2</mi> <mi>′</mi> </msup> <mo>+</mo> <msup> <mi>y3</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x0</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x2</mi> <mi>′</mi> </msup> <mo>+</mo> <msup> <mi>x3</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y3</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> <mo>*</mo> <mi>W</mi> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mi>Z</mi> <mo>=</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x3</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y2</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y3</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>-</mo> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x2</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>x3</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y1</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y3</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>14</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00012.png"> <img id="EMI-M00012" file="US06404815-20020611-M00012.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00012" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00012.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00012" attachment-type="nb" file="US06404815-20020611-M00012.NB"> </attachment> </attachments> </maths> </p>
    <p>(x<b>0</b>, y<b>0</b>): pel position of the left upper corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>1</b>, y<b>1</b>): pel position of the right upper corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>2</b>, y<b>2</b>): pel position of the left lower corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>3</b>, y<b>3</b>): pel position of the right lower corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>0</b>′, y<b>0</b>′): displaced pel position of (x<b>0</b>, y<b>0</b>) by the first motion vector (dx<b>0</b>, dy<b>0</b>)</p>
    <p>(x<b>1</b>′, y<b>1</b>′): displaced pel position of (x<b>1</b>, y<b>1</b>) by the second motion vector (dx<b>1</b>, dy<b>1</b>)</p>
    <p>(x<b>2</b>′, y<b>2</b>′): displaced pel position of (x<b>2</b>, y<b>2</b>) by the third motion vector (dx<b>2</b>, dy<b>2</b>)</p>
    <p>(x<b>3</b>′, y<b>3</b>′): displaced pel position of (x<b>3</b>, y<b>3</b>) by the fourth motion vector (dx<b>3</b>, dy<b>3</b>)</p>
    <p>W: x<b>1</b>−x<b>0</b> </p>
    <p>H: y<b>2</b>−y<b>0</b> </p>
    <p>The transformation pattern data <b>26</b> <i>a </i>can take the form of a bit directly indicating one of the above equations (9) through (13), or the form can be bits indicating the number of motion vectors. The input picture portion pel (i, j) can be corresponded to the reference picture portion pel (i′, j′) using the above transformation equations. On computation of corresponding pel position, the value of a prediction picture sample pel position should be obtained to the extent of precision indicated by the interpolation precision indicating data <b>60</b>. For example, on rounding to half-pel precision, the sample pel position (i′, j′) obtained by the above transformation equation should be rounded off to the half-pel precision. On rounding to quarter-pel precision, the sample pel position (i′, j′) obtained by the above transformation equation should be rounded off to the quarter-pel precision. This sample pel precision indicating data is extracted from the bitstream.</p>
    <p>As has been described, in this embodiment, corresponding pel determining rule is set directly by the motion vector <b>25</b> <i>b </i>and the prediction picture sample pel position is determined based on the corresponding pel determining rule.</p>
    <p>2) Prediction picture Generating Data Readout</p>
    <p>The memory readout address generator <b>38</b> <i>b </i>generates memory address specifying an address of picture data required for generating prediction picture within the reference picture from the addresses stored in the frame memory <b>54</b> based on the prediction picture sample pel position supplied from the corresponding pel determinator <b>37</b> <i>b</i>. Then, the memory readout address generator <b>38</b> <i>b </i>readouts prediction picture generating data.</p>
    <p>3) Prediction picture Generation</p>
    <p>On addressing only integer pels of the pels included in the prediction picture, the prediction picture generating data becomes prediction picture pel without any changes. According to this embodiment, sample pel position obtained by addressing the prediction picture portion can be pel value of predetermined precision as described above, for example, half-pel, quarter-pel. If the prediction picture sample pel is integer precision, the interpolation processor <b>232</b> <i>b </i>generates integer pel value of the prediction picture based on the indication of integer precision supplied from the interpolation precision indicating data <b>60</b>. In this embodiment, the corresponding pel determinator has already rounded off the final sample pel position to the precision indicated by the interpolation precision indicating data <b>60</b>. The interpolation processor implements the process of the following equation (15) as shown in FIG. <b>39</b>. If the prediction picture sample pel position is half-pel, the interpolation process becomes the same as the process implemented by the interpolator <b>232</b> described in the first embodiment. <maths> <math> <mtable> <mtr> <mtd> <mrow> <mo></mo> <mtable> <mtr> <mtd> <mrow> <mrow> <mover> <mi>I</mi> <mo>⋒</mo> </mover> <mo></mo> <mrow> <mo>(</mo> <mrow> <mi>x</mi> <mo>,</mo> <mi>y</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>=</mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mrow> <msub> <mi>W</mi> <mi>x1</mi> </msub> <mo></mo> <msub> <mi>W</mi> <mi>y1</mi> </msub> <mo></mo> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>i</mi> <mi>p</mi> </msub> <mo>,</mo> <msub> <mi>j</mi> <mi>p</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>+</mo> <mrow> <msub> <mi>W</mi> <mi>x2</mi> </msub> <mo></mo> <msub> <mi>W</mi> <mi>y1</mi> </msub> <mo></mo> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>i</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> <mo>,</mo> <msub> <mi>j</mi> <mi>p</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>+</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mrow> <msub> <mi>W</mi> <mi>x1</mi> </msub> <mo></mo> <msub> <mi>W</mi> <mi>y2</mi> </msub> <mo></mo> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>i</mi> <mi>p</mi> </msub> <mo>,</mo> <mrow> <msub> <mi>j</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>+</mo> <mrow> <msub> <mi>W</mi> <mi>x2</mi> </msub> <mo></mo> <msub> <mi>W</mi> <mi>y2</mi> </msub> <mo></mo> <mrow> <mi>I</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <mrow> <msub> <mi>i</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> <mo>,</mo> <mrow> <msub> <mi>j</mi> <mi>p</mi> </msub> <mo>+</mo> <mn>1</mn> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>W</mi> <mi>x2</mi> </msub> <mo>=</mo> <mrow> <msup> <mi>i</mi> <mrow> <mi>′</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> </mrow> </msup> <mo>-</mo> <msub> <mi>i</mi> <mi>p</mi> </msub> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>W</mi> <mi>x1</mi> </msub> <mo>=</mo> <mrow> <mn>1.0</mn> <mo>-</mo> <msub> <mi>W</mi> <mi>x2</mi> </msub> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>W</mi> <mi>y2</mi> </msub> <mo>=</mo> <mrow> <msup> <mi>j</mi> <mi>′</mi> </msup> <mo>-</mo> <msub> <mi>j</mi> <mi>p</mi> </msub> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>W</mi> <mi>y1</mi> </msub> <mo>=</mo> <mrow> <mn>1.0</mn> <mo>-</mo> <msub> <mi>W</mi> <mi>y2</mi> </msub> </mrow> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>15</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00013.png"> <img id="EMI-M00013" file="US06404815-20020611-M00013.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00013" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00013.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00013" attachment-type="nb" file="US06404815-20020611-M00013.NB"> </attachment> </attachments> </maths> </p>
    <p>As explained above, the video decoder according to this embodiment can efficiently predicts motion having various complexity and obtain decoded picture from the encoded bitstream by simple computation of the sample pel position using 0 through plural motion vectors.</p>
    <p>In the video encoders of the first through sixth embodiments and the video decoder of the seventh embodiment use integer pel and half-pel for addressing to implement a high-speed complex encoding and decoding of the picture by motion compensation of the transformed block.</p>
    <p>On the contrary, though the video decoder of this embodiment is configured as the same as the foregoing embodiments, the video decoder of the embodiment computes for corresponding pel determination to obtain better matching of the reference picture block to the input picture block by better motion compensation, which enables to obtain more smooth motion for block matching.</p>
    <p>In this embodiment, as well as in the seventh embodiment, the decoder can decode a bitstream, which is obtained by prediction error signal encoded in encoding method other than the orthogonal transformation encoding method, by changing element except the motion compensator <b>90</b> for decoding the prediction error signal.</p>
    <p>Further, the above example of this embodiment, where decoding is implemented by a unit of fixed size of block, can be applied not only to a decoder, where decoding is implemented by a unit of a frame of normal television signal, but also to a decoder, where decoding is implemented by a unit of picture object having an arbitrary shape consisting of fixed size of blocks (e.g., Video Object Plane), as well as the seventh embodiment.</p>
    <heading>Embodiment 9</heading> <p>The above explanation of the foregoing embodiments does not mention the number of pels included in one input picture block for detecting motion. Namely, the block can have an arbitrary height (H) of pels and an arbitrary width (W) of pels in the foregoing embodiments. In this embodiment, the number of pels for H and W is limited to a number of power of 2 to simplify the computation of the pel position. In this way, the load of the corresponding pel determinator decreases, and a high-speed operation can be performed.</p>
    <p>The decoder of the present embodiment is the same as the decoder of the eighth embodiment except an operation of a corresponding pel determinator <b>37</b> <i>c </i>in the motion compensator <b>90</b>. In the following, only the operation of the corresponding pel determinator will be explained.</p>
    <p>FIG. 40 is a flowchart showing a process of an operation of the corresponding pel determinator <b>37</b> <i>c. </i> </p>
    <p>FIG. 41 explains the operation of the corresponding pel determinator <b>37</b> <i>c. </i> </p>
    <p>The operation of the corresponding pel determinator <b>37</b> <i>c </i>of this embodiment will be explained below in relation to FIG. <b>40</b>.</p>
    <p>According to this embodiment, the corresponding pel determinator <b>37</b> <i>c </i>receives the motion vector <b>25</b> <i>b</i>, the transformation pattern data <b>26</b> <i>a</i>, the interpolation precision indicating data <b>91</b>, and the input picture portion position <b>27</b> <i>a </i>and computes the prediction picture sample pel position corresponding to each pel within the input picture portion to output. In this case, the input picture portion position <b>27</b> <i>a </i>is fixed value for each input picture portion. The motion vector <b>25</b> <i>b </i>and the transformation pattern data <b>26</b> <i>a </i>can be fixed value for each input picture portion, or can be encoded for using the same motion vector and the same transformation pattern data for all input picture portions included in larger picture consisting of a plurality of input picture portions (e.g., a picture frame, and VOP disclosed in ISO/IEC JTC1/SC29/WG11). In the following explanation, it is assumed that at most 3 motion vectors are used.</p>
    <p>As shown in FIG. 41, the motion vectors <b>25</b> <i>b </i>are from pel (x<b>0</b>, y<b>0</b>) to pel (x<b>0</b>+W′, y<b>0</b>)(W′≧W, W′=2<sup>m</sup>) and to pel (x<b>0</b>, y<b>0</b>+H′)(H′≧H, H′=2<sup>n</sup>), which are points on lines extended from the left upper corner and the right lower corner of the circumscribing rectangle within the input picture portion at a distance representable by power of 2 from the left upper corner. The following transformation equations (16) through (19) are obtained based on these motion vectors corresponding to the transformation pattern data <b>26</b> <i>a. </i> </p>
    <p>1-1) Without Motion (required number of vectors: 0)</p>
    <p>
      <maths> <formula-text>(i′, j′)=(i, j)  (16)</formula-text> </maths> </p>
    <p>1-2) Translation (required number of vectors: 1)</p>
    <p>
      <maths> <formula-text>(i′, j′)=(i+dx<b>0</b>, j+dy<b>0</b>)  (17)</formula-text> </maths> </p>
    <p>1-3) Isotropic Transformation (required number of vectors: 2) <maths> <math> <mtable> <mtr> <mtd> <mrow> <mrow> <mtable> <mtr> <mtd> <mrow> <msup> <mi>i</mi> <mi>′</mi> </msup> <mo>=</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>″</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <msup> <mi>W</mi> <mi>′</mi> </msup> </mrow> <mo>*</mo> <mi>i</mi> </mrow> <mo>+</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>y0</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y1</mi> <mi>″</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <msup> <mi>W</mi> <mi>′</mi> </msup> </mrow> <mo>*</mo> <mi>j</mi> </mrow> <mo>+</mo> </mrow> </mrow> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>x0</mi> <mi>′</mi> </msup> <mo>-</mo> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mi>x0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>″</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>+</mo> <mrow> <mi>y0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y0</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y1</mi> <mi>″</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <msup> <mi>W</mi> <mi>′</mi> </msup> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msup> <mi>j</mi> <mi>′</mi> </msup> <mo>=</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>y1</mi> <mi>″</mi> </msup> <mo>-</mo> <msup> <mi>y0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <msup> <mi>W</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>*</mo> <mi>i</mi> </mrow> <mo>+</mo> <mrow> <mo>(</mo> <mrow> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>″</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <msup> <mi>W</mi> <mi>′</mi> </msup> </mrow> <mo>*</mo> <mi>j</mi> </mrow> <mo>+</mo> </mrow> </mrow> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y0</mi> <mi>′</mi> </msup> <mo>-</mo> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mi>y0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>″</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>+</mo> <mrow> <mi>x0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y0</mi> <mi>′</mi> </msup> <mo>-</mo> <msup> <mi>y1</mi> <mi>″</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <msup> <mi>W</mi> <mi>′</mi> </msup> </mrow> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo></mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>18</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00014.png"> <img id="EMI-M00014" file="US06404815-20020611-M00014.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00014" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00014.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00014" attachment-type="nb" file="US06404815-20020611-M00014.NB"> </attachment> </attachments> </maths> </p>
    <p>where,</p>
    <p>(x<b>0</b>, y<b>0</b>): pel position of the left upper corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>1</b>, y<b>1</b>): pel position of the right upper corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>0</b>′, y<b>0</b>′): displaced pel position of (x<b>0</b>, y<b>0</b>) by the first motion vector (dx<b>0</b>, dy<b>0</b>)</p>
    <p>(x<b>1</b>′, y<b>1</b>′): displaced pel position of (x<b>0</b>+W′, y<b>0</b>) by the second motion vector (dx<b>1</b>, dy<b>1</b>)</p>
    <p>1-4) Affine Transformation (required number of motion vectors: 3) <maths> <math> <mtable> <mtr> <mtd> <mrow> <mrow> <mtable> <mtr> <mtd> <mrow> <msup> <mi>i</mi> <mi>′</mi> </msup> <mo>=</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>″</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <msup> <mi>W</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>*</mo> <mi>i</mi> </mrow> <mo>+</mo> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x2</mi> <mi>″</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <msup> <mi>H</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>*</mo> <mi>j</mi> </mrow> <mo>+</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>x0</mi> <mi>′</mi> </msup> <mo>-</mo> <mrow> <mrow> <mi>x0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>x1</mi> <mi>″</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>/</mo> <msup> <mi>W</mi> <mi>′</mi> </msup> </mrow> <mo>+</mo> <mrow> <mi>y0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>x2</mi> <mi>″</mi> </msup> <mo>-</mo> <msup> <mi>x0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <msup> <mi>H</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msup> <mi>j</mi> <mi>′</mi> </msup> <mo>=</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>y1</mi> <mi>″</mi> </msup> <mo>-</mo> <msup> <mi>y0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <msup> <mi>W</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>*</mo> <mi>i</mi> </mrow> <mo>+</mo> <mrow> <mrow> <mo>(</mo> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>y2</mi> <mi>″</mi> </msup> <mo>-</mo> <msup> <mi>y0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <msup> <mi>H</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> <mo>*</mo> <mi>j</mi> </mrow> <mo>+</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <msup> <mi>y0</mi> <mi>′</mi> </msup> <mo>-</mo> <mrow> <mrow> <mi>x0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y1</mi> <mi>″</mi> </msup> <mo>-</mo> <msup> <mi>y0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> <mo>/</mo> <msup> <mi>W</mi> <mi>′</mi> </msup> </mrow> <mo>+</mo> <mrow> <mi>y0</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msup> <mi>y2</mi> <mi>″</mi> </msup> <mo>-</mo> <msup> <mi>y0</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> <mo>)</mo> </mrow> <mo>/</mo> <msup> <mi>H</mi> <mi>′</mi> </msup> </mrow> <mo>)</mo> </mrow> </mrow> </mtd> </mtr> </mtable> <mo>)</mo> </mrow> <mo></mo> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>19</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00015.png"> <img id="EMI-M00015" file="US06404815-20020611-M00015.TIF" img-content="math" img-format="tif" alt="Figure US06404815-20020611-M00015" src="//patentimages.storage.googleapis.com/US6404815B1/US06404815-20020611-M00015.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00015" attachment-type="nb" file="US06404815-20020611-M00015.NB"> </attachment> </attachments> </maths> </p>
    <p>where,</p>
    <p>(x<b>0</b>, y<b>0</b>): pel position of the left upper corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>1</b>, y<b>1</b>): pel position of the right upper corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>2</b>, y<b>2</b>): pel position of the left lower corner of the circumscribing rectangle within the input picture portion</p>
    <p>(x<b>0</b>′, y<b>0</b>′): displaced pel position of (x<b>0</b>, y<b>0</b>) by the first motion vector (dxo, dy<b>0</b>)</p>
    <p>(x<b>1</b>″, y<b>1</b>″): displaced pel position of (x<b>0</b>+W′, y<b>0</b>) by the second motion vector (dx<b>1</b>, dy<b>1</b>)</p>
    <p>(x<b>2</b>″, y<b>2</b>″): displaced pel position of (x<b>0</b>, y<b>0</b>+H′) by the third motion vector (dx<b>2</b>, dy<b>2</b>)</p>
    <p>The transformation pattern data <b>26</b> <i>a </i>can take the form of a bit sequence consisting of plural bits directly indicating one of the above transformation equations (16) through (19), or the form can be bits indicating the number of motion vectors, as each transformation corresponds to number of vectors.</p>
    <p>By the above transformation equations, the input picture portion pel (i, j) can be corresponded to the reference picture portion pel (i′, j′). On computation of corresponding pel position, the value of a prediction picture sample pel position should be obtained to the extent of precision indicated by the interpolation precision indicating data <b>60</b>. For example, on rounding to half-pel precision, the sample pel position (i′, j′) obtained by the above transformation equation should be rounded off to the half-pel precision. On rounding to quarter-pel precision, the sample pel position (i′, j′) obtained by the above transformation equation should be rounded off to the quarter-pel precision. This sample pel precision indicating data is extracted from the bitstream.</p>
    <p>As has been described, in this embodiment, corresponding pel determining rule is set directly by the motion vector <b>25</b> <i>b </i>and the prediction picture sample pel position is determined based on the corresponding pel determining rule.</p>
    <p>2) Prediction picture Generating Data Readout</p>
    <p>3) Prediction picture Generation</p>
    <p>As the operation of this embodiment in connection with the above 2) and 3) is the same as the eighth embodiment, the detailed explanation will be omitted.</p>
    <p>As explained above, the video decoder according to this embodiment can determine sample pel position in a high-speed by simple bit shifting operation instead of division by W′ or H′ on computing sample pel position using <b>0</b> through plural motion vectors. The video decoder of the embodiment can efficiently predict motion having various complexity to obtain decoded picture from the encoded bitstream.</p>
    <p>The motion compensation according to this embodiment can be applied to a video decoder based on another encoding method by changing corresponding element and the video decoder can obtain the same efficiency. Further, this embodiment can be applied to a decoder, where decoding is implemented by a unit of picture object having an arbitrary shape consisting of fixed size of block (e.g., Video Object Plane), as well as the seventh embodiment.</p>
    <p>A video encoder and a video decoder of the present invention can be combined to configure a characteristic video encoding and decoding system.</p>
    <p>Further, by executing the operations illustrated in the flow charts described hereinbefore, that is, by providing a transformed block matching step, a corresponding pel determination step, a motion compensated prediction picture generating step, and a decoding adding step, a characteristic video encoding and decoding method can be obtained.</p>
    <heading>Industrial Applicability</heading> <p>As described above, according to the present invention, real integer sample point pels and half-pels interposed midway therebetween in a transformed prediction picture portion obtained by memory addressing alone are used to perform motion compensated prediction. Consequently, efficient and accurate motion compensated prediction can be performed for a picture portion in which correct motion compensated prediction cannot be performed by only using a motion vector limited to translational displacement. Further, this prediction can be implemented without performing complex arithmetic operations which are required when using the affine motion model. Motion compensated prediction using a transformed picture portion for various types of object motion which cannot be expressed in terms of a numeral equation as well as object rotation and scaling which are easily expressible in terms of a numeral equation can also be performed by the invention. A video decoder of the present invention can also recreate a high-quality picture efficiently.</p>
    <p>Further, if only the memory addressing based on the corresponding pel determination is performed, various object motion including rotation, scaling down, or scaling up can be correctly predicted without performing complex pel interpolation such as when the affine motion model is applied.</p>
    <p>Further, by using the motion vector representing a translational displacement of an object through block matching, a transformed block search range used in transformed block matching can be effectively reduced. Overall operation processes for motion compensated prediction can also be reduced.</p>
    <p>Further, if only the memory addressing is performed, various object motion including simply reducing or enlarging can be efficiently predicted without performing complex pel interpolation such as when the affine motion model is applied.</p>
    <p>Further, corresponding pel determination can be made by using a transformation pattern table. Consequently, motion compensated prediction for object motion such as that required for using the affine motion model and represented by an arbitrary transformed prediction picture portion and which cannot be expressed in terms of a simple numerical equation can be performed.</p>
    <p>Further, by using a filter, a variation in the spatial frequency characteristic within a transformed block can be eliminated. Thus, a prediction mismatch can be reduced.</p>
    <p>According to the present invention, a video decoder combined with a video encoder capable of performing transformed block matching and motion compensated prediction is provided. Consequently, the video decoder of the present invention can decode encoded picture data obtained by implementing rapid and optimum motion compensated prediction, into a complete picture.</p>
    <p>Further, on addressing by the video decoder, the decoder can implement complex motion prediction including various motion for decoding, which enables to reproduce the picture having smoother motion.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5311310">US5311310</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 1, 1992</td><td class="patent-data-table-td patent-date-value">May 10, 1994</td><td class="patent-data-table-td ">Bell Communications Research, Inc.</td><td class="patent-data-table-td ">High efficiency coder and method employing overlapped motion compensation and perfect reconstruction filter banks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5376971">US5376971</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 30, 1993</td><td class="patent-data-table-td patent-date-value">Dec 27, 1994</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Picture encoding apparatus and picture decoding apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5574504">US5574504</a></td><td class="patent-data-table-td patent-date-value">Aug 22, 1995</td><td class="patent-data-table-td patent-date-value">Nov 12, 1996</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Methods and systems for encoding and decoding picture signals and related picture-signal recording media</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5598215">US5598215</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 23, 1994</td><td class="patent-data-table-td patent-date-value">Jan 28, 1997</td><td class="patent-data-table-td ">Nippon Telegraph And Telephone Corporation</td><td class="patent-data-table-td ">Moving image encoder and decoder using contour extraction</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5684538">US5684538</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 17, 1995</td><td class="patent-data-table-td patent-date-value">Nov 4, 1997</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">System and method for performing video coding/decoding using motion compensation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5963259">US5963259</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 15, 1997</td><td class="patent-data-table-td patent-date-value">Oct 5, 1999</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Video coding/decoding system and video coder and video decoder used for the same system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5978030">US5978030</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 5, 1995</td><td class="patent-data-table-td patent-date-value">Nov 2, 1999</td><td class="patent-data-table-td ">Daewoo Electronics Co., Ltd.</td><td class="patent-data-table-td ">Method and apparatus for encoding a video signal using feature point based motion estimation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6144701">US6144701</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 10, 1997</td><td class="patent-data-table-td patent-date-value">Nov 7, 2000</td><td class="patent-data-table-td ">Sarnoff Corporation</td><td class="patent-data-table-td ">Stereoscopic video coding and decoding apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6208690">US6208690</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 9, 1997</td><td class="patent-data-table-td patent-date-value">Mar 27, 2001</td><td class="patent-data-table-td ">Sharp Kabushiki Kaisha</td><td class="patent-data-table-td ">Method of motion-compensated interframe-prediction in a video-coding device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6249318">US6249318</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 12, 1997</td><td class="patent-data-table-td patent-date-value">Jun 19, 2001</td><td class="patent-data-table-td ">8×8, Inc.</td><td class="patent-data-table-td ">Video coding/decoding arrangement and method therefor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6249613">US6249613</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 31, 1998</td><td class="patent-data-table-td patent-date-value">Jun 19, 2001</td><td class="patent-data-table-td ">Sharp Laboratories Of America, Inc.</td><td class="patent-data-table-td ">Mosaic generation and sprite-based coding with automatic foreground and background separation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="http://www.google.com/url?id=GmpaBAABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3DH048585U%26KC%3DU%26FT%3DD&amp;usg=AFQjCNFzn3zQ4dR5SjQQrk1KYDaD6vPteQ">JPH048585U</a></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">Title not available</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="http://www.google.com/url?id=GmpaBAABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3DH0698314A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNGgyC9Ug75Zzl306plnmQNLMcc3iA">JPH0698314A</a></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">Title not available</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="http://www.google.com/url?id=GmpaBAABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3DH0750773A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNGo9w8MgTmfGtQ27XyranW3Hk8qlQ">JPH0750773A</a></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">Title not available</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="http://www.google.com/url?id=GmpaBAABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3DH05219498A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNEEx5EaevF91BIedE7BlWgJnOqafw">JPH05219498A</a></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">Title not available</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="http://www.google.com/url?id=GmpaBAABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3DH05244585A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNHbar-tPLMv64b04s_5TsQNYP-5Ow">JPH05244585A</a></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">Title not available</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="http://www.google.com/url?id=GmpaBAABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3DH06153185A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNFOhQr8HhOPS-bxe3WChsJ2oz690Q">JPH06153185A</a></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">Title not available</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">ISO/IEC 11172-2 (14 pages).</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Motion Compensated Prediction Using An Affine Motion Model (Technical Report of the Institute of Electronics, Information and Communication Engineers, IE 94-36).</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6690731">US6690731</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 1, 1998</td><td class="patent-data-table-td patent-date-value">Feb 10, 2004</td><td class="patent-data-table-td ">Neostar, Inc.</td><td class="patent-data-table-td ">Method and apparatus for diagonal processing of video data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6853752">US6853752</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 21, 2001</td><td class="patent-data-table-td patent-date-value">Feb 8, 2005</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Method and apparatus for dynamic loop and post filtering</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7110456">US7110456</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 4, 2002</td><td class="patent-data-table-td patent-date-value">Sep 19, 2006</td><td class="patent-data-table-td ">Mitsubishi Denki Kabushiki Kaisha</td><td class="patent-data-table-td ">Video encoder, video decoder, video encoding method, video decoding method, and video encoding and decoding system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7181070">US7181070</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 29, 2002</td><td class="patent-data-table-td patent-date-value">Feb 20, 2007</td><td class="patent-data-table-td ">Altera Corporation</td><td class="patent-data-table-td ">Methods and apparatus for multiple stage video decoding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8018463">US8018463</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 9, 2005</td><td class="patent-data-table-td patent-date-value">Sep 13, 2011</td><td class="patent-data-table-td ">Nvidia Corporation</td><td class="patent-data-table-td ">Processor for video data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8098734">US8098734</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 29, 2009</td><td class="patent-data-table-td patent-date-value">Jan 17, 2012</td><td class="patent-data-table-td ">Mitsubishi Denki Kabushiki Kaisha</td><td class="patent-data-table-td ">Video encoder, video decoder, video encoding method, video decoding method, and video encoding and decoding system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8130825">US8130825</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 9, 2005</td><td class="patent-data-table-td patent-date-value">Mar 6, 2012</td><td class="patent-data-table-td ">Nvidia Corporation</td><td class="patent-data-table-td ">Processor for video data encoding/decoding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8170105">US8170105</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 23, 2006</td><td class="patent-data-table-td patent-date-value">May 1, 2012</td><td class="patent-data-table-td ">Mitsubishi Denki Kabushiki Kaisha</td><td class="patent-data-table-td ">Video decoder and video decoding method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8179971">US8179971</a></td><td class="patent-data-table-td patent-date-value">Dec 8, 2003</td><td class="patent-data-table-td patent-date-value">May 15, 2012</td><td class="patent-data-table-td ">G&amp;H Nevada-Tek</td><td class="patent-data-table-td ">Method and apparatus for video data compression</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8194742">US8194742</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 13, 2006</td><td class="patent-data-table-td patent-date-value">Jun 5, 2012</td><td class="patent-data-table-td ">Mitsubishi Denki Kabushiki Kaisha</td><td class="patent-data-table-td ">Video decoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8504659">US8504659</a></td><td class="patent-data-table-td patent-date-value">May 7, 2008</td><td class="patent-data-table-td patent-date-value">Aug 6, 2013</td><td class="patent-data-table-td ">Altera Corporation</td><td class="patent-data-table-td ">Apparatus and method for adaptive multimedia reception and transmission in communication environments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101379816B?cl=en">CN101379816B</a></td><td class="patent-data-table-td patent-date-value">May 11, 2006</td><td class="patent-data-table-td patent-date-value">Dec 22, 2010</td><td class="patent-data-table-td ">高通股份有限公司</td><td class="patent-data-table-td ">Temporal error concealment for bi-directionally predicted frames</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc375/defs375.htm&usg=AFQjCNFcuagMfSu6xvEzMh0uBF37Cw37ZA#C375S240160">375/240.16</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc375/defs375.htm&usg=AFQjCNFcuagMfSu6xvEzMh0uBF37Cw37ZA#C375SE07109">375/E07.109</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc375/defs375.htm&usg=AFQjCNFcuagMfSu6xvEzMh0uBF37Cw37ZA#C375SE07211">375/E07.211</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04N0019500000">H04N19/50</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0009000000">G06T9/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04N0007240000">H04N7/24</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N19/00739">H04N19/00739</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N19/0063">H04N19/0063</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N19/00612">H04N19/00612</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N19/006">H04N19/006</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=GmpaBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N19/00781">H04N19/00781</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">H04N7/26M2H</span>, <span class="nested-value">H04N7/36C4</span>, <span class="nested-value">H04N7/26M2N</span>, <span class="nested-value">H04N7/50</span>, <span class="nested-value">H04N7/26M2S</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Nov 13, 2013</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 12, 2009</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 4, 2007</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 7 AND 13 IS CONFIRMED. CLAIMS 1-6, 8-12 AND 14 ARE CANCELLED. CLAIM 15 IS DETERMINED TO BE PATENTABLE AS AMENDED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 18, 2005</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 24, 2004</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20040702</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 3, 1998</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">MITSUBISHI DENKI KABUSHIKI KAISHA, JAPAN</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SEKIGUCHI, SHUNICHI;ASAI, KOHTARO;MARUKAMI, TOKUMICHI;AND OTHERS;REEL/FRAME:009637/0368;SIGNING DATES FROM 19971117 TO 19971118</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U3_dtXNqtMcw0R5yMDWNs3ZJcR_8Q\u0026id=GmpaBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U0yRh8eYkA-BLHt43WwTKOSp4r2Ig\u0026id=GmpaBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U1yiPvrHb-tTeZWQ8FhuqFjz9BUmw","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Image_encoder_image_decoder_image_encodi.pdf?id=GmpaBAABERAJ\u0026output=pdf\u0026sig=ACfU3U3FfpFsxI_Y_zn6wr3xX5Qp54bMdQ"},"sample_url":"http://www.google.com/patents/reader?id=GmpaBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>