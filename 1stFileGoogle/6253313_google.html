<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6253313 - Parallel processor system for processing natural concurrencies and method ... - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Parallel processor system for processing natural concurrencies and method therefor"><meta name="DC.contributor" content="Gordon Edward Morrison" scheme="inventor"><meta name="DC.contributor" content="Christopher Bancroft Brooks" scheme="inventor"><meta name="DC.contributor" content="Frederick George Gluck" scheme="inventor"><meta name="DC.contributor" content="Biax Corporation" scheme="assignee"><meta name="DC.date" content="1995-6-7" scheme="dateSubmitted"><meta name="DC.description" content="A computer processing system containing a plurality of identical processor elements each of which does not retain execution state information from prior operations. The plurality of identical processor elements operate on a statically compiled program which, based upon detected natural concurrencies in the basic blocks of the programs, provide logical processor numbers and an instruction firing time to each instruction in each basic block. Each processor element is capable of executing instructions on a per instruction basis such that dependent instructions can execute on the same or different processor elements. A given processor element is capable of executing an instruction from one context followed by an instruction from another context through use of shared storage resources."><meta name="DC.date" content="2001-6-26" scheme="issued"><meta name="DC.relation" content="US:3343135" scheme="references"><meta name="DC.relation" content="US:3611306" scheme="references"><meta name="DC.relation" content="US:3771141" scheme="references"><meta name="DC.relation" content="US:4104720" scheme="references"><meta name="DC.relation" content="US:4109311" scheme="references"><meta name="DC.relation" content="US:4153932" scheme="references"><meta name="DC.relation" content="US:4181936" scheme="references"><meta name="DC.relation" content="US:4228495" scheme="references"><meta name="DC.relation" content="US:4229790" scheme="references"><meta name="DC.relation" content="US:4241398" scheme="references"><meta name="DC.relation" content="US:4270167" scheme="references"><meta name="DC.relation" content="US:4435758" scheme="references"><meta name="DC.relation" content="US:4466061" scheme="references"><meta name="DC.relation" content="US:4468736" scheme="references"><meta name="citation_patent_number" content="US:6253313"><meta name="citation_patent_application_number" content="US:08/480,691"><link rel="canonical" href="http://www.google.com/patents/US6253313"/><meta property="og:url" content="http://www.google.com/patents/US6253313"/><meta name="title" content="Patent US6253313 - Parallel processor system for processing natural concurrencies and method therefor"/><meta name="description" content="A computer processing system containing a plurality of identical processor elements each of which does not retain execution state information from prior operations. The plurality of identical processor elements operate on a statically compiled program which, based upon detected natural concurrencies in the basic blocks of the programs, provide logical processor numbers and an instruction firing time to each instruction in each basic block. Each processor element is capable of executing instructions on a per instruction basis such that dependent instructions can execute on the same or different processor elements. A given processor element is capable of executing an instruction from one context followed by an instruction from another context through use of shared storage resources."/><meta property="og:title" content="Patent US6253313 - Parallel processor system for processing natural concurrencies and method therefor"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("JKntU5ndKMilsQSOoIDwDg"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("NOR"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("JKntU5ndKMilsQSOoIDwDg"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("NOR"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6253313?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6253313"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=XkFVBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6253313&amp;usg=AFQjCNExxKw02gPIZmmdiugk-bEGYyJ0RA" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6253313.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6253313.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6253313" style="display:none"><span itemprop="description">A computer processing system containing a plurality of identical processor elements each of which does not retain execution state information from prior operations. The plurality of identical processor elements operate on a statically compiled program which, based upon detected natural concurrencies...</span><span itemprop="url">http://www.google.com/patents/US6253313?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6253313 - Parallel processor system for processing natural concurrencies and method therefor</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6253313 - Parallel processor system for processing natural concurrencies and method therefor" title="Patent US6253313 - Parallel processor system for processing natural concurrencies and method therefor"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6253313 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 08/480,691</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Jun 26, 2001</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Jun 7, 1995</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Oct 31, 1985</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US5021945">US5021945</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5517628">US5517628</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5765037">US5765037</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">08480691, </span><span class="patent-bibdata-value">480691, </span><span class="patent-bibdata-value">US 6253313 B1, </span><span class="patent-bibdata-value">US 6253313B1, </span><span class="patent-bibdata-value">US-B1-6253313, </span><span class="patent-bibdata-value">US6253313 B1, </span><span class="patent-bibdata-value">US6253313B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Gordon+Edward+Morrison%22">Gordon Edward Morrison</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Christopher+Bancroft+Brooks%22">Christopher Bancroft Brooks</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Frederick+George+Gluck%22">Frederick George Gluck</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Biax+Corporation%22">Biax Corporation</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6253313.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6253313.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6253313.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (14),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (33),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (30),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (13)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6253313&usg=AFQjCNEgrmjyJNkt6TrwvKfPzTy6WolTYA">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6253313&usg=AFQjCNEs8Vc8bZirI-6vTPCb9psJiyb0FA">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6253313B1%26KC%3DB1%26FT%3DD&usg=AFQjCNHneWfjJLmuK1VTJpTB5g_r1hdAPw">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54776037" lang="EN" load-source="patent-office">Parallel processor system for processing natural concurrencies and method therefor</invention-title></span><br><span class="patent-number">US 6253313 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA72575571" lang="EN" load-source="patent-office"> <div class="abstract">A computer processing system containing a plurality of identical processor elements each of which does not retain execution state information from prior operations. The plurality of identical processor elements operate on a statically compiled program which, based upon detected natural concurrencies in the basic blocks of the programs, provide logical processor numbers and an instruction firing time to each instruction in each basic block. Each processor element is capable of executing instructions on a per instruction basis such that dependent instructions can execute on the same or different processor elements. A given processor element is capable of executing an instruction from one context followed by an instruction from another context through use of shared storage resources.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(18)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6253313B1/US06253313-20010626-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6253313B1/US06253313-20010626-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(25)</span></span></div><div class="patent-text"><div mxw-id="PCLM28659735" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>We claim: </claim-statement> <div class="claim"> <div num="1" id="US-6253313-B1-CLM-00001" class="claim">
      <div class="claim-text">1. A parallel processor system for processing natural concurriencies in streams of low level instructions contained in a plurality of programs in said system, each of said streams having a plurality of single entry-single exit (SESE) basic blocks (BBs), said system comprising:</div>
      <div class="claim-text">means (<b>160</b>) for statically adding intelligence to each instruction in each of said plurality of basic blocks for each said program, said added intelligence at least having a logical processor number (LPN) and an instruction firing time (IFT) </div>
      <div class="claim-text">a plurality of contexts (<b>660</b>), each of said contexts being assigned to one of said plurality of programs for processing one of said programs, each of said contexts having at least a plurality of registers and a plurality of condition code storages for containing processing status information, </div>
      <div class="claim-text">a plurality of logical resource drivers (LRDs) with each logical resource driver being assigned to on eof said plurality of contexts, each of said logical resource drivers being receptive of said basic blocks corresponding to the program instruction stream of said assigned program from said adding means, each of said logical resource drivers comprising: </div>
      <div class="claim-text">(a) a plurality of queues (<b>1560</b>), and </div>
      <div class="claim-text">(b) means (<b>630</b>, <b>6200</b> operative on said plurality of said basic blocks containing said intelligence from said adding means for delivering said instruction sin each said basic block into said plurality of queues based on said logical processor number, said instruction sin each said queue being entered according to said instruction firing time wherein the earliest instruction firing time is entered first, </div>
      <div class="claim-text">a plurality of individual processor elements (PEs), each of said processor elements being free of any context information, </div>
      <div class="claim-text">means (<b>650</b>) connecting said plurality of processor elements to said plurality of logical resource drivers for transferring said instructions with the earliest instruction firing time first in said queues from each of said logical resource drivers, in a predetermined order, to individually assigned processor elements, each said processor element being capable of processing said transferred instruction, </div>
      <div class="claim-text">first means (<b>670</b>) for connecting each of said processor elements with any one of said plurality of contexts, each of said processor elements being capable of accessing said plurality of registers and condition code storages in a program's context during the processing of the program's instruction, </div>
      <div class="claim-text">a plurality of memory locations (<b>610</b>), and </div>
      <div class="claim-text">second means (<b>620</b>, <b>630</b>) for connecting each of said processor elements with any one of said plurality of memory locations, each said processor element being capable of accessing said memory locations during said processing of each said instruction. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" id="US-6253313-B1-CLM-00002" class="claim">
      <div class="claim-text">2. The processor system of claim <b>1</b> further comprising, a system for using multiple sets of condition values, the processor system comprising:</div>
      <div class="claim-text">an opcode storage configured to buffer a plurality of opcodes corresponding to at least some of said low level instructions to be processed; </div>
      <div class="claim-text">a first circuit coupled to said opcode storage, said first circuit configured to receive an opcode of a first type of instruction and to generate a set of at least one condition value; </div>
      <div class="claim-text">a condition storage coupled to said first circuit, said condition storage configured to store a plurality of said sets of condition values; </div>
      <div class="claim-text">a second circuit coupled to said opcode storage, said second circuit configured to receive an opcode of a second type of instruction and to generate a set of at least one output value, said output value for each second said type of instruction depending on a particular one of said stored sets of condition values; and </div>
      <div class="claim-text">an access circuit coupled between said condition storage, and said second circuit, said access circuit configured to access said particular stored set of condition values for each said opcode of second type. </div>
    </div>
    </div> <div class="claim"> <div num="3" id="US-6253313-B1-CLM-00003" class="claim">
      <div class="claim-text">3. In an instruction processing apparatus, a system for using multiple sets of condition values within a single context, the system comprising:</div>
      <div class="claim-text">an opcode storage configured to buffer a plurality of opcodes corresponding to at least some of said instructions to be processed from said context; </div>
      <div class="claim-text">a first circuit coupled to said opcode storage, said first circuit configured to receive opcodes of a first type of instruction and addresses for storing sets of at least one condition value, and to generate for each said opcode of said first type a set of at least one condition value associated with one of the addresses; </div>
      <div class="claim-text">a condition storage coupled to said first circuit, said condition storage configured to store a plurality of said sets of condition values at storage locations based upon addresses received from the first circuit; </div>
      <div class="claim-text">a second circuit coupled to said opcode storage, said second circuit configured to receive opcodes of a second type of instruction and condition storage addresses, and to generate a set of at least one output value, said output value for each said second type of instruction depending on a particular one of said stored sets of condition values associated with the condition storage address; and </div>
      <div class="claim-text">an access circuit coupled between said condition storage and said second circuit, said access circuit configured to access by the condition storage address said particular one of said stored sets of condition values for each said opcode of said second type. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" id="US-6253313-B1-CLM-00004" class="claim">
      <div class="claim-text">4. The system of claim <b>3</b> further wherein said first type of instruction consists of arithmetic and logic type instructions.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" id="US-6253313-B1-CLM-00005" class="claim">
      <div class="claim-text">5. The system of claim <b>3</b> further wherein said second type of instruction consists of branch type instructions.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" id="US-6253313-B1-CLM-00006" class="claim">
      <div class="claim-text">6. The system of claim <b>3</b> further wherein said plurality of sets of condition values comprises at least three sets of condition values.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" id="US-6253313-B1-CLM-00007" class="claim">
      <div class="claim-text">7. The system of claim <b>6</b> further wherein there are four sets of condition values.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" id="US-6253313-B1-CLM-00008" class="claim">
      <div class="claim-text">8. The system of claim <b>3</b> further wherein said access circuit comprises an address selection circuit.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" id="US-6253313-B1-CLM-00009" class="claim">
      <div class="claim-text">9. The system of claim <b>8</b> further wherein said address selection circuit receives an address input specified by said second type of instruction.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" id="US-6253313-B1-CLM-00010" class="claim">
      <div class="claim-text">10. The system of claim <b>3</b> further comprising third circuits that receive opcode inputs of said first type of instructions and generate additional sets of condition values.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" id="US-6253313-B1-CLM-00011" class="claim">
      <div class="claim-text">11. The system of claim <b>3</b> further wherein said set of output values comprises a target address of a conditional branch type instruction.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" id="US-6253313-B1-CLM-00012" class="claim">
      <div class="claim-text">12. The system of claim <b>3</b> further comprising a general purpose register file separate from said condition storage.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" id="US-6253313-B1-CLM-00013" class="claim">
      <div class="claim-text">13. The system of claim <b>3</b> further wherein said sets of condition values are stored as condition codes, each said condition code comprising a plurality of flag bits.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" id="US-6253313-B1-CLM-00014" class="claim">
      <div class="claim-text">14. The system of claim <b>13</b> further wherein one of said flag bits represents a zero result of said first type of instruction.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" id="US-6253313-B1-CLM-00015" class="claim">
      <div class="claim-text">15. The system of claim <b>14</b> further wherein said first type of instruction is a decrement instruction.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" id="US-6253313-B1-CLM-00016" class="claim">
      <div class="claim-text">16. The system of claim <b>3</b> further wherein said opcode storage comprises a processor instruction queue, said queue configured to buffer instructions waiting to be executed and to track the execution status of instructions that are issued.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" id="US-6253313-B1-CLM-00017" class="claim">
      <div class="claim-text">17. The system of claim <b>3</b> further comprising an allocate circuit coupled between said first circuit and said condition storage, said allocate circuit configured to assign said set of condition values to a physical location in said condition storage.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" id="US-6253313-B1-CLM-00018" class="claim">
      <div class="claim-text">18. The system of claim <b>17</b> further wherein said physical location is assigned based on an address specified by said first type of instruction.</div>
    </div>
    </div> <div class="claim"> <div num="19" id="US-6253313-B1-CLM-00019" class="claim">
      <div class="claim-text">19. A method of processing a stream of instructions within a single context using multiple sets of at least one condition value, a first type of said instructions each generating a set of at least one condition value and a second type of said instructions each producing a result that depends on one of said generated sets of at least one condition value, the method comprising:</div>
      <div class="claim-text">issuing a first instruction of said first type with a first address for storing a first set of at least one condition value; </div>
      <div class="claim-text">generating a first set of at least one condition value; </div>
      <div class="claim-text">storing said first set of at least one condition value in a storage location having said first address in a condition storage, wherein said condition storage has a plurality of locations, each said location is associated with a different address and is configured to store a plurality of condition values; </div>
      <div class="claim-text">no earlier than issuing said first instruction of said first type, issuing a second instruction of said first type with a second address for storing a second set of at least one condition value; </div>
      <div class="claim-text">generating a second set of at least one condition value; </div>
      <div class="claim-text">storing said second set of at least one condition value in a second storage location having said second address in said condition storage, wherein said first storage location and said second storage location are different storage locations in said condition storage; </div>
      <div class="claim-text">no earlier than issuing said second instruction of said first type, issuing an instruction of said second type with said first address that depends on said first set of at least one condition value stored in said first storage location; and </div>
      <div class="claim-text">accessing said first set of at least one condition value stored in said condition storage having said first address. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" id="US-6253313-B1-CLM-00020" class="claim">
      <div class="claim-text">20. The method of claim <b>19</b> further wherein said processing of said first instruction of said first type comprises the step of decrementing a data value.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" id="US-6253313-B1-CLM-00021" class="claim">
      <div class="claim-text">21. The method of claim <b>19</b> further wherein said processing of said instruction of said second type comprises the step of evaluating whether a branch is taken.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="22" id="US-6253313-B1-CLM-00022" class="claim">
      <div class="claim-text">22. The method of claim <b>19</b> further wherein said processing step comprises the step of storing at least three sets of condition values.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="23" id="US-6253313-B1-CLM-00023" class="claim">
      <div class="claim-text">23. The method of claim <b>22</b> further wherein there are four sets of condition values.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="24" id="US-6253313-B1-CLM-00024" class="claim">
      <div class="claim-text">24. The method of claim <b>19</b> further comprising the step of generating a target address of a branch type instruction.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" id="US-6253313-B1-CLM-00025" class="claim">
      <div class="claim-text">25. The method of claim <b>19</b> further wherein each step of storing sets of condition values comprises the step of storing condition codes, each said condition code having a plurality of flag bits.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES54566452" lang="EN" load-source="patent-office" class="description">
    <p>This is a divisional of U.S. Ser. No. 08/254,687, filed Jun. 6, 1994, now U.S. Pat. No. 5,517,628, which is a divisional of Ser. No. 08/093,794, filed Jul. 19, 1993, now abandoned, which is a continuation of Ser. No. 07/913,736, filed Jul. 14, 1992, now abandoned, which is a continuation of Ser. No. 07/560,093, filed Jul. 30, 1990, now abandoned, which is a divisional of Ser. No. 07/372,247, filed Jun. 26, 1989, now U.S. Pat. No. 5,021,945, which is a divisional of Ser. No. 06/794,221, filed Oct. 31, 1985, now U.S. Pat. No. 4,847,755.</p>
    <heading>BACKGROUND OF THE INVENTION</heading> <p>1. Field of the Invention</p>
    <p>This invention generally relates to parallel processor computer systems and, more particularly, to parallel processor computer systems having software for detecting natural concurrencies in instruction streams and having a plurality of processor elements for processing the detected natural concurrencies.</p>
    <p>2. Description of the Prior Art</p>
    <p>Almost all prior art computer systems are of the Von Neumann construction. In fact, the first four generations of computers are Von Neumann machines which use a single large processor to sequentially process data. In recent years, considerable effort has been directed towards the creation of a fifth generation computer which is not of the Von Neumann type. One characteristic of the so-called fifth generation computer relates to its ability to perform parallel computation through use of a number of processor elements. With the advent of very large scale integration (VLSI) technology, the economic cost of using a number of individual processor elements becomes cost effective.</p>
    <p>Whether or not an actual fifth generation machine has yet been constructed is subject to debate, but various features have been defined and classified. Fifth-generation machines should be capable of using multiple-instruction, multiple-data (MIMD) streams rather than simply being a single instruction, multiple-data (SIMD) system typical of fourth generation machines. The present invention is of the fifth-generation non-Von Neumann type. It is capable of using MIMD streams in single context (SC-MIMD) or in multiple context (MC-MIMD) as those terms are defined below. The present invention also finds application in the entire computer classification of single and multiple context SIMD (SC-SIMD and MC-SIMD) machines as well as single and multiple context, single-instruction, single data (SC-SISD and MC-SISD) machines.</p>
    <p>While the design of fifth-generation computer systems is fully in a state of flux, certain categories of systems have been defined. Some workers in the field base the type of computer upon the manner in which control or synchronization of the system is performed. The control classification includes control-driven, data-driven, and reduction (or demand) driven. The control-driven system utilizes a centralized control such as a program counter or a master processor to control processing by the slave processors. An example of a control-driven machine is the Non-von-1 machine at Columbia University. In data-driven systems, control of the system results from the actual arrival of data required for processing. An example of a data-driven machine is the University of Manchester dataflow machine developed in England by Ian Watson. Reduction driven systems control processing when the processed activity demands results to occur. An example of a reduction processor is the MAGO reduction machine being developed at the University of North Carolina, Chapel Hill. The characteristics of the non-von-1 machine, the Manchester machine, and the MAGO reduction machine are carefully discussed in Davis, Computer Architecture, <i>IEEE Spectrum</i>, November, 1983. In comparison, data-driven and demand-driven systems are decentralized approaches whereas control-driven systems represent a centralized approach. The present invention is more properly categorized in a fourth classification which could be termed time-driven. Like data-driven and demand-driven systems, the control system of the present invention is decentralized. However, like the control-driven system, the present invention conducts processing when an activity is ready for execution.</p>
    <p>Most computer systems involving parallel processing concepts have proliferated from a large number of different types of computer architectures. In such cases, the unique nature of the computer architecture mandates or requires either its own processing language or substantial modification of an existing language to be adapted for use. To take advantage of the highly parallel structure of such computer architectures, the programmer is required to have an intimate knowledge of the computer architecture in order to write the necessary software. As a result, preparing programs for these machines requires substantial amounts of the users effort, money and time.</p>
    <p>Concurrent to this activity, work has also been progressing on the creation of new software and languages, independent of a specific computer architecture, that will expose (in a more direct manner), the inherent parallelism of the computation process. However, most effort in designing supercomputers has been concentrated in developing new hardware with much less effort directed to developing new software.</p>
    <p>Davis has speculated that the best approach to the design of a fifth-generation machine is to concentrate efforts on the mapping of the concurrent program tasks in the software onto the physical hardware resources of the computer architecture. Davis terms this approach one of task-allocation and touts it as being the ultimate key to successful fifth-generation architectures. He categorizes the allocation strategies into two generic types. Static allocations are performed once, prior to execution, whereas dynamic allocations are performed by the hardware whenever the program is executed or run. The present invention utilizes a static allocation strategy and provides task allocations for a given program after compilation and prior to execution. The recognition of the task allocation approach in the design of fifth generation machines was used by Davis in the design of his Data-driven Machine-II constructed at the University of Utah. In the Data-driven Machine-II, the program was compiled into a program graph that resembles the actual machine graph or architecture.</p>
    <p>Task allocation is also referred to as scheduling in Gaiski et al, Essential Issues in Multi-processor Systems, <i>Computer</i>, June, 1985. Gajski et al set forth levels of scheduling to include high level, intermediate level, and low level scheduling. The present invention is one of low-level scheduling, but it does not use conventional scheduling policies of first-in-first-out, round-robin, shortest type in job-first, or shortest-remaining-time. Gajski et al also recognize the advantage of static scheduling in that overhead costs are paid at compile time. However, Gajski et al's recognized disadvantage, with respect to static scheduling, of possible inefficiencies in guessing the run time profile of each task is not found in the present invention. Therefore, the conventional approaches to low-level static scheduling found in the Occam language and the Bulldog compiler are not found in the software portion of the present invention. Indeed, the low-level static scheduling of the present invention provides the same type, if not better, utilization of the processors commonly seen in dynamic scheduling by the machine at run time. Furthermore, the low-level static scheduling of the present invention is performed automatically without intervention of programmers as required (for example) in the Occam language.</p>
    <p>Davis further recognizes that communication is a critical feature in concurrent processing in that the actual physical topology of the system significantly influences the overall performance of the system.</p>
    <p>For example, the fundamental problem found in most data-flow machines is the large amount of communication overhead in moving data between the processors. When data is moved over a bus, significant overhead, and possible degradation of the system, can result if data must contend for access to the bus. For example, the Arvind data-flow machine, referenced in Davis, utilizes an I-structure stream in order to allow the data to remain in one place which then becomes accessible by all processors. The present invention, in one aspect, teaches a method of hardware and software based upon totally coupling the hardware resources thereby significantly simplifying the communication problems inherent in systems that perform multiprocessing.</p>
    <p>Another feature of non-Von Neumann type multiprocessor systems is the level of granularity of the parallelism being processed. Gajski et al term this partitioning. The goal in designing a system, according to Gajski et al, is to obtain as much parallelism as possible with the lowest amount of overhead. The present invention performs concurrent processing at the lowest level available, the per instruction level. The present invention, in another aspect, teaches a method whereby this level of parallelism is obtainable without execution time overhead.</p>
    <p>Despite all of the work that has been done with multi-processor parallel machines, Davis (Id. at 99) recognizes that such software and/or hardware approaches are primarily designed for individual tasks and are not universally suitable for all types of tasks or programs as has been the hallmark with Von Neumann architectures. The present invention sets forth a computer system and method that is generally suitable for many different types of tasks since it operates on the natural concurrencies existent in the instruction stream at a very fine level of granularity.</p>
    <p>All general purpose computer systems and many special purpose computer systems have operating systems or monitor/control programs which support the processing of multiple activities or programs. In some cases this processing occurs simultaneously; in other cases the processing alternates among the activities such that only one activity controls the processing resources at any one time. This latter case is often referred to as time sharing, time slicing, or concurrent (versus simultaneous) execution, depending on the particular computer system. Also depending on the specific system, these individual activities or programs are usually referred to as tasks, processes, or contexts. In all cases, there is a method to support the switching of control among these various programs and between the programs and the operating system, which is usually referred to as task switching, process switching, or context switching. Throughout this document, these terms are considered synonymous, and the terms context and context switching are generally used.</p>
    <p>The present invention, therefore, pertains to a non-Von Neumann MIMD computer system capable of simultaneously operating upon many different and conventional programs by one or more different users. The natural concurrencies in each program are statically allocated, at a very fine level of granularity, and intelligence is added to the instruction stream at essentially the object code level. The added intelligence can include, for example, a logical processor number and an instruction firing time in order to provide the time-driven decentralized control for the present invention. The detection and low level scheduling of the natural concurrencies and the adding of the intelligence occurs only once for a given program, after conventional compiling of the program, without user intervention and prior to execution. The results of this static allocation are executed on a system containing a plurality of processor elements. In one embodiment of the invention, the processors are identical. The processor elements, in this illustrated embodiment, contain no execution state information from the execution of previous instructions, that is, they are context free. In addition, a plurality of context files, one for each user, are provided wherein the plurality of processor elements can access any storage resource contained in any context file through total coupling of the processor element to the shared resource during the processing of an instruction. In a preferred aspect of the present invention, no condition code or results registers are found on the individual processor elements.</p>
    <heading>SUMMARY OF INVENTION</heading> <p>The present invention provides a method and a system that is non-Von Neumann and one which is adaptable for use in single or multiple context SISD, SIMD, and MIMD configurations. The method and system is further operative upon a myriad of conventional programs without user intervention.</p>
    <p>In one aspect, the present invention statically determines at a very fine level of granularity, the natural concurrencies in the basic blocks (BBs) of programs at essentially the object code level and adds intelligence to the instruction stream in each basic block to provide a time driven decentralized control. The detection and low level scheduling of the natural concurrencies and the addition of the intelligence occurs only once for a given program after conventional compiling and prior to execution. At this time, prior to program execution, the use during later execution of all instruction resources is assigned.</p>
    <p>In another aspect, the present invention further executes the basic blocks containing the added intelligence on a system containing a plurality of processor elements each of which, in this particular embodiment, does not retain execution state information from prior operations. Hence, all processor elements in accordance with this embodiment of the invention are context free. Instructions are selected for execution based on the instruction firing time. Each processor element in this embodiment is capable of executing instructions on a per-instruction basis such that dependent instructions can execute on the same or different processor elements. A given processor element in the present invention is capable of executing an instruction from one context followed by an instruction from another context. All operating and context information necessary for processing a given instruction is then contained elsewhere in the system.</p>
    <p>It should be noted that many alternative implementations of context free processor elements are possible. In a non-pipelined implementation each processor element is monolithic and executes a single instruction to its completion prior to accepting another instruction.</p>
    <p>In another aspect of the invention, the context free processor is a pipelined processor element, in which each instruction requires several machine instruction clock cycles to complete. In general, during each clock cycle, a new instruction enters the pipeline and a completed instruction exists the pipeline, giving an effective instruction execution time of a single instruction clock cycle. However, it is also possible to microcode some instructions to perform complicated functions requiring many machine instruction cycles. In such cases the entry of new instructions is suspended until the complex instruction completes, after which the normal instruction entry and exit sequence in each clock cycle continues. Pipelining is a standard processor implementation technique and is discussed in more detail later.</p>
    <p>The system and method of the present invention are described in the following drawing and specification.</p>
    <heading>DESCRIPTION OF THE DRAWING</heading> <p>Other objects, features, and advantages of the invention will appear from the following description taken together with the drawings in which:</p>
    <p>FIG. 1 is the generalized flow representation of the TOLL software of the present invention;</p>
    <p>FIG. 2 is a graphic representation of a sequential series of basic blocks found within the conventional compiler output;</p>
    <p>FIG. 3 is a graphical presentation of the extended intelligence added to each basic block according to one embodiment of the present invention;</p>
    <p>FIG. 4 is a graphical representation showing the details of the extended intelligence added to each instruction within a given basic block according to one embodiment of the present invention;</p>
    <p>FIG. 5 is the breakdown of the basic blocks into discrete execution sets;</p>
    <p>FIG. 6 is a block diagram presentation of the architectural structure of apparatus according to a preperred embodiment of the present invention;</p>
    <p>FIGS. 7<i>a</i>-<b>7</b> <i>c </i>represent an illustration of the network interconnections during three successive instruction firing times;</p>
    <p>FIGS. 8-11 are the flow diagrams setting forth features of the software according to one embodiment of the present invention;</p>
    <p>FIG. 12 is a diagram describing one preferred form of the execution sets in the TOLL software:</p>
    <p>FIG. 13 sets forth the register file organization according to a preferred embodiment of the present invention:</p>
    <p>FIG. 14 illustrates a transfer between registers in different levels during a subroutine call;</p>
    <p>FIG. 15 sets forth the structure of a logical resource driver (LRD) according to a preferred embodiment of the present invention;</p>
    <p>FIG. 16 sets forth the structure of an instruction cache control and of the caches according to a preferred embodiment of the present invention;</p>
    <p>FIG. 17 sets forth the structure of a PIQ buffer unit and a PIQ bus interface unit according to a preferred embodiment of the present invention;</p>
    <p>FIG. 18 sets forth interconnection of processor elements through the PE-LRD network to a PIQ processor alignment circuit according to a preferred embodiment of the present invention;</p>
    <p>FIG. 19 sets forth the structure of a branch execution unit according to a preferred embodiment of the present invention;</p>
    <p>FIG. 20 illustrates the organization of the condition code storage of a context file according to a preferred embodiment of the present invention;</p>
    <p>FIG. 21 sets forth the structure of one embodiment of a pipelined processor element according to the present invention; and</p>
    <p>FIGS. <b>22</b>(<i>a</i>) through <b>22</b>(<i>d</i>) set forth the data structures used in connection with the processor element of FIG. <b>21</b>.</p>
    <heading>GENERAL DESCRIPTION</heading> <p>1. Introduction</p>
    <p>In the following two sections, a general description of the software and hardware of the present invention takes place. The system of the present invention is designed based upon a unique relationship between the hardware and software components. While many prior art approaches have primarily provided for multiprocessor parallel processing based upon a new architecture design or upon unique software algorithms, the present invention is based upon a unique hardware/software relationship. The software of the present invention provides the intelligent information for the routing and synchronization of the instruction streams through the hardware. In the performance of these tasks, the software spatially and temporally manages all user accessible resources, for example, general registers, condition code storage registers, memory and stack pointers. The routing and synchronization are performed without user intervention, and do not require changes to the original source code. Additionally, the analysis of an instruction stream to provide the additional intelligent information for controlling the routing and synchronization of the instruction stream is performed only once during the program preparation process (often called static allocation) of a given piece of software, and is not performed during execution (often called dynamic allocation) as is found in some conventional prior art approaches. The analysis effected according to the invention is hardware dependent, is performed on the object code output from conventional compilers, and advantageously, is therefore programming language independent.</p>
    <p>In other words, the software, according to the invention, maps the object code program onto the hardware of the system so that it executes more efficiently than is typical of prior art systems. Thus the software must handle all hardware idiosyncrasies and their effects on execution of the program instructions stream. For example, the software must accommodate, when necessary, processor elements which are either monolithic single cycle or pipelined.</p>
    <p>2. General Software Description</p>
    <p>Referring to FIG. 1, the software of the present invention, generally termed TOLL, is located in a computer processing system <b>160</b>. Processing system <b>160</b> operates on a standard compiler output <b>100</b> which is typically object code or an intermediate object code such as p-code. The output of a conventional compiler is a sequential stream of object code instructions hereinafter referred to as the instruction stream. Conventional language processors typically perform the following functions in generating the sequential instruction stream:</p>
    <p>1. lexical scan of the input text,</p>
    <p>2. syntactical scan of the condensed input text including symbol table construction,</p>
    <p>3. performance of machine independent optimization including parallelism detection and vectorization, and</p>
    <p>4. an intermediate (PSEUDO) code generation taking into account instruction functionality, resources required, and hardware structural properties.</p>
    <p>In the creation of the sequential instruction stream, the conventional compiler creates a series of basic blocks (BBs) which are single entry single exit (SESE) groups of contiguous instructions. See, for example, Alfred v. Aho and Jeffery D. Ullman, <i>Principles of Compiler Design</i>, Addison Wesley, 1979, pg. 6, 409, 412-413 and David Gries, <i>Compiler Construction for Digital Computers</i>, Wiley, 1971. The conventional compiler, although it utilizes basic block information in the performance of its tasks, provides an output stream of sequential instructions without any basic block designations. The TOLL software, in this illustrated embodiment of the present invention, is designed to operate on the formed basic blocks (BBs) which are created within a conventional compiler. In each of the conventional SESE basic blocks there is exactly one branch (at the end of the block) and there are no control dependencies. The only relevant dependencies within the block are those between the resources required by the instructions.</p>
    <p>The output of the compiler <b>100</b> in the basic block format is illustrated in FIG. <b>2</b>. Referring to FIG. 1, the TOLL software <b>110</b> of the present invention being processed in the computer <b>160</b> performs three basic determining functions on the compiler output <b>100</b>. These functions are to analyze the resource usage of the instructions <b>120</b>, extend intelligence for each instruction in each basic block <b>130</b>, and to build execution sets composed of one or more basic blocks <b>140</b>. The resulting output of these three basic functions <b>120</b>, <b>130</b>, and <b>140</b> from processor <b>160</b> is the TOLL software output <b>150</b> of the present invention.</p>
    <p>As noted above, the TOLL software of the present invention operates on a compiler output <b>100</b> only once and without user intervention. Therefore, for any given program, the TOLL software need operate on the compiler output <b>100</b> only once.</p>
    <p>The functions <b>120</b>, <b>130</b>, <b>140</b> of the TOLL software <b>110</b> are, for example, to analyze the instruction stream in each basic block for natural concurrencies, to perform a translation of the instruction stream onto the actual hardware system of the present invention, to alleviate any hardware induced idiosyncrasies that may result from the translation process, and to encode the resulting instruction stream into an actual machine language to be used with the hardware of the present invention. The TOLL software <b>110</b> performs these functions by analyzing the instruction stream and then assigning processor elements and resources as a result thereof. In one particular embodiment, the processors are context free. The TOLL software <b>110</b> provides the synchronization of the overall system by, for example, assigning appropriate firing times to each instruction in the output instruction stream.</p>
    <p>Instructions can be dependent on one another in a variety of ways although there are only three basic types of dependencies. First, there are procedural dependencies due to the actual structure of the instruction stream; that is, instructions may follow one another in other than a sequential order due to branches, jumps, etc. Second, operational dependencies are due to the finite number of hardware elements present in the system. These hardware elements include the general registers, condition code storage, stack pointers, processor elements, and memory. Thus if two instructions are to execute in parallel, they must not require the same hardware element unless they are both reading that element (provided of course, that the element is capable of being read simultaneously). Finally, there are data dependencies between instructions in the instruction stream. This form of dependency will be discussed at length later and is particularly important if the processor elements include pipelined processors. Within a basic block, however, only data and operational dependencies are present.</p>
    <p>The TOLL software <b>110</b> must maintain the proper execution of a program. Thus, the TOLL software must assure that the code output <b>150</b>, which represents instructions which will execute in parallel, generates the same results as those of the original serial code. To do this, the code <b>150</b> must access the resources in the same relative sequence as the serial code for instructions that are dependent on one another; that is, the relative ordering must be satisfied. However, independent sets of instructions may be effectively executed out of sequence.</p>
    <p>In Table 1 is set forth an example of a SESE basic block representing the inner loop of a matrix multiply routine. While, this example will be used throughout this specification, the teachings of the present invention are applicable to any instruction stream. Referring to Table 1, the instruction designation is set forth in the right hand column and a conventional object code functional representation, for this basic block, is represented in the left hand column.</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="3" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="35PT"> </colspec> <colspec colname="1" align="left" colwidth="91PT"> </colspec> <colspec colname="2" align="left" colwidth="91PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" class="description-td" colspan="3">TABLE 1</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">OBJECT CODE</td>
                <td morerows="0" valign="top" class="description-td">INSTRUCTION</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">LD R0, (R10) +</td>
                <td morerows="0" valign="top" class="description-td">I0</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">LD R1, (R11) +</td>
                <td morerows="0" valign="top" class="description-td">I1</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">MM R0, R1, R2</td>
                <td morerows="0" valign="top" class="description-td">I2</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">ADD R2, R3, R3</td>
                <td morerows="0" valign="top" class="description-td">I3</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">DEC R4</td>
                <td morerows="0" valign="top" class="description-td">I4</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">BRNZR LOOP</td>
                <td morerows="0" valign="top" class="description-td">I5</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>The instruction stream contained within the SESE basic block set forth in Table 1 performs the following functions. In instruction I<b>0</b>, register R<b>0</b> is loaded with the contents of memory whose address is contained in R<b>10</b>. The instruction shown above increments the contents of R<b>10</b> after the address has been fetched from R<b>10</b>. The same statement can be made for instruction I<b>1</b>, with the exception that register R<b>1</b> is loaded and register R<b>11</b> is incremented. Instruction I<b>2</b> causes the contents of registers R<b>0</b> and R<b>1</b> to be multiplied and the result is stored in register R<b>2</b>. In instruction I<b>3</b>, the contents of register R<b>2</b> and register R<b>3</b> are added and the result is stored in register R<b>3</b>. in instruction I<b>4</b>, register R<b>4</b> is decremented. Instructions I<b>2</b>, I<b>3</b> and I<b>4</b> also generate a set of condition codes that reflect the status of their respective execution. In instruction I<b>5</b>, the contents of register R<b>4</b> are indirectly tested for zero (via the condition codes generated by instruction I<b>4</b>). A branch occurs if the decrement operation produced a non-zero value; otherwise execution proceeds with the first instruction of the next basic block.</p>
    <p>Referring to FIG. 1, the first function performed by the TOLL software <b>110</b> is to analyze the resource usage of the instructions. In the illustrated example, these are instructions I<b>0</b> through I<b>5</b> of Table I. The TOLL software <b>110</b> thus analyzes each instruction to ascertain the resource requirements of the instruction.</p>
    <p>This analysis is important in determining whether or not any resources are shared by any instructions and, therefore, whether or not the instructions are independent of one another. Clearly, mutually independent instructions can be executed in parallel and are termed naturally concurrent. Instructions that are independent can be executed in parallel and do not rely on one another for any information nor do they share any hardware resources in other than a read only manner.</p>
    <p>On the other hand, instructions that are dependent on one another can be formed into a set wherein each instruction in the set is dependent on every other instruction in that set. The dependency may not be direct. The set can be described by the instructions within the set, or conversely, by the resources used by the instructions in the set. Instructions within different sets are completely independent of one another, that is, there are no resources shared by the sets. Hence, the sets are independent of one another.</p>
    <p>In the example of Table 1, the TOLL software will determine that there are two independent sets of dependent instructions:</p>
    <p>Set 1: CC<b>1</b>: I<b>0</b>, I<b>1</b>, I<b>2</b>, I<b>3</b> </p>
    <p>Set 2: CC<b>2</b>: I<b>4</b>, I<b>5</b> </p>
    <p>As can be seen, instructions I<b>4</b> and I<b>5</b> are independent of instructions I<b>0</b>-I<b>3</b>. In set 2, I<b>5</b> is directly dependent on I<b>4</b>. In set 1, I<b>2</b> is directly dependent on I<b>0</b> and I<b>1</b>. Instruction I<b>3</b> is directly dependent on I<b>2</b> and indirectly dependent on I<b>0</b> and I<b>1</b>.</p>
    <p>The TOLL software of the present invention detects these independent sets of dependent instructions and assigns a condition code group of designation(s), such as CC<b>1</b> and CC<b>2</b>, to each set. This avoids the operational dependency that would occur if only one group or set of condition codes were available to the instruction stream.</p>
    <p>In other words, the results of the execution of instructions I<b>0</b> and I<b>1</b> are needed for the execution of instruction I<b>2</b>. Similarly, the results of the execution of instruction I<b>2</b> are needed for the execution of instruction I<b>3</b>. In performing this analyses, the TOLL software <b>110</b> determines if an instruction will perform a read and/or a write to a resource. This functionality is termed the resource requirement analysis of the instruction stream.</p>
    <p>It should be noted that, unlike the teachings of the prior art, the present invention teaches that it is not necessary for dependent instructions to execute on the same processor element. The determination of dependencies is needed only to determine condition code sets and to determine instruction firing times, as will be described later. The present invention can execute dependent instructions on different processor elements, in one illustrated embodiment, because of the context free nature of the processor elements and the total coupling of the processor elements to the shared resources, such as the register files, as will also be described below.</p>
    <p>The results of the analysis stage <b>120</b>, for the example set forth in Table 1, are set forth in Table 2.</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="2" colsep="0" rowsep="0" align="left"> <colspec colname="1" align="left" colwidth="56PT"> </colspec> <colspec colname="2" align="left" colwidth="161PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td namest="1" nameend="2" morerows="0" rowsep="1" valign="top" class="description-td" colspan="2">TABLE 2</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="2"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">INSTRUCTION</td>
                <td morerows="0" valign="top" class="description-td">FUNCTION</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="2"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">I0</td>
                <td morerows="0" valign="top" class="description-td">Memory Read, Reg. Write, Reg. Read &amp; Write</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">I1</td>
                <td morerows="0" valign="top" class="description-td">Memory Read, Reg. Write, Reg. Read &amp; write</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">I2</td>
                <td morerows="0" valign="top" class="description-td">Two Reg. Reads, Reg. Write, Set Cond. Code (Set #1)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">I3</td>
                <td morerows="0" valign="top" class="description-td">Two Reg. Reads, Reg. Write, Set Cond. Code (Set #1)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">I4</td>
                <td morerows="0" valign="top" class="description-td">Read Reg., Reg. Write, Set Cond. Code (Set #2)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">I5</td>
                <td morerows="0" valign="top" class="description-td">Read Cond. Code (Set #2)</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="2"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>In Table 2, for instructions I<b>0</b> and I<b>1</b>, a register is read and written followed by a memory read (at a distinct address), followed by a register write. Likewise, condition code writes and register reads and writes occur for instructions I<b>2</b> through I<b>4</b>. Finally, instruction I<b>5</b> is a simple read of a condition code storage register and a resulting branch or loop.</p>
    <p>The second step or pass <b>130</b> through the SESE basic block <b>100</b> is to add or extend intelligence to each instruction within the basic block. In the preferred embodiment of the invention, this is the assignment of an instruction's execution time relative to the execution times of the other instructions in the stream, the assignment of a processor number on which the instruction is to execute and the assignment of any so-called static shared context storage mapping information that may be needed by the instruction.</p>
    <p>In order to assign the firing time to an instruction, the temporal usage of each resource required by the instruction must be considered. In the illustrated embodiment, the temporal usage of each resource is characterized by a free time and a load time. The free time is the last time the resource was read or written by an instruction. The load time is the last time the resource was modified by an instruction. If an instruction is going to modify a resource, it must execute the modification after the last time the resource was used, in other words, after the free time. If an instruction is going to read the resource, it must perform the read after the last time the resource has been loaded, in other words, after the load time.</p>
    <p>The relationship between the temporal usage of each resource and the actual usage of the resource is as follows. If an instruction is going to write/modify the resource, the last time the resource is read or written by other instructions (i.e., the free time for the resource) plus one time interval will be the earliest firing time for this instruction. The plus one time interval comes from the fact that an instruction is still using the resource during the free time. On the other hand, if the instruction reads a resource, the last time the resource is modified by other instructions (i.e., the load time for the resource) plus one time interval will be the earliest instruction firing time. The plus one time interval comes from the time required for the instruction that is performing the load to execute.</p>
    <p>The discussion above assumes that the exact location of the resource that is accessed is known. This is always true of resources that are directly named such as general registers and condition code storage. However, memory operations may, in general, be to locations unknown at compile time. In particular, addresses that are generated by effective addressing constructs fall in this class. In the previous example, it has been assumed (for the purposes of communicating the basic concepts of TOLL) that the addresses used by instructions I<b>0</b> and I<b>1</b> are distinct. If this were not the case, the TOLL software would assure that only those instructions that did not use memory would be allowed to execute in parallel with an instruction that was accessing an unknown location in memory.</p>
    <p>The instruction firing time is evaluated by the TOLL software <b>110</b> for each resource that the instruction uses. These candidate firing times are then compared to determine which is the largest or latest time. The latest time determines the actual firing time assigned to the instruction. At this point, the TOLL software <b>110</b> updates all of the resources' free and load times, to reflect the firing time assigned to the instruction. The TOLL software <b>110</b> then proceeds to analyze the next instruction.</p>
    <p>There are many methods available for determining inter-instruction dependencies within a basic block. The previous discussion is just one possible implementation assuming a specific compiler-TOLL partitioning. Many other compiler-TOLL partitionings and methods for determining inter-instruction dependencies may be possible and realizable to one skilled in the art. Thus, the illustrated TOLL software uses a linked list analysis to represent the data dependencies within a basic block. Other possible data structures that could be used are trees, stacks, etc.</p>
    <p>Assume a linked list representation is used for the analysis and representation of the inter-instruction dependencies. Each register is associated with a set of pointers to the instructions that use the value contained in that register. For the matrix multiply example in Table 1, the resource usage is set forth in Table 3:</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="4" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="35PT"> </colspec> <colspec colname="1" align="left" colwidth="63PT"> </colspec> <colspec colname="2" align="left" colwidth="56PT"> </colspec> <colspec colname="3" align="left" colwidth="63PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" class="description-td" colspan="4">TABLE 3</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Resource</td>
                <td morerows="0" valign="top" class="description-td">Loaded By</td>
                <td morerows="0" valign="top" class="description-td">Read By</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R0</td>
                <td morerows="0" valign="top" class="description-td">I0</td>
                <td morerows="0" valign="top" class="description-td">I2</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R1</td>
                <td morerows="0" valign="top" class="description-td">I1</td>
                <td morerows="0" valign="top" class="description-td">I2</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R2</td>
                <td morerows="0" valign="top" class="description-td">I2</td>
                <td morerows="0" valign="top" class="description-td">I3</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R3</td>
                <td morerows="0" valign="top" class="description-td">I3</td>
                <td morerows="0" valign="top" class="description-td">I3, I2</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R4</td>
                <td morerows="0" valign="top" class="description-td">I4</td>
                <td morerows="0" valign="top" class="description-td">I5</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R10</td>
                <td morerows="0" valign="top" class="description-td">I0</td>
                <td morerows="0" valign="top" class="description-td">I0</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R11</td>
                <td morerows="0" valign="top" class="description-td">I1</td>
                <td morerows="0" valign="top" class="description-td">I1</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>Thus, by following the Read by links and knowing the resource utilization for each instruction, the independencies of Sets 1 and 2, above, are constructed in the analyze instruction stage <b>120</b> (FIG. 1) by TOLL <b>110</b>.</p>
    <p>For purposes of analyzing further the example of Table 1, it is assumed that the basic block commences with an arbitrary time interval in an instruction stream, such as, for example, time interval T<b>16</b>. In other words, this particular basic block in time sequence is assumed to start with time interval T<b>16</b>. The results of the analysis in stage <b>120</b> are set forth in Table 4.</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="8" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="14PT"> </colspec> <colspec colname="1" align="left" colwidth="35PT"> </colspec> <colspec colname="2" align="left" colwidth="28PT"> </colspec> <colspec colname="3" align="left" colwidth="28PT"> </colspec> <colspec colname="4" align="left" colwidth="28PT"> </colspec> <colspec colname="5" align="left" colwidth="28PT"> </colspec> <colspec colname="6" align="left" colwidth="28PT"> </colspec> <colspec colname="7" align="left" colwidth="28PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="7" morerows="0" rowsep="1" valign="top" class="description-td" colspan="8">TABLE 4</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="7" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="8"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">REG</td>
                <td morerows="0" valign="top" class="description-td">I0</td>
                <td morerows="0" valign="top" class="description-td">I1</td>
                <td morerows="0" valign="top" class="description-td">I2</td>
                <td morerows="0" valign="top" class="description-td">I3</td>
                <td morerows="0" valign="top" class="description-td">I4</td>
                <td morerows="0" valign="top" class="description-td">I5</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="7" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="8"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R0</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">T17</td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R1</td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R2</td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">
                  <u>T17</u>
                </td>
                <td morerows="0" valign="top" class="description-td">T18</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R3</td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">
                  <u>T18</u>
                </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R4</td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">T16</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">CC1</td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">T17</td>
                <td morerows="0" valign="top" class="description-td">T18</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">CC2</td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">
                  <u>T17</u>
                </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R10</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R11</td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">T16</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="7" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="8"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>The vertical direction in Table 4 represents the general registers and condition code storage registers. The horizontal direction in the table represents the instructions in the basic block example of Table 1. The entries in the table represent usage of a register by an instruction. Thus, instruction I<b>0</b> requires that register R<b>10</b> be read and written and register R<b>0</b> written at time T<b>16</b>, the start of execution of the basic block.</p>
    <p>Under the teachings of the present invention, there is no reason that registers R<b>1</b>, R<b>11</b>, and R<b>4</b> cannot also have operations performed on them during time T<b>16</b>. The three instructions, I<b>0</b>, I<b>1</b>, and I<b>4</b>, are data independent of each other and can be executed concurrently during time T<b>16</b>. Instruction I<b>2</b>, however, requires first that registers R<b>0</b> and R<b>1</b> be loaded so that the results of the load operation can be multiplied. The results of the multiplication are stored in register R<b>2</b>. Although, register R<b>2</b> could in theory be operated on in time T<b>16</b>, instruction I<b>2</b> is data dependent upon the results of loading registers R<b>0</b> and R<b>1</b>, which occurs during time T<b>16</b>. Therefore, the completion of instruction I<b>2</b> must occur during or after time frame T<b>17</b>. Hence, in Table 4 above, the entry T<b>17</b> for the intersection of instruction I<b>2</b> and register R<b>2</b> is underlined because it is data dependent. Likewise, instruction I<b>3</b> requires data in register R<b>2</b> which first occurs during time T<b>17</b>. Hence, instruction I<b>3</b> can operate on register R<b>2</b> only during or after time T<b>18</b>. Instruction I<b>5</b> depends upon the reading of the condition code storage CC<b>2</b> which is updated by instruction I<b>4</b>. The reading of the condition code storage CC<b>2</b> is data dependent upon the results stored in time T<b>16</b> and, therefore, must occur during or after the next time, T<b>17</b>.</p>
    <p>Hence, in stage <b>130</b>, the object code instructions are assigned instruction firing times (IFTs) as set forth in Table 5 based upon the above analysis.</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="3" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="28PT"> </colspec> <colspec colname="1" align="left" colwidth="84PT"> </colspec> <colspec colname="2" align="left" colwidth="105PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" class="description-td" colspan="3">TABLE 5</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">OBJECT CODE</td>
                <td morerows="0" valign="top" class="description-td">INSTRUCTION FIRING</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">INSTRUCTION</td>
                <td morerows="0" valign="top" class="description-td">TIME (IFT)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I0</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I1</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I2</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I3</td>
                <td morerows="0" valign="top" class="description-td">T18</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I4</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I5</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>Each of the instructions in the sequential instruction stream in a basic block can be performed in the assigned time intervals. As is clear in Table 5, the same six instructions of Table 1, normally processed sequentially in six cycles, can be processed, under the teachings of the present invention, in only three firing times: T<b>16</b>, T<b>17</b>, and T<b>18</b>. The instruction firing time (IFT) provides the time-driven feature of the present invention.</p>
    <p>The next function performed by stage <b>130</b>, in the illustrated embodiment, is to reorder the natural concurrencies in the instruction stream according to instruction firing times (IFTs) and then to assign the instructions to the individual logical parallel processors. It should be noted that the reordering is only required due to limitations in currently available technology. If true fully associative memories were available, the reordering of the stream would not be required and the processor numbers could be assigned in a first come, first served manner. The hardware of the instruction selection mechanism could be appropriately modified by one skilled in the art to address this mode of operation.</p>
    <p>For example, assuming currently available technology, and a system with four parallel processor elements (PEs) and a branch execution unit (BEU) within each LRD, the processor elements and the branch execution unit can be assigned, under the teachings of the present invention, as set forth in Table 6 below. It should be noted that the processor elements execute all non-branch instructions, while the branch execution unit (BEU) of the present invention executes all branch instructions. These hardware circuitries will be described in greater detail subsequently.</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="5" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="14PT"> </colspec> <colspec colname="1" align="center" colwidth="84PT"> </colspec> <colspec colname="2" align="center" colwidth="42PT"> </colspec> <colspec colname="3" align="center" colwidth="35PT"> </colspec> <colspec colname="4" align="center" colwidth="42PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="4" morerows="0" rowsep="1" valign="top" class="description-td" colspan="5">TABLE 6</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="4" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="5"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Logical Processor Number</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
                <td morerows="0" valign="top" class="description-td">T18</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="4" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="5"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">0</td>
                <td morerows="0" valign="top" class="description-td">I0</td>
                <td morerows="0" valign="top" class="description-td">I2</td>
                <td morerows="0" valign="top" class="description-td">I3</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">1</td>
                <td morerows="0" valign="top" class="description-td">I1</td>
                <td morerows="0" valign="top" class="description-td"></td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">2</td>
                <td morerows="0" valign="top" class="description-td">I4</td>
                <td morerows="0" valign="top" class="description-td"></td>
                <td morerows="0" valign="top" class="description-td"></td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">3</td>
                <td morerows="0" valign="top" class="description-td"></td>
                <td morerows="0" valign="top" class="description-td"></td>
                <td morerows="0" valign="top" class="description-td"></td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">BEU</td>
                <td morerows="0" valign="top" class="description-td"></td>
                <td morerows="0" valign="top" class="description-td">I5 (delay)</td>
                <td morerows="0" valign="top" class="description-td"></td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="4" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="5"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>Hence, under the teachings of the present invention, during time interval T<b>16</b>, parallel processor elements <b>0</b>, <b>1</b>, and <b>2</b> concurrently process instructions I<b>0</b>, I<b>1</b>, and I<b>4</b> respectively. Likewise, during the next time interval T<b>17</b>, parallel processor element <b>0</b> and the BEU concurrently process instructions I<b>2</b> and I<b>5</b> respectively. And finally, during time interval T<b>18</b>, processor element <b>0</b> processes instruction I<b>3</b>. During instruction firing times T<b>16</b>, T<b>17</b>, and T<b>18</b>, parallel processor element <b>3</b> is not utilized in the example of Table 1. In actuality, since the last instruction is a branch instruction, the branch cannot occur until the last processing is finished in time T<b>18</b> for instruction I<b>3</b>. A delay field is built into the processing of instruction I<b>5</b> so that even though it is processed in time interval T<b>17</b> (the earliest possible time), its execution is delayed so that looping or branching out occurs after instruction I<b>3</b> has executed.</p>
    <p>In summary, the TOLL software <b>110</b> of the present illustrated embodiment, in stage <b>130</b>, examines each individual instruction and its resource usage both as to type and as to location (if known) (e.g., Table 3). It then assigns instruction firing times (IFTs) on the basis of this resource usage (e.g., Table 4), reorders the instruction stream based upon these firing times (e.g., Table 5) and assigns logical processor numbers (LPNs) (e.g., Table 6) as a result thereof.</p>
    <p>The extended intelligence information involving the logical processor number (LPN) and the instruction firing time (IFT) is, in the illustrated embodiment, added to each instruction of the basic block as shown in FIGS. 3 and 4. As will also be pointed out subsequently, the extended intelligence (EXT) for each instruction in a basic block (BB) will be correlated with the actual physical processor architecture of the present invention. The correlation is performed by the system hardware. It is important to note that the actual hardware may contain less, the same as, or more physical processor elements than the number of logical processor elements.</p>
    <p>The Shared Context Storage Mapping (SCSM) information in FIG. <b>4</b> and attached to each instruction in this illustrated and preferred embodiment of the invention, has a static and a dynamic component. The static component of the SCSM information is attached by the TOLL software or compiler and is a result of the static analysis of the instruction stream. Dynamic information is attached at execution time by a logical resource drive (LRD) as will be discussed later.</p>
    <p>At this stage <b>130</b>, the illustrated TOLL software <b>110</b> has analyzed the instruction stream as a set of single entry single exit (SESE) basic blocks (BBs) for natural concurrencies that can be processed individually by separate processor elements (PEs) and has assigned to each instruction an instruction firing time (IFT) and a logical processor number (LPN). Under the teachings of the present invention, the instruction stream is thus pre-processed by the TOLL software to statically allocate all processing resources in advance of execution. This is done once for any given program and is applicable to any one of a number of different program languages such as FORTRAN, COBOL, PASCAL, BASIC, etc.</p>
    <p>Referring to FIG. 5, a series of basic blocks (BBs) can form a single execution set (ES) and in stage <b>140</b>, the TOLL software <b>110</b> builds such execution sets (ESs). Once the TOLL software identifies an execution set <b>500</b>, header <b>510</b> and/or trailer <b>520</b> information is added at the beginning and/or end of the set. In the preferred embodiment, only header information <b>510</b> is attached at the beginning of the set, although the invention is not so limited.</p>
    <p>Under the teachings of the present invention, basic blocks generally follow one another in the instruction stream. There may be no need for reordering of the basic blocks even though individual instructions within a basic block, as discussed above, are reordered and assigned extended intelligence information. However, the invention is not so limited. Each basic block is single entry and single exit (SESE) with the exit through a branch instruction. Typically, the branch to another instruction is within a localized neighborhood such as within <b>400</b> instructions of the branch. The purpose of forming the execution sets (stage <b>140</b>) is to determine the minimum number of basic blocks that can exist within an execution set such that the number of instruction cache faults is minimized. In other words, in a given execution set, branches or transfers out of an execution set are statistically minimized. The TOLL software in stage <b>140</b>, can use a number of conventional techniques for solving this linear programming-like problem, a problem which is based upon branch distances and the like. The purpose is to define an execution set as set forth in FIG. 5 so that the execution set can be placed in a hardware cache, as will be discussed subsequently, to minimize instruction cache faults (i.e., transfers out of the execution set).</p>
    <p>What has been set forth above is an example, illustrated using Tables 1 through 6, of the TOLL software <b>110</b> in a single context application. In essence, the TOLL software determines the natural concurrencies within the instruction streams for each basic block within a given program. The TOLL software adds, in the illustrated embodiment, an instruction firing time (IFT) and a logical processor number (LPN) to each instruction in accordance with the determined natural concurrencies. All processing resources are statically allocated in advance of processing. The TOLL software of the present invention can be used in connection with a number of simultaneously executing different programs, each program being used by the same or different users on a processing system of the present invention as will be described and explained below.</p>
    <p>3. General Hardware Description</p>
    <p>Referring to FIG. 6, the block diagram format of the system architecture of the present invention, termed the TDA system architecture <b>600</b>, includes a memory sub-system <b>610</b> interconnected to a plurality of logical resource drivers (LRDs) <b>620</b> over a network <b>630</b>. The logical resource drivers <b>620</b> are further interconnected to a plurality of processor elements <b>640</b> over a network <b>650</b>. Finally, the plurality of processor elements <b>640</b> are interconnected over a network <b>670</b> to the shared resources containing a pool of register set and condition code set files <b>660</b>. The LRD-memory network <b>630</b>, the PE-LRD network <b>650</b>, and the PE-context file network <b>670</b> are full access networks that could be composed of conventional crossbar networks, omega networks, banyan networks, or the like. The networks are full access (non-blocking in space) so that, for example, any processor element <b>640</b> can access any register file or condition code storage in any context (as defined hereinbelow) file <b>660</b>. Likewise, any processor element <b>640</b> can access any logical resource driver <b>620</b> and any logical resource driver <b>620</b> can access any portion of the memory subsystem <b>610</b>. In addition, the PE-LRD and PE-context file networks are non-blocking in time. In other words, these two networks guarantee access to any resource from any resource regardless of load conditions on the network. The architecture of the switching elements of the PE-LRD network <b>650</b> and the PE-context file network <b>670</b> are considerably simplified since the TOLL software guarantees that collisions in the network will never occur. The diagram of FIG. 6 represents an MIMD system wherein each context file <b>660</b> corresponds to at least one user program.</p>
    <p>The memory subsystem <b>610</b> can be constructed using a conventional memory architecture and conventional memory elements. There are many such architectures and elements that could be employed by a person skilled in the art and which would satisfy the requirements of this system. For example, a banked memory architecture could be used. (<i>High Speed Memory Systems, </i>A. V. Pohm and O. P. Agrawal, Reston Publishing Co., 1983.)</p>
    <p>The logical resource drivers <b>620</b> are unique to the system architecture <b>600</b> of the present invention. Each illustrated LRD provides the data cache and instruction selection support for a single user (who is assigned a context file) on a timeshared basis. The LRDs receive execution sets from the various users wherein one or more execution sets for a context are stored on an LRD. The instructions within the basic blocks of the stored execution sets are stored in queues based on the previously assigned logical processor number. For example, if the system has 64 users and 8 LRDS, 8 users would share an individual LRD on a timeshared basis. The operating system determines which user is assigned to which LRD and for how long. The LRD is detailed at length subsequently.</p>
    <p>The processor elements <b>640</b> are also unique to the TDA system architecture and will be discussed later. These processor elements in one particular aspect of the invention display a context free stochastic property in which the future state of the system depends only on the present state of the system and not on the path by which the present state was achieved. As such, architecturally, the context free processor elements are uniquely different from conventional processor elements in two ways. First, the elements have no internal permanent storage or remnants of past events such as general purpose registers or program status words. Second, the elements do not perform any routing or synchronization functions. These tasks are performed by the TOLL software and are implemented in the LRDs. The significance of the architecture is that the context free processor elements of the present invention are a true shared resource to the LRDs. In another preferred particular embodiment of the invention wherein pipelined processor elements are employed, the processors are not strictly context free as was described previously.</p>
    <p>Finally, the register set and condition code set files <b>660</b> can also be constructed of commonly available components such as AMD 29300 series register files, available from Advanced Micro Devices, 901 Thompson Place, P.O. Box 3453, Sunnyvale, Calif. 94088. However, the particular configuration of the files <b>660</b> illustrated in FIG. 6 is unique under the teachings of the present invention and will be discussed later.</p>
    <p>The general operation of the present invention, based upon the example set forth in Table 1, is illustrated with respect to the processor-context register file communication in FIGS. 7<i>a</i>, <b>7</b> <i>b</i>, and <b>7</b> <i>c</i>. As mentioned, the time-driven control of the present illustrated embodiment of the invention is found in the addition of the extended intelligence relating to the logical processor number (LPN) and the instruction firing time (IFT) as specifically set forth in FIG. <b>4</b>. FIG. 7 generally represents the configuration of the processor elements PE<b>0</b> through PE<b>3</b> with registers R<b>0</b> through R<b>5</b>, . . . , R<b>10</b> and R<b>11</b> of the register set and condition code set file <b>660</b>.</p>
    <p>In explaining the operation of the TDA system architecture <b>600</b> for the single user example in Table 1, reference is made to Tables 3 through 5. In the example, for instruction firing time T<b>16</b>, the context file-PE network <b>670</b> interconnects processor element PE<b>0</b> with registers R<b>0</b> and R<b>10</b>, processor element PE<b>1</b> with registers R<b>1</b> and R<b>11</b>, and processor element PE<b>2</b> with register R<b>4</b>. Hence, during time T<b>16</b>, the three processor elements PE<b>0</b>, PE<b>1</b>, and PE<b>2</b> process instructions I<b>0</b>, I<b>1</b>, and I<b>4</b> concurrently and store the results in registers R<b>0</b>, R<b>10</b>, R<b>1</b>, R<b>11</b>, and R<b>4</b>. During time T<b>16</b>, the LRD <b>620</b> selects and delivers the instructions that can fire (execute) during time T<b>17</b> to the appropriate processor elements. Referring to FIG. 7<i>b</i>, during instruction firing time T<b>17</b>, only processor element PE<b>0</b>, which is now assigned to process instruction I<b>2</b> interconnects with registers R<b>0</b>, R<b>1</b>, and R<b>2</b>. The BEU (not shown in FIGS. 7<i>a</i>, <b>7</b> <i>b</i>, and <b>7</b> <i>c</i>) is also connected to the condition code storage. Finally, referring to FIG. 7<i>c</i>, during instruction firing time T<b>18</b>, processor element PE<b>0</b> is connected to registers R<b>2</b> and R<b>3</b>.</p>
    <p>Several important observations need to be made. First, when a particular processor element (PE) places results of its operation in a register, any processor element, during a subsequent instruction firing time (IFT), can be interconnected to that register as it executes its operation. For example, processor element PE<b>1</b> for instruction I<b>1</b> loads register R<b>1</b> with the contents of a memory location during IFT T<b>16</b> as shown in FIG. 7<i>a</i>. During instruction firing time T<b>17</b>, processor element PE<b>0</b> is interconnected with register R<b>1</b> to perform an additional operation on the results stored therein. Under the teachings of the present invention, each processor element (PE) is totally coupled to the necessary registers in the register file <b>660</b> during any particular instruction firing time (IFT) and, therefore, there is no need to move the data out of the register file for delivery to another resource; e.g. in another processor's register as in some conventional approaches.</p>
    <p>In other words, under the teachings of the present invention, each processor element can be totally coupled, during any individual instruction firing time, to any shared register in files <b>660</b>. In addition, under the teachings of the present invention, none of the processor elements has to contend (or wait) for the availability of a particular register or for results to be placed in a particular register as is found in some prior art systems. Also, during any individual firing time, any processor element has full access to any configuration of registers in the register set file <b>660</b> as if such registers were its own internal registers.</p>
    <p>Hence, under the teachings of the present invention, the intelligence added to the instruction stream is based upon detected natural concurrencies within the object code. The detected concurrencies are analyzed by the TOLL software, which in one illustrated embodiment logically assigns individual logical processor elements (LPNs) to process the instructions in parallel, and unique firing times (IFTs) so that each processor element (PE), for its given instruction, will have all necessary resources available for processing according to its instruction requirements. In the above example, the logical processor numbers correspond to the actual processor assignment, that is, LPN<b>0</b> corresponds to PE<b>0</b>, LPN<b>1</b> to PE<b>1</b>, LPN<b>2</b> to PE<b>2</b>, and LPN<b>3</b> to PE<b>3</b>. The invention is not so limited since any order such as LPN<b>0</b> to PE<b>1</b>, LPN<b>1</b> to PE<b>2</b>, etc. could be used. Or, if the TDA system had more or less than four processors, a different assignment could be used as will be discussed.</p>
    <p>The timing control for the TDA system is provided by the instruction firing times, that is, the system is time-driven. As can be observed in FIGS. 7<i>a</i>through <b>7</b> <i>c</i>, during each individual instruction firing time, the TDA system architecture composed of the processor elements <b>640</b> and the PE-register set file network <b>670</b>, takes on a new and unique particular configuration fully adapted to enable the individual processor elements to concurrently process instructions while making full use of all the available resources. The processor elements can be context free and thereby data, condition, or information relating to past processing is not required, nor does it exist, internally to the processor element. The context free processor elements react only to the requirements of each individual instruction and are interconnected by the hardware to the necessary shared registers.</p>
    <p>4. Summary</p>
    <p>In summary, the TOLL software <b>110</b> for each different program or compiler output <b>100</b> analyzes the natural concurrencies existing in each single entry, single exit (SESE) basic block (BB) and adds intelligence, including in one illustrated embodiment, a logical processor number (LPN) and an instruction firing time (IFT), to each instruction. In an MIMD system of the present invention as shown in FIG. 6, each context file would contain data from a different user executing a program. Each user is assigned a different context file and, as shown in FIG. 7, the processor elements (PEs) are capable of individually accessing the necessary resources such as registers and condition codes storage required by the instruction. The instruction itself carries the shared resource information (that is, the registers and condition code storage). Hence, the TOLL software statically allocates only once for each program the necessary information for controlling the processing of the instruction in the TDA system architecture illustrated in FIG. 6 to insure a time-driven decentralized control wherein the memory, the logical resource drivers, the processor elements, and the context shared resources are totally coupled through their respective networks in a pure, non-blocking fashion.</p>
    <p>The logical resource drivers (LRDs) <b>620</b> receive the basic blocks formed in an execution set and are responsible for delivering each instruction to the selected processor element <b>640</b> at the instruction firing time (IFT). While the example shown in FIG. 7 is a simplistic representation for a single user, it is to be expressly understood that the delivery by the logical resource driver <b>620</b> of the instructions to the processor elements <b>640</b>, in a multi-user system, makes full use of the processor elements as will be fully discussed subsequently. Because the timing and the identity of the shared resources and the processor elements are all contained within the extended intelligence added to the instructions by the TOLL software, each processor element <b>640</b> can be completely (or in some instances substantially) context free and, in fact, from instruction firing time to instruction firing time can process individual instructions of different users as delivered by the various logical resource drivers. As will be explained, in order to do this, the logical resource drivers <b>620</b>, in a predetermined order, deliver the instructions to the processor elements <b>640</b> through the PE-LRD network <b>650</b>.</p>
    <p>It is the context free nature of the processor elements which allows the independent access by any processor element of the results of data generation/manipulation from any other processor element following the completion of each instruction execution. In the case of processors which are not context free, in order for one processor to access data created by another, specific actions (usually instructions which move data from general purpose registers to memory) are required in order to extract the data from one processor and make it available to another.</p>
    <p>It is also the context free nature of the processor elements that permits the true sharing of the processor elements by multiple LRDs. This sharing can be as fine-grained as a single instruction cycle. No programming or special processor operations are needed to save the state of one context (assigned to one LRD), which has control of one or more processor elements, in order to permit control by another context (assigned to a second LRD). In processors which are not context free, which is the case for the prior art, specific programming and special machine operations are required in such state-saving as part of the process of context switching.</p>
    <p>There is one additional alternative in implementing the processor elements of the present invention, which is a modification to the context free concept: an implementation which provides the physically total interconnection discussed above, but which permits, under program control, a restriction upon the transmission of generated data to the register file following completion of certain instructions.</p>
    <p>In a fully context free implementation, at the completion of each instruction which enters the processor element, the state of the context is entirely captured in the context storage file. In the alternative case, transmission to the register file is precluded and the data is retained within the processor and made available (for example, through data chaining) to succeeding instructions which further manipulate the data. Ultimately, data is transmitted to the register file after some finite sequence of instructions completes; however, it is only the final data that is transmitted.</p>
    <p>This can be viewed as a generalization of the case of a microcoded complex instruction as described above, and can be considered a substantially context free processor element implementation. In such an implementation, the TOLL software would be required to ensure that dependent instructions execute on the same processor element until such time as data is ultimately transmitted to the context register file. As with pipelined processor elements, this does not change the overall functionality and architecture of the TOLL software, but mainly affects the efficient scheduling of instructions among processor elements to make optimal use of each instruction cycle on all processor elements.</p>
    <heading>DETAILED DESCRIPTION</heading> <p>1. Detailed Description of Software</p>
    <p>In FIGS. 8 through 11, the details of the TOLL software <b>110</b> of the present invention are set forth. Referring to FIG. 8, the conventional output from a compiler is delivered to the TOLL software at the start stage <b>800</b>. The following information is contained within the conventional compiler output <b>800</b>: (a) instruction functionality, (b) resources required by the instruction, (c) locations of the resources (if possible), and (d) basic block boundaries. The TOLL software then starts with the first instruction at stage <b>810</b> and proceeds to determine which resources are used in stage <b>820</b> and how the resources are used in stage <b>830</b>. This process continues for each instruction within the instruction stream through stages <b>840</b> and <b>850</b> as was discussed in the previous section.</p>
    <p>After the last instruction is processed, as tested in stage <b>840</b>, a table is constructed and initialized with the free time and load time for each resource. Such a table is set forth in Table 7 for the inner loop matrix multiply example and at initialization, the table contains all zeros. The initialization occurs in stage <b>860</b> and once constructed the TOLL software proceeds to start with the first basic block in stage <b>870</b>.</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="4" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="28PT"> </colspec> <colspec colname="1" align="left" colwidth="63PT"> </colspec> <colspec colname="2" align="left" colwidth="70PT"> </colspec> <colspec colname="3" align="left" colwidth="56PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" class="description-td" colspan="4">TABLE 7</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Resource</td>
                <td morerows="0" valign="top" class="description-td">Load Time</td>
                <td morerows="0" valign="top" class="description-td">Free Time</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R0</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R1</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R2</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R3</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R4</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R10</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R11</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
                <td morerows="0" valign="top" class="description-td">T0</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>Referring to FIG. 9, the TOLL software continues the analysis of the instruction stream with the first instruction of the next basic block in stage <b>900</b>. As stated previously, TOLL performs a static analysis of the instruction stream. Static analysis assumes (in effect) straight line code, that is, each instruction is analyzed as it is seen in a sequential manner. In other words, static analysis assumes that a branch is never taken. For non-pipelined instruction execution, this is not a problem, as there will never be any dependencies that arise as a result of a branch. Pipelined execution is discussed subsequently (although, it can be stated that the use of pipelining will only affect the delay value of the branch instruction).</p>
    <p>Clearly, the assumption that a branch is never taken is incorrect. However, the impact of encountering a branch in the instruction stream is straightforward. As stated previously, each instruction is characterized by the resources (or physical hardware elements) it uses. The assignment of the firing time (and in the illustrated embodiment, the logical processor number) is dependent on how the instruction stream accesses these resources. Within this particular embodiment of the TOLL software, the usage of each resource is represented, as noted above, by data structures termed the free and load times for that resource. As each instruction is analyzed in sequence, the analysis of a branch impacts these data structures in the following manner.</p>
    <p>When all of the instructions of a basic block have been assigned firing times, the maximum firing time of the current basic block (the one the branch is a member of) is used to update all resources load and free times (to this value). When the next basic block analysis begins, the proposed firing time is then given as the last maximum value plus one. Hence, the load and free times for each of the register resources R<b>0</b> through R<b>4</b>, R<b>10</b> and R<b>11</b> are set forth below in Table 8, for the example, assuming the basic block commences with a time of T<b>16</b>.</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="3" colsep="0" rowsep="0" align="left"> <colspec colname="1" align="center" colwidth="84PT"> </colspec> <colspec colname="2" align="center" colwidth="42PT"> </colspec> <colspec colname="3" align="center" colwidth="91PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td namest="1" nameend="3" morerows="0" rowsep="1" valign="top" class="description-td" colspan="3">TABLE 8</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">Resource</td>
                <td morerows="0" valign="top" class="description-td">Load Time</td>
                <td morerows="0" valign="top" class="description-td">Free Time</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">R0</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">R1</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">R2</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">R3</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">R4</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">R10</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">R11</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
                <td morerows="0" valign="top" class="description-td">T15</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>Hence, the TOLL software sets a proposed firing time (PFT) in stage <b>910</b> to the maximum firing time plus one of the previous basic blocks firing times. In the context of the above example, the previous basic block's last firing time is T<b>15</b>, and the proposed firing time for the instructions in this basic block commence with T<b>16</b>.</p>
    <p>In stage <b>920</b>, the first resource used by the first instruction, which in this case is register R<b>0</b> of instruction I<b>0</b>, is analyzed. In stage <b>930</b>, a determination is made as to whether or not the resource is read. In the above example, for instruction I<b>0</b>, register R<b>0</b> is not read but is written and, therefore, stage <b>940</b> is next entered to make the determination of whether or not the resource is written. In this case, instruction I<b>0</b> writes into register R<b>0</b> and stage <b>942</b> is entered. Stage <b>942</b> makes a determination as to whether the proposed firing time (PFT) for instruction I<b>0</b> is less than or equal to the free time for the resource. In this case, referring to Table 8, the resource free time for register R<b>0</b> is T<b>15</b> and, therefore, the instruction proposed firing time of T<b>16</b> is greater than the resource free time of T<b>15</b> and the determination is no and stage <b>950</b> is accessed.</p>
    <p>The analysis by the TOLL software proceeds to the next resource which in the case, for instruction I<b>0</b>, is register R<b>10</b>. This resource is both read and written by the instruction. Stage <b>930</b> is entered and a determination is made as to whether or not the instruction reads the resource. It does, so stage <b>932</b> is entered where a determination is made as to whether the current proposed firing time for the instruction (T<b>16</b>) is less than the resource load time (T<b>15</b>). It is not, so stage <b>940</b> is entered. Here a determination is made as to whether the instruction writes the resource. It does; so stage <b>942</b> is entered. In this stage a determination is made as to whether the proposed firing time for the instruction (T<b>16</b>) is less than the free time for the resource (T<b>15</b>). It is not, and stage <b>950</b> is accessed. The analysis by the TOLL software proceeds either to the next resource (there is none for instruction I<b>0</b>) or to B (FIG. 10) if the last resource for the instruction has been processed.</p>
    <p>Hence, the answer to the determination at stage <b>950</b> is affirmative and the analysis then proceeds to FIG. <b>10</b>. In FIG. 10, the resource free and load times will be set. At stage <b>1000</b>, the first resource for instruction I<b>0</b> is register R<b>0</b>. The first determination in stage <b>1010</b> is whether or not the instruction reads the resource. As before, register R<b>0</b> in instruction I<b>0</b> is not read but written and the answer to this determination is no in which case the analysis then proceeds to stage <b>1020</b>. In stage <b>1020</b>, the answer to the determination as to whether or not the resource is written is yes and the analysis proceeds to stage <b>1022</b>. Stage <b>1022</b> makes the determination as to whether or not the proposed firing time for the instruction is greater than the resource load time. In the example, the proposed firing time is T<b>16</b> and, with reference back to Table 8, the firing time T<b>16</b> is greater than the load time T<b>15</b> for register R<b>0</b>. Hence, the response to this determination is yes and stage <b>1024</b> is entered. In stage <b>1024</b>, the resource load time is set equal to the instruction's proposed firing time and the table of resources (Table 8) is updated to reflect that change. Likewise, stage <b>1026</b> is entered and the resource free-time is updated and set equal to the instruction's proposed firing time plus one or T<b>17</b> (T<b>16</b> plus one).</p>
    <p>Stage <b>1030</b> is then entered and a determination made as to whether there are any further resources used by this instruction. There is one, register R<b>10</b>, and the analysis processes this resource. The next resource is acquired at stage <b>1070</b>. Stage <b>1010</b> is then entered where a determination is made as to whether or not the resource is read by the instruction. It is and so stage <b>1012</b> is entered where a determination is made as to whether the current proposed firing time (T<b>16</b>) is greater than the resource's free-time (T<b>15</b>). It is, and therefore stage <b>1014</b> is entered where the resource's free-time is updated to reflect the use of this resource by this instruction. The method next checks at stage <b>1020</b> whether the resource is written by the instruction. It is, and so stage <b>1022</b> is entered where a determination is made as to whether or not the current proposed firing time (T<b>16</b>) is greater than the load time of the resource (T<b>15</b>). It is, so stage <b>1024</b> is entered. In this stage, the resource's load-time is updated to reflect the firing time of the instruction, that is, the load-time is set to T<b>16</b>. Stage <b>1026</b> is then entered where the resource's free-time is updated to reflect the execution of the instruction, that is, the free-time is set to T<b>17</b>. Stage <b>1030</b> is then entered where a determination is made as to whether or not this is the last resource used by the instruction. It is, and therefore, stage <b>1040</b> is entered. The instruction firing time (IFT) is now set to equal the proposed firing time (PFT) of T<b>16</b>. Stage <b>1050</b> is then accessed which makes a determination as to whether or not this is the last instruction in the basic block, which in this case is no; and stage <b>1060</b> is entered to proceed to the next instruction, I<b>1</b>, which enters the analysis stage at A<b>1</b> of FIG. <b>9</b>.</p>
    <p>The next instruction in the example is I<b>1</b> and the identical analysis is had for instruction I<b>1</b> for registers R<b>1</b> and R<b>11</b> as presented for instruction I<b>0</b> with registers R<b>0</b> and R<b>10</b>. In Table 9 below, a portion of the resource Table 8 is modified to reflect these changes. (Instructions I<b>0</b> and I<b>1</b> have been fully processed by the TOLL software.)</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="4" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="28PT"> </colspec> <colspec colname="1" align="left" colwidth="63PT"> </colspec> <colspec colname="2" align="left" colwidth="70PT"> </colspec> <colspec colname="3" align="left" colwidth="56PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" class="description-td" colspan="4">TABLE 9</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Resource</td>
                <td morerows="0" valign="top" class="description-td">Load Time</td>
                <td morerows="0" valign="top" class="description-td">Free Time</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R0</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R1</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R10</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R11</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>The next instruction in the basic block example is instruction I<b>2</b> which involves a read of registers R<b>0</b> and R<b>1</b> and a write into register R<b>2</b>. Hence, in stage <b>910</b> of FIG. 9, the proposed firing time for the instruction is set to T<b>16</b> (T<b>15</b> plus 1). Stage <b>920</b> is then entered and the first resource in instruction I<b>2</b> is register R<b>0</b>. The first determination made in stage <b>930</b> is yes and stage <b>932</b> is entered. At stage <b>932</b>, a determination is made whether the instruction's proposed firing time of T<b>16</b> is less than or equal to the resource register R<b>0</b> load time of T<b>16</b>. It is important to note that the resource load time for register R<b>0</b> was updated during the analysis of register R<b>0</b> for instruction I<b>0</b> from time T<b>15</b> to time T<b>16</b>. The answer to this determination in stage <b>932</b> is that the proposed firing time equals the resource load time (T<b>16</b> equals T<b>16</b>) and stage <b>934</b> is entered. In stage <b>934</b>, the instruction proposed firing time is updated to equal the resource load time plus one or, in this case, T<b>17</b> (T<b>16</b> plus one). The instruction I<b>2</b> proposed firing time is now updated to T<b>17</b>. Now stage <b>940</b> is entered and since instruction I<b>2</b> does not write resource R<b>0</b>, the answer to the determination is no and stage <b>950</b> and then stage <b>960</b> are entered to process the next resource which in this case is register R<b>1</b>.</p>
    <p>Stage <b>960</b> initiates the analysis to take place for register R<b>1</b> and a determination is made in stage <b>930</b> whether or not the resource is read. The answer, of course, is yes and stage <b>932</b> is entered. This time the instruction proposed firing time is T<b>17</b> and a determination is made whether or not the instruction proposed firing time of T<b>17</b> is less than or equal to the resource load time for register R<b>1</b> which is T<b>16</b>. Since the instruction proposed firing time is greater than the register load time (T<b>17</b> is greater than T<b>16</b>), the answer to this determination is no and stage <b>940</b>. The register is not written by this instruction and, therefore, the analysis proceeds to stage <b>950</b>. The next resource to be processed for instruction I<b>2</b>, in stage <b>960</b>, is resource register R<b>2</b>.</p>
    <p>The first determination of stage <b>930</b> is whether or not this resource R<b>2</b> is read. It is not and hence the analysis moves to stage <b>940</b> and then to stage <b>942</b>. At this point in time the instruction I<b>2</b> proposed firing time is T<b>17</b> and in stage <b>942</b> a determination is made whether or not the instruction's proposed firing time of T<b>17</b> is less than or equal to resources, R<b>2</b> free time which in Table 8 above is T<b>15</b>. The answer to this determination is no and therefore stage <b>950</b> is entered. This is the last resource processed for this instruction and the analysis continues in FIG. <b>10</b>.</p>
    <p>Referring to FIG. 10, the first resource R<b>0</b> for instruction I<b>2</b> is analyzed. In stage <b>1010</b>, the determination is made whether or not this resource is read and the answer is yes. Stage <b>1012</b> is then entered to make the determination whether the proposed firing time T<b>17</b> of instruction I<b>2</b> is greater than the resource free-time for register R<b>0</b>. In Table 9, the free-time for register R<b>0</b> is T<b>17</b> and the answer to the determination is no since both are equal. Stage <b>1020</b> is then entered which also results in a no answer transferring the analysis to stage <b>1030</b>. Since this is not the last resource to be processed for instruction I<b>2</b>, stage <b>1070</b> is entered to advance the analysis to the next resource register R<b>1</b>. Precisely the same path through FIG. 10 occurs for register R<b>1</b> as for register R<b>0</b>. Next, stage <b>1070</b> initiates processing of register R<b>2</b>. In this case, the answer to the determination at stage <b>1010</b> is no and stage <b>1020</b> is accessed. Since register R<b>2</b> for instruction I<b>2</b> is written, stage <b>1022</b> is accessed. In this case, the proposed firing time of instruction I<b>2</b> is T<b>17</b> and the resource load-time is T<b>15</b> from Table 8. Hence, the proposed firing time is greater than the load time and stage <b>1024</b> is accessed. Stages <b>1024</b> and <b>1026</b> cause the load time and the free time for register R<b>2</b> to be advanced, respectively, to T<b>17</b> and T<b>18</b>, and the resource table is updated as shown in FIG. <b>10</b>:</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="4" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="28PT"> </colspec> <colspec colname="1" align="left" colwidth="63PT"> </colspec> <colspec colname="2" align="left" colwidth="70PT"> </colspec> <colspec colname="3" align="left" colwidth="56PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" class="description-td" colspan="4">TABLE 10</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Resource</td>
                <td morerows="0" valign="top" class="description-td">Load-Time</td>
                <td morerows="0" valign="top" class="description-td">Free-Time</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R0</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R1</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R2</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
                <td morerows="0" valign="top" class="description-td">T18</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>As this is the last resource processed, for instruction I<b>2</b>, the proposed firing time of T<b>17</b> becomes the actual firing time (stage <b>1040</b>) and the next instruction is analyzed.</p>
    <p>It is in this fashion that each of the instructions in the inner loop matrix multiply example are analyzed so that when fully analyzed the final resource table appears as in Table 11 below:</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="4" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="28PT"> </colspec> <colspec colname="1" align="left" colwidth="63PT"> </colspec> <colspec colname="2" align="left" colwidth="70PT"> </colspec> <colspec colname="3" align="left" colwidth="56PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" class="description-td" colspan="4">TABLE 11</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Resource</td>
                <td morerows="0" valign="top" class="description-td">Load-Time</td>
                <td morerows="0" valign="top" class="description-td">Free-Time</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R0</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R1</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R2</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
                <td morerows="0" valign="top" class="description-td">T18</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R3</td>
                <td morerows="0" valign="top" class="description-td">T18</td>
                <td morerows="0" valign="top" class="description-td">T19</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R4</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R10</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R11</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>Referring to FIG. 11, the TOLL software, after performing the tasks set forth in FIGS. 9 and 10, enters stage <b>1100</b>. Stage <b>1100</b> sets all resource free and load times to the maximum of those within the given basic block. For example, the maximum time set forth in Table 11 is T<b>19</b> and, therefore, all free and load times are set to time T<b>19</b>. Stage <b>1110</b> is then entered to make the determination whether this is the last basic block to be processed. If not, stage <b>1120</b> is entered to proceed with the next basic block. If this is the last basic block, stage <b>1130</b> is entered and starts again with the first basic block in the instruction stream. The purpose of this analysis is to logically reorder the instructions within each basic block and to assign logical processor numbers to each instruction. This is summarized in Table 6 for the inner loop matrix multiply example. Stage <b>1140</b> performs the function of sorting the instruction in each basic block in ascending order using the instruction firing time (IFT) as the basis. Stage <b>1150</b> is then entered wherein the logical processor numbers (LPNs) are assigned. In making the assignment of the processor elements, the instructions of a set, that is those having the same instruction firing time (IFT), are assigned logical processor numbers on a first come, first serve basis. For example, in reference back to Table 6, the first set of instructions for firing time T<b>16</b> are I<b>0</b>, I<b>1</b>, and I<b>4</b>. These instructions are assigned respectively to processors PE<b>0</b>, PE<b>1</b>, and PE<b>2</b>. Next, during time T<b>17</b>, the second set of instructions I<b>2</b> and I<b>5</b> are assigned to processors PE<b>0</b> and PE<b>1</b>, respectively. Finally, during the final time T<b>18</b>, the final instruction I<b>3</b> is assigned to processor PE<b>0</b>. It is to be expressly understood that the assignment of the processor elements could be effected using other methods and is based upon the actual architecture of the processor element and the system. As is clear, in the preferred embodiment the set of instructions are assigned to the logical processors on a first in time basis. After making the assignment, stage <b>1160</b> is entered to determine whether or not the last basic block has been processed and if not, stage <b>1170</b> brings forth the next basic block and the process is repeated until finished.</p>
    <p>Hence, the output of the TOLL software, in this illustrated embodiment, results in the assignment of the instruction firing time (IFT) for each of the instructions as shown in FIG. <b>4</b>. As previously discussed, the instructions are reordered, based upon the natural concurrencies appearing in the instruction stream, according to the instruction firing times; and, then, individual logical processors are assigned as shown in Table 6. While the discussion above has concentrated on the inner loop matrix multiply example, the analysis set forth in FIGS. 9 through 11 can be applied to any SESE basic block (BB) to detect the natural concurrencies contained therein and then to assign the instruction firing times (IFTs) and the logical processor numbers (LPNs) for each user's program. This intelligence can then be added to the reordered instructions within the basic block. This is only done once for a given program and provides the necessary time-driven decentralized control and processor mapping information to run on the TDA system architecture of the present invention.</p>
    <p>The purpose of the execution sets, referring to FIG. 12, is to optimize program execution by maximizing instruction cache hits within an execution set or, in other words, to statically minimize transfers by a basic block within an execution set to a basic block in another execution set. Support of execution sets consists of three major components: data structure definitions, pre-execution time software which prepares the execution set data structures, and hardware to support the fetching and manipulation of execution sets in the process of executing the program.</p>
    <p>The execution set data structure consists of a set of one or more basic blocks and an attached header. The header contains the following information: the address <b>1200</b> of the start of the actual instructions (this is implicit if the header has a fixed length), the length <b>1210</b> of the execution set (or the address of the end of the execution set), and zero or more addresses <b>1220</b> of potential successor (in terms of program execution) execution sets.</p>
    <p>The software required to support execution sets manipulates the output of the post-compile processing. That processing performs dependency analysis, resource analysis, resource assignment, and individual instruction stream reordering. The formation of execution sets uses one or more algorithms for determining the probable order and frequency of execution of the basic blocks. The basic blocks are grouped accordingly. The possible algorithms are similar to the algorithms used in solving linear programming problems for least-cost routing. In the case of execution sets, cost is associated with branching. Branching between basic blocks contained in the same execution set incurs no penalty with respect to cache operations because it is assumed that the instructions for the basic blocks of an execution set are resident in the cache in the steady state. Cost is then associated with branching between basic blocks of different execution sets, because the instructions of the basic blocks of a different execution set are assumed not to be in cache. Cache misses delay program execution while the retrieval and storage of the appropriate block from main memory to cache is made.</p>
    <p>There are several possible algorithms which can be used to assess and assign costs under the teaching of the present invention. One algorithm is the static branch cost approach. In accordance with this method, one begins by placing basic blocks into execution sets based on block contiguity and a maximum allowable execution set size (this would be an implementation limit, such as maximum instruction cache size). The information about branching between basic blocks is known and is an output of the compiler. Using this information, the apparatus calculates the cost of the resulting grouping of basic blocks into execution sets based on the number of (static) branches between basic blocks in different execution sets. The apparatus can then use standard linear programming techniques to minimize this cost function, thereby obtaining the optimal grouping of basic blocks into execution sets. This algorithm has the advantage of ease of implementation; however, it ignores the actual dynamic branching patterns which occur during actual program execution.</p>
    <p>Other algorithms could be used in accordance with the teachings of the present invention which provide a better estimation of actual dynamic branch patterns. One example would be the collection of actual branch data from a program execution, and the resultant re-grouping of the basic blocks using a weighted assignment of branch costs based on the actual inter-block branching. Clearly, this approach is data dependent. Another approach would be to allow the programmer to specify branch probabilities, after which the weighted cost assignment would be made. This approach has the disadvantages of programmer intervention and programmer error. Still other approaches would be based using parameters, such as limiting the number of basic blocks per execution set, and applying heuristics to these parameters.</p>
    <p>The algorithms described above are not unique to the problem of creating execution sets. However, the use of execution sets as a means of optimizing instruction cache performance is novel. Like the novelty of pre-execution time assignment of processor resources, the pre-execution time grouping of basic blocks for maximizing cache performance is not found in prior art.</p>
    <p>The final element required to support the execution sets is the hardware. As will be discussed subsequently, this hardware includes storage to contain the current execution set starting and ending addresses and to contain the other execution set header data. The existence of execution sets and the associated header data structures are, in fact, transparent to the actual instruction fetching from cache to the processor elements. The latter depends strictly upon the individual instruction and branch addresses. The execution set hardware operates independently of instruction fetching to control the movement of instruction words from main memory to the instruction cache. This hardware is responsible for fetching basic blocks of instructions into the cache until either the entire execution set resides in cache or program execution has reached a point that a branch has occurred to a basic block outside the execution set. At this point, since the target execution set is not resident in cache, the execution set hardware begins fetching the basic blocks belonging to the target execution set.</p>
    <p>Referring to FIG. 13, the structure of the register set file <b>660</b> for context file zero (the structure being the same for each context file) has L+1 levels of register sets with each register set containing n+1 separate registers. For example, n could equal 31 for a total of 32 registers. Likewise, the L could equal 15 for a total of 16 levels. Note that these registers are not shared between levels; that is, each level has a set of registers which is physically distinct from the registers of each other level.</p>
    <p>Each level of registers corresponds to that group of registers available to a subroutine executing at a particular depth relative to the main program. For example, the set of registers at level zero can be available to the main program; the set of registers at level one can be available to a first level subroutine that is called directly from the main program; the set of registers at level two can be available to any subroutine (a second level subroutine) called directly by a first level subroutine; the set of registers at level three can be available to any subroutine called directly by a second level subroutine; and so on.</p>
    <p>As these sets of registers are independent, the maximum number of levels corresponds to the number of subroutines that can be nested before having to physically share any registers between subroutines, that is, before having to store the contents of any registers in main memory. The register sets, in their different levels, constitute a shared resource of the present invention and significantly saves system overhead during subroutine calls since only rarely do sets of registers need to be stored, for example in a stack, in memory.</p>
    <p>Communication between different levels of subroutines takes place, in the preferred illustrated embodiment, by allowing each subroutine up to three possible levels from which to obtain a register: the current level, the previous (calling) level (if any) and the global (main program) level. The designation of which level of registers is to be accessed, that is, the level relative to the presently executing main program or subroutine, uses the static SCSM information attached to the instruction by the TOLL software. This information designates a level relative to the instruction to be processed. This can be illustrated by a subroutine call for a SINE function that takes as its argument a value representing an angular measure and returns the trigonometric SINE of that measure. The main program is set forth in Table 12; and the subroutine is set forth in Table 13.</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="3" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="28PT"> </colspec> <colspec colname="1" align="left" colwidth="91PT"> </colspec> <colspec colname="2" align="left" colwidth="98PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" class="description-td" colspan="3">TABLE 12</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Main Program</td>
                <td morerows="0" valign="top" class="description-td">Purpose</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">LOAD X, R1</td>
                <td morerows="0" valign="top" class="description-td">Load X from memory</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">into Reg R1 for</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">parameter passing</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">CALL SINE</td>
                <td morerows="0" valign="top" class="description-td">Subroutine Call -</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Returns result in</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Reg R2</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">LOAD R2, R3</td>
                <td morerows="0" valign="top" class="description-td">Temporarily save</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">results in Reg R3</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">LOAD Y, R1</td>
                <td morerows="0" valign="top" class="description-td">Load Y from memory</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">into Reg R1 for</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">parameter passing</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">CALL SINE</td>
                <td morerows="0" valign="top" class="description-td">Subroutine Call -</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Returns result in</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Reg R2</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">MULT R2, R3, R4</td>
                <td morerows="0" valign="top" class="description-td">Multiply Sin (x)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">with Sin (y) and</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">store result in</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Reg R4</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">STORE R4, Z</td>
                <td morerows="0" valign="top" class="description-td">Store final result</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">in memory at Z</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>The SINE subroutine is set forth in Table 13:</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="4" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="14PT"> </colspec> <colspec colname="1" align="left" colwidth="49PT"> </colspec> <colspec colname="2" align="left" colwidth="77PT"> </colspec> <colspec colname="3" align="left" colwidth="77PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" class="description-td" colspan="4">TABLE 13</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Instruction</td>
                <td morerows="0" valign="top" class="description-td">Subroutine</td>
                <td morerows="0" valign="top" class="description-td">Purpose</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I0</td>
                <td morerows="0" valign="top" class="description-td">Load R1 (L0), R2</td>
                <td morerows="0" valign="top" class="description-td">Load Reg R2,</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">level 1 with</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">contents of Reg</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">R1, level 0</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Ip-1</td>
                <td morerows="0" valign="top" class="description-td">(Perform SINE), R7</td>
                <td morerows="0" valign="top" class="description-td">Calculate SINE</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">function and</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">store result in</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Reg R7, level 1</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Ip</td>
                <td morerows="0" valign="top" class="description-td">Load R7, R2 (L0)</td>
                <td morerows="0" valign="top" class="description-td">Load Reg R2, level</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">0 with contents of</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Reg R7, level 1</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="3" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="4"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>Hence, under the teachings of the present invention and with reference to FIG. 14, instruction I<b>0</b> of the subroutine loads register R<b>2</b> of the current level (the subroutine's level or called level) with the contents of register R<b>1</b> from the previous level (the calling routine or level). Note that the subroutine has a full set of registers with which to perform the processing independent of the register set of the calling routine. Upon completion of the subroutine call, instruction Ip causes register R<b>7</b> of the current level to be stored in register R<b>2</b> of the calling routine's level (which returns the results of the SINE routine back to the calling program's register set).</p>
    <p>As described in more detail in connection with FIG. 22, the transfer between the levels occurs through the use of the SCSM dynamically generated information which can contain the absolute value of the current procedural level of the instruction (that is, the level of the called routine), the previous procedural level (that is, the level of the calling routine) and the context identifier. The absolute dynamic SCSM level information is generated by the LRD from the relative (static) SCSM information provided by the TOLL software. The context identifier is only used when processing a number of programs in a multi-user system. The relative SCSM information is shown in Table 13 for register R<b>1</b> (of the calling routine) as R<b>1</b>(L<b>0</b>) and for register R<b>2</b> as R<b>2</b>(L<b>0</b>). All registers of the current level have appended an implied (00) signifying the current procedural level.</p>
    <p>This method and structure described in connection with FIGS. 13 and 14 differ substantially from prior art approaches where physical sharing of the same registers occurs between registers of a subroutine and its calling routine. By thereby limiting the number of registers that are available for use by the subroutine, more system overhead for storing the registers in main memory is required. See, for example, the MIPS approach as set forth in Reduced Instruction Set Computers David A. Patterson, Communications of the ACM, January, 1985, Vol. 28, No. 1, Pgs. 8-21) In that reference, the first sixteen registers are local registers to be used solely by the subroutine, the next eight registers, registers <b>16</b> through <b>23</b>, are shared between the calling routine and the subroutine, and final eight registers, registers <b>24</b> through <b>31</b> are shared between the global (or main) program and the subroutine. Clearly, out of 32 registers that are accessible by the subroutine, only 16 are dedicated solely for use by the subroutine in the processing of its program. In the processing of complex subroutines, the limited number of registers that are dedicated solely to the subroutine may not (in general) be sufficient for the processing of the subroutine. Data shuffling (entailing the storing of intermediate data in memory) must then occur, resulting in significant overhead in the processing of the routine.</p>
    <p>Under the teachings of the present invention, the relative transfers between the levels which are known to occur at compile time are specified by adding the requisite information to the register identifiers as shown in FIG. 4 (the SCSM data), to appropriately map the instructions between the various levels. Hence, a completely independent set of registers is available to the calling routine and to each level of subroutine. The calling routine, in addition to accessing its own complete set of registers, can also gain direct access to a higher set of registers using the aforesaid static SCSM mapping code which is added to the instruction, as previously described. There is literally no reduction in the size of the register set available to a subroutine as specifically found in prior art approaches. Furthermore, the mapping code for the SCSM information can be a field of sufficient length to access any number of desired levels. For example, in one illustrated embodiment, a calling routine can access up to seven higher levels in addition to its own registers with a field of three bits. The present invention is not to be limited to any particular number of levels nor to any particular number of registers within a level. Under the teachings of the present invention, the mapping shown in FIG. 14 is a logical mapping and not a conventional physical mapping. For example, three levels, such as the calling routine level, the called level, and the global level require three bit maps. The relative identification of the levels can be specified by a two bit word in the static SCSM, for example, the calling routine by (00), the subordinate level by (01), and the global level by (11). Thus, each user's program is analyzed and the static SCSM relative procedural level information, also designated a window code, is added to the instructions prior to the issuance of the user program to a specific LRD. Once the user is assigned to a specific LRD, the static SCSM level informatin is used to generate the LRD dependent and dynamic SCSM information which is added as it is needed.</p>
    <p>2. Detailed Description of the Hardware</p>
    <p>As shown in FIG. 6, the TDA system <b>600</b> of the present invention is composed of memory <b>610</b>, logical resource drivers (LRD) <b>620</b>, processor elements (PEs) <b>640</b>, and shared context storage files <b>660</b>. The following detailed description starts with the logical resource drivers since the TOLL software output is loaded into this hardware.</p>
    <p>a. Logical Resource Drivers (LRDS)</p>
    <p>The details of a particular logical resource driver (LRD) is set forth in FIG. <b>15</b>. As shown in FIG. 6, each logical resource driver <b>620</b> is interconnected to the LRD-memory network <b>630</b> on one side and to the processor elements <b>640</b> through the PE-LRD network <b>650</b> on the other side. If the present invention were a SIMD machine, then only one LRD is provided and only one context file is provided. For MIMD capabilities, one LRD and one context file is provided for each user so that, in the embodiment illustrated in FIG. 6, up to n users can be accommodated.</p>
    <p>The logical resource driver <b>620</b> is composed of a data cache section <b>1500</b> and an instruction selection section <b>1510</b>. In the instruction selection section, the following components are interconnected. An instruction cache address translation unit (ATU) <b>1512</b> is interconnected to the LRD-memory network <b>630</b> over a bus <b>1514</b>. The instruction cache ATU <b>1512</b> is further interconnected over a bus <b>1516</b> to an instruction cache control circuit <b>1518</b>. The instruction cache control circuit <b>1518</b> is interconnected over lines <b>1520</b> to a series of cache partitions <b>1522</b> <i>a</i>, <b>1522</b> <i>b</i>, <b>1522</b> <i>c</i>, and <b>1522</b> <i>d</i>. Each of the cache partitions is respectively connected over busses <b>1524</b> <i>a</i>, <b>1524</b> <i>b</i>, <b>1524</b> <i>c</i>, and <b>1524</b> <i>d </i>to the LRD-memory network <b>630</b>. Each cache partition circuit is further interconnected over lines <b>1536</b> <i>a</i>, <b>1536</b> <i>b</i>, <b>1536</b> <i>c</i>, and <b>1536</b> <i>d </i>to a processor instruction queue (PIQ) bus interface unit <b>1544</b>. The PIQ bus interface unit <b>1544</b> is connected over lines <b>1546</b> to a branch execution unit (BEU) <b>1548</b> which in turn is connected over lines <b>1550</b> to the PE-context file network <b>670</b>. The PIQ bus interface unit <b>1544</b> is further connected over lines <b>1552</b> <i>a</i>, <b>1552</b> <i>b</i>, <b>1552</b> <i>c</i>, and <b>1552</b> <i>d </i>to a processor instruction queue (PIQ) buffer unit <b>1560</b> which in turn is connected over lines <b>1562</b> <i>a</i>, <b>1562</b> <i>b</i>, <b>1562</b> <i>c</i>, and <b>1562</b> <i>d </i>to a processor instruction queue (PIQ) processor assignment circuit <b>1570</b>. The PIQ processor assignment circuit <b>1570</b> is in turn connected over lines <b>1572</b> <i>a</i>, <b>1572</b> <i>b</i>, <b>1572</b> <i>c</i>, and <b>1572</b> <i>d </i>to the PE-LRD network <b>650</b> and hence to the processor elements <b>640</b>.</p>
    <p>On the data cache portion <b>1500</b>, a data cache ATU <b>1580</b> is interconnected over bus <b>1582</b> to the LRD-memory network <b>630</b> and is further interconnected over bus <b>1584</b> to a data cache control circuit <b>1586</b> and over lines <b>1588</b> to a data cache interconnection network <b>1590</b>. The data cache control <b>1586</b> is also interconnected to data cache partition circuits <b>1592</b> <i>a</i>, <b>1592</b> <i>b</i>, <b>1592</b> <i>c </i>and <b>1592</b> <i>d </i>over lines <b>1593</b>. The data cache partition circuits, in turn, are interconnected over lines <b>1594</b> <i>a</i>, <b>1594</b> <i>b</i>, <b>1594</b> <i>c</i>, and <b>1594</b> <i>d </i>to the LRD-memory network <b>630</b>. Furthermore, the data cache partition circuits <b>1592</b> are interconnected over lines <b>1596</b> <i>a</i>, <b>1596</b> <i>b</i>, <b>1596</b> <i>c</i>, and <b>1596</b> <i>d </i>to the data cache interconnection network <b>1590</b>. Finally, the data cache interconnection network <b>1590</b> is interconnected over lines <b>1598</b> <i>a</i>, <b>1598</b> <i>b</i>, <b>1598</b> <i>c</i>, and <b>1598</b> <i>d </i>to the PE-LRD network <b>650</b> and hence to the processor elements <b>640</b>.</p>
    <p>In operation, each logical resource driver (LRD) <b>620</b> has two sections, the data cache portion <b>1500</b> and the instruction selection portion <b>1510</b>. The data cache portion <b>1500</b> acts as a high speed data buffer between the processor elements <b>640</b> and memory <b>610</b>. Note that due to the number of memory requests that must be satisfied per unit time, the data cache <b>1500</b> is interleaved. All data requests made to memory by a processor element <b>640</b> are issued on the data cache interconnection network <b>1590</b> and intercepted by the data cache <b>1592</b>. The requests are routed to the appropriate data cache <b>1592</b> by the data cache interconnection network <b>1590</b> using the context identifier that is part of the dynamic SCSM information attached by the LRD to each instruction that is executed by the processors. The address of the desired datum determines in which cache partition the datum resides. If the requested datum is present (that is, a data cache hit occurs), the datum is sent back to the requesting processor element <b>640</b>.</p>
    <p>If the requested datum is not present in data cache, the address delivered to the cache <b>1592</b> is sent to the data cache ATU <b>1580</b> to be translated into a system address. The system address is then issued to memory. In response, a block of data from memory (a cache line or block) is delivered into the cache partition circuits <b>1592</b> under control of data cache control <b>1586</b>. The requested data, that is resident in this cache block, is then sent through the data cache interconnection network <b>1590</b> to the requesting processor element <b>640</b>. It is to be expressly understood that this is only one possible design. The data cache portion is of conventional design and many possible implementations are realizable to one skilled in the art. As the data cache is of standard functionality and design, it will not be discussed further.</p>
    <p>The instruction selection portion <b>1510</b> of the LRD has three major functions; instruction caching, instruction queueing and branch execution. The system function of the instruction cache portion of selection portion <b>1510</b> is typical of any instruction caching mechanism. It acts as a high speed instruction buffer between the processors and memory. However, the current invention presents methods and an apparatus configuration for realizing this function that are unique.</p>
    <p>One purpose of the instruction portion <b>1510</b> is to receive execution sets from memory, place the sets into the caches <b>1522</b> and furnish the instructions within the sets, on an as needed basis, to the processor elements <b>640</b>. As the system contains multiple, generally independent, processor elements <b>640</b>, requests to the instruction cache are for a group of concurrently executable instructions. Again, due to the number of requests that must be satisfied per unit time, the instruction cache is interleaved. The group size ranges from none to the number of processors available to the users. The groups are termed packets, although this does not necessarily imply that the instructions are stored in a contiguous manner. Instructions are fetched from the cache on the basis of their instruction firing time (IFT). The next instruction firing time register contains the firing time of the next packet of instructions to be fetched. This register may be loaded by the branch execution unit <b>1548</b> of the LRD as well as incremented by the cache control unit <b>1518</b> when an instruction fetch has been completed.</p>
    <p>The next IFT register (NIFTR) is a storage register that is accessible from the context control unit <b>1518</b> and the branch execution unit <b>1548</b>. Due to its simple functionality, it is not explicitly shown. Technically, it is a part of the instruction cache control unit <b>1518</b>, and is further buried in the control unit <b>1660</b> (FIG. <b>16</b>). The key point here is that the NIFTR is merely a storage register which can be incremented or loaded.</p>
    <p>The instruction cache selection portion <b>1510</b> receives the instructions of an execution set from memory over bus <b>1524</b> and, in a round robin manner, places instructions word into each cache partitions, <b>1522</b> <i>a</i>, <b>1522</b> <i>b</i>, <b>1522</b> <i>c </i>and <b>1522</b> <i>d</i>. In other words, the instructions in the execution set are directed so that the first instruction is delivered to cache partition <b>1522</b> <i>a</i>, the second instruction to cache partition <b>1522</b> <i>b</i>, the third instruction to cache partition <b>1522</b> <i>c</i>, and the fourth instruction to cache partition <b>1522</b> <i>d</i>. The fifth instruction is then directed to cache partition <b>1522</b> <i>a</i>, and so on until all of the instructions in the execution set are delivered into the cache partition circuits.</p>
    <p>All the data delivered to the cache partitions are not necessarily stored in the cache. As will be discussed, the execution set header and trailer may not be stored. Each cache partition attaches a unique identifier (termed a tag) to all the information that is to be stored in that cache partition. The identifier is used to verify that information obtained from the cache is indeed the information desired.</p>
    <p>When a packet of instructions is requested, each cache partition determines if the partition contains an instruction that is a member of the requested packet. If none of the partitions contain an instruction that is a member of the requested packet (that is, a miss occurs), the execution set that contains the requested packet is requested from memory in a manner analogous to a data cache miss.</p>
    <p>If a hit occurs (that is, at least one of the partitions <b>1522</b> contains an instruction from the requested packet), the partition(s) attach any appropriate dynamic SCSM information to the instruction(s). The dynamic SCSM information, which can be attached to each instruction, is important for multi-user applications. The dynamically attached SCSM information identifies the context file (see FIG. 6) assigned to a given user. Hence, under the teachings of the present invention, the system <b>600</b> is capable of delay free switching among many user context files without requiring a master processor or access to memory.</p>
    <p>The instruction(s) are then delivered to the PrQ bus interface unit <b>1544</b> of the LRD <b>620</b> where it is routed to the appropriate PIQ buffers <b>1560</b> according to the logical processor number (LPN) contained in the extended intelligence that the TOLL software, in the illustrated embodiment, has attached to the instruction. The instructions in the PIQ buffer unit <b>1560</b> are buffered for assignment to the actual processor elements <b>640</b>. The processor assignment is performed by the PIQ processor assignment unit <b>1570</b>. The assignment of the physical processor elements is performed on the basis of the number of processor elements currently available and the number of instructions that are available to be assigned. These numbers are dynamic. The selection process is set forth below.</p>
    <p>The details of the instruction cache control <b>1518</b> and of each cache partition <b>1522</b> of FIG. 15 are set forth in FIG. <b>16</b>. In each cache partition circuit <b>1522</b>, five circuits are utilized. The first circuit is the header route circuit <b>1600</b> which routes an individual word in the header of the execution set over a path <b>1520</b> <i>b </i>to the instruction cache context control unit <b>1660</b>. The control of the header route circuit <b>1600</b> is effected over path <b>1520</b> <i>a </i>by the header path select circuit <b>1602</b>. The header path select circuit <b>1602</b> based upon the address received over lines <b>1520</b> <i>b </i>from the control unit <b>1660</b> selectively activates the required number of header routers <b>1600</b> in the cache partitions. For example, if the execution set has two header words, only the first two header route circuits <b>1600</b> are activated by the header path select circuit <b>1602</b> and therefore two words of header information are delivered over bus <b>1520</b> <i>b </i>to the control unit <b>1660</b> from the two activated header route circuits <b>1600</b> of cache partition circuits <b>1522</b> <i>a </i>and <b>1522</b> <i>b </i>(not shown). As mentioned, successive words in the execution set are delivered to successive cache partition circuits <b>1522</b>.</p>
    <p>For example, assume that the data of Table 1 represents an entire execution set and that appropriate header words appear at the beginning of the execution set. The instructions with the earliest instruction firing times (IFTs) are listed first and for a given IFT, those instructions with the lowest logical processor number are listed first. The table reads:</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="1" colsep="0" rowsep="0" align="left"> <colspec colname="1" align="center" colwidth="217PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td namest="1" nameend="1" morerows="0" rowsep="1" valign="top" class="description-td" colspan="1">TABLE 14</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="1" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="1"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">Header Word 1</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">Header Word 2</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">I0 (T16) (PE0)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">I1 (T16) (PE1)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">I4 (T16) (PE2)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">I2 (T17) (PE0)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">I5 (T17) (PE1)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">I3 (T18) (PE0)</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="1" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="1"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>Hence, the example of Table 1 (that is, the matrix multiply inner loop), now has associated with it two header words and the extended information defining the firing time (IFT) and the logical processor number (LPN). As shown in Table 14, the instructions were reordered by the TOLL software according to the firing times. Hence, as the execution set shown in Table 14 is delivered through the LRD-memory network <b>630</b> from memory, the first word (Header Word <b>1</b>) is routed by partition CACHE0 to the control unit <b>1660</b>. The second word (Header Word <b>2</b>) is routed by partition CACHE1 (FIG. 15) to the control unit <b>1660</b>. Instruction I<b>0</b> is delivered to partition CACHE2, instruction I<b>1</b> to partition CACHE3, instruction I<b>2</b> to partition CACHE0, and so forth. As a result, the cache partitions <b>1522</b> now contain the instructions as shown in Table 15:</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="5" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="21PT"> </colspec> <colspec colname="1" align="center" colwidth="28PT"> </colspec> <colspec colname="2" align="center" colwidth="70PT"> </colspec> <colspec colname="3" align="center" colwidth="28PT"> </colspec> <colspec colname="4" align="center" colwidth="70PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="4" morerows="0" rowsep="1" valign="top" class="description-td" colspan="5">TABLE 15</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="4" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="5"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Cache0</td>
                <td morerows="0" valign="top" class="description-td">Cache1</td>
                <td morerows="0" valign="top" class="description-td">Cache2</td>
                <td morerows="0" valign="top" class="description-td">Cache3</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="4" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="5"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I0</td>
                <td morerows="0" valign="top" class="description-td">I1</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I4</td>
                <td morerows="0" valign="top" class="description-td">I2</td>
                <td morerows="0" valign="top" class="description-td">I5</td>
                <td morerows="0" valign="top" class="description-td">I3</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="4" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="5"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>It is important to clarify that the above example has only one basic block in the execution set (that is, it is a simplistic example). In actuality, an execution set would have a number of basic blocks.</p>
    <p>The instructions are then delivered for storage into a cache random access memory (RAM) <b>1610</b> resident in each cache partition. Each instruction is delivered from the header router <b>1600</b> over a bus <b>1602</b> to the tag attacher circuit <b>1604</b> and then over a line <b>1606</b> into the RAM <b>1610</b>. The tag attacher circuit <b>1604</b> is under control of a tag generation circuit <b>1612</b> and is interconnected therewith over a line <b>1520</b> <i>c</i>. Cache RAM <b>1610</b> could be a conventional cache high speed RAM as found in conventional superminicomputers.</p>
    <p>The tag generation circuit <b>1612</b> provides a unique identification code (ID) for attachment to each instruction before storage of that instruction in the designated RAM <b>1610</b>. The assigning of process identification tags to instructions stored in cache circuits is conventional and is done to prevent aliasing of the instructions. Cache Memories by Alan J. Smith, ACM Computing Surveys, Vol. 14, September, 1982. The tag comprises a sufficient amount of information to uniquely identify it from each other instruction and user. The illustrated instructions already include the IFT and LPN, so that subsequently, when instructions are retrieved for execution, they can be fetched based on their firing times. As shown in Table 16, below, each instruction containing the extended information and the hardware tag is stored, as shown, for the above example:</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="3" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="35PT"> </colspec> <colspec colname="1" align="left" colwidth="77PT"> </colspec> <colspec colname="2" align="left" colwidth="105PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" class="description-td" colspan="3">TABLE 16</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">CACHE0:</td>
                <td morerows="0" valign="top" class="description-td">I4 (T16) (PE2) (ID2)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">CACHE1:</td>
                <td morerows="0" valign="top" class="description-td">I2 (T17) (PE0) (ID3)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">CACHE2:</td>
                <td morerows="0" valign="top" class="description-td">I0 (T16) (PE0) (ID0)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I5 (T17) (PE1) (ID4)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">CACHE3:</td>
                <td morerows="0" valign="top" class="description-td">I1 (T16) (PE1) (ID1)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I3 (T18) (PE0) (1D5)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>As stated previously, the purpose of the cache partition circuits <b>1522</b> is to provide a high speed buffer between the slow main memory <b>610</b> and the fast processor elements <b>640</b>. Typically, the cache RAM <b>1610</b> is a high speed memory capable of being quickly accessed. If the RAM <b>1610</b> were a true associative memory, as can be witnessed in Table 16, each RAM <b>1610</b> could be addressed based upon instruction firing times (IFTs). At the present time, such associative memories are not economically justifiable and an IFT to cache address translation circuit <b>1620</b> must be utilized. Such a circuit is conventional in design and controls the addressing of each RAM <b>1610</b> over a bus <b>1520</b> <i>d</i>. The purpose of circuit <b>1620</b> is to generate the RAM address of the desired instructions given the instruction firing time. Hence, for instruction firing time T<b>16</b>, CACHE0, CACHE2, and CACHE3, as seen in Table 16, would produce instructions I<b>4</b>, I<b>0</b>, and I<b>1</b> respectively.</p>
    <p>When the cache RAMs <b>1610</b> are addressed, those instructions associated with a specific firing time are delivered over lines <b>1624</b> into a tag compare and privilege check circuit <b>1630</b>. The purpose of the tag compare and privilege check circuit <b>1630</b> is to compare the hardware tags (ID) to generated tags to verify that the proper instruction has been delivered. The reference tag is generated through a second tag generation circuit <b>1632</b> which is interconnected to the tag compare and privilege check circuit <b>1630</b> over a line <b>1520</b> <i>e</i>. A privilege check is also performed on the delivered instruction to verify that the operation requested by the instruction is permitted given the privilege status of the process (e.g., system program, application program, etc.). This is a conventional check performed by computer processors which support multiple levels of processing states. A hit/miss circuit <b>1640</b> determines which RAMs <b>1610</b> have delivered the proper instructions to the PIQ bus interface unit <b>1544</b> in response to a specific instruction fetch request.</p>
    <p>For example, and with reference back to Table 16, if the RAMs <b>1610</b> are addressed by circuit <b>1620</b> for instruction firing time T<b>16</b>, CACHE0, CACHE2, and CACHE3 would respond with instructions thereby comprising a hit indication on those cache partitions. Cache 1 would not respond and that would constitute a miss indication and this would be determined by circuit <b>1640</b> over line <b>1520</b> <i>g</i>. Thus, each instruction, for instruction firing time T<b>16</b>, is delivered over bus <b>1632</b> into the SCSM attacher <b>1650</b> wherein dynamic SCSM information, if any, is added to the instruction by an SCSM attacher hardware <b>1650</b>. For example, hardware <b>1650</b> can replace the static SCSM procedural level information (which is a relative value) with the actual procedural level values. The actual values are generataed from a procedural level counter data and the static SCSM information.</p>
    <p>When all of the instructions associated with an individual firing time have been read from the RAM <b>1610</b>, the hit and miss circuit <b>1640</b> over lines <b>1646</b> informs the instruction cache control unit <b>1660</b> of this information. The instruction cache context control unit <b>1660</b> contains the next instruction firing time register, a part of the instruction cache control <b>1518</b> which increments the instruction firing time to the next value. Hence, in the example, upon the completion of reading all instructions associated with instruction firing time T<b>16</b>, the instruction cache context control unit <b>1660</b> increments to the next firing time, T<b>17</b>, and delivers this information over lines <b>1664</b> to an access resolution circuit <b>1670</b>, and over lines <b>1520</b> <i>f </i>to the tag compare and privilege check circuit <b>1630</b>. Also note that there may be firing times which have no valid instructions, possibly due to operational dependencies detected by the TOLL software. In this case, no instructions would be fetched from the cache and transmitted to the PIQ interface.</p>
    <p>The access resolution circuit <b>1670</b> coordinates which circuitry has access to the instruction cache RAMs <b>1610</b>. Typically, these RAMs can satisfy only a single request at each clock cycle. Since there could be two requests to the RAMs at one time, an arbitration method must be implemented to determine which circuitry obtains access. This is a conventional issue in the design of cache memory, and the access resolution circuit resolves the priority question as is well known in the field.</p>
    <p>The present invention can and preferably does support several users simultaneously in both time and space. In previous prior art approaches (CDC, IBM, etc.), multi-user support was accomplished solely by timesharing the processor(s). In other words, the processors were shared in time. In this system, multi-user support is accomplished (in space) by assigning an LRD to each user that is given time on the processor elements. Thus, there is a spatial aspect to the sharing of the processor elements. The operating system of the machine deals with those users assigned to the same LRD in a timeshared manner, thereby adding the temporal dimension to the sharing of the processors.</p>
    <p>Multi-user support is accomplished by the multiple LRDs, the use of plural processor elements, and the multiple context files <b>660</b> supporting the register files and condition code storage. As several users may be executing in the processor elements at the same time, additional pieces of information must be attached to each instruction prior to its execution to uniquely identify the instruction source and any resources that it may use. For example, a register identifier must contain the absolute value of the subroutine procedural level and the context identifier as well as the actual register number. Memory addresses must also contain the LRD identifier from which the instruction was issued to be properly routed through the LRD-Memory interconnection network to the appropriate data cache.</p>
    <p>The additional and required information comprises two components, a static and a dynamic component; and the information is termed shared context storage mapping (SCSM). The static information results from the compiler output and the TOLL software gleans the information from the compiler generated instruction stream and attaches the register information to the instruction prior to its being received by an LRD.</p>
    <p>The dynamic information is hardware attached to the instruction by the LRD prior to its issuance to the processors. This information is composed of the context/LRD identifier corresponding to the LRD issuing the instruction, the absolute value of the current procedural level of the instruction, the process identifier of the current instruction stream, and preferably the instruction status information that would normally be contained in the processors of a system having processors that are not context free. This later information would be composed of error masks, floating point format modes, rounding modes, and so on.</p>
    <p>In the operation of the circuitry in FIG. 16, one or more execution sets are delivered into the instruction cache circuitry. The header information for each set is delivered to one or more successive cache partitions and is routed to the context control unit <b>1660</b>. The instructions in the execution set are then individually, on a round robin basis, routed to each successive cache partition unit <b>1522</b>. A hardware identification tag is attached to each instruction and the instruction is then stored in RAM <b>1610</b>. As previously discussed, each execution set is of sufficient length to minimize instruction cache defaults and the RAM <b>1610</b> is of sufficient size to store the execution sets. When the processor elements require the instructions, the number and cache locations of the valid instructions matching the appropriate IFTs are determined. The instructions stored in the RAM's <b>1610</b> are read out; the identification tags are verified; and the privilege status checked. The instructions are then delivered to the PIQ bus interface unit <b>1544</b>. Each instruction that is delivered to the PIQ bus interface unit <b>1544</b>, as is set forth in Table 17, includes the identification tag (ID) and the hardware added SCSM information.</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="3" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="28PT"> </colspec> <colspec colname="1" align="left" colwidth="63PT"> </colspec> <colspec colname="2" align="left" colwidth="126PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" class="description-td" colspan="3">TABLE 17</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">CACHE0:</td>
                <td morerows="0" valign="top" class="description-td">I4 (T16) (PE2) (ID2) (SCSM0)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">CACHE1:</td>
                <td morerows="0" valign="top" class="description-td">I2 (T17) (PE0) (ID3) (SCSM1)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">CACHE2:</td>
                <td morerows="0" valign="top" class="description-td">I0 (T16) (PE0) (ID0) (SCSM2)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I5 (T17) (PE1) (ID4) (SCSM3)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">CACHE3:</td>
                <td morerows="0" valign="top" class="description-td">I1 (T16) (PE1) (ID1) (SCSM4)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I3 (T18) (PE0) (ID5) (SCSM5)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="2" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="3"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>If an instruction is not stored in RAM <b>1610</b>, a cache miss occurs and a new execution set containing the instruction is read from main memory over lines <b>1523</b>.</p>
    <p>In FIG. 17, the details of the PIQ bus interface unit <b>1544</b> and the PIQ buffer unit <b>1560</b> are set forth. Referring to FIG. 17, the PIQ bus interface unit <b>1544</b> receives instructions as set forth in Table 17, above, over lines <b>1536</b>. A search tag hardware <b>1702</b> has access to the value of the present instruction firing time over lines <b>1549</b> and searches the cache memories <b>1522</b> to determine the address(es) of those registers containing instructions having the correct firing times. The search tag hardware <b>1702</b> then makes available to the instruction cache control circuitry <b>1518</b> the addresses of those memory locations for determination by the instruction cache control of which instructions to next select for delivery to the PIQ bus interface <b>1544</b>.</p>
    <p>These instructions access, in parallel, a two-dimensional array of bus interface units (BIU's) <b>1700</b>. The bus interface units <b>1700</b> are interconnected in a full access non-blocking network by means of connections <b>1710</b> and <b>1720</b>, and connect over lines <b>1552</b> to the PIQ buffer unit <b>1560</b>. Each bus interface unit (BIU) <b>1700</b> is a conventional address comparison circuit composed of: TI 74L85 4 bit magnitude comparators, Texas Instruments Company, P.O. Box 225012, Dallas, Tex. 75265. In the matrix multiply example, for instruction firing time T<b>16</b>, CACHE0 contains instruction I<b>4</b> and CACHE3 (corresponding to CACHE n in FIG. 17) contains instruction I<b>1</b>. The logical processor number assigned to instruction I<b>4</b> is PE<b>2</b>. The logical processor number PE<b>2</b> activates a select (SEL) signal of the bus interface unit <b>1700</b> for processor instruction queue <b>2</b> (this is the BIU<b>3</b> corresponding to the CACHE0 unit containing the instruction). In this example, only that BIU<b>3</b> is activated and the remaining bus interface units <b>1700</b> for that BIU<b>3</b> row and column are not activated. Likewise, for CACHE3 (CACHE n in FIG. <b>17</b>), the corresponding BIU<b>2</b> is activated for processor instruction QUEUE <b>1</b>.</p>
    <p>The PIQ buffer unit <b>1560</b> is comprised of a number of processor instruction queues <b>1730</b> which store the instructions received from the PIQ bus interface unit <b>1544</b> in a first in-first out (FIFO) fashion as shown in Table 18:</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="5" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="28PT"> </colspec> <colspec colname="1" align="center" colwidth="21PT"> </colspec> <colspec colname="2" align="center" colwidth="77PT"> </colspec> <colspec colname="3" align="center" colwidth="21PT"> </colspec> <colspec colname="4" align="center" colwidth="70PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="4" morerows="0" rowsep="1" valign="top" class="description-td" colspan="5">TABLE 18</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="4" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="5"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">PIQ0</td>
                <td morerows="0" valign="top" class="description-td">PIQ1</td>
                <td morerows="0" valign="top" class="description-td">PIQ2</td>
                <td morerows="0" valign="top" class="description-td">PIQ3</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="4" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="5"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I0</td>
                <td morerows="0" valign="top" class="description-td">I1</td>
                <td morerows="0" valign="top" class="description-td">I4</td>
                <td morerows="0" valign="top" class="description-td"></td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I2</td>
                <td morerows="0" valign="top" class="description-td"></td>
                <td morerows="0" valign="top" class="description-td"></td>
                <td morerows="0" valign="top" class="description-td"></td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I3</td>
                <td morerows="0" valign="top" class="description-td"></td>
                <td morerows="0" valign="top" class="description-td"></td>
                <td morerows="0" valign="top" class="description-td"></td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="4" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="5"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>In addition to performing instruction queueing functions, the PIQ's <b>1730</b> also keep track of the execution status of each instruction that is issued to the processor elements <b>640</b>. In an ideal system, instructions could be issued to the processor elements every clock cycle without worrying about whether or not the instructions have finished execution. However, the processor elements <b>640</b> in the system may not be able to complete an instruction every clock cycle due to the occurrence of exceptional conditions, such as a data cache miss and so on. As a result, each PIQ <b>1730</b> tracks all instructions that it has issued to the processor elements <b>640</b> that are still in execution. The primary result of this tracking is that the PIQ's <b>1730</b> perform the instruction clocking function for the LRD <b>620</b>. In other words, the PIQ's <b>1730</b> determine when the next firing time register can be updated when executing straightline code. This in turn begins a new instruction fetch cycle.</p>
    <p>Instruction clocking is accomplished by having each PIQ <b>1730</b> form an instruction done signal that specifies that the instructions issued by a given PIQ either have executed or, in the case of pipelined PE's, have proceeded to the next stage. This is then combined with all other PIQ instruction done signals from this LRD and is used to gate the increment signal that increments the next firing time register. The done signals are delivered over lines <b>1564</b> to the instruction cache control <b>1518</b>.</p>
    <p>Referring to Figute <b>18</b>, the PIQ processor assignment circuit <b>1570</b> contains a two dimensional array of network interface units (NIU's) <b>1800</b> interconnected as a full access switch to the PE-LRD network <b>650</b> and then to the various processor elements <b>640</b>. Each network interface unit (NIU) <b>1800</b> is comprised of the same circuitry as the bus interface units (BIU) <b>1700</b> of FIG. <b>17</b>. In normal operation, the processor instruction queue #<b>0</b> (PIQ<b>0</b>) can directly access processor element <b>0</b> by activating the NIU<b>0</b> associated with the column corresponding to queue #<b>0</b>, the remaining network interface units NIU<b>0</b>, NIU<b>1</b>, NIU<b>2</b>, NIU<b>3</b> of the PIQ processor alignment circuit for that column and row being deactivated. Likewise, processor instruction queue #<b>3</b> (PIQ<b>3</b>) normally accesses processor element <b>3</b> by activating the NIU<b>3</b> of the column associated with queue #<b>3</b>, the remaining NIU<b>0</b>, NIU<b>1</b> NIU<b>2</b>, and NIU<b>3</b> of that column and row being deactivated. The activation of the network interface units <b>1800</b> is under the control of an instruction select and assignment unit <b>1810</b>.</p>
    <p>Unit <b>1810</b> receives signals from the PIQ's <b>1730</b> within the LRD that the unit <b>1810</b> is a member of over lines <b>1811</b>, from all other units <b>1810</b> (of other LRD's) over lines <b>1813</b>, and from the processor elements <b>640</b> through the network <b>650</b>. Each PIQ <b>1730</b> furnishes the unit <b>1810</b> with a signal that corresponds to I have an instruction that is ready to be assigned to a processor. The other PIQ buffer units furnish this unit <b>1810</b> and every other unit <b>1810</b> with a signal that corresponds to My PIQ <b>1730</b> (#x) has an instruction ready to be assigned to a processor. Finally, the processor elements furnish each unit <b>1810</b> in the system with a signal that corresponds to I can accept a new instruction.</p>
    <p>The unit <b>1810</b> on an LRD transmits signals to the PIQs <b>1730</b> of its LRD over lines <b>1811</b>, to the network interface units <b>1800</b> of its LRD over lines <b>1860</b> and to the other units <b>1810</b> of the other LRDs in the system over lines <b>1813</b>. The unit <b>1810</b> transmits a signal to each PIQ <b>1730</b> that corresponds to Gate your instruction onto the PE-LRD interface bus (<b>650</b>). The unit transmits a select signal to the network interface units <b>1800</b>. Finally, the unit <b>1810</b> transmits a signal that corresponds to I have used processor element #x to each other unit <b>1810</b> in the system for each processor which it is using.</p>
    <p>In addition, each unit <b>1810</b> in each LRD has associated with it a priority that corresponds to the priority of the LRD. This is used to order the LRDs into an ascending order from zero to the number of LRDs in the system. The method used for assigning the processor elements is as follows. Given that the LRDs are ordered, many allocation schemes are possible (e.g., round robin, first come first served, time slice, etc.). However, these are implementation details and do not impact the functionality of this unit under the teachings of the present invention.</p>
    <p>Consider the LRD with the current highest priority. This LRD gets all the processor elements that it requires and assigns the instructions that are ready to be executed to the available processor elements. If the processor elements are context free, the processor elements can be assigned in any manner whatsoever. Typically, however, assuming that all processors are functioning correctly, instructions from PIQ #<b>0</b> are routed to processor element #<b>0</b>, provided of course, processor element #<b>0</b> is available.</p>
    <p>The unit <b>1810</b> in the highest priority LRD then transmits this information to all other units <b>1810</b> in the system. Any processors left open are then utilized by the next highest priority LRD with instructions that can be executed. This allocation continues until all processors have been assigned. Hence, processors may be assigned on a priority basis in a daisy chained manner.</p>
    <p>If a particular processor element, for example, element <b>1</b> has failed, the instruction selective assignment unit <b>1810</b> can deactivate that processor element by deactivating all network instruction units NIU<b>1</b>. It can then, through hardware, reorder the processor elements so that, for example, processor element <b>2</b> receives all instructions logically assigned to processor element <b>1</b>, processor element <b>3</b> is now assigned to receive all instructions logically assigned to processor <b>2</b>, etc. Indeed, redundant processor elements and network interface units can be provided to the system to provide for a high degree of fault tolerance.</p>
    <p>Clearly, this is but one possible implementation. Other methods are also realizable.</p>
    <p>b. Branch Execution Unit (BEU)</p>
    <p>Referring to FIG. 19, the Branch Execution Unit (BEU) <b>1548</b> is the unit in the present invention responsible for the execution of all branch instructions which occur at the end of each basic block. There is, in the illustrated embodiment, one BEU <b>1548</b> for each supported context and so, with reference to FIG. 6, n supported contexts require n BEU's. The illustrated embodiment uses one BEU for each supported context because each BEU <b>1548</b> is of simple design and, therefore, the cost of sharing a BEU between plural contexts would be more expensive than allowing each context to have its own BEU.</p>
    <p>The BEU <b>1548</b> executes branches in a conventional manner with the exception that the branch instructions are executed outside the PE's <b>640</b>. The BEU <b>1548</b> evaluates the branch condition and, when the target address is selected, generates and places this address directly into the next instruction fetch register. The target address generation is conventional for unconditional and conditional branches that are not procedure calls or returns. The target address can be (a) taken directly from the instruction, (b) an offset from the current contents of the next instruction fetch register, or (c) an offset of a general purpose register of the context register file.</p>
    <p>A return branch from a subroutine is handled in a slightly different fashion. To understand the subroutine return branch, discussion of the subroutine call branch is required. When the branch is executed, a return address is created and stored. The return address is normally the address of the instruction following the subroutine call. The return address can be stored in a stack in memory or in other storage local to the branch execution unit. In addition, the execution of the subroutine call increments the procedural level counter.</p>
    <p>The return from a subroutine branch is also an unconditional branch. However, rather than containing the target address within the instruction, this type of branch reads the previously stored return address from storage, decrements the procedural level counter, and loads the next instruction fetch register with the return address. The remainder of the disclosure discusses the evaluation and execution of conditional branches. It should be noted the that techniques described also apply to unconditional branches, since these are, in effect, conditional branches in which the condition is always satisfied. Further, these same techniques also apply to the subroutine call and return branches, which perform the additional functions described above.</p>
    <p>To speed up conditional branches, the determination of whether a conditional branch is taken or not, depends solely on the analysis of the appropriate set of condition codes. Under the teachings of the present invention, no evaluation of data is performed other than to manipulate the condition codes appropriately. In addition, an instruction, which generates a condition code that a branch will use, can transmit the code to BEU <b>1548</b> as well as to the condition code storage. This eliminates the conventional extra waiting time required for the code to become valid in the condition code storage prior to a BEU being able to fetch it.</p>
    <p>The present invention also makes extensive use of delayed branching to guarantee program correctness. When a branch has executed and its effects are being propagated in the system, all instructions that are within the procedural domain of the branch must either have been executed or be in the process of being executed, as discussed in connection with the example of Table 6. In other words, changing the next-instruction pointer (in response to the branch) takes place after the current firing time has been updated to point to the firing time that follows the last (temporally executed) instruction of the branch. Hence, in the example of Table 6, instruction I<b>5</b> at firing time T<b>17</b> is delayed until the completion of T<b>18</b> which is the last firing time for this basic block. The instruction time for the next basic block is then T<b>19</b>.</p>
    <p>The functionality of the BEU <b>1548</b> can be described as a four-state state machine:</p>
    <p>Stage 1: Instruction decode</p>
    <p>Operation decode</p>
    <p>Delay field decode</p>
    <p>Condition code access decode</p>
    <p>Stage 2: Condition code fetch/receive</p>
    <p>Stage 3: Branch operation evaluation</p>
    <p>Stage 4: Next instruction fetch location and firing time update</p>
    <p>Along with determining the operation to be performed, the first stage also determines how long fetching can continue to take place after receipt of the branch by the BEU, and how the BEU is to access the condition codes for a conditional branch, that is, are they received or fetched.</p>
    <p>Referring to FIG. 19, the branch instruction is delivered over bus <b>1546</b> from the PIQ bus interface unit <b>1544</b> into the instruction register <b>1900</b> of the BEU <b>1548</b>. The fields of the instruction register <b>1900</b> are designated as: FETCH/ENABLE, CONDITION CODE ADDRESS, OP CODE, DELAY FIELD, and TARGET ADDRESS. The instruction register <b>1900</b> is connected over lines <b>1910</b> <i>a </i>and <b>1910</b> <i>b </i>to a condition code access unit <b>1920</b>, over lines <b>1910</b> <i>c </i>to an evaluation unit <b>1930</b>, over lines <b>1910</b> <i>d </i>to a delay unit <b>1940</b>, and over lines <b>1910</b> <i>e </i>to a next instruction interface <b>1950</b>.</p>
    <p>Once an instruction has been issued to BEU <b>1548</b> from the PIQ bus interface <b>1544</b>, instruction fetching must be held up until the value in the delay field has been determined. This value is measured relative to the receipt of the branch by the BEU, that is stage 1. If there are no instructions that may be overlapped with this branch, this field value is zero. In this case, instruction fetching is held up until the outcome of the branch has been determined. If this field is non-zero, instruction fetching may continue for a number of firing times given by the value in this field.</p>
    <p>The condition code access unit <b>1920</b> is connected to the register filePE network <b>670</b> over lines <b>1550</b> and to the evaluation unit <b>1930</b> over lines <b>1922</b>. During stage 2 operation, the condition code access decode unit <b>1920</b> determines whether or not the condition codes must be fetched by the instruction, or whether the instruction that determines the branch condition delivers them. As there is only one instruction per basic block that will determine the conditional branch, there will never be more than one condition code received by the BEU for a basic block. As a result, the actual timing of when the condition code is received is not important. If it comes earlier than the branch, no other codes will be received prior to the execution of the branch. If it comes later, the branch will be waiting and the codes received will always be the right ones. Note that the condition code for the basic block can include plural codes received at the same or different times by the BEU.</p>
    <p>The evaluation unit <b>1930</b> is connected to the next instruction interface <b>1950</b> over lines <b>1932</b>. The next instruction interface <b>1950</b> is connected to the instruction cache control circuit <b>1518</b> over lines <b>1549</b> and to the delay unit <b>1940</b> over lines <b>1942</b>; and the delay unit <b>1940</b> is also connected to the instruction cache control unit <b>1518</b> over lines <b>1549</b>.</p>
    <p>During the evaluation stage of operation, the condition codes are combined according to a Boolean function that represents the condition being tested. In the final stage of operation, either fetching of the sequential instruction stream continues, if a conditional branch is not taken, or the next instruction pointer is loaded, if the branch is taken.</p>
    <p>The impact of a branch in the instruction stream can be described as follows. Instructions, as discussed, are sent to their respective PIQ's <b>1730</b> by analysis of the resident logical processor number (LPN). Instruction fetching can be continued until a branch is encountered, that is, until an instruction is delivered to the instruction register <b>1900</b> of the BEU <b>1548</b>. At this point, in a conventional system without delayed branching, fetching would be stopped until the resolution of the branch instruction is complete. See, for example, Branch Prediction Strategies and Branch Target Buffer Design, J. F. K. Lee &amp; A. J. Smith, IEEE Computer Magazine, January, 1984.</p>
    <p>In the present system, which includes delayed branching, instructions must continue to be fetched until the next instruction fetched is the last instruction of the basic block to be executed. The time that the branch is executed is then the last time that fetching of an instruction can take place without a possibility of modifying the next instruction address. Thus, the difference between when the branch is fetched and when the effects of the branch are actually felt corresponds to the number of additional firing time cycles during which fetching can be continued.</p>
    <p>The impact of this delay is that the BEU <b>1548</b> must have access to the next instruction firing time register of the cache controller <b>1518</b>. Further, the BEU <b>1548</b> can control the initiation or disabling of the instruction fetch process performed by the instruction cache control unit <b>1518</b>. These tasks are accomplished by signals over bus <b>1549</b>.</p>
    <p>In operation the branch execution unit (BEU) <b>1548</b> functions as follows. The branch instruction, such as instruction I<b>5</b> in the example above, is loaded into the instruction register <b>1900</b> from the PIQ bus interface unit <b>1544</b>. The contents of the instruction register then control the further operation of BEU <b>1548</b>. The FETCH-ENABLE field indicates whether or not the condition code access unit <b>1920</b> should retrieve the condition code located at the address stored in the CC-ADX field (called FETCH) or whether the condition code will be delivered by the generating instruction.</p>
    <p>If a FETCH is requested, the unit <b>1920</b> accesses the register file-PE network <b>670</b> (see FIG. 6) to access the condition code storage <b>2000</b> which is shown in FIG. <b>20</b>. Referring to FIG. 20, the condition code storage <b>2000</b>, for each context file, is shown in the generalized case. A set of registers CC<sub>xy </sub>are provided for storing condition codes for procedural level y. Hence, the condition code storage <b>2000</b> is accessed and addressed by the unit <b>1920</b> to retrieve, pursuant to a FETCH request, the necessary condition code. The actual condition code and an indication that the condition code is received by the unit <b>1920</b> is delivered over lines <b>1922</b> to the evaluation unit <b>1930</b>. The OPCODE field, delivered to the evaluation unit <b>1930</b>, in conjunction with the received condition code, functions to deliver a branch taken signal over line <b>1932</b> to the next instruction interface <b>1950</b>. The evaluation unit <b>1930</b> is comprised of standard gate arrays such as those from LSI Logic Corporation, 1551 McCarthy Blvd., Milpitas, Calif. 95035.</p>
    <p>The evaluation unit <b>1930</b> accepts the condition code set that determines whether or not the conditional branch is taken, and under control of the OPCODE field, combines the set in a Boolean function to generate the conditional branch taken signal.</p>
    <p>The next instruction interface <b>1950</b> receives the branch target address from the TARGET-ADX field of the instruction register <b>1900</b> and the branch taken signal over line <b>1932</b>. However, the interface <b>1950</b> cannot operate until an enable signal is received from the delay unit <b>1940</b> over lines <b>1942</b>.</p>
    <p>The delay unit <b>1940</b> determines the amount of time that instruction fetching can be continued after the receipt of a branch instruction by the BEU. Previously, it has been described that when a branch instruction is received by the BEU, instruction fetching continues for one more cycle and then stops. The instruction fetched during this cycle is held up from passing through PIQ bus interface unit <b>1544</b> until the length of the delay field has been determined. For example, if the delay field is zero (implying that the branch is to be executed immediately), these instructions must still be withheld from the PIQ bus buffer unit until it is determined whether or not these are the right instructions to be fetched. If the delay field is non-zero, the instructions would be gated into the PIQ buffer unit as soon as the delay value was determined to be non-zero. The length of the delay is obtained from DELAY field of the instruction register <b>1900</b>. The delay unit receives the delay length from register <b>1900</b> and clock impulses from the context control <b>1518</b> over lines <b>1549</b>. The delay unit <b>1940</b> decrements the value of the delay at each clock pulse; and when fully decremented, the interface unit <b>1950</b> becomes enabled.</p>
    <p>Hence, in the discussion of Table 6, instruction is assigned a firing time T<b>17</b> but is delayed until firing time T<b>18</b>. During the delay time, the interface <b>1950</b> signals the instruction cache control <b>1518</b> over line <b>1549</b> to continue to fetch instructions to finish the current basic block. When enabled, the interface unit <b>1950</b> delivers the next address (that is, the branch execution address) for the next basic block into the instruction cache control <b>1518</b> over lines <b>1549</b>.</p>
    <p>In summary and for the example on Table 6, the branch instruction I<b>5</b> is loaded into the instruction register <b>1900</b> during time T<b>17</b>. However, a delay of one firing time (DELAY) is also loaded into the instruction register <b>1900</b> as the branch instruction cannot be executed until the last instruction I<b>3</b> is processed during time T<b>18</b>. Hence, even though the instruction I<b>5</b> is loaded in register <b>1900</b>, the branch address for the next basic block, which is contained in the TARGET ADDRESS, does not become effective until the completion of time T<b>18</b>. In the meantime, the next instruction interface <b>1950</b> issues instructions to the cache control <b>1518</b> to continue processing the stream of instructions in the basic block. Upon the expiration of the delay, the interface <b>1950</b> is enabled, and the branch is executed by delivering the address of the next basic block to the instruction cache control <b>1518</b>.</p>
    <p>Note that the delay field is used to guarantee the execution of all instructions in the basic block governed by this branch in single cycle context free PE's. A small complexity is encountered when the PE's are pipelined. In this case, there exist data dependencies between the instructions from the basic block just executed, and the instructions from the basic block to be executed. The TOLL software can analyze these dependencies when the next basic block is only targeted by the branch from this basic block. If the next basic block is targeted by more than one branch, the TOLL software cannot resolve the various branch possibilities and lets the pipelines drain, so that no data dependencies are violated. One mechanism for allowing the pipelines to drain is to insert NO-OP (no operation) instructions into the instruction stream. An alternate method provides an extra field in the branch instruction which inhibits the delivery of new instructions to the processor elements for a time determined by the data in the extra field.</p>
    <p>c. Processor Elements (PE)</p>
    <p>So far in the discussions pertaining to the matrix multiply example, a single cycle processor element has been assumed. In other words, an instruction is issued to the processor element and the processor element completely executes the instruction before proceeding to the next instruction. However, greater performance can be obtained by employing pipelined processor elements. Accordingly, the tasks performed by the TOLL software change slightly. In particular, the assignment of the processor elements is more complex than is shown in the previous example; and the hazards that characterize a pipeline processor must be handled by the TOLL software. The hazards that are present in any pipelined processor manifest themselves as a more sophisticated set of data dependencies. This can be encoded into the TOLL software by one practiced in the art. See for example, T. K. R. Gross, Stanford University, 1983, Code Optimization of Pipeline Constraints, Doctorate Dissertation Thesis.</p>
    <p>The assignment of the processors is dependent on the implementation of the pipelines and again, can be performed by one practiced in the art. A key parameter is determining how data is exchanged between the pipelines. For example, assume that each pipeline contains feedback paths between its stages. In addition, assume that the pipelines can exchange results only through the register sets <b>660</b>. Instructions would be assigned to the pipelines by determining sets of dependent instructions that are contained in the instruction stream and then assigning each specific set to a specific pipeline. This minimizes the amount of communication that must take place between the pipelines (via the register set), and hence speeds up the execution time of the program. The use of the logical processor number guarantees that the instructions will execute on the same pipeline.</p>
    <p>Alternatively, if there are paths available to exchange data between the pipelines, dependent instructions may be distributed across several pipeline processors instead of being assigned to a single pipeline. Again, the use of multiple pipelines and the interconnection network between them that allows the sharing of intermediate results manifests itself as a more sophisticated set of data dependencies imposed on the instruction stream. Clearly, the extension of the teachings of this invention to a pipelined system is within the skill of one practiced in the art.</p>
    <p>Importantly, the additional data (chaining) paths do not change the fundamental context free nature of the processor elements of the present invention. That is, at any given time (for example, the completion of any given instruction cycle), the entire process state associated with a given program (that is, context) is captured completely external to the processor elements. Data chaining results merely in a transitory replication of some of the data generated within the processor elements during a specific instruction clock cycle.</p>
    <p>Referring to FIG. 21, a particular processor element <b>640</b> has a four-stage pipeline processor element. All processor elements <b>640</b> according to the illustrated embodiment are identical. It is to be expressly understood, that any prior art type of processor element such as a micro-processor or other pipeline architecture could not be used under the teachings of the present invention, because such processors retain substantial state information of the program they are processing. However, such a processor could be programmed with software to emulate or simulate the type of processor necessary for the present invention.</p>
    <p>The design of the processor element is determined by the instruction set architecture generated by the TOLL software and, therefore, from a conceptual viewpoint, is the most implementation dependent portion of this invention. In the illustrated embodiment shown in FIG. 21, each processor element pipeline operates autonomously of the other processor elements in the system. Each processor element is homogeneous and is capable, by itself, of executing all computational and data memory accessing instructions. In making computational executions, transfers are from register to register and for memory interface instructions, the transfers are from memory to registers or from registers to memory.</p>
    <p>Referring to FIG. 21, the four-stage pipeline for the processor element <b>640</b> of the illustrated embodiment includes four discrete instruction registers <b>2100</b>, <b>2110</b>, <b>2120</b>, and <b>2130</b>. Each processor element also includes four stages: stage 1, <b>2140</b>; stage 2, <b>2150</b>; stage 3, <b>2160</b>, and stage 4, <b>2170</b>. The first instruction register <b>2100</b> is connected through the network <b>650</b> to the PIQ processor assignment circuit <b>1570</b> and receives that information over bus <b>2102</b>. The instruction register <b>2100</b> then controls the operation of stage 1 which includes the hardware functions of instruction decode and register <b>0</b> fetch and register <b>1</b> fetch. The first stage <b>2140</b> is interconnected to the instruction register over lines <b>2104</b> and to the second instruction register <b>2110</b> over lines <b>2142</b>. The first stage <b>2140</b> is also connected over a bus <b>2144</b> to the second stage <b>2150</b>. Register <b>0</b> fetch and register <b>1</b> fetch of stage 1 are connected over lines <b>2146</b> and <b>2148</b>, respectively, to network <b>670</b> for access to the register file <b>660</b>.</p>
    <p>The second instruction register <b>2110</b> is further interconnected to the third instruction register <b>2120</b> over lines <b>2112</b> and to the second stage <b>2150</b> over lines <b>2114</b>. The second stage <b>2150</b> is also connected over a bus <b>2152</b> to the third stage <b>2160</b> and further has the memory write (MEM WRITE) register fetch hardware interconnected over lines <b>2154</b> to network <b>670</b> for access to the register file <b>660</b> and its condition code (CC) hardware connected over lines <b>2156</b> through network <b>670</b> to the condition code storage of context file <b>660</b>.</p>
    <p>The third instruction register <b>2120</b> is interconnected over lines <b>2122</b> to the fourth instruction register <b>2130</b> and is also connected over lines <b>2124</b> to the third stage <b>2160</b>. The third stage <b>2160</b> is connected over a bus <b>2162</b> to the fourth stage <b>2170</b> and is further interconnected over lines <b>2164</b> through network <b>650</b> to the data cache interconnection network <b>1590</b>.</p>
    <p>Finally, the fourth instruction register <b>2130</b> is interconnected over lines <b>2132</b> to the fourth stage, and the fourth stage has its store hardware (STORE) output connected over lines <b>2172</b> and its effective address update (EFF. ADD.) hardware circuit connected over lines <b>2174</b> to network <b>670</b> for access to the register file <b>660</b>. In addition, the fourth stage has its condition code store (CC STORE) hardware connected over lines <b>2176</b> through network <b>670</b> to the condition code storage of context file <b>660</b>.</p>
    <p>The operation of the four-stage pipeline shown in FIG. 21 will now be discussed with respect to the example of Table 1 and the information contained in Table 19 which describes the operation of the processor element for each instruction.</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="2" colsep="0" rowsep="0" align="left"> <colspec colname="OFFSET" align="left" colwidth="42PT"> </colspec> <colspec colname="1" align="left" colwidth="175PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="1" morerows="0" rowsep="1" valign="top" class="description-td" colspan="2">TABLE 19</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="1" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="2"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Instruction I0, (I1):</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 1 - Fetch Reg to form Mem-adx</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 2 - Form Mem-adx</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 3 - Perform Memory Read</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 4 - Store R0, (R1)</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Instruction I2:</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 1 - Fetch Reg R0 and R1</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 2 - No-Op</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 3 - Perform multiply</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 4 - Store R2 and CC</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Instruction I3:</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 1 - Fetch Reg R2 and R3</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 2 - No-Op</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 3 - Perform addition</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 4 - Store R3 and CC</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Instruction I4:</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 1 - Fetch Reg R4</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 2 - No-Op</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 3 - Perform decrement</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">Stage 4 - Store R4 and CC</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td namest="OFFSET" nameend="1" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="2"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>For instructions I<b>0</b> and I<b>1</b>, the performance by the processor element <b>640</b> in FIG. 21 is the same except in stage 4. The first stage is to fetch the memory address from the register which contains the address in the register file. Hence, stage 1 interconnects circuitry <b>2140</b> over lines <b>2146</b> through network <b>670</b> to that register and downloads it into register <b>0</b> from the interface of stage 1. Next, the address is delivered over bus <b>2144</b> to stage 2, and the memory write hardware forms the memory address. The memory address is then delivered over bus <b>2152</b> to the third stage which reads memory over <b>2164</b> through network <b>650</b> to the data cache interconnection network <b>1590</b>. The results of the read operation are then stored and delivered to stage 4 for storage in register R<b>0</b>. Stage 4 delivers the data over lines <b>2172</b> through network <b>670</b> to register R<b>0</b> in the register file. The same operation takes place for instruction I<b>1</b> except that the results are stored in register <b>1</b>. Hence, the four stages of the pipeline (Fetch, Form Memory Address, Perform Memory Read, and Store The Results) flow data through the pipe in the manner discussed, and when instruction I<b>0</b> has passed through stage 1, the first stage of instruction I<b>1</b> commences. This overlapping or pipelining is conventional in the art.</p>
    <p>Instruction I<b>2</b> fetches the information stored in registers R<b>0</b> and R<b>1</b> in the register file <b>660</b> and delivers them into registers REG<b>0</b> and REG<b>1</b> of stage 1. The contents are delivered over bus <b>2144</b> through stage 2 as a no operation and then over bus <b>2152</b> into stage 3. A multiply occurs with the contents of the two registers, the results are delivered over bus <b>2162</b> into stage 4 which then stores the results over lines <b>2172</b> through network <b>670</b> into register R<b>2</b> of the register file <b>660</b>. In addition, the condition code data is stored over lines <b>2176</b> in the condition code storage of context files <b>660</b>.</p>
    <p>Instruction I<b>3</b> performs the addition of the data in registers R<b>2</b> and R<b>3</b> in the same fashion, to store the results, at stage 4, in register R<b>3</b> and to update the condition code data for that instruction. Finally, instruction I<b>4</b> operates in the same fashion except that stage 3 performs a decrement of the contents of register R<b>4</b>.</p>
    <p>Hence, according to the example of Table I, the instructions for PE<b>0</b>, would be delivered from the PIQ<b>0</b> in the following order: I<b>0</b>, I<b>2</b>, and I<b>3</b>. These instructions would be sent through the PE<b>0</b> pipeline stages (S<b>1</b>, S<b>2</b>, S<b>3</b>, and S<b>4</b>), based the upon the instruction firing times (T<b>16</b>, T<b>17</b>, and T<b>18</b>), as follows:</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup cols="8" colsep="0" rowsep="0" align="left"> <colspec colname="1" align="left" colwidth="21PT"> </colspec> <colspec colname="2" align="center" colwidth="28PT"> </colspec> <colspec colname="3" align="center" colwidth="28PT"> </colspec> <colspec colname="4" align="center" colwidth="28PT"> </colspec> <colspec colname="5" align="center" colwidth="28PT"> </colspec> <colspec colname="6" align="center" colwidth="28PT"> </colspec> <colspec colname="7" align="center" colwidth="28PT"> </colspec> <colspec colname="8" align="center" colwidth="28PT"> </colspec> <thead valign="bottom"> <tr class="description-tr"> <td namest="1" nameend="8" morerows="0" rowsep="1" valign="top" class="description-td" colspan="8">TABLE 20</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="8" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="8"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">PE</td>
                <td morerows="0" valign="top" class="description-td">Inst</td>
                <td morerows="0" valign="top" class="description-td">T16</td>
                <td morerows="0" valign="top" class="description-td">T17</td>
                <td morerows="0" valign="top" class="description-td">T18</td>
                <td morerows="0" valign="top" class="description-td">T19</td>
                <td morerows="0" valign="top" class="description-td">T20</td>
                <td morerows="0" valign="top" class="description-td">T21</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="8" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="8"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">PE0:</td>
                <td morerows="0" valign="top" class="description-td">10</td>
                <td morerows="0" valign="top" class="description-td">S1</td>
                <td morerows="0" valign="top" class="description-td">S2</td>
                <td morerows="0" valign="top" class="description-td">S3</td>
                <td morerows="0" valign="top" class="description-td">S4</td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I2</td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">S1</td>
                <td morerows="0" valign="top" class="description-td">S2</td>
                <td morerows="0" valign="top" class="description-td">S3</td>
                <td morerows="0" valign="top" class="description-td">S4</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">I3</td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td"> </td>
                <td morerows="0" valign="top" class="description-td">S1</td>
                <td morerows="0" valign="top" class="description-td">S2</td>
                <td morerows="0" valign="top" class="description-td">S3</td>
                <td morerows="0" valign="top" class="description-td">S4</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">PE1:</td>
                <td morerows="0" valign="top" class="description-td">I1</td>
                <td morerows="0" valign="top" class="description-td">S1</td>
                <td morerows="0" valign="top" class="description-td">S2</td>
                <td morerows="0" valign="top" class="description-td">S3</td>
                <td morerows="0" valign="top" class="description-td">S4</td>
              </tr> <tr class="description-tr"> <td morerows="0" valign="top" class="description-td">PE2:</td>
                <td morerows="0" valign="top" class="description-td">I4</td>
                <td morerows="0" valign="top" class="description-td">S1</td>
                <td morerows="0" valign="top" class="description-td">S2</td>
                <td morerows="0" valign="top" class="description-td">S3</td>
                <td morerows="0" valign="top" class="description-td">S4</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="8" morerows="0" rowsep="1" valign="top" align="center" class="description-td" colspan="8"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>The schedule illustrated in Table 20 is not however possible unless data chaining is introduced within the pipeline processor (intraprocessor data chaining) as well as between pipeline processors (interprocessor data chaining). The requirement for data chaining occurs because an instruction no longer completely executes within a single time cycle illustrated by, for example, instruction firing time T<b>16</b>. Thus, for a pipeline processor, the TOLL software must recognize that the results of the store which occurs at stage 4 (T<b>19</b>) of instructions I<b>0</b> and I<b>1</b> are needed to perform the multiply at stage 3 (T<b>19</b>) of instruction I<b>2</b>, and that fetching of those operands normally takes place at stage 1 (T<b>17</b>) of instruction I<b>2</b>. Accordingly, in the normal operation of the pipeline, for processors PE<b>0</b> and PE<b>1</b>, the operand data from registers R<b>0</b> and R<b>1</b> is not available until the end of firing time T<b>18</b> while it is needed by stage 1 of instruction I<b>2</b> at time T<b>17</b>.</p>
    <p>To operate according to the schedule illustrated in Table 20, additional data (chaining) paths must be made available to the processors, paths which exist both internal to the processors and between processors. These paths, well known to those practiced in the art, are the data chaining paths. They are represented, in FIG. 21, as dashed lines <b>2180</b> and <b>2182</b>. Accordingly, therefore, the resolution of data dependencies between instructions and all scheduling of processor resources which are performed by the TOLL software prior to program execution, take into account the availability of data chaining when needed to make available data directly from the output, for example, of one stage of the same processor or a stage of a different processor. This data chaining capability is well known to those practiced in the art and can be implemented easily in the TOLL software analysis by recognizing each stage of the pipeline processor as being, in effect, a separate processor having resource requirements and certain dependencies, that is, that an instruction when started through a pipeline will preferably continue in that same pipeline through all of its processing stages. With this in mind, the speed up in processing can be observed in Table 20 where the three machine cycle times for the basic block are completed in a time of only six pipeline cycles. It should be borne in mind that the cycle time for a pipeline is approximately one-fourth the cycle time for the non-pipeline processor in the illustrated embodiment of the invention.</p>
    <p>The pipeline of FIG. 21 is composed of four equal (temporal) length stages. The first stage <b>2140</b> performs the instruction decode, determines what registers to fetch and store, and performs up to two source register fetches which can be required for the execution of the instruction.</p>
    <p>The second stage <b>2150</b> is used by the computational instructions for the condition code fetch if required. It is also the effective address generation stage for the memory interface instructions.</p>
    <p>The effective address operations that are supported in the preferred embodiment of the invention are:</p>
    <p>1. Absolute address</p>
    <p>The full memory address is contained in the instruction.</p>
    <p>2. Register indirect</p>
    <p>The full memory address is contained in a register.</p>
    <p>3. Register indexed/based</p>
    <p>The full memory address is formed by combining the designated registers and immediate data.</p>
    <p>a. Rn op K</p>
    <p>b. Rn op Rm</p>
    <p>c. Rn op K op Rm</p>
    <p>d. Rn op Rm op K</p>
    <p>where op can be addition (+), subtraction (), or multiplication (*) and K is a constant.</p>
    <p>As an example, the addressing constructs presented in the matrix multiply inner loop example are formed from case 3-a where the constant K is the length of a data element within the array and the operation is addition (+).</p>
    <p>At a conceptual level, the effective addressing portion of a memory access instruction is composed of three basic functions; the designation and procurement of the registers and immediate data needed for the calculation, the combination of these operands in order to form the desired address, and if necessary, updating of any one of the registers involved. This functionality is common in the prior art and is illustrated by the autoincrement and autodecrement modes of addressing available in the DEC processor architecture. See, for example, DEC VAX Architecture Handbook.</p>
    <p>Aside from the obvious hardware support required, the effective addressing is supported by the TOLL software, and impacts the TOLL software by adding functionality to the memory accessing instructions. In other words, an effective address memory access can be interpreted as a concatenation of two operations, the first being the effective address calculation and the second being the actual memory access. This functionality can be easily encoded into the TOLL software by one skilled in the art in much the same manner as an add, subtract or multiply instruction would be.</p>
    <p>The described effective addressing constructs are to be interpreted as but one possible embodiment of a memory accessing system. There are a plethora of other methods and modes for generating a memory address that are known to those skilled in the art. In other words, the effective addressing constructs described above are for design completeness only, and are not to be construed as a key element in the design of the system.</p>
    <p>Referring to FIG. 22, various structures of data or data fields within the pipeline processor element of FIG. 21 are illustrated for a system which is a multi-user system in both time and space. As a result, across the multiple pipelines, instructions from different users may be executing, each with its own processor state. As the processor state is not typically associated with the processor element, the instruction must carry along the identifiers that specify this state. This processor state is supported by the LRD, register file and condition code file assigned to the user.</p>
    <p>A sufficient amount of information must be associated with each instruction so that each memory access, condition code access or register access can uniquely identify the target of the access. In the case of the registers and condition codes, this additional information constitutes the absolute value of the procedural level (PL) and context identifiers (CI) and is attached to the instruction by the SCSM attachment unit <b>1650</b>. This is illustrated in FIGS. 22<i>a</i>, <b>22</b> <i>b </i>and <b>22</b> <i>c </i>respectively. The context identifier portion is used to determine which register or condition code plane (FIG. 6) is being accessed. The procedural level is used to determine which procedural level of registers (FIG. 13) is to be accessed.</p>
    <p>Memory accesses also require that the LRD that supports the current user be identified so that the appropriate data cache can be accessed. This is accomplished through the context identifier. The data cache access further requires that a process identifier (PID) for the current user be available to verify that the data present in the cache is indeed the data desired. Thus, an address issued to the data cache takes the form of FIG. 22<i>d</i>. The miscellaneous field is composed of additional information describing the access, for example, read or write, user or system, etc.</p>
    <p>Finally, due to the fact that there can be several users executing across the pipelines during a single time interval, information that controls the execution of the instructions, and which would normally be stored within the pipeline, must be associated with each instruction instead. This information is reflected in the ISW field of an instruction word as illustrated in FIG. 22<i>a</i>. The information in this field is composed of control fields like error masks, floating point format descriptors, rounding mode descriptors, etc. Each instruction would have this field attached, but, obviously, may not require all the information. This information is used by the ALU stage <b>2160</b> of the processor element.</p>
    <p>This instruction information relating to the ISW field, as well as the procedural level, context identification and process identifier, are attached dynamically by the SCSM attacher (<b>1650</b>) as the instruction is issued from the instruction cache.</p>
    <p>Although the system of the present invention has been specifically set forth in the above disclosure, it is to be understood that modifications and variations can be made thereto which would still fall within the scope and coverage of the following claims.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3343135">US3343135</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 13, 1964</td><td class="patent-data-table-td patent-date-value">Sep 19, 1967</td><td class="patent-data-table-td ">Ibm</td><td class="patent-data-table-td ">Compiling circuitry for a highly-parallel computing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3611306">US3611306</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 5, 1969</td><td class="patent-data-table-td patent-date-value">Oct 5, 1971</td><td class="patent-data-table-td ">Burroughs Corp</td><td class="patent-data-table-td ">Mechanism to control the sequencing of partially ordered instructions in a parallel data processing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3771141">US3771141</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 8, 1971</td><td class="patent-data-table-td patent-date-value">Nov 6, 1973</td><td class="patent-data-table-td ">Culler Harrison Inc</td><td class="patent-data-table-td ">Data processor with parallel operations per instruction</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4104720">US4104720</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 29, 1976</td><td class="patent-data-table-td patent-date-value">Aug 1, 1978</td><td class="patent-data-table-td ">Data General Corporation</td><td class="patent-data-table-td ">CPU/Parallel processor interface with microcode extension</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4109311">US4109311</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 23, 1976</td><td class="patent-data-table-td patent-date-value">Aug 22, 1978</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Instruction execution modification mechanism for time slice controlled data processors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4153932">US4153932</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 19, 1975</td><td class="patent-data-table-td patent-date-value">May 8, 1979</td><td class="patent-data-table-td ">Massachusetts Institute Of Technology</td><td class="patent-data-table-td ">Data processing apparatus for highly parallel execution of stored programs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4181936">US4181936</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 12, 1977</td><td class="patent-data-table-td patent-date-value">Jan 1, 1980</td><td class="patent-data-table-td ">Siemens Aktiengesellschaft</td><td class="patent-data-table-td ">Data exchange processor for distributed computing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4228495">US4228495</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 19, 1978</td><td class="patent-data-table-td patent-date-value">Oct 14, 1980</td><td class="patent-data-table-td ">Allen-Bradley Company</td><td class="patent-data-table-td ">Multiprocessor numerical control system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4229790">US4229790</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 16, 1978</td><td class="patent-data-table-td patent-date-value">Oct 21, 1980</td><td class="patent-data-table-td ">Denelcor, Inc.</td><td class="patent-data-table-td ">Concurrent task and instruction processor and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4241398">US4241398</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 29, 1978</td><td class="patent-data-table-td patent-date-value">Dec 23, 1980</td><td class="patent-data-table-td ">United Technologies Corporation</td><td class="patent-data-table-td ">Computer network, line protocol system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4270167">US4270167</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 30, 1978</td><td class="patent-data-table-td patent-date-value">May 26, 1981</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Apparatus and method for cooperative and concurrent coprocessing of digital information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4435758">US4435758</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 13, 1982</td><td class="patent-data-table-td patent-date-value">Mar 6, 1984</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method for conditional branch execution in SIMD vector processors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4466061">US4466061</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 8, 1982</td><td class="patent-data-table-td patent-date-value">Aug 14, 1984</td><td class="patent-data-table-td ">Burroughs Corporation</td><td class="patent-data-table-td ">Concurrent processing elements for using dependency free code</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4468736">US4468736</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 8, 1982</td><td class="patent-data-table-td patent-date-value">Aug 28, 1984</td><td class="patent-data-table-td ">Burroughs Corporation</td><td class="patent-data-table-td ">In a data processing system</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6351802">US6351802</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 3, 1999</td><td class="patent-data-table-td patent-date-value">Feb 26, 2002</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Method and apparatus for constructing a pre-scheduled instruction cache</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6578137">US6578137</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 8, 2001</td><td class="patent-data-table-td patent-date-value">Jun 10, 2003</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Branch and return on blocked load or store</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7032226">US7032226</a></td><td class="patent-data-table-td patent-date-value">Jun 30, 2000</td><td class="patent-data-table-td patent-date-value">Apr 18, 2006</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Methods and apparatus for managing a buffer of events in the background</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7042887">US7042887</a></td><td class="patent-data-table-td patent-date-value">Jul 5, 2001</td><td class="patent-data-table-td patent-date-value">May 9, 2006</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Method and apparatus for non-speculative pre-fetch operation in data packet processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7058064">US7058064</a></td><td class="patent-data-table-td patent-date-value">Dec 14, 2000</td><td class="patent-data-table-td patent-date-value">Jun 6, 2006</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Queueing system for processors in packet routing operations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7058065">US7058065</a></td><td class="patent-data-table-td patent-date-value">Aug 7, 2001</td><td class="patent-data-table-td patent-date-value">Jun 6, 2006</td><td class="patent-data-table-td ">Mips Tech Inc</td><td class="patent-data-table-td ">Method and apparatus for preventing undesirable packet download with pending read/write operations in data packet processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7065096">US7065096</a></td><td class="patent-data-table-td patent-date-value">Aug 20, 2001</td><td class="patent-data-table-td patent-date-value">Jun 20, 2006</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Method for allocating memory space for limited packet head and/or tail growth</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7076630">US7076630</a></td><td class="patent-data-table-td patent-date-value">Jun 14, 2001</td><td class="patent-data-table-td patent-date-value">Jul 11, 2006</td><td class="patent-data-table-td ">Mips Tech Inc</td><td class="patent-data-table-td ">Method and apparatus for allocating and de-allocating consecutive blocks of memory in background memo management</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7082552">US7082552</a></td><td class="patent-data-table-td patent-date-value">Sep 11, 2001</td><td class="patent-data-table-td patent-date-value">Jul 25, 2006</td><td class="patent-data-table-td ">Mips Tech Inc</td><td class="patent-data-table-td ">Functional validation of a packet management unit</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7139901">US7139901</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 7, 2001</td><td class="patent-data-table-td patent-date-value">Nov 21, 2006</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Extended instruction set for packet processing applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7155516">US7155516</a></td><td class="patent-data-table-td patent-date-value">Sep 25, 2001</td><td class="patent-data-table-td patent-date-value">Dec 26, 2006</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Method and apparatus for overflowing data packets to a software-controlled memory when they do not fit into a hardware-controlled memory</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7165257">US7165257</a></td><td class="patent-data-table-td patent-date-value">Aug 10, 2001</td><td class="patent-data-table-td patent-date-value">Jan 16, 2007</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Context selection and activation mechanism for activating one of a group of inactive contexts in a processor core for servicing interrupts</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7197043">US7197043</a></td><td class="patent-data-table-td patent-date-value">Apr 6, 2006</td><td class="patent-data-table-td patent-date-value">Mar 27, 2007</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Method for allocating memory space for limited packet head and/or tail growth</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7233331">US7233331</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 14, 2001</td><td class="patent-data-table-td patent-date-value">Jun 19, 2007</td><td class="patent-data-table-td ">Square Enix Co., Ltd.</td><td class="patent-data-table-td ">Parallel object task engine and processing method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7280548">US7280548</a></td><td class="patent-data-table-td patent-date-value">Apr 6, 2006</td><td class="patent-data-table-td patent-date-value">Oct 9, 2007</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Method and apparatus for non-speculative pre-fetch operation in data packet processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7376811">US7376811</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 6, 2001</td><td class="patent-data-table-td patent-date-value">May 20, 2008</td><td class="patent-data-table-td ">Netxen, Inc.</td><td class="patent-data-table-td ">Method and apparatus for performing computations and operations on data using data steering</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7447887">US7447887</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 7, 2006</td><td class="patent-data-table-td patent-date-value">Nov 4, 2008</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Multithread processor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7502876">US7502876</a></td><td class="patent-data-table-td patent-date-value">Jun 23, 2000</td><td class="patent-data-table-td patent-date-value">Mar 10, 2009</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Background memory manager that determines if data structures fits in memory with memory state transactions map</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7526636">US7526636</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 12, 2004</td><td class="patent-data-table-td patent-date-value">Apr 28, 2009</td><td class="patent-data-table-td ">Infineon Technologies Ag</td><td class="patent-data-table-td ">Parallel multithread processor (PMT) with split contexts</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7551626">US7551626</a></td><td class="patent-data-table-td patent-date-value">Feb 23, 2006</td><td class="patent-data-table-td patent-date-value">Jun 23, 2009</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Queueing system for processors in packet routing operations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7644307">US7644307</a></td><td class="patent-data-table-td patent-date-value">Apr 29, 2006</td><td class="patent-data-table-td patent-date-value">Jan 5, 2010</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Functional validation of a packet management unit</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7649901">US7649901</a></td><td class="patent-data-table-td patent-date-value">Jun 13, 2001</td><td class="patent-data-table-td patent-date-value">Jan 19, 2010</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Method and apparatus for optimizing selection of available contexts for packet processing in multi-stream packet processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7661112">US7661112</a></td><td class="patent-data-table-td patent-date-value">Apr 5, 2006</td><td class="patent-data-table-td patent-date-value">Feb 9, 2010</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Methods and apparatus for managing a buffer of events in the background</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7715410">US7715410</a></td><td class="patent-data-table-td patent-date-value">Mar 23, 2006</td><td class="patent-data-table-td patent-date-value">May 11, 2010</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Queueing system for processors in packet routing operations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7765554">US7765554</a></td><td class="patent-data-table-td patent-date-value">Dec 5, 2006</td><td class="patent-data-table-td patent-date-value">Jul 27, 2010</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Context selection and activation mechanism for activating one of a group of inactive contexts in a processor core for servicing interrupts</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7877481">US7877481</a></td><td class="patent-data-table-td patent-date-value">Oct 25, 2006</td><td class="patent-data-table-td patent-date-value">Jan 25, 2011</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Method and apparatus for overflowing data packets to a software-controlled memory when they do not fit into a hardware-controlled memory</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7945433">US7945433</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 30, 2007</td><td class="patent-data-table-td patent-date-value">May 17, 2011</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Hardware simulation accelerator design and method that exploits a parallel structure of user models to support a larger user model size</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8081645">US8081645</a></td><td class="patent-data-table-td patent-date-value">Dec 29, 2009</td><td class="patent-data-table-td patent-date-value">Dec 20, 2011</td><td class="patent-data-table-td ">Mips Technologies, Inc.</td><td class="patent-data-table-td ">Context sharing between a streaming processing unit (SPU) and a packet management unit (PMU) in a packet processing environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8102552">US8102552</a></td><td class="patent-data-table-td patent-date-value">Apr 3, 2008</td><td class="patent-data-table-td patent-date-value">Jan 24, 2012</td><td class="patent-data-table-td ">Sharp Laboratories Of America, Inc.</td><td class="patent-data-table-td ">Performance monitoring and control of a multifunction printer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8279886">US8279886</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 30, 2004</td><td class="patent-data-table-td patent-date-value">Oct 2, 2012</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Dataport and methods thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8315990">US8315990</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 8, 2007</td><td class="patent-data-table-td patent-date-value">Nov 20, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Consistency sensitive streaming operators</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8392924">US8392924</a></td><td class="patent-data-table-td patent-date-value">Apr 3, 2008</td><td class="patent-data-table-td patent-date-value">Mar 5, 2013</td><td class="patent-data-table-td ">Sharp Laboratories Of America, Inc.</td><td class="patent-data-table-td ">Custom scheduling and control of a multifunction printer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8413169">US8413169</a></td><td class="patent-data-table-td patent-date-value">Oct 21, 2009</td><td class="patent-data-table-td patent-date-value">Apr 2, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Time-based event processing using punctuation events</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc712/defs712.htm&usg=AFQjCNFSlkDLYE-yriA4W3Ix_OZ4YREIEw#C712S226000">712/226</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc712/defs712.htm&usg=AFQjCNFSlkDLYE-yriA4W3Ix_OZ4YREIEw#C712SE09062">712/E09.062</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc712/defs712.htm&usg=AFQjCNFSlkDLYE-yriA4W3Ix_OZ4YREIEw#C712S233000">712/233</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc712/defs712.htm&usg=AFQjCNFSlkDLYE-yriA4W3Ix_OZ4YREIEw#C712S228000">712/228</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc712/defs712.htm&usg=AFQjCNFSlkDLYE-yriA4W3Ix_OZ4YREIEw#C712SE09049">712/E09.049</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc712/defs712.htm&usg=AFQjCNFSlkDLYE-yriA4W3Ix_OZ4YREIEw#C712SE09083">712/E09.083</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc712/defs712.htm&usg=AFQjCNFSlkDLYE-yriA4W3Ix_OZ4YREIEw#C712SE09056">712/E09.056</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc712/defs712.htm&usg=AFQjCNFSlkDLYE-yriA4W3Ix_OZ4YREIEw#C712SE09082">712/E09.082</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc712/defs712.htm&usg=AFQjCNFSlkDLYE-yriA4W3Ix_OZ4YREIEw#C712S234000">712/234</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0009380000">G06F9/38</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0009400000">G06F9/40</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0009440000">G06F9/44</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0009450000">G06F9/45</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0009420000">G06F9/42</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F9/4426">G06F9/4426</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F9/3867">G06F9/3867</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F9/3804">G06F9/3804</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F8/445">G06F8/445</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F9/4425">G06F9/4425</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F9/30123">G06F9/30123</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F8/45">G06F8/45</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F9/3836">G06F9/3836</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=XkFVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F9/3855">G06F9/3855</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06F9/38B2</span>, <span class="nested-value">G06F8/445</span>, <span class="nested-value">G06F8/45</span>, <span class="nested-value">G06F9/44F1A</span>, <span class="nested-value">G06F9/44F1A1</span>, <span class="nested-value">G06F9/38P</span>, <span class="nested-value">G06F9/38E</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Jan 30, 2013</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">11</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 30, 2013</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 28, 2010</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 28, 2010</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">May 24, 2010</td><td class="patent-data-table-td ">PRDP</td><td class="patent-data-table-td ">Patent reinstated due to the acceptance of a late maintenance fee</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100528</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 18, 2009</td><td class="patent-data-table-td ">FP</td><td class="patent-data-table-td ">Expired due to failure to pay maintenance fee</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090626</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 26, 2009</td><td class="patent-data-table-td ">REIN</td><td class="patent-data-table-td ">Reinstatement after maintenance fee payment confirmed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 5, 2009</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 11, 2008</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 1-25 IS CONFIRMED. NEW CLAIMS 26 AND 27 ARE ADDED AND DETERMINED TO BE PATENTABLE.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 3, 2006</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20060209</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 27, 2004</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 16, 1998</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">EQUIPMENT INVESTMENT &amp; MANAGEMENT CO., FLORIDA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:BIAX CORPORATION;REEL/FRAME:009500/0466</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19980630</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 5, 1996</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">BIAX CORPORATION, FLORIDA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MCC DEVELOPMENT LTD.;REEL/FRAME:007786/0906</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19870910</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U0Ss1xyT0oSyeQYiFB-9AFqrp0Wdw\u0026id=XkFVBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U1AO5EvV-zk_--LfgTKMBj0q3WvlA\u0026id=XkFVBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U2NKUBlZ1VmsDG6RpnSg9ScJhBSYg","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Parallel_processor_system_for_processing.pdf?id=XkFVBAABERAJ\u0026output=pdf\u0026sig=ACfU3U2nSiq6UEJFZ0tR_nbr8F4KFF9YMg"},"sample_url":"http://www.google.com/patents/reader?id=XkFVBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>