<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6263311 - Method and system for providing security using voice recognition - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Method and system for providing security using voice recognition"><meta name="DC.contributor" content="Robert Dildy" scheme="inventor"><meta name="DC.contributor" content="Advanced Micro Devices, Inc." scheme="assignee"><meta name="DC.date" content="1999-1-11" scheme="dateSubmitted"><meta name="DC.description" content="Method and system for providing security to a space by detecting an unauthorized presence in the space using voice recognition. A plurality of acceptable voice patterns is generated from sounds of authorized users and stored in the security system. Sounds in the space to be protected are detected and a current voice pattern is generated from at least one sound detected in the space to be protected. The current voice pattern is compared with entries in the stored voice lists to determine a level of difference or a level of similarity between the current voice pattern and the content of the voice list. If the determined level constitutes a discrepancy according to predetermined criteria, an alarm response is initiated. Further, if the determined level constitutes a matching between the current voice pattern and an entry in an emergency list of voice codes that were entered by authorized users, a predetermined emergency action is performed. The advantages of the voice recognition as a security measure may be also integrated into existing security systems."><meta name="DC.date" content="2001-7-17" scheme="issued"><meta name="DC.relation" content="US:4590604" scheme="references"><meta name="DC.relation" content="US:4998279" scheme="references"><meta name="citation_patent_number" content="US:6263311"><meta name="citation_patent_application_number" content="US:09/228,475"><link rel="canonical" href="http://www.google.com/patents/US6263311"/><meta property="og:url" content="http://www.google.com/patents/US6263311"/><meta name="title" content="Patent US6263311 - Method and system for providing security using voice recognition"/><meta name="description" content="Method and system for providing security to a space by detecting an unauthorized presence in the space using voice recognition. A plurality of acceptable voice patterns is generated from sounds of authorized users and stored in the security system. Sounds in the space to be protected are detected and a current voice pattern is generated from at least one sound detected in the space to be protected. The current voice pattern is compared with entries in the stored voice lists to determine a level of difference or a level of similarity between the current voice pattern and the content of the voice list. If the determined level constitutes a discrepancy according to predetermined criteria, an alarm response is initiated. Further, if the determined level constitutes a matching between the current voice pattern and an entry in an emergency list of voice codes that were entered by authorized users, a predetermined emergency action is performed. The advantages of the voice recognition as a security measure may be also integrated into existing security systems."/><meta property="og:title" content="Patent US6263311 - Method and system for providing security using voice recognition"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("ZkXsU6DQA6jisATktoHYCg"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("GBR"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("ZkXsU6DQA6jisATktoHYCg"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("GBR"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6263311?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6263311"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=SKFVBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6263311&amp;usg=AFQjCNFCMZBENw_RoM0XbrlLS1pjHrqq3Q" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6263311.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6263311.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6263311" style="display:none"><span itemprop="description">Method and system for providing security to a space by detecting an unauthorized presence in the space using voice recognition. A plurality of acceptable voice patterns is generated from sounds of authorized users and stored in the security system. Sounds in the space to be protected are detected and...</span><span itemprop="url">http://www.google.com/patents/US6263311?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6263311 - Method and system for providing security using voice recognition</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6263311 - Method and system for providing security using voice recognition" title="Patent US6263311 - Method and system for providing security using voice recognition"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6263311 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/228,475</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Jul 17, 2001</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Jan 11, 1999</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Jan 11, 1999</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">09228475, </span><span class="patent-bibdata-value">228475, </span><span class="patent-bibdata-value">US 6263311 B1, </span><span class="patent-bibdata-value">US 6263311B1, </span><span class="patent-bibdata-value">US-B1-6263311, </span><span class="patent-bibdata-value">US6263311 B1, </span><span class="patent-bibdata-value">US6263311B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Robert+Dildy%22">Robert Dildy</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Advanced+Micro+Devices,+Inc.%22">Advanced Micro Devices, Inc.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6263311.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6263311.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6263311.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (2),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (27),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (6),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (14)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=SKFVBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6263311&usg=AFQjCNFj0cH2gLfwPTN5ttTHQPMP4IVN8A">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=SKFVBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6263311&usg=AFQjCNFDlAMSES7rUCw8JlW7nhjugWVjGQ">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=SKFVBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6263311B1%26KC%3DB1%26FT%3DD&usg=AFQjCNG9lO41HQ6x1RZsTMMxTPIVzYck6w">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54787756" lang="EN" load-source="patent-office">Method and system for providing security using voice recognition</invention-title></span><br><span class="patent-number">US 6263311 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA72586035" lang="EN" load-source="patent-office"> <div class="abstract">Method and system for providing security to a space by detecting an unauthorized presence in the space using voice recognition. A plurality of acceptable voice patterns is generated from sounds of authorized users and stored in the security system. Sounds in the space to be protected are detected and a current voice pattern is generated from at least one sound detected in the space to be protected. The current voice pattern is compared with entries in the stored voice lists to determine a level of difference or a level of similarity between the current voice pattern and the content of the voice list. If the determined level constitutes a discrepancy according to predetermined criteria, an alarm response is initiated. Further, if the determined level constitutes a matching between the current voice pattern and an entry in an emergency list of voice codes that were entered by authorized users, a predetermined emergency action is performed. The advantages of the voice recognition as a security measure may be also integrated into existing security systems.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(6)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6263311B1/US06263311-20010717-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6263311B1/US06263311-20010717-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6263311B1/US06263311-20010717-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6263311B1/US06263311-20010717-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6263311B1/US06263311-20010717-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6263311B1/US06263311-20010717-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6263311B1/US06263311-20010717-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6263311B1/US06263311-20010717-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6263311B1/US06263311-20010717-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6263311B1/US06263311-20010717-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6263311B1/US06263311-20010717-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6263311B1/US06263311-20010717-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(16)</span></span></div><div class="patent-text"><div mxw-id="PCLM28671144" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is: </claim-statement> <div class="claim"> <div num="1" id="US-6263311-B1-CLM-00001" class="claim">
      <div class="claim-text">1. A method for determining an unauthorized presence in a space to be protected comprising:</div>
      <div class="claim-text">storing a voice list, wherein said voice list includes a plurality of voice patterns; </div>
      <div class="claim-text">detecting a sound in said space, wherein said sound is indicative of a presence of at least one source of said sound in said space; </div>
      <div class="claim-text">generating a current voice pattern from said sound; </div>
      <div class="claim-text">comparing said current voice pattern with at least one of said plurality of voice patterns in said voice list to determined if said presence in said space is said unauthorized presence; and </div>
      <div class="claim-text">initiating an alarm response if said presence in said space is said unauthorized presences; </div>
      <div class="claim-text">wherein said plurality of voice patterns are generated from sounds of one or more authorized users and sounds from one or more non-human sources. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" id="US-6263311-B1-CLM-00002" class="claim">
      <div class="claim-text">2. The method as recited in claim <b>1</b> wherein said generating said current voice pattern from said sound includes using voice recognition to analyze said sound.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" id="US-6263311-B1-CLM-00003" class="claim">
      <div class="claim-text">3. The method as recited in claim <b>2</b> wherein said comparing includes using said voice recognition to determine a first level of discrepancy between said current voice pattern and a content of said voice list, wherein said first level of discrepancy is determined based on first predetermined criteria.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" id="US-6263311-B1-CLM-00004" class="claim">
      <div class="claim-text">4. The method as recited in claim <b>3</b> wherein said first level of discrepancy is used to determined if said presence in said space is said unauthorized presence based on said first predetermined criteria.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" id="US-6263311-B1-CLM-00005" class="claim">
      <div class="claim-text">5. The method as recited in claim <b>1</b> wherein said alarm response includes at least one action of a plurality of actions, wherein said action includes activating at least one device from a plurality of devices, and wherein if at least two actions of said plurality of actions are executed, said at least two actions are executed in a first sequence from a plurality of sequences based on second predetermined criteria.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" id="US-6263311-B1-CLM-00006" class="claim">
      <div class="claim-text">6. The method as recited in claim <b>5</b>, further comprising storing an emergency voice code list, wherein said emergency voice code list includes a plurality of voice codes, wherein said plurality of voice codes are generated by said sounds of said one or more authorized users and sounds from said one or more non-human sources, wherein if said current voice pattern corresponds to one of said plurality of voice codes in said emergency voice code list, at least one predetermined action of said plurality of actions is initiated.</div>
    </div>
    </div> <div class="claim"> <div num="7" id="US-6263311-B1-CLM-00007" class="claim">
      <div class="claim-text">7. A system for providing security to a space by detecting an unauthorized presence in said space, the system comprising:</div>
      <div class="claim-text">a first sensor configured to detect a sound in said space, wherein said sound is indicative of at least one source of said sound in said space, and wherein said source is indicative of a presence in said space; </div>
      <div class="claim-text">a voice-processing unit coupled to said first sensor and configured to store a voice list, wherein said voice list includes a plurality of voice patterns, analyze said sound in said space, generate a current voice pattern from said sound detected by said first sensor, compare said current voice pattern with at least one of said plurality of voice patterns in said voice list to determined if said presence in said space is said unauthorized presence; and </div>
      <div class="claim-text">a control unit coupled to said voice-processing unit and said first sensor and configured to initiate an alarm response if said presence in said space is said unauthorized presence; </div>
      <div class="claim-text">wherein said plurality of voice patterns are generated from sounds of one or more authorized users and sounds from one or more non-human sources. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" id="US-6263311-B1-CLM-00008" class="claim">
      <div class="claim-text">8. The system as recited in claim <b>7</b> further comprises a first control device from a plurality of devices coupled to said control unit, wherein said first control device is used to perform a first control function.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" id="US-6263311-B1-CLM-00009" class="claim">
      <div class="claim-text">9. The system as recited in claim <b>8</b> wherein said alarm response initiated by said control unit includes at least one action of a plurality of actions, wherein said action includes activating said at least one of said plurality of devices.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" id="US-6263311-B1-CLM-00010" class="claim">
      <div class="claim-text">10. The system as recited in claim <b>9</b> wherein said control unit is further configured to execute at least two actions of said plurality of actions in a first sequence from a plurality of sequences based on second predetermined criteria.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" id="US-6263311-B1-CLM-00011" class="claim">
      <div class="claim-text">11. The system as recited in claim <b>9</b> wherein said voice-processing unit is further configured to store an emergency voice code list, wherein said emergency voice code list includes a plurality of voice codes, wherein said plurality of voice codes are generated by said sounds of said one or more authorized users and sounds from said one or more non-human sources.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" id="US-6263311-B1-CLM-00012" class="claim">
      <div class="claim-text">12. The system as recited in claim <b>11</b> wherein if said voice-processing unit determines that said current voice pattern corresponds to one of said plurality of voice codes in said emergency voice code list, said control unit initiates at least one predetermined action from said plurality of actions.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" id="US-6263311-B1-CLM-00013" class="claim">
      <div class="claim-text">13. The system as recited in claim <b>7</b> wherein said voice-processing unit generates said plurality of voice patterns by recording said sounds of said one or more authorized users and sounds from said one or more non-human sources.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" id="US-6263311-B1-CLM-00014" class="claim">
      <div class="claim-text">14. The system as recited in claim <b>7</b> wherein said voice-processing unit uses voice recognition to recognize said sound, generate said current voice pattern from said sound, and compare said current voice pattern with a content of said voice list.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" id="US-6263311-B1-CLM-00015" class="claim">
      <div class="claim-text">15. The system as recited in claim <b>14</b> wherein said voice-processing unit comparing said current voice pattern with at least one of said plurality of voice patterns in said voice list to determine if said presence in said space is said unauthorized presence includes using a first level of discrepancy between said current voice pattern and said content of said voice list, wherein said first level of discrepancy is based on first predetermined criteria, and wherein said first level of discrepancy is used to determine if said presence in said space is said unauthorized presence based on said first predetermined criteria.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" id="US-6263311-B1-CLM-00016" class="claim">
      <div class="claim-text">16. The system as recited in claim <b>7</b>, further comprising a second sensor configured to detect a parameter other than said sound, wherein said second sensor is coupled to said control unit, wherein said control unit is further configured to use an output of said second sensor to initiate a control action based on third predetermined criteria.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES54602745" lang="EN" load-source="patent-office" class="description">
    <heading>BACKGROUND OF THE INVENTION</heading> <p>1. Field of the Invention</p>
    <p>The present invention relates generally to security systems and more particularly to security systems with voice recognition functions.</p>
    <p>2. Description of the Relevant Art</p>
    <p>The use of security systems to protect people and buildings has seen an increasing demand. The basic idea of a security system is sensing a predetermined parameter and delivering a response upon changes in the value of that parameter. The parameter(s) detected by the security system usually indicates the existence of an unacceptable condition, such as an open door which should be closed, the presence of an intruder, or a condition of an emergency nature (such as fire or flood). Security systems employ many types of sensors such as motion sensors for both the indoors and outdoors, glass-break detectors, magnetic contact sensors, flood detectors, temperature sensors, smoke detectors, sensitized door mats, etc. The use of specific sensors and security systems vary from person to person and from building to building based on the security needs and concerns.</p>
    <p>Security systems have continuously become more complex as the employment of different technologies in security systems has substantially increased to combat security challenges. Many of today's security systems employ some software programming and hardware adjustments to provide control functions and automation, such as operating lights, appliances, thermostats, motorized drapes, phones, and other devices. A security system may be configured to send an emergency signal to a specific party, such as dialing a police or a physician upon detection of an emergency condition. The emergency condition is usually programmed or pre-entered in the security system. The initiation of an action (when emergency conditions exist) is either prompted by the user or automatically started by the security system.</p>
    <p>To increase the functionality of security systems, so that they meet changing security needs, modem security systems may employ existing or dedicated phone lines and phone systems to deliver extended functionality. For example, a security system may be configured to initiate an intercom phone call upon detecting knocking on the door, whereby the user is able to converse with visitors before opening the door. Further, a security system's owner may call a security system from a remote location using a phone to change security setup or conditions. Although the owner accesses the security system by entering a password or a form of access information, an unauthorized user may be able to disarm the system or change the existing security criteria if he knows the access information, thus straying the value of the security system.</p>
    <p>Many security systems utilize a telephone control module that converts touch-tone tones into control commands. Any command that is enacted by the security system may now be performed from the keypad of a touch-tone phone. For example, if a user forgets to arm the security system while leaving his house, the arming of the system may be performed by a few keys on the car phone. Therefore, developments in telephony and other technologies contribute to the sophistication of security systems, for example, using computer links, through serial port connections, to provide advantages of a processor or building security systems with their own processors.</p>
    <p>As mentioned above, the integration between the security system and other technologies of home and office automation provides an extended functionality and complexity to security systems. Unfortunately, security systems are still vulnerable to unauthorized individuals. An unauthorized individual may reach the protected areas without detection by the security system. For example, an owner may admit an unauthorized individual into the protected area before realizing the mistake. An individual may reach the protected area by passing sensors without detection. An unauthorized individual may breach the security system with stolen access information. A security need may arise without prior identification of an unauthorized individual. Further, a security need may arise independent of the existence of an unauthorized individual. Therefore, reducing or eliminating the security systems vulnerability is desirable. Further, increasing the capability of the security systems to satisfy even better security needs is also desirable.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>The present invention comprises method and system for detecting an unauthorized presence in a space (such as a room) to be protected, or subjected to security measures. The protection is provided by detecting the voice or sound of an intruder, or an unauthorized user, in the space that is subjected to the security measures. Further, the method and system provide additional security measures to the space to be protected by providing recognition to predetermined voice codes provided by authorized users. A voice list including a plurality of authorized voice patterns is generated and stored. When the system is activated, sensors in the space to be protected are employed to detect sounds in the space. The detected sound is indicative of a sound source in the protected space and the sound source is indicative of a presence (authorized or unauthorized) in the space to be protected. The detected sound is used to generate a current voice pattern indicative of the characteristics of the sound source. The generated current voice pattern is compared against the plurality of authorized voice patterns that were stored in the voice list. If the current voice pattern matches an entry in the voice list (based on predetermined criteria), then the detected sound indicates an authorize presence (such as a homeowner), otherwise the detected sound indicates an unauthorized presence (such as an intruder). If an unauthorized presence is detected, in the space to be protected, the system is configured to initiate an alarm response.</p>
    <p>The method and system further comprise initiating an alarm response if the current voice pattern matches a special voice pattern (or voice code) of an authorized user in the space to be protected. Voice codes may be stored along with voice patterns in the voice list or may be stored in a separate emergency voice list that contains only voice codes. Upon a determination of a match between a generated voice pattern and a voice code, a predetermined special action is initiated by the system. Thus, an alarm response may be triggered based on the detection of a voice code (that is indicative of an emergency need) by an authorized user requesting an action (such as dialing 911) to be performed by the security system.</p>
    <p>Broadly speaking, the present invention contemplates a method and system that advantageously improve the security of a space, to be protected, by providing detection and recognition to sounds generated in that space. The sound is detected by a sound sensor and processed in a voice recognition unit to generate a current voice pattern. The current voice pattern is compared with a plurality of authorized voice patterns that are stored in a voice list in the system. The authorized voice patterns reference the presence of authorized sounds in the space, such as those generated by authorized individuals. If the detected sound, upon processing in the voice-processing unit, indicates a presence of an unauthorized individual or an emergency request by an authorized user, the system is configured to initiate an alarm response that advantageously provides better security measures to the protected space and its users.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p>Other objects and advantages of the present invention may become apparent upon reading the following detailed description and upon reference to the accompanying drawings in which:</p>
    <p>FIG. 1 is a block diagram of a security system that provides voice recognition functions and improves security by responding and recognizing sounds in the space to be protected;</p>
    <p>FIG. 1A is a block diagram of a security system that provides voice recognition and improves security by responding to sounds including voice codes by authorized users in the space to be protected;</p>
    <p>FIG. 2 is a flow chart of a method to provide security to a space using voice recognition;</p>
    <p>FIG. 2A is a flow chart of a method to provide security to a space using voice recognition including voice codes; and</p>
    <p>FIG. 3 is a block diagram of a security system incorporating a voice recognition system in addition to other security measures.</p>
    <p>While the invention is susceptible to various modifications and alternative forms, specific embodiments thereof are shown by way of example in the drawings and will herein be described in detail. It should be understood, however, that the drawings and detailed description thereto are not intended to limit the invention to the particular form disclosed, but on the contrary, the intention is to cover all modifications, equivalents and alternatives falling within the spirit and scope of the present invention as defined by the appended claims.</p>
    <heading>DETAILED DESCRIPTION OF THE DRAWINGS</heading> <p>Turning now to FIG. 1, a block diagram of a security system is shown that provides voice recognition and improves security by detecting an unauthorized presence in a space to be protected. In one embodiment, the security system <b>100</b> utilizes an X number of sound sensors <b>110</b>A, <b>111</b>B, <b>110</b>C, . . . , <b>100</b>X, to detect sounds generated in the space to be protected. The number of sensors used in system <b>100</b> depends on the size and type of the space to be protected, the security needs of the space, and the spectrum of sound frequencies to be covered in the security design of the space. In this description, the reference to a group of elements which share the same numeric (such as <b>110</b>A, <b>111</b>B, <b>110</b>C, . . . , <b>110</b>X) may be collectively indicated by referring to the numeric alone. For example, the numeric <b>110</b> is used to collectively reference sound sensors <b>111</b>A, <b>110</b>B, <b>110</b>C, . . . , <b>110</b>X. Sound sensors <b>110</b> may be transducers (or transmitters) that generate voltage (or current) signals in response to changes in acoustic waves received by the sensors. In one embodiment, sensors <b>110</b> are of the same type. They may be distributed in different locations in the space to be protected based on the space's security needs. In another embodiments, sensors <b>110</b> are of different types to provide a wide spectrum of sound detection capabilities appropriate to the space's security needs. Preferably; the number, type, and distribution of sensors <b>110</b> are selected to satisfy the security needs of the space to be protected.</p>
    <p>In one embodiment, sensors <b>110</b> are coupled to a voice-processing unit <b>120</b>. Signals generated by sensors <b>110</b> are transmitted to unit <b>120</b> for processing. The voice-processing unit <b>120</b> comprises a voice recognition unit <b>140</b> and a voice list <b>130</b>. The voice recognition unit <b>140</b> may be any voice recognition unit that is suitable to the security needs of the space to be protected. The voice recognition unit <b>140</b> preferably comprises a processor, such as CPV or DSP, which executes voice recognition software from a memory. The voice list preferably comprises a random access memory. In one embodiment, the voice list <b>130</b> is included within the memory of the voice recognition unit <b>140</b>. Alternatively, the voice list <b>130</b> may be included within the voice-processing unit <b>120</b> as an external element to the voice recognition unit <b>140</b>. In the embodiment of FIG. 1, the voice list <b>130</b> is configured to store a plurality of voice patterns. Generally speaking, the voice patterns stored in the voice list <b>130</b> are acceptable or authorized voice patterns of recognized users. The acceptable voice patterns are those patterns of authorized individuals whom when they are present in the space to be protected do not constitute a security concern. For example, the voice list <b>130</b> stores sounds of an owner of the security system, sounds of other family members in the home, and may be sounds of other individuals who may be normally visiting the house such friends, cleaning personnel, and babysitters. The voice list <b>130</b> may further include non-human sounds such as pets in the house.</p>
    <p>In one embodiment, the voice recognition unit <b>140</b> is employed to generate acceptable voice patterns from normal conversations of authorized users. A recording may be initiated, by an authorized user, to record sounds of authorized individuals during their conversations. The recorded conversations may be used to create voice patterns for the authorized individuals that are stored in the voice list <b>130</b>. Alternatively, voice patterns may be generated by recording specific words and phrases pronounced by each of the authorized individuals. The generated voice patterns are stored in the voice list <b>130</b>. Generally speaking, each voice pattern includes at least one word pronounced by an authorized user. The user is any human or non-human source of sound. Since the voice list <b>130</b> is developed from acceptable voice patterns, the availability of a sound, generated by a source in the space to be protected, that matches (based on predetermined criteria) at least one entry in the voice list <b>130</b> may be considered acceptable.</p>
    <p>In one embodiment, the voice-processing unit <b>120</b> is configured to analyze at least one sound detected by one of the sound sensors <b>120</b> in the space to be protected. Analysis of the detected sound includes generating a current voice pattern. The current voice pattern is generated when one or more of the sensors <b>110</b> transmit at least one signal to the voice-processing unit <b>120</b>. Therefore, the current voice pattern is indicative of the sound source in the space to be protected. Upon the generation of the current voice pattern, the voice-processing unit <b>120</b> compares the current voice pattern with the voice patterns stored in the voice list <b>130</b>. The result of the comparison between the current voice pattern and the content of the voice list <b>130</b> indicates if the current voice pattern has correspondence (for example, similarity) with at least one of the voice patterns that were stored in the voice list <b>130</b>. Accordingly, if the result of the above comparison indicates a current voice pattern that is not within an acceptable range of correspondence (according to predetermined criteria), an unacceptable voice pattern is detected.</p>
    <p>Generally speaking, the detection of the unacceptable voice pattern by the voice-processing unit <b>120</b> indicates the existence of an unauthorized presence in the space to be protected. In one embodiment, the voice recognition unit <b>140</b> performs the generation and analysis of the current voice patterns. It is noted that many levels of voice-processing sophistication are known in the art. Any level of sophistication that is satisfactory to the needs of the security system may be implemented by the voice-processing unit <b>120</b>. Therefore, the level of sophistication for the voice-processing functions of system <b>100</b> is dependent on the depth of the security needs in the space to be protected. The description of a specific sophistication is not intended to restrict the present invention, but rather it is presented as an example. Accordingly, any level of sophistication of voice-processing for system <b>100</b> is contemplated in the present invention.</p>
    <p>In the embodiment of system <b>100</b>, the voice-processing unit <b>120</b> is configured to assert at least one signal to control unit <b>150</b> when a discrepancy, between the current voice pattern and the content of the voice list <b>130</b>, is detected. The discrepancy between the current voice pattern and the content of the voice list <b>130</b> may be obtained when the current voice pattern matches no entry in the voice list <b>130</b>. Alternatively, a discrepancy may be also obtained based on a partial matching (instead of no matching) between the current voice pattern and the content of the voice list <b>130</b>. The decision on the size of difference between the current voice pattern and the content of the voice list <b>130</b> that would constitute a discrepancy, between the current voice pattern and the content in the voice list <b>130</b>, is dependent on the voice-processing unit <b>120</b> sophistication and the depth of the security needs in the space to be protected. Furthermore, two or more levels of sophistication may be utilized by the voice-processing unit <b>120</b>. In the latter, the level of sophistication to be used in analyzing the detected sounds by sensors <b>110</b> is at least dependent on the nature of sound detected by one or more of the sensors <b>1</b> <b>10</b>.</p>
    <p>A first level of sophistication for the voice-processing functions of system <b>100</b> may be employed to arrive into a discrepancy between a current voice pattern and the content of the voice list <b>130</b> based on a total mismatch. A second level of sophistication may be employed by the voice-processing unit <b>120</b> to arrive into a discrepancy based on a partial mismatch. For example, when a sound form an unauthorized individual is detected by the voice-processing unit <b>120</b>, a discrepancy is obtained. As mentioned above, the decision when a discrepancy occur depends on the level of sophistication chosen for the voice-processing functions of the voice-processing unit <b>120</b>, and the depth of the security measures needed in the space to be protected.</p>
    <p>Control unit <b>150</b> is configured to control signal at least one of the control devices <b>160</b>A, <b>160</b>B, . . . , <b>160</b>X, when a discrepancy (between the current voice pattern and the content of the voice list <b>130</b>) is obtained by the voice-processing unit <b>120</b>. The number of control devices <b>160</b> may vary depending on the action (or the alarm response) that the system <b>100</b> is designed for. Further, a control device <b>160</b> may be any device adapted to perform a function in response to a control signal received from the control unit <b>150</b>. For example, a control device <b>160</b> may be an audio alarm, a visual alarm, an electrical door locker/opener, a power controller, a light controller, a phone system, a module to dial a number through the phone system, a display device . . . etc.</p>
    <p>In one embodiment, the control unit <b>150</b> is configured to perform a sequence of actions upon one or more of the assertion signals received from the voice-processing unit <b>120</b>. In another embodiment, the sequence of actions performed by the control unit <b>150</b> is dependent on a second group of assertion signals supplied by the voice-processing unit <b>120</b>. In the latter, a second control action may be performed based on a second discrepancy determined by the voice-processing unit <b>120</b> after determining a first discrepancy. Further, a second control action may be performed based on the nature of the first control action. It will be appreciated by those skilled in the art that many combinations of the control devices <b>160</b> and configurations of the control unit <b>150</b> are possible. The above description is given as an example and is not intended to restrict the scope of the present innovation.</p>
    <p>Turning now to FIG. 1A, a block diagram of a security system that provides voice recognition and improves security by responding to a sound including an emergency voice list is shown. The security system <b>100</b>A is similar to the embodiment of system <b>100</b>, however, it includes an emergency voice list <b>135</b>. The emergency voice list <b>135</b> is generated in a manner similar to that of the voice list <b>130</b> as described above. In this embodiment, the additional voice list <b>135</b> contains special voice codes (emergency voice patterns) of authorized users. The voice codes are specific patterns of voice that are entered by authorized users. The security system is configured to perform a specific alarm response when one or more of the voice codes are detected. Accordingly, when a current voice pattern is generated, the voice-processing unit <b>120</b> compares the current voice pattern with both contents of the voice list <b>130</b> and the emergency voice list <b>135</b>. If the current voice pattern matches an entry in the emergency voice list <b>135</b>, a pre-determined alarm response is initiated. Therefore, the voice-processing unit <b>120</b> compares the current voice pattern with the content of the emergency voice list <b>135</b> looking for a matching rather than a discrepancy as in the case of the voice list <b>130</b>.</p>
    <p>As mentioned above, the voice codes (in the emergency voice list <b>135</b>) may be used to signal the security system to perform a special function by announcing one or more of the voice codes. The users of the security system already know the response of the security system when one of the voice codes is announced. For example, a code take me to haven may be a voice code adapted to signal system <b>100</b>A to initiate a phone call to the police. The advantage of incorporating the emergency voice list <b>135</b> is the ability to trigger the security system to perform a security function upon announcing one of the voice codes by an authorized individual. The latter case is specifically valuable when a circumstance arisen such that the authorized user is unable to perform a security function (for example, dialing 911) except by announcing a voice code. For example, an owner may initiate a 911 call while he is in company with other individuals without their knowledge or he may initiate the call while he is handicapped from reaching the phone due to an illness or accident. It should be noted that the level of full matching or partial matching required in performing the comparison between the current voice pattern and the content of the emergency voice list <b>135</b> is dependent on the level(s) of sophistication employed by the voice-processing unit <b>120</b> and on the depth of the security measures needed in the space to be protected.</p>
    <p>Turning now to FIG. 2, a flow chart of a method to provide security to a space upon detection of a sound (using voice recognition) in the space under security is shown. In the embodiment of FIG. 2, storing of a voice list is performed in step <b>210</b>. The voice list includes a plurality of voice patterns. Generally speaking, the voice patterns are acceptable voice patterns. The acceptable voice patterns are those which do not constitute a concern from a security standpoint. For example, the voice list that is stored in step <b>210</b> may include a sound of an owner of a house, sounds of other family members in the home, and sounds of other individuals who may be normally visiting the house such as friends, cleaning personals, and babysitters. Other sounds that may also be stored in the voice list are non-human sounds such as those of pets in the space to be protected.</p>
    <p>In step <b>220</b>, an existence of a sound in the space to be protected in detected. Detection of the sound may be achieved by any sound sensor or detector that is satisfactory to the security needs in the space to be protected. Upon detecting a sound (step <b>220</b>), a current voice pattern is generated in step <b>230</b>. The voice pattern may be a single sound or a combination of sounds. The current voice pattern generated in step <b>230</b> is analyzed with respect to the content of the voice list stored in step <b>210</b>. In step <b>240</b> a comparison between the current voice pattern and the content of the voice list is performed. The analysis of the current voice pattern with the content of the voice list may be achieved by using one or more voice recognition techniques.</p>
    <p>In step <b>250</b> a determination of a discrepancy between the current voice pattern and the content of the voice list is performed. If the result of step <b>250</b> indicates a discrepancy, one or more alarm responses are initiated in step <b>260</b>. In one embodiment, the discrepancy that is determined in step <b>250</b> (between the current voice pattern generated in step <b>230</b> and the content of the voice list stored in step <b>210</b>) is obtained when the current voice pattern matches no entry in the voice list. In another embodiment, the discrepancy may be determined based on a partial matching between the current voice pattern and the content of the voice list using predetermined criteria. The decision to be made in step <b>250</b> is generally based on the size of the difference between the current voice pattern and the content of the voice list. The size of the difference may be used to indicate an existence of a discrepancy between the current voice pattern and the content in the voice list stored in step <b>210</b>. It should be noted that the determination of a discrepancy that leads to initiating an alarm response in step <b>260</b> is dependent on the predetermined criteria. The latter is also dependent on the level of sophistication employed by the security system and the security needs in the space to be protected. It should be also noted that any level of sophistication is contemplated by the present invention. Further, more than one level of sophistication may be utilized in at least step <b>240</b>.</p>
    <p>In one embodiment, the initiation of an alarm response in step <b>260</b> may include a number of actions designed to provide security measures. For example, the initiation of an alarm response in step <b>260</b> may include two or more security actions. Any of the actions may be initiated based on a level of discrepancy that is determined by step <b>250</b>. Further, a sequence of alarm responses may be initiated in step <b>260</b> upon one or more discrepancy levels that are determined in step <b>250</b>. It should be noted that the number of alarm responses is dependent on the security measures needed in the space to be protected. Further, the alarm response initiated in step <b>260</b> may also depend on both the sophistication of the comparison performed in step <b>240</b> and the security needs of the space to be protected. Generally speaking, an initiation of an alarm response in step <b>260</b> may result in the activation of one or more devices; such as audio alarms, visual alarms, electrical door lockers/openers, electric power controllers, light controllers, phone systems, controllers to dial phone numbers, display devices . . . etc.</p>
    <p>Turning now to FIG. 2A, a flow chart of a method to provide security to a space using voice recognition including an emergency voice list is shown. In the embodiment of FIG. 2A, an emergency voice list and a voice list are stored in step <b>215</b>. Steps <b>220</b>, <b>230</b>, and <b>260</b> of FIG. 2A are the same as in FIG. 2 described earlier. The emergency voice list comprises of voice codes of authorized users of the space to be protected. The voice codes in the emergency voice list may be used to signal the security system to perform a special function. The users of the security system already know the response of the security system when one of the voice codes is announced. The advantage of incorporating the emergency voice list is the ability to trigger the security system to perform an emergency function (such as calling 911) by an authorized user, while the ability of the authorized user to initiate such action is restricted otherwise.</p>
    <p>In step <b>245</b> a comparison between the current voice pattern and the contents of the voice list and the emergency voice list is performed. The analysis of the current voice pattern with the contents of the voice lists may be achieved by using one or more voice recognition techniques. In step <b>255</b> a determination of a discrepancy between the current voice pattern and the content of the voice list, or a matching between the current voice pattern and the content of the emergency voice list is performed. If a discrepancy (or a matching) exists according to predetermined criteria, one or more alarm responses are initiated in step <b>260</b>. In one embodiment, a discrepancy is obtained when the current voice pattern matches no entry in the voice list. Further, a matching is obtained when the current voice pattern matches an entry in the emergency voice list. In another embodiment, the discrepancy may be determined in step <b>255</b> from a partial matching between the current voice pattern and the content of the voice list. Likewise, the matching may be determined in step <b>255</b> from a partial discrepancy between the current voice pattern and the content of the emergency voice list.</p>
    <p>The decision to be made in step <b>255</b> is generally based on the size of either the difference or the similarity between the current voice pattern and the contents of the voice list and the emergency voice list, respectively. The size of the difference may be used to indicate an existence of a discrepancy between the current voice pattern and the content in the voice. Likewise, the size of the similarity may be used to indicate an existence of a matching between the current voice pattern and the content in the emergency voice list. It should be noted that the determination of a discrepancy, or a matching, that leads to initiating an alarm response in step <b>260</b> is depended on the level(s) of sophistication of voice-processing functions and the security needs in the space to be protected. It should be also noted that any level of sophistication is contemplated by the present invention. Further, more than one level of sophistication may be utilized in analyzing the current voice pattern and voice patterns stored in both the voice list and the emergency list at least in step <b>245</b>.</p>
    <p>In one embodiment, the initiation of an alarm response in step <b>260</b> may include a number of actions designed to provide security needs. One or more of the action levels may be initiated in step <b>260</b> based on a level of discrepancy or matching that was determined by step <b>255</b>. Further, a sequence of alarm responses may be initiated in step <b>260</b> upon one or more discrepancy or matching levels determined by step <b>255</b>. It should be noted that the number of alarm responses is dependent on the security measures needed in the space to be protected or on the voice codes. Further, the alarm response initiated in step <b>260</b> may also depend on both the sophistication of the comparison performed in step <b>245</b> and the security needs of the space to be protected.</p>
    <p>Turning now to FIG. 3, a block diagram of a security system <b>300</b> incorporating a voice recognition system is shown. In one embodiment, the security system <b>300</b> includes a voice-processing unit <b>320</b>, at least one sensor <b>310</b>, a control unit <b>350</b>, and at least one control device <b>360</b>. Generally speaking, the voice recognition functions of system <b>300</b> and the alarm responses are similar to those of system <b>100</b> or <b>100</b>A described earlier. Thus, the advantages of system <b>100</b> or <b>100</b>A in relation to voice recognition may be readily integrated to traditional security systems.</p>
    <p>In the embodiment of system <b>300</b>, a plurality of sensors <b>310</b> is configured to detect at least two parameters, wherein one of the parameters is sound. The other parameters may be smoke, temperature, water, electromagnetic field, light, stress . . . etc. The number of sensors <b>310</b> may vary depending on the size of the space to be protected, the depth of the security needs, the type of the space, and the spectrum of sound frequencies to be covered in the security design of the space. A sensor <b>310</b> may be any type transducers or transmitters that are used to generate an electrical signal in response to changes in the parameter being monitored. In one embodiment, sensors <b>310</b> may be placed in different locations in the space to be protected. However, one or more of the sensors may also be placed outside the space to be protected and used to provide a reference signal.</p>
    <p>In one embodiment, at least one sound sensor <b>310</b>A is coupled to the voice-processing unit <b>320</b>. The signal generated by the sound sensor <b>310</b>A is delivered to unit <b>320</b> for processing. It should be noted that a copy signal from the sensor <b>310</b>A may be provided to control unit <b>350</b> as well. In another embodiment, the sound sensor signal is provided to the voice-processing unit <b>320</b> through the control unit <b>350</b>. The voice-processing unit <b>320</b> comprises a voice recognition unit <b>340</b>, a voice list <b>330</b>, and an emergency voice list <b>335</b>. The voice recognition unit <b>340</b> may be any suitable voice recognition unit known to those skilled in the art. In one embodiment, the voice list <b>330</b> or the emergency voice list <b>335</b> is included within the voice recognition unit <b>340</b>. Alternatively, the voice list <b>330</b> or the emergency voice list <b>335</b> may be included within the voice-processing unit <b>320</b>, as an external element to the voice recognition unit <b>340</b>. In one embodiment, system <b>300</b> includes either the voice list <b>330</b> or the emergency voice list <b>350</b>. In the embodiment shown in FIG. 3, voice list <b>330</b> is adapted to store a plurality of voice patterns and the emergency voice list is adapted to store a plurality of special coded voice patterns or voice codes. The content of either the voice list <b>330</b> or the emergency voice list <b>335</b> includes at least one word pronounced by at least one user. The user is any human or non-human source of sound, in which the availability of the sound or the type of the sound generated by that source in the space to be protected indicate the presence of the sound in the space.</p>
    <p>In one embodiment, the voice-processing unit <b>320</b> is configured to analyze at least one sound detected in the space to be protected. Analysis of the sound may include generating a current voice pattern. Upon the generation of the current voice, the voice-processing unit <b>320</b> compares the current voice pattern with the contents of the voice list and the emergency voice list. The result of the comparison between the current voice pattern and the content of the voice list <b>330</b> identifies a level of discrepancy, while between the current voice pattern and the content of the emergency voice list <b>335</b> identifies a level of matching. The level of discrepancy indicates the existence of an unauthorized presence in the space to be protected. The level of matching indicates the need to perform a specific security measure. The level of discrepancy or the level of matching that is determined by the voice-processing unit <b>320</b> is dependent on a size of the difference between the current voice pattern and the content of the voice list <b>330</b>, or the size of the similarly between the current voice pattern and the content of the emergency voice list <b>335</b>. It should be noted that several levels of sophistication for the voice recognition and processing functions of system <b>300</b> may be employed to suite different security needs.</p>
    <p>In the embodiment of FIG. 3, the control unit <b>350</b> of system <b>300</b> is configured to receive signals from the sensors <b>310</b> and the voice-processing unit <b>320</b>, and to supply at least one control signal to at least one of the control devices <b>360</b>. The control unit <b>350</b> uses the received signal(s) (representing sound and other parameters) to generate a control action according to predetermined criteria. Thus, system <b>300</b> integrates functions provided by traditional security systems and the voice processing functions of the present invention. The number of control devices <b>360</b> may vary depending on the action measures that the system <b>300</b> is designed for. Further, a control device may be any device adapted to perform a function in response to a control signal received from the control unit <b>350</b>. For example, a control device <b>360</b> may be an audio alarm, a visual alarm, an electrical door locker/opener, an electricity controller, a light controller, a phone system, a controller to dial a number through the phone system, a display device . . . etc. In one embodiment, the control unit <b>350</b> is configured to perform a sequence of actions upon one or more signals received from the voice-processing unit <b>320</b> and the sensors <b>310</b>. In another embodiment, the sequence of actions performed by the control unit <b>350</b> depends on signals supplied by the voice-processing unit <b>320</b> and at least one of the sensors <b>360</b>. It will be appreciated to those skilled in the art having the advantages of this disclosure that many combinations of the control devices <b>360</b> and configurations of the control unit <b>350</b> are possible.</p>
    <p>While the present invention has been described with reference to particular embodiments, it will be understood that the embodiments are illustrative and that the invention scope is not so limited. Any variations, modifications, additions and improvements to the embodiments described are possible. These variations, modifications, additions and improvements may fall within the scope of the invention as detailed within the following claims.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4590604">US4590604</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 13, 1983</td><td class="patent-data-table-td patent-date-value">May 20, 1986</td><td class="patent-data-table-td ">Westinghouse Electric Corp.</td><td class="patent-data-table-td ">Voice-recognition elevator security system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4998279">US4998279</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 18, 1989</td><td class="patent-data-table-td patent-date-value">Mar 5, 1991</td><td class="patent-data-table-td ">Weiss Kenneth P</td><td class="patent-data-table-td ">Method and apparatus for personal verification utilizing nonpredictable codes and biocharacteristics</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6393305">US6393305</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 7, 1999</td><td class="patent-data-table-td patent-date-value">May 21, 2002</td><td class="patent-data-table-td ">Nokia Mobile Phones Limited</td><td class="patent-data-table-td ">Secure wireless communication user identification by voice recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6496111">US6496111</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 29, 2000</td><td class="patent-data-table-td patent-date-value">Dec 17, 2002</td><td class="patent-data-table-td ">Ray N. Hosack</td><td class="patent-data-table-td ">Personal security system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6810380">US6810380</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 28, 2001</td><td class="patent-data-table-td patent-date-value">Oct 26, 2004</td><td class="patent-data-table-td ">Bellsouth Intellectual Property Corporation</td><td class="patent-data-table-td ">Personal safety enhancement for communication devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7116247">US7116247</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 23, 2004</td><td class="patent-data-table-td patent-date-value">Oct 3, 2006</td><td class="patent-data-table-td ">Omron Corporation</td><td class="patent-data-table-td ">Security arrangement with in-vehicle mounted terminal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7188110">US7188110</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 11, 2000</td><td class="patent-data-table-td patent-date-value">Mar 6, 2007</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Secure and convenient method and apparatus for storing and transmitting telephony-based data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7299177">US7299177</a></td><td class="patent-data-table-td patent-date-value">May 30, 2003</td><td class="patent-data-table-td patent-date-value">Nov 20, 2007</td><td class="patent-data-table-td ">American Express Travel Related Services Company, Inc.</td><td class="patent-data-table-td ">Speaker recognition in a multi-speaker environment and comparison of several voice prints to many</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7376565">US7376565</a></td><td class="patent-data-table-td patent-date-value">Dec 15, 2003</td><td class="patent-data-table-td patent-date-value">May 20, 2008</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method, system, and apparatus for monitoring security events using speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7778832">US7778832</a></td><td class="patent-data-table-td patent-date-value">Sep 26, 2007</td><td class="patent-data-table-td patent-date-value">Aug 17, 2010</td><td class="patent-data-table-td ">American Express Travel Related Services Company, Inc.</td><td class="patent-data-table-td ">Speaker recognition in a multi-speaker environment and comparison of several voice prints to many</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7860714">US7860714</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 1, 2005</td><td class="patent-data-table-td patent-date-value">Dec 28, 2010</td><td class="patent-data-table-td ">Nippon Telegraph And Telephone Corporation</td><td class="patent-data-table-td ">Detection system for segment including specific sound signal, method and program for the same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7893826">US7893826</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 7, 2005</td><td class="patent-data-table-td patent-date-value">Feb 22, 2011</td><td class="patent-data-table-td ">Vendolocus Ab</td><td class="patent-data-table-td ">Alarm system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7899742">US7899742</a></td><td class="patent-data-table-td patent-date-value">Jun 19, 2003</td><td class="patent-data-table-td patent-date-value">Mar 1, 2011</td><td class="patent-data-table-td ">American Express Travel Related Services Company, Inc.</td><td class="patent-data-table-td ">System and method for facilitating a subsidiary card account</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7904299">US7904299</a></td><td class="patent-data-table-td patent-date-value">Feb 11, 2008</td><td class="patent-data-table-td patent-date-value">Mar 8, 2011</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Method, system, and apparatus for monitoring security events using speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7930389">US7930389</a></td><td class="patent-data-table-td patent-date-value">Nov 20, 2007</td><td class="patent-data-table-td patent-date-value">Apr 19, 2011</td><td class="patent-data-table-td ">The Invention Science Fund I, Llc</td><td class="patent-data-table-td ">Adaptive filtering of annotated messages or the like</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7940897">US7940897</a></td><td class="patent-data-table-td patent-date-value">Jun 24, 2005</td><td class="patent-data-table-td patent-date-value">May 10, 2011</td><td class="patent-data-table-td ">American Express Travel Related Services Company, Inc.</td><td class="patent-data-table-td ">Word recognition system and method for customer and employee assessment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8036892">US8036892</a></td><td class="patent-data-table-td patent-date-value">Jul 8, 2010</td><td class="patent-data-table-td patent-date-value">Oct 11, 2011</td><td class="patent-data-table-td ">American Express Travel Related Services Company, Inc.</td><td class="patent-data-table-td ">Speaker recognition in a multi-speaker environment and comparison of several voice prints to many</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8065404">US8065404</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 2007</td><td class="patent-data-table-td patent-date-value">Nov 22, 2011</td><td class="patent-data-table-td ">The Invention Science Fund I, Llc</td><td class="patent-data-table-td ">Layering destination-dependent content handling guidance</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8082225">US8082225</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 2007</td><td class="patent-data-table-td patent-date-value">Dec 20, 2011</td><td class="patent-data-table-td ">The Invention Science Fund I, Llc</td><td class="patent-data-table-td ">Using destination-dependent criteria to guide data transmission decisions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8165886">US8165886</a></td><td class="patent-data-table-td patent-date-value">Sep 29, 2008</td><td class="patent-data-table-td patent-date-value">Apr 24, 2012</td><td class="patent-data-table-td ">Great Northern Research LLC</td><td class="patent-data-table-td ">Speech interface system and method for control and interaction with applications on a computing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8175884">US8175884</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 20, 2012</td><td class="patent-data-table-td patent-date-value">May 8, 2012</td><td class="patent-data-table-td ">Gary Jay Morris</td><td class="patent-data-table-td ">Environmental condition detector with validated personalized verbal messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8219407">US8219407</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 2008</td><td class="patent-data-table-td patent-date-value">Jul 10, 2012</td><td class="patent-data-table-td ">Great Northern Research, LLC</td><td class="patent-data-table-td ">Method for processing the output of a speech recognizer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8237571">US8237571</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 6, 2009</td><td class="patent-data-table-td patent-date-value">Aug 7, 2012</td><td class="patent-data-table-td ">Industrial Technology Research Institute</td><td class="patent-data-table-td ">Alarm method and system based on voice events, and building method on behavior trajectory thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8428954">US8428954</a></td><td class="patent-data-table-td patent-date-value">May 5, 2012</td><td class="patent-data-table-td patent-date-value">Apr 23, 2013</td><td class="patent-data-table-td ">Gary Jay Morris</td><td class="patent-data-table-td ">Environmental condition detector with validated personalized verbal messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8682982">US8682982</a></td><td class="patent-data-table-td patent-date-value">Jun 19, 2007</td><td class="patent-data-table-td patent-date-value">Mar 25, 2014</td><td class="patent-data-table-td ">The Invention Science Fund I, Llc</td><td class="patent-data-table-td ">Preliminary destination-dependent evaluation of message content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090296897">US20090296897</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 29, 2008</td><td class="patent-data-table-td patent-date-value">Dec 3, 2009</td><td class="patent-data-table-td ">Charles Pearson</td><td class="patent-data-table-td ">System and Method of Telephone Input for Alarm Events</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100127878">US20100127878</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 6, 2009</td><td class="patent-data-table-td patent-date-value">May 27, 2010</td><td class="patent-data-table-td ">Yuh-Ching Wang</td><td class="patent-data-table-td ">Alarm Method And System Based On Voice Events, And Building Method On Behavior Trajectory Thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120004913">US20120004913</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 30, 2011</td><td class="patent-data-table-td patent-date-value">Jan 5, 2012</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Method and apparatus for controlling operation of portable terminal using microphone</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2007018414A1?cl=en">WO2007018414A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 11, 2006</td><td class="patent-data-table-td patent-date-value">Feb 15, 2007</td><td class="patent-data-table-td ">Fawoo Light Panel Inc</td><td class="patent-data-table-td ">System for emergency pilot lamp</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=SKFVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S273000">704/273</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=SKFVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S247000">704/247</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=SKFVBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704SE17003">704/E17.003</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=SKFVBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0017000000">G10L17/00</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=SKFVBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L17/005">G10L17/005</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G10L17/00U</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">May 21, 2013</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 1-16 IS CONFIRMED.NEW CLAIMS 17-26 ARE ADDED AND DETERMINED TO BE PATENTABLE.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 2, 2013</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 8, 2012</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20120330</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 7, 2010</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">LEGERITY, INC., TEXAS</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20040213</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20101207</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">INTELLECTUAL VENTURES I LLC, DELAWARE</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">MERGER;ASSIGNOR:KMB CAPITAL FUND LLC;REEL/FRAME:025467/0123</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">LEGERITY HOLDINGS, INC., TEXAS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE BY SECURED PARTY;ASSIGNOR:MORGAN STANLEY &amp; CO. INCORPORATED, AS FACILITY COLLATERAL AGENT;REEL/FRAME:025461/0704</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">LEGERITY INTERNATIONAL, INC., TEXAS</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 1, 2010</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE BY SECURED PARTY;ASSIGNOR:MORGAN STANLEY &amp; CO. INCORPORATED, AS COLLATERAL AGENT;REEL/FRAME:025077/0178</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">LEGERITY, INC., TEXAS</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">LEGERITY INTERNATIONAL, INC., TEXAS</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20040220</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">LEGERITY HOLDINGS, INC., TEXAS</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 30, 2008</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 1, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">KMB CAPITAL FUND LLC, CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:LEGERITY, INC.;REEL/FRAME:016323/0757</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20031222</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 9, 2005</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 9, 2005</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 2, 2005</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 21, 2002</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">MORGAN STANLEY &amp; CO. INCORPORATED, AS FACILITY COL</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNORS:LEGERITY, INC.;LEGERITY HOLDINGS, INC.;LEGERITY INTERNATIONAL, INC.;REEL/FRAME:013372/0063</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20020930</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 23, 2001</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">LEGERITY, INC., TEXAS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:ADVANCED MICRO DEVICES, INC.;REEL/FRAME:011700/0686</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20000731</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">LEGERITY, INC. BLDG. 3, M/S 310 4509 FREIDRICH LAN</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 14, 2000</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">MORGAN STANLEY &amp; CO. INCORPORATED, NEW YORK</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY INTEREST;ASSIGNOR:LEGERITY, INC.;REEL/FRAME:011601/0539</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20000804</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">MORGAN STANLEY &amp; CO. INCORPORATED 1585 BROADWAY NE</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 11, 1999</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">ADVANCED MICRO DEVICES, INC., CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:DILDY, ROBERT;REEL/FRAME:009722/0815</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19990107</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U3do-Qq0sITPqdVp5edyaSb0wXoNg\u0026id=SKFVBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U3If3tAFTk3FbKtky-KjkAk5RC6Nw\u0026id=SKFVBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U0-oA9N0rSqVrDVF0lBBWy7zPMfMw","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Method_and_system_for_providing_security.pdf?id=SKFVBAABERAJ\u0026output=pdf\u0026sig=ACfU3U3Qq0JXA0M9hR1ZyaFjskT4LIogJg"},"sample_url":"http://www.google.com/patents/reader?id=SKFVBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>