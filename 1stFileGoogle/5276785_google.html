<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US5276785 - Moving viewpoint with respect to a target in a three-dimensional workspace - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Moving viewpoint with respect to a target in a three-dimensional workspace"><meta name="DC.contributor" content="Jock Mackinlay" scheme="inventor"><meta name="DC.contributor" content="George G. Robertson" scheme="inventor"><meta name="DC.contributor" content="Stuart K. Card" scheme="inventor"><meta name="DC.contributor" content="Xerox Corporation" scheme="assignee"><meta name="DC.date" content="1990-8-2" scheme="dateSubmitted"><meta name="DC.description" content="Images are presented on a display to produce the perception of viewpoint motion in a three-dimensional workspace. The user can indicate a point of interest (POI) or other region on a surface in an image and request viewpoint motion. In response, another image is presented from a viewpoint that is displaced as requested. The user can request viewpoint motion radially toward or away from the POI, and can also request viewpoint motion laterally toward a normal of the surface at the POI. Radial and lateral viewpoint motion can be combined. The orientation of the viewpoint can be shifted during lateral motion to keep the POI in the field of view, and can also be shifted to bring the POI toward the center of the field of view. In a sequence of steps of viewpoint motion, the radial viewpoint displacement in each step can be a proportion of the distance to the POI so that the radial displacements follow a logarithmic function and define an asymptotic path that approaches but does not reach the POI. While requesting viewpoint motion with a keyboard, the user can independently request POI motion with the mouse. In response, the POI moves within the bounds of the surface that includes the POI, and a shape within the image indicates the POI position."><meta name="DC.date" content="1994-1-4" scheme="issued"><meta name="DC.relation" content="EP:0353952:A2" scheme="references"><meta name="DC.relation" content="US:4734690" scheme="references"><meta name="DC.relation" content="US:4991022" scheme="references"><meta name="DC.relation" content="US:5019809" scheme="references"><meta name="DC.relation" content="US:5021976" scheme="references"><meta name="DC.relation" content="US:5072412" scheme="references"><meta name="DC.relation" content="US:5095302" scheme="references"><meta name="DC.relation" content="US:5103217" scheme="references"><meta name="DC.relation" content="US:5107443" scheme="references"><meta name="DC.relation" content="US:5124693" scheme="references"><meta name="DC.relation" content="US:5129054" scheme="references"><meta name="citation_reference" content="&quot;MAG (6D),&quot; IRIS-4D Series Man Pages, IRIX User&#39;s Reference Manual, Section 6, Release 3.3, Silicon Graphics, Inc., Mountain View, California, Apr. 1990 p. 1."><meta name="citation_reference" content="&quot;Window,&quot; Graphics Library Programming Guide, IRIS-4D Series, Document Version 2.0, Coordinate Transformations, Document No. 007-1210-020, Silicon Graphics, Inc., Mountain View, California, May 1990, pp. 7-8 and 7-9."><meta name="citation_reference" content="Badler, N. I., Manochehri, K. H. Baraff, D., &quot;Multi-Dimensional Input Techniques and Articulated Figure Positioning by Multiple Constraints,&quot; Proceedings, 1986 Workshop on Interactive 3D Graphics, ACM, Oct. 23-24, 1986, pp. 151-169."><meta name="citation_reference" content="Badler, N. I., Manochehri, K. H. Baraff, D., Multi Dimensional Input Techniques and Articulated Figure Positioning by Multiple Constraints, Proceedings, 1986 Workshop on Interactive 3 D Graphics, ACM, Oct. 23 24, 1986, pp. 151 169."><meta name="citation_reference" content="Bier, E. A., Snap Dragging: Interactive Geometric Design in Two and Three Dimensions, Xerox Corporation, EDL 89 2, Sep. 1989, p. 117."><meta name="citation_reference" content="Bier, E. A., Snap-Dragging: Interactive Geometric Design in Two and Three Dimensions, Xerox Corporation, EDL-89-2, Sep. 1989, p. 117."><meta name="citation_reference" content="Brooks, Jr., F. P., &quot;Walkthrough-A Dynamic Graphics System for Simulating Virtual Buildings,&quot; Proceedings, 1986 Workshop on Interactive 3D Graphics, ACM, Oct. 23-24, 1986, pp. 9-21."><meta name="citation_reference" content="Brooks, Jr., F. P., Walkthrough A Dynamic Graphics System for Simulating Virtual Buildings, Proceedings, 1986 Workshop on Interactive 3 D Graphics, ACM, Oct. 23 24, 1986, pp. 9 21."><meta name="citation_reference" content="Burton, R. R., Sketch: A Drawing Program for Interlisp D, Xerox Corporation, Palo Alto Research Center, ISL 14, Aug. 1985, pp. 44 49."><meta name="citation_reference" content="Burton, R. R., Sketch: A Drawing Program for Interlisp-D, Xerox Corporation, Palo Alto Research Center, ISL-14, Aug. 1985, pp. 44-49."><meta name="citation_reference" content="Dalton, R., &quot;Beyond Bifocals: Help for Tired Eyes,&quot; Loftus, vol. 4, No. 10, Oct. 1988, pp. 17-18."><meta name="citation_reference" content="Dalton, R., Beyond Bifocals: Help for Tired Eyes, Loftus, vol. 4, No. 10, Oct. 1988, pp. 17 18."><meta name="citation_reference" content="Fairchild, K. M., Poltrock, S. E., and Furnas, G. W., &quot;Semi Net: Three-Dimensional Graphic Representations of Large Knowledge Bases,&quot; in Guindon, R., ed., Cognitive Science and its Applications for Human-Computer Interaction, Lawrence Erlbaum, Hillsdale, N.J., 1988, pp. 201-233."><meta name="citation_reference" content="Fairchild, K. M., Poltrock, S. E., and Furnas, G. W., Semi Net: Three Dimensional Graphic Representations of Large Knowledge Bases, in Guindon, R., ed., Cognitive Science and its Applications for Human Computer Interaction, Lawrence Erlbaum, Hillsdale, N.J., 1988, pp. 201 233."><meta name="citation_reference" content="Foley, J. D., van Dam, A., Feiner, S. K., Hughes, J. F., Computer Graphics, Second Edition, 1990, Reading, Mass.: Addison Wesley, Chapter 21, pp. 1057 1081."><meta name="citation_reference" content="Foley, J. D., van Dam, A., Feiner, S. K., Hughes, J. F., Computer Graphics, Second Edition, 1990, Reading, Mass.: Addison-Wesley, Chapter 21, pp. 1057-1081."><meta name="citation_reference" content="Glassner, A. S., 3 D Computer Graphics, Second Edition, 1989, New York: Design Press, Chapter 14, pp. 177 194."><meta name="citation_reference" content="Glassner, A. S., 3D Computer Graphics, Second Edition, 1989, New York: Design Press, Chapter 14, pp. 177-194."><meta name="citation_reference" content="Haeberli, P. E., &quot;ConMan: A Visual Programming Language for Interactive Graphics,&quot; Computer Graphics, vol. 22, No. 4, Aug. 1988, pp. 103-111."><meta name="citation_reference" content="Haeberli, P. E., ConMan: A Visual Programming Language for Interactive Graphics, Computer Graphics, vol. 22, No. 4, Aug. 1988, pp. 103 111."><meta name="citation_reference" content="Herot, C. F., Carling, R., Friedell, M., and Kramlich, D., &quot;A Prototype Spatial Data Management System,&quot; ACM, 1980; pp. 63-70."><meta name="citation_reference" content="Herot, C. F., Carling, R., Friedell, M., and Kramlich, D., A Prototype Spatial Data Management System, ACM, 1980; pp. 63 70."><meta name="citation_reference" content="Hochberg, J., Brooks, V., &quot;Film Cutting and Visual Momentum,&quot; in Senders, J. W., Fisher, D. F., and Monty, R. A., Eds. Eye Movements and the Higher Psychological Functions, Hillsdale, N.J.: Lawrence Erlbaum Associates; 1978; pp. 293-313."><meta name="citation_reference" content="Hochberg, J., Brooks, V., Film Cutting and Visual Momentum, in Senders, J. W., Fisher, D. F., and Monty, R. A., Eds. Eye Movements and the Higher Psychological Functions, Hillsdale, N.J.: Lawrence Erlbaum Associates; 1978; pp. 293 313."><meta name="citation_reference" content="InLarge brochure, Berkeley Systems, Inc., Berkeley, California, 1989."><meta name="citation_reference" content="MAG (6D), IRIS 4D Series Man Pages, IRIX User s Reference Manual, Section 6, Release 3.3, Silicon Graphics, Inc., Mountain View, California, Apr. 1990 p. 1."><meta name="citation_reference" content="Robertson, G. G., Card, S. K., and Mackinlay, J. D., &quot;The Cognitive Coprocessor Architecture for Interactive User Interfaces,&quot; Proceedings of the ACM SIGGRAPH Symposium on User Interface Software and Technology, Williamsburg, Va., Nov. 13-15, 1989, pp. 10-18."><meta name="citation_reference" content="Robertson, G. G., Card, S. K., and Mackinlay, J. D., The Cognitive Coprocessor Architecture for Interactive User Interfaces, Proceedings of the ACM SIGGRAPH Symposium on User Interface Software and Technology, Williamsburg, Va., Nov. 13 15, 1989, pp. 10 18."><meta name="citation_reference" content="Ware, C., Osborne, S., &quot;Exploration and Virtual Camera Control in Virtual Three Dimensional Environments,&quot; Proceedings, 1990 Symposium on Interactive 3D Graphics Computer Graphics 24, #4, Mar. 1970, pp. 175-183."><meta name="citation_reference" content="Ware, C., Osborne, S., Exploration and Virtual Camera Control in Virtual Three Dimensional Environments, Proceedings, 1990 Symposium on Interactive 3 D Graphics Computer Graphics 24, 4, Mar. 1970, pp. 175 183."><meta name="citation_reference" content="Window, Graphics Library Programming Guide, IRIS 4D Series, Document Version 2.0, Coordinate Transformations, Document No. 007 1210 020, Silicon Graphics, Inc., Mountain View, California, May 1990, pp. 7 8 and 7 9."><meta name="citation_reference" content="Woods, D. D., &quot;Visual Momentum: A Concept to Improve the Cognitive Coupling of Person and Computer,&quot; Int. J. Man-Machine Studies 21, 1984, pp. 229-244."><meta name="citation_reference" content="Woods, D. D., Visual Momentum: A Concept to Improve the Cognitive Coupling of Person and Computer, Int. J. Man Machine Studies 21, 1984, pp. 229 244."><meta name="citation_patent_number" content="US:5276785"><meta name="citation_patent_application_number" content="US:07/561,627"><link rel="canonical" href="http://www.google.com/patents/US5276785"/><meta property="og:url" content="http://www.google.com/patents/US5276785"/><meta name="title" content="Patent US5276785 - Moving viewpoint with respect to a target in a three-dimensional workspace"/><meta name="description" content="Images are presented on a display to produce the perception of viewpoint motion in a three-dimensional workspace. The user can indicate a point of interest (POI) or other region on a surface in an image and request viewpoint motion. In response, another image is presented from a viewpoint that is displaced as requested. The user can request viewpoint motion radially toward or away from the POI, and can also request viewpoint motion laterally toward a normal of the surface at the POI. Radial and lateral viewpoint motion can be combined. The orientation of the viewpoint can be shifted during lateral motion to keep the POI in the field of view, and can also be shifted to bring the POI toward the center of the field of view. In a sequence of steps of viewpoint motion, the radial viewpoint displacement in each step can be a proportion of the distance to the POI so that the radial displacements follow a logarithmic function and define an asymptotic path that approaches but does not reach the POI. While requesting viewpoint motion with a keyboard, the user can independently request POI motion with the mouse. In response, the POI moves within the bounds of the surface that includes the POI, and a shape within the image indicates the POI position."/><meta property="og:title" content="Patent US5276785 - Moving viewpoint with respect to a target in a three-dimensional workspace"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("LpftU6KeIsXXsQSmqIGICw"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("ITA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("LpftU6KeIsXXsQSmqIGICw"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("ITA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us5276785?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US5276785"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=Lrw3BAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS5276785&amp;usg=AFQjCNHOepJ1bLTpw_7UrkIsqfJ8493Zbw" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US5276785.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US5276785.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US5276785" style="display:none"><span itemprop="description">Images are presented on a display to produce the perception of viewpoint motion in a three-dimensional workspace. The user can indicate a point of interest (POI) or other region on a surface in an image and request viewpoint motion. In response, another image is presented from a viewpoint that is displaced...</span><span itemprop="url">http://www.google.com/patents/US5276785?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US5276785 - Moving viewpoint with respect to a target in a three-dimensional workspace</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US5276785 - Moving viewpoint with respect to a target in a three-dimensional workspace" title="Patent US5276785 - Moving viewpoint with respect to a target in a three-dimensional workspace"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US5276785 A</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 07/561,627</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Jan 4, 1994</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Aug 2, 1990</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Aug 2, 1990</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/DE69130198D1">DE69130198D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69130198T2">DE69130198T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0471484A2">EP0471484A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0471484A3">EP0471484A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0471484B1">EP0471484B1</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">07561627, </span><span class="patent-bibdata-value">561627, </span><span class="patent-bibdata-value">US 5276785 A, </span><span class="patent-bibdata-value">US 5276785A, </span><span class="patent-bibdata-value">US-A-5276785, </span><span class="patent-bibdata-value">US5276785 A, </span><span class="patent-bibdata-value">US5276785A</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Jock+Mackinlay%22">Jock Mackinlay</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22George+G.+Robertson%22">George G. Robertson</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Stuart+K.+Card%22">Stuart K. Card</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Xerox+Corporation%22">Xerox Corporation</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US5276785.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5276785.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5276785.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (11),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (33),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (127),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (13),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (10)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=Lrw3BAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/5276785&usg=AFQjCNGh9GEjXVVfpHDkI36YeXPVpt8ihA">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=Lrw3BAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D5276785&usg=AFQjCNFL6T_AtExcmoqdXVdli2d4m3Zomw">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=Lrw3BAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D5276785A%26KC%3DA%26FT%3DD&usg=AFQjCNFk8iAWOnBh6k-C6phFIFzYbBgLhA">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT53811947" lang="EN" load-source="patent-office">Moving viewpoint with respect to a target in a three-dimensional workspace</invention-title></span><br><span class="patent-number">US 5276785 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA37286038" lang="EN" load-source="patent-office"> <div class="abstract">Images are presented on a display to produce the perception of viewpoint motion in a three-dimensional workspace. The user can indicate a point of interest (POI) or other region on a surface in an image and request viewpoint motion. In response, another image is presented from a viewpoint that is displaced as requested. The user can request viewpoint motion radially toward or away from the POI, and can also request viewpoint motion laterally toward a normal of the surface at the POI. Radial and lateral viewpoint motion can be combined. The orientation of the viewpoint can be shifted during lateral motion to keep the POI in the field of view, and can also be shifted to bring the POI toward the center of the field of view. In a sequence of steps of viewpoint motion, the radial viewpoint displacement in each step can be a proportion of the distance to the POI so that the radial displacements follow a logarithmic function and define an asymptotic path that approaches but does not reach the POI. While requesting viewpoint motion with a keyboard, the user can independently request POI motion with the mouse. In response, the POI moves within the bounds of the surface that includes the POI, and a shape within the image indicates the POI position.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(12)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5276785-2.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5276785-2.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5276785-3.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5276785-3.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5276785-4.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5276785-4.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5276785-5.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5276785-5.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5276785-6.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5276785-6.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5276785-7.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5276785-7.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5276785-8.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5276785-8.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5276785-9.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5276785-9.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5276785-10.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5276785-10.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5276785-11.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5276785-11.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5276785-12.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5276785-12.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5276785-13.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5276785-13.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(64)</span></span></div><div class="patent-text"><div mxw-id="PCLM4699112" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed:</claim-statement> <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1. A method of operating a system that includes a display, a user input device, and a processor connected for receiving signals from the user input device and for presenting images on the display; the user input device providing region indicating signals indicating regions within images presented and motion requesting signals requesting viewpoint motion; the method comprising steps of:<div class="claim-text">presenting a first image on the display; the first image including a first surface that is perceptible as viewed from a first viewpoint within a three-dimensional workspace; the step of presenting the first image comprising a substep of storing viewpoint coordinate data indicating a position of the first viewpoint in the three-dimensional workspace;</div> <div class="claim-text">receiving a first region indicating signal and a first motion requesting signal from the user input device; the first region indicating signal indicating a first region on the first surface; the first motion requesting signal requesting viewpoint motion relative to the first region; and</div> <div class="claim-text">presenting a second image on the display; the second image including a second surface that is perceptible as a continuation of the first surface viewed from a second viewpoint within the three-dimensional workspace, the second viewpoint being displaced from the position indicated by the stored viewpoint coordinate data relative to the first region on the first surface in accordance with the first motion requesting signal.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2. The method of claim 1 in which the first region indicating signal indicates a point on the first surface.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3. The method of claim 1 in which the first viewpoint is perceptible as positioned at a first distance from the first region and the second viewpoint is perceptible as positioned at a second distance from the first region, the second distance being shorter than the first distance.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4. The method of claim 1 in which the first viewpoint is perceptible as positioned at a first distance from the first region and the second viewpoint is perceptible as positioned at a second distance from the first region, the second distance being longer than the first distance.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5. The method of claim I in which the first surface is perceptible as having a normal in the first region, the second viewpoint being closer to the normal than the first viewpoint.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6. The method of claim 5 in which the first viewpoint is perceptible as having a first direction of orientation and the second viewpoint is perceptible as having a second direction of orientation; the second direction of orientation being shifted toward the first region from the first direction of orientation.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7. The method of claim 6 in which the second viewpoint is perceptible as closer to the normal than the first viewpoint by an are length and the second direction of orientation is perceptible as shifted by a shift angle from the first direction of orientation; the shift angle being an angle subtended by the arc length.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8. The method of claim 6 in which the second direction of orientation is perceptible as shifted by a shift angle from the first direction of orientation; the second image being perceptible as having a center of view; the shift angle bringing the first region toward the center of view.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9. The method of claim 1 in which the first viewpoint is perceptible as having a first direction of orientation and the second viewpoint is perceptible as having a second direction of orientation; the second direction of orientation being shifted toward the first region from the first direction of orientation.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" class="claim">
      <div class="claim-text">10. The method of claim 1, further comprising a step of receiving a second region indicating signal from the user input device; the second region indicating signal indicating a second region on the second surface, the second region being perceptible as a displaced continuation of the first region.</div>
    </div>
    </div> <div class="claim"> <div num="11" class="claim">
      <div class="claim-text">11. A method of operating a system that includes a display, a user input device, and a processor connected for receiving signals from the user input device and for presenting images on the display; the user input device providing region indicating signals indicating regions within images presented on the display and motion requesting signals requesting viewpoint motion; the method comprising a sequence of steps, each step comprising substeps of:<div class="claim-text">presenting a respective image on the display; each respective image including a respective surface that is perceptible as being viewed from a respective viewpoint within a three-dimensional workspace; the substep of presenting the respective image comprising a substep of storing respective coordinate data indicating a position of the respective viewpoint in the three-dimensional workspace; and</div> <div class="claim-text">receiving a respective region indicating signal and a respective motion requesting signal from the user input device; each respective region indicating signal indicating a respective region on the respective surface; each respective motion requesting signal requesting viewpoint motion relative to the respective region;</div> <div class="claim-text">the sequence of steps including a first step and a number of following steps, each following step having a next preceding step; the respective surface of each following step being perceptible as a continuation of the respective surface of the next preceding step; the respective viewpoint of each following step being displaced from the position indicated by the respective coordinate data stored in the next preceding step relative to the respective region of the next preceding step in accordance with the respective motion requesting signal of the next preceding step.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" class="claim">
      <div class="claim-text">12. The method of claim 11 in which each step's respective region indicating signal indicates a respective point on the respective surface.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" class="claim">
      <div class="claim-text">13. The method of claim 11 in which the respective viewpoint of each following step is displaced from the respective viewpoint of the next preceding step by a respective approach displacement toward the respective region of the next preceding step.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" class="claim">
      <div class="claim-text">14. The method of claim 13 in which the respective approach displacements define an asymptotic path in the three-dimensional workspace.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" class="claim">
      <div class="claim-text">15. The method of claim 13 in which the respective approach displacements follow a logarithmic function.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" class="claim">
      <div class="claim-text">16. The method of claim 13 in which the respective viewpoint and region of each step are perceptible as positioned at a respective distance from each other; the respective approach displacement of each following step being a proportion of the respective distance of the next preceding step.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" class="claim">
      <div class="claim-text">17. The method of claim 11 in which the respective viewpoint of each following step is displaced from the respective viewpoint of the next preceding step by a respective retreat displacement away from the respective region of the next preceding step.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" class="claim">
      <div class="claim-text">18. The method of claim 17 in which the respective retreat displacements follow a logarithmic function.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" class="claim">
      <div class="claim-text">19. The method of claim 17 in which the respective viewpoint and region of each step are perceptible as positioned at a respective distance from each other; the respective retreat displacement of each following step being a proportion of the respective distance of the next preceding step.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" class="claim">
      <div class="claim-text">20. The method of claim 11 in which the respective surface of each step is perceptible as having a respective normal in the respective region; the respective viewpoint of each following step being displaced from the respective viewpoint of the next preceding step by a respective lateral displacement toward the respective normal of the next preceding step.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" class="claim">
      <div class="claim-text">21. The method of claim 20 in which the respective lateral displacements define an asymptotic path in the three-dimensional workspace.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="22" class="claim">
      <div class="claim-text">22. The method of claim 20 in which the respective lateral displacements follow a logarithmic function.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="23" class="claim">
      <div class="claim-text">23. The method of claim 20 in which the respective normal of each step includes a respective lateral position point; the respective viewpoint and lateral position point of each following step being perceptible as positioned at a respective lateral distance from each other; the respective lateral displacement of each following step being a proportion of the respective lateral distance of the next preceding step.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="24" class="claim">
      <div class="claim-text">24. The method of claim 11 in which the respective viewpoint of each step is perceptible as having a respective direction of orientation; the respective direction of orientation of each following step being shifted toward the respective region of the next preceding step from the respective direction of orientation of the next preceding step.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" class="claim">
      <div class="claim-text">25. The method of claim 11, in which each following step further comprises a substep of presenting within the respective image a shape positioned about the respective region of the next preceding step.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="26" class="claim">
      <div class="claim-text">26. The method of claim 25 in which the shape is presented so that it is perceptible as being parallel to the respective surface at the respective region of the next preceding step.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="27" class="claim">
      <div class="claim-text">27. The method of claim 11 in which the respective surface of each step is bounded; the respective motion requesting signal of the first step being a beginning motion requesting signal and the respective motion requesting signal of a second step that is one of the following steps being an ending motion requesting signal, the respective region of each step after the first step through the second step being constrained to stay within the bounded respective surface of the step.</div>
    </div>
    </div> <div class="claim"> <div num="28" class="claim">
      <div class="claim-text">28. A method of operating a system that includes a display, a user input device, and a processor connected for receiving signals from the user input device and for presenting images on the display; the user input device providing motion requesting signals requesting viewpoint motion; the method comprising steps of:<div class="claim-text">presenting a first image on the display; the first image including a first surface that is perceptible as being viewed from a first viewpoint within a three-dimensional workspace; the first surface including a first region; the first viewpoint being positioned at a first distance from the first region;</div> <div class="claim-text">receiving a first motion requesting signal requesting viewpoint motion from the user input device; and</div> <div class="claim-text">presenting a second image on the display; the second image including a second surface that is perceptible as a continuation of the first surface viewed from a second viewpoint within the three-dimensional workspace; the second viewpoint being displaced by a First displacement from the first viewpoint in accordance with the first motion requesting signal; the first displacement being a function of the first distance.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="29" class="claim">
      <div class="claim-text">29. The method of claim 28 in which the first displacement is an approach displacement toward the first region.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="30" class="claim">
      <div class="claim-text">30. The method of claim 28 in which the first displacement is a retreat displacement away from the first region.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="31" class="claim">
      <div class="claim-text">31. The method of claim 28 in which the first displacement is a logarithmic function of the first distance.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="32" class="claim">
      <div class="claim-text">32. The method of claim 28 in which the first displacement is a proportion of the first distance.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="33" class="claim">
      <div class="claim-text">33. The method of claim 28 in which the first region includes a point, the first distance being a distance between the first viewpoint and the point.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="34" class="claim">
      <div class="claim-text">34. The method of claim 28 in which the first surface is perceptible as having a normal in the first region, the first displacement being a lateral displacement toward the normal.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="35" class="claim">
      <div class="claim-text">35. The method of claim 34 in which the first displacement is a proportion of a line between the first viewpoint and a normal point, the normal point being on the normal and at the first distance from the region.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="36" class="claim">
      <div class="claim-text">36. The method of claim 35 in which the line is an arc.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="37" class="claim">
      <div class="claim-text">37. The method of claim 35 in which the line is a chord.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="38" class="claim">
      <div class="claim-text">38. The method of claim 34 in which the first viewpoint is perceptible as having a first direction of orientation and the second viewpoint is perceptible as having a second direction of orientation; the second direction of orientation being shifted toward the first region from the first direction of orientation.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="39" class="claim">
      <div class="claim-text">39. The method of claim 34 in which the second viewpoint is perceptible as closer to the normal than the first viewpoint by an arc length and the second direction of orientation is perceptible as shifted by a shift angle from the first direction of orientation; the shift angle being an angle subtended by the arc length.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="40" class="claim">
      <div class="claim-text">40. The method of claim 34 in which the second direction of orientation is perceptible as shifted by a shift angle from the first direction of orientation; the second image being perceptible as having a center of view; the shift angle bringing the first region toward the center of view.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="41" class="claim">
      <div class="claim-text">41. The method of claim 28 in which the first viewpoint is perceptible as having a first direction of orientation and the second viewpoint is perceptible as having a second direction of orientation; the second direction of orientation being shifted toward the first region from the first direction of orientation.</div>
    </div>
    </div> <div class="claim"> <div num="42" class="claim">
      <div class="claim-text">42. A method of operating a system that includes a display, a user input device, and a processor connected for receiving signals from the user input device and for presenting images on the display; the user input device providing motion requesting signals requesting viewpoint motion; the method comprising a sequence of steps, each step comprising substeps of:<div class="claim-text">presenting a respective image on the display; each respective image including a respective surface that is perceptible as being viewed from a respective viewpoint within a three-dimensional workspace; each step's respective surface including a respective region; the respective viewpoint being positioned at a respective distance from the respective region; and</div> <div class="claim-text">receiving a respective motion requesting signal requesting viewpoint motion from the user input device; each respective motion requesting signal requesting viewpoint motion;</div> <div class="claim-text">the sequence of steps including a first step and a number of following steps, each following step having a next preceding step; the respective surface of each following step being perceptible as a continuation of the respective surface of the next preceding step; the respective viewpoint of each following step being displaced by a respective displacement from the respective viewpoint of the next preceding step in accordance with the next preceding step's motion requesting signal; the respective displacement of each following step being a function of the respective distance of the next preceding step.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="43" class="claim">
      <div class="claim-text">43. The method of claim 42 in which the respective surface of each step is perceptible as having a respective normal in the respective region; the respective displacement of each following step including a respective radial component that is a function of the respective distance of the next preceding step and a respective lateral component toward the respective normal of the next preceding step.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="44" class="claim">
      <div class="claim-text">44. The method of claim 43 in which the respective radial component of each following step is a first proportion of the respective distance of the next preceding step; the respective radial component indicating a respective intermediate viewpoint; the respective normal of each step including a respective lateral position point; the respective intermediate viewpoint of each following step and the lateral position point of the next preceding step being perceptible as positioned at a respective lateral distance from each other; the respective lateral component of each following step being a second proportion of the respective lateral distance.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="45" class="claim">
      <div class="claim-text">45. The method of claim 44 in which the second proportion is greater than the first proportion.</div>
    </div>
    </div> <div class="claim"> <div num="46" class="claim">
      <div class="claim-text">46. A method of operating a system that includes a display, a user input device, and a processor connected for receiving signals from the user input device and for presenting images on the display; the user input device providing motion requesting signals requesting viewpoint motion; the method comprising a sequence of steps, each step comprising substeps of:<div class="claim-text">presenting a respective image on the display; each respective image including a respective surface that is perceptible as being viewed from a respective viewpoint within a three-dimensional workspace; each step's respective surface including a respective region; the respective viewpoint being positioned at a respective distance from the respective region; and</div> <div class="claim-text">receiving a respective motion requesting signal requesting viewpoint motion from the user input device; each respective motion requesting signal requesting viewpoint motion;</div> <div class="claim-text">the sequence of steps including a first step and a number of following steps, each following step having a next preceding step; the respective surface of each following step being perceptible as a continuation of the respective surface of the next preceding step; the respective viewpoint of each following step being displaced by a respective displacement from the respective viewpoint of the next preceding step in accordance with the next preceding step's motion requesting signal;</div> <div class="claim-text">the following steps including first and second following steps; the respective motion requesting signal of the next preceding step of the first following step requesting motion toward the respective region of the next preceding step; the respective motion requesting signal of the next preceding step of the second following step requesting motion away from the respective region of the next preceding step; the respective displacement of the first following step including a first proportional component that is a first proportion of the respective distance of the next preceding step; the respective displacement of the second following step including a second proportional component that is a second proportion of the respective distance of the next preceding step.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="47" class="claim">
      <div class="claim-text">47. The method of claim 46 in which the first following step is the next preceding step of the second following step, the second proportion and the first proportion being such that the first and second proportional components are equal in magnitude.</div>
    </div>
    </div> <div class="claim"> <div num="48" class="claim">
      <div class="claim-text">48. A method of operating a system that includes a display, a user input device, and a processor connected for receiving signals from the user input device and for presenting images on the display; the user input device providing motion requesting signals requesting viewpoint motion; the method comprising a sequence of steps, each step comprising substeps of:<div class="claim-text">presenting a respective image on the display; each respective image including a respective surface that is perceptible as being viewed from a respective viewpoint within a three-dimensional workspace; each step's respective surface including a respective region; the substep of presenting the respective image comprising a substep of storing respective coordinate data indicating a position of the respective viewpoint in the three-dimensional workspace; and</div> <div class="claim-text">receiving a respective motion requesting signal requesting viewpoint motion from the user input device; each respective motion requesting signal requesting viewpoint motion toward the respective region;</div> <div class="claim-text">the sequence of steps including a first step and a number of following steps, each following step having a next preceding step; the respective surface of each following step being perceptible as a continuation of the respective surface of the next preceding step; the respective viewpoint of each following step being displaced by a respective displacement from the respective viewpoint of the next preceding step toward the respective region of the next preceding step; the respective displacement of each following step being a function of the position indicated by the next preceding step's respective coordinate data such that the respective displacements define an asymptotic path.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="49" class="claim">
      <div class="claim-text">49. The method of claim 48 in which each step's respective region includes a respective point, the respective displacement of each following step being a function of a respective distance between the position indicated by the next preceding step's respective coordinate data and the next preceding step's respective point.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="50" class="claim">
      <div class="claim-text">50. The method of claim 49 in which the respective displacement of each following step is a logarithmic function of the respective distance.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="51" class="claim">
      <div class="claim-text">51. The method of claim 49 in which the respective displacement of each following step is a proportion of the respective distance.</div>
    </div>
    </div> <div class="claim"> <div num="52" class="claim">
      <div class="claim-text">52. A method of operating a system that includes a display, user input means for providing signals, and a processor connected for receiving signals from the user input means and for presenting images on the display; the user input means providing motion requesting signals; the motion requesting signals requesting viewpoint motion and point of interest motion; the user input means being structured so that the user can request viewpoint motion and point of interest motion independently; the method comprising steps of:<div class="claim-text">presenting a first image on the display; the first image including a first surface that is perceptible as viewed from a first viewpoint within a three-dimensional workspace; the first image including a first point of interest on the first surface; the step of presenting the first image comprising a substep of storing viewpoint coordinate data indicating a position of the first viewpoint in the three-dimensional workspace;</div> <div class="claim-text">receiving a first motion requesting signal set from the user input means, the first motion requesting signal set requesting a first viewpoint motion and a first point of interest motion; and</div> <div class="claim-text">in response to the first motion requesting signal, presenting a second image on the display; the second image including a second surface that is perceptible as a continuation of the first surface viewed from a second viewpoint within the three-dimensional workspace, the second viewpoint being displaced from the position indicated by the stored viewpoint coordinate data in accordance with the first viewpoint motion; the second image including a second point of interest on the second surface, the second point of interest being displaced in accordance with the first point of interest motion.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="53" class="claim">
      <div class="claim-text">53. The method of claim 52 in which the first and second surfaces are each bounded; the first and second points of interest being constrained to stay within the bounded first and second surfaces, respectively.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="54" class="claim">
      <div class="claim-text">54. The method of claim 52 in which the first image further includes a first shape indicating the first point of interest and the second image further includes a second shape indicating the second point of interest, the second shape being perceptible as a moved continuation of the first shape.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="55" class="claim">
      <div class="claim-text">55. The method of claim 52 in which the first viewpoint motion includes radial motion, the second viewpoint being displaced radially along a ray extending from the second point of interest through the first viewpoint.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="56" class="claim">
      <div class="claim-text">56. The method of claim 55 in which the second viewpoint is displaced radially toward the second point of interest.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="57" class="claim">
      <div class="claim-text">57. The method of claim 55 in which the second viewpoint is displaced radially away from the second point of interest.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="58" class="claim">
      <div class="claim-text">58. The method of claim 52 in which the second surface is perceptible as having a normal at the second point of interest, the first viewpoint motion including lateral motion, the second viewpoint being displaced laterally toward the normal.</div>
    </div>
    </div> <div class="claim"> <div num="59" class="claim">
      <div class="claim-text">59. A system comprising:<div class="claim-text">a display;</div> <div class="claim-text">user input means for providing signals; and</div> <div class="claim-text">a processor connected for receiving signals from the user input means and for presenting images on the display; the processor having memory;</div> <div class="claim-text">the user input means providing motion requesting signals requesting viewpoint motion and point of interest motion; the user input means being structured so that the user can request viewpoint motion and point of interest motion independently;</div> <div class="claim-text">the processor comprising first means for presenting a first image on the display; the first image including a first surface that is perceptible as viewed from a first viewpoint within a three-dimensional workspace; the first image including a first point of interest on the first surface; the first means further storing viewpoint coordinate data in memory, the viewpoint coordinate data indicating a position of the first viewpoint in the three-dimensional workspace;</div> <div class="claim-text">the processor further comprising second means for receiving a first motion requesting signal set from the user input means, the first motion requesting signal set requesting a first viewpoint motion and a first point of interest motion;</div> <div class="claim-text">the processor further comprising third means for responding to the first motion requesting signal set by presenting a second image on the display; the second image including a second surface that is perceptible as a continuation of the first surface viewed from a second viewpoint within the three-dimensional workspace, the second viewpoint being displaced from the position indicated by the stored viewpoint coordinate data in accordance with the first viewpoint motion; the second image including a second point of interest on the second surface, the second point of interest being displaced in accordance with the first point of interest motion.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="60" class="claim">
      <div class="claim-text">60. The system of claim 59 in which the the user input means comprises a mouse, the user requesting point of interest motion by operating the mouse, the first motion requesting signal set including data indicating mouse operation.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="61" class="claim">
      <div class="claim-text">61. The system of claim 59 in which the user input means comprises a keyboard, the user requesting viewpoint motion by operating the keyboard, the first motion requesting signal set including data indicating keyboard operation.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="62" class="claim">
      <div class="claim-text">62. The system of claim 61 in which the user requests viewpoint motion relative to the second point of interest, the keyboard having a first key and a second key, the user operating the first key to request motion toward the second point of interest, the user operating the second key to request motion away from the second point of interest.</div>
    </div>
    </div> <div class="claim"> <div num="63" class="claim">
      <div class="claim-text">63. A method of operating a system that includes a display, a user input device, and a processor connected for receiving signals from the user input device and for presenting images on the display; the user input device providing region indicating signals indicating regions within images presented and motion requesting signals requesting viewpoint motion; the method comprising steps of:<div class="claim-text">presenting a first image on the display; the first image including a first surface that is perceptible as viewed from a first viewpoint within a three-dimensional workspace; the step of presenting the first image comprising a substep of storing first viewpoint coordinate data indicating a position of the first viewpoint in the three-dimensional workspace.</div> <div class="claim-text">receiving a first region indicating signal and a first motion requesting signal from the user input device; the first region indicating signal indicating a point within a first region on the first surface; the first motion requesting signal requesting viewpoint motion relative to the first region;</div> <div class="claim-text">using the first region indicating signal, the first motion requesting signal, and the first viewpoint coordinate data to obtain second viewpoint coordinate data indicating a position of a second viewpoint in the three-dimensional workspace; the position of the second viewpoint being moved from the position of the first viewpoint relative to the first region on the first surface in accordance with the viewpoint motion requested by the first motion requesting signal; and</div> <div class="claim-text">using the second viewpoint coordinate data to present a second image on the display; the second image including a second surface that is perceptible as a continuation of the first surface viewed from the second viewpoint within the three-dimensional workspace.</div> </div>
    </div>
    </div> <div class="claim"> <div num="64" class="claim">
      <div class="claim-text">64. A method of operating a system that includes a display, a user input device, and a processor connected for receiving signals from the user input device and for presenting images on the display; the user input device providing region indicating signals indicating regions within images presented on the display and motion requesting signals requesting viewpoint motion; the method comprising a sequence of steps, each step comprising substeps of:<div class="claim-text">presenting a respective image on the display; each respective image including a respective surface that is perceptible as being viewed from a respective viewpoint within a three-dimensional workspace; the substep of presenting the respective image comprising a substep of storing respective coordinate data indicating a position of the respective viewpoint in the three-dimensional workspace; and</div> <div class="claim-text">receiving a respective region indicating signal and a respective motion requesting signal from the user input device; each respective region indicating signal indicating a point within a respective region on the respective surface; each respective motion requesting signal requesting viewpoint motion relative to the respective region;</div> <div class="claim-text">the sequence of steps including a first step and a number of following steps, each following step having a next preceding step; each following step further comprising a substep of using the respective region indicating signal, the respective motion requesting signal, and the respective coordinate data stored in the next preceding step to obtain the respective coordinate data stored in the following step; the respective surface of each following step being perceptible as a continuation of the respective surface of the next preceding step; the respective viewpoint of each following step being displaced from the position indicated by the respective coordinate data stored in the next preceding step relative to the respective region of the next preceding step in accordance with the respective region of the next preceding step in accordance with the respective motion requesting signal of the next preceding step.</div> </div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES66458048" lang="EN" load-source="patent-office" class="description">
    <heading>BACKGROUND OF THE INVENTION</heading> <p>The present invention relates to techniques for producing the perception of a moving viewpoint within a three-dimensional space presented on a display.</p>
    <p>Fairchild, K. M., Poltrock, S. E., and Furnas, G. W., "SemNet: Three-Dimensional Graphic Representations of Large Knowledge Bases," in Guindon, R., ed., Cognitive Science and its Applications for Human-Computer Interaction, Lawrence Erlbaum, Hillsdale, N.J., 1988, pp. 201-233, describe SemNet, a three-dimensional graphical interface. SemNet provides semantic navigation techniques such as relative movement, absolute movement, and teleportation. Section 5 discusses navigation and browsing, including viewpoint movement techniques such as moving the viewpoint close to an element that the user needs to inspect. Section 5.2 describes methods for moving the viewpoint to determine the portion of a knowledge base that is displayed. Section 5.2.1 describes relative movement, with independent controls for three orthogonal rotations of the viewpoint and movement forward and backward along the line of sight. Tools for adjusting the velocity of movement and rotation are provided, but relative movement is slow and awkward to use. Section 5.2.2 describes absolute movement in which the user can point to a desired viewpoint location on a map of the three-dimensional knowledge space. The map can have two or three two-dimensional parts, with each part representing a coordinate plane in the space, and the user can manipulate the position of the viewpoint by moving an asterisk in one plane at a time using the mouse. A filter ensures that the viewpoint moves smoothly, retaining the experience of travel through a three-dimensional space. Although absolute movement is quicker and easier to use than relative movement, it is not very accurate and moving the viewpoint in more than one map is confusing. Section 5.2.3 describes teleportation, in which a user can pick a recently visited knowledge element from a menu and instantly move to the location of the knowledge element. Section 5.2.4 describes hyperspace movement, in which the nodes connected to a selected knowledge element are temporarily moved to positions around it, and then snap back to their original positions after a new node is selected.</p>
    <p>Burton, R. R., Sketch: A Drawing Program for Interlisp-D, Xerox Corporation, Palo Alto Research Center. ISL-14, August 1985, pp. 44-49, describes techniques for changing the part of a sketch seen in a window. A sketch is a collection of elements such as lines and text. The sketch has a world coordinate space and the position of each element is given by values in this space. A sketch is viewed and edited inside a window, which shows a region of the coordinate space of a sketch and displays any of the elements that are in the region. The region is determined by the window's scale, its size, and the values of its left and bottom coordinate. As illustrated in FIGS. 59-62, a window's scale can be changed by the Move view command or the Autozoom command. The user can select the Move view command from the command menu and then use the cursor to specify a new portion of the sketch that is to appear in the window by depressing a mouse button at one corner and sweeping the cursor to the other corner; the specified region is scaled to fill the sketch window. The user can select the Autozoom command from the command menu, then move the cursor to the point in the sketch around which zooming will occur, then press one of two buttons to indicate whether to zoom in or zoom out; zooming in makes the image larger but with the point under the cursor in the same location, while zooming out makes the image smaller with the point under the cursor in the same location. The image continues to grow or shrink around the position of the cursor as long as either button is down.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>The present invention provides techniques for operating a system to produce the perception of a moving viewpoint within a three-dimensional workspace. When the user indicates a point of interest on an object, the viewpoint can approach the point of interest asymptotically, with both radial and lateral motion. The orientation of the viewpoint can rotate to keep the point of interest in the field of view. The field of view can also be centered about the point of interest by rotating the viewpoint.</p>
    <p>One aspect of the invention is based on the recognition of a basic problem in moving viewpoint in a three-dimensional workspace. It is frequently desirable to move the viewpoint closer to a specific target. For example, a user may wish to examine a detail of an object at close range. Conventional techniques do not provide an easy way for the user to obtain such viewpoint motion.</p>
    <p>This aspect is further based on the discovery of a user interface technique that solves this problem. The user can indicate a target region and, in response, the viewpoint moves to an appropriate viewing position.</p>
    <p>This technique can be implemented with a pointing device such as a mouse. The user can click a mouse button to indicate a region on the surface of the object to which the pointer is currently pointing. The user can also provide a signal requesting viewpoint motion toward an indicated point in the region, referred to as the "point of interest" or "POI". When the user requests viewpoint motion toward the POI, referred to as a "POI approach", the system can provide animated motion so that object constancy is preserved.</p>
    <p>A related aspect of the invention is based on the recognition of a problem in performing POI approach. If viewpoint motion is rapid, the user has difficulty controlling the motion so that it stops at an appropriate position. But if viewpoint motion is slow, it requires too much time. Conventional viewpoint motion techniques do not handle this conflict satisfactorily.</p>
    <p>This aspect is further based on the discovery that this problem can be solved by performing POI approach asymptotically based on coordinate data indicating the positions of the viewpoint and the POI in the three-dimensional workspace. For example, the viewpoint can move toward the POI along a ray in successively smaller increments that approach a final viewing position asymptotically.</p>
    <p>This solution can be implemented with a logarithmic motion function. During each cycle of animation, the x-, y-, and z- displacements between the current viewpoint position and the POI can be reduced by the same proportional amount, referred to as an approach proportionality constant. As a result, a target object appears to grow at a constant rate of proportionality, making it easy to predict when the viewpoint will reach a desired position. This provides rapid motion initially, then progressively slower motion, allowing the user to control the motion more efficiently by repositioning the POI as the viewpoint nears the target. Also, this implementation provides the perception of natural movement in the three-dimensional workspace. POI approach can be constrained so that the viewpoint does not come too close to the POI.</p>
    <p>Several closely related aspects of the invention are based on the recognition that POI approach does not meet all the viewpoint movement needs of a typical user.</p>
    <p>One problem with simple POI approach is that it does not orient the viewpoint appropriately. This problem can be solved by adjusting the viewpoint, either during POI approach or independent of approach. One way to adjust the viewpoint is to move the viewpoint laterally toward the surface normal at the POI. Another is to rotate the viewpoint to keep the POI at the same position in the field of view or to move it toward the center of the field of view.</p>
    <p>Lateral viewpoint motion has the incidental effect in many cases of moving the POI away from the center of the field of view. This problem can be solved with compensating viewpoint rotation. If the viewpoint is rotated through an angle equal to the angle subtended by lateral viewpoint motion, the POI will stay at the same position in the field of view.</p>
    <p>In many cases, viewpoint motion as described above will nonetheless leave the POI at a substantial distance from the center of the field of view. This problem can be solved with centering viewpoint rotation. At each step, the viewpoint can be rotated up to a maximum viewpoint rotation in order to center the POI. This centering can be performed in addition to viewpoint rotation to compensate for lateral viewpoint motion.</p>
    <p>If these and other types of viewpoint motion are provided in an inappropriate manner, the resulting motion may be awkward and confusing. Specifically, if the user must control too many degrees of freedom, the user may have difficulty obtaining a desired viewpoint motion.</p>
    <p>Another technique is based on the discovery that different types of viewpoint motion can be integrated if the displacement between two steps is a function of distance between the viewpoint and the POI. With this approach, the displacement for each type of viewpoint motion from a given point can be independent of previous viewpoint motion and can be determined with a function that is compatible with other types of viewpoint motion from the same point.</p>
    <p>One example of this approach is the integration of POI approach with viewpoint motion away from the POI, referred to herein as "POI retreat." As described above, POI approach can follow a logarithmic function with an approach proportionality constant. For symmetry between POI approach and retreat, the POI retreat function can be a logarithmic function with a retreat proportionality constant such that each retreating step between two points is equal in length to an approaching step in the opposite direction between the same two points.</p>
    <p>Lateral viewpoint motion can also be integrated with POI approach and retreat to provide motion toward the surface normal at the POI. During each animation cycle, the displacement from POI approach or retreat is used to obtain an intermediate viewpoint; a vector normal to the POI is obtained and a lateral position point on the vector normal is found at a distance equal to the distance from the POI to the intermediate viewpoint; and the ending viewpoint is then found along a line from the intermediate viewpoint to the lateral position point. The line can be an arc or a chord. The displacement from the intermediate viewpoint to the ending viewpoint can be a proportion of the line, found using a lateral proportionality constant. To integrate this lateral motion with POI approach, the lateral proportionality constant should be sufficiently larger than the approach proportionality constant that the viewpoint comes close to the normal before reaching an appropriate distance for viewing the POI.</p>
    <p>Another aspect of the invention is based on the recognition of an underlying problem in viewpoint motion relative to a POI. As viewpoint motion progresses, the user may wish to adjust the POI position, especially during POI approach in which the POI and the surrounding area become progressively larger on the display. The user could adjust POI position by ending viewpoint motion relative to the current POI and by then indicating a new POI and requesting viewpoint motion relative to the new POI. But this would produce an awkward sequence of viewpoint movements.</p>
    <p>This aspect is further based on the discovery of a technique that adjusts POI position without interrupting viewpoint motion. With this technique, the user can produce a desired viewpoint motion while independently adjusting POI position. The user can control viewpoint motion by using keys to select from a few simple choices, such as moving the viewpoint toward the POI, moving the viewpoint away from the POI, or keeping the viewpoint at the previous position; in a lateral mode, each type of radial viewpoint motion can be combined with lateral viewpoint motion, with an additional choice for moving the viewpoint laterally without moving it toward or away from the POI. The user can control POI position using a user input device such as a mouse to indicate changes in position. Independently requesting viewpoint motion and POI position adjustment is especially effective because a typical user can readily make both types of requests at the same time without confusion. For example, the user can use one hand to request viewpoint motion and the other hand to control POI position.</p>
    <p>A closely related aspect of the invention is based on the recognition that POI position adjustment can inadvertently lead to a jump of the POI from one object to another. This problem can be solved by constraining the POI to stay on the same object's surface during a viewpoint movement. This solution can be implemented by presenting a circle or other shape on the object's surface, centered on the POI, to assist the user in positioning the POI. When the user adjusts the POI's position, such as by operating a mouse, another circle is presented at the adjusted position, perceptible as a moved continuation of the previous circle.</p>
    <p>The following description, the drawings and the claims further set forth these and other objects, features and advantages of the invention.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p>FIG. 1 is a schematic view of a surface perceptible from a viewpoint in a three-dimensional workspace.</p>
    <p>FIG. 2A is a schematic view of a presented image that includes the surface shown in FIG. 1.</p>
    <p>FIG. 2B is a schematic view of another presented image that includes a surface that is perceptible as a continuation of the surface of FIG. 2A viewed from a different viewpoint.</p>
    <p>FIG. 3 is a flow chart showing general steps in viewpoint motion.</p>
    <p>FIG. 4A is a plane view showing radial viewpoint motion toward a point on a surface without centering. FIG. 4B is a plane view showing radial viewpoint motion toward a point on a surface with centering.</p>
    <p>FIG. 5 is a plane view showing lateral viewpoint motion with viewpoint rotation and also showing viewpoint motion that includes lateral and radial components.</p>
    <p>FIG. 6 is a three-dimensional view showing viewpoint motion that includes lateral and radial components.</p>
    <p>FIG. 7 is a plane view showing viewpoint motion with point of interest motion.</p>
    <p>FIG. 8 is a flow chart showing general steps in presenting a three-dimensional workspace from a new viewpoint in response to a request for viewpoint motion and point of interest motion.</p>
    <p>FIG. 9 is a block diagram showing components in a system that provides viewpoint motion and point of interest motion.</p>
    <p>FIG. 10 is a flow chart showing steps in an animation loop that provides viewpoint motion and point of interest motion.</p>
    <p>FIG. 11 is a flow chart showing steps in finding a starting point of interest in FIG. 10.</p>
    <p>FIG. 12 is a flow chart showing steps in finding a current point of interest and viewpoint position in FIG. 10.</p>
    <p>FIG. 13 is a flow chart showing steps in radial viewpoint motion in FIG. 12.</p>
    <p>FIG. 14 is a flow chart showing steps in lateral viewpoint motion in FIG. 12.</p>
    <heading>DETAILED DESCRIPTION</heading> <heading>A. Conceptual Framework</heading> <p>The following conceptual framework is helpful in understanding the broad scope of the invention, and the terms defined below have the meanings indicated throughout this application, including the claims. This conceptual framework is a modification and extension of that set forth in copending, coassigned U.S. patent application Ser. No. 07/488,587, entitled "Display of a Workspace with Stretching," incorporated herein by reference.</p>
    <p>A "data processing system" is a system that processes data. A "data processor" or "processor" is any component or system that can process data, and may include one or more central processing units or other processing components.</p>
    <p>"User input means" is means for providing signals based on actions of a user. User input means can include one or more "user input devices" that provide signals based on actions of a user, such as a keyboard or a mouse. The set of signals provided by user input means can therefore include data indicating mouse operation and data indicating keyboard operation.</p>
    <p>An "image" is a pattern of light. An "image output device" is a device that can provide output defining an image. A "display" is an image output device that provides output that defines an image in a visible form. A display may, for example, include a cathode ray tube; an array of light emitting, reflecting, or absorbing elements; a structure that presents marks on paper or another medium; or any other structure capable of defining an image in a visible form. To "present an image" on a display is to operate the display so that a viewer can perceive the image.</p>
    <p>A wide variety of display techniques for data processing systems are available including, for example, various graphical user interfaces, but, despite their diversity, these techniques have certain common characteristics. One fundamental common characteristic is that a display produces human perceptions. In this application, the term "display feature" refers to any human perception produced by a display.</p>
    <p>A "display object" or "object" is a display feature that is perceptible as a coherent unity. An "object surface" or "surface" is a display feature that is perceptible as a surface of a display object; for example, the outer boundary of a three-dimensional display object is a surface. A "region" on a surface is a bounded area of the surface; for example, a single point is the smallest possible region of any surface. A "shape" is a display object that has a distinguishable outline; for example, a circular display object is a shape.</p>
    <p>An image "includes" an object, a surface, a region, or a shape if presentation of the image can produce perception of the object, surface, region, or shape.</p>
    <p>A "workspace" is perceived when objects or other display features in an image are perceived as having positions in a space. A "three-dimensional workspace" is a workspace that is perceptible as extending in three orthogonal dimensions. Typically, a display has a two-dimensional display surface and the perception of a third dimension is produced by visual clues such as perspective lines extending toward a vanishing point; obscuring of distant objects by near objects; size changes in objects moving toward or away from the viewer; perspective shaping of objects; different shading of objects at different distances from the viewer; and so forth. Three-dimensional workspaces include not only workspaces in which all of these cues combine to produce the perception of three dimensions, but also workspaces in which a single cue can produce the perception of three dimensions. For example, a workspace with overlapping display objects or a workspace within which a view can zoom in on an object can be a three-dimensional workspace even though objects within it are presented in orthographic projection, without perspective.</p>
    <p>A three-dimensional workspace is typically perceived as being viewed from a position within the workspace, and this position is the "viewpoint." The viewpoint's "direction of orientation" is the direction from the viewpoint into the field of view along the axis at the center of the field of view.</p>
    <p>In order to present a three-dimensional workspace, a system may store data indicating "coordinates" of the position of an object, a viewpoint, or other display feature in the workspace. Data indicating coordinates of a display feature can then be used in presenting the display feature so that it is perceptible as positioned at the indicated coordinates. The "distance"  between two display features is the perceptible distance between them, and can be determined from their coordinates if they are presented so that they appear to be positioned at their coordinates.</p>
    <p>A signal from a user input device "indicates" a region of a surface if the signal includes data from which the region can be identified. For example, if a signal includes data indicating a mouse pointer displacement, a system can find a point in the display plane based on the previous pointer position. This point can then be used to project a ray from the viewpoint into the three-dimensional workspace being presented, and the coordinates of display features can be used to find the nearest display feature intersected by the ray. The point or a set of points at the intersection can thus be identified as the region.</p>
    <p>A "normal" within a region on a surface is a line that intersects the surface within the region at a right angle. For example, the "horizontal normal" can be defined as a line in a plane parallel to the x-z coordinate plane that is perpendicular to the boundary of the surface in the plane.</p>
    <p>A second display feature is perceptible as a "continuation" of a first display feature when presentation of the second display feature follows presentation of the first display feature in such a way that the user perceives the first display feature as being continued when the second display feature is presented. This can occur when the successive display of two display features is so close in time and space that they appear to be the same display feature. An example of this is the phenomenon called "object constancy."</p>
    <p>An "animation loop" is a repeated operation in which each repetition presents an image and in which objects and other display features in each image appear to be continuations of objects and display features in the next preceding image. If the user is providing signals through a user input means, the signals can be queued as events and each loop can handle some events from the queue.</p>
    <p>A second display feature is perceptible as a "moved continuation" or a "displaced continuation" of a first display feature if it is perceptible as a continuation in a different position. The first display feature is perceived as "moving" or as having "movement" or "motion" or as being "displaced" within a workspace.</p>
    <p>"Viewpoint motion" or "viewpoint displacement" occurs when a sequence of images is presented that are perceptible as views of a three-dimensional workspace from a moving or displaced viewpoint. This perception may result from perception of objects in the workspace as continuations. Viewpoint motion is "relative" to a point or other region of the image if the viewpoint is perceived as moving with respect to the point or other region. A "point of interest" or "POI" is a point indicated by the user and relative to which the viewpoint can move.</p>
    <p>A "displacement" is a distance by which a feature or the viewpoint is perceived as being displaced within a workspace.</p>
    <p>"Radial motion" or "radial displacement" is perceived as motion or displacement along one or more rays. A ray extends from a "radial source."</p>
    <p>The viewpoint can move or be displaced radially toward or away from a radial source in a three-dimensional space, and the radial source can be a POI.</p>
    <p>"Lateral motion" or "lateral displacement" is perceived as motion or displacement in a direction lateral to one or more rays. The viewpoint can move or be displaced laterally in a direction perpendicular to a ray extending from a POI, for example, and the lateral motion can be toward a normal of the POI.</p>
    <p>The viewpoint's direction of orientation "shifts" when it changes by some angle, referred to as a "shift angle." The direction of orientation can shift without viewpoint motion. For example, the direction of orientation can shift by an angle that brings a POI closer to the center of the field of view. Signals from user input means can request motion of the viewpoint and motion of the POI. If the user can request viewpoint and POI motion separately and can request both types of motion simultaneously, the user input means is structured so that the user can request viewpoint motion and POI motion "independently." For example, the user can operate a mouse or other pointing device to request POI motion with one hand and can independently operate keys on a keyboard to request viewpoint motion with the other hand.</p>
    <p>A moving viewpoint is perceived as following or defining a "path" within a workspace. An "asymptotic path" is a path on which the perceived velocity decreases such that the path approaches but does not reach an asymptote.</p>
    <p>When the viewpoint is perceived as following an asymptotic path, the displacements between successive positions follow an "asymptotic function." An example of an asymptotic function is a function in which a logarithm approaches zero asymptotically as time increases. The term "logarithmic function" includes such functions as well as functions that approximate them.</p>
    <p>A "function of a distance" between two points or positions is a function that produces, for each of a set of distances, a set of respective values. For example, one simple logarithmic function of the distance between two points or positions can be obtained by taking a "proportion" of the distance, meaning a part of the distance that is greater than zero but less than the entire distance. A proportion of a distance can be obtained by multiplying the distance by a "proportionality constant," with the proportionality constant having a magnitude greater than zero and less than one. Another example of a function of a distance between first and second points is a function that finds a third point that is at the same distance from the first point as the second point is.</p>
    <p>A "function of a position" is a function that produces, for each of a set of positions, a set of respective values. For example, one simple logarithmic function of a position is a logarithmic function of the distance between the position and another position, as described above in relation to a function of a distance.</p>
    <heading>B. General Features</heading> <p>FIGS. 1-8 illustrate general features of the invention. FIG. 1 shows a surface perceptible in a three-dimensional workspace. FIGS. 2A and 2B show images before and after viewpoint motion. FIG. 3 is a flow chart showing general steps in presenting a sequence of images with viewpoint motion. FIGS. 4A and 4B are plane views showing radial viewpoint motion along an asymptotic path toward a point of interest on a surface, with FIG. 4B also showing a centering operation. FIG. 5 is a plane view showing lateral viewpoint motion along an asymptotic path that follows an arc and also showing viewpoint motion that includes both radial and lateral motion. FIG. 6 is a three-dimensional view showing viewpoint motion that includes both radial and lateral motion, illustrating lateral motion along an asymptotic path that follows a chord. FIG. 7 is a plane view showing radial viewpoint motion with point of interest motion on a surface. FIG. 8 is a flow chart showing steps in viewpoint motion and adjustment of the point's position on the surface.</p>
    <p>FIG. 1 shows surface 10, perceptible as being viewed from viewpoint 14 in a three-dimensional workspace. Viewpoint 14 is shown at the origin of a coordinate system, oriented with its axis of viewing along the z axis. A dashed line extends from viewpoint 14 to point 16 on surface 10. Point 16 is indicated by a circle whose position can be controlled by a user input device such as a mouse.</p>
    <p>FIG. 2A shows image 20, within which surface 10 is perceptible as viewed from viewpoint 14 in a three-dimensional workspace. FIG. 2B shows image 22, with surface 24 including point 26 in a circle. By presenting an appropriate sequence of images, surface 24 can be perceptible as a continuation of surface 10 but viewed from a different viewpoint in the three-dimensional workspace. When a user indicates point 16 and requests viewpoint movement toward point 16, a system presenting image 20 can respond with a sequence of images ending in image 22 so that the user can see point 26, perceptible as a continuation of point 16, and the surrounding area in greater detail.</p>
    <p>FIG. 3 shows general steps a system can perform in presenting such a sequence. The step in box 30 presents the first image of the sequence, including a surface that is perceptible in a three-dimensional workspace. The step in box 32 receives a signal set from a user input device indicating a POI on the surface and requesting viewpoint motion relative to the POI. In response, the step in box 34 presents an image that is perceptible as a view with the viewpoint moved relative to the POI. The image presented in box 34 includes a surface that is perceptible as a continuation of the surface presented in box 30. The step in box 36 receives another signal set, this time requesting both POI and viewpoint motion. The step in box 38 responds by presenting an image that is perceptible as a view with the POI moved and with the viewpoint moved relative to the POI. The image presented in box 38 includes a surface that is perceptible as a continuation of the surface presented in box 30 and the moved POI is perceptible as a moved continuation of the previous POI, with the POI motion occurring on the surface. The steps in boxes 36 and 38 can be repeated until a satisfactory image is obtained.</p>
    <p>FIG. 4A illustrates a technique for moving a viewpoint radially toward an indicated POI on surface 50. Viewpoint 52 is the first in a sequence of viewpoint positions and is oriented with its direction of view along the v axis, an axis defined as the initial direction of view, with its origin at viewpoint 52. In FIG. 4A, the ray along which radial motion occurs extends from the viewpoint through a POI on surface 50.</p>
    <p>In response to a first signal requesting viewpoint motion toward the POI, an image is presented showing object 50 from viewpoint 54, displaced from viewpoint 52 along the ray from viewpoint 52 through the POI. Viewpoint 54 can be displaced toward the POI by a distance that is a proportion of the distance from viewpoint 52 to the POI.</p>
    <p>Similarly, in response to second and third signals requesting further viewpoint motion toward the same POI, images can be presented from viewpoints 56 and 58, displaced along the ray toward the POI by the same proportion. Furthermore, in response to a fourth signal requesting viewpoint motion away from the POI, the viewpoint could be displaced from viewpoint 58 to viewpoint 56, retracing the path followed in approaching the POI.</p>
    <p>Viewpoint motion as in FIG. 4A follows an asymptotic path, because the path approaches but does not reach the POI. Specifically, POI approach along the asymptotic path is initially rapid and then progressively slower, allowing the user to control motion more easily as the POI is viewed at closer range.</p>
    <p>FIG. 4A also illustrates why radial POI approach is not sufficient for satisfactory viewing of the POI: As shown for viewpoint 54, with its direction of view along axis υ' parallel to axis υ, the direction of view does not change during radial POI approach. When viewed from viewpoints 54, 56, and 58, surface 50 is perceptibly closer than from viewpoint 52, but it is poorly oriented for viewing and the POI remains at the periphery of the field of view.</p>
    <p>FIG. 4B illustrates how viewpoint centering can be combined with radial POI approach to provide satisfactory viewing of the POI. Surface 60 and viewpoint 62 correspond to surface 50 and viewpoint 52 in FIG. 4A. Viewpoint 64 corresponds to viewpoint 54, but is partially rotated toward the POI so that the POI is nearer to the center of the field of view. The direction of view of viewpoint 64 is along axis υ' which, rather than being parallel to axis υ, is at an angle between axis υ and the ray from viewpoint 64 to the POI. Viewpoint 66 corresponds to viewpoint 56, but is fully rotated toward the POI so that the POI is at the center of the field of view. Viewpoint 68, corresponding to viewpoint 58, is not rotated any further, so that the POI remains at the center of the field of view. The directions of view of viewpoints 66 and 68 are along axis υ" which is not parallel to axis υ or to axis υ', but rather is along the ray from each viewpoint to the POI.</p>
    <p>The centering illustrated in FIG. 4B can be achieved by determining, at each step, the remaining angle between the direction of gaze and the ray to the POI. If the remaining angle exceeds a maximum single step rotation, the viewpoint is rotated by the single step rotation. Otherwise the viewpoint is rotated by the full remaining angle.</p>
    <p>FIG. 5 shows how lateral viewpoint motion and viewpoint rotation can be combined with viewpoint approach to obtain satisfactory POI viewing. Object 70 is initially viewed from viewpoint 72, with direction of view illustratively along the z axis as shown.</p>
    <p>Lateral viewpoint motion with rotation but without radial motion is illustrated by viewpoints 74, 76, and 78, each displaced toward point 80, which is positioned on the horizontal normal to the POI on surface 70 and, in this special case, is in the same x-z plane as viewpoint 72 and the POI; in the general case, as described below in relation to FIG. 6, the viewpoint is not in the same plane as the POI, so that lateral motion also includes a y component. Viewpoint rotation in the x-z plane maintains the viewer's sense of the vertical. The lateral displacements in FIG. 5 are illustratively along an are, and follow an asymptotic path.</p>
    <p>Viewpoints 74, 76, and 78 also illustrate viewpoint rotation. The direction of view of each of these viewpoints is rotated from the previous viewpoint to keep the POI in the field of view. The viewpoint rotation also includes viewpoint centering as described in relation to FIG. 4, which brings the POI to the center of the field of view, as shown.</p>
    <p>Lateral viewpoint motion with rotation and with radial motion is illustrated by viewpoints 84, 86, and 88. This sequence of viewpoints could occur, for example, if initial viewpoint 72 is in the same x-z plane as the POI and if the POI is not moved during viewpoint motion. In this case, radial viewpoint motion occurs in the same plane as lateral viewpoint motion. In effect, radial and lateral motion components can be combined to provide the viewpoint motion illustrated by viewpoints 84, 86 and 88. The rate of lateral motion can be sufficiently greater than the rate of approach motion that the viewpoint approaches the normal before it approaches the surface, allowing the user to adjust the distance from the surface along the normal.</p>
    <p>FIG. 6 illustrates another example of viewpoint motion that includes radial motion, lateral motion, and viewpoint centering. Surface 100 is perceptible in a three-dimensional workspace. At POI 102, surface 100 has normal 104, and horizontal normal 106 is the projection of normal 104 onto a horizontal plane that includes POI 102.</p>
    <p>In response to a signal requesting viewpoint motion from initial viewpoint 110 toward POI 102, coordinates of intermediate viewpoint 112 are found and used in obtaining coordinates of ending viewpoint 114. In other words, the viewpoint moves from initial viewpoint 110 to ending viewpoint 114 in a single step, with intermediate viewpoint 112 being used for computational purposes.</p>
    <p>The coordinates of intermediate viewpoint 112 are found through a radial displacement from initial viewpoint 110 along the ray from POI 102 through initial viewpoint 110. Initial viewpoint 110 is below the horizontal plane that includes POI 102, so that the radial displacement includes x, y, and z components.</p>
    <p>The coordinates of ending viewpoint 114 are found through a lateral displacement from intermediate viewpoint 112 along a chord. As shown, the chord can connect intermediate viewpoint 112 and normal point 116, a point on horizontal normal 106 which is at the same distance from POI 102 as intermediate viewpoint 112; alternatively, lateral displacement could be along a chord connecting intermediate viewpoint 112 to a point on normal 104 or along an arc as in FIG. 5.</p>
    <p>FIG. 6 shows the projection of viewpoints 110, 112, and 114 and of normal point 116 onto the x-z plane to illustrate how the lateral displacement can be obtained. After the coordinates of intermediate viewpoint 112 are obtained, projection 120 of initial viewpoint 110 is not involved in the computation of lateral displacement. Projection 122 of intermediate viewpoint 112 and projection 126 of point 116 are the endpoints of a projected chord. Projection 124 of ending viewpoint 114 is on the chord, offset from projection 122 by a proportion of the chord. The x and z offsets from projection 122 to projection 124 are the same as the x and z components of the lateral displacement from viewpoint 112 to viewpoint 114.</p>
    <p>The y component of the lateral displacement bears the same proportion to the y offset between viewpoint 112 and normal point 116 as the x and z components bear to the x and z offsets. In obtaining the y component, however, a test may be done to determine whether normal 104 at POI 102 is parallel or nearly parallel to the y axis. If not, the lateral displacement can be applied using the x, y, and z components as described above. But if normal 104 is parallel to the y axis, it may be preferable not to apply a lateral displacement--instead, viewpoint motion can be limited to radial motion and rotation of the viewpoint to bring the POI toward the center of the field of view. This avoids the possibility of a view directly downward or directly upward.</p>
    <p>If the same proportion is used for each of a series of steps, the lateral motion follows an asymptotic path. The function used to obtain the lateral displacement at each step is a logarithmic function. A similar function could be applied to lateral motion along an arc, as in FIG. 5.</p>
    <p>FIG. 7 illustrates viewpoint motion together with point of interest motion. Surface 140 is perceptible in a three-dimensional workspace, and includes POI 142 and POI 144. From initial viewpoint 150, radial motion is requested toward POI 142, so that an image is presented from viewpoint 152 on the ray from POI 142 through viewpoint 150. Then, while the request for radial motion toward the POI continues, a request to move to POI 144 is also received, so that an image is presented from viewpoint 154 on the ray from POI 144 through viewpoint 152.</p>
    <p>FIG. 8 shows steps that can be performed within the steps in boxes 34 and 38 in FIG. 3 to provide viewpoint motion as illustrated in FIGS. 4-7. The step in box 170 begins by obtaining a new ray from a user signal, which can be received in the steps in boxes 32 and 36 in FIG. 3 from a mouse or other user input device that can indicate a ray in a three-dimensional workspace. The new ray can be indicated by a unit vector with the same source as the previous ray, for example, and will ordinarily be close to the direction of the previous ray because movement of a mechanical pointing device by a user is relatively slow compared to the speed of computation.</p>
    <p>The step in box 172 finds a new POI by finding the intersection of the new ray and the surface of a selected object. If the new ray does not intersect the surface of the selected object, the new POI can be the point on the object's surface that is closest to the new ray; the new POI could alternatively be kept within the previously selected object's surface by mapping the two-dimensional signal onto the surface rather than onto the entire display surface, so that the ray would always intersect the surface. In the step in box 32 in FIG. 3, the signal set includes an indication of a newly selected object. In the step in box 36, the selected object is the previously selected object.</p>
    <p>The step in box 180 branches based on a signal selecting a type of viewpoint motion, which can be received in the steps in boxes 32 and 36 in FIG. 3 from keys on a keyboard or mouse. If the signal selects no viewpoint motion, the step in box 182 takes the previous position as the new position. If the signal selects viewpoint motion toward or away from the POI, the step in box 184 branches based on whether the requested motion is toward or away from the POI. If toward the POI, the step in box 186 takes as the new radial position the next position toward the POI on an asymptotic path. If away from the POI, the step in box 188 takes as the new radial position the next position away from the POI on the asymptotic path. If the signal selects lateral motion only, the step in box 190 takes the previous position as the new radial position.</p>
    <p>When the new radial position has been obtained, and if the system is in a mode that includes lateral motion, the step in box 192 takes as the new position the next position on a lateral asymptotic path from the new radial position toward the POI normal. This step also rotates the viewpoint as appropriate, including centering. If the system is not in the lateral mode, the new position is the new radial position from box 186, box 188, or box 190.</p>
    <p>Finally, the step in box 194 presents an image in which the object is perceptible as viewed from a viewpoint at the new position from box 182 or box 192. Then the system returns to box 36 for the next step of viewpoint motion.</p>
    <heading>C. An Implementation</heading> <p>The invention could be implemented on various data processing systems. It has been successfully implemented on a Silicon Graphics Iris workstation that includes the graphics engine option.</p>
    <heading>1. The System</heading> <p>FIG. 9 shows components of a system implementing the invention, including relevant items in memory. System 200 includes processor 202 which is connected for receiving input signals from keyboard and mouse 204 and for presenting images on display 206. Processor 202 operates by accessing program memory 212 to retrieve instructions, which it then executes. During execution of instructions, processor 202 may access data memory 214, in addition to receiving input signals and presenting images.</p>
    <p>Program memory 212 includes operating system 220, which includes instructions for performing graphics operations, all of which is part of the Silicon Graphics Iris workstation with graphics engine. In preparation for an interactive session, processor 202 executes setup and initialization software 222. In the current implementation, processor 202 is set up to execute Common Lisp and Common Lisp Object System code and is initialized with parameters, several of which are mentioned below. The other routines in program memory 212 in FIG. 9 are implemented with Common Lisp Object System classes and methods.</p>
    <p>In response to an appropriate call, processor 202 executes animation loop routine 224, which includes a loop that continues until terminated by an appropriate signal from keyboard and mouse 204. Each cycle of the loop can use double buffer techniques to present a respective image on display 206, with the respective images together forming a sequence such that display features in each image appear to be continuations of display features in the previous image in accordance with object constancy techniques.</p>
    <p>Each animation cycle includes a call to input handling subroutines 226 to receive and handle the next item on a FIFO event queue maintained by operating system 220. The event queue includes signals from the user such as keystrokes, mouse events, mouse pointer movement into or out of a window, and mouse pointer movement reshaping or moving a window, and can also include events from other sources such as from another process.</p>
    <p>Each animation cycle also includes a call to viewpoint motion subroutines 232 to determine the current position of the viewpoint. Then the animation cycle calls 3D workspace subroutines 228 to redraw the three-dimensional workspace. In redrawing the workspace, 3D workspace subroutines 228 call object drawing subroutines 230 to redraw each object in the workspace.</p>
    <p>Data memory 214 includes 3D workspace data structure 240, object data structures 242, viewpoint data structure 244, as well as other data stored and accessed during execution of instructions in program memory 212. 3D workspace data structure 240 can include a list of objects in the workspace and data indicating the extent of the workspace. Object data structures 242 can include, for each object, type data indicating its geometric shape, coordinate data indicating a position within the three-dimensional workspace, extent data indicating a region such as a cube or sphere that includes the object, and a list of other objects that are attached to the object, if any. Viewpoint data structure 244 can include coordinate data indicating a position of the viewpoint within the three-dimensional workspace, data indicating a direction of gaze, and data indicating a direction of body. Together, workspace data structure 240, object data structures 242, and viewpoint data structure 244 provide a model of the workspace and its contents.</p>
    <heading>2. The Animation Loop</heading> <p>Animation loop routine 224 could be implemented in various ways. FIG. 10 shows relevant steps of an animation loop executed in the current implementation of the invention.</p>
    <p>The step in box 260 retrieves the next event from the event queue for handling. The step in box 262 branches based on the next event. If the next event is a signal requesting start of POI flight, implemented as a middle mouse button down click, the step in box 264 performs a pick operation to find the object currently pointed to; sets a current selection variable to indicate that the object pointed to is currently selected; finds the starting POI and, if the selected object is planar, the normal at the POI; and performs other appropriate operations for the newly selected object. The step in box 264 can include accessing object data structures 242 to retrieve coordinate data indicating an object's position. On the other hand, if the next event is a signal requesting end of POI flight, implemented as a middle mouse button up click, the step in box 266 resets the current selection variable to indicate that the object is no longer currently selected. If the next event is another signal, it is handled appropriately in box 268. The step in box 268 may include storing data indicating a key click or other input signal received.</p>
    <p>The step in box 270 finds the current POI and viewpoint position for use in redrawing the workspace and objects, as discussed in greater detail below. In the simplest case, the viewpoint does not move, so that coordinate data indicating the previous viewpoint position can be retrieved by accessing viewpoint data structure 244.</p>
    <p>The step in box 272 draws the three-dimensional workspace for viewing from the current viewpoint. This step can draw the workspace with various cues to promote the perception of three dimensions, including corners, shading, and other visual cues to indicate walls, a ceiling, and a floor. This step can include accessing workspace data structure 240 to retrieve data indicating the extent of the workspace.</p>
    <p>The step in box 280 begins an iterative loop that draws each object. As noted above, workspace data structure 240 includes a list of the objects in the workspace, and this list can be followed by the iterative loop. The step in box 282 performs operations to find the position of the next object on the list and to draw the object at its position. Object data structures 242 can be accessed to retrieve data for each object. The currently selected object can be drawn with a POI circle on the appropriate object's closest surface, centered on the POI. The step in box 280 may include techniques like those described in copending, coassigned U.S. patent application Ser. No. 07/562,048, entitled "Moving an Object in a Three-Dimensional Workspace," incorporated herein by reference.</p>
    <p>When all the objects have been drawn, the step in box 284 switches buffers so that the workspace and objects drawn in boxes 272 and 282 are presented on display 206. Then, the loop returns to its beginning.</p>
    <p>The animation loop can include various additional operations. For example, if the viewpoint is moved into a position so that it bumps against a wall of the workspace, the view of the workspace can be greyed to give a visual cue.</p>
    <heading>3. Finding POI</heading> <p>The step in box 264 in FIG. 10 could be implemented in various ways. FIG. 11 shows general steps in finding the starting POI.</p>
    <p>The step in box 300 begins by getting a set of picked objects. On the Silicon Graphics workstation, the pick and endpick functions can be used to determine whether rendered objects extend into a picking area; objects that extend into the picking area are included in the set of picked objects.</p>
    <p>The step in box 302 reads the current mouse position and uses the two-dimensional coordinates of the position indicated by the mouse to produce data indicating the source and direction of a new ray extending from the viewpoint through the position indicated by the mouse. On the Silicon Graphics workstation, the mapw function can be used to obtain the coordinates of the new ray using the coordinates of the current mouse position. Before calling the mapw function, an appropriate transformation matrix is set up using viewpoint data structure 244. The coordinates returned by the mapw function can then be used to produce a unit vector indicating the direction of the new ray.</p>
    <p>The step in box 310 begins an iterative loop that handles each of the picked objects. The loop begins in box 312 by finding the point where the new ray from box 302 intersects the next picked object. For a spherical object, for example, this can be done by translating the sphere to the origin and similarly translating the source of the ray, then solving for the distance from the ray's source to the sphere's surface using a quadratic equation. The smallest valid solution is taken as the distance, and the unit vector indicating the direction of the new ray is then used to find the components of the coordinates of the intersection point.</p>
    <p>The test in box 314 determines whether the picked object being handled is the first picked object or is closer to the source of the ray than the previous closest picked object. If either condition is met, the step in box 316 sets a value indicating that this picked object is the closest picked object so far and saves the intersection point's coordinates and distance from the ray source.</p>
    <p>When all the picked objects have been handled by the iterative loop, the step in box 320 sets values indicating that the closest object found is the currently selected POI object and that the starting POI is at the coordinates of the intersection point on the currently selected POI object. The step in box 320 can also blank the cursor so that it can be replaced by a POI pattern. If the currently selected POI object is planar, the step in box 320 can also calculate the horizontal normal to the plane at the starting POI and a two-dimensional bounding box for the object. Then the routine continues to the step in box 270 in FIG. 10.</p>
    <heading>4. POI and Viewpoint Motion</heading> <p>Finding the current POI and viewpoint in the step in box 270 in FIG. 10 could be implemented in various ways. FIG. 12 shows steps in finding the current POI and general steps in finding the current viewpoint. FIG. 13 shows steps in radial viewpoint motion. FIG. 14 shows steps in lateral viewpoint motion. The step in box 350 in FIG. 12 begins by branching based on whether a POI object is currently selected. If not, the subroutine ends, but if a POI object is currently selected, the step in box 352 reads the current mouse position and uses the two-dimensional coordinates of the position indicated by the mouse to produce data indicating the source and direction of a new ray extending from the viewpoint through the position indicated by the mouse. On the Silicon Graphics workstation, the mapw function can be used to obtain the coordinates of this ray using the coordinates of the current mouse position. Before calling the mapw function, an appropriate transformation matrix is set up using viewpoint data structure 244. The coordinates returned by the mapw function can then be used to produce a unit vector indicating the direction of the new ray.</p>
    <p>The step in box 360 branches based on whether the current POI object is a complex object, meaning an object that includes a number of attached simple objects. If so, the step in box 362 finds the POI on the simple object closest to the ray from box 352. If the object is already a simple object, the step in box 364 finds the POI on its surface. The POI may be found differently for differently objects, depending on their geometry. For example, if the object is planar, such as a rectangle, and can be described with plane equations, the intersection with a ray can be calculated by parametric substitution. A similar parametric substitution can be used for spherical objects.</p>
    <p>The step in box 370 branches based on whether either of the keys requesting viewpoint motion are depressed. If neither is depressed, the subroutine ends. If either is depressed, the step in box 372 performs radial viewpoint motion as requested. Then, if the step in box 374 determines that the system is in a mode that allows lateral viewpoint motion, the step in box 376 performs appropriate lateral motion before the subroutine ends.</p>
    <p>FIG. 13 shows steps that can be performed to implement the step in box 372 in FIG. 12. The step in box 400 begins by branching on the keys that request viewpoint motion. The space bar can indicate viewpoint motion toward the POI and the left alt key can indicate viewpoint motion away from the POI. If both are depressed, lateral viewpoint motion toward the POI normal can be provided if in a mode allowing lateral motion.</p>
    <p>If the space bar is depressed, indicating motion toward the POI, the step in box 402 finds a new radial position of the viewpoint on an asymptotic path toward the POI. The following equations can be used to obtain new viewpoint coordinates eye<sub>x</sub>, eye<sub>y</sub>, and eye<sub>z</sub> :</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">eye<sub>x</sub> =eye<sub>x</sub> -percentRadialApproach×(eye<sub>x</sub> -poi<sub>x</sub>);</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">eye<sub>y</sub> =eye<sub>y</sub> -percentRadialApproach×(eye<sub>y</sub> -poi<sub>y</sub>); and</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">eye<sub>z</sub> =eye<sub>z</sub> -percentRadialApproach×(eye<sub>z</sub> -poi<sub>z</sub>),</pre>
    
    <p>where percentRadialApproach can be a value stored during initialization. The choice of a value depends on the speed of the animation loop being used; a wide range of values around 0.15 have been used to produce satisfactory motion.</p>
    <p>If the left alt key is depressed, indicating motion away from the POI, the step in box 404 finds a new radial position of the viewpoint on an asymptotic path away from the POI. The following equations can be used to obtain new viewpoint coordinates eye<sub>x</sub>, eye<sub>y</sub>, and eye<sub>z</sub> :</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">eye<sub>x</sub> =eye<sub>x</sub> +percentRadialRetreat×(eye<sub>x</sub> -poi<sub>x</sub>);</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">eye<sub>y</sub> =eye<sub>y</sub> +percentRadialRetreat×(eye<sub>y</sub> -poi<sub>y</sub>); and</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">eye<sub>z</sub> =eye<sub>z</sub> +percentRadialRetreat×(eye<sub>z</sub> -poi<sub>z</sub>),</pre>
    
    <p>where percentRadialRetreat can be a value stored during initialization, and can be chosen such that the asymptotic path away from the POI retraces the asymptotic path toward the POI.</p>
    <p>If both the space bar and the left alt key are depressed, indicating lateral motion only, the step in box 406 sets the new radial position of the viewpoint equal to the previous viewpoint position.</p>
    <p>The step in box 410 tests the new radial position from box 402, box 404, or box 406 to determine whether it is too close to the POI position. This can be done by comparing the distance between the new radial position and the POI position with a minimum distance. If the new radial position is too close, the subroutine continues to box 374 in FIG. 12 without changing the previous viewpoint position. But if the new radial position is far enough away from the POI, the step in box 412 performs a clipping operation on the new radial position and the workspace boundaries and then moves the viewpoint to the clipped new radial position. The step in box 412 could, for example, clip the new radial position with boundaries defined by walls or other display features.</p>
    <p>FIG. 14 shows steps that can be performed to implement the step in box 376 in FIG. 12. The step in box 430 begins by finding the horizontal distance from the POI to the viewpoint, which is simply the square root of the sum of the squares of the differences between the x and z coordinates of the POI and the viewpoint, and by finding a normal point that is on the horizontal normal at the POI and is also the horizontal distance from the POI. The normal point can be obtained by the following equations:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">normal<sub>x</sub> =poi<sub>x</sub> +horzDistance×poiNormal<sub>x</sub> ;</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">normal<sub>y</sub> =poi<sub>y</sub> ; and</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">normal<sub>z</sub> =poi<sub>z</sub> +horzDistance×poiNormal<sub>z</sub>.</pre>
    
    <p>In the general case, the normal point could be on the normal at the POI, so that the following equations could be used:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">normal<sub>x</sub> =poi<sub>x</sub> +distance×poiNormal<sub>x</sub> ;</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">normal<sub>y</sub> =poi<sub>y</sub> +distance×poiNormal<sub>y</sub> ; and</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">normal<sub>z</sub> =poi<sub>z</sub> +distance×poiNormal<sub>z</sub>,</pre>
    
    <p>where distance is the distance from the POI to the viewpoint in three dimensions.</p>
    <p>The step in box 432 tests whether the horizontal distance from box 430 is so small that lateral motion is inappropriate, in which case the step in box 434 sets the new position to the previous viewpoint position.</p>
    <p>The step in box 440 branches based on which mode of lateral viewpoint motion is in effect. In the chord mode, the viewpoint follows a chord as illustrated in FIG. 6. In the arc mode, the viewpoint follows an arc as illustrated in FIG. 5.</p>
    <p>In the chord mode, the step in box 442 sets the new position to a point on the chord between the viewpoint position and the normal point. The viewpoint can follow an asymptotic path along the chord, in which case a logarithmic function can be used to find the new position. The following equations can be used:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">eye<sub>x</sub> =eye<sub>x</sub> +percentLateral×(eye<sub>x</sub> -normal<sub>x</sub>);</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">eye<sub>y</sub> =eye<sub>y</sub> +percentLateral×(eye<sub>y</sub> -normal<sub>y</sub>); and</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">eye<sub>z</sub> =eye<sub>z</sub> +percentLateral×(eye<sub>z</sub> -normal<sub>z</sub>),</pre>
    
    <p>where percentLateral can be a value stored during initialization. The choice of a value depends on the value of percentRadialApproach, because the viewpoint should move to the normal before the desired radial distance is reached. With percentRadialApproach having the value 0.15, the value 0.25 for percentLateral has been used to produce satisfactory lateral motion.</p>
    <p>In the arc mode, the step in box 444 finds the total lateral angle from the viewpoint position to the normal point. This lateral angle, θ, can be found by the equations:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">θ<sub>eye</sub> =atan (eye<sub>x</sub> -poi<sub>x</sub>, eye<sub>z</sub> -poi<sub>z</sub>);</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">θ<sub>normal</sub> =atan (poiNormal<sub>x</sub>, poiNormal<sub>z</sub>); and</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">θ=θ<sub>normal</sub> -θ<sub>eye</sub>,</pre>
    
    <p>in which atan is the arctangent function.</p>
    <p>The step in box 450 then tests whether θ is too large to permit lateral motion to the normal point in a single step. The viewpoint position can follow an asymptotic path along the arc, using the following equation:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">θ=(1-percentRotate)×θ,</pre>
    
    <p>where percentRotate can be a value stored during initialization, as discussed above in relation to percentLateral. Alternatively, θ can be compared with a maximum step angle such as n/10; if θ is too large, the step in box 452 limits θ to the maximum step angle.</p>
    <p>Then, the step in box 454 sets the new position to a point on the arc, which can be done with the following equations:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">eye<sub>x</sub> =poi<sub>x</sub> +horzDistance×sin(θ<sub>eye</sub> +θ);</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">eye<sub>y</sub> =poi<sub>y</sub> ; and</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">eye<sub>z</sub> =poi<sub>z</sub> +horzDistance×cos(θ<sub>eye</sub> +θ).</pre>
    
    <p>For the general case, it is also necessary to calculate a second angle for lateral motion in the y dimension, which can be done with the following equation:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">θ<sub>v</sub> =atan(poiNormal<sub>y</sub>,k)-atan(eye<sub>y</sub> -poi<sub>y</sub>,horzDistance),</pre>
    
    <p>where k is the horizontal distance of the poiNormal vector.</p>
    <p>When the new position has been found in box 434, box 442, or box 454, the step in box 460 clips the new position to a workspace boundary. The workspace can be thought of as a room, so that the new position is clipped to be within the floor, walls, and ceiling of the room. Clipping can also be used when the viewpoint would fall within the region of the workspace occupied by an object, to move the viewpoint outside the object. After clipping, the arctangent function can be used to recalculate the current angle of the viewpoint and the actual lateral angle for use in subsequent calculations.</p>
    <p>The step in box 462 finds a POI rotation angle that compensates for lateral motion. Without compensation, lateral motion moves the POI out from under the mouse cursor and could move the POI outside the field of view, which would be confusing to the user. For rotation in an x-z plane, the POI rotation angle can be the same as the angle θ described above. For the general case, where the viewpoint is also rotated in the y dimension, the POI rotation angle combines θ and θ<sub>v</sub>, calculated as set forth above.</p>
    <p>The step in box 470 tests whether the viewpoint is being moved in a centering mode. In a centering mode, the viewpoint is also rotated to bring the POI to the center of the field of view. If in the centering mode, the step in box 472 finds the centering angle between the orientation of the viewpoint and the direction of a ray from the viewpoint to the POI, which can be done in the x-z plane using the following equation:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">θ=atan(poi<sub>x</sub> -eye<sub>x</sub>, poi<sub>z</sub> -eye<sub>z</sub>)-atan(dob<sub>x</sub>, dob<sub>z</sub>),</pre>
    
    <p>where dob indicates a horizontal direction of body vector of length one. The direction of body vector indicates the direction of the viewpoint in the x-z plane.</p>
    <p>The step in box 474 compares the centering angle from box 472 with a maximum centering angle such as eighteen degrees to determine whether it is too large. If so, the step in box 476 limits the centering angle by setting it to the maximum centering angle.</p>
    <p>When the POI rotation angle and the centering angle have both been found, the step in box 480 determines whether the sum of the two angles, θ<sub>t</sub>, is sufficiently greater than zero to provide meaningful viewpoint rotation. If so, the step in box 482 rotates the viewpoint accordingly. This step can be performed by modifying the direction of body vector using the following equations:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">dob<sub>x</sub> =dob<sub>x</sub> +sinθ<sub>t</sub> </pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">dob<sub>z</sub> =dob<sub>z</sub> +cosθ<sub>t</sub>.</pre>
    
    <p>In an x-z plane, the viewpoint can be rotated by modifying only the dob, because the dob is implemented with x and z components only. For the general case, where the viewpoint is also rotated in the y dimension, the direction of gaze vector, designated dog, also assumed to be of length one, can be modified with the following equation:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">dog<sub>y</sub> =dog<sub>y</sub> +sin (θ<sub>v</sub>),</pre>
    
    <p>where θ<sub>v</sub> is calculated as set forth above. The dog vector can be thought of as a vector from the viewpoint into the field of view that follows the axis of a viewing frustum.</p>
    <p>When the viewpoint has been rotated, the step in box 484 moves the mouse cursor to the resulting position of the POI. This is necessary so that the user can continue to control POI position.</p>
    <heading>D. Miscellaneous</heading> <p>The invention has been described with the use of a mouse to provide signals requesting POI motion and a keyboard to provide signals requesting viewpoint motion. The invention could also be implemented with a multidimensional input device such as a VPL glove to point a ray into a three-dimensional workspace. The same input device could also be used to request viewpoint motion, such as by squeezing to indicate a requested type of motion.</p>
    <p>The invention can be implemented with the animation techniques described in Robertson, G. G., Card, S. K., and Mackinlay, J. D., "The Cognitive Coprocessor Architecture for Interactive User Interfaces," Proceedings of the ACM SIGGRAPH Symposium on User Interface Software and Technology, Williamsburg, Va., Nov. 13-15, 1989, pp. 10-18, incorporated herein by reference.</p>
    <p>Although the invention has been described in relation to various implementations, together with modifications, variations and extensions thereof, other implementations, modifications, variations and extensions are within the scope of the invention. The invention is therefore not limited by the description contained herein or by the drawings, but only by the claims.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4734690">US4734690</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 6, 1987</td><td class="patent-data-table-td patent-date-value">Mar 29, 1988</td><td class="patent-data-table-td ">Tektronix, Inc.</td><td class="patent-data-table-td ">Method and apparatus for spherical panning</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4991022">US4991022</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 20, 1989</td><td class="patent-data-table-td patent-date-value">Feb 5, 1991</td><td class="patent-data-table-td ">Rca Licensing Corporation</td><td class="patent-data-table-td ">Apparatus and a method for automatically centering a video zoom and pan display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5019809">US5019809</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 14, 1990</td><td class="patent-data-table-td patent-date-value">May 28, 1991</td><td class="patent-data-table-td ">University Of Toronto Innovations Foundation</td><td class="patent-data-table-td ">Two-dimensional emulation of three-dimensional trackball</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5021976">US5021976</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 14, 1988</td><td class="patent-data-table-td patent-date-value">Jun 4, 1991</td><td class="patent-data-table-td ">Microelectronics And Computer Technology Corporation</td><td class="patent-data-table-td ">Method and system for generating dynamic, interactive visual representations of information structures within a computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5072412">US5072412</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 25, 1987</td><td class="patent-data-table-td patent-date-value">Dec 10, 1991</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">User interface with multiple workspaces for sharing display system objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5095302">US5095302</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 19, 1989</td><td class="patent-data-table-td patent-date-value">Mar 10, 1992</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Three dimensional mouse via finger ring or cavity</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5103217">US5103217</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 29, 1988</td><td class="patent-data-table-td patent-date-value">Apr 7, 1992</td><td class="patent-data-table-td ">Quantel Limited</td><td class="patent-data-table-td ">Electronic image processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5107443">US5107443</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 7, 1988</td><td class="patent-data-table-td patent-date-value">Apr 21, 1992</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Private regions within a shared workspace</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5124693">US5124693</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 2, 1991</td><td class="patent-data-table-td patent-date-value">Jun 23, 1992</td><td class="patent-data-table-td ">International Business Machines</td><td class="patent-data-table-td ">Three dimensional graphic display with user defined vanishing point</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5129054">US5129054</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 12, 1990</td><td class="patent-data-table-td patent-date-value">Jul 7, 1992</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Specifying 3d reference points in 2d graphic displays</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0353952A2?cl=en">EP0353952A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 27, 1989</td><td class="patent-data-table-td patent-date-value">Feb 7, 1990</td><td class="patent-data-table-td ">The Grass Valley Group, Inc.</td><td class="patent-data-table-td ">Reduced viewport for graphics display</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="MAG+%286D%29%2C"'>MAG (6D),</a>" IRIS-4D Series Man Pages, IRIX User's Reference Manual, Section 6, Release 3.3, Silicon Graphics, Inc., Mountain View, California, Apr. 1990 p. 1.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Window%2C"'>Window,</a>" Graphics Library Programming Guide, IRIS-4D Series, Document Version 2.0, Coordinate Transformations, Document No. 007-1210-020, Silicon Graphics, Inc., Mountain View, California, May 1990, pp. 7-8 and 7-9.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Badler, N. I., Manochehri, K. H. Baraff, D., "<a href='http://scholar.google.com/scholar?q="Multi-Dimensional+Input+Techniques+and+Articulated+Figure+Positioning+by+Multiple+Constraints%2C"'>Multi-Dimensional Input Techniques and Articulated Figure Positioning by Multiple Constraints,</a>" Proceedings, 1986 Workshop on Interactive 3D Graphics, ACM, Oct. 23-24, 1986, pp. 151-169.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Badler, N. I., Manochehri, K. H. Baraff, D., Multi Dimensional Input Techniques and Articulated Figure Positioning by Multiple Constraints, Proceedings, 1986 Workshop on Interactive 3 D Graphics, ACM, Oct. 23 24, 1986, pp. 151 169.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Bier, E. A., Snap Dragging: Interactive Geometric Design in Two and Three Dimensions, Xerox Corporation, EDL 89 2, Sep. 1989, p. 117.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bier, E. A., Snap-Dragging: Interactive Geometric Design in Two and Three Dimensions, Xerox Corporation, EDL-89-2, Sep. 1989, p. 117.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Brooks, Jr., F. P., "<a href='http://scholar.google.com/scholar?q="Walkthrough-A+Dynamic+Graphics+System+for+Simulating+Virtual+Buildings%2C"'>Walkthrough-A Dynamic Graphics System for Simulating Virtual Buildings,</a>" Proceedings, 1986 Workshop on Interactive 3D Graphics, ACM, Oct. 23-24, 1986, pp. 9-21.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Brooks, Jr., F. P., Walkthrough A Dynamic Graphics System for Simulating Virtual Buildings, Proceedings, 1986 Workshop on Interactive 3 D Graphics, ACM, Oct. 23 24, 1986, pp. 9 21.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Burton, R. R., Sketch: A Drawing Program for Interlisp D, Xerox Corporation, Palo Alto Research Center, ISL 14, Aug. 1985, pp. 44 49.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Burton, R. R., Sketch: A Drawing Program for Interlisp-D, Xerox Corporation, Palo Alto Research Center, ISL-14, Aug. 1985, pp. 44-49.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Dalton, R., "<a href='http://scholar.google.com/scholar?q="Beyond+Bifocals%3A+Help+for+Tired+Eyes%2C"'>Beyond Bifocals: Help for Tired Eyes,</a>" Loftus, vol. 4, No. 10, Oct. 1988, pp. 17-18.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Dalton, R., Beyond Bifocals: Help for Tired Eyes, Loftus, vol. 4, No. 10, Oct. 1988, pp. 17 18.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Fairchild, K. M., Poltrock, S. E., and Furnas, G. W., "<a href='http://scholar.google.com/scholar?q="Semi+Net%3A+Three-Dimensional+Graphic+Representations+of+Large+Knowledge+Bases%2C"'>Semi Net: Three-Dimensional Graphic Representations of Large Knowledge Bases,</a>" in Guindon, R., ed., Cognitive Science and its Applications for Human-Computer Interaction, Lawrence Erlbaum, Hillsdale, N.J., 1988, pp. 201-233.</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Fairchild, K. M., Poltrock, S. E., and Furnas, G. W., Semi Net: Three Dimensional Graphic Representations of Large Knowledge Bases, in Guindon, R., ed., Cognitive Science and its Applications for Human Computer Interaction, Lawrence Erlbaum, Hillsdale, N.J., 1988, pp. 201 233.</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Foley, J. D., van Dam, A., Feiner, S. K., Hughes, J. F., Computer Graphics, Second Edition, 1990, Reading, Mass.: Addison Wesley, Chapter 21, pp. 1057 1081.</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Foley, J. D., van Dam, A., Feiner, S. K., Hughes, J. F., Computer Graphics, Second Edition, 1990, Reading, Mass.: Addison-Wesley, Chapter 21, pp. 1057-1081.</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Glassner, A. S., 3 D Computer Graphics, Second Edition, 1989, New York: Design Press, Chapter 14, pp. 177 194.</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Glassner, A. S., 3D Computer Graphics, Second Edition, 1989, New York: Design Press, Chapter 14, pp. 177-194.</td></tr><tr><td class="patent-data-table-td ">19</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Haeberli, P. E., "<a href='http://scholar.google.com/scholar?q="ConMan%3A+A+Visual+Programming+Language+for+Interactive+Graphics%2C"'>ConMan: A Visual Programming Language for Interactive Graphics,</a>" Computer Graphics, vol. 22, No. 4, Aug. 1988, pp. 103-111.</td></tr><tr><td class="patent-data-table-td ">20</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Haeberli, P. E., ConMan: A Visual Programming Language for Interactive Graphics, Computer Graphics, vol. 22, No. 4, Aug. 1988, pp. 103 111.</td></tr><tr><td class="patent-data-table-td ">21</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Herot, C. F., Carling, R., Friedell, M., and Kramlich, D., "<a href='http://scholar.google.com/scholar?q="A+Prototype+Spatial+Data+Management+System%2C"'>A Prototype Spatial Data Management System,</a>" ACM, 1980; pp. 63-70.</td></tr><tr><td class="patent-data-table-td ">22</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Herot, C. F., Carling, R., Friedell, M., and Kramlich, D., A Prototype Spatial Data Management System, ACM, 1980; pp. 63 70.</td></tr><tr><td class="patent-data-table-td ">23</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Hochberg, J., Brooks, V., "<a href='http://scholar.google.com/scholar?q="Film+Cutting+and+Visual+Momentum%2C"'>Film Cutting and Visual Momentum,</a>" in Senders, J. W., Fisher, D. F., and Monty, R. A., Eds. Eye Movements and the Higher Psychological Functions, Hillsdale, N.J.: Lawrence Erlbaum Associates; 1978; pp. 293-313.</td></tr><tr><td class="patent-data-table-td ">24</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Hochberg, J., Brooks, V., Film Cutting and Visual Momentum, in Senders, J. W., Fisher, D. F., and Monty, R. A., Eds. Eye Movements and the Higher Psychological Functions, Hillsdale, N.J.: Lawrence Erlbaum Associates; 1978; pp. 293 313.</td></tr><tr><td class="patent-data-table-td ">25</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">InLarge brochure, Berkeley Systems, Inc., Berkeley, California, 1989.</td></tr><tr><td class="patent-data-table-td ">26</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">MAG (6D), IRIS 4D Series Man Pages, IRIX User s Reference Manual, Section 6, Release 3.3, Silicon Graphics, Inc., Mountain View, California, Apr. 1990 p. 1.</td></tr><tr><td class="patent-data-table-td ">27</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Robertson, G. G., Card, S. K., and Mackinlay, J. D., "<a href='http://scholar.google.com/scholar?q="The+Cognitive+Coprocessor+Architecture+for+Interactive+User+Interfaces%2C"'>The Cognitive Coprocessor Architecture for Interactive User Interfaces,</a>" Proceedings of the ACM SIGGRAPH Symposium on User Interface Software and Technology, Williamsburg, Va., Nov. 13-15, 1989, pp. 10-18.</td></tr><tr><td class="patent-data-table-td ">28</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Robertson, G. G., Card, S. K., and Mackinlay, J. D., The Cognitive Coprocessor Architecture for Interactive User Interfaces, Proceedings of the ACM SIGGRAPH Symposium on User Interface Software and Technology, Williamsburg, Va., Nov. 13 15, 1989, pp. 10 18.</td></tr><tr><td class="patent-data-table-td ">29</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ware, C., Osborne, S., "<a href='http://scholar.google.com/scholar?q="Exploration+and+Virtual+Camera+Control+in+Virtual+Three+Dimensional+Environments%2C"'>Exploration and Virtual Camera Control in Virtual Three Dimensional Environments,</a>" Proceedings, 1990 Symposium on Interactive 3D Graphics Computer Graphics 24, #4, Mar. 1970, pp. 175-183.</td></tr><tr><td class="patent-data-table-td ">30</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Ware, C., Osborne, S., Exploration and Virtual Camera Control in Virtual Three Dimensional Environments, Proceedings, 1990 Symposium on Interactive 3 D Graphics Computer Graphics 24, 4, Mar. 1970, pp. 175 183.</td></tr><tr><td class="patent-data-table-td ">31</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Window, Graphics Library Programming Guide, IRIS 4D Series, Document Version 2.0, Coordinate Transformations, Document No. 007 1210 020, Silicon Graphics, Inc., Mountain View, California, May 1990, pp. 7 8 and 7 9.</td></tr><tr><td class="patent-data-table-td ">32</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Woods, D. D., "<a href='http://scholar.google.com/scholar?q="Visual+Momentum%3A+A+Concept+to+Improve+the+Cognitive+Coupling+of+Person+and+Computer%2C"'>Visual Momentum: A Concept to Improve the Cognitive Coupling of Person and Computer,</a>" Int. J. Man-Machine Studies 21, 1984, pp. 229-244.</td></tr><tr><td class="patent-data-table-td ">33</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Woods, D. D., Visual Momentum: A Concept to Improve the Cognitive Coupling of Person and Computer, Int. J. Man Machine Studies 21, 1984, pp. 229 244.</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5333254">US5333254</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 2, 1991</td><td class="patent-data-table-td patent-date-value">Jul 26, 1994</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Methods of centering nodes in a hierarchical display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5359703">US5359703</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 2, 1990</td><td class="patent-data-table-td patent-date-value">Oct 25, 1994</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Moving an object in a three-dimensional workspace</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5384908">US5384908</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 30, 1991</td><td class="patent-data-table-td patent-date-value">Jan 24, 1995</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Avoiding oscillation in interactive animation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5422987">US5422987</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 19, 1992</td><td class="patent-data-table-td patent-date-value">Jun 6, 1995</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Method and apparatus for changing the perspective view of a three-dimensional object image displayed on a display screen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5513303">US5513303</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 5, 1994</td><td class="patent-data-table-td patent-date-value">Apr 30, 1996</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Moving an object in a three-dimensional workspace</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5557714">US5557714</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 29, 1993</td><td class="patent-data-table-td patent-date-value">Sep 17, 1996</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">In a computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5574836">US5574836</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 22, 1996</td><td class="patent-data-table-td patent-date-value">Nov 12, 1996</td><td class="patent-data-table-td ">Broemmelsiek; Raymond M.</td><td class="patent-data-table-td ">Interactive display apparatus and method with viewer position compensation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5590268">US5590268</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 30, 1994</td><td class="patent-data-table-td patent-date-value">Dec 31, 1996</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">System and method for evaluating a workspace represented by a three-dimensional model</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5608850">US5608850</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 14, 1994</td><td class="patent-data-table-td patent-date-value">Mar 4, 1997</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Transporting a display object coupled to a viewpoint within or between navigable workspaces</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5689628">US5689628</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 14, 1994</td><td class="patent-data-table-td patent-date-value">Nov 18, 1997</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Coupling a display object to a viewpoint in a navigable workspace</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5694533">US5694533</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 19, 1996</td><td class="patent-data-table-td patent-date-value">Dec 2, 1997</td><td class="patent-data-table-td ">Sony Corportion</td><td class="patent-data-table-td ">3-Dimensional model composed against textured midground image and perspective enhancing hemispherically mapped backdrop image for visual realism</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5734384">US5734384</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 15, 1996</td><td class="patent-data-table-td patent-date-value">Mar 31, 1998</td><td class="patent-data-table-td ">Picker International, Inc.</td><td class="patent-data-table-td ">Medical diagnostic imaging system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5742779">US5742779</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 3, 1996</td><td class="patent-data-table-td patent-date-value">Apr 21, 1998</td><td class="patent-data-table-td ">Tolfa Corporation</td><td class="patent-data-table-td ">Method of communication using sized icons, text, and audio</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5790950">US5790950</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 17, 1995</td><td class="patent-data-table-td patent-date-value">Aug 4, 1998</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Computer graphics apparatus having an improved walk-through function</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5808613">US5808613</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 28, 1996</td><td class="patent-data-table-td patent-date-value">Sep 15, 1998</td><td class="patent-data-table-td ">Silicon Graphics, Inc.</td><td class="patent-data-table-td ">Network navigator with enhanced navigational abilities</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5831630">US5831630</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 31, 1995</td><td class="patent-data-table-td patent-date-value">Nov 3, 1998</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Three-dimensional model processing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5832139">US5832139</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 31, 1996</td><td class="patent-data-table-td patent-date-value">Nov 3, 1998</td><td class="patent-data-table-td ">Omniplanar, Inc.</td><td class="patent-data-table-td ">Method and apparatus for determining degrees of freedom of a camera</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5856844">US5856844</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 19, 1996</td><td class="patent-data-table-td patent-date-value">Jan 5, 1999</td><td class="patent-data-table-td ">Omniplanar, Inc.</td><td class="patent-data-table-td ">Method and apparatus for determining position and orientation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5900879">US5900879</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 28, 1997</td><td class="patent-data-table-td patent-date-value">May 4, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Three-dimensional workspace interactive display having browsing viewpoints for navigation and work viewpoints for user-object interactive non-navigational work functions with automatic switching to browsing viewpoints upon completion of work functions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5913918">US5913918</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 12, 1996</td><td class="patent-data-table-td patent-date-value">Jun 22, 1999</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Automotive navigation apparatus and recording medium storing program therefor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5936624">US5936624</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 7, 1997</td><td class="patent-data-table-td patent-date-value">Aug 10, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Data processing system having a logical containment system and method therefor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5969720">US5969720</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 7, 1997</td><td class="patent-data-table-td patent-date-value">Oct 19, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Data processing system and method for implementing an informative container for a file system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5969724">US5969724</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 31, 1997</td><td class="patent-data-table-td patent-date-value">Oct 19, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and system for navigating through opaque structures on a display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5973694">US5973694</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 21, 1998</td><td class="patent-data-table-td patent-date-value">Oct 26, 1999</td><td class="patent-data-table-td ">Chatham Telecommunications, Inc.,</td><td class="patent-data-table-td ">Method of communication using sized icons, text, and audio</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5973697">US5973697</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 27, 1997</td><td class="patent-data-table-td patent-date-value">Oct 26, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and system for providing preferred face views of objects in a three-dimensional (3D) environment in a display in a computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5973700">US5973700</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 16, 1992</td><td class="patent-data-table-td patent-date-value">Oct 26, 1999</td><td class="patent-data-table-td ">Eastman Kodak Company</td><td class="patent-data-table-td ">Method and apparatus for optimizing the resolution of images which have an apparent depth</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6043813">US6043813</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 14, 1995</td><td class="patent-data-table-td patent-date-value">Mar 28, 2000</td><td class="patent-data-table-td ">Raytheon Company</td><td class="patent-data-table-td ">Interactive computerized witness interrogation recording tool</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6052130">US6052130</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 20, 1996</td><td class="patent-data-table-td patent-date-value">Apr 18, 2000</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Data processing system and method for scaling a realistic object on a user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6054985">US6054985</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 27, 1997</td><td class="patent-data-table-td patent-date-value">Apr 25, 2000</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Data processing system and method for simulating compound objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6054996">US6054996</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 20, 1996</td><td class="patent-data-table-td patent-date-value">Apr 25, 2000</td><td class="patent-data-table-td ">Interntional Business Machines Corporation</td><td class="patent-data-table-td ">Data processing system and method for controlling a view of a realistic object in a display device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6064389">US6064389</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 27, 1997</td><td class="patent-data-table-td patent-date-value">May 16, 2000</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Distance dependent selective activation of three-dimensional objects in three-dimensional workspace interactive displays</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6078329">US6078329</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 27, 1996</td><td class="patent-data-table-td patent-date-value">Jun 20, 2000</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Virtual object display apparatus and method employing viewpoint updating for realistic movement display in virtual reality</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6081270">US6081270</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 27, 1997</td><td class="patent-data-table-td patent-date-value">Jun 27, 2000</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and system for providing an improved view of an object in a three-dimensional environment on a computer display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6104406">US6104406</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 4, 1997</td><td class="patent-data-table-td patent-date-value">Aug 15, 2000</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Back away navigation from three-dimensional objects in three-dimensional workspace interactive displays</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6111581">US6111581</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 27, 1997</td><td class="patent-data-table-td patent-date-value">Aug 29, 2000</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and system for classifying user objects in a three-dimensional (3D) environment on a display in a computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6121971">US6121971</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 27, 1997</td><td class="patent-data-table-td patent-date-value">Sep 19, 2000</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and system for providing visual hierarchy of task groups and related viewpoints of a three dimensional environment in a display of a computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6160553">US6160553</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 14, 1998</td><td class="patent-data-table-td patent-date-value">Dec 12, 2000</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Methods, apparatus and data structures for providing a user interface, which exploits spatial memory in three-dimensions, to objects and in which object occlusion is avoided</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6166738">US6166738</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 14, 1998</td><td class="patent-data-table-td patent-date-value">Dec 26, 2000</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Methods, apparatus and data structures for providing a user interface, which exploits spatial memory in three-dimensions, to objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6188405">US6188405</a></td><td class="patent-data-table-td patent-date-value">Sep 14, 1998</td><td class="patent-data-table-td patent-date-value">Feb 13, 2001</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Methods, apparatus and data structures for providing a user interface, which exploits spatial memory, to objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6243093">US6243093</a></td><td class="patent-data-table-td patent-date-value">Sep 14, 1998</td><td class="patent-data-table-td patent-date-value">Jun 5, 2001</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Methods, apparatus and data structures for providing a user interface, which exploits spatial memory in three-dimensions, to objects and which visually groups matching objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6249290">US6249290</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 14, 1998</td><td class="patent-data-table-td patent-date-value">Jun 19, 2001</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Object oriented zooming graphical user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6281877">US6281877</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 27, 1997</td><td class="patent-data-table-td patent-date-value">Aug 28, 2001</td><td class="patent-data-table-td ">British Telecommunications Plc</td><td class="patent-data-table-td ">Control interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6307567">US6307567</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 22, 1997</td><td class="patent-data-table-td patent-date-value">Oct 23, 2001</td><td class="patent-data-table-td ">Richfx, Ltd.</td><td class="patent-data-table-td ">Model-based view extrapolation for interactive virtual reality systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6320582">US6320582</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 6, 1996</td><td class="patent-data-table-td patent-date-value">Nov 20, 2001</td><td class="patent-data-table-td ">Sega Enterprises, Ltd.</td><td class="patent-data-table-td ">Image generation apparatus, image generation method, game machine using the method, and medium</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6346938">US6346938</a></td><td class="patent-data-table-td patent-date-value">Apr 27, 1999</td><td class="patent-data-table-td patent-date-value">Feb 12, 2002</td><td class="patent-data-table-td ">Harris Corporation</td><td class="patent-data-table-td ">Computer-resident mechanism for manipulating, navigating through and mensurating displayed image of three-dimensional geometric model</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6414677">US6414677</a></td><td class="patent-data-table-td patent-date-value">Sep 14, 1998</td><td class="patent-data-table-td patent-date-value">Jul 2, 2002</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Methods, apparatus and data structures for providing a user interface, which exploits spatial memory in three-dimensions, to objects and which visually groups proximally located objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6426757">US6426757</a></td><td class="patent-data-table-td patent-date-value">Mar 4, 1996</td><td class="patent-data-table-td patent-date-value">Jul 30, 2002</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and apparatus for providing pseudo-3D rendering for virtual reality computer user interfaces</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6529210">US6529210</a></td><td class="patent-data-table-td patent-date-value">Mar 23, 1999</td><td class="patent-data-table-td patent-date-value">Mar 4, 2003</td><td class="patent-data-table-td ">Altor Systems, Inc.</td><td class="patent-data-table-td ">Indirect object manipulation in a simulation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6587106">US6587106</a></td><td class="patent-data-table-td patent-date-value">Feb 10, 1998</td><td class="patent-data-table-td patent-date-value">Jul 1, 2003</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Computer graphics apparatus having an improved walk-through function</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6721952">US6721952</a></td><td class="patent-data-table-td patent-date-value">Mar 7, 1997</td><td class="patent-data-table-td patent-date-value">Apr 13, 2004</td><td class="patent-data-table-td ">Roxio, Inc.</td><td class="patent-data-table-td ">Method and system for encoding movies, panoramas and large images for on-line interactive viewing and gazing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6917360">US6917360</a></td><td class="patent-data-table-td patent-date-value">Jun 21, 2002</td><td class="patent-data-table-td patent-date-value">Jul 12, 2005</td><td class="patent-data-table-td ">Schlumberger Technology Corporation</td><td class="patent-data-table-td ">System and method for adaptively labeling multi-dimensional images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6987512">US6987512</a></td><td class="patent-data-table-td patent-date-value">Mar 29, 2001</td><td class="patent-data-table-td patent-date-value">Jan 17, 2006</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">3D navigation techniques</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7042449">US7042449</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 28, 2002</td><td class="patent-data-table-td patent-date-value">May 9, 2006</td><td class="patent-data-table-td ">Autodesk Canada Co.</td><td class="patent-data-table-td ">Push-tumble three dimensional navigation system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7148892">US7148892</a></td><td class="patent-data-table-td patent-date-value">Jun 3, 2005</td><td class="patent-data-table-td patent-date-value">Dec 12, 2006</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">3D navigation techniques</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7168051">US7168051</a></td><td class="patent-data-table-td patent-date-value">Dec 20, 2000</td><td class="patent-data-table-td patent-date-value">Jan 23, 2007</td><td class="patent-data-table-td ">Addnclick, Inc.</td><td class="patent-data-table-td ">System and method to configure and provide a network-enabled three-dimensional computing environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7190365">US7190365</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 6, 2001</td><td class="patent-data-table-td patent-date-value">Mar 13, 2007</td><td class="patent-data-table-td ">Schlumberger Technology Corporation</td><td class="patent-data-table-td ">Method for navigating in a multi-scale three-dimensional scene</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7259778">US7259778</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 13, 2004</td><td class="patent-data-table-td patent-date-value">Aug 21, 2007</td><td class="patent-data-table-td ">L-3 Communications Corporation</td><td class="patent-data-table-td ">Method and apparatus for placing sensors using 3D models</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7269632">US7269632</a></td><td class="patent-data-table-td patent-date-value">Jun 5, 2001</td><td class="patent-data-table-td patent-date-value">Sep 11, 2007</td><td class="patent-data-table-td ">Xdyne, Inc.</td><td class="patent-data-table-td ">Networked computer system for communicating and operating in a virtual reality environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7375649">US7375649</a></td><td class="patent-data-table-td patent-date-value">Aug 24, 2006</td><td class="patent-data-table-td patent-date-value">May 20, 2008</td><td class="patent-data-table-td ">Triangle Software Llc</td><td class="patent-data-table-td ">Traffic routing based on segment travel time</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7413514">US7413514</a></td><td class="patent-data-table-td patent-date-value">Sep 21, 2005</td><td class="patent-data-table-td patent-date-value">Aug 19, 2008</td><td class="patent-data-table-td ">Kabushiki Kaisha Sega Enterprises</td><td class="patent-data-table-td ">Video game machine with rotational mechanism</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7434177">US7434177</a></td><td class="patent-data-table-td patent-date-value">Dec 20, 1999</td><td class="patent-data-table-td patent-date-value">Oct 7, 2008</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">User interface for providing consolidation and access</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7508321">US7508321</a></td><td class="patent-data-table-td patent-date-value">Aug 15, 2006</td><td class="patent-data-table-td patent-date-value">Mar 24, 2009</td><td class="patent-data-table-td ">Triangle Software Llc</td><td class="patent-data-table-td ">System and method for predicting travel time for a travel route</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7522186">US7522186</a></td><td class="patent-data-table-td patent-date-value">Jul 24, 2002</td><td class="patent-data-table-td patent-date-value">Apr 21, 2009</td><td class="patent-data-table-td ">L-3 Communications Corporation</td><td class="patent-data-table-td ">Method and apparatus for providing immersive surveillance</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7526738">US7526738</a></td><td class="patent-data-table-td patent-date-value">Aug 20, 2007</td><td class="patent-data-table-td patent-date-value">Apr 28, 2009</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">User interface for providing consolidation and access</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7557730">US7557730</a></td><td class="patent-data-table-td patent-date-value">May 21, 2007</td><td class="patent-data-table-td patent-date-value">Jul 7, 2009</td><td class="patent-data-table-td ">Triangle Software Llc</td><td class="patent-data-table-td ">GPS-generated traffic information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7570261">US7570261</a></td><td class="patent-data-table-td patent-date-value">Mar 4, 2004</td><td class="patent-data-table-td patent-date-value">Aug 4, 2009</td><td class="patent-data-table-td ">Xdyne, Inc.</td><td class="patent-data-table-td ">Apparatus and method for creating a virtual three-dimensional environment, and method of generating revenue therefrom</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7589732">US7589732</a></td><td class="patent-data-table-td patent-date-value">Nov 5, 2002</td><td class="patent-data-table-td patent-date-value">Sep 15, 2009</td><td class="patent-data-table-td ">Autodesk, Inc.</td><td class="patent-data-table-td ">System and method of integrated spatial and temporal navigation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7633520">US7633520</a></td><td class="patent-data-table-td patent-date-value">Jun 21, 2004</td><td class="patent-data-table-td patent-date-value">Dec 15, 2009</td><td class="patent-data-table-td ">L-3 Communications Corporation</td><td class="patent-data-table-td ">Method and apparatus for providing a scalable multi-camera distributed video processing and visualization surveillance system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7646394">US7646394</a></td><td class="patent-data-table-td patent-date-value">Mar 7, 2005</td><td class="patent-data-table-td patent-date-value">Jan 12, 2010</td><td class="patent-data-table-td ">Hrl Laboratories, Llc</td><td class="patent-data-table-td ">System and method for operating in a virtual environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7667700">US7667700</a></td><td class="patent-data-table-td patent-date-value">Mar 7, 2005</td><td class="patent-data-table-td patent-date-value">Feb 23, 2010</td><td class="patent-data-table-td ">Hrl Laboratories, Llc</td><td class="patent-data-table-td ">System and method for navigating operating in a virtual environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7796134">US7796134</a></td><td class="patent-data-table-td patent-date-value">May 31, 2005</td><td class="patent-data-table-td patent-date-value">Sep 14, 2010</td><td class="patent-data-table-td ">Infinite Z, Inc.</td><td class="patent-data-table-td ">Multi-plane horizontal perspective display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7812841">US7812841</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 29, 2005</td><td class="patent-data-table-td patent-date-value">Oct 12, 2010</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Display controlling apparatus, information terminal unit provided with display controlling apparatus, and viewpoint location controlling apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7818685">US7818685</a></td><td class="patent-data-table-td patent-date-value">Apr 21, 2009</td><td class="patent-data-table-td patent-date-value">Oct 19, 2010</td><td class="patent-data-table-td ">Definiens Ag</td><td class="patent-data-table-td ">Method for navigating between sections in a display space</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7830385">US7830385</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 2008</td><td class="patent-data-table-td patent-date-value">Nov 9, 2010</td><td class="patent-data-table-td ">Kulas Charles J</td><td class="patent-data-table-td ">Script control for gait animation in a scene generated by a computer rendering engine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7880642">US7880642</a></td><td class="patent-data-table-td patent-date-value">Jun 10, 2009</td><td class="patent-data-table-td patent-date-value">Feb 1, 2011</td><td class="patent-data-table-td ">Triangle Software Llc</td><td class="patent-data-table-td ">GPS-generated traffic information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7880738">US7880738</a></td><td class="patent-data-table-td patent-date-value">Jul 14, 2006</td><td class="patent-data-table-td patent-date-value">Feb 1, 2011</td><td class="patent-data-table-td ">Molsoft Llc</td><td class="patent-data-table-td ">Structured documents and systems, methods and computer programs for creating, producing and displaying three dimensional objects and other related information in those structured documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7893935">US7893935</a></td><td class="patent-data-table-td patent-date-value">Nov 30, 2009</td><td class="patent-data-table-td patent-date-value">Feb 22, 2011</td><td class="patent-data-table-td ">Hrl Laboratories, Llc</td><td class="patent-data-table-td ">Method and system for hybrid trackball and immersive navigation in a virtual environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7907167">US7907167</a></td><td class="patent-data-table-td patent-date-value">May 8, 2006</td><td class="patent-data-table-td patent-date-value">Mar 15, 2011</td><td class="patent-data-table-td ">Infinite Z, Inc.</td><td class="patent-data-table-td ">Three dimensional horizontal perspective workstation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7922582">US7922582</a></td><td class="patent-data-table-td patent-date-value">May 28, 2008</td><td class="patent-data-table-td patent-date-value">Apr 12, 2011</td><td class="patent-data-table-td ">Nintendo Co., Ltd.</td><td class="patent-data-table-td ">Hand-held game apparatus and game program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8032843">US8032843</a></td><td class="patent-data-table-td patent-date-value">Mar 24, 2009</td><td class="patent-data-table-td patent-date-value">Oct 4, 2011</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">User interface for providing consolidation and access</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8044953">US8044953</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 28, 2006</td><td class="patent-data-table-td patent-date-value">Oct 25, 2011</td><td class="patent-data-table-td ">Autodesk, Inc.</td><td class="patent-data-table-td ">System for interactive 3D navigation for proximal object inspection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8075401">US8075401</a></td><td class="patent-data-table-td patent-date-value">Mar 5, 2009</td><td class="patent-data-table-td patent-date-value">Dec 13, 2011</td><td class="patent-data-table-td ">Nintendo, Co., Ltd.</td><td class="patent-data-table-td ">Hand-held game apparatus and game program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8102395">US8102395</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 19, 2007</td><td class="patent-data-table-td patent-date-value">Jan 24, 2012</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Display apparatus, image processing apparatus and image processing method, imaging apparatus, and program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8150941">US8150941</a></td><td class="patent-data-table-td patent-date-value">Sep 8, 2011</td><td class="patent-data-table-td patent-date-value">Apr 3, 2012</td><td class="patent-data-table-td ">Xdyne, Inc.</td><td class="patent-data-table-td ">Networked computer system for communicating and operating in a virtual reality environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8199150">US8199150</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 23, 2005</td><td class="patent-data-table-td patent-date-value">Jun 12, 2012</td><td class="patent-data-table-td ">Quonsil Pl. 3, Llc</td><td class="patent-data-table-td ">Multi-level control language for computer animation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8267780">US8267780</a></td><td class="patent-data-table-td patent-date-value">Apr 22, 2005</td><td class="patent-data-table-td patent-date-value">Sep 18, 2012</td><td class="patent-data-table-td ">Nintendo Co., Ltd.</td><td class="patent-data-table-td ">Game console and memory card</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8323104">US8323104</a></td><td class="patent-data-table-td patent-date-value">May 4, 2004</td><td class="patent-data-table-td patent-date-value">Dec 4, 2012</td><td class="patent-data-table-td ">Nintendo Co., Ltd.</td><td class="patent-data-table-td ">Hand-held game apparatus and game program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8337304">US8337304</a></td><td class="patent-data-table-td patent-date-value">Aug 14, 2009</td><td class="patent-data-table-td patent-date-value">Dec 25, 2012</td><td class="patent-data-table-td ">Nintendo Co., Ltd.</td><td class="patent-data-table-td ">Game console</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8358222">US8358222</a></td><td class="patent-data-table-td patent-date-value">Dec 13, 2010</td><td class="patent-data-table-td patent-date-value">Jan 22, 2013</td><td class="patent-data-table-td ">Triangle Software, Llc</td><td class="patent-data-table-td ">GPS-generated traffic information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8371939">US8371939</a></td><td class="patent-data-table-td patent-date-value">Nov 4, 2011</td><td class="patent-data-table-td patent-date-value">Feb 12, 2013</td><td class="patent-data-table-td ">Nintendo Co., Ltd.</td><td class="patent-data-table-td ">Hand-held game apparatus and game program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8417822">US8417822</a></td><td class="patent-data-table-td patent-date-value">Sep 8, 2011</td><td class="patent-data-table-td patent-date-value">Apr 9, 2013</td><td class="patent-data-table-td ">Xdyne, Inc.</td><td class="patent-data-table-td ">Networked computer system for communicating and operating in a virtual reality environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8429245">US8429245</a></td><td class="patent-data-table-td patent-date-value">Sep 8, 2011</td><td class="patent-data-table-td patent-date-value">Apr 23, 2013</td><td class="patent-data-table-td ">Xdyne, Inc.</td><td class="patent-data-table-td ">Networked computer system for communicating and operating in a virtual reality environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8433094">US8433094</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 30, 2003</td><td class="patent-data-table-td patent-date-value">Apr 30, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">System, method and article of manufacture for detecting collisions between video images generated by a camera and an object depicted on a display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8448087">US8448087</a></td><td class="patent-data-table-td patent-date-value">Sep 28, 2010</td><td class="patent-data-table-td patent-date-value">May 21, 2013</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Method for navigating between sections on a display space</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8477139">US8477139</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 29, 2008</td><td class="patent-data-table-td patent-date-value">Jul 2, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Touch screen device, method, and graphical user interface for manipulating three-dimensional virtual objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8508534">US8508534</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 30, 2008</td><td class="patent-data-table-td patent-date-value">Aug 13, 2013</td><td class="patent-data-table-td ">Adobe Systems Incorporated</td><td class="patent-data-table-td ">Animating objects using relative motion</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8515669">US8515669</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 25, 2010</td><td class="patent-data-table-td patent-date-value">Aug 20, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Providing an improved view of a location in a spatial environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8515718">US8515718</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 31, 2008</td><td class="patent-data-table-td patent-date-value">Aug 20, 2013</td><td class="patent-data-table-td ">Siemens Ag</td><td class="patent-data-table-td ">Method and device for visualizing an installation of automation systems together with a workpiece</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8531312">US8531312</a></td><td class="patent-data-table-td patent-date-value">Jul 30, 2012</td><td class="patent-data-table-td patent-date-value">Sep 10, 2013</td><td class="patent-data-table-td ">Triangle Software Llc</td><td class="patent-data-table-td ">Method for choosing a traffic route</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8539085">US8539085</a></td><td class="patent-data-table-td patent-date-value">Sep 8, 2011</td><td class="patent-data-table-td patent-date-value">Sep 17, 2013</td><td class="patent-data-table-td ">Xydne, Inc.</td><td class="patent-data-table-td ">Networked computer system for communicating and operating in a virtual reality environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8564455">US8564455</a></td><td class="patent-data-table-td patent-date-value">Jul 30, 2012</td><td class="patent-data-table-td patent-date-value">Oct 22, 2013</td><td class="patent-data-table-td ">Triangle Software Llc</td><td class="patent-data-table-td ">Generating visual information associated with traffic</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8574069">US8574069</a></td><td class="patent-data-table-td patent-date-value">Oct 23, 2012</td><td class="patent-data-table-td patent-date-value">Nov 5, 2013</td><td class="patent-data-table-td ">Nintendo Co., Ltd.</td><td class="patent-data-table-td ">Hand-held game apparatus and game program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8619072">US8619072</a></td><td class="patent-data-table-td patent-date-value">Mar 4, 2009</td><td class="patent-data-table-td patent-date-value">Dec 31, 2013</td><td class="patent-data-table-td ">Triangle Software Llc</td><td class="patent-data-table-td ">Controlling a three-dimensional virtual broadcast presentation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8626434">US8626434</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 1, 2012</td><td class="patent-data-table-td patent-date-value">Jan 7, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Automatic adjustment of a camera view for a three-dimensional navigation system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8640044">US8640044</a></td><td class="patent-data-table-td patent-date-value">Oct 4, 2011</td><td class="patent-data-table-td patent-date-value">Jan 28, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">User interface for providing consolidation and access</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8640045">US8640045</a></td><td class="patent-data-table-td patent-date-value">Oct 4, 2011</td><td class="patent-data-table-td patent-date-value">Jan 28, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">User interface for providing consolidation and access</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8655980">US8655980</a></td><td class="patent-data-table-td patent-date-value">Aug 17, 2007</td><td class="patent-data-table-td patent-date-value">Feb 18, 2014</td><td class="patent-data-table-td ">Xdyne, Inc.</td><td class="patent-data-table-td ">Networked computer system for communicating and operating in a virtual reality environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8660780">US8660780</a></td><td class="patent-data-table-td patent-date-value">Dec 9, 2011</td><td class="patent-data-table-td patent-date-value">Feb 25, 2014</td><td class="patent-data-table-td ">Pelmorex Canada Inc.</td><td class="patent-data-table-td ">System and method for delivering departure notifications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8667081">US8667081</a></td><td class="patent-data-table-td patent-date-value">Sep 8, 2011</td><td class="patent-data-table-td patent-date-value">Mar 4, 2014</td><td class="patent-data-table-td ">Xdyne, Inc.</td><td class="patent-data-table-td ">Networked computer system for communicating and operating in a virtual reality environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8674996">US8674996</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 2008</td><td class="patent-data-table-td patent-date-value">Mar 18, 2014</td><td class="patent-data-table-td ">Quonsil Pl. 3, Llc</td><td class="patent-data-table-td ">Script control for lip animation in a scene generated by a computer rendering engine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8717359">US8717359</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 2008</td><td class="patent-data-table-td patent-date-value">May 6, 2014</td><td class="patent-data-table-td ">Quonsil Pl. 3, Llc</td><td class="patent-data-table-td ">Script control for camera positioning in a scene generated by a computer rendering engine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8717360">US8717360</a></td><td class="patent-data-table-td patent-date-value">Jun 10, 2010</td><td class="patent-data-table-td patent-date-value">May 6, 2014</td><td class="patent-data-table-td ">Zspace, Inc.</td><td class="patent-data-table-td ">Presenting a view within a three dimensional scene</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8717423">US8717423</a></td><td class="patent-data-table-td patent-date-value">Feb 2, 2011</td><td class="patent-data-table-td patent-date-value">May 6, 2014</td><td class="patent-data-table-td ">Zspace, Inc.</td><td class="patent-data-table-td ">Modifying perspective of stereoscopic images based on changes in user viewpoint</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8718910">US8718910</a></td><td class="patent-data-table-td patent-date-value">Nov 14, 2011</td><td class="patent-data-table-td patent-date-value">May 6, 2014</td><td class="patent-data-table-td ">Pelmorex Canada Inc.</td><td class="patent-data-table-td ">Crowd sourced traffic reporting</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8725396">US8725396</a></td><td class="patent-data-table-td patent-date-value">May 18, 2012</td><td class="patent-data-table-td patent-date-value">May 13, 2014</td><td class="patent-data-table-td ">Pelmorex Canada Inc.</td><td class="patent-data-table-td ">System for providing traffic data and driving efficiency data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8767040">US8767040</a></td><td class="patent-data-table-td patent-date-value">Jan 11, 2012</td><td class="patent-data-table-td patent-date-value">Jul 1, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Method and system for displaying panoramic imagery</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8781718">US8781718</a></td><td class="patent-data-table-td patent-date-value">Jan 28, 2013</td><td class="patent-data-table-td patent-date-value">Jul 15, 2014</td><td class="patent-data-table-td ">Pelmorex Canada Inc.</td><td class="patent-data-table-td ">Estimating time travel distributions on signalized arterials</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8786464">US8786464</a></td><td class="patent-data-table-td patent-date-value">Jan 22, 2013</td><td class="patent-data-table-td patent-date-value">Jul 22, 2014</td><td class="patent-data-table-td ">Pelmorex Canada Inc.</td><td class="patent-data-table-td ">GPS generated traffic information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8786529">US8786529</a></td><td class="patent-data-table-td patent-date-value">May 18, 2011</td><td class="patent-data-table-td patent-date-value">Jul 22, 2014</td><td class="patent-data-table-td ">Zspace, Inc.</td><td class="patent-data-table-td ">Liquid crystal variable drive voltage</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090303231">US20090303231</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 29, 2008</td><td class="patent-data-table-td patent-date-value">Dec 10, 2009</td><td class="patent-data-table-td ">Fabrice Robinet</td><td class="patent-data-table-td ">Touch Screen Device, Method, and Graphical User Interface for Manipulating Three-Dimensional Virtual Objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100332006">US20100332006</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 31, 2008</td><td class="patent-data-table-td patent-date-value">Dec 30, 2010</td><td class="patent-data-table-td ">Siemens Ag</td><td class="patent-data-table-td ">Method and Device for Visualizing an Installation of Automation Systems Together with a Workpiece</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110157235">US20110157235</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 28, 2009</td><td class="patent-data-table-td patent-date-value">Jun 30, 2011</td><td class="patent-data-table-td ">Landmark Graphics Corporation</td><td class="patent-data-table-td ">Method and system of displaying data sets indicative of physical parameters associated with a formation penetrated by a wellbore</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110320116">US20110320116</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 25, 2010</td><td class="patent-data-table-td patent-date-value">Dec 29, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Providing an improved view of a location in a spatial environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120072100">US20120072100</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 22, 2010</td><td class="patent-data-table-td patent-date-value">Mar 22, 2012</td><td class="patent-data-table-td ">Nokia Corporation</td><td class="patent-data-table-td ">Method and apparatus for determining a relative position of a sensing location with respect to a landmark</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130167002">US20130167002</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 9, 2012</td><td class="patent-data-table-td patent-date-value">Jun 27, 2013</td><td class="patent-data-table-td ">Lester F. Ludwig</td><td class="patent-data-table-td ">Surface-Curve Graphical Intersection Tools and Primitives for Data Visualization, Tabular Data, and Advanced Spreadsheets</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130191712">US20130191712</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 6, 2012</td><td class="patent-data-table-td patent-date-value">Jul 25, 2013</td><td class="patent-data-table-td ">Lester F. Ludwig</td><td class="patent-data-table-td ">Surface-Surface Graphical Intersection Tools and Primitives for Data Visualization, Tabular Data, and Advanced Spreadsheets</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130222364">US20130222364</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 29, 2012</td><td class="patent-data-table-td patent-date-value">Aug 29, 2013</td><td class="patent-data-table-td ">Daniel Kraus</td><td class="patent-data-table-td ">Manipulation of User Attention with Respect to a Simulated Field of View for Geographic Navigation Via Constrained Focus on, Perspective Attraction to, and/or Correction and Dynamic Adjustment of, Points of Interest</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=Lrw3BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc345/defs345.htm&usg=AFQjCNF0b52M2HqQQp5rThx3mQ75nwjbGg#C345S427000">345/427</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=Lrw3BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0019200000">G06T19/20</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=Lrw3BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0013200000">G06T13/20</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=Lrw3BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0015000000">G06T15/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=Lrw3BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0003048000">G06F3/048</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=Lrw3BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0003033000">G06F3/033</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=Lrw3BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0011800000">G06T11/80</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=Lrw3BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06T15/20">G06T15/20</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=Lrw3BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06T19/00">G06T19/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=Lrw3BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F3/04815">G06F3/04815</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06T19/00</span>, <span class="nested-value">G06T19/00N</span>, <span class="nested-value">G06F3/0481E</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Nov 23, 2010</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIMS 1-4, 9, 28-30, 41, 42, 52, 54 AND 55 ARE CANCELLED. CLAIMS 5-8, 10-27, 31-40, 43-51, 53 AND 56-64 WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 17, 2009</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090812</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 17, 2005</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 1, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">XEROX CORPORATION, CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE OF PATENTS;ASSIGNOR:JP MORGAN CHASE BANK, N.A.;REEL/FRAME:016408/0016</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20050330</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">XEROX CORPORATION 800 LONG RIDGE ROAD P.O. BOX 160</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE OF PATENTS;ASSIGNOR:JP MORGAN CHASE BANK, N.A. /AR;REEL/FRAME:016408/0016</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 15, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">IP INNOVATION, LLC, ILLINOIS</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">TECHNOLOGY LICENSING CORPORATION, NEVADA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:XEROX CORPORATION;REEL/FRAME:016153/0116</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20041206</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">IP INNOVATION, LLC 500 SKOKIE BLVD., SUITE 585NORT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:XEROX CORPORATION /AR;REEL/FRAME:016153/0116</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 31, 2003</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">JPMORGAN CHASE BANK, AS COLLATERAL AGENT, TEXAS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:XEROX CORPORATION;REEL/FRAME:015134/0476</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20030625</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">JPMORGAN CHASE BANK, AS COLLATERAL AGENT LIEN PERF</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:XEROX CORPORATION /AR;REEL/FRAME:015134/0476B</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:XEROX CORPORATION;REEL/FRAME:15134/476</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">JPMORGAN CHASE BANK, AS COLLATERAL AGENT,TEXAS</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 28, 2002</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">BANK ONE, NA, AS ADMINISTRATIVE AGENT, ILLINOIS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY INTEREST;ASSIGNOR:XEROX CORPORATION;REEL/FRAME:013153/0001</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20020621</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 11, 2001</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 12, 1997</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 2, 1990</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">XEROX CORPORATION, A CORP OF NY, CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST.;ASSIGNORS:MACKINLAY, JOCK;ROBERTSON, GEORGE G.;CARD, STUART K.;REEL/FRAME:005400/0896</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19900802</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U2vLtBEAE8AQp0TTYHtMoWBElkcHQ\u0026id=Lrw3BAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U14iljbIgqbpN0Pi6jf4bIBN0sJxA\u0026id=Lrw3BAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U1luMjFpHG5hKzVgZ_zZqYySwwkBw","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Moving_viewpoint_with_respect_to_a_targe.pdf?id=Lrw3BAABERAJ\u0026output=pdf\u0026sig=ACfU3U0xhKzTY3JuXvzNtfLcB1WujyA0-A"},"sample_url":"http://www.google.com/patents/reader?id=Lrw3BAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>