<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7039767 - Method and system for coherently caching I/O devices across a network - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Method and system for coherently caching I/O devices across a network"><meta name="DC.contributor" content="James Ian Percival" scheme="inventor"><meta name="DC.contributor" content="Superspeed Software, Inc." scheme="assignee"><meta name="DC.date" content="2004-4-8" scheme="dateSubmitted"><meta name="DC.description" content="The cache keeps regularly accessed disk I/O data within RAM that forms part of a computer systems main memory. The cache operates across a network of computers systems, maintaining cache coherency for the disk I/O devices that are shared by the multiple computer systems within that network. Read access for disk I/O data that is contained within the RAM is returned much faster than would occur if the disk I/O device was accessed directly. The data is held in one of three areas of the RAM for the cache, dependent on the size of the I/O access. The total RAM containing the three areas for the cache does not occupy a fixed amount of a computers main memory. The RAM for the cache grows to contain more disk I/O data on demand and shrinks when more of the main memory is required by the computer system for other uses. The user of the cache is allowed to specify which size of I/O access is allocated to the three areas for the RAM, along with a limit for the total amount of main memory that will be used by the cache at any one time."><meta name="DC.date" content="2006-5-2" scheme="issued"><meta name="DC.relation" content="JP:S6436351" scheme="references"><meta name="DC.relation" content="US:3820078" scheme="references"><meta name="DC.relation" content="US:4622631" scheme="references"><meta name="DC.relation" content="US:4755930" scheme="references"><meta name="DC.relation" content="US:4775955" scheme="references"><meta name="DC.relation" content="US:4849879" scheme="references"><meta name="DC.relation" content="US:5025366" scheme="references"><meta name="DC.relation" content="US:5055999" scheme="references"><meta name="DC.relation" content="US:5060144" scheme="references"><meta name="DC.relation" content="US:5062055" scheme="references"><meta name="DC.relation" content="US:5067071" scheme="references"><meta name="DC.relation" content="US:5091846" scheme="references"><meta name="DC.relation" content="US:5136691" scheme="references"><meta name="DC.relation" content="US:5185878" scheme="references"><meta name="DC.relation" content="US:5210865" scheme="references"><meta name="DC.relation" content="US:5241641" scheme="references"><meta name="DC.relation" content="US:5265235" scheme="references"><meta name="DC.relation" content="US:5276835" scheme="references"><meta name="DC.relation" content="US:5282272" scheme="references"><meta name="DC.relation" content="US:5287473" scheme="references"><meta name="DC.relation" content="US:5297269" scheme="references"><meta name="DC.relation" content="US:5301290" scheme="references"><meta name="DC.relation" content="US:5303362" scheme="references"><meta name="DC.relation" content="US:5307506" scheme="references"><meta name="DC.relation" content="US:5323403" scheme="references"><meta name="DC.relation" content="US:5327556" scheme="references"><meta name="DC.relation" content="US:5335327" scheme="references"><meta name="DC.relation" content="US:5347648" scheme="references"><meta name="DC.relation" content="US:5353430" scheme="references"><meta name="DC.relation" content="US:5363490" scheme="references"><meta name="DC.relation" content="US:5369757" scheme="references"><meta name="DC.relation" content="US:5390318" scheme="references"><meta name="DC.relation" content="US:5408653" scheme="references"><meta name="DC.relation" content="US:5426747" scheme="references"><meta name="DC.relation" content="US:5452447" scheme="references"><meta name="DC.relation" content="US:5499367" scheme="references"><meta name="DC.relation" content="US:5566315" scheme="references"><meta name="DC.relation" content="US:5577226" scheme="references"><meta name="DC.relation" content="US:5606681" scheme="references"><meta name="DC.relation" content="US:5787300" scheme="references"><meta name="DC.relation" content="US:5918244" scheme="references"><meta name="DC.relation" content="US:6370615" scheme="references"><meta name="DC.relation" content="US:6651136" scheme="references"><meta name="citation_reference" content="&quot;1.05a Cache Manager&quot;, AFS distributed tilesystem FAQ, http://www.cis.ohio state.edu/hypertext/faw/usenet/afs-faq/faw-doc-14.html."><meta name="citation_reference" content="&quot;1.2.7 Distributed File System&quot;, IBM Book Manager Book Server, 1989, IBM Corporation, 1996."><meta name="citation_reference" content="&quot;5.3.3 HPFS386 Architecture&quot;, IBM Book Manager Book Server, 1989, IBM Corporation; 1996."><meta name="citation_reference" content="&quot;Abstracts of Some of C. Mohan&#39;s Papers and Patents&quot;, IBM Almaden Research, pp. 1-38."><meta name="citation_reference" content="&quot;Cache-Coherency Protocols Keep Data Consistent&quot;, Gallant, J., Electronic Technology for Engineers and Engineering Managers, Mar. 14, 1991."><meta name="citation_reference" content="&quot;Disk Cache Replacement Policies for Network Fileservers&quot;, Willick, D.L., Distributed Computing Systems, 1993 Int&#39;l. Conf., pp. 2-11."><meta name="citation_reference" content="&quot;Diskeeper User&#39;s Guide V2.1-Errata&quot;, Executive Software, Glendale, CA.; Apr. 1989."><meta name="citation_reference" content="&quot;Distributed Shared Memory: A Survey of Issues and Algorithms&quot;, Computer, pp. 52-60, Aug. 1991."><meta name="citation_reference" content="&quot;Executive Software&#39;s Newkeeper&quot;, Executive Software, Glendale, CA.; vol. 4 Issue 4; 1991."><meta name="citation_reference" content="&quot;Executive Software&#39;s Newskeeper&quot;, Executive Software, Glendale, CA., vol. 3, Issue 8, May/Jun. 1990."><meta name="citation_reference" content="&quot;Executive Software&#39;s Newskeeper&quot;, Executive Software, Glendale, CA., vol. 3, Issue 9, Jul./Aug. 1990."><meta name="citation_reference" content="&quot;Executive Software&#39;s Newskeeper&quot;, Executive Software, Glendale, CA., vol. 4, Issue 1, 1991."><meta name="citation_reference" content="&quot;Executive Software&#39;s Newskeeper&quot;, Executive Software, Glendale, CA., vol. 4, Issue 3, 1991."><meta name="citation_reference" content="&quot;Field Test Guide for I/0 Express Dynamic Data Caching for VAX/VMS&quot;, Executive Software, Glendale, CA.&#39;Sep. 1989."><meta name="citation_reference" content="&quot;File System Operation in a VAXcluster Environment&quot;, Chapter 8, VMS File SystemInternals, McCoy, Digital Press, 1990."><meta name="citation_reference" content="&quot;Global File Sharing&quot;, Sun Microsystems, Inc., 1996."><meta name="citation_reference" content="&quot;HPFS and Fat File Systems Description&quot;,http://www.022bbs.com/file-c/tips/DIHPFT.FAX."><meta name="citation_reference" content="&quot;I/0 Express Technical Reports&quot;, Executive Software International, Glendale, CA.; Feb. 1992-Jan. 1993."><meta name="citation_reference" content="&quot;I/0 Express User&#39;s Guide&quot;, Executive Software International, Glendale, CA.; Jun. 1990."><meta name="citation_reference" content="&quot;Linked List Cache Coherence for Scalable Shared Memory Multiprocessors&quot;, Thapar, Manu et al., Parallel Processing, 1993 Symposium, pp. 34-43."><meta name="citation_reference" content="&quot;NFS Performance&quot;, Sun Microsystems, Inc., 1996."><meta name="citation_reference" content="&quot;SuperCache-Open VMS I/O Performance Accelerator&quot;, TurboSystems International s.a., distributed by EEC Systems, Inc., Sudbury, MA. 1993."><meta name="citation_reference" content="&quot;SuperCacheTM V.1.2 User and Installation Guide, a TurboWareTlwl Product&quot;, V1.2-08."><meta name="citation_reference" content="&quot;SuperCacheTNI-Open VMS I/O Performance Accelerator&quot;, Software Product Description SuperCacherM, 1992, 1993 Turbo Systems International s.a."><meta name="citation_reference" content="&quot;The Design and Implementation of a Distributed File System&quot;, Goldstein, Digital TechnicalJournal, No. 5, Sep. 1987."><meta name="citation_reference" content="&quot;The S3.Manufacturing Procedure Scalable Shared MemoryMultiprocessor&quot;, Nowatzyk, Andreas et al., System Sciences, 1994 Ann. Hawaii Int&#39;l. Conf., vol., I, Jan. 4, 1994, pp. 144-153."><meta name="citation_reference" content="&quot;The Stanford Dash Multiprocessor&quot;, Lenoski et al., Computer, IEEE Computer Society, Mar. 1992, pp. 63-79."><meta name="citation_reference" content="&quot;The VAX/VMS Distributed Lock Manager&quot;, Snaman, Jr., William E et al., Digital.Technical Journal, No. 5, Sep. 1987."><meta name="citation_reference" content="&quot;VMS File System Internals&quot; Kirby McCoy, Digital Press, Digital Equipment Corporation, 1990."><meta name="citation_reference" content="&quot;xFS; A Wide Area Mass Storage File System&quot;, Wang, Randolph Y. et al., Workstation Operating Systems, 1993, pp. 71-178."><meta name="citation_reference" content="Armstrong et al. &quot;Oracle7 (TM) Server Administrator&#39;s Guide&quot;, Oracle Corporation, Aug. 1993."><meta name="citation_reference" content="Armstrong et al. &quot;Oracle7 (TM) Server Concepts Manual&quot; Oracle Corporation, Aug. 1993."><meta name="citation_reference" content="Atkinson et al. &quot;Persistant Object Systems&quot;, Tarascon, 1994, pp. 217-234."><meta name="citation_reference" content="Bell, Les, &quot;OS/2 High Performance File System&quot;, http://www.lesbell.com.ati/hpfstest.html."><meta name="citation_reference" content="Bennett et al. &quot;Munin: Shared Memory for Distributed Memory Multiprocessors&quot;, Computer Science Technical Report, Rice University, pp. 1-22, Apr. 1989."><meta name="citation_reference" content="Bobrowski et al. &quot;Oracle7 (TM) Server Administrator&#39;s Guide&quot;, Oracle Corporation, Dec. 1992."><meta name="citation_reference" content="Bobrowski et al. &quot;Oracle7 (TM) Server Concepts Manual&quot;, Oracle Corporation, Dec. 1992."><meta name="citation_reference" content="Bowen, Ted S. &quot;EEC ups ante in VMS disk caching arena with three-tiered package for VAXClusters.&quot; Digital Review, Cahners Publishing Co., Mar. 16, 1992 v9 n6 p6(1)."><meta name="citation_reference" content="Carter et al. &quot;Implementation and Performance of Munin&quot;, Computer Science Laboratory, Rice University, pp. 152-164, 1991."><meta name="citation_reference" content="Carter, B. &quot;Efficient Distributed Shared Memory Based on Multi-Protocol Release Consistency&quot;, Thesis Paper, Rice University, Sep. 1993, 128 pp."><meta name="citation_reference" content="Dahlin et al. &quot;A Quantitative Analysis of Cache Policies for Scalable Network File Systems&quot;, Computer Science Division, University of California ar Berkeley, 10 pp."><meta name="citation_reference" content="Davis, S. &quot;Design of VMS Volume Shadowing Phase II-Host-based Shadowing&quot;, Digital Technical Journal, vol. 3, No. 3, pp. 7-15, 1991."><meta name="citation_reference" content="Defendant Oracle Corporation&#39;s Answer, Jun. 11, 2004, in SuperSpeed Software, Inc., v. Oracle Corporation, U.S. District Court for the District of Southern District of Texas Corpus Christi Division, C-04-214."><meta name="citation_reference" content="Defendant Oracle Corporation&#39;s Original (Corrected) Answer, Jun. 17, 2004, in SuperSpeed Software, Inc., v. Oracle Corporation, U.S. District Court for the District of Southern District of Texas Corpus Christi Division, C-04-214."><meta name="citation_reference" content="Dimmick, S. &quot;Oracle Database Administrator&#39;s Guide Version 6&quot;, Oracle Corporation, Nov. 1988."><meta name="citation_reference" content="Duncan, Ray, &quot;Design Goals and Implementation of the New High Performance File System&quot;, Microsoft Systems Journal, Sep. 1989, vol. 4, No. 5."><meta name="citation_reference" content="Feeley et al. &quot;Implementing Global Memory Management in a Workstation Cluster&quot;, Department of Computer Science and Engineering, University of Washington and DEC Systems Research Center, 11 pp."><meta name="citation_reference" content="Franklin et al. &quot;Global Memory Management in Client-Server DBMS Architectures&quot;, Proceedings of the 18&lt;SUP&gt;th &lt;/SUP&gt;VLDB Conference, pp. 596-609, Aug. 1992."><meta name="citation_reference" content="Gray, Cary G. et al., &quot;Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency&quot;, 1989 ACM 089791-338-3/89/0012/0202."><meta name="citation_reference" content="Howard et al. &quot;Scale and Performance in a Distributed File System&quot;, ACM Transactions on Computer Systems, vol. 6 No. 1, Feb. 1988, pp. 51-81."><meta name="citation_reference" content="Howard, John, &quot;An Overview of the Andrew File System&quot;, USENIX Winter Conference,Feb. 9-12, 1988, Dallas, Texas."><meta name="citation_reference" content="I/O Express Technical Reports, Executive Software International Feb. 1992-Jan. 1993."><meta name="citation_reference" content="I/O Express User&#39;s Guide, Executive Software International Jun. 1, 1990."><meta name="citation_reference" content="Jang, Saqib, &quot;NFS and DFS A Functional Comparison&quot;, AUSPEX Technical Report Apr. 15, 1997."><meta name="citation_reference" content="Johnson &quot;Squeezing Top Performance from an AXP Server&quot;, Johnson, DEC Professional, vol. 12, Issue 4, Apr., 1993."><meta name="citation_reference" content="Kahhaleh, B. &quot;Analysis of Memory Latency Factors and their Impact on KSRI MPP Performance&quot;, Department of Electrical Engineering and Computer Science, University of Michigan, pp. 1-17, Apr. 1993."><meta name="citation_reference" content="Kazar, Michael L., &quot;Synchronization and Caching Issues in the Andrew File System&quot;, USENIX Winter Conference, Feb. 9-12, 1988, Dallas, Texas."><meta name="citation_reference" content="Keleher et al. &quot;TreadMarks: Distributed Shared Memory on Standard Workstations and Operating Systems&quot;, Department of Computer Science, Rice University, 16 pp."><meta name="citation_reference" content="Kent, C. &quot;Cache Coherence in Distributed Systems&quot;, Digital Western Research Laboratory, (C) 1986, 1987, pp. 90."><meta name="citation_reference" content="Kredel et al. &quot;Oracle on the KSRI Parallel Computer&quot;, Computing Center, University of Mannheim, Aug. 1994, pp. 1-8."><meta name="citation_reference" content="Kronenberg et al. &quot;The Vaxcluster Concept: An Overview of a Distributed System&quot;, Digital Technical Journal, No. 5, pp. 7-21."><meta name="citation_reference" content="Kumar, Puneet et al., &quot;Log-Based Directory Resolution in the Coda File System&quot;, 1993 IEEE 0-8186-3330-1/93."><meta name="citation_reference" content="Larus J. &quot;Compiling for Shared-Memory and Message-Passing Computer&quot;, Computer Sciences Department, University of Wisconsin, Nov. 12, 1993, pp. 1-14."><meta name="citation_reference" content="Leach et al. &quot;The File System of an Integrated Local Network&quot;, ACM, (C) 1985 pp. 309-324."><meta name="citation_reference" content="Levy et al. &quot;Distributed File Systems: Concepts and Examples&quot;, ACM Computing Surveys, vol. 22, No. 4, Dec. 1990, pp. 321-374."><meta name="citation_reference" content="Li et al. &quot;Memory Coherence in Shared Virtual Memory Systems&quot;, ACM (C) 1986, pp. 229-239."><meta name="citation_reference" content="Li et al. &quot;Memory Coherence in Shared Virtual Memory Systems&quot;, ACM Transactions on Computer Systems, vol. 7, No. 4, pp. 321-359, Nov. 1989."><meta name="citation_reference" content="Macklem, Rick, &quot;Lessons Learned Tuning the 4.3BSD Reno Implementation of the NFS Protocol&quot;, USENIX, Winter&#39;91, Dallas, Texas."><meta name="citation_reference" content="McKusick, Marshall et al., &quot;The Network Filesystem&quot;, Chapter 9 of &quot;The Design and Implementation of the 4ABSD Operating System&quot;, 1996."><meta name="citation_reference" content="Mena, Agustin et al., &quot;Performance Characteristics of the DCE Distributed File Service&quot;, IBM Distributed Computing Environment White Paper."><meta name="citation_reference" content="Mogul, Jeffrey C., &quot;A Recovery Protocol for Spritely NFS&quot;, USENIX Association File Systems Workshop, May 21-22, 1992, Ann Arbor, Michigan."><meta name="citation_reference" content="Mohan et al. &quot;Algorithms for Flexible Space Management in Transaction Systems Supporting Fine-Granularity Locking&quot;, Advances in Database Technology-EDBT &#39;94, 4&lt;SUP&gt;th &lt;/SUP&gt;International Conference on Extending Database Technology, pp. 131-144, Mar. 1994."><meta name="citation_reference" content="Mohan et al. &quot;Efficient Locking and Caching of Data in the Multisystem Shared Disks Transaction Environment&quot;, Advances in Database Technology-EDBT &#39;92 3&lt;SUP&gt;rd &lt;/SUP&gt;International Conference on Extending Database Technology, pp. 453-468, Mar. 1992."><meta name="citation_reference" content="Mohan et al. &quot;Efficient Locking and Caching of Data in the Multisystem Shared Disks Transaction Environment&quot;, IBM Research Report, Aug. 21, 1991, pp. 1-15."><meta name="citation_reference" content="Mohan et al. &quot;Recovery and Coherency-Control Protocols for Fast Intersystem Page Transfer and Fine-Granularity Locking in a Shared Disks Transaction Environment&quot;, IBM Research Report, Mar. 15, 1991, pp. 1-31."><meta name="citation_reference" content="Mohan, et al. &quot;Solutions to Hot Spot Problems in a Shared Disks Transaction Environment&quot; IBM Research Report, Aug. 5, 1991, pp. 453-468."><meta name="citation_reference" content="Molesky et al. &quot;Efficient Locking for Shared Memory Database Systems&quot;, Department of Computer Science, University of Massachusetts, Mar. 1994, 28 pp."><meta name="citation_reference" content="Morris et al. &quot;Andrew: A Distributed Personal Computing Environment&quot;, Communications of the ACM, vol. 29, No. 3, Mar. 1986, pp. 184-201."><meta name="citation_reference" content="Nelson et al. &quot;Caching in the Sprite Network File System&quot; ACM Transactions on Computer Systems, vol. 6, No. 1, Feb. 1988, pp. 134-154."><meta name="citation_reference" content="OS/2&#39;s History and Purposehttp://sunsite.nus.sg/pub/os2/phamiacy/Should-Mstory-andPurpose.html."><meta name="citation_reference" content="PC Tech, &quot;Understanding the OS/2 CONFIG-SYS File&quot;, PC Magazine."><meta name="citation_reference" content="Pink et al. &quot;Low Latency File Access in a High Bandwidth Environment&quot;, pp. 117-122."><meta name="citation_reference" content="Pirotte et al. &quot;Advanced in Database Technology-EDBT &#39;92&quot;, 3&lt;SUP&gt;rd &lt;/SUP&gt;International Conference on Extending Database Technology, Vienna, Austria, Mar. 23-27, 1992."><meta name="citation_reference" content="Plaintiff SuperSpeed Software, Inc.&#39;s Complaint, Apr. 14, 2004, in SuperSpeed Software, Inc., v. Oracle Corporation, U.S. District Court for the District of Southern District of Texas Corpus Christi Division, C-04-214."><meta name="citation_reference" content="Raina &quot;Virtual Shared Memory: A Survey of Techniques and Systems&quot;, Department of Computer Science, University of Bristol, United Kingdom, pp. 1-31."><meta name="citation_reference" content="Satyanarayanan, M &quot;On the Influence of Scale in a Distributed System&quot;, IEEE, (C) 1988, pp. 10-18."><meta name="citation_reference" content="Satyanarayanan, M &quot;The Influence of Scale on Distributed File System Design&quot;, IEEE, vol. 18, No. 1 (C) Jan. 1992, pp. 1-8."><meta name="citation_reference" content="Satyanarayanan, M. &quot;Scalable, Secure, and Highly Available Distributed File System&quot;, IEEE (C) 1990, pp. 9-21."><meta name="citation_reference" content="Seltzer, Margo et al., &quot;An Implementation of a Log-Structured File System for UNIX&quot;, 1993 Winter USENIX, Jan. 25-29, 1993, San Diego, California."><meta name="citation_reference" content="Sinniah, Raymond R., &quot;An Introduction to the Andrew File System&quot; http://homepages.uel.ac.uk/5291 n/afs/afsdoc.html."><meta name="citation_reference" content="Snaman Jr., et al. &quot;The VAX/VMS Distributed Lock Manager&quot;, Digital Technical Journal, No. 5, pp. 29-44, Sep. 1987."><meta name="citation_reference" content="Snaman Jr., W. &quot;Application Design in a VAXcluster System&quot;, Digital Technical Journal, vol. 3, pp. 1-10, 1991."><meta name="citation_reference" content="Steven et al. &quot;The KSRI: Bridging the Gap Between Shared Memory and MPP&#39;s&quot;, IEEE (C) 1993, pp. 285-294."><meta name="citation_reference" content="SuperCache-Open VMS I/O Performance Accelerator, 1992, 1993."><meta name="citation_reference" content="Tanenbaum, Andrew S., &quot;Operating Systems: Design and Implementation&quot;, Prentice-Hall, Inc., 1987."><meta name="citation_reference" content="Transarc Corporation, &quot;The AFS File System in Distributed Computing Environments&quot;, May 1, 1996."><meta name="citation_reference" content="Tseng et al. &quot;Parallel Database Processing on the KSRI Computer&quot;, ACM (C) 1993, pp. 453-455."><meta name="citation_reference" content="TURBOCACHET/TURBODISKTM Software Product Description, EEC Systems, Incorporated, Revised Feb. 24, 1992."><meta name="citation_reference" content="TURBOCACHETM Software Product Description, EEC Systems, Incorporated, Revised:Feb. 24, 1992."><meta name="citation_reference" content="TURBOCACHETM/TURBODISK Software Installation and User&#39;s Guide, EEC Systems Incorporated, Feb. 24, 1992."><meta name="citation_reference" content="TURBOCACHETM/TURBODISKT, Cover Letter and Release Notes (*read me first*), EEC Systems, Incorporated, Feb. 24, 1992."><meta name="citation_reference" content="TURBOCACHETM/TURBODISKTm Quick Start Guide, EEC Systems Incorporated, Feb. 24, 1992."><meta name="citation_patent_number" content="US:7039767"><meta name="citation_patent_application_number" content="US:10/709,040"><link rel="canonical" href="http://www.google.com/patents/US7039767"/><meta property="og:url" content="http://www.google.com/patents/US7039767"/><meta name="title" content="Patent US7039767 - Method and system for coherently caching I/O devices across a network"/><meta name="description" content="The cache keeps regularly accessed disk I/O data within RAM that forms part of a computer systems main memory. The cache operates across a network of computers systems, maintaining cache coherency for the disk I/O devices that are shared by the multiple computer systems within that network. Read access for disk I/O data that is contained within the RAM is returned much faster than would occur if the disk I/O device was accessed directly. The data is held in one of three areas of the RAM for the cache, dependent on the size of the I/O access. The total RAM containing the three areas for the cache does not occupy a fixed amount of a computers main memory. The RAM for the cache grows to contain more disk I/O data on demand and shrinks when more of the main memory is required by the computer system for other uses. The user of the cache is allowed to specify which size of I/O access is allocated to the three areas for the RAM, along with a limit for the total amount of main memory that will be used by the cache at any one time."/><meta property="og:title" content="Patent US7039767 - Method and system for coherently caching I/O devices across a network"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("VJ7tU7zbD4XjsASykYD4Aw"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("LUX"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("VJ7tU7zbD4XjsASykYD4Aw"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("LUX"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7039767?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7039767"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=fxtzBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7039767&amp;usg=AFQjCNEMSTBqI24fsru6H8MVBPgTagvR_A" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7039767.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7039767.pdf"></a><a class="appbar-application-grant-link" data-label="Application" href="/patents/US20040186958"></a><a class="appbar-application-grant-link" data-selected="true" data-label="Grant" href="/patents/US7039767"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7039767" style="display:none"><span itemprop="description">The cache keeps regularly accessed disk I/O data within RAM that forms part of a computer systems main memory. The cache operates across a network of computers systems, maintaining cache coherency for the disk I/O devices that are shared by the multiple computer systems within that network. Read access...</span><span itemprop="url">http://www.google.com/patents/US7039767?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7039767 - Method and system for coherently caching I/O devices across a network</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7039767 - Method and system for coherently caching I/O devices across a network" title="Patent US7039767 - Method and system for coherently caching I/O devices across a network"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7039767 B2</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 10/709,040</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">May 2, 2006</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Apr 8, 2004</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">May 6, 1994</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Lapsed</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US5577226">US5577226</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5918244">US5918244</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6370615">US6370615</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6651136">US6651136</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7017013">US7017013</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7111129">US7111129</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20020069323">US20020069323</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20040078429">US20040078429</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20040186958">US20040186958</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20050066123">US20050066123</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20060294318">US20060294318</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">10709040, </span><span class="patent-bibdata-value">709040, </span><span class="patent-bibdata-value">US 7039767 B2, </span><span class="patent-bibdata-value">US 7039767B2, </span><span class="patent-bibdata-value">US-B2-7039767, </span><span class="patent-bibdata-value">US7039767 B2, </span><span class="patent-bibdata-value">US7039767B2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22James+Ian+Percival%22">James Ian Percival</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Superspeed+Software,+Inc.%22">Superspeed Software, Inc.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7039767.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7039767.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7039767.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (43),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (102),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (1),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (19),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (8)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7039767&usg=AFQjCNHfuy5zrsy7s3Pxsx4wZrm_yg1ESQ">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7039767&usg=AFQjCNFKoQTXHZHx302UMT1S2pIKX6_Ztw">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7039767B2%26KC%3DB2%26FT%3DD&usg=AFQjCNHI11g-iMghHi_qw5PoDqF0U5kmQA">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55562704" lang="EN" load-source="patent-office">Method and system for coherently caching I/O devices across a network</invention-title></span><br><span class="patent-number">US 7039767 B2</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA50968247" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">The cache keeps regularly accessed disk I/O data within RAM that forms part of a computer systems main memory. The cache operates across a network of computers systems, maintaining cache coherency for the disk I/O devices that are shared by the multiple computer systems within that network. Read access for disk I/O data that is contained within the RAM is returned much faster than would occur if the disk I/O device was accessed directly. The data is held in one of three areas of the RAM for the cache, dependent on the size of the I/O access. The total RAM containing the three areas for the cache does not occupy a fixed amount of a computers main memory. The RAM for the cache grows to contain more disk I/O data on demand and shrinks when more of the main memory is required by the computer system for other uses. The user of the cache is allowed to specify which size of I/O access is allocated to the three areas for the RAM, along with a limit for the total amount of main memory that will be used by the cache at any one time.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(48)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00018.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00018.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00019.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00019.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00020.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00020.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00021.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00021.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00022.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00022.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00023.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00023.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00024.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00024.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00025.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00025.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00026.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00026.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00027.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00027.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00028.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00028.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00029.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00029.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00030.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00030.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00031.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00031.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00032.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00032.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00033.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00033.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00034.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00034.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00035.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00035.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00036.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00036.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00037.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00037.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00038.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00038.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00039.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00039.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00040.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00040.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00041.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00041.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00042.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00042.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00043.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00043.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00044.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00044.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00045.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00045.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00046.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00046.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7039767B2/US07039767-20060502-D00047.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7039767B2/US07039767-20060502-D00047.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(82)</span></span></div><div class="patent-text"><div mxw-id="PCLM9004143" lang="EN" load-source="patent-office" class="claims">
  <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
    <div class="claim-text">1. A computer network that comprises:
<div class="claim-text">one or more I/O devices on which data may be stored in files; and</div>
<div class="claim-text">multiple computers coupled together, with each computer including a system memory having a plurality of caches with different bucket sizes created by cache software,</div>
<div class="claim-text">wherein each of the multiple computers is configured to cache data from the one or more I/O devices in the plurality of caches, and</div>
<div class="claim-text">wherein each cache in the plurality of caches is capable of caching data from multiple files stored on the one or more I/O devices.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
    <div class="claim-text">2. The computer network of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each cache in the plurality of caches stores data from multiple files.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
    <div class="claim-text">3. The computer network of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more I/O devices include at least one hard disk drive.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
    <div class="claim-text">4. The computer network of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each computer is configured to cache said data in system memory using at least three caches with different bucket sizes.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
    <div class="claim-text">5. The computer network of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein additional computers may join the network, and wherein each of the multiple computers is configured to monitor which computers in the network are configured to cache said data.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
    <div class="claim-text">6. The computer network of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein each of the multiple computers is configured to determine a remote connection address to only those other computers in the network that are configured to cache said data.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
    <div class="claim-text">7. The computer network of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein each of the multiple computers is further configured to send and receive targeted messages for maintaining cache coherency.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
    <div class="claim-text">8. The computer network of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein each of the multiple computers is further configured to determine a list of computers in the network that are configured to cache said data, and configured to update the list when additional computers join the network.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00009" num="00009" class="claim">
    <div class="claim-text">9. A computer network that comprises:
<div class="claim-text">one or more I/O devices capable of storing data in a file; and</div>
<div class="claim-text">multiple computers coupled together, with each computer including a system memory having multiple caches with different bucket sizes created by cache software, and</div>
<div class="claim-text">wherein each of the multiple caches is capable of caching data from said file.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
    <div class="claim-text">10. The computer network of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein each computer caches said data in system memory using at least three caches with different bucket sizes.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
    <div class="claim-text">11. The computer network of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein computers may join the network, and wherein each of said multiple computers monitors which computers in the network are configured to cache said data.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
    <div class="claim-text">12. The computer network of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein each of the multiple computers determine a remote connection address to only those other computers in the network that are configured to cache said data.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
    <div class="claim-text">13. The computer network of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein each of the multiple computers sends and receives targeted messages for maintaining cache coherency.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00014" num="00014" class="claim">
    <div class="claim-text">14. A computer program product comprising: at least one computer usable medium having a computer readable cache program thereon, the cache program to be executed by each of multiple computers coupled together in a network with at least one I/O device on which data may be stored in files, each of said multiple computers having an associated system memory, wherein the cache program comprises:
<div class="claim-text">code to create in the associated system memory at least two caches having different bucket sizes, wherein each cache is capable of caching data from multiple files stored on said I/O device; and</div>
<div class="claim-text">a routine that configures the computer to use said caches to cache data from said I/O device into caches of suitable bucket size.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
    <div class="claim-text">15. The cache program of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising:
<div class="claim-text">code that configures the computer to maintain cache coherency by using targeted messages to invalidate remotely cached copies of data that has been modified.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
    <div class="claim-text">16. The cache program of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising:
<div class="claim-text">code that configures the computer to determine which computers in the network are configured to cache said data.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
    <div class="claim-text">17. The cache program of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising:
<div class="claim-text">code that configures the computer to determine a list of computers that are configured to cache said data, and further configures the computer to update the list after a computer that is configured to cache said data joins the network.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
    <div class="claim-text">18. The cache program of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising:
<div class="claim-text">code that configures the computer to determine a remote connection address for each computer in the list.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
    <div class="claim-text">19. The cache program of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the at least one I/O device comprises at least one hard disk drive.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
    <div class="claim-text">20. The cache program of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the cache program configures each of the multiple computers to create in their associated system memory at least three caches having different bucket sizes.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00021" num="00021" class="claim">
    <div class="claim-text">21. A computer program product comprising: at least one computer usable medium having a computer readable cache program thereon, the cache program to be executed by each of multiple computers coupled together in a network with at least one I/O device on which data may be stored in files, each of said multiple computers having an associated system memory, wherein the cache program comprises:
<div class="claim-text">code to create in the associated system memory at least two caches having different bucket sizes, wherein each cache is capable of caching data from a given file stored on said I/O device; and</div>
<div class="claim-text">a routine that configures the computer to use said caches to cache data from said I/O device into a cache of suitable bucket size.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00022" num="00022" class="claim">
    <div class="claim-text">22. The cache program of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising:
<div class="claim-text">code that configures the computer to maintain cache coherency by using targeted messages to invalidate remotely cached copies of data that has been modified.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00023" num="00023" class="claim">
    <div class="claim-text">23. The cache program of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising:
<div class="claim-text">code that configures the computer to determine which computers in the network are configured to cache said data.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00024" num="00024" class="claim">
    <div class="claim-text">24. The cache program of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising:
<div class="claim-text">code that configures the computer to determine a list of computers that are configured to cache said data, and further configures the computer to determine a remote connection address for each computer in the list.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00025" num="00025" class="claim">
    <div class="claim-text">25. A method of caching data on a network from a shared I/O device that is coupled to a computer that includes a system memory, wherein the shared I/O device stores said data in multiple files, wherein the method comprises:
<div class="claim-text">creating in system memory through use of cache software at least two caches with different bucket sizes; and</div>
<div class="claim-text">caching in each of the two caches data from multiple files stored on the shared I/O device.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00026" num="00026" class="claim">
    <div class="claim-text">26. The method of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein the I/O device is a hard disk drive.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00027" num="00027" class="claim">
    <div class="claim-text">27. The method of <claim-ref idref="CLM-00025">claim 25</claim-ref>, wherein said creating includes creating three caches with different bucket sizes.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00028" num="00028" class="claim">
    <div class="claim-text">28. A method of caching data on a network from a file on at least one shared I/O device that is coupled to a computer having a system memory, wherein the method comprises:
<div class="claim-text">creating in system memory at least two caches with different bucket sizes; and</div>
<div class="claim-text">caching in each of the two caches data from said file.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00029" num="00029" class="claim">
    <div class="claim-text">29. The method of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein the at least one I/O device comprises a hard disk drive.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00030" num="00030" class="claim">
    <div class="claim-text">30. The method of <claim-ref idref="CLM-00028">claim 28</claim-ref>, wherein said creating includes creating three caches with different bucket sizes.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00031" num="00031" class="claim">
    <div class="claim-text">31. A computer network that comprises:
<div class="claim-text">one or more I/O devices configured to store data; and</div>
<div class="claim-text">multiple computers coupled together, wherein each of the multiple computers is configured to cache said data and send targeted remote invalidate messages to only those computers configured to cache data stored on the I/O device to which the invalidate messages relate, and</div>
<div class="claim-text">wherein each of the multiple computers is configured to monitor which computers in the network are configured to cache said data.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00032" num="00032" class="claim">
    <div class="claim-text">32. The computer network of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein each of the multiple computers is further configured to determine a remote connection address for only those other computers in the network that are configured to cache said data.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00033" num="00033" class="claim">
    <div class="claim-text">33. The computer network of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein each of the multiple computers is further configured to send and receive targeted messages for maintaining cache coherency.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00034" num="00034" class="claim">
    <div class="claim-text">34. The computer network of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein each of the multiple computers is further configured to determine a list of computers in the network that are configured to cache said data, and is still further configured to update the list when a computer that is configured to cache said data joins the network.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00035" num="00035" class="claim">
    <div class="claim-text">35. The computer network of <claim-ref idref="CLM-00034">claim 34</claim-ref>, wherein each of the multiple computers is further configured to determine a remote connection address for each computer in the list, and is still further configured to maintain cache coherency by communicating targeted messages among said multiple computers.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00036" num="00036" class="claim">
    <div class="claim-text">36. The computer network of <claim-ref idref="CLM-00031">claim 31</claim-ref>, wherein the one or more I/O devices include one or more hard disk drives.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00037" num="00037" class="claim">
    <div class="claim-text">37. A computer program product comprising: at least one computer usable medium having a computer readable cache program thereon, the cache program to be executed by each of multiple computers coupled together in a network with at least one I/O device configured to store data, the cache program comprising:
<div class="claim-text">a routine that configures each of said multiple computers to cache said data from said at least one I/O device; and</div>
<div class="claim-text">a program that configures each of said multiple computers (i) to monitor which computers in the network are configured to cache said data; and (ii) to send an invalidation message to only the nodes caching said data upon writing data to said at least one I/O device.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00038" num="00038" class="claim">
    <div class="claim-text">38. The cache program of <claim-ref idref="CLM-00037">claim 37</claim-ref>, wherein the program further configures each of said multiple computers to determine a remote connection address for only those other computers in the network having said routine operable to cache said data.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00039" num="00039" class="claim">
    <div class="claim-text">39. The cache program of <claim-ref idref="CLM-00038">claim 38</claim-ref>, wherein said routine further configures each of said multiple computers to send and receive targeted messages for maintaining cache coherency.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00040" num="00040" class="claim">
    <div class="claim-text">40. The cache program of <claim-ref idref="CLM-00037">claim 37</claim-ref>, wherein the program further configures each of said multiple computers to:
<div class="claim-text">determine a list of computers in the network that are configured to cache said data; and</div>
<div class="claim-text">update the list when a computer that is configured to cache said data joins the network.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00041" num="00041" class="claim">
    <div class="claim-text">41. The cache program of <claim-ref idref="CLM-00040">claim 40</claim-ref>, wherein the program further configures each of said multiple computers to determine a remote connection address for each computer in the list; and wherein the routine further configures each of said multiple computers to maintain cache coherency by communicating targeted messages among said multiple computers.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00042" num="00042" class="claim">
    <div class="claim-text">42. The cache program of <claim-ref idref="CLM-00037">claim 37</claim-ref>, wherein the at least one I/O device comprises a hard disk drive.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00043" num="00043" class="claim">
    <div class="claim-text">43. A method of caching, in a network having multiple computers and at least one I/O device, wherein each of the multiple computers has an associated system memory, wherein the at least one I/O device stores data, wherein the method comprises:
<div class="claim-text">determining a list of computers in the network that are configured to cache said data;</div>
<div class="claim-text">updating the list when a computer joins the network;</div>
<div class="claim-text">determining a remote connection address for each computer in the list;</div>
<div class="claim-text">caching said data in the system memory of one or more of said multiple computers; and</div>
<div class="claim-text">maintaining cache coherency by communicating targeted invalidation messages to only those computers on the list for the data to which such message relates.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00044" num="00044" class="claim">
    <div class="claim-text">44. The method of <claim-ref idref="CLM-00043">claim 43</claim-ref>, wherein the at least one I/O device includes a hard disk drive.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00045" num="00045" class="claim">
    <div class="claim-text">45. The method of <claim-ref idref="CLM-00043">claim 43</claim-ref>, wherein said caching includes:
<div class="claim-text">creating in each of the associated system memories at least two caches with different bucket sizes; and</div>
<div class="claim-text">caching in each of the two caches data from multiple files stored on the I/O device.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00046" num="00046" class="claim">
    <div class="claim-text">46. The method of <claim-ref idref="CLM-00045">claim 45</claim-ref>, wherein said creating includes creating in each of the associated system memories three caches with different bucket sizes.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00047" num="00047" class="claim">
    <div class="claim-text">47. The method of <claim-ref idref="CLM-00043">claim 43</claim-ref>, wherein said data is a single file, and wherein said caching includes:
<div class="claim-text">creating in each of the associated system memories at least two caches with different bucket sizes,</div>
<div class="claim-text">wherein each cache is capable of storing a portion of said data.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00048" num="00048" class="claim">
    <div class="claim-text">48. The method of <claim-ref idref="CLM-00047">claim 47</claim-ref>, wherein said creating includes creating in each of the associated system memories three caches with different bucket sizes.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00049" num="00049" class="claim">
    <div class="claim-text">49. A computer network that comprises:
<div class="claim-text">one or more I/O devices configured to store data; and</div>
<div class="claim-text">multiple computers coupled together, wherein each of the multiple computers is configured to cache data from the one or more I/O devices,</div>
<div class="claim-text">wherein each of the multiple computers is configured (i) to determine for each of the one or more I/O devices a list of computers in the network that are configured to cache data from that device and (ii) to maintain cache coherency among said multiple computers by communicating targeted messages based on the lists.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00050" num="00050" class="claim">
    <div class="claim-text">50. The computer network of <claim-ref idref="CLM-00049">claim 49</claim-ref>, wherein each of the multiple computers is further configured to determine a remote connection address for all other computers in each list.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00051" num="00051" class="claim">
    <div class="claim-text">51. The computer network of <claim-ref idref="CLM-00049">claim 49</claim-ref>, wherein each of the multiple computers is further configured to update the lists when a computer joins the network.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00052" num="00052" class="claim">
    <div class="claim-text">52. A computer program product comprising: at least one computer usable medium having a computer readable cache program thereon, the cache program to be executed by each of multiple computers, wherein the multiple computers are coupled together in a network that includes multiple I/O devices, wherein the cache program configures each of the multiple computers to:
<div class="claim-text">determine for each of the multiple I/O devices a list of computers in the network that are configured to cache data from that I/O device;</div>
<div class="claim-text">send and receive targeted messages based on the lists to maintain cache coherency; and</div>
<div class="claim-text">update the lists when a computer joins the network.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00053" num="00053" class="claim">
    <div class="claim-text">53. The cache program of clam <b>52</b>, wherein the cache program further configures each of the multiple computers to:
<div class="claim-text">determine a remote connection address for each remote computer in the lists.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00054" num="00054" class="claim">
    <div class="claim-text">54. The cache program of <claim-ref idref="CLM-00053">claim 53</claim-ref>, wherein the cache program further configures each of the multiple computers to:
<div class="claim-text">establish a communications channel with each remote computer in the sets.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00055" num="00055" class="claim">
    <div class="claim-text">55. The cache program of <claim-ref idref="CLM-00054">claim 54</claim-ref>, wherein the cache program further configures each of the multiple computers to:
<div class="claim-text">enable communication of cache data invalidation messages via the communications channels.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00056" num="00056" class="claim">
    <div class="claim-text">56. The cache program of <claim-ref idref="CLM-00052">claim 52</claim-ref>, wherein the I/O device is a hard disk drive.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00057" num="00057" class="claim">
    <div class="claim-text">57. A method of caching in multiple computers data from one or more I/O devices coupled to a network, wherein the method comprises:
<div class="claim-text">determining for each of the one or more I/O devices a list of computers in the network that are configured to cache data from that I/O device;</div>
<div class="claim-text">maintaining cache coherency using the lists to send targeted messages; and</div>
<div class="claim-text">updating the lists when a computer joins the network.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00058" num="00058" class="claim">
    <div class="claim-text">58. The method of <claim-ref idref="CLM-00057">claim 57</claim-ref>, further comprising:
<div class="claim-text">determining a remote connection address for each remote computer in the lists.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00059" num="00059" class="claim">
    <div class="claim-text">59. The method of <claim-ref idref="CLM-00058">claim 58</claim-ref>, further comprising:
<div class="claim-text">establishing a communications channel with each remote computer in the lists.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00060" num="00060" class="claim">
    <div class="claim-text">60. The method of <claim-ref idref="CLM-00059">claim 59</claim-ref>, further comprising:
<div class="claim-text">enabling communication of cache data invalidation messages via the communications channels.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00061" num="00061" class="claim">
    <div class="claim-text">61. A computer network that comprises:
<div class="claim-text">one or more I/O devices configured to store data; and</div>
</div>
    <div class="claim-text">multiple computers coupled together, wherein the multiple computers are each configured to cache a respective set of I/O devices selected from said one or more I/O devices, and wherein the sets are each independently changeable while caching operations are ongoing; wherein the multiple computers each include a system memory having a plurality of caches with different bucket sizes created by cache software.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00062" num="00062" class="claim">
    <div class="claim-text">62. A computer program product comprising: at least one computer usable medium having a computer readable cache program thereon, the cache program capable of being installed on each of multiple servers clustered together in a network with a set of I/O devices configured to store data, the cache program comprising:
<div class="claim-text">a cache software routine that configures the executing server to cache data from a subset of said I/O devices; and</div>
</div>
    <div class="claim-text">a procedure that configures the executing server to change the subset of said I/O devices while caching operations are ongoing wherein said cache software routine further configures the executing server to create in system memory at least two caches having different bucket sizes.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00063" num="00063" class="claim">
    <div class="claim-text">63. A method of caching in a network having multiple servers and multiple I/O devices, wherein each of the multiple servers has an associated system memory, and wherein the multiple I/O devices are each configured to store data, the method comprising:
<div class="claim-text">caching on each of the multiple servers data from a respective set of said I/O devices wherein the caches on the multiple servers have multiple bucket sizes created by cache software; and</div>
<div class="claim-text">dynamically changing the set of I/O devices being cached by one of the multiple servers.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00064" num="00064" class="claim">
    <div class="claim-text">64. The method of <claim-ref idref="CLM-00063">claim 63</claim-ref>, further comprising:
<div class="claim-text">independently changing the set of I/O devices being cached by a different one of the multiple servers.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00065" num="00065" class="claim">
    <div class="claim-text">65. The method of <claim-ref idref="CLM-00063">claim 63</claim-ref>, wherein said changing comprises including an additional I/O device in caching operations performed by said one of the multiple servers.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00066" num="00066" class="claim">
    <div class="claim-text">66. The method of <claim-ref idref="CLM-00065">claim 65</claim-ref>, wherein said including an additional I/O device comprises notifying other servers with access to said additional I/O device that said one of the multiple servers is caching said additional I/O device.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00067" num="00067" class="claim">
    <div class="claim-text">67. A method of caching in multiple computers data from one or more information storage devices coupled to a network, wherein the method comprises:
<div class="claim-text">constructing for each computer a set of all information storage devices accessible by that computer;</div>
<div class="claim-text">determining for each information storage device in each set a list of computers in the network that are configured to cache data from that information storage device for the purpose of sending targeted invalidation messages; and</div>
<div class="claim-text">reconstructing the lists after detecting a change in network configuration.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00068" num="00068" class="claim">
    <div class="claim-text">68. The method of <claim-ref idref="CLM-00067">claim 67</claim-ref>, wherein the change in network configuration includes a computer joining the network.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00069" num="00069" class="claim">
    <div class="claim-text">69. The method of <claim-ref idref="CLM-00067">claim 67</claim-ref>, wherein said constructing includes:
<div class="claim-text">each computer identifying all computers in the network; and</div>
<div class="claim-text">each computer determining all information storage devices that it can access directly and all information storage devices it can access via other computers.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00070" num="00070" class="claim">
    <div class="claim-text">70. The method of <claim-ref idref="CLM-00067">claim 67</claim-ref>, wherein said determining includes:
<div class="claim-text">enabling a selected computer's caching of data from a given information storage device;</div>
<div class="claim-text">notifying all computers with access to the given information storage device that the selected computer is configured to cache data from the given storage device.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00071" num="00071" class="claim">
    <div class="claim-text">71. The method of <claim-ref idref="CLM-00067">claim 67</claim-ref>, wherein said reconstructing includes:
<div class="claim-text">disabling caching on all computers in the network;</div>
<div class="claim-text">individually enabling a selected computer's caching of data from a given storage device; and</div>
<div class="claim-text">with each enabling operation, notifying all computers with access to the given information storage device that the selected computer is configured to cache data from the given storage device.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00072" num="00072" class="claim">
    <div class="claim-text">72. The method of <claim-ref idref="CLM-00067">claim 67</claim-ref>, further comprising:
<div class="claim-text">caching data from one or more information storage devices in a system memory of one or more of said computers.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00073" num="00073" class="claim">
    <div class="claim-text">73. The method of <claim-ref idref="CLM-00072">claim 72</claim-ref>, wherein said caching includes:
<div class="claim-text">creating in each of the associated system memories at least two caches with different bucket sizes; and</div>
<div class="claim-text">caching in each of the two caches data from multiple files stored on the information storage device.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00074" num="00074" class="claim">
    <div class="claim-text">74. A computer network that comprises:
<div class="claim-text">one or more I/O devices; and</div>
<div class="claim-text">multiple servers clustered together, wherein each of the multiple servers is configured to determine a set of all I/O devices accessible by that server,</div>
<div class="claim-text">wherein each of the multiple servers is further configured to construct for each storage device in the set a list of servers that are configured to cache data from that I/O device for the purpose of sending targeted invalidation messages to only those servers on the list for any given I/O device and</div>
<div class="claim-text">wherein each of the multiple servers is further configured to reconstruct the lists after detecting a change in network configuration.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00075" num="00075" class="claim">
    <div class="claim-text">75. The computer network of <claim-ref idref="CLM-00074">claim 74</claim-ref>, wherein the change in network configuration includes a server joining the network.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00076" num="00076" class="claim">
    <div class="claim-text">76. The computer network of <claim-ref idref="CLM-00074">claim 74</claim-ref>, wherein as part of said constructing, each server is configured, as part of enabling caching for a given I/O device, to notify all servers with access to that given I/O device that the server is configured to cache data from that given I/O storage device.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00077" num="00077" class="claim">
    <div class="claim-text">77. A computer program product comprising: at least one computer usable medium having a computer readable cache program thereon, the cache program that can be installed on, and executed by, each of multiple servers clustered together in a network that includes at least one information storage device, wherein the cache program configures each of the multiple servers to:
<div class="claim-text">construct for each server a set of all I/O devices accessible by that server;</div>
<div class="claim-text">determine for each I/O device in the set a list of servers that are configured to cache data from that I/O device for the purpose of sending targeted invalidation messages to only those servers on the list for any given I/O device; and</div>
<div class="claim-text">reconstruct the lists after detecting a change in network configuration.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00078" num="00078" class="claim">
    <div class="claim-text">78. The cache program of <claim-ref idref="CLM-00077">claim 77</claim-ref>, wherein the change in network configuration includes a server joining the network.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00079" num="00079" class="claim">
    <div class="claim-text">79. The cache program of <claim-ref idref="CLM-00077">claim 77</claim-ref>, wherein as part of said constructing, the cache program configures each of the multiple servers to:
<div class="claim-text">identify all servers in the network; and</div>
<div class="claim-text">determine all I/O devices that the server can access directly and all I/O devices that it can access via other servers.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00080" num="00080" class="claim">
    <div class="claim-text">80. The cache program of <claim-ref idref="CLM-00077">claim 77</claim-ref>, wherein as part of said determining, the cache program configures each of the multiple servers to:
<div class="claim-text">begin caching data from a given I/O device only after notifying all other servers with access to the given information storage device that the server is configured to cache data from the given storage device.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00081" num="00081" class="claim">
    <div class="claim-text">81. The cache program of <claim-ref idref="CLM-00077">claim 77</claim-ref>, wherein as part of said reconstructing, the cache program configures each of the multiple servers to:
<div class="claim-text">disable caching of all I/O devices; and</div>
<div class="claim-text">re-enable caching of a given I/O device only after notifying all other servers with access to the given I/O device that the server is configured to cache data from the given I/O device.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00082" num="00082" class="claim">
    <div class="claim-text">82. The cache program of <claim-ref idref="CLM-00077">claim 77</claim-ref>, wherein the cache program further configures each of the multiple servers to:
<div class="claim-text">cache data from one or more I/O devices in a system memory, wherein the caching includes creating in the system memory at least two caches with different bucket sizes.</div>
</div>
  </div>
</div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES16026687" lang="EN" load-source="patent-office" class="description">
<heading>CROSS REFERENCE TO RELATED APPLICATIONS</heading> <p num="p-0002">This is a continuation of co-pending U.S. Ser. No. 10/683,853, filed Oct. 10, 2003, which is a continuation of U.S. Ser. No. 10/052,873, filed Jan. 16, 2002, now U.S. Pat. No. 6,651,136, which is a continuation of U.S. Ser. No. 09/300,633, filed Apr. 27, 1999, now U.S. Pat. No. 6,370,615, which is a continuation of U.S. Ser. No. 08/657,777, filed May 31, 1996, now U.S. Pat. No. 5,918,244, which is a continuation of U.S. Ser. No. 08/238,815, filed May 6, 1994, now U.S. Pat. No. 5,577,226, the full disclosures of which are hereby incorporated by reference herein.</p>
<heading>BACKGROUND OF INVENTION</heading> <p num="p-0003">The present invention is directed to a disk caching technique using software, in particular, disk caching software for use on an OpenVMS operating system. OpenVMS is the operating system used on VAX and Alpha AXP computers.</p>
  <p num="p-0004">Computer users are always looking for ways to speed up operations on their computers. One source of the drag on computer speed is the time it takes to conduct an input/output operation to the hard disk drive or other mechanical disk devices. Such devices are slowed by mechanical movement latencies and I/O bus traffic requirements. One conventional method for avoiding this speed delay is to cache frequently accessed disk data in the computer main memory. Access to this cached data in main memory is much quicker than always accessing the hard disk drive for the data. Access speed to a hard disk drive is replaced by main memory access speed to the data resident in the cache.</p>
  <p num="p-0005">There is a significant down side to the conventional form of caching techniques. Caches are conventionally organized as to be made up of fixed sized areas, known as buckets, where the disk data is stored, with all the buckets added together making up the fixed total size of the computer main memory allocated for use by the cache. No matter what size the original disk access was this data has to be accommodated in the cache buckets. Thus, if the disk access size was very small compared to the cache bucket size, then most of the bucket storage area is wasted, containing no valid disk data at all. If the disk was accessed by many of these smaller accesses, then the cache buckets would get used up by these small data sizes and the cache would not apparently be able to hold as much data as was originally expected. If the disk access size was larger than the cache bucket size, either the data is not accommodated in the cache, or several cache buckets have to be used to accommodate the disk data which makes cache management very complicated. With this conventional approach to disk caching the computer user has to try to compromise with the single cache bucket size for all users on the computer system. If the computer is used for several different applications, then either the cache bucket size has to be biased to one type of application being at a disadvantage to all the other applications, or the cache bucket size has to averaged against all applications with the cache being at less an advantage as would be desired. It is an object of the present invention to reduce this down side of using a disk cache.</p>
  <heading>SUMMARY OF INVENTION</heading> <p num="p-0006">In accordance with the embodiment of the invention, the total cache is organized into three separate caches each having a different cache bucket size associated with it for small, medium, and large, disk access sizes. The computer user has control over the bucket sizes for each of the three cache areas.</p>
  <p num="p-0007">In accordance with the embodiment of the invention, the computer user has control over which disks on the computer system will be included in the caching and which disks on the computer system are to be excluded from the caching.</p>
  <p num="p-0008">In accordance with the embodiment of the invention, the total cache size contained in the computer main memory, being made up of the three cache areas, does not have a singular fixed size and will change dependent on the computer systems use. The total cache size is allowed to grow in response to high disk access demand, and to reduce when the available computer main memory becomes at a premium to the computer users. Thus the computer main memory used by the cache fluctuates dependent on disk data access and requirements of the computer main memory. The computer user has control over the upper and lower limits of which the total cache size occupies the computers main memory. The total cache will then be made up of mainly the small, or the medium, or the large bucket areas, or a spread of the three cache area sizes dependent on how the cached disks are accessed on the system.</p>
  <p num="p-0009">In accordance with the embodiment of the invention, once the total cache size has grown to its upper limit further new demands on cache data are handled by cache bucket replacement, which operates on a least recently used algorithm. This cache bucket replacement will also occur if the total cache size is inhibited from growing owing to a high demand on computer main memory by other applications and users of the computer system.</p>
  <p num="p-0010">In accordance with the embodiment of the invention, when a disk which is being cached is subject to a new read data access by some computer user, the required disk data is sent to the computer user and also copied into an available cache bucket dependent on size fit. This cache bucket is either newly obtained from the computer main memory or by replacing an already resident cache bucket using a least recently used algorithm. If this disk data, now resident in the cache, is again requested by a read access of some computer user, the data is returned to the requesting user directly from the cache bucket and does not involve any hard disk access at all. The data is returned at the faster computer main memory access speed, showing the speed advantage of using a disk cache mechanism.</p>
  <p num="p-0011">In accordance with the embodiment of the invention, when a disk which is being cached is subject to a new read data access by some computer user and this disk access is larger than all three cache bucket sizes, the disk data is not copied to the cache. This oversize read access, along with other cache statistics are recorded allowing the computer user to interrogate the use of the cache. Using these statistics the computer user can adjust the size of the three cache buckets to best suit the disk use on the computer system.</p>
  <p num="p-0012">In accordance with the embodiment of the invention, when a write access is performed to a disk which is being cached and the disk data area being written was previously read into the cache, i.e. an update operation on the disk data, the current cache buckets for the previous read disk data area are invalidated on all computers on the network.</p>
  <p num="p-0013">Other objects and advantages of the invention will become apparent during the following description of the presently preferred embodiments of the invention taken in conjunction with the drawing.</p>
<description-of-drawings> <heading>BRIEF DESCRIPTION OF DRAWINGS</heading> <p num="p-0014"> <figref idrefs="DRAWINGS">FIGS. 1A to 1B</figref> is a schematic block diagram of the disk cache software of the invention implemented on a computer running an OpenVMS operating system.</p>
    <p num="p-0015">FIGS. <b>2</b>A to <b>2</b>D-<b>1</b> are flow diagrams of the program steps for initial loading into the computer system for the disk cache software of the invention.</p>
    <p num="p-0016"> <figref idrefs="DRAWINGS">FIGS. 3A to 3C</figref> are flow diagrams of the program steps performed when the disk cache software is started for the present invention.</p>
    <p num="p-0017">FIGS. <b>4</b>A to <b>4</b>H-<b>2</b> are flow diagrams on the program steps for selecting a disk I/O device to be included into, or excluded from, the cache software of the invention.</p>
    <p num="p-0018"> <figref idrefs="DRAWINGS">FIGS. 5A to 5P</figref> are flow diagrams on the program steps performed by the active data caching of a disk I/O device in the cache software of the invention.</p>
  </description-of-drawings> <heading>DETAILED DESCRIPTION</heading> <p num="p-0019">Referring now to the drawings, a disk cache (<b>10</b>) of the present invention is schematically shown in <figref idrefs="DRAWINGS">FIGS. 1A to 1B</figref>. All data accesses by the operating system of the associated computer to any of the disks (<b>12</b>) on the system are intercepted by the cache driver (<b>10</b>). The operating system may be any commonly available system, however, the presently preferred embodiment of the invention is implemented in conjunction with an OpenVMS system (<b>14</b>). When the cache driver (<b>10</b>) is first loaded on the operating system all the disks (<b>12</b>) present on the computer system are located and a disk control structure, referred to herein as a TCB (“the control block”)(<b>16</b>), is built for each separate disk (<b>12</b>). The disks (<b>12</b>) can be locally connected to the computer containing this cache driver (<b>10</b>), or the disks (<b>12</b>) can be remotely connected to some other computer that this computer has a remote connection to. The presently preferred embodiment of the invention uses remote disks that are connected by the OpenVMS VMScluster and VAXcluster software. The VMS cluster software is operable on 64-bit or 32-bit architecture computer systems. The VAXcluster software only permits 32-bit computers. A TCB (<b>16</b>) disk control structure contains the cache status information for the disk (<b>12</b>), cache monitor statistics for the disk (<b>12</b>), and a list of remote computers containing their own copy of the cache driver (<b>10</b>) that can access the disk (<b>12</b>).</p>
  <p num="p-0020">The cache driver (<b>10</b>) maintains remote message communication channels (<b>18</b>) with other cache drivers loaded on other computers that can access a common set of disks (<b>12</b>). Whenever the OpenVMS system (<b>14</b>) changes the data on the disk (<b>12</b>), for example by doing a write data access to the disk (<b>12</b>), the cache driver (<b>10</b>) uses its remote message communication channels (<b>18</b>) to send a message to each of the remote cache drivers in the list contained in the TCB (<b>16</b>) disk control structure. Conversely, a remote cache driver would send a message to this cache driver (<b>10</b>), via the remote message communication channels (<b>18</b>), to inform this cache driver (<b>10</b>) of a change in the data for some remotely connected disk (<b>12</b>). The cache driver (<b>10</b>) would use this incoming message to invalidate any possible previously locally cached data for the area on the remotely connected disk (<b>12</b>) that has been changed by the remote OpenVMS system.</p>
  <p num="p-0021">The cached disk (<b>12</b>) data is held in computer RAM (<b>20</b>) allocated from OpenVMS systems (<b>14</b>) available free memory. This RAM (<b>20</b>) area is allocated on demand in chunks (<b>22</b>) that relate to the bucket size for which of the three caches, small, medium, or large, that the disk (<b>12</b>) read access size fits. For each cache data bucket (<b>22</b>) a corresponding bucket control structure, referred to herein as a TCMB (“the cache memory block”) (<b>24</b>), is built with the TCMB (<b>24</b>) space allocated from the OpenVMS systems (<b>14</b>) pool. The TCMB (<b>24</b>) bucket control structure contains pointers to the RAM (<b>20</b>) area containing the cache data bucket (<b>22</b>). The TCMB (<b>24</b>) bucket control structure is held in one of three queues off a cache control structure, referred to herein as a TCH (“the cache hash”)(<b>26</b>). There are three TCH (<b>26</b>) cache control structures, one for each of the three cache bucket sizes, small, medium and large. Each TCH (<b>26</b>) cache control structure contains cache statistics for the particular sized cache, small, medium, or large, three queue list heads where TCMB (<b>24</b>) bucket control structures are held, these being the free queue (<b>27</b>), the LRU queue (<b>28</b>), and the in-progress queue (<b>29</b>). Each TCH (<b>26</b>) cache control structure also contains a disk block value hash table (<b>30</b>) which also points to TCMB's (<b>24</b>) for a set of disk block areas.</p>
  <p num="p-0022">When the OpenVMS system (<b>14</b>) performs a read data I/O access to a disk (<b>12</b>) the cache driver (<b>10</b>) software intercepts the I/O. Using the size of the read data access the cache driver (<b>10</b>) selects which of the three caches, small, medium, or large, the data transfer fits. Having selected the appropriate sized cache the TCH (<b>26</b>) cache control structure is selected. Using the read data I/O access disk block as a pointer into the disk block value hash table (<b>30</b>) of the TCH (<b>26</b>), the cache driver (<b>10</b>) attempts to locate a matching TCMB (<b>24</b>) bucket control structure. If a matching TCMB (<b>24</b>) for the disk (<b>12</b>) and its disk area is found a cache hit is assumed and the data is returned to the OpenVMS system (<b>14</b>) from the cache data bucket (<b>22</b>) held in the computer RAM (<b>20</b>). The data is returned at the faster computer main memory access speed, showing the speed advantage of using a disk cache mechanism. If no matching TCMB (<b>24</b>) bucket control structure is found for the disk (<b>12</b>) and its disk area, a cache miss is assumed.</p>
  <p num="p-0023">For a cache miss an unused TCMB (<b>24</b>) bucket control structure and its corresponding cache data bucket (<b>22</b>) is assigned for the read data I/O access. This unused TCMB (<b>24</b>) with its corresponding cache data bucket (<b>22</b>) is first attempted to be allocated from the TCMB free queue (<b>27</b>) off the associated TCH (<b>26</b>) cache control structure. How TCMB's (<b>24</b>) with their corresponding cache data buckets (<b>22</b>) get to the free queue (<b>27</b>) will be described later. If there are no TCMB's (<b>24</b>) on the free queue (<b>27</b>), the cache driver (<b>10</b>) attempts to allocate extra computer RAM (<b>20</b>) space for a new cache data bucket (<b>22</b>), matching the bucket size, with a new TCMB (<b>24</b>) bucket control structure. If the OpenVMS system (<b>14</b>) indicates there is insufficient available free memory for this new cache data bucket (<b>22</b>) and TCMB (<b>24</b>) assignment, or the cache driver has reached its memory limit set by the computer user when the cache was started, the cache driver (<b>10</b>) attempts to reuse a TCMB (<b>24</b>) with its corresponding cache data bucket (<b>22</b>) from the back of the TCMB least recently used, LRU, queue (<b>28</b>) off the appropriate TCH (<b>26</b>) cache control structure. How TCMB's (<b>24</b>) with their corresponding cache data buckets (<b>22</b>) get to the LRU queue (<b>28</b>) will be described later. If there are no TCMB (<b>24</b>) bucket control structures with their corresponding cache data bucket (<b>22</b>) on the LRU queue (<b>28</b>), no cache data space can be assigned to this read data I/O access and the disk (<b>12</b>) is accessed normally for the required read data. If a TCMB (<b>24</b>) bucket control structure with its corresponding cache data bucket (<b>22</b>) was obtained from one of the three sources described above, cache data space can be assigned for this disk (<b>12</b>) read data. The disk (<b>12</b>) is accessed normally, however the read data is not only sent to the requesting user on the OpenVMS system (<b>14</b>), but also copied to the cache data bucket (<b>22</b>). The corresponding TCMB (<b>24</b>) bucket control structure, for the cache data bucket (<b>22</b>), is filled in to contain a pointer to the corresponding TCB (<b>16</b>) disk control structure along with the disk block area that the cache data bucket (<b>22</b>) contains. Whilst the disk (<b>12</b>) read data I/O was in progress the TCMB (<b>24</b>) bucket control structure and its corresponding cache data bucket (<b>22</b>) was placed on the in-progress queue (<b>29</b>) of the associated TCH (<b>26</b>). This allows the cache driver (<b>10</b>) to deal with another disk cache access whilst current accesses are progressing, making the cache driver multithreaded. When the disk (<b>12</b>) read data I/O completes and the disk data has been copied to the cache data bucket (<b>22</b>), the corresponding TCMB (<b>24</b>) bucket control structure is placed at the front of the LRU queue (<b>28</b>) off the associated TCH (<b>26</b>) cache control structure. The starting disk block that this cached data bucket (<b>22</b>) and corresponding TCMB (<b>24</b>) bucket control structure is hashed, using the size of the cache bucket as the hash control, and the resulting hash value is used to place the TCMB (<b>24</b>) in a chain of similar hash values within the disk block value hash table (<b>30</b>) of the associated TCH (<b>26</b>) cache control structure.</p>
  <p num="p-0024">When the OpenVMS system (<b>14</b>) performs a write data I/O access to a disk (<b>12</b>) the cache driver (<b>10</b>) software intercepts the I/O. The cache driver (<b>10</b>) will search for possible matching TCMB (<b>24</b>) bucket control structures with their corresponding cache data buckets (<b>22</b>) in all three TCH (<b>26</b>) cache control structures, for the disk and the range of disk blocks in the write data I/O access. Using the write data I/O access disk block as a pointer into the disk block value hash table (<b>30</b>) of each of the three TCH's (<b>26</b>), the cache driver (<b>10</b>) attempts to locate matching TCMB (<b>24</b>) bucket control structures. For each matching TCMB (<b>24</b>) bucket control structure found, the TCMB (<b>24</b>) and its corresponding cache data bucket (<b>22</b>) are invalidated. The invalidated TCMB (<b>24</b>) and its cache data bucket (<b>22</b>) are normally placed on the free queue (<b>27</b>) of the associated TCH (<b>26</b>) cache control structure to be used by some future cache data operation, however, if the OpenVMS system (<b>14</b>) indicates there are insufficient available free pages for the OpenVMS system (<b>14</b>), the cache data bucket (<b>22</b>) RAM space is returned to the OpenVMS system (<b>14</b>) free pages and the corresponding TCMB (<b>24</b>) space is returned to the OpenVMS system (<b>14</b>) pool. The TCB (<b>16</b>) disk control structure is located from invalidated TCMB (<b>24</b>) bucket control structure, with the TCMB (<b>24</b>) then disassociated with the TCB (<b>16</b>) disk control structure. The list of remote computers that can access the disk (<b>12</b>) is obtained from the TCB (<b>16</b>) disk control structure and a message is sent to all these remote computers using the remote message communication channels (<b>18</b>). On receipt of the message the cache driver (<b>10</b>) on the remote computers will invalidate any TCMB (<b>24</b>) bucket control structures and the corresponding cache data buckets (<b>22</b>) for the disk (<b>12</b>) and the disk block area range found in the write data I/O.</p>
  <p num="p-0025">Every so often, using a timing mechanism present within the OpenVMS system (<b>14</b>), a system memory check (<b>32</b>) will run. This system memory check (<b>32</b>) looks at the available free pages and pool of the OpenVMS system (<b>14</b>). If the checks indicate there is insufficient memory available to the OpenVMS system (<b>14</b>) cache data buckets (<b>22</b>) are released, along with their corresponding TCMB (<b>24</b>) bucket control structures, back to the OpenVMS system (<b>14</b>) in a similar way to the write data I/O described above. The cache data buckets (<b>22</b>) are released by first using the free queue (<b>27</b>) of TCMB's (<b>24</b>) for the TCH's (<b>26</b>), then the LRU queue (<b>28</b>), and finally the in-progress queue (<b>29</b>), until the OpenVMS system (<b>14</b>) indicates that it again has sufficient available free pages.</p>
  <p num="p-0026">In order to set the cache (<b>10</b>) characteristics and select disks (<b>12</b>) to include in the cache of the invention a user command interface (<b>34</b>) is provided. In the presently preferred embodiment, this is accessed via a CACHE command. The CACHE commands allow the cache (<b>10</b>) to start with selected characteristics such as the bucket size of the three caches for small, medium, and large, disk transfers, along with the upper and lower limits of computer RAM (<b>20</b>), which the cache driver (<b>10</b>) can use to accommodate the cache data buckets (<b>22</b>). The CACHE commands allow which disks (<b>12</b>) on the system are to be included in the cache and which disks (<b>12</b>) are to be excluded from the cache. The CACHE commands allow the computer user to view the status of the cache, along with the cache and disk statistics, either as a one shot display or continuously updated in a screen display bar chart.</p>
  <p num="p-0027">The support code (<b>36</b>) for the cache of the invention periodically obtains cache and disk use statistics from the cache driver (<b>10</b>). This period is set from the CACHE command of the user interface (<b>34</b>). The cache and disk statistics obtained by the support code (<b>36</b>) is written to log files (<b>38</b>). These log files (<b>38</b>) contain cache statistics over a period of time, in order to be used by the computer user in adjusting the cache characteristics to best match the system on which the cache (<b>10</b>) of the invention is being used.</p>
  <p num="p-0028">Referring now to FIGS. <b>2</b>A to <b>2</b>D-<b>1</b>, the instruction flow for the initial loading into the computer system of the cache software is illustrated. The operating software loads the cache software of the invention into the system (<b>40</b>) and calls the cache software at its controller initialisation entry point. The cache status is set to ‘off’ (<b>42</b>). The routine “io intercept global” is called (<b>44</b>). Referring to <figref idrefs="DRAWINGS">FIG. 2</figref> <i>b </i>for the “io intercept global” program flow (<b>64</b>), the program gets the start of the locally attached I/O device list for the computer system (<b>66</b>). The program gets the next I/O device from the I/O device list (<b>68</b>), which at this point will be the first I/O device in the list, and checks to see if the I/O device is one of the disk device types (<b>70</b>). If not, the program checks to see if all the I/O devices for the system have been checked (<b>72</b>). If there are further I/O devices connected to the system (<b>72</b>) the program repeats the loop by getting the next I/O device in the list (<b>68</b>) until all devices have been checked. When an I/O device is found to be one of the disk device types supported by the cache software of the invention (<b>70</b>), the program intercepts the I/O entry point for the I/O device (<b>74</b>) by replacing it with an entry into the program routine “process io” (<b>400</b>, <figref idrefs="DRAWINGS">FIG. 5A</figref>) within the cache software of the invention. A TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device is built (<b>76</b>). The TCB is set to ‘exclude’ mode and ‘statistics only’ mode (<b>78</b>), this stops the disk I/O device from being cached when the user starts the cache, until the user selectively includes this disk I/O device in the set of cached disks by the appropriate CACHE user command (<b>34</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>). The list of remote computers in the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) that will contain their own copy of the cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) that access the disk I/O device is cleared (<b>80</b>). The program flow then returns to the loop to see if there are further I/O devices attached to this computer system (<b>72</b>). Having searched through all the I/O devices connected to this computer system (<b>72</b>), the program will get the I/O device list of the next remote computer system that this local computer system can access (<b>82</b>). The presently preferred embodiment of the invention is implemented in conjunction with an OpenVMS system and uses the VMS cluster and VAX cluster software, within the OpenVMS system, to access remote I/O devices and computer systems. The program will check to see if all the remote computer systems have been searched (<b>84</b>), if not, the program repeats the loop searching for disk I/O devices supported by the cache software of the invention (<b>68</b>). When the program has searched through all the remote computer system I/O devices, the “io intercept global” program flow exits (<b>86</b>).</p>
  <p num="p-0029">Returning now to <figref idrefs="DRAWINGS">FIG. 2A</figref>, once all the disk I/O devices that the cache software of the invention supports have been intercepted (<b>44</b>), the program continues to set-up the remote computer communication channels. The presently preferred embodiment of the invention is implemented in conjunction with an OpenVMS system and uses the VMScluster and VAXcluster software, within the OpenVMS system, for the remote computer communications. The message structures for the remote computer communications are initialised (<b>46</b>). The cache status flag ‘disable’ is set (<b>48</b>), the ‘disable’ flag is used to indicate that the remote computer connections are inconsistent, which will temporarily disable caching operations until the remote computer connections are completely formed in a consistent state. Using the OpenVMS VMScluster and VAXcluster programs, the cache software of the invention is set to listen for incoming requests for connections from remote computer systems (<b>50</b>). On receipt of an incoming connection request, the program routine “someone found us” (<b>104</b>, <figref idrefs="DRAWINGS">FIG. 2C</figref>) within the cache software of the invention will be called. Using the OpenVMS VMScluster and VAXcluster programs, the cache software of the invention is set to poll for remote computer systems that are running the cache software of the invention (<b>52</b>). When a remote system running the cache software of the invention is found, the program routine “connect to remote” (<b>90</b>, <figref idrefs="DRAWINGS">FIG. 2C</figref>) within the cache software of the invention will be called. The program routines “connect to remote” (<b>90</b>, <figref idrefs="DRAWINGS">FIG. 2C</figref>) and “someone found us” (<b>104</b>, <figref idrefs="DRAWINGS">FIG. 2C</figref>) will form the remote computer communications channels down which cache software message communications of the invention will be sent. To enable the cache software of the invention to identify OpenVMS computer systems joining the network of VMScluster and VAXcluster systems, the cache software of the invention is set to poll for remote computer systems running the OpenVMS VMScluster and VAXcluster program “connection manager” (<b>54</b>). The OpenVMS VMScluster and VAXcluster program “connection manager” has to be run by all OpenVMS computer systems participating in the network of computers of a VMScluster and VAXcluster. When a remote system running the OpenVMS VMScluster and VAXcluster program “connection manager” is found, the program routine “found connection manager” (<b>110</b>, <figref idrefs="DRAWINGS">FIG. 2C</figref>) within the cache software of the invention will be called. The timer program “scan routine” (<b>120</b>, <figref idrefs="DRAWINGS">FIG. 2D</figref>) within the cache software of the invention is set to run in 40 seconds from this point, using a timer mechanism within OpenVMS (<b>56</b>). The cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) is set to be on-line and available to the OpenVMS system (<b>58</b>). The load initialization for the cache software of the invention then exits (<b>60</b>).</p>
  <p num="p-0030">Referring to <figref idrefs="DRAWINGS">FIG. 2C</figref>, the remote communication connection program routines “connect to remote” and “someone found us” along with “found connection manager”, will be described. When the OpenVMS VMScluster and VAXcluster system finds that a remote system is running the cache software of the invention, it calls the program routine “connect to remote” (<b>90</b>). The program requests the OpenVMS VMScluster and VAXcluster system to attempt to form a connection with the remote system (<b>92</b>). When a message is received from a remote system running the cache software of the invention, the program routine “message receive” (<b>286</b> <figref idrefs="DRAWINGS">FIG. 4D</figref>, <b>372</b> <figref idrefs="DRAWINGS">FIG. 4H</figref>, <b>644</b> <figref idrefs="DRAWINGS">FIG. 5N</figref>) within the cache software of the invention will be called. When the remote system running the cache software of the invention accepts the connection, the program proceeds by disabling the OpenVMS VMScluster and VAXcluster system from polling for this remote system again, in order that only one connection is formed between the two systems (<b>94</b>). Extra message buffers are allocated for this new remote connection (<b>96</b>). The program then calls “io intercept global” (<figref idrefs="DRAWINGS">FIG. 2B</figref>) to look for any new disk I/O devices that may have come available to cache with the presence of this new remote system (<b>98</b>). The remote connection address is then saved within the cache software of the invention (<b>100</b>) and the “connect to remote” program exits. On the remote system running the cache software of the invention, when a connect request is received the OpenVMS VMScluster and VAXcluster system calls the “someone found us” program (<b>104</b>). The program disables the OpenVMS VMScluster and VAXcluster system from polling for this remote system again, in order that only one connection is formed between the two systems (<b>106</b>). The program then requests that the OpenVMS VMScluster and VAXcluster system accepts the connection from the remote system (<b>108</b>). When a message is received from a remote system running the cache software of the invention, the program routine “message receive” (<b>286</b> <figref idrefs="DRAWINGS">FIG. 4D</figref>, <b>372</b> <figref idrefs="DRAWINGS">FIG. 4H</figref>, <b>644</b> <figref idrefs="DRAWINGS">FIG. 5N</figref>) within the cache software of the invention will be called. The program then proceeds to its exit in the same way as “connect to remote” (<b>96</b>–<b>102</b>).</p>
  <p num="p-0031">When a new OpenVMS system joins the network of computer systems in the VMScluster and VAXcluster system, the cache software of the invention on each of the current OpenVMS systems will be called at its “found connection manager” (<b>110</b>) program entry point. The program firstly sets the cache ‘disable’ status flag (<b>112</b>) The ‘disable’ flag is used to indicate that the remote computer connections are inconsistent, which will temporarily disable caching operations until these connections are completely formed in a consistent state. The program disables the OpenVMS VMScluster and VAXcluster system from polling for the “connection manager” on this remote system again (<b>114</b>), as the cache software of the invention is now aware of this new system. The timer program “scan routine” (<b>120</b>, <figref idrefs="DRAWINGS">FIG. 2D</figref>) within the cache software of the invention is set to run in 60 seconds from this point. The “found connection manager” program then exits (<b>118</b>).</p>
  <p num="p-0032">Referring now to <figref idrefs="DRAWINGS">FIG. 2D</figref>, the timer program “scan routine” (<b>120</b>) will be described. The program looks into the OpenVMS system database and counts all the computer systems present in the network of computer systems in the VMScluster and VAXcluster systems, storing this count as the ‘node count’ (<b>122</b>). The program counts all the remote connections this cache software of the invention has to other cache software of the invention present on other computer systems in the VMScluster and VAXcluster system, storing this count as the ‘connection count’ (<b>124</b>). The program then compares the ‘node count’ against the ‘connection count’ for equality (<b>126</b>). If the counts are equal the cache ‘disable’ status flag is cleared (<b>128</b>), allowing cache operations to proceed. Otherwise the cache ‘disable’ status flag is set (<b>130</b>), disabling cache operations until the counts become equal. The program then looks to see if the cache is off (<b>132</b>), if so, the “scan routine” is scheduled to run again in 10 seconds from this point (<b>134</b>) and the program exits (<b>136</b>). The cache is set to off when the cache software of the invention is loaded into the operating software. The cache is set to on by the user CACHE command. If the cache is turned on, the program proceeds to calculate the hit rate of the three caches, small, medium, and large, based on the number of hits over time (<b>138</b>). The program checks the available free memory of the OpenVMS system (<b>140</b>). If the available free memory is low (<b>142</b>), the cache software of the invention will release some of the memory held by the cache back to the OpenVMS system (<b>144</b>). The memory will be chosen from the cache with the lowest hit rate, then the next lowest, etc., until the OpenVMS systems available free memory is nominal. The detailed program flow for the release of memory is not included in these descriptions. The “scan routine” is scheduled to run again in 60 seconds from this point (<b>146</b>) and the program exits (<b>148</b>).</p>
  <p num="p-0033">Referring now to <figref idrefs="DRAWINGS">FIGS. 3A to 3C</figref>, the program steps performed when the disk cache software is started for the present invention will be described. The cache is started from the user CACHE command interface (<b>34</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>). The CACHE command can work either as a menu driven interactive display mode, or as a single command line input for which the presently preferred embodiment defines as the CACHE START command. When starting the cache the user can specify the bucket sizes for the three caches, small, medium, and large, along with other factors, such as the maximum amount of memory the cache software of the invention is allowed to use for the cached data. Default values will be used for any of the factors not specified by the user when the cached is started. From the CACHE START command the program starts executing in the user interface code (<b>34</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>), called at the “start command” entry point (<b>150</b>). The program begins by checking that the user has sufficient operating system privilege to alter the cache state (<b>152</b>). If not, the program exits in error (<b>154</b>). The program obtains the total amount of memory in the system from OpenVMS (<b>156</b>). The program checks whether cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) has been loaded into the system (<b>158</b>). If not, the cache driver is loaded (<b>160</b>) into the computer system. The current settings for the cache is obtained from the cache driver characteristics and status (<b>162</b>). These settings will be used as the defaults for any factors not specified by the user in the CACHE command, allowing the cache to be restarted with the same characteristics between successive starting and stopping of the cache, except for those that the user explicitly changes. From the obtained current cache status the program checks whether the cache is already on (<b>164</b>), having already been started and if so, exits in error (<b>166</b>). The program sets all the required cache characteristics from those explicitly specified by the user in the CACHE command and the defaults for any not specified (<b>168</b>), into a set-up buffer. If the OpenVMS system is cooperating in a VMScluster and VAXcluster (<b>170</b>), the program verifies that the OpenVMS system ‘alloclass’ parameter is set to some non-zero value (<b>172</b>). If the OpenVMS system ‘alloclass’ parameter is currently set to 0, the program exits in error (<b>174</b>). The OpenVMS system ‘alloclass’ parameter forms part of the disk I/O device name, allowing consistent multipath accesses for the disk I/O devices in the VMScluster and VAXcluster environment. The program checks that the software licence for the cache software of the invention is valid (<b>176</b>). If not, the program exits in error (<b>178</b>). The maximum amount of disk I/O devices allowed to be cached is obtained from the software licensing information, the value is placed into the cache set-up buffer (<b>180</b>). The cache set-up buffer is then sent (<b>182</b>) by the user command interface code (<b>34</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) to the cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>). The remaining cache start and set up takes place in the cache driver, which runs at a high privilege on the system, allowing the code to directly interface into the OpenVMS system. On receipt of the cache start set-up information, the cache driver begins execution at its “start set mode” entry point (<b>184</b>). The program checks to see if the cache is currently shutting down (<b>186</b>), from a previous user request to stop the cache software of the invention. If so, the program exits in error (<b>188</b>) and the user is requested to wait until caching is fully stopped. The program will check to see if the cache is currently on (<b>190</b>), having already been started from a previous request. If so, the program exits in error (<b>191</b>). The program copies the set-up buffer information from the user start request into the characteristic data cells for the cache (<b>192</b>). The program allocates and initialises the three TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structures from the system pool (<b>194</b>), for the three caches, small, medium and large. For each TCH cache control structure, the program allocates the disk block value hash table (<b>30</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>), dependent on the cache size (<b>196</b>). Each disk block value hash table (<b>30</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) is allocated from the systems available free memory. The cache bucket size for each of the three caches, small, medium, and large, from the user set-up buffer are recorded in the associated TCH (<b>198</b>) The program then gets the first TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) disk control structure (<b>200</b>), setting the TCB to ‘exclude’ mode and ‘default’ mode (<b>202</b>). If there are more TCB's (<b>204</b>), the program gets the next TCB and repeats the loop (<b>200</b>–<b>204</b>), setting each TCB to ‘exclude’ mode and ‘default’ mode until all TCB's are acted upon. The TCB ‘exclude’ mode inhibits the disk I/O device associated with that TCB to have its data cached, until the user explicitly includes that disk I/O device. The TCB ‘default’ mode operates as an indicator to the active caching “process io” program (<b>400</b>, <figref idrefs="DRAWINGS">FIG. 5A</figref>) that caching has been started. The cache is turned on by clearing the cache ‘off’ status flag and setting the cache ‘on’ status flag (<b>206</b>). The program then exits in success (<b>208</b>).</p>
  <p num="p-0034">Referring now to FIGS. <b>4</b>A to <b>4</b>H-<b>2</b>, the program steps for selecting a disk to be included into, or excluded from, the cache software of the invention will be described. The user selects a disk I/O device to be included, or excluded, from the cache software of the invention via the user CACHE command interface (<b>34</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>). The CACHE command can work either as a menu driven interactive display mode, or as a single command line input for which the presently preferred embodiment defines as the CACHE DISK command. When using the CACHE DISK command, the user specifies the name of the disk I/O device as known by the OpenVMS system and whether the disk is to included, or excluded from, the cache software of the invention. From the CACHE DISK command the program starts executing in the user interface code (<b>34</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>), called at the “disk command” entry point (<b>210</b>). The program begins by checking that the user has sufficient operating system privilege to alter the cache state (<b>212</b>). If not, the program exits in error (<b>214</b>). The program checks to see if the disk I/O device does in fact exist on the OpenVMS system, by attempting to assign an I/O channel to the disk I/O device. (<b>216</b>). Failure to assign an I/O channel to the disk I/O device results in the program exiting in error (<b>218</b>). The program gets the characteristics of the disk I/O device (<b>220</b>) and from these characteristics, checks that the disk I/O device is one of the disk I/O device types that are supported by the cache software of the invention (<b>222</b>). If not, the program exits in error (<b>224</b>). The presently preferred embodiment of the invention supports all mechanical disk I/O devices and solid state disk I/O devices that can exist on an OpenVMS system. The presently preferred embodiment of the invention does not support pseudo disk I/O devices that can exist on an Open VMS system, such as a RAMdisk. These pseudo disk I/O devices do not exist on an I/O bus channel, but totally within the physical memory of the Open VMS system Caching these pseudo disk I/O devices in physical memory achieves little, if no, speed advantage on the read I/O and write I/O data transfers to these devices and further reduces the amount of available physical memory to the OpenVMS system unnecessarily. Having verified that the disk I/O device specified in the CACHE DISK command is one of the supported types by the cache software of the invention, the program then checks the CACHE DISK command for an exclude request (<b>226</b>). If the CACHE DISK command requests that the disk I/O device be excluded from the cache software of the invention, the program sends an “exclude disk” I/O command (<b>228</b>) to the cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>), specifying the name of the disk I/O device to be excluded from the cache software of the invention. If the CACHE DISK command is not an exclude request, the program checks whether this is an include request (<b>230</b>). If neither an exclude or include request was specified with the CACHE DISK command, the program exits in error (<b>232</b>). For a CACHE DISK include request command, the program checks whether the OpenVMS system is participating in a VMScluster and VAXcluster (<b>234</b>). If not, the program sends an “include disk” I/O command (<b>236</b>) to the cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>), specifying the name of the disk I/O device to be included in the active cache operations of the invention. If the OpenVMS system is participating in a VMScluster and VAXcluster (<b>234</b>), the program checks whether the disk I/O device specified in the CACHE DISK include request command is the quorum disk for the VMScluster and VAXcluster (<b>238</b>). If the disk I/O device is the quorum disk for the VMScluster and VAXcluster, the program exits in error (<b>240</b>), else the program sends an “include disk” I/O command (<b>236</b>) to the cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>), specifying the name of the disk I/O device to be included in the cache software of the invention. Caching the quorum disk of a VMScluster and VAXcluster could cause possible VMScluster and VAXcluster problems. Not all VMScluster and VAXcluster configurations use a quorum disk. Those VMScluster and VAXcluster configurations that do use a quorum disk use a file on the quorum disk to identify new OpenVMS systems joining the VMScluster and VAXcluster. The new OpenVMS system joining the VMScluster and VAXcluster would not have the cache software of the invention running in its system memory. A write to the file on the quorum disk by this new OpenVMS system would not be intercepted by the cache software of the invention, running on the present OpenVMS systems in the VMScluster and VAXcluster. The cache for the quorum disk data blocks that contain the file for the quorum disk of a VMScluster and VAXcluster would not get altered, and the present OpenVMS systems in the VMScluster and VAXcluster would not notice this new OpenVMS system attempting to join the VMScluster and VAXcluster. For this reason the cache software of the invention will not include the quorum disk of a VMScluster and VAXcluster in its caching operations.</p>
  <p num="p-0035">Referring to <figref idrefs="DRAWINGS">FIG. 4B</figref>, the “include disk” I/O command in the cache driver will now be described. The cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) begins at its “include disk” I/O command entry point (<b>242</b>). Using the disk I/O device in the “include disk” I/O command, the program gets the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device (<b>244</b>). The program checks the number of disks currently cached against the maximum permitted disks (<b>246</b>). The maximum permitted disks that can be cached by the invention at any one time was set during a CACHE START (<figref idrefs="DRAWINGS">FIGS. 3A to 3C</figref>) function. If the current amount of disks cached by the invention are at the maximum permitted, the program exits in error (<b>248</b>), else the program counts this disk as one more cached by the invention (<b>250</b>) The TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device to be included in the cache has the ‘exclude’ mode bit cleared (<b>252</b>). Clearing the ‘exclude’ mode bit in the TCB for the disk I/O device will allow the disk's data to be cached, as will be seen in the description for active cache operations. The program will check if there are any remote connections to cache drivers (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) in other OpenVMS systems of a VMScluster and VAXcluster (<b>254</b>). If there is a remote connection, the program will build an “include disk” communications message (<b>256</b>) and send this message to the remote OpenVMS system (<b>258</b>), specified in the remote connection. The program will then loop to see if there are any more remote connections sending a communications message to each remote connection. If there were no remote connections originally, or the “include disk” communications message has been sent to each remote connection present, the program checks whether the disk I/O device being included in cache operations is part of a disk volume shadow set (<b>260</b>). If not the program exits (<b>262</b>), with the disk I/O device specified in the user CACHE DISK command being successively included in cache operations. If the disk I/O device being included is part of a disk volume shadow set (<b>260</b>), the program gets the name of the shadow set master device (<b>264</b>) from data structures for the disk I/O device from within the OpenVMS system. The program then gets the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the shadow set master device (<b>266</b>) and clears the ‘exclude’ mode bit in this TCB (<b>268</b>). From the shadow set master device the program gets the first disk I/O device that is a member of the disk volume shadow set (<b>270</b>). The program locates the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for this disk volume set member disk I/O device (<b>272</b>) and clears the ‘exclude’ mode bit in this TCB (<b>274</b>). The program will check if there are any remote connections to cache drivers (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) in other OpenVMS systems of a VMScluster and VAXcluster (<b>276</b>). If there is a remote connection, the program will build an “include disk” communications message (<b>278</b>) and send this message to the remote OpenVMS system (<b>280</b>), specified in the remote connection. The program will then loop to see if there are any more remote connections, sending a communications message to each remote connection. If there were no remote connections originally, or the “include disk” communications message has been sent to each remote connection present the program gets the next disk I/O device that is a member of the disk volume shadow set (<b>282</b>). The program loops for each successive disk volume shadow set member disk I/O device, clearing the ‘exclude’ mode bit for each disk I/O device TCB (<b>270</b>–<b>282</b>). When all the disk volume shadow set member disk I/O devices have been dealt with, the program successfully exits (<b>284</b>). This procedure ensures that all members of a disk volume shadow set, including the shadow set master device, are included in cache operations whenever a single disk volume set member disk I/O device, or the shadow set master device, is named as the disk in the CACHE DISK include command, ensuring consistent cache operations for the complete disk volume shadow set.</p>
  <p num="p-0036">Referring to <figref idrefs="DRAWINGS">FIG. 4D</figref>, the program flow for an “include disk” message received over a remote communications channel connection will be described. For all received remote communications message the cache software of the invention will be called at the “message receive” (<b>286</b>) entry point. The program gets the message type from the communications message packet (<b>288</b>) and for an “include disk” message dispatches to the “remote include” program flow (<b>290</b>). The communications message contains the name of the disk I/O device being included, the program will search down all TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structures within the cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) on this OpenVMS system (<b>292</b>) looking for a TCB for this disk I/O device. If this OpenVMS system can access the disk I/O device named in the communication message, indicated by the presence of a TCB for that disk I/O device, the program continues, else the program exits (<b>294</b>) and ignores the communications message. The program checks whether the disk I/O device named in the communications message is a member of a disk volume shadow set (<b>296</b>). If not, the program sets the ‘broadcast’ mode bit in the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device named in the communications message (<b>298</b>), entering the remote connection address, over which the message was received, in the TCB for the disk I/O device (<b>300</b>). The program then exits (<b>302</b>). The ‘broadcast’ mode bit will cause the cache software of the invention to communicate to all remote connection addresses, found within the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure, any write I/O data operations to the disk I/O device from this OpenVMS system. This will ensure that the cache drivers (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>), on those remote connections, that have the disk I/O device included in their cache operations maintain a consistent view of the data within their cache. This is described further within the “active cache operations” <figref idrefs="DRAWINGS">FIGS. 5A to 5P</figref>. If the disk I/O device named in the communications message is a member of a disk volume shadow set (<b>296</b>), the program gets the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the shadow set master device (<b>304</b>). The ‘broadcast’ mode bit is set (<b>306</b>) in the shadow set master device (TCB). The remote connection address over which the message was received is entered in the TCB for the shadow set master device (<b>308</b>), before proceeding with the TCB for the disk I/O device (<b>298</b>) as described above.</p>
  <p num="p-0037">Referring back to <figref idrefs="DRAWINGS">FIG. 4A</figref>, the program flow for a CACHE DISK command that excludes a disk from cache operations will now be described. The user CACHE command interface (<b>34</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>), having processed the CACHE DISK command for an exclude function would send an “exclude disk” I/O command (<b>228</b>) to the cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>), specifying the name of the disk I/O device to be excluded from the active cache operations of the invention.</p>
  <p num="p-0038">Referring now to <figref idrefs="DRAWINGS">FIG. 4E</figref>, the “exclude disk” I/O command in the cache driver will now be described. The cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) begins at its “exclude disk” I/O command entry point (<b>310</b>). Using the disk I/O device in the “exclude disk” I/O command, the program gets the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) disk control structure for the disk I/O device (<b>312</b>). The program reduces the number of disks currently cached by one (<b>314</b>). The program will check if there are any remote connections to cache drivers (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) in other OpenVMS systems of a VMScluster and VAXcluster (<b>316</b>). If there is a remote connection, the program will build an “exclude disk” communications message (<b>318</b>) and send this message to the remote OpenVMS system (<b>320</b>), specified in the remote connection. The program will then loop to see if there are any more remote connections, sending a communications message to each remote connection. If there were no remote connections originally, or the “exclude disk” communications message has been sent to each remote connection present, the program checks whether the disk I/O device being excluded from cache operations is part of a disk volume shadow set (<b>322</b>). If not, the program calls the routine “clear cache data” (<b>350</b>, <figref idrefs="DRAWINGS">FIG. 4G</figref>) to remove any cached data for the disk I/O device being excluded (<b>324</b>). On return the program sets the ‘exclude’ mode bit within the TCB (<b>325</b>) for the disk I/O device and then successfully exits (<b>326</b>). By setting the ‘exclude’ mode bit in the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure, the disk I/O device will have its I/O data excluded from being cached by the invention. If the disk I/O device being excluded from the active cache operations of the invention was a member of a disk volume shadow set (<b>322</b>), the program gets the name of the shadow set master device (<b>328</b>) using data structures within the OpenVMS system. The program then gets the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the shadow set master device (<b>330</b>) and sets the ‘exclude’ mode bit within that TCB (<b>332</b>). The program gets the first disk volume shadow set member device (<b>334</b>) using data structures within the OpenVMS system. The TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for this shadow member disk I/O device is located (<b>336</b>). The program will check if there are any remote connections to cache drivers (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) in other OpenVMS systems of a VMScluster and VAXcluster (<b>338</b>). If there is a remote connection, the program will build an “exclude disk” communications message (<b>340</b>) and send this message to the remote OpenVMS system (<b>342</b>), specified in the remote connection. The program will then loop to see if there are any more remote connections, sending a communications message to each remote connection. If there were no remote connections originally, or the “exclude disk” communications message has been sent to each remote connection present, the program calls (<b>344</b>) the routine “clear cache data” (<b>350</b>, <figref idrefs="DRAWINGS">FIG. 4G</figref>) to remove any cached data for the shadow set member disk I/O device being excluded. On return the program sets the ‘exclude’ mode bit in the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref> disk control structure for the disk volume shadow set member (<b>345</b>). The program gets the next shadow set member disk I/O device (<b>346</b>) and loops (<b>336</b>), sending the “exclude disk” communications message to all remote OpenVMS systems that can access this device and clears the data for this disk I/O device from the cache, using the routine “clear cache data”. When the program has dealt with all the disk volume shadow set members the program successfully exits (<b>348</b>). The cache software of the invention ensures a consistent view for a disk volume shadow set, by excluding all members of a disk volume shadow set whenever a single shadow set member disk I/O device is excluded.</p>
  <p num="p-0039">Referring to <figref idrefs="DRAWINGS">FIG. 4G</figref>, the program flow for the “clear cache data” (<b>350</b>) routine will now be described. The program gets the next—TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure for the three caches, small, medium, and large, of the invention (<b>352</b>). At this point, this Will be the first TCH in the cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) of the invention. The program gets the disk block value hash table (<b>30</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) for this TCH (<b>354</b>). The disk block value hash table consists of a list of singularly linked lists of TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structures with associated cache data buckets (<b>22</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) contained in the cache RAM (<b>20</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>). The program gets the next list entry in the disk block value hash table (<b>356</b>) and gets the next TCMB in that list entry (<b>358</b>). If there are no TCMB's in this list, or the program has reached the end of the list, the program loops to get the next list entry in the disk value hash table (<b>356</b>), until the program has dealt with all the list entries in the disk value hash table, when the program loops to get the next TCH (<b>352</b>). When the program locates a TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure in the disk value hash table (<b>30</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>), the program checks whether the disk I/O device being excluded from the cache operations if the invention is associated with this TCMB (<b>360</b>). If not, the program loops the get the next TCMB in the list (<b>358</b>). When the program finds a TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure associated with the disk I/O device being excluded from the cache operations of the invention, the program removes the TCMB from the list entry within the disk value hash table (<b>362</b>) and removes the TCMB from the LRU queue (<b>28</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) of TCMB's. The TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure is then placed on the free queue (<b>27</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) of TCMB's (<b>364</b>). The program then loops to deal with the next TCMB from the list entry in the disk value hash table (<b>358</b>). When all three TCH (<b>26</b>) cache control structures for the three caches, small, medium, and large, of the invention have been operated upon, the program clears the disk block allocated count within the TCB (<b>368</b>) and then returns to the caller of the “clear cache data” routine (<b>370</b>). This disk block allocation count, within the TCB, is both used as a performance monitor value and as an indicator that the disk I/O device, associated with this TCB, owns some cache data buckets (<b>22</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) contained in the cache RAM (<b>20</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>).</p>
  <p num="p-0040">Referring to <figref idrefs="DRAWINGS">FIG. 4H</figref>, the program flow for an “exclude disk” message received over a remote communications channel connection will be described. For all received remote communications message the cache software of the invention will be called at the “message receive” (<b>372</b>) entry point. The program gets the message type from the communications message packet (<b>374</b>) and for en ‘exclude disk’ message dispatches to the “remote exclude” program flow (<b>376</b>). The communications message contains the name of the disk I/O device being excluded, the program will search down all TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structures within the cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) on this OpenVMS system (<b>378</b>) looking for a TCB for this disk I/O device. If this OpenVMS system can access the disk I/O device named in the communication message, indicated by the presence of a TCB for that disk I/O device, the program continues, else the program exits (<b>380</b>) and ignores the communications message. The program checks whether the disk I/O device named in the communications message is a member of a disk volume shadow set (<b>382</b>). If not, the program deletes the remote connection address, over which the message was received, from the TCB for the disk I/O device (<b>384</b>). If the TCB for the disk I/O device contains other remote connection addresses (<b>386</b>), the program exits (<b>390</b>), indicating that other remote OpenVMS systems can access the device and have the disk I/O device included in their active cache operations of the invention. If the TCB for the disk I/O device now contains no more remote connection addresses (<b>386</b>), the program clears the ‘broadcast’ mode bit in this TCB (<b>388</b>) before exiting (<b>390</b>). The ‘broadcast’ mode bit of the TCB was described above in the “remote include” (<b>290</b>, <figref idrefs="DRAWINGS">FIG. 4D</figref>) program flow. If the disk I/O device named in the ‘exclude disk’ communications message was a member of a disk volume shadow set (<b>382</b>), the program gets the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the shadow set master device (<b>392</b>). As with the disk I/O device named in the ‘exclude disk’ message described above, the program deletes the remote connection address, over which the message was received, from the TCB for the shadow set master device (<b>394</b>). If there are no other remote connection addresses present in the TCB for the shadow set master device (<b>396</b>), the program clears the ‘broadcast’ mode in the TCB for the shadow set master device (<b>398</b>), else the ‘broadcast’ mode bit is left set. The program continues to deal with the TCB for the disk I/O device named in the ‘exclude disk’ message (<b>384</b>).</p>
  <p num="p-0041">Referring to <figref idrefs="DRAWINGS">FIGS. 5A to 5P</figref>, program flow performed by the active data caching of a disk I/O device in the cache software of the invention will be described. Whenever any I/O operation is performed on a disk I/O device, that I/O operation will be intercepted by the cache software of the invention and the program will commence running at the “process io” (<b>400</b>) entry point. The disk I/O device interception was enabled for the cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>), when the cache software was initially loaded into the OpenVMS system and when a new OpenVMS system joined the systems participating in a VMScluster and VAXcluster, see the description for <figref idrefs="DRAWINGS">FIGS. 2A–2D</figref> above. The program locates the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device (<b>402</b>). If the TCB is not found, the program calls “io intercept device” (<b>404</b>) to build a TCB for the device. The program flow for “io intercept device” is not included in the description for the invention. The program flow for “io intercept device” builds a single TCB for a disk I/O device unit, in the same manner as “io intercept global” (<b>64</b>, <figref idrefs="DRAWINGS">FIG. 2B</figref>) does for all disk I/O device units. The presently preferred embodiment of the invention operates on the OpenVMS system. The OpenVMS system specifies the I/O entry point for an I/O device in the device driver for the controller of the I/O device. The controller of the I/O device can have several I/O device units connected to it, but all these I/O device units share the same I/O entry point for the controller. An I/O device unit is identified by a data structure connected in a list of I/O device unit data structures off a single data structure for the I/O device controller. The program “io intercept global” (<b>64</b>, <figref idrefs="DRAWINGS">FIG. 2B</figref>), called during initial loading of the cache software of the invention and when a new OpenVMS system joins a VMScluster and VAXcluster, locates all disk I/O device units accessible by the OpenVMS system, building a TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for that disk I/O device unit, by looking at all the I/O device unit data structures off all the single data structure for the disk I/O device controllers. OpenVMS systems can implement a storage device architecture, known as Digital Storage Architecture (DSA), along with a communications protocol, known as Mass Storage Control Protocol (MSCP), which dictate that a disk I/O device is allowed to come on-line and available to the OpenVMS system after the OpenVMS system has been loaded and initialised. The software for the DSA and MSCP will cause a new data structure, for this recently available disk I/O device, to be built and connected into the list of other I/O device unit structures off the single data structure for the I/O devices controller. This newly available disk I/O device still shares the same I/O entry point for its controller, in this way the cache software of the invention can intercept an I/O operation for this newly available disk I/O device, but not have a TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure built for it via “io intercept global” (<b>64</b>, <figref idrefs="DRAWINGS">FIG. 2B</figref>). Hence the need for the “io intercept device” (<b>404</b>) program within the “process io” (<b>400</b>) program flow. Having located the TCB (<b>402</b>), or having built a new TCB for a newly available disk I/O device (<b>404</b>), the I/O intercept “process io” program flow proceeds.</p>
  <p num="p-0042">The program checks whether the disk I/O device, whose I/O operation has been intercepted, is a disk volume shadow set master device (<b>406</b>). If so, the program exits via the “basic statistics” program flow (<b>660</b>, <figref idrefs="DRAWINGS">FIG. 50</figref>). Disk volume shadow set master devices are not physical disk I/O device units. Disk volume shadow set master devices are pseudo disk I/O devices generated by an OpenVMS system to bind together a set of physical disk I/O devices forming the disk volume shadow set. Therefore no caching of I/O data is performed by the invention for disk volume shadow set master devices. Any I/O data destined for the disk volume shadow set will be redirected by the software for the disk volume shadow set master device to an appropriate physical disk I/O device, within the disk volume shadow set. The I/O operation intercept “process io” (<b>400</b>) program flow will subsequently intercept the I/O operation to the physical disk I/O device, caching the I/O data for that physical disk I/O device as necessary.</p>
  <p num="p-0043">Having determined that the disk I/O device, whose I/O operation has been intercepted, is a physical device (<b>406</b>), the program looks at the current mode of the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the I/O device (<b>410</b>). If the current mode of the TCB is unknown (<b>412</b>), the program exits via the I/O devices original program for its I/O entry point (<b>414</b>). If the current mode of the TCB is ‘statistics only’ (<b>416</b>), the program exits via the “basic statistics” program flow (<b>660</b>, <figref idrefs="DRAWINGS">FIG. 50</figref>). The mode of ‘statistics only’ is the mode the TCB is set to when the TCB is initially built and active cache operations have not been started via a user CACHE START command. When active cache operations have been started via a user CACHE START command, all TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structures are set to ‘default’ mode (<b>202</b>, <figref idrefs="DRAWINGS">FIG. 3C</figref>). If the current mode of the TCB is ‘default’ (<b>420</b>), the program exits via the “cache on” program flow (<b>424</b> <figref idrefs="DRAWINGS">FIG. 5B</figref>).</p>
  <p num="p-0044">Referring now to <figref idrefs="DRAWINGS">FIG. 5B</figref>, the program flow for “cache on” (<b>424</b>) will be described. The program firstly checks whether this is a process swap I/O operation (<b>426</b>). If so, the program increments by one the count for the number of process swap I/O operations on the OpenVMS system (<b>428</b>). The swap count, not shown in these descriptions of the invention, will affect the total amount of RAM the cache software of the invention is allowed to have for its cached data storage. The program dispatches on the I/O function of the intercepted I/O operation on the disk I/O device (<b>430</b>). The presently preferred embodiment of the invention only supports the OpenVMS I/O functions; ‘io_unload”, ‘io_packack’, ‘io_readlblk’, ‘io_readpblk’, ‘io_writelblk’, ‘io_writepblk’, and ‘io_dse’. For all other OpenVMS I/O functions (<b>431</b>) the program exits via the I/O devices original program for its I/O entry point (<b>432</b>). If the OpenVMS I/O function is ‘io_unload’ (final disk volume dismount operation), or ‘io_packack’ (initial disk volume mount operation) (<b>433</b>), the program calls (<b>434</b>) the “clear cache data” (<b>350</b>, <figref idrefs="DRAWINGS">FIG. 4G</figref>) program flow, on return exiting via the I/O devices original program for its I/O entry point (<b>432</b>). If the OpenVMS I/O function is ‘io_readlblk’ (read logical blocks of disk I/O data), or ‘io_readpblk’ (read physical blocks of disk I/O data) (<b>435</b>), the program dispatches to the “read data” (<b>440</b>, <figref idrefs="DRAWINGS">FIG. 5C</figref>) program flow. If the OpenVMS I/O function is ‘io_writelblk’ (write logical blocks of disk I/O data), or ‘io_writepblk’ (write physical blocks of disk I/O data), or ‘io_dse’ (write data security erase pattern) (<b>437</b>), the program dispatches to the “write data” (<b>572</b>, <figref idrefs="DRAWINGS">FIG. 5K</figref>) program flow.</p>
  <p num="p-0045">Referring to <figref idrefs="DRAWINGS">FIG. 5C</figref>, the “read data” (<b>440</b>) program flow will now be described. The program checks that the byte count for the intercepted read I/O data function is a non-zero positive value (<b>442</b>). If not, the program exits via the “I/O function exit” (<b>564</b>, <figref idrefs="DRAWINGS">FIG. 5J</figref>) program flow. The program records the positive byte count of the intercepted read I/O data function in the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device (<b>446</b>). The program increments the read I/O data function count by one in the TCB (<b>448</b>). The byte count of this intercepted read I/O data function is maximized against previous intercepted read I/O data function byte counts for the disk I/O device (<b>450</b>), the maximized value being recorded in the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device. The above three recorded values form part of the performance monitoring capabilities of the invention. The program checks whether the cache status flag ‘disable’ is set (<b>452</b>), if so, the program exits via the “I/O function exit” (<b>564</b>, <figref idrefs="DRAWINGS">FIG. 5J</figref>) program flow. The cache status flag ‘disable’ indicates that some OpenVMS system in the VMScluster and VAXcluster does not have the cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) of the invention loaded. This normally would indicate that some OpenVMS system is currently joining the VMScluster and VAXcluster and has not yet successfully loaded the cache software of the invention. Alternatively, this would indicate an inconsistent installation of the cache software of the invention. In any case, the cache status flag ‘disable’ indicates an inconsistent view of the cache for the invention across the VMScluster and VAXcluster, preventing active cache operations (and possible subsequent corruption) of the data contained in a disk I/O device. The program next checks the ‘exclude’ mode bit in the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device (<b>454</b>). If this ‘exclude’ mode bit is set, indicating that the I/O data for the disk I/O device is currently excluded from the cache of the invention, the program exits via the “I/O function exit” (<b>564</b>, <figref idrefs="DRAWINGS">FIG. 5J</figref>) program flow. The user CACHE DISK command is used to include a disk I/O device into the active cache operations of the invention, by clearing the ‘exclude’ mode bit in TCB for the disk I/O device (<b>274</b>, <figref idrefs="DRAWINGS">FIG. 4C</figref>). The program checks whether the disk I/O device is currently subject to mount verification on the OpenVMS system (<b>456</b>), indicating that the OpenVMS system is checking the integrity of the volume mounted in the disk I/O device. If so, the program exits via the “I/O function exit” (<b>564</b>, <figref idrefs="DRAWINGS">FIG. 5J</figref>) program flow, allowing the read I/O data to come directly from the disk I/O device. The program next checks whether the read I/O data function involves a partial block transfer (<b>458</b>). If so, the program exits via the “I/O function exit” (<b>564</b>, <figref idrefs="DRAWINGS">FIG. 5J</figref>) program flow. Having carried out the initial checks over the disk I/O device and its intercepted read I/O data transfer, the program can now access the cache of the invention.</p>
  <p num="p-0046">The program matches the byte count size of the intercepted read I/O data transfer against the three cache sizes (<b>460</b>), small, medium, or large, attempting to choose which of the three TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structures this read I/O data will be targeted at. If the byte count size of the intercepted read I/O data transfer is larger than the largest of the three caches, the program increments by one (<b>462</b>) the oversize count in the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device, recording for the performance monitoring capabilities of the invention. The program then exits via the “I/O function exit” (<b>564</b>, <figref idrefs="DRAWINGS">FIG. 5J</figref>) program flow. Having chosen which of the three caches, small, medium, or large, the byte count size of the intercepted read I/O data fits (<b>460</b>). The program hashes the starting disk block value of the intercepted read I/O data transfer (<b>464</b>) and uses this hash value as a pointer into the disk block value hash table (<b>30</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>), to find the start of the hash chain for the TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structures with a matching disk block value. Using the cache bucket size against the starting disk block value of the intercepted read I/O data transfer, the program calculates the lowest disk block starting value (<b>466</b>) that could include this intercepted read I/O data transfer starting disk block in its cache bucket. If this lower limit involves searching the previous hash chain list (<b>468</b>), the program starts searching from this previous hash chain (<b>470</b>). The program gets a TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure from the hash chain (<b>472</b>) and checks whether the disk I/O device associated with the TCMB is the same I/O device as in the intercepted read I/O data transfer (<b>474</b>). If not, the program loops to get the next TCMB (<b>472</b>). When the end of the hash chain is reached, the program checks whether the search commenced with the previous hash chain list as to that required from the starting disk block value in the intercepted read I/O data transfer when the lowest disk block limit was calculated (<b>476</b>). If so, the program starts searching at the start of the actual hash chain (<b>478</b>) for the starting disk block value in the intercepted read I/O data transfer and loops to get a TCMB from that hash chain (<b>472</b>). When the program locates a TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure on the hash chain that is associated with the disk I/O device in the intercepted read I/O data transfer (<b>474</b>), the program checks whether the block range limits of the intercepted read I/O data transfer fall within the range of disk blocks in the TCMB cache data bucket (<b>480</b>), if it does then a cache hit is assumed (<b>482</b>) and the “read cache hit” (<b>546</b>, <figref idrefs="DRAWINGS">FIG. 51</figref>) program flow is followed. If the disk block range does not match (<b>480</b>), the program loops to get the next TCMB from the hash chain (<b>472</b>). When all the TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structures have been searched in the one, or two, hash chains into which the disk block range could fall, with no matching disk block range found for the disk I/O device, a cache miss is assumed (<b>484</b>) and the program follows the “read cache miss” program (<b>486</b>, <figref idrefs="DRAWINGS">FIG. 5F</figref>) flow.</p>
  <p num="p-0047">Referring to <figref idrefs="DRAWINGS">FIG. 5F</figref>, the “read cache miss” program (<b>486</b>) flow will be described. The cache miss count is incremented by one (<b>488</b>) in the TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure, for the selected cache, small, medium, or large. This cache miss count in the TCH is used in the performance monitoring by the invention. The program attempts to allocate a TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure, with its corresponding cache data bucket (<b>22</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>), from the free queue (<b>27</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) of the selected TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure (<b>490</b>). If the program obtains a TCMB from the free queue, this TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure is filled in with the I/O transfer specifications from the intercepted read I/O data transfer (<b>492</b>). The TCMB is paced on the in-progress queue (<b>29</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) of the selected TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure (<b>494</b>). The read data I/O transfer is adjusted (<b>496</b>), so that once again the I/O transfer will be intercepted by the routine “read complete” (<b>524</b>, <figref idrefs="DRAWINGS">FIG. 5H</figref>) in the cache software of the invention, when the read I/O data has completely transferred from the disk I/O device, into the OpenVMS system memory area originally specified in the intercepted read I/O data transfer. The adjusted read I/O data transfer request is then sent to the disk I/O devices original program for its I/O entry point (<b>498</b>) and the program exits (<b>500</b>).</p>
  <p num="p-0048">If the program failed to get a TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure from the free queue (<b>490</b>), the program checks there is sufficient available free memory in the OpenVMS system (<b>502</b>) to allocate a new TCMB and corresponding cache data bucket. If there are sufficient available free memory to allocate more cache space, the program checks whether the cache of the invention has reached its allowable memory limits (<b>504</b>), set by the user when the cache was started with a CACHE START command. If not, the program can allocate a new TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure from the OpenVMS system pool (<b>506</b>) and enough RAM space from the available free memory of OpenVMS to hold the corresponding cache data bucket (<b>508</b>) for the TCMB. The TCMB is associated with the disk I/O device, whose read I/O data transfer was intercepted, and the disk block allocated count within the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure, for the disk I/O device, is increased for this intercepted read I/O data transfer (<b>510</b>). The allocated memory count of the selected TCH (<b>26</b> <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure, is increased by the equivalent cache bucket size (<b>512</b>), to indicate more RAM allocated to this cache. The program proceeds as if a TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) was obtained from the free queue (<b>492</b>–<b>500</b>).</p>
  <p num="p-0049">If there were insufficient available free memory within the OpenVMS system (<b>502</b>), or the cache of the invention has reached its allowable memory limits (<b>504</b>), the program has to try and reuse a current cache bucket for this new intercepted read I/O data transfer (<b>514</b>). The program checks whether the selected TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure has any RAM space allocated, by checking its allocated memory count (<b>516</b>). If the TCH has no allocated memory space then it cannot have any TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structures associated with it, so the program exits via the “I/O function exit” (<b>564</b>, <figref idrefs="DRAWINGS">FIG. 5J</figref>) program flow. If the TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure has memory allocated to it, the program removes (<b>518</b>) a TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure from the front of the LRU queue (<b>28</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>). The program reduces (<b>520</b>) the disk block allocated count within the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device, that was originally associated with this TCB. The TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure from the LRU queue is reallocated to the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure, for the disk I/O device of this newly intercepted read I/O data transfer (<b>522</b>). The disk block allocated count in the TCB for this disk I/O device incremented for this intercepted read I/O data transfer. The program proceeds as if a TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) was obtained from the free queue (<b>492</b>–<b>500</b>).</p>
  <p num="p-0050">Referring to <figref idrefs="DRAWINGS">FIG. 5H</figref>, after the adjusted read I/O data transfer sent to the disk I/O device completes, the cache software of the invention once again intercepts this I/O completion at its “read complete” (<b>524</b>) program entry point. From the completed read I/O data transfer, the program locates the TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure associated with the originally intercepted read I/O data transfer (<b>526</b>). The program checks whether the I/O completed successfully by the disk I/O device (<b>528</b>). If so, the program verifies that the TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure has not been invalidated (<b>530</b>) whilst it was on the in-progress queue (<b>29</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>). If not, the intercepted read I/O data transfer can be cached, so the program copies the read I/O data (<b>532</b>) from the OpenVMS system memory area to which the disk I/O data was transferred into the cache data bucket (<b>22</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) specified in the associated TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure. The TCMB is then removed from the in-progress queue of the selected TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure (<b>534</b>) and placed at the front of the LRU queue (<b>536</b>). The starting disk block value in the read I/O data transfer is hashed and the TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure is placed at the end of the resultant hash chain, for the selected TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure (<b>538</b>). The program sends the read I/O data completion onto the originator of the intercepted read I/O data transfer (<b>540</b>), then exits (<b>541</b>). If the I/O completed in error (<b>528</b>), or the TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure was invalidated (<b>530</b>), the read I/O data is not cached. The TCMB is removed from the in-progress queue (<b>542</b>) and placed on the free queue (<b>543</b>) of the selected TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure. The invalidate count within the TCH is incremented by one (<b>544</b>) for the performance monitoring of the invention. The program sends the read I/O data completion onto the originator of the intercepted read I/O data transfer (<b>540</b>), then exits (<b>541</b>).</p>
  <p num="p-0051">Referring back to <figref idrefs="DRAWINGS">FIG. 5E</figref>, if a TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure, found on a hash chain (<b>472</b>), matches the disk I/O device (<b>474</b>) and the disk block range (<b>480</b>) within the intercepted read I/O data transfer, a cache hit is assumed (<b>482</b>).</p>
  <p num="p-0052">Referring now to <figref idrefs="DRAWINGS">FIG. 5I</figref>, the program follows the “read cache hit” (<b>546</b>) program flow. The matching TCMB is moved to the front of the LRU queue of the selected TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure (<b>548</b>). The data in the corresponding cache data bucket (<b>22</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) is copied to the OpenVMS system memory area specified in the intercepted read I/O data transfer (<b>550</b>). The program checks whether the TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure has been invalidated (<b>552</b>). If not, the cache hit count of the selected TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure is incremented by one (<b>554</b>). The read I/O data completion is sent onto the originator of the intercepted read I/O data transfer (<b>556</b>) and the program exits (<b>558</b>). For this cache hit, no disk I/O device data transfer was involved, the requested read I/O data transfer was sent to the requester at memory speed from the RAM area of the cache, illustrating the speed advantage of using the cache of the invention for read I/O data transfers. If the TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure for the cache hit was invalidated (<b>552</b>), the program increments by one the cache miss count (<b>560</b>) in the TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure. The program exits via the “I/O function exit” (<b>564</b>, <figref idrefs="DRAWINGS">FIG. 5J</figref>) program flow, with the read I/O data transferring directly from the disk I/O device.</p>
  <p num="p-0053">Referring to <figref idrefs="DRAWINGS">FIG. 5J</figref>, the “I/O function exit” (<b>564</b>) program flow will be described. The “I/O function exit” (<b>564</b>) exit path is followed by the read I/O and write I/O active cache operations of the invention, when the cache has been turned on by a user CACHE START command and the I/O data is not targeted at the cache data held in the RAM (<b>20</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>). The program calculates the minimum required OpenVMS system free memory (<b>565</b>) from the set-up information sent to the cache driver (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>), by the user CACHE START command, and compares this value to the current available free memory on the OpenVMS system (<b>566</b>). If there are more available free memory on the OpenVMS system than the minimum requirements of the cache of the invention, the program exits via the intercepted disk I/O devices original program for its I/O entry point (<b>568</b>). If the value of the current available free memory on the OpenVMS system is less than the minimum requirements of the cache of the invention, the program releases and returns to OpenVMS sufficient cache data buckets (<b>22</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) from the RAM (<b>20</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>), until the OpenVMS system available free memory is greater than the requirements of the cache of the invention, or no more RAM (<b>20</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) is owned by the cache of the invention (<b>570</b>). Releasing and returning the cache data buckets (<b>22</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) also entails returning the corresponding TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structures to the OpenVMS system pool. The program will choose the cache data buckets (<b>22</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) starting from the cache that has been least used, determined by the cache hit rate in the performance counters of the TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structures, working towards the cache that has most use. The program flow for the release of the cache data buckets and TCMB's is not detailed in these descriptions. Once sufficient cache data buckets (<b>22</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) have been returned to the OpenVMS system, so that there are sufficient available free memory on the OpenVMS system, the program exits via the intercepted disk I/O devices original program for its I/O entry point (<b>568</b>).</p>
  <p num="p-0054">Referring to <figref idrefs="DRAWINGS">FIG. 5K</figref>, the “write data” (<b>572</b>) program flow will now be described. The program checks that the byte count for the intercepted write I/O data function is a non-zero positive value (<b>574</b>). If not, the program exits via the “I/O function exit” (<b>564</b>, <figref idrefs="DRAWINGS">FIG. 5J</figref>) program flow. The program records the positive byte count of the intercepted write I/O data function in the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device (<b>578</b>). The program increments the write I/O data function count by one in the TCB (<b>580</b>) The above two recorded values form part of the performance monitoring capabilities of the invention. The program checks whether the intercepted disk I/O device is currently subject to mount verification on the OpenVMS system (<b>582</b>), indicating that the OpenVMS system is checking the integrity of the volume mounted in the disk I/O device. If so, the program exits via the “I/O function exit” (<b>564</b>, <figref idrefs="DRAWINGS">FIG. 5J</figref>) program flow, allowing the write I/O data to go directly to the disk I/O device. The program next checks the “exclude” mode bit in the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device (<b>584</b>). If this ‘exclude’ mode bit is set, indicating that the I/O data for the disk I/O device is currently excluded from the cache of the invention on this OpenVMS system, the program checks whether other OpenVMS systems in the VMScluster and VAXcluster have the disk I/O device included in their active cache operations of the invention, by checking whether the ‘broadcast’ mode bit is set in the TCB (<b>586</b>). If no other OpenVMS systems in the VMScluster and VAXcluster have the intercepted disk I/O device included in their active cache operations of the invention, the program exits via the “I/O function exit” (<b>564</b>, <figref idrefs="DRAWINGS">FIG. 5J</figref>) program flow. If the ‘broadcast’ mode bit is set in the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device (<b>586</b>), the “write invalidate” (<b>626</b>, <figref idrefs="DRAWINGS">FIG. 5P</figref>) program flow is entered.</p>
  <p num="p-0055">If the intercepted disk I/O device has been included in the active cache operations of this OpenVMS system (<b>584</b>), the program calls the “cache data invalidate” program (<b>588</b>, <figref idrefs="DRAWINGS">FIG. 5L</figref>).</p>
  <p num="p-0056">Referring to <figref idrefs="DRAWINGS">FIG. 5L</figref>, the “cache data invalidate” program invalidates the cached data blocks in all three caches, small, medium, and large, that match the disk block range in this intercepted write I/O data transfer for the disk I/O device The program selects a TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure (<b>589</b>) and calculates the lowest and highest possible cached disk block range, using the starting disk block value and byte count in the intercepted write I/O data transfer against the cache bucket size for the selected cache of the invention (<b>590</b>). The program hashes the lowest and highest disk block range values (<b>592</b>). The program will use these hash values as pointers into the disk block value hash table (<b>30</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) of the TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure, to find the start of the hash chain for the TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structures with matching disk block values. Using the lowest calculated hash pointer the program selects the equivalent hash chain list (<b>594</b>) of TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structures in the disk block value hash table (<b>30</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>). The program selects a TCMB on the hash chain (<b>596</b>) and checks whether the disk I/O device associated with the TCMB is the same as the disk I/O device in the intercepted write I/O data transfer (<b>598</b>). If not, the program loops to get the next TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure from the hash chain list (<b>596</b>). If this TCMB is associated with the disk I/O device in the intercepted write I/O data transfer, the program checks whether the disk block range in the TCMB falls anywhere within the range of disk blocks in the intercepted write I/O data transfer (<b>600</b>). If not, the program loops to get the next TCMB (<b>596</b>). If any of the disk blocks in the selected TCMB do fall in the range of disk blocks in the intercepted write I/O data transfer, the program reduces the allocated block count in the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device, by the cache bucket size (<b>602</b>). The program then removes the TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure from the hash chain list (<b>604</b>). The TCMB is removed (<b>606</b>) from the LRU queue (<b>28</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) and inserted (<b>608</b>) on the free queue (<b>27</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) of the selected TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure. The program increments by one the cache invalidate count of the TCH (<b>610</b>) as part of the performance monitoring of the invention and loops to get the next TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure from the hash chain list (<b>596</b>). Once all the TCMB's in the hash chain has been searched, the program checks whether it has searched all the hash chain lists in the lowest and highest disk block range of the intercepted write I/O data transfer (<b>612</b>). If not, the program selects the next hash chain list to search (<b>614</b>) and loops to get a TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure from that list (<b>596</b>). When all the possible hash chain lists for the range of disk blocks in the intercepted write I/O data transfer have been searched, the program selects (<b>616</b>) the in-progress queue (<b>29</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) of the TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure to search next. The program selects a TCMB on the in-progress queue (<b>618</b>) and checks whether the disk I/O device associated with the TCMB is the same as the disk I/O device in the intercepted write I/O data transfer (<b>620</b>). If not, the program loops to get the next TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structure from the in-progress queue (<b>618</b>). If this TCMB is associated with the disk I/O device in the intercepted write I/O data transfer, the program checks whether the disk block range in the TCMB falls anywhere within the range of disk blocks in the intercepted write I/O data transfer (<b>622</b>). If not, the program loops to get the next TCMB (<b>618</b>) If any of the disk blocks in the selected TCMB do fall in the range of disk blocks in the intercepted write I/O data transfer, the program sets the ‘invalidated’ bit in the TCMB (<b>624</b>) and loops to get the next TCMB on the in-progress queue (<b>618</b>). When the program has searched all TCMB (<b>24</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) bucket control structures on the in-progress queue, the program loops (<b>589</b>) to get the next TCH (<b>26</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) cache control structure. When the program has dealt with all three TCH's, the “cache data invalidate” program returns to its caller (<b>625</b>).</p>
  <p num="p-0057">Referring back to <figref idrefs="DRAWINGS">FIG. 5K</figref>, on return from the “cache data invalidate” program, the “write invalidate” (<b>626</b>, <figref idrefs="DRAWINGS">FIG. 5P</figref>) program flow is entered.</p>
  <p num="p-0058">Referring now to <figref idrefs="DRAWINGS">FIGS. 5M and 5P</figref>, the “write invalidate” (<b>626</b>) program flow will be described. The intercepted write I/O data transfer is altered to once again intercept the I/O transfer when it completes (<b>628</b>). The cache software of the invention will be called at its “write complete” (<b>632</b>) entry point when the write I/O data transfer completes. The program exits via the “I/O function exit” (<b>564</b>, <figref idrefs="DRAWINGS">FIG. 5J</figref>) program flow, with the adjusted write I/O data transfer being sent to the disk I/O device. When the write I/O data transfer has been completed by the disk I/O device, the cache software of the invention intercepts the I/O completion and is called at its “write complete” (<b>632</b>) entry point. The program gets the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the intercepted disk I/O device (<b>634</b>). The program will check if there are any remote connections to cache drivers (<b>10</b>, <figref idrefs="DRAWINGS">FIG. 1A</figref>) in other OpenVMS systems of a VMScluster and VAXcluster (<b>636</b>). If there is a remote connection, the program will build an “invalidate disk” communications message (<b>638</b>) and send this message to the remote OpenVMS system (<b>640</b>), specified in the remote connection. The program will then loop to see if there are any more remote connections (<b>636</b>), sending a communications message to each remote connection. If there were no remote connections originally, or the “invalidate disk” communications message has been sent to each remote connection present, the program sends the write I/O data completion onto the originator of the intercepted write I/O data transfer (<b>642</b>) The program then exits (<b>643</b>).</p>
  <p num="p-0059">Referring to <figref idrefs="DRAWINGS">FIG. 5N</figref>, for all received remote communications message the cache software of the invention will be called at the “message receive” (<b>644</b>) entry point. The program gets the message type from the communications message packet (<b>648</b>) and for an ‘invalidate disk’ message dispatches to the “remote invalidate” program flow (<b>650</b>). The program will check if the cache of the invention has been started (<b>652</b>) on this OpenVMS system, by a user CACHE START command. If not, the program exits (<b>654</b>) ignoring this message. If the cache of the invention has been started, the program attempts to locate a TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device named in the ‘invalidate disk’ communications message (<b>656</b>). If this OpenVMS system does not have a TCB for the disk I/O device, the program exits (<b>654</b>) ignoring the message. The program then calls the “cache data invalidate” program (<b>588</b>, <figref idrefs="DRAWINGS">FIG. 5L</figref>), described above and on return exits (<b>658</b>).</p>
  <p num="p-0060">Referring back to <figref idrefs="DRAWINGS">FIG. 5A</figref>, if the intercepted I/O operation was to a disk volume shadow set master or the cache has not been started on the OpenVMS system via a CACHE START command, the active cache operations of the invention calls the “basic statistics” (<b>660</b>, <figref idrefs="DRAWINGS">FIG. 50</figref>) program flow.</p>
  <p num="p-0061">Referring to <figref idrefs="DRAWINGS">FIG. 5O</figref>, the “basic statistics” (<b>660</b>) program flow will now be described. The program dispatches on the I/O function of the intercepted I/O operation on the disk I/O device (<b>662</b>). The presently preferred embodiment of the invention only supports the OpenVMS I/O functions; ‘io_readlblk’, ‘io_readpblk’, ‘io_writelblk’, ‘io_writepblk’, and ‘io_dse’. For all other OpenVMS I/O functions (<b>663</b>) the program exits via the I/O devices original program for its I/O entry point (<b>664</b>). For intercepted read I/O data operations, ‘io_readlblk’ and ‘io_readpblk’ (<b>665</b>), the program records the performance monitoring read I/O data statistics (<b>666</b>) into the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device. The program then exits via the I/O devices original program for its I/O entry point (<b>664</b>). For intercepted write I/O data operations, ‘io_writelblk’, ‘io_writepblk’, and ‘io_dse’ (<b>667</b>), the program records the performance monitoring write I/O data statistics (<b>668</b>) into the TCB (<b>16</b>, <figref idrefs="DRAWINGS">FIG. 1B</figref>) disk control structure for the disk I/O device. The program checks whether the intercepted disk I/O device is a disk volume shadow set master (<b>669</b>). If so, the program exits via the I/O devices original program for its I/O entry point (<b>664</b>), having no cached data for these pseudo devices. If the intercepted disk I/O device is some physical device, the program checks whether the ‘broadcast’ mode bit is set in the TCB (<b>670</b>). If not, the program exits via the I/O devices original program for its I/O entry point (<b>664</b>). If the ‘broadcast’ mode bit is set in the TCB for the disk I/O device, some other OpenVMS system in the VMScluster and VAXcluster has this disk I/O device included in their active cache operations, the “write invalidate” (<b>626</b>, <figref idrefs="DRAWINGS">FIG. 5P</figref>) program flow is then entered.</p>
  <p num="p-0062">This now completes the description for active cache operations by the invention.</p>
  <heading>REFERENCE MATERIAL</heading> <p num="p-0063">The present preferred embodiment of the invention operates under the OpenVMS system. The help in an understanding of the I/O processes in this cache application the reader may find the following OpenVMS documentation useful.</p>
  <p num="p-0064">The contents of the following books are hereby incorporated by reference herein.
</p> <ul> <li id="ul0001-0001" num="0064">Title: VAX/VMS Internals and Data Structures: version 5.2; Authors: Ruth E. Goldenberg, Lawrence J. Kenah, with the assistance of Denise E. Dumas; Publisher: Digital Press; ISBN:1-55558-059-9</li> <li id="ul0001-0002" num="0065">Title: VMS File System Internals; Author: Kirby McCoy; Publisher: Digital Press; ISBN:1-55558-056-4</li> </ul> <p num="p-0065">Open VMS Manuals: The following manuals are contained in the various OpenVMS Manual documentation sets and kits available from Digital Equipment Corporation.</p>
  <p num="p-0066">The following two manuals are contained in the Open VMS Optional Documentation kit:
</p> <ul> <li id="ul0002-0001" num="0068">Title: OpenVMS VAX Device Support Manual; Order No.: AA-PWC8A-TE</li> <li id="ul0002-0002" num="0069">Title: OpenVMS VAX Device Support Reference Manual; Order No.: AA-PWC9A-TE</li> </ul> <p num="p-0067">The following manual is contained in the Advanced System Management kit within the Open VMS Standard Documentation set:
</p> <ul> <li id="ul0003-0001" num="0071">Title: VMScluster Systems for Open VMS; Order No.: AA-PV5WA-TK</li> </ul> <p num="p-0068">The following two manuals are contained in the Open VMS Systems Integrated Products documentation:
</p> <ul> <li id="ul0004-0001" num="0073">Title: VAX Volume Shadowing Manual; Order No.: AA-LB18A-TE</li> <li id="ul0004-0002" num="0074">Title: Volume Shadowing for Open VMS; Order No.: AA-PVXMA-TE</li> </ul> <p num="p-0069">The above OpenVMS manuals can be obtained from Digital Equipment Corporation at the following address: Digital Equipment Corporation; P.O. Box CS2008; Nashua, N.H. 03051USA</p>
  <p num="p-0070">All of the above listed OpenVMS manuals are hereby incorporated by reference herein.</p>
</div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3820078">US3820078</a></td><td class="patent-data-table-td patent-date-value">Oct 5, 1972</td><td class="patent-data-table-td patent-date-value">Jun 25, 1974</td><td class="patent-data-table-td ">Honeywell Inf Systems</td><td class="patent-data-table-td ">Multi-level storage system having a buffer store with variable mapping modes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4622631">US4622631</a></td><td class="patent-data-table-td patent-date-value">Dec 30, 1983</td><td class="patent-data-table-td patent-date-value">Nov 11, 1986</td><td class="patent-data-table-td ">Plexus Computers, Inc.</td><td class="patent-data-table-td ">Data processing system having a data coherence solution</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4755930">US4755930</a></td><td class="patent-data-table-td patent-date-value">Jun 27, 1985</td><td class="patent-data-table-td patent-date-value">Jul 5, 1988</td><td class="patent-data-table-td ">Encore Computer Corporation</td><td class="patent-data-table-td ">Hierarchical cache memory system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4775955">US4775955</a></td><td class="patent-data-table-td patent-date-value">Dec 18, 1987</td><td class="patent-data-table-td patent-date-value">Oct 4, 1988</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Cache coherence mechanism based on locking</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4849879">US4849879</a></td><td class="patent-data-table-td patent-date-value">Sep 2, 1986</td><td class="patent-data-table-td patent-date-value">Jul 18, 1989</td><td class="patent-data-table-td ">Digital Equipment Corp</td><td class="patent-data-table-td ">Data processor performance advisor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5025366">US5025366</a></td><td class="patent-data-table-td patent-date-value">Jan 20, 1988</td><td class="patent-data-table-td patent-date-value">Jun 18, 1991</td><td class="patent-data-table-td ">Advanced Micro Devices, Inc.</td><td class="patent-data-table-td ">Organization of an integrated cache unit for flexible usage in cache system design</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5055999">US5055999</a></td><td class="patent-data-table-td patent-date-value">Dec 22, 1987</td><td class="patent-data-table-td patent-date-value">Oct 8, 1991</td><td class="patent-data-table-td ">Kendall Square Research Corporation</td><td class="patent-data-table-td ">Multiprocessor digital data processing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5060144">US5060144</a></td><td class="patent-data-table-td patent-date-value">Mar 16, 1989</td><td class="patent-data-table-td patent-date-value">Oct 22, 1991</td><td class="patent-data-table-td ">Unisys Corporation</td><td class="patent-data-table-td ">Locking control with validity status indication for a multi-host processor system that utilizes a record lock processor and a cache memory for each host processor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5062055">US5062055</a></td><td class="patent-data-table-td patent-date-value">May 29, 1990</td><td class="patent-data-table-td patent-date-value">Oct 29, 1991</td><td class="patent-data-table-td ">Digital Equipment Corporation</td><td class="patent-data-table-td ">Data processor performance advisor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5067071">US5067071</a></td><td class="patent-data-table-td patent-date-value">Feb 27, 1985</td><td class="patent-data-table-td patent-date-value">Nov 19, 1991</td><td class="patent-data-table-td ">Encore Computer Corporation</td><td class="patent-data-table-td ">Multiprocessor computer system employing a plurality of tightly coupled processors with interrupt vector bus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5091846">US5091846</a></td><td class="patent-data-table-td patent-date-value">Oct 30, 1989</td><td class="patent-data-table-td patent-date-value">Feb 25, 1992</td><td class="patent-data-table-td ">Intergraph Corporation</td><td class="patent-data-table-td ">Cache providing caching/non-caching write-through and copyback modes for virtual addresses and including bus snooping to maintain coherency</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5136691">US5136691</a></td><td class="patent-data-table-td patent-date-value">Jan 20, 1988</td><td class="patent-data-table-td patent-date-value">Aug 4, 1992</td><td class="patent-data-table-td ">Advanced Micro Devices, Inc.</td><td class="patent-data-table-td ">Methods and apparatus for caching interlock variables in an integrated cache memory</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5185878">US5185878</a></td><td class="patent-data-table-td patent-date-value">Dec 12, 1990</td><td class="patent-data-table-td patent-date-value">Feb 9, 1993</td><td class="patent-data-table-td ">Advanced Micro Device, Inc.</td><td class="patent-data-table-td ">Programmable cache memory as well as system incorporating same and method of operating programmable cache memory</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5210865">US5210865</a></td><td class="patent-data-table-td patent-date-value">Aug 4, 1992</td><td class="patent-data-table-td patent-date-value">May 11, 1993</td><td class="patent-data-table-td ">Digital Equipment Corporation</td><td class="patent-data-table-td ">Transferring data between storage media while maintaining host processor access for I/O operations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5241641">US5241641</a></td><td class="patent-data-table-td patent-date-value">Mar 28, 1990</td><td class="patent-data-table-td patent-date-value">Aug 31, 1993</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Hierarchical cache memory apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5265235">US5265235</a></td><td class="patent-data-table-td patent-date-value">Feb 26, 1993</td><td class="patent-data-table-td patent-date-value">Nov 23, 1993</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Consistency protocols for shared memory multiprocessors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5276835">US5276835</a></td><td class="patent-data-table-td patent-date-value">Dec 14, 1990</td><td class="patent-data-table-td patent-date-value">Jan 4, 1994</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Non-blocking serialization for caching data in a shared cache</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5282272">US5282272</a></td><td class="patent-data-table-td patent-date-value">Jan 28, 1993</td><td class="patent-data-table-td patent-date-value">Jan 25, 1994</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Interrupt distribution scheme for a computer bus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5287473">US5287473</a></td><td class="patent-data-table-td patent-date-value">Dec 14, 1990</td><td class="patent-data-table-td patent-date-value">Feb 15, 1994</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Non-blocking serialization for removing data from a shared cache</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5297269">US5297269</a></td><td class="patent-data-table-td patent-date-value">May 24, 1993</td><td class="patent-data-table-td patent-date-value">Mar 22, 1994</td><td class="patent-data-table-td ">Digital Equipment Company</td><td class="patent-data-table-td ">Cache coherency protocol for multi processor computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5301290">US5301290</a></td><td class="patent-data-table-td patent-date-value">Mar 14, 1990</td><td class="patent-data-table-td patent-date-value">Apr 5, 1994</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method for minimizing lock processing while ensuring consistency among pages common to local processor caches and a shared external store</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5303362">US5303362</a></td><td class="patent-data-table-td patent-date-value">Mar 20, 1991</td><td class="patent-data-table-td patent-date-value">Apr 12, 1994</td><td class="patent-data-table-td ">Digital Equipment Corporation</td><td class="patent-data-table-td ">Coupled memory multiprocessor computer system including cache coherency management protocols</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5307506">US5307506</a></td><td class="patent-data-table-td patent-date-value">Sep 16, 1992</td><td class="patent-data-table-td patent-date-value">Apr 26, 1994</td><td class="patent-data-table-td ">Digital Equipment Corporation</td><td class="patent-data-table-td ">High bandwidth multiple computer bus apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5323403">US5323403</a></td><td class="patent-data-table-td patent-date-value">Oct 13, 1992</td><td class="patent-data-table-td patent-date-value">Jun 21, 1994</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method for generating identical dependent processes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5327556">US5327556</a></td><td class="patent-data-table-td patent-date-value">May 11, 1993</td><td class="patent-data-table-td patent-date-value">Jul 5, 1994</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Fast intersystem page transfer in a data sharing environment with record locking</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5335327">US5335327</a></td><td class="patent-data-table-td patent-date-value">Mar 29, 1993</td><td class="patent-data-table-td patent-date-value">Aug 2, 1994</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">External memory control techniques with multiprocessors improving the throughput of data between a hierarchically upper processing unit and an external memory with efficient use of a cache memory</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5347648">US5347648</a></td><td class="patent-data-table-td patent-date-value">Jul 15, 1992</td><td class="patent-data-table-td patent-date-value">Sep 13, 1994</td><td class="patent-data-table-td ">Digital Equipment Corporation</td><td class="patent-data-table-td ">Digital computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5353430">US5353430</a></td><td class="patent-data-table-td patent-date-value">Oct 20, 1993</td><td class="patent-data-table-td patent-date-value">Oct 4, 1994</td><td class="patent-data-table-td ">Zitel Corporation</td><td class="patent-data-table-td ">Method of operating a cache system including determining an elapsed time or amount of data written to cache prior to writing to main storage</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5363490">US5363490</a></td><td class="patent-data-table-td patent-date-value">Feb 3, 1992</td><td class="patent-data-table-td patent-date-value">Nov 8, 1994</td><td class="patent-data-table-td ">Unisys Corporation</td><td class="patent-data-table-td ">Apparatus for and method of conditionally aborting an instruction within a pipelined architecture</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5369757">US5369757</a></td><td class="patent-data-table-td patent-date-value">Jun 18, 1991</td><td class="patent-data-table-td patent-date-value">Nov 29, 1994</td><td class="patent-data-table-td ">Digital Equipment Corporation</td><td class="patent-data-table-td ">Recovery logging in the presence of snapshot files by ordering of buffer pool flushing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5390318">US5390318</a></td><td class="patent-data-table-td patent-date-value">May 16, 1994</td><td class="patent-data-table-td patent-date-value">Feb 14, 1995</td><td class="patent-data-table-td ">Digital Equipment Corporation</td><td class="patent-data-table-td ">Managing the fetching and replacement of cache entries associated with a file system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5408653">US5408653</a></td><td class="patent-data-table-td patent-date-value">Apr 15, 1992</td><td class="patent-data-table-td patent-date-value">Apr 18, 1995</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Efficient data base access using a shared electronic store in a multi-system environment with shared disks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5426747">US5426747</a></td><td class="patent-data-table-td patent-date-value">Mar 22, 1991</td><td class="patent-data-table-td patent-date-value">Jun 20, 1995</td><td class="patent-data-table-td ">Object Design, Inc.</td><td class="patent-data-table-td ">Method and apparatus for virtual memory mapping and transaction management in an object-oriented database system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5452447">US5452447</a></td><td class="patent-data-table-td patent-date-value">Dec 21, 1992</td><td class="patent-data-table-td patent-date-value">Sep 19, 1995</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5499367">US5499367</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 1994</td><td class="patent-data-table-td patent-date-value">Mar 12, 1996</td><td class="patent-data-table-td ">Oracle Corporation</td><td class="patent-data-table-td ">System for database integrity with multiple logs assigned to client subsets</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5566315">US5566315</a></td><td class="patent-data-table-td patent-date-value">Dec 30, 1994</td><td class="patent-data-table-td patent-date-value">Oct 15, 1996</td><td class="patent-data-table-td ">Storage Technology Corporation</td><td class="patent-data-table-td ">Process of predicting and controlling the use of cache memory in a computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5577226">US5577226</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 6, 1994</td><td class="patent-data-table-td patent-date-value">Nov 19, 1996</td><td class="patent-data-table-td ">Eec Systems, Inc.</td><td class="patent-data-table-td ">Method and system for coherently caching I/O devices across a network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5606681">US5606681</a></td><td class="patent-data-table-td patent-date-value">Mar 2, 1994</td><td class="patent-data-table-td patent-date-value">Feb 25, 1997</td><td class="patent-data-table-td ">Eec Systems, Inc.</td><td class="patent-data-table-td ">Method and device implementing software virtual disk in computer RAM that uses a cache of IRPs to increase system performance</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5787300">US5787300</a></td><td class="patent-data-table-td patent-date-value">Jan 11, 1996</td><td class="patent-data-table-td patent-date-value">Jul 28, 1998</td><td class="patent-data-table-td ">Oracle Corporation</td><td class="patent-data-table-td ">Method and apparatus for interprocess communications in a database environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5918244">US5918244</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 31, 1996</td><td class="patent-data-table-td patent-date-value">Jun 29, 1999</td><td class="patent-data-table-td ">Eec Systems, Inc.</td><td class="patent-data-table-td ">Method and system for coherently caching I/O devices across a network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6370615">US6370615</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 27, 1999</td><td class="patent-data-table-td patent-date-value">Apr 9, 2002</td><td class="patent-data-table-td ">Superspeed Software, Inc.</td><td class="patent-data-table-td ">Method and system for coherently caching I/O devices across a network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6651136">US6651136</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 16, 2002</td><td class="patent-data-table-td patent-date-value">Nov 18, 2003</td><td class="patent-data-table-td ">Superspeed Software, Inc.</td><td class="patent-data-table-td ">Method and system for coherently caching I/O devices across a network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="http://www.google.com/url?id=fxtzBAABERAJ&amp;q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DJP%26NR%3DS6436351A%26KC%3DA%26FT%3DD&amp;usg=AFQjCNHJ4YuCbSqKwegRFolhdJz28BkK_w">JPS6436351A</a></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td patent-date-value"></td><td class="patent-data-table-td "> </td><td class="patent-data-table-td citation-no-title">Title not available</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="1.05a+Cache+Manager"'>1.05a Cache Manager</a>", AFS distributed tilesystem FAQ, http://www.cis.ohio state.edu/hypertext/faw/usenet/afs-faq/faw-doc-14.html.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="1.2.7+Distributed+File+System"'>1.2.7 Distributed File System</a>", IBM Book Manager Book Server, 1989, IBM Corporation, 1996.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="5.3.3+HPFS386+Architecture"'>5.3.3 HPFS386 Architecture</a>", IBM Book Manager Book Server, 1989, IBM Corporation; 1996.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Abstracts+of+Some+of+C.+Mohan%27s+Papers+and+Patents"'>Abstracts of Some of C. Mohan's Papers and Patents</a>", IBM Almaden Research, pp. 1-38.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Cache-Coherency+Protocols+Keep+Data+Consistent"'>Cache-Coherency Protocols Keep Data Consistent</a>", Gallant, J., Electronic Technology for Engineers and Engineering Managers, Mar. 14, 1991.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Disk+Cache+Replacement+Policies+for+Network+Fileservers"'>Disk Cache Replacement Policies for Network Fileservers</a>", Willick, D.L., Distributed Computing Systems, 1993 Int'l. Conf., pp. 2-11.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Diskeeper+User%27s+Guide+V2.1-Errata"'>Diskeeper User's Guide V2.1-Errata</a>", Executive Software, Glendale, CA.; Apr. 1989.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Distributed+Shared+Memory%3A+A+Survey+of+Issues+and+Algorithms"'>Distributed Shared Memory: A Survey of Issues and Algorithms</a>", Computer, pp. 52-60, Aug. 1991.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Executive+Software%27s+Newkeeper"'>Executive Software's Newkeeper</a>", Executive Software, Glendale, CA.; vol. 4 Issue 4; 1991.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Executive+Software%27s+Newskeeper"'>Executive Software's Newskeeper</a>", Executive Software, Glendale, CA., vol. 3, Issue 8, May/Jun. 1990.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Executive+Software%27s+Newskeeper"'>Executive Software's Newskeeper</a>", Executive Software, Glendale, CA., vol. 3, Issue 9, Jul./Aug. 1990.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Executive+Software%27s+Newskeeper"'>Executive Software's Newskeeper</a>", Executive Software, Glendale, CA., vol. 4, Issue 1, 1991.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Executive+Software%27s+Newskeeper"'>Executive Software's Newskeeper</a>", Executive Software, Glendale, CA., vol. 4, Issue 3, 1991.</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Field+Test+Guide+for+I%2F0+Express+Dynamic+Data+Caching+for+VAX%2FVMS"'>Field Test Guide for I/0 Express Dynamic Data Caching for VAX/VMS</a>", Executive Software, Glendale, CA.'Sep. 1989.</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="File+System+Operation+in+a+VAXcluster+Environment"'>File System Operation in a VAXcluster Environment</a>", Chapter 8, VMS File SystemInternals, McCoy, Digital Press, 1990.</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Global+File+Sharing"'>Global File Sharing</a>", Sun Microsystems, Inc., 1996.</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="HPFS+and+Fat+File+Systems+Description"'>HPFS and Fat File Systems Description</a>",http://www.022bbs.com/file-c/tips/DIHPFT.FAX.</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="I%2F0+Express+Technical+Reports"'>I/0 Express Technical Reports</a>", Executive Software International, Glendale, CA.; Feb. 1992-Jan. 1993.</td></tr><tr><td class="patent-data-table-td ">19</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="I%2F0+Express+User%27s+Guide"'>I/0 Express User's Guide</a>", Executive Software International, Glendale, CA.; Jun. 1990.</td></tr><tr><td class="patent-data-table-td ">20</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Linked+List+Cache+Coherence+for+Scalable+Shared+Memory+Multiprocessors"'>Linked List Cache Coherence for Scalable Shared Memory Multiprocessors</a>", Thapar, Manu et al., Parallel Processing, 1993 Symposium, pp. 34-43.</td></tr><tr><td class="patent-data-table-td ">21</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="NFS+Performance"'>NFS Performance</a>", Sun Microsystems, Inc., 1996.</td></tr><tr><td class="patent-data-table-td ">22</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="SuperCache-Open+VMS+I%2FO+Performance+Accelerator"'>SuperCache-Open VMS I/O Performance Accelerator</a>", TurboSystems International s.a., distributed by EEC Systems, Inc., Sudbury, MA. 1993.</td></tr><tr><td class="patent-data-table-td ">23</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="SuperCacheTM+V.1.2+User+and+Installation+Guide%2C+a+TurboWareTlwl+Product"'>SuperCacheTM V.1.2 User and Installation Guide, a TurboWareTlwl Product</a>", V1.2-08.</td></tr><tr><td class="patent-data-table-td ">24</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="SuperCacheTNI-Open+VMS+I%2FO+Performance+Accelerator"'>SuperCacheTNI-Open VMS I/O Performance Accelerator</a>", Software Product Description SuperCacherM, 1992, 1993 Turbo Systems International s.a.</td></tr><tr><td class="patent-data-table-td ">25</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="The+Design+and+Implementation+of+a+Distributed+File+System"'>The Design and Implementation of a Distributed File System</a>", Goldstein, Digital TechnicalJournal, No. 5, Sep. 1987.</td></tr><tr><td class="patent-data-table-td ">26</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="The+S3.Manufacturing+Procedure+Scalable+Shared+MemoryMultiprocessor"'>The S3.Manufacturing Procedure Scalable Shared MemoryMultiprocessor</a>", Nowatzyk, Andreas et al., System Sciences, 1994 Ann. Hawaii Int'l. Conf., vol., I, Jan. 4, 1994, pp. 144-153.</td></tr><tr><td class="patent-data-table-td ">27</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="The+Stanford+Dash+Multiprocessor"'>The Stanford Dash Multiprocessor</a>", Lenoski et al., Computer, IEEE Computer Society, Mar. 1992, pp. 63-79.</td></tr><tr><td class="patent-data-table-td ">28</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="The+VAX%2FVMS+Distributed+Lock+Manager"'>The VAX/VMS Distributed Lock Manager</a>", Snaman, Jr., William E et al., Digital.Technical Journal, No. 5, Sep. 1987.</td></tr><tr><td class="patent-data-table-td ">29</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="VMS+File+System+Internals"'>VMS File System Internals</a>" Kirby McCoy, Digital Press, Digital Equipment Corporation, 1990.</td></tr><tr><td class="patent-data-table-td ">30</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="xFS%3B+A+Wide+Area+Mass+Storage+File+System"'>xFS; A Wide Area Mass Storage File System</a>", Wang, Randolph Y. et al., Workstation Operating Systems, 1993, pp. 71-178.</td></tr><tr><td class="patent-data-table-td ">31</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Armstrong et al. "<a href='http://scholar.google.com/scholar?q="Oracle7+%28TM%29+Server+Administrator%27s+Guide"'>Oracle7 (TM) Server Administrator's Guide</a>", Oracle Corporation, Aug. 1993.</td></tr><tr><td class="patent-data-table-td ">32</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Armstrong et al. "<a href='http://scholar.google.com/scholar?q="Oracle7+%28TM%29+Server+Concepts+Manual"'>Oracle7 (TM) Server Concepts Manual</a>" Oracle Corporation, Aug. 1993.</td></tr><tr><td class="patent-data-table-td ">33</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Atkinson et al. "<a href='http://scholar.google.com/scholar?q="Persistant+Object+Systems"'>Persistant Object Systems</a>", Tarascon, 1994, pp. 217-234.</td></tr><tr><td class="patent-data-table-td ">34</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bell, Les, "<a href='http://scholar.google.com/scholar?q="OS%2F2+High+Performance+File+System"'>OS/2 High Performance File System</a>", http://www.lesbell.com.ati/hpfstest.html.</td></tr><tr><td class="patent-data-table-td ">35</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bennett et al. "<a href='http://scholar.google.com/scholar?q="Munin%3A+Shared+Memory+for+Distributed+Memory+Multiprocessors"'>Munin: Shared Memory for Distributed Memory Multiprocessors</a>", Computer Science Technical Report, Rice University, pp. 1-22, Apr. 1989.</td></tr><tr><td class="patent-data-table-td ">36</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bobrowski et al. "<a href='http://scholar.google.com/scholar?q="Oracle7+%28TM%29+Server+Administrator%27s+Guide"'>Oracle7 (TM) Server Administrator's Guide</a>", Oracle Corporation, Dec. 1992.</td></tr><tr><td class="patent-data-table-td ">37</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bobrowski et al. "<a href='http://scholar.google.com/scholar?q="Oracle7+%28TM%29+Server+Concepts+Manual"'>Oracle7 (TM) Server Concepts Manual</a>", Oracle Corporation, Dec. 1992.</td></tr><tr><td class="patent-data-table-td ">38</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bowen, Ted S. "<a href='http://scholar.google.com/scholar?q="EEC+ups+ante+in+VMS+disk+caching+arena+with+three-tiered+package+for+VAXClusters."'>EEC ups ante in VMS disk caching arena with three-tiered package for VAXClusters.</a>" Digital Review, Cahners Publishing Co., Mar. 16, 1992 v9 n6 p6(1).</td></tr><tr><td class="patent-data-table-td ">39</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Carter et al. "<a href='http://scholar.google.com/scholar?q="Implementation+and+Performance+of+Munin"'>Implementation and Performance of Munin</a>", Computer Science Laboratory, Rice University, pp. 152-164, 1991.</td></tr><tr><td class="patent-data-table-td ">40</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Carter, B. "<a href='http://scholar.google.com/scholar?q="Efficient+Distributed+Shared+Memory+Based+on+Multi-Protocol+Release+Consistency"'>Efficient Distributed Shared Memory Based on Multi-Protocol Release Consistency</a>", Thesis Paper, Rice University, Sep. 1993, 128 pp.</td></tr><tr><td class="patent-data-table-td ">41</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Dahlin et al. "<a href='http://scholar.google.com/scholar?q="A+Quantitative+Analysis+of+Cache+Policies+for+Scalable+Network+File+Systems"'>A Quantitative Analysis of Cache Policies for Scalable Network File Systems</a>", Computer Science Division, University of California ar Berkeley, 10 pp.</td></tr><tr><td class="patent-data-table-td ">42</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Davis, S. "<a href='http://scholar.google.com/scholar?q="Design+of+VMS+Volume+Shadowing+Phase+II-Host-based+Shadowing"'>Design of VMS Volume Shadowing Phase II-Host-based Shadowing</a>", Digital Technical Journal, vol. 3, No. 3, pp. 7-15, 1991.</td></tr><tr><td class="patent-data-table-td ">43</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Defendant Oracle Corporation's Answer, Jun. 11, 2004, in SuperSpeed Software, Inc., v. Oracle Corporation, U.S. District Court for the District of Southern District of Texas Corpus Christi Division, C-04-214.</td></tr><tr><td class="patent-data-table-td ">44</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Defendant Oracle Corporation's Original (Corrected) Answer, Jun. 17, 2004, in SuperSpeed Software, Inc., v. Oracle Corporation, U.S. District Court for the District of Southern District of Texas Corpus Christi Division, C-04-214.</td></tr><tr><td class="patent-data-table-td ">45</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Dimmick, S. "<a href='http://scholar.google.com/scholar?q="Oracle+Database+Administrator%27s+Guide+Version+6"'>Oracle Database Administrator's Guide Version 6</a>", Oracle Corporation, Nov. 1988.</td></tr><tr><td class="patent-data-table-td ">46</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Duncan, Ray, "<a href='http://scholar.google.com/scholar?q="Design+Goals+and+Implementation+of+the+New+High+Performance+File+System"'>Design Goals and Implementation of the New High Performance File System</a>", Microsoft Systems Journal, Sep. 1989, vol. 4, No. 5.</td></tr><tr><td class="patent-data-table-td ">47</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Feeley et al. "<a href='http://scholar.google.com/scholar?q="Implementing+Global+Memory+Management+in+a+Workstation+Cluster"'>Implementing Global Memory Management in a Workstation Cluster</a>", Department of Computer Science and Engineering, University of Washington and DEC Systems Research Center, 11 pp.</td></tr><tr><td class="patent-data-table-td ">48</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Franklin et al. "<a href='http://scholar.google.com/scholar?q="Global+Memory+Management+in+Client-Server+DBMS+Architectures"'>Global Memory Management in Client-Server DBMS Architectures</a>", Proceedings of the 18&lt;SUP&gt;th &lt;/SUP&gt;VLDB Conference, pp. 596-609, Aug. 1992.</td></tr><tr><td class="patent-data-table-td ">49</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Gray, Cary G. et al., "<a href='http://scholar.google.com/scholar?q="Leases%3A+An+Efficient+Fault-Tolerant+Mechanism+for+Distributed+File+Cache+Consistency"'>Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency</a>", 1989 ACM 089791-338-3/89/0012/0202.</td></tr><tr><td class="patent-data-table-td ">50</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Howard et al. "<a href='http://scholar.google.com/scholar?q="Scale+and+Performance+in+a+Distributed+File+System"'>Scale and Performance in a Distributed File System</a>", ACM Transactions on Computer Systems, vol. 6 No. 1, Feb. 1988, pp. 51-81.</td></tr><tr><td class="patent-data-table-td ">51</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Howard, John, "<a href='http://scholar.google.com/scholar?q="An+Overview+of+the+Andrew+File+System"'>An Overview of the Andrew File System</a>", USENIX Winter Conference,Feb. 9-12, 1988, Dallas, Texas.</td></tr><tr><td class="patent-data-table-td ">52</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">I/O Express Technical Reports, Executive Software International Feb. 1992-Jan. 1993.</td></tr><tr><td class="patent-data-table-td ">53</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">I/O Express User's Guide, Executive Software International Jun. 1, 1990.</td></tr><tr><td class="patent-data-table-td ">54</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Jang, Saqib, "<a href='http://scholar.google.com/scholar?q="NFS+and+DFS+A+Functional+Comparison"'>NFS and DFS A Functional Comparison</a>", AUSPEX Technical Report Apr. 15, 1997.</td></tr><tr><td class="patent-data-table-td ">55</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Johnson "<a href='http://scholar.google.com/scholar?q="Squeezing+Top+Performance+from+an+AXP+Server"'>Squeezing Top Performance from an AXP Server</a>", Johnson, DEC Professional, vol. 12, Issue 4, Apr., 1993.</td></tr><tr><td class="patent-data-table-td ">56</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kahhaleh, B. "<a href='http://scholar.google.com/scholar?q="Analysis+of+Memory+Latency+Factors+and+their+Impact+on+KSRI+MPP+Performance"'>Analysis of Memory Latency Factors and their Impact on KSRI MPP Performance</a>", Department of Electrical Engineering and Computer Science, University of Michigan, pp. 1-17, Apr. 1993.</td></tr><tr><td class="patent-data-table-td ">57</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kazar, Michael L., "<a href='http://scholar.google.com/scholar?q="Synchronization+and+Caching+Issues+in+the+Andrew+File+System"'>Synchronization and Caching Issues in the Andrew File System</a>", USENIX Winter Conference, Feb. 9-12, 1988, Dallas, Texas.</td></tr><tr><td class="patent-data-table-td ">58</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Keleher et al. "<a href='http://scholar.google.com/scholar?q="TreadMarks%3A+Distributed+Shared+Memory+on+Standard+Workstations+and+Operating+Systems"'>TreadMarks: Distributed Shared Memory on Standard Workstations and Operating Systems</a>", Department of Computer Science, Rice University, 16 pp.</td></tr><tr><td class="patent-data-table-td ">59</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kent, C. "<a href='http://scholar.google.com/scholar?q="Cache+Coherence+in+Distributed+Systems"'>Cache Coherence in Distributed Systems</a>", Digital Western Research Laboratory, (C) 1986, 1987, pp. 90.</td></tr><tr><td class="patent-data-table-td ">60</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kredel et al. "<a href='http://scholar.google.com/scholar?q="Oracle+on+the+KSRI+Parallel+Computer"'>Oracle on the KSRI Parallel Computer</a>", Computing Center, University of Mannheim, Aug. 1994, pp. 1-8.</td></tr><tr><td class="patent-data-table-td ">61</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kronenberg et al. "<a href='http://scholar.google.com/scholar?q="The+Vaxcluster+Concept%3A+An+Overview+of+a+Distributed+System"'>The Vaxcluster Concept: An Overview of a Distributed System</a>", Digital Technical Journal, No. 5, pp. 7-21.</td></tr><tr><td class="patent-data-table-td ">62</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kumar, Puneet et al., "<a href='http://scholar.google.com/scholar?q="Log-Based+Directory+Resolution+in+the+Coda+File+System"'>Log-Based Directory Resolution in the Coda File System</a>", 1993 IEEE 0-8186-3330-1/93.</td></tr><tr><td class="patent-data-table-td ">63</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Larus J. "<a href='http://scholar.google.com/scholar?q="Compiling+for+Shared-Memory+and+Message-Passing+Computer"'>Compiling for Shared-Memory and Message-Passing Computer</a>", Computer Sciences Department, University of Wisconsin, Nov. 12, 1993, pp. 1-14.</td></tr><tr><td class="patent-data-table-td ">64</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Leach et al. "<a href='http://scholar.google.com/scholar?q="The+File+System+of+an+Integrated+Local+Network"'>The File System of an Integrated Local Network</a>", ACM, (C) 1985 pp. 309-324.</td></tr><tr><td class="patent-data-table-td ">65</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Levy et al. "<a href='http://scholar.google.com/scholar?q="Distributed+File+Systems%3A+Concepts+and+Examples"'>Distributed File Systems: Concepts and Examples</a>", ACM Computing Surveys, vol. 22, No. 4, Dec. 1990, pp. 321-374.</td></tr><tr><td class="patent-data-table-td ">66</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Li et al. "<a href='http://scholar.google.com/scholar?q="Memory+Coherence+in+Shared+Virtual+Memory+Systems"'>Memory Coherence in Shared Virtual Memory Systems</a>", ACM (C) 1986, pp. 229-239.</td></tr><tr><td class="patent-data-table-td ">67</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Li et al. "<a href='http://scholar.google.com/scholar?q="Memory+Coherence+in+Shared+Virtual+Memory+Systems"'>Memory Coherence in Shared Virtual Memory Systems</a>", ACM Transactions on Computer Systems, vol. 7, No. 4, pp. 321-359, Nov. 1989.</td></tr><tr><td class="patent-data-table-td ">68</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Macklem, Rick, "<a href='http://scholar.google.com/scholar?q="Lessons+Learned+Tuning+the+4.3BSD+Reno+Implementation+of+the+NFS+Protocol"'>Lessons Learned Tuning the 4.3BSD Reno Implementation of the NFS Protocol</a>", USENIX, Winter'91, Dallas, Texas.</td></tr><tr><td class="patent-data-table-td ">69</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">McKusick, Marshall et al., "<a href='http://scholar.google.com/scholar?q="The+Network+Filesystem%22%2C+Chapter+9+of+%22The+Design+and+Implementation+of+the+4ABSD+Operating+System"'>The Network Filesystem", Chapter 9 of "The Design and Implementation of the 4ABSD Operating System</a>", 1996.</td></tr><tr><td class="patent-data-table-td ">70</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mena, Agustin et al., "<a href='http://scholar.google.com/scholar?q="Performance+Characteristics+of+the+DCE+Distributed+File+Service"'>Performance Characteristics of the DCE Distributed File Service</a>", IBM Distributed Computing Environment White Paper.</td></tr><tr><td class="patent-data-table-td ">71</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mogul, Jeffrey C., "<a href='http://scholar.google.com/scholar?q="A+Recovery+Protocol+for+Spritely+NFS"'>A Recovery Protocol for Spritely NFS</a>", USENIX Association File Systems Workshop, May 21-22, 1992, Ann Arbor, Michigan.</td></tr><tr><td class="patent-data-table-td ">72</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mohan et al. "<a href='http://scholar.google.com/scholar?q="Algorithms+for+Flexible+Space+Management+in+Transaction+Systems+Supporting+Fine-Granularity+Locking"'>Algorithms for Flexible Space Management in Transaction Systems Supporting Fine-Granularity Locking</a>", Advances in Database Technology-EDBT '94, 4&lt;SUP&gt;th &lt;/SUP&gt;International Conference on Extending Database Technology, pp. 131-144, Mar. 1994.</td></tr><tr><td class="patent-data-table-td ">73</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mohan et al. "<a href='http://scholar.google.com/scholar?q="Efficient+Locking+and+Caching+of+Data+in+the+Multisystem+Shared+Disks+Transaction+Environment"'>Efficient Locking and Caching of Data in the Multisystem Shared Disks Transaction Environment</a>", Advances in Database Technology-EDBT '92 3&lt;SUP&gt;rd &lt;/SUP&gt;International Conference on Extending Database Technology, pp. 453-468, Mar. 1992.</td></tr><tr><td class="patent-data-table-td ">74</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mohan et al. "<a href='http://scholar.google.com/scholar?q="Efficient+Locking+and+Caching+of+Data+in+the+Multisystem+Shared+Disks+Transaction+Environment"'>Efficient Locking and Caching of Data in the Multisystem Shared Disks Transaction Environment</a>", IBM Research Report, Aug. 21, 1991, pp. 1-15.</td></tr><tr><td class="patent-data-table-td ">75</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mohan et al. "<a href='http://scholar.google.com/scholar?q="Recovery+and+Coherency-Control+Protocols+for+Fast+Intersystem+Page+Transfer+and+Fine-Granularity+Locking+in+a+Shared+Disks+Transaction+Environment"'>Recovery and Coherency-Control Protocols for Fast Intersystem Page Transfer and Fine-Granularity Locking in a Shared Disks Transaction Environment</a>", IBM Research Report, Mar. 15, 1991, pp. 1-31.</td></tr><tr><td class="patent-data-table-td ">76</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mohan, et al. "<a href='http://scholar.google.com/scholar?q="Solutions+to+Hot+Spot+Problems+in+a+Shared+Disks+Transaction+Environment"'>Solutions to Hot Spot Problems in a Shared Disks Transaction Environment</a>" IBM Research Report, Aug. 5, 1991, pp. 453-468.</td></tr><tr><td class="patent-data-table-td ">77</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Molesky et al. "<a href='http://scholar.google.com/scholar?q="Efficient+Locking+for+Shared+Memory+Database+Systems"'>Efficient Locking for Shared Memory Database Systems</a>", Department of Computer Science, University of Massachusetts, Mar. 1994, 28 pp.</td></tr><tr><td class="patent-data-table-td ">78</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Morris et al. "<a href='http://scholar.google.com/scholar?q="Andrew%3A+A+Distributed+Personal+Computing+Environment"'>Andrew: A Distributed Personal Computing Environment</a>", Communications of the ACM, vol. 29, No. 3, Mar. 1986, pp. 184-201.</td></tr><tr><td class="patent-data-table-td ">79</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Nelson et al. "<a href='http://scholar.google.com/scholar?q="Caching+in+the+Sprite+Network+File+System"'>Caching in the Sprite Network File System</a>" ACM Transactions on Computer Systems, vol. 6, No. 1, Feb. 1988, pp. 134-154.</td></tr><tr><td class="patent-data-table-td ">80</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">OS/2's History and Purposehttp://sunsite.nus.sg/pub/os2/phamiacy/Should-Mstory-andPurpose.html.</td></tr><tr><td class="patent-data-table-td ">81</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">PC Tech, "<a href='http://scholar.google.com/scholar?q="Understanding+the+OS%2F2+CONFIG-SYS+File"'>Understanding the OS/2 CONFIG-SYS File</a>", PC Magazine.</td></tr><tr><td class="patent-data-table-td ">82</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Pink et al. "<a href='http://scholar.google.com/scholar?q="Low+Latency+File+Access+in+a+High+Bandwidth+Environment"'>Low Latency File Access in a High Bandwidth Environment</a>", pp. 117-122.</td></tr><tr><td class="patent-data-table-td ">83</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Pirotte et al. "<a href='http://scholar.google.com/scholar?q="Advanced+in+Database+Technology-EDBT+%2792"'>Advanced in Database Technology-EDBT '92</a>", 3&lt;SUP&gt;rd &lt;/SUP&gt;International Conference on Extending Database Technology, Vienna, Austria, Mar. 23-27, 1992.</td></tr><tr><td class="patent-data-table-td ">84</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Plaintiff SuperSpeed Software, Inc.'s Complaint, Apr. 14, 2004, in SuperSpeed Software, Inc., v. Oracle Corporation, U.S. District Court for the District of Southern District of Texas Corpus Christi Division, C-04-214.</td></tr><tr><td class="patent-data-table-td ">85</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Raina "<a href='http://scholar.google.com/scholar?q="Virtual+Shared+Memory%3A+A+Survey+of+Techniques+and+Systems"'>Virtual Shared Memory: A Survey of Techniques and Systems</a>", Department of Computer Science, University of Bristol, United Kingdom, pp. 1-31.</td></tr><tr><td class="patent-data-table-td ">86</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Satyanarayanan, M "<a href='http://scholar.google.com/scholar?q="On+the+Influence+of+Scale+in+a+Distributed+System"'>On the Influence of Scale in a Distributed System</a>", IEEE, (C) 1988, pp. 10-18.</td></tr><tr><td class="patent-data-table-td ">87</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Satyanarayanan, M "<a href='http://scholar.google.com/scholar?q="The+Influence+of+Scale+on+Distributed+File+System+Design"'>The Influence of Scale on Distributed File System Design</a>", IEEE, vol. 18, No. 1 (C) Jan. 1992, pp. 1-8.</td></tr><tr><td class="patent-data-table-td ">88</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Satyanarayanan, M. "<a href='http://scholar.google.com/scholar?q="Scalable%2C+Secure%2C+and+Highly+Available+Distributed+File+System"'>Scalable, Secure, and Highly Available Distributed File System</a>", IEEE (C) 1990, pp. 9-21.</td></tr><tr><td class="patent-data-table-td ">89</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Seltzer, Margo et al., "<a href='http://scholar.google.com/scholar?q="An+Implementation+of+a+Log-Structured+File+System+for+UNIX"'>An Implementation of a Log-Structured File System for UNIX</a>", 1993 Winter USENIX, Jan. 25-29, 1993, San Diego, California.</td></tr><tr><td class="patent-data-table-td ">90</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Sinniah, Raymond R., "<a href='http://scholar.google.com/scholar?q="An+Introduction+to+the+Andrew+File+System"'>An Introduction to the Andrew File System</a>" http://homepages.uel.ac.uk/5291 n/afs/afsdoc.html.</td></tr><tr><td class="patent-data-table-td ">91</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Snaman Jr., et al. "<a href='http://scholar.google.com/scholar?q="The+VAX%2FVMS+Distributed+Lock+Manager"'>The VAX/VMS Distributed Lock Manager</a>", Digital Technical Journal, No. 5, pp. 29-44, Sep. 1987.</td></tr><tr><td class="patent-data-table-td ">92</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Snaman Jr., W. "<a href='http://scholar.google.com/scholar?q="Application+Design+in+a+VAXcluster+System"'>Application Design in a VAXcluster System</a>", Digital Technical Journal, vol. 3, pp. 1-10, 1991.</td></tr><tr><td class="patent-data-table-td ">93</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Steven et al. "<a href='http://scholar.google.com/scholar?q="The+KSRI%3A+Bridging+the+Gap+Between+Shared+Memory+and+MPP%27s"'>The KSRI: Bridging the Gap Between Shared Memory and MPP's</a>", IEEE (C) 1993, pp. 285-294.</td></tr><tr><td class="patent-data-table-td ">94</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">SuperCache-Open VMS I/O Performance Accelerator, 1992, 1993.</td></tr><tr><td class="patent-data-table-td ">95</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Tanenbaum, Andrew S., "<a href='http://scholar.google.com/scholar?q="Operating+Systems%3A+Design+and+Implementation"'>Operating Systems: Design and Implementation</a>", Prentice-Hall, Inc., 1987.</td></tr><tr><td class="patent-data-table-td ">96</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Transarc Corporation, "<a href='http://scholar.google.com/scholar?q="The+AFS+File+System+in+Distributed+Computing+Environments"'>The AFS File System in Distributed Computing Environments</a>", May 1, 1996.</td></tr><tr><td class="patent-data-table-td ">97</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Tseng et al. "<a href='http://scholar.google.com/scholar?q="Parallel+Database+Processing+on+the+KSRI+Computer"'>Parallel Database Processing on the KSRI Computer</a>", ACM (C) 1993, pp. 453-455.</td></tr><tr><td class="patent-data-table-td ">98</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">TURBOCACHET/TURBODISKTM Software Product Description, EEC Systems, Incorporated, Revised Feb. 24, 1992.</td></tr><tr><td class="patent-data-table-td ">99</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">TURBOCACHETM Software Product Description, EEC Systems, Incorporated, Revised:Feb. 24, 1992.</td></tr><tr><td class="patent-data-table-td ">100</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">TURBOCACHETM/TURBODISK Software Installation and User's Guide, EEC Systems Incorporated, Feb. 24, 1992.</td></tr><tr><td class="patent-data-table-td ">101</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">TURBOCACHETM/TURBODISKT, Cover Letter and Release Notes (*read me first*), EEC Systems, Incorporated, Feb. 24, 1992.</td></tr><tr><td class="patent-data-table-td ">102</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">TURBOCACHETM/TURBODISKTm Quick Start Guide, EEC Systems Incorporated, Feb. 24, 1992.</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7831642">US7831642</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 8, 2004</td><td class="patent-data-table-td patent-date-value">Nov 9, 2010</td><td class="patent-data-table-td ">Symantec Operating Corporation</td><td class="patent-data-table-td ">Page cache management for a shared file</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc711/defs711.htm&usg=AFQjCNFvcWlFh993jZihOpA01Yh9sD4HJQ#C711S141000">711/141</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc711/defs711.htm&usg=AFQjCNFvcWlFh993jZihOpA01Yh9sD4HJQ#C711S163000">711/163</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc711/defs711.htm&usg=AFQjCNFvcWlFh993jZihOpA01Yh9sD4HJQ#C711S118000">711/118</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc711/defs711.htm&usg=AFQjCNFvcWlFh993jZihOpA01Yh9sD4HJQ#C711SE12019">711/E12.019</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc711/defs711.htm&usg=AFQjCNFvcWlFh993jZihOpA01Yh9sD4HJQ#C711S152000">711/152</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc711/defs711.htm&usg=AFQjCNFvcWlFh993jZihOpA01Yh9sD4HJQ#C711S113000">711/113</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc711/defs711.htm&usg=AFQjCNFvcWlFh993jZihOpA01Yh9sD4HJQ#C711SE12025">711/E12.025</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0012120000">G06F12/12</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0012000000">G06F12/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0012080000">G06F12/08</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F12/0815">G06F12/0815</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F2212/311">G06F2212/311</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F12/0813">G06F12/0813</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F12/0866">G06F12/0866</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F12/123">G06F12/123</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=fxtzBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F12/0837">G06F12/0837</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06F12/08B4P6</span>, <span class="nested-value">G06F12/08B12</span>, <span class="nested-value">G06F12/08B4N</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Jun 24, 2014</td><td class="patent-data-table-td ">FP</td><td class="patent-data-table-td ">Expired due to failure to pay maintenance fee</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20140502</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 2, 2014</td><td class="patent-data-table-td ">LAPS</td><td class="patent-data-table-td ">Lapse for failure to pay maintenance fees</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 13, 2013</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 5, 2010</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIM 15 IS CANCELLED. CLAIMS 1, 2, 3, 14, 19, 25 AND 26 ARE DETERMINED TO BE PATENTABLE AS AMENDED. CLAIMS 4, 20 AND 27, DEPENDENT ON AN AMENDED CLAIM, ARE DETERMINED TO BE PATENTABLE. CLAIMS 5-13, 16-18, 21-24 AND 28-82 WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 30, 2009</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 10, 2009</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090116</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 24, 2008</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080411</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 4, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SUPERSPEED LLC, MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:SUPERSPEED SOFTWARE, INC.;REEL/FRAME:016967/0246</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20051227</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U3vz75bwTQK2J4ZrUzPY3LIy254DQ\u0026id=fxtzBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U20zT-1YXOHy9k2w3O7c2BrC62dEg\u0026id=fxtzBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U2SAZiRb7o4lPAJ1Lf54nIBCXGeFQ","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Method_and_system_for_coherently_caching.pdf?id=fxtzBAABERAJ\u0026output=pdf\u0026sig=ACfU3U2yCvXnEjgvVhLEk9V9oYigKWXRpQ"},"sample_url":"http://www.google.com/patents/reader?id=fxtzBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>