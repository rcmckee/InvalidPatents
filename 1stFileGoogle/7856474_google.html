<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7856474 - Method and apparatus for identifying documents using a handheld device - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_4ff636b3d23669b7103f3b3a3a18b4cd/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_4ff636b3d23669b7103f3b3a3a18b4cd__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Method and apparatus for identifying documents using a handheld device"><meta name="DC.contributor" content="Raymond F. Ratcliff" scheme="inventor"><meta name="DC.contributor" content="Wireless Recognition Technologies Llc" scheme="assignee"><meta name="DC.date" content="2007-7-20" scheme="dateSubmitted"><meta name="DC.description" content="A method and apparatus for sending information to a data processing apparatus for identifying a document to share with a recipient. A handheld device is capable of communicating with the data processing apparatus. Human-readable information is captured from the document and stored in the handheld device as document data. A communications path is established between the handheld device and the data processing apparatus. The document data is sent to the data processing apparatus through the communications path. Reference documents are provided. Each reference document has reference data stored in a memory. At least a portion of the received document data is extracted as scanning data. The reference data is retrieved from the memory. The scanning data is compared with the reference data. When the scanning data matches at least a portion of the reference data of one of the reference documents, the one reference document is selected as the identified document for forwarding to the recipient."><meta name="DC.date" content="2010-12-21" scheme="issued"><meta name="DC.relation" content="US:20010039497:A1" scheme="references"><meta name="DC.relation" content="US:20020013832:A1" scheme="references"><meta name="DC.relation" content="US:20020019844:A1" scheme="references"><meta name="DC.relation" content="US:20020111210:A1" scheme="references"><meta name="DC.relation" content="US:20020116291:A1" scheme="references"><meta name="DC.relation" content="US:20030055679:A1" scheme="references"><meta name="DC.relation" content="US:20030157976:A1" scheme="references"><meta name="DC.relation" content="US:5655081" scheme="references"><meta name="DC.relation" content="US:5680548" scheme="references"><meta name="DC.relation" content="US:5740549" scheme="references"><meta name="DC.relation" content="US:5978594" scheme="references"><meta name="DC.relation" content="US:5978829" scheme="references"><meta name="DC.relation" content="US:6032137" scheme="references"><meta name="DC.relation" content="US:6041398" scheme="references"><meta name="DC.relation" content="US:6112225" scheme="references"><meta name="DC.relation" content="US:6167428" scheme="references"><meta name="DC.relation" content="US:6167462" scheme="references"><meta name="DC.relation" content="US:6229139" scheme="references"><meta name="DC.relation" content="US:6510509" scheme="references"><meta name="DC.relation" content="US:6515988" scheme="references"><meta name="DC.relation" content="US:6628412" scheme="references"><meta name="DC.relation" content="US:6707581" scheme="references"><meta name="DC.relation" content="US:6725250" scheme="references"><meta name="DC.relation" content="US:6732141" scheme="references"><meta name="DC.relation" content="US:6765559" scheme="references"><meta name="DC.relation" content="US:6771568" scheme="references"><meta name="DC.relation" content="US:6782144" scheme="references"><meta name="DC.relation" content="US:7343324" scheme="references"><meta name="DC.relation" content="US:7392287" scheme="references"><meta name="citation_patent_number" content="US:7856474"><meta name="citation_patent_application_number" content="US:11/780,895"><link rel="canonical" href="http://www.google.com/patents/US7856474"/><meta property="og:url" content="http://www.google.com/patents/US7856474"/><meta name="title" content="Patent US7856474 - Method and apparatus for identifying documents using a handheld device"/><meta name="description" content="A method and apparatus for sending information to a data processing apparatus for identifying a document to share with a recipient. A handheld device is capable of communicating with the data processing apparatus. Human-readable information is captured from the document and stored in the handheld device as document data. A communications path is established between the handheld device and the data processing apparatus. The document data is sent to the data processing apparatus through the communications path. Reference documents are provided. Each reference document has reference data stored in a memory. At least a portion of the received document data is extracted as scanning data. The reference data is retrieved from the memory. The scanning data is compared with the reference data. When the scanning data matches at least a portion of the reference data of one of the reference documents, the one reference document is selected as the identified document for forwarding to the recipient."/><meta property="og:title" content="Patent US7856474 - Method and apparatus for identifying documents using a handheld device"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("L1zpU6H_H6bjsATb14LoCg"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407464522.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("USA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("L1zpU6H_H6bjsATb14LoCg"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407464522.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("USA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7856474?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7856474"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=lWixBgABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7856474&amp;usg=AFQjCNHaWe1Jet96df7-FBppvi8dLmUJtQ" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7856474.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7856474.pdf"></a><a class="appbar-application-grant-link" data-label="Application" href="/patents/US20080010346"></a><a class="appbar-application-grant-link" data-selected="true" data-label="Grant" href="/patents/US7856474"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7856474" style="display:none"><span itemprop="description">A method and apparatus for sending information to a data processing apparatus for identifying a document to share with a recipient. A handheld device is capable of communicating with the data processing apparatus. Human-readable information is captured from the document and stored in the handheld device...</span><span itemprop="url">http://www.google.com/patents/US7856474?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7856474 - Method and apparatus for identifying documents using a handheld device</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7856474 - Method and apparatus for identifying documents using a handheld device" title="Patent US7856474 - Method and apparatus for identifying documents using a handheld device"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7856474 B2</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 11/780,895</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Dec 21, 2010</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Jul 20, 2007</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Mar 27, 2001</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7392287">US7392287</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8285791">US8285791</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20020143875">US20020143875</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20080010346">US20080010346</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20100039533">US20100039533</a>, </span><span class="patent-bibdata-value"><a href="/patents/US20130024199">US20130024199</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">11780895, </span><span class="patent-bibdata-value">780895, </span><span class="patent-bibdata-value">US 7856474 B2, </span><span class="patent-bibdata-value">US 7856474B2, </span><span class="patent-bibdata-value">US-B2-7856474, </span><span class="patent-bibdata-value">US7856474 B2, </span><span class="patent-bibdata-value">US7856474B2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Raymond+F.+Ratcliff%22">Raymond F. Ratcliff</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Wireless+Recognition+Technologies+Llc%22">Wireless Recognition Technologies Llc</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7856474.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7856474.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7856474.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (29),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (1),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (9),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (7)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=lWixBgABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7856474&usg=AFQjCNEXfaPWAiqpxYlwX6dvS6MUjQTBUw">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=lWixBgABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7856474&usg=AFQjCNHcLq49pFOtw83CCtkUrO1U4Gsjdg">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=lWixBgABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7856474B2%26KC%3DB2%26FT%3DD&usg=AFQjCNHJYMALSZTJv8OtBVK4YKKxSOgo9w">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT99406065" lang="EN" load-source="patent-office">Method and apparatus for identifying documents using a handheld device</invention-title></span><br><span class="patent-number">US 7856474 B2</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA81860964" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">A method and apparatus for sending information to a data processing apparatus for identifying a document to share with a recipient. A handheld device is capable of communicating with the data processing apparatus. Human-readable information is captured from the document and stored in the handheld device as document data. A communications path is established between the handheld device and the data processing apparatus. The document data is sent to the data processing apparatus through the communications path. Reference documents are provided. Each reference document has reference data stored in a memory. At least a portion of the received document data is extracted as scanning data. The reference data is retrieved from the memory. The scanning data is compared with the reference data. When the scanning data matches at least a portion of the reference data of one of the reference documents, the one reference document is selected as the identified document for forwarding to the recipient.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(4)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7856474B2/US07856474-20101221-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7856474B2/US07856474-20101221-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7856474B2/US07856474-20101221-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7856474B2/US07856474-20101221-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7856474B2/US07856474-20101221-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7856474B2/US07856474-20101221-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7856474B2/US07856474-20101221-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7856474B2/US07856474-20101221-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(56)</span></span></div><div class="patent-text"><div mxw-id="PCLM33976656" lang="EN" load-source="patent-office" class="claims">
    <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
      <div class="claim-text">1. A method comprising:
<div class="claim-text">storing a plurality of reference documents in a memory device, including storing reference data for each reference document of the plurality of reference documents, wherein the reference data comprises data representing human-readable content from an associated reference document of the plurality of reference documents;</div>
<div class="claim-text">receiving over a wireless connection, document data at a data processing system from a handheld device, wherein the document data corresponds to content of one of the reference documents of the plurality of reference documents, and wherein the document data comprises data captured from human-readable content in a document when scanned by the handheld device, wherein the human-readable content comprises originally published content of the document;</div>
<div class="claim-text">extracting at least a portion of the received document data as scanning data, wherein the scanning data comprises data representing human-readable content from the document;</div>
<div class="claim-text">retrieving from the memory device the reference data for at least one reference document of the plurality of reference documents;</div>
<div class="claim-text">comparing the scanning data with the retrieved reference data, wherein comparing comprises a comparison of (a) the scanning data corresponding to the human-readable content of the document, and (b) the retrieved reference data for the at least one reference document of the plurality of reference documents comprising human-readable content of the at least one reference document as originally published; and</div>
<div class="claim-text">identifying, when the step of comparing the scanning data with the retrieved reference data indicates the scanning data matches at least a portion of the retrieved reference data, one or more reference documents associated with said matched reference data as the reference documents corresponding to the document data received from the handheld device.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
      <div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising sending at least a portion of the identified reference documents to the handheld device.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
      <div class="claim-text">3. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein at least one of the steps of receiving, retrieving, comparing and identifying are performed by a data processing system of a server connected to the handheld device by a wireless network connection.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
      <div class="claim-text">4. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising presenting the matching documents via the handheld device on a user interface capable of receiving user input identifying one of the two or more reference documents of the plurality of reference documents that are identified as the one reference document as the document scanned by the handheld device.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
      <div class="claim-text">5. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further wherein the scanning data does not correspond to non-human-readable information captured from the document that identifies the document and was added to the document after it was originally published.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
      <div class="claim-text">6. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising sending at least a portion of human-readable content from each of a plurality of matching documents of the plurality of reference documents to the handheld device, wherein the matching documents comprise two or more reference documents of the plurality of reference documents that are identified as the one reference document as the document scanned by the handheld device.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
      <div class="claim-text">7. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the document data is received from the handled device over a wireless data connection.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
      <div class="claim-text">8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the scanning data extracted from the received document data includes digital text data.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
      <div class="claim-text">9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the digital text data is derived from any one of a text data and a graphical data.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
      <div class="claim-text">10. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising sending at least a portion of the identified document to a the receiving address.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
      <div class="claim-text">11. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the receiving address information identifies the receiving address of the handheld device.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
      <div class="claim-text">12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein sending at least a portion of the identified document includes:
<div class="claim-text">sending at least a portion of the identified document to the receiving address via transmission over a data network coupling the handheld device to the data processing system.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
      <div class="claim-text">13. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a user operating the handheld devices is informed if no match is found between the document data received from the handheld device and the retrieved reference data.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00014" num="00014" class="claim">
      <div class="claim-text">14. A data processing system for identifying one of a plurality of reference documents, from information received from a handheld device in communication with a data processing system, the data processing system being coupled to a data network and each reference document having reference data, the data processing system comprising:
<div class="claim-text">at least one memory device in which a plurality of instructions are stored; and</div>
<div class="claim-text">a processor coupled to the at least one memory device and capable of executing instructions in the memory device to one or more of said plurality of instructions to: (i) access said reference data in the at least one memory device, and (ii) receive the information from the handheld device, wherein the information comprises actual data scanned from a document by the handheld device, wherein execution of the instructions causing a plurality of steps to be performed including:
<div class="claim-text">storing a plurality of reference documents, each reference document having associated reference data stored in the at least one memory device coupled to the data processing system, wherein the associated reference data for each reference document comprises data representing content from the associated reference document of the plurality of reference documents;</div>
<div class="claim-text">receiving the information from a handheld device in communication with the data processing system;</div>
<div class="claim-text">extracting at least a portion of the received document data as scanning data, wherein the scanning data corresponds to data captured by the handheld device from human-readable content of a document;</div>
<div class="claim-text">retrieving reference data for at least one reference document of the plurality of reference documents from the at least one memory device;</div>
<div class="claim-text">comparing said scanning data with said reference data, wherein said comparing comprises a comparison of (a) scanning data corresponding to the human-readable content of the one reference document, and (b) the reference data for the at least one reference document of the plurality of reference documents comprising human-readable content of the at least one reference document as originally published; and</div>
<div class="claim-text">identifying, when the scanning data matches at least a portion of the reference data associated with the at least one reference document, the at least one reference document as an identified document.</div>
</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
      <div class="claim-text">15. The data processing system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the execution of the instructions by the processor causes further steps to be performed, namely:
<div class="claim-text">receiving address information identifying a receiving address for a recipient;</div>
<div class="claim-text">establishing a communications path between the data processing system and a device of the recipient via the data network, and</div>
<div class="claim-text">sending, using the address information, the at least a portion of human-readable content of the identified reference document to the receiving address of the recipient via the communications path.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
      <div class="claim-text">16. The data processing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein sending at least a portion of the identified reference document includes:
<div class="claim-text">sending at least a portion of the identified reference document to the receiving address via transmission over a data network coupling the handheld device to the data processing system.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
      <div class="claim-text">17. The data processing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein sending at least a portion of the identified reference document includes:
<div class="claim-text">sending at least a portion of the identified reference document to the receiving address via a communications transmission.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
      <div class="claim-text">18. The data processing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein sending at least a portion of the identified reference document includes sending an image of at least a portion of the identified reference document.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
      <div class="claim-text">19. The data processing system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, further comprising presenting the matching documents via the handheld device on a user interface capable of receiving user input identifying one of the two or more reference documents of the plurality of reference documents that are identified as the one reference document as the document scanned by the handheld device.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00020" num="00020" class="claim">
      <div class="claim-text">20. A method comprising:
<div class="claim-text">capturing information regarding a document utilizing a handheld device, wherein the information comprises actual data from the document, wherein the captured information corresponds to human-readable content from the document and does not comprise information exclusively machine readable;</div>
<div class="claim-text">storing the captured information in a memory of the handheld device;</div>
<div class="claim-text">establishing a communications path between the handheld device and a data processing system coupled to a data network;</div>
<div class="claim-text">sending the captured information from the handheld device to the data processing system via the communications path; and</div>
<div class="claim-text">receiving, at the handheld device from the data processing system via the communications path, data representing at least a portion of data from a reference document identified based on the captured information.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00021" num="00021" class="claim">
      <div class="claim-text">21. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein the step of receiving at least a portion of data from a reference document comprises receiving at least a portion of human-readable content of the document.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00022" num="00022" class="claim">
      <div class="claim-text">22. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the step of receiving includes receiving an image of a portion of the document.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00023" num="00023" class="claim">
      <div class="claim-text">23. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the document is an electronic document.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00024" num="00024" class="claim">
      <div class="claim-text">24. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the document is a physical document.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00025" num="00025" class="claim">
      <div class="claim-text">25. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the physical document provides information to a user.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00026" num="00026" class="claim">
      <div class="claim-text">26. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein capturing the information includes capturing an image of a portion of a physical copy of the document.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00027" num="00027" class="claim">
      <div class="claim-text">27. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein capturing the information includes:
<div class="claim-text">scanning the document to generate scanned information; and</div>
<div class="claim-text">converting the scanned information to digital text data;</div>
<div class="claim-text">wherein storing the captured information includes storing the digital text data.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00028" num="00028" class="claim">
      <div class="claim-text">28. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, wherein capturing the information includes:
<div class="claim-text">receiving audio information as spoken audio, and converting the audio information to digital audio data; and</div>
<div class="claim-text">wherein storing the captured information includes storing the digital audio data.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00029" num="00029" class="claim">
      <div class="claim-text">29. The method of <claim-ref idref="CLM-00020">claim 20</claim-ref>, further comprising receiving the matching documents via the handheld device on a user interface capable of receiving user input identifying one of the two or more reference documents of the plurality of reference documents that are identified as the one reference document as the document scanned by the handheld device.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00030" num="00030" class="claim">
      <div class="claim-text">30. A wireless handheld device operable to establish a wireless data communications path with a data processing system coupled to a wireless data network, comprising:
<div class="claim-text">(i) a transceiver operating to receive and transmit a wireless data connection with the data processing system coupled to the wireless data network, the transceiver being capable of:
<div class="claim-text">establishing a wireless communications path between the handheld device and the data processing system coupled to the wireless data network;</div>
<div class="claim-text">sending captured information from the handheld device to the data processing system via the wireless communications path; and</div>
</div>
<div class="claim-text">(ii) a memory in which a plurality of instructions are stored; and</div>
<div class="claim-text">(iii) a processor coupled to the memory and capable of executing the instructions in the memory, wherein execution of the instructions causes a plurality of steps to be performed including:
<div class="claim-text">capturing information regarding a document utilizing the handheld device as scanning data, wherein the information comprises actual data from the document, wherein the captured information corresponds to human-readable content from the document;</div>
<div class="claim-text">storing the captured information in a memory of the handheld device;</div>
<div class="claim-text">receiving, at the handheld device from the data processing system via the wireless data communications path, data representing a comparing operation and an identifying operation performed at a remote server;</div>
<div class="claim-text">the comparing operation comprising: comparing the scanning data with reference data retrieved at the server wherein a comparison is performed of (a) the scanning data corresponding to the human-readable content of the document, and (b) the retrieved reference data for the at least one reference document of a plurality of reference documents comprising human-readable content; and</div>
<div class="claim-text">the identifying operation comprising: identifying, when the comparing of the scanning data with the retrieved reference data indicates the scanning data matches at least a portion of the retrieved reference data, one or more reference documents associated with said matched reference data as the reference documents corresponding to the data received from the handheld device.</div>
</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00031" num="00031" class="claim">
      <div class="claim-text">31. The handheld device of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the document is an electronic document.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00032" num="00032" class="claim">
      <div class="claim-text">32. The handheld device of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the document is a physical document.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00033" num="00033" class="claim">
      <div class="claim-text">33. The handheld device of <claim-ref idref="CLM-00032">claim 32</claim-ref>, wherein capturing the information includes capturing an image of a portion of the document.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00034" num="00034" class="claim">
      <div class="claim-text">34. The handheld device of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein capturing the information includes:
<div class="claim-text">scanning the document to generate scanned information, and converting the scanned information to digital text data; and</div>
<div class="claim-text">wherein storing the captured information includes storing the digital text data.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00035" num="00035" class="claim">
      <div class="claim-text">35. The handheld device of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein capturing the information includes:
<div class="claim-text">receiving audio information as spoken audio, and</div>
<div class="claim-text">converting the audio information to digital audio data,</div>
<div class="claim-text">wherein storing the captured information includes storing the digital audio data.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00036" num="00036" class="claim">
      <div class="claim-text">36. The handheld device of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the handheld device is a cellular phone.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00037" num="00037" class="claim">
      <div class="claim-text">37. The handheld device of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the handheld device is a personal digital assistant.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00038" num="00038" class="claim">
      <div class="claim-text">38. The handheld device of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the handheld device effects communications over the wireless data communications path with other network devices.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00039" num="00039" class="claim">
      <div class="claim-text">39. The handheld device of <claim-ref idref="CLM-00030">claim 30</claim-ref>, wherein the handheld device captures the information using an image capture capability of the handheld device.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00040" num="00040" class="claim">
      <div class="claim-text">40. A non-transitory processor readable storage medium containing processor readable program code such that when executed by a processor in a data processing system, performs a method for identifying one of a plurality of reference documents, each reference document having reference data stored in a memory, based on information received by the data processing system from a handheld device in wireless communication with the data processing system over a data network, the method comprising:
<div class="claim-text">storing a plurality of reference documents in a memory device, including storing reference data for each reference document of the plurality of reference documents, wherein the reference data comprises data representing content from an associated reference document of the plurality of reference documents, wherein the content comprises human-readable content of the associated reference document;</div>
<div class="claim-text">receiving document data at a data processing system from a handheld device over the wireless communication, wherein the document data corresponds to the human-readable content of one of the reference documents of the plurality of reference documents, and wherein the document data comprises data captured from human-readable content in the one reference document when scanned by the handheld device, wherein the human-readable content comprises originally published content of the one reference document;</div>
<div class="claim-text">extracting at least a portion of the received document data as scanning data, wherein the scanning data corresponds to the human-readable content from the document;</div>
<div class="claim-text">retrieving from the memory device the reference data for at least one reference document of the plurality of reference documents;</div>
<div class="claim-text">comparing the scanning data with the retrieved reference data, wherein comparing comprises a comparison of (a) the scanning data corresponding to the human-readable content of the document, and (b) the reference data for the at least one reference document of the plurality of reference documents comprising human-readable content of the at least one reference document as originally published; and</div>
<div class="claim-text">identifying, when the scanning data matches at least a portion of the retrieved reference data, the one reference document as the document.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00041" num="00041" class="claim">
      <div class="claim-text">41. The processor readable storage medium of <claim-ref idref="CLM-00040">claim 40</claim-ref>, wherein the document is an electronic document.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00042" num="00042" class="claim">
      <div class="claim-text">42. The processor readable storage medium of <claim-ref idref="CLM-00040">claim 40</claim-ref>, wherein the document is a physical document.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00043" num="00043" class="claim">
      <div class="claim-text">43. The processor readable storage medium of <claim-ref idref="CLM-00042">claim 42</claim-ref>, wherein the human-readable content comprises actual data that is in printed or graphical format and capable of being read by a human without requiring machine intervention.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00044" num="00044" class="claim">
      <div class="claim-text">44. The processor readable storage medium of <claim-ref idref="CLM-00040">claim 40</claim-ref>, further comprising converting the scanning data to digital text data and storing the digital text data.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00045" num="00045" class="claim">
      <div class="claim-text">45. The processor readable storage medium of <claim-ref idref="CLM-00040">claim 40</claim-ref>, further comprising converting the scanning data to digital audio data and storing the digital audio data.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00046" num="00046" class="claim">
      <div class="claim-text">46. The processor readable storage medium of <claim-ref idref="CLM-00040">claim 40</claim-ref>, further comprising presenting the matching documents via the handheld device on a user interface capable of receiving user input identifying one of the two or more reference documents of the plurality of reference documents that are identified as the one reference document as the document scanned by the handheld device.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00047" num="00047" class="claim">
      <div class="claim-text">47. A method, comprising:
<div class="claim-text">optically scanning a human-readable content from a document comprising multiple pieces of human-readable content;</div>
<div class="claim-text">generating a scanned information from the human-readable content;</div>
<div class="claim-text">converting the scanned information into a digital document data representative of the human-readable content and operable for being stored by a data processing system;</div>
<div class="claim-text">storing the digital document data in a memory of a data processing system;</div>
<div class="claim-text">storing a plurality of reference documents, wherein the storing comprises storing a plurality of reference data for each of the plurality of reference documents in a memory of a data processing system, wherein said reference data comprises data representing content from an associated one of said reference documents of said plurality of reference documents, and wherein the content comprises a human-readable content of the associated reference document;</div>
<div class="claim-text">comparing at least one first data to a database of at least one second data, at least one of the at least one said data being received over a wireless connection, wherein the at least one first data comprises the digital document data, the at least one second data comprises at least one of the plurality of reference data, and the database comprises said memory of the data processing system storing the plurality of reference data; and</div>
<div class="claim-text">identifying, when the comparing indicates the first data matches at least a portion of one or more of the second data, the one or more reference documents associated with said matched reference data as the respective one or more reference documents corresponding to the scanned human-readable content.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00048" num="00048" class="claim">
      <div class="claim-text">48. The method of <claim-ref idref="CLM-00047">claim 47</claim-ref>, wherein the storing of the digital document data is performed in a memory of a data processing system of a handheld device.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00049" num="00049" class="claim">
      <div class="claim-text">49. The method of <claim-ref idref="CLM-00048">claim 48</claim-ref>, wherein the storing of the plurality of reference documents is performed in a memory of a data processing system of a server.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00050" num="00050" class="claim">
      <div class="claim-text">50. The method of <claim-ref idref="CLM-00049">claim 49</claim-ref>, wherein the comparing is performed in the data processing system of the server.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00051" num="00051" class="claim">
      <div class="claim-text">51. The method of <claim-ref idref="CLM-00050">claim 50</claim-ref>, wherein the identifying is performed in the data processing system of the server, and the results thereof are transmitted to the handheld device.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00052" num="00052" class="claim">
      <div class="claim-text">52. The method of <claim-ref idref="CLM-00051">claim 51</claim-ref>, wherein the data processing system of the server cannot identify a single one of the reference documents corresponding to the first data.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00053" num="00053" class="claim">
      <div class="claim-text">53. The method of <claim-ref idref="CLM-00052">claim 52</claim-ref>, wherein the fact that the data processing system of the server cannot identify a single one of the reference documents corresponding to the first data is communicated over a wireless, cellular connection to the handheld device.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00054" num="00054" class="claim">
      <div class="claim-text">54. The method of <claim-ref idref="CLM-00053">claim 53</claim-ref>, wherein the handheld device is operable to permit a user to optically re-scan a human-readable content from a document comprising multiple pieces of human-readable content, which action causes re-execution of the steps of generating a scanned information, converting the scanned information into a digital document data, storing the digital document data in a memory of a data processing system, storing a plurality of reference documents, comparing at least one first data to a database of at least one second data, and identifying the one or more reference documents associated with said matched reference data.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00055" num="00055" class="claim">
      <div class="claim-text">55. The method of <claim-ref idref="CLM-00049">claim 49</claim-ref>, wherein the memory of the data processing system of the server comprises any one of a directly coupled repository and a repository coupled by a data network connection.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00056" num="00056" class="claim">
      <div class="claim-text">56. The method of <claim-ref idref="CLM-00048">claim 48</claim-ref>, wherein the handheld device comprises at least one of: a mobile phone; and a personal digital assistant (PDA).</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES39883699" lang="EN" load-source="patent-office" class="description">
    <heading>CROSS REFERENCE TO RELATED APPLICATIONS</heading> <p num="p-0002">This application is a Continuation of U.S. application Ser. No. 09/818,003 filed Mar. 27, 2001, the entirety of which is incorporated herein by reference.</p>
    <heading>FIELD</heading> <p num="p-0003">The present invention relates generally to sharing information and, more particularly, to identifying a document from information input to a handheld device and forwarding the document to a designated recipient.</p>
    <heading>BACKGROUND</heading> <p num="p-0004">Sharing information from documents is generally a manual and time-consuming process. When an individual reads a newspaper or magazine article and wishes to share the article with someone, he must endure a multi-stepped process fraught with frustration and potential for mistake.</p>
    <p num="p-0005">A number of conventional means for sharing documents are available, although none are particularly palatable. In the above example, to share the newspaper or magazine article, the individual would have to choose one of the following means: physically tear out or photocopy the article and mail it, photocopy the article and fax it, read the article over the phone, scan the article into a computer and send it electronically, or visit the website for the newspaper or magazine, find the article, then send the uniform resource locator (“URL”) for the website to the desired recipient.</p>
    <p num="p-0006">The tasks above are needlessly time consuming and problematic. In the time required to manipulate the physical document and arrange for sending, the recipient could have already read the article and discussed it with the sender, if only the recipient had received the article sooner. Moreover, with all of the effort required on the part of the sender to coordinate sending the document, there is a strong likelihood the sender may lose interest altogether and not even attempt to send the article.</p>
    <heading>SUMMARY</heading> <p num="p-0007">One aspect of the present invention relates to sending information to a data processing apparatus for identification of a document having the information. A handheld device having a memory is capable of communicating with the data processing apparatus. Information is captured from the document. The captured information is stored in the memory of the handheld device as document data. A communications path is established between the handheld device and the data processing apparatus. The document data is retrieved from the memory of the handheld device and sent to the data processing apparatus through the communications path for identification of the document.</p>
    <p num="p-0008">Another aspect of the present invention relates to identifying the document for sharing with a recipient, in the data processing apparatus. Reference documents are provided. Each reference document has reference data stored in a memory. The document data received from the handheld device is associated with one of the reference documents. At least a portion of the received document data is extracted as scanning data. The reference data is retrieved from the memory. The scanning data is compared with the reference data. When the scanning data matches at least a portion of the reference data of one of the reference documents, the one reference document is selected as the identified document.</p>
    <description-of-drawings> <heading>BRIEF DESCRIPTION OF THE FIGURES</heading> <p num="p-0009">The invention may be better understood with reference to the following figures. The components in the figures are not necessarily to scale, emphasis instead being placed upon clear illustration of principles.</p>
      <p num="p-0010"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a block diagram of a system <b>100</b> for identifying a document and forwarding the document to a designated recipient, constructed according to an exemplary embodiment of the present invention;</p>
      <p num="p-0011"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a flow diagram of a method <b>200</b> for identifying a document and forwarding the document to a designated recipient, performed in accordance with an exemplary embodiment of the present invention; and</p>
      <p num="p-0012"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a block diagram of a data processing apparatus <b>300</b> constructed according to an exemplary embodiment of the present invention.</p>
    </description-of-drawings> <heading>DETAILED DESCRIPTION</heading> <p num="p-0013"> <figref idrefs="DRAWINGS">FIG. 1</figref> is a block diagram of a system <b>100</b> for identifying a document and forwarding the document to a designated recipient, constructed according to an exemplary embodiment of the present invention. In <figref idrefs="DRAWINGS">FIG. 1</figref>, a user <b>105</b>, also referred to herein as a sender, locates a data source such as document <b>110</b>. In some examples, the document <b>110</b> is a physical document such as an article in a periodical like a newspaper or magazine. In other examples, the document <b>110</b> is in electronic form, such as a word processing document or HTML document displayed on a handheld device or other data processing apparatus. Upon reading the document <b>110</b>, the user <b>105</b> realizes he has several associates or desired recipients who, he believes, would also be interested in reading the document <b>110</b>.</p>
    <p num="p-0014">In <figref idrefs="DRAWINGS">FIG. 1</figref>, the user <b>105</b> operates a handheld device such as a mobile phone <b>115</b> or personal digital assistant (“PDA”) <b>120</b>. Other exemplary handheld devices include the following sold under their respective trademarks: Handspring VISOR™, Palm PALM™, HP JORNADA™, Compaq IPAQ™, Research In Motion BLACKBERRY™, NEOPOINT® Smart Phone, PSION® Series 7, NOKIA® Communicator 9000il, Samsung SCH-3500 Smart Phone, and SPRINT PCS TOUCHPOINT™. Other suitable handheld devices include watches and combinations of the above handheld devices. Such watches and devices include Qbe Personal Computing Tablet, QUBIT™ Tablet, Intel Tablet, ONHAND™ PC, daVinci, Franklin REX, Sharp ZAURUS®, Motorola PAGEWRITER® 2000x, and Sharp telMail TM-20.</p>
    <p num="p-0015">In <figref idrefs="DRAWINGS">FIG. 1</figref>, each of the handheld devices <b>115</b> and <b>120</b> includes a memory for storing data, such as a memory <b>310</b> described below with reference to <figref idrefs="DRAWINGS">FIG. 3</figref>. The various handheld devices operated by user <b>105</b> are capable of communicating with a data processing apparatus such as a server <b>125</b>. A communications path can be established between the handheld devices and the server <b>125</b> by conventional techniques, including cellular and other wireless means.</p>
    <p num="p-0016">In some exemplary embodiments, part or all of server <b>125</b> is implemented as the data processing apparatus <b>300</b> described with reference to <figref idrefs="DRAWINGS">FIG. 3</figref>. World Wide Web (“Web”) servers may be readily incorporated. The server <b>125</b> is coupled to and in communication with a data network <b>130</b> such as the Internet, using conventional techniques understood by those skilled in the art. The server <b>125</b> is in communication with a storage means <b>140</b> such as a database or other suitable repository. In one example, server <b>125</b> is directly coupled to repository <b>140</b>. In another example, server <b>125</b> communicates with repository <b>140</b> via data network <b>130</b>. Reference data is stored in storage means <b>140</b> for use by server <b>125</b>, as explained in greater detail below.</p>
    <p num="p-0017">In <figref idrefs="DRAWINGS">FIG. 1</figref>, one example of data network <b>130</b> is the Internet. In other examples, data network <b>130</b> is any network with sufficient bandwidth to transmit data signals received from handheld devices such as mobile phone <b>115</b> and PDA <b>120</b>. Suitable networks include frame relay (FR) networks, ATM networks, wide area networks (WAN), and local area networks (LAN). Other suitable networks <b>130</b> include satellite transmission, radio broadcasting, cable television broadcasting, direct line-of-site transmission, telecom fiber optic transmission, cellular transmission, and wireless transmission, as will be understood by the skilled artisan.</p>
    <p num="p-0018">In <figref idrefs="DRAWINGS">FIG. 1</figref>, a recipient <b>135</b> is also capable of communicating with data network <b>130</b> by conventional means. For example, recipient <b>135</b> may be or include a data processing apparatus or computer system such as a laptop computer, handheld device, cellular phone with data network capabilities, and other devices capable of receiving data signals such as e-mail messages from data network <b>130</b>.</p>
    <p num="p-0019"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a flow diagram of a method <b>200</b> for identifying a document and forwarding the document to a designated recipient, performed in accordance with an exemplary embodiment of the present invention. In <figref idrefs="DRAWINGS">FIG. 2</figref>, the method begins with user <b>105</b> identifying document <b>110</b> in step <b>205</b>.</p>
    <p num="p-0020">In step <b>210</b>, the user <b>105</b> captures information from the document <b>110</b>. In some exemplary embodiments, the handheld device includes an input device such as a microphone and is thus capable of receiving audio voice signals. The user <b>105</b> can convey information by speaking into the microphone. In an alternative embodiment, the microphone is physically separated from the handheld device, yet coupled to the device so the audio signals can easily pass to the device for storage in memory. In another exemplary embodiment, the handheld device is provided with a scanning mechanism. In one example, the scanning mechanism is provided as a “plug-in” cartridge that interfaces with the handheld device, as with the Handspring VISOR™ PDA. The scanner can be coupled to the handheld device by other means for scanning the information from the document and conveying the information to the handheld device for storage. Conventional scanners may be used, as will be understood by those skilled in the art. Other conventional means for converting printed text from the document to digital data may readily be used.</p>
    <p num="p-0021">Various types of information can be captured from the document and stored in the handheld device. Suitable information includes: title of the document <b>110</b>, author, publication name (e.g., name of newspaper or magazine), and the date of publication for document <b>110</b>. The captured information can include one or more of these types of information and/or other types of information.</p>
    <p num="p-0022">When the information is captured by the scanner or scanning mechanism, the scanned information is converted to digital data using conventional techniques. Also, audio signals received by the microphone are converted to digital data using a digital-to-audio (“D/A”) converter or other suitable means. The digital data is then stored in a memory within the handheld device as captured information or document data.</p>
    <p num="p-0023">In <figref idrefs="DRAWINGS">FIG. 2</figref>, the user also provides address information identifying a receiving address for desired recipient <b>135</b> in step <b>215</b>. In one example, an e-mail address for the desired recipient is spoken into the microphone of the handheld device using techniques described above. In another example, the name of the recipient is spoken into the device for a later table lookup operation performed by the server <b>125</b>. In yet another example, an e-mail address or mailing address is typed into the handheld device by the user using a keypad incorporated for data entry. This address information is stored in the memory of the handheld device.</p>
    <p num="p-0024">In <figref idrefs="DRAWINGS">FIG. 2</figref>, after the document information is captured and the address information received by the handheld device, a communications path is established between the handheld device and the server <b>125</b>. In some exemplary embodiments, such communications are established via one or more of the many types of data networks <b>130</b> described above. In step <b>220</b>, the captured document information and the address information are retrieved from the memory in the handheld device and sent to the server <b>125</b> via the established communications path. The server <b>125</b> then receives the captured document information and the address information.</p>
    <p num="p-0025">In step <b>225</b>, the server <b>125</b> then extracts at least a portion of the received document data as scanning data. This extraction may be performed using conventional voice recognition programs to extract portions of digital audio signals and convert these to text-based digital data. Suitable voice recognition programs include Dragon NATURALLY SPEAKING®, Lernout &amp; Hauspie L&amp;H VOICE XPRESS™, and Lernout &amp; Hauspie Power Translator Pro. In step <b>225</b>, the server attempts to identify the source document <b>110</b> using the captured information. To this end, server <b>125</b> cross-references the extracted portion of the information with reference data associated with a plurality of reference documents stored in repository <b>140</b>. That is, the data processing apparatus accesses the reference data, and compares the scanning data with the reference data. In an alternative embodiment, this cross-referencing operation is performed by a human who uses the extracted portion of the information to index physical documents to identify one or more documents having information matching the extracted portion of information.</p>
    <p num="p-0026">In step <b>230</b>, the server <b>125</b> extracts the address data from the information received from the handheld device. For digital address data representing an e-mail address, phone number, or postal mailing address spoken into the microphone, a conventional voice recognition program is used to convert the voice-based address data to text-based digital data. The server <b>125</b> then attempts to identify the recipient using the text-based digital data. In some embodiments, when the address data is spoken or typed exactly, the text-based address data is used directly as the mailing address. In other embodiments, a table lookup operation is performed by server <b>125</b> using the text-based address data by cross-referencing a directory or listing of e-mail addresses associated with real names. This listing is maintained in storage medium <b>140</b>. In an alternative embodiment, this table lookup operation is performed by a human, as will be appreciated by those skilled in the art.</p>
    <p num="p-0027">Proceeding to step <b>240</b>, when the server cannot identify one of the referenced documents in step <b>225</b>, or server <b>125</b> cannot identify recipient <b>135</b> from the address information <b>230</b>, server <b>125</b> sends a signal to the handheld device operated by the user, such as an e-mail message passed via data network <b>130</b>. This signal includes a message requesting further information and/or clarification of the data already submitted. For example, if the system identified two or more documents matching the captured information, the message prompts the user to specify which of the identified documents to send to recipient <b>135</b>. Alternatively, the user may be prompted to re-enter address information. The method then returns to step <b>220</b>, when the user sends such information to the server. In steps <b>225</b> and/or <b>230</b>, the server again attempts to identify the document and/or identify the recipient.</p>
    <p num="p-0028">In <figref idrefs="DRAWINGS">FIG. 2</figref>, the loop represented by steps <b>220</b>, <b>225</b>, <b>230</b>, and <b>240</b>, repeats as necessary until the proper information is gathered by server <b>125</b>. In step <b>235</b>, the identified document can then be sent to the person identified in the identification information. Depending on the type of address information input by the user <b>105</b>, this sending can be performed by attaching an electronic copy of the identified document to an e-mail message and sending the message to a designated e-mail address of the recipient, or sending an electronic copy of the document by facsimile transmission to a designated phone number. Alternatively, a hard copy of the document can be mailed to a residential mailing address for the recipient.</p>
    <p num="p-0029"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a block diagram of a data processing apparatus <b>300</b> that can be incorporated as part of both the handheld device and/or the server <b>125</b> in <figref idrefs="DRAWINGS">FIG. 1</figref>. The data processing apparatus <b>300</b> includes a processor <b>305</b> for executing program instructions stored in a memory <b>310</b>. In some embodiments, processor <b>305</b> includes a single microprocessor, while in others, processor <b>305</b> includes a plurality of microprocessors to define a multi-processor system. The memory <b>310</b> stores instructions and data for execution by processor <b>305</b>, including instructions and data for performing the methods described above. Depending upon the extent of software implementation in data processing apparatus <b>300</b>, the memory <b>310</b> stores executable code when in operation. The memory <b>310</b> includes, for example, banks of read-only memory (ROM), dynamic random access memory (DRAM), as well as high-speed cache memory.</p>
    <p num="p-0030">In <figref idrefs="DRAWINGS">FIG. 3</figref>, within data processing apparatus <b>300</b>, an operating system comprises program instruction sequences that provide a platform for the methods described above. The operating system provides a software platform upon which application programs may execute, in a manner readily understood by those skilled in the art. The data processing apparatus <b>300</b> further comprises one or more applications having program instruction sequences for performing the methods described above.</p>
    <p num="p-0031">In <figref idrefs="DRAWINGS">FIG. 3</figref>, the data processing apparatus <b>300</b> incorporates any combination of additional devices. These include, but are not limited to, a mass storage device <b>315</b>, one or more peripheral devices <b>320</b>, an audio means <b>325</b>, one or more input devices <b>330</b>, one or more portable storage medium drives <b>335</b>, a graphics subsystem <b>340</b>, a display <b>345</b>, and one or more output devices <b>350</b>. The various components are connected via an appropriate bus <b>355</b> as known by those skilled in the art. In alternative embodiments, the components are connected through other communications media known in the art. In one example, processor <b>305</b> and memory <b>310</b> are connected via a local microprocessor bus; while mass storage device <b>315</b>, peripheral devices <b>320</b>, portable storage medium drives <b>335</b>, and graphics subsystem <b>340</b> are connected via one or more input/output (“I/O”) buses.</p>
    <p num="p-0032">In <figref idrefs="DRAWINGS">FIG. 3</figref>, mass storage device <b>315</b> is implemented as fixed and/or removable media, for example, as a magnetic, optical, or magneto-optical disk drive. The drive is preferably a non-volatile storage device for storing data and instructions for use by processor <b>305</b>. In some embodiments, mass storage device <b>315</b> stores client and server information, code for carrying out methods in accordance with exemplary embodiments of the invention, and computer instructions for processor <b>305</b>. In other embodiments, computer instructions for performing methods in accordance with exemplary embodiments of the invention also are stored in processor <b>305</b>. The computer instructions are programmed in a suitable language such as Java or C++.</p>
    <p num="p-0033">In <figref idrefs="DRAWINGS">FIG. 3</figref>, the portable storage medium drive <b>335</b>, in some embodiments, operates in conjunction with a portable non-volatile storage medium, such as a floppy disk, CD-ROM, or other computer-readable medium, to input and output data and code to and from the data processing apparatus <b>300</b>. In some embodiments, methods performed in accordance with exemplary embodiments of the invention are implemented using computer instructions that are stored on such a portable medium and input to the data processing apparatus <b>300</b> via portable storage medium drive <b>335</b>.</p>
    <p num="p-0034">In <figref idrefs="DRAWINGS">FIG. 3</figref>, the peripheral devices <b>320</b> include any type of computer support device, such as an I/O interface, to add functionality to data processing apparatus <b>300</b>. In one example, the peripheral devices include a network interface card for interfacing the data processing apparatus <b>300</b> to a network, a modem, and the like. The peripheral devices also include input devices to provide a portion of a user interface and may include an alphanumeric keypad or a pointing device such as a mouse, a trackball, a stylus, or cursor direction keys. The I/O interface comprises conventional circuitry for controlling input devices and performing particular signal conversions upon I/O data. The I/O interface may include, for example, a keyboard controller, a serial port controller, and/or digital signal processing circuitry.</p>
    <p num="p-0035">In <figref idrefs="DRAWINGS">FIG. 3</figref>, the graphics subsystem <b>340</b> and the display <b>345</b> provide output alternatives of the system. The graphics subsystem <b>340</b> and display <b>345</b> include conventional circuitry for operating upon and outputting data to be displayed, where such circuitry preferably includes a graphics processor, a frame buffer, and display driving circuitry. The display <b>345</b> may include a cathode ray tube (CRT) display, a liquid crystal display (LCD), or other suitable devices. The display <b>345</b> preferably can display at least 256 colors. The graphics subsystem <b>340</b> receives textual and graphical information and processes the information for output to the display <b>345</b>. A video card in the data processing apparatus <b>300</b> also comprises a part of graphics subsystem <b>340</b> and also preferably supports at least 256 colors. For optimal results in viewing digital images, the user should use a video card and monitor that can display the True Color (24 bit color) setting. This setting enables the user to view digital images with photographic image quality.</p>
    <p num="p-0036">In <figref idrefs="DRAWINGS">FIG. 3</figref>, audio means <b>325</b> preferably includes a sound card, on-board sound processing hardware, or a device with built-in processing devices that attach via Universal Serial Bus (USB) or IEEE 1394 (Firewire). The audio means <b>325</b> receives audio signals from a peripheral microphone. In addition, audio means <b>325</b> may include a processor for processing sound. The signals can be processed by the processor in audio means <b>325</b> of data processing apparatus <b>300</b> and passed to other devices as, for example, streaming audio signals.</p>
    <p num="p-0037">In some embodiments, programs for performing methods in accordance with exemplary embodiments of the invention are embodied as computer program products. These generally include a storage medium or media having instructions stored thereon used to program a computer to perform the methods described above. Examples of suitable storage medium or media include any type of disk including floppy disks, optical disks, DVDs, CD ROMs, magnetic optical disks, RAMs, EPROMs, EEPROMs, magnetic or optical cards, hard disk, flash card, smart card, and other media.</p>
    <p num="p-0038">Stored on one or more of the computer readable media, the program includes software for controlling both the hardware of a general purpose or specialized computer or microprocessor. This software also enables the computer or microprocessor to interact with a human or other mechanism -utilizing the results of exemplary embodiments of the invention. Such software includes, but is not limited to, device drivers, operating systems and user applications. Preferably, such computer readable media further include software for performing the methods described above.</p>
    <p num="p-0039">In certain other embodiments, a program for performing an exemplary method of the invention or an aspect thereof is situated on a carrier wave such as an electronic signal transferred over a data network. Suitable networks include the internet, a frame relay network, an ATM network, a wide area network (WAN), or a local area network (LAN). Those skilled in the art will recognize that merely transferring the program over the network, rather than executing the program on a computer system or other device, does not avoid the scope of the invention.</p>
    <p num="p-0040">It should be emphasized that the above-described embodiments of the invention are merely possible examples of implementations set forth for a clear understanding of the principles of the invention. Variations and modifications may be made to the above-described embodiments of the invention without departing from the spirit and principles of the invention. All such modifications and variations are intended to be included herein within the scope of the invention and protected by the following claims.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5655081">US5655081</a></td><td class="patent-data-table-td patent-date-value">Mar 8, 1995</td><td class="patent-data-table-td patent-date-value">Aug 5, 1997</td><td class="patent-data-table-td ">Bmc Software, Inc.</td><td class="patent-data-table-td ">System for monitoring and managing computer resources and applications across a distributed computing environment using an intelligent autonomous agent architecture</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5680548">US5680548</a></td><td class="patent-data-table-td patent-date-value">Dec 2, 1994</td><td class="patent-data-table-td patent-date-value">Oct 21, 1997</td><td class="patent-data-table-td ">Xcellenet, Inc.</td><td class="patent-data-table-td ">Systems and methods for work assignment and distribution from a server to remote/mobile nodes</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5740549">US5740549</a></td><td class="patent-data-table-td patent-date-value">Jun 12, 1995</td><td class="patent-data-table-td patent-date-value">Apr 14, 1998</td><td class="patent-data-table-td ">Pointcast, Inc.</td><td class="patent-data-table-td ">Information and advertising distribution system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5978594">US5978594</a></td><td class="patent-data-table-td patent-date-value">Mar 6, 1997</td><td class="patent-data-table-td patent-date-value">Nov 2, 1999</td><td class="patent-data-table-td ">Bmc Software, Inc.</td><td class="patent-data-table-td ">System for managing computer resources across a distributed computing environment by first reading discovery information about how to determine system resources presence</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5978829">US5978829</a></td><td class="patent-data-table-td patent-date-value">Nov 5, 1996</td><td class="patent-data-table-td patent-date-value">Nov 2, 1999</td><td class="patent-data-table-td ">A.T. &amp; T. Corporation</td><td class="patent-data-table-td ">Apparatus and methods for sharing idle workstations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6032137">US6032137</a></td><td class="patent-data-table-td patent-date-value">May 19, 1998</td><td class="patent-data-table-td patent-date-value">Feb 29, 2000</td><td class="patent-data-table-td ">Csp Holdings, Llc</td><td class="patent-data-table-td ">Remote image capture with centralized processing and storage</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6041398">US6041398</a></td><td class="patent-data-table-td patent-date-value">Jun 26, 1992</td><td class="patent-data-table-td patent-date-value">Mar 21, 2000</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Massively parallel multiple-folded clustered processor mesh array</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6112225">US6112225</a></td><td class="patent-data-table-td patent-date-value">Mar 30, 1998</td><td class="patent-data-table-td patent-date-value">Aug 29, 2000</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Task distribution processing system and the method for subscribing computers to perform computing tasks during idle time</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6167428">US6167428</a></td><td class="patent-data-table-td patent-date-value">May 27, 1999</td><td class="patent-data-table-td patent-date-value">Dec 26, 2000</td><td class="patent-data-table-td ">Ellis; Frampton E.</td><td class="patent-data-table-td ">Personal computer microprocessor firewalls for internet distributed processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6167462">US6167462</a></td><td class="patent-data-table-td patent-date-value">Dec 11, 1998</td><td class="patent-data-table-td patent-date-value">Dec 26, 2000</td><td class="patent-data-table-td ">Hewlett-Packard Company</td><td class="patent-data-table-td ">Remote scanning through a computer system network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6229139">US6229139</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 11, 2000</td><td class="patent-data-table-td patent-date-value">May 8, 2001</td><td class="patent-data-table-td ">Xros, Inc.</td><td class="patent-data-table-td ">Handheld document scanner</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6510509">US6510509</a></td><td class="patent-data-table-td patent-date-value">Mar 29, 1999</td><td class="patent-data-table-td patent-date-value">Jan 21, 2003</td><td class="patent-data-table-td ">Pmc-Sierra Us, Inc.</td><td class="patent-data-table-td ">Method and apparatus for high-speed network rule processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6515988">US6515988</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 17, 1998</td><td class="patent-data-table-td patent-date-value">Feb 4, 2003</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Token-based document transactions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6628412">US6628412</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 5, 1999</td><td class="patent-data-table-td patent-date-value">Sep 30, 2003</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">Methods of document management and automated document tracking, and a document management system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6707581">US6707581</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 27, 2000</td><td class="patent-data-table-td patent-date-value">Mar 16, 2004</td><td class="patent-data-table-td ">Denton R. Browning</td><td class="patent-data-table-td ">Remote information access system which utilizes handheld scanner</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6725250">US6725250</a></td><td class="patent-data-table-td patent-date-value">Dec 17, 1998</td><td class="patent-data-table-td patent-date-value">Apr 20, 2004</td><td class="patent-data-table-td ">Ellis, Iii Frampton E.</td><td class="patent-data-table-td ">Global network computers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6732141">US6732141</a></td><td class="patent-data-table-td patent-date-value">Nov 26, 1997</td><td class="patent-data-table-td patent-date-value">May 4, 2004</td><td class="patent-data-table-td ">Frampton Erroll Ellis</td><td class="patent-data-table-td ">Commercial distributed processing by personal computers over the internet</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6765559">US6765559</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 20, 2001</td><td class="patent-data-table-td patent-date-value">Jul 20, 2004</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Page information display method and device and storage medium storing program for displaying page information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6771568">US6771568</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 12, 2001</td><td class="patent-data-table-td patent-date-value">Aug 3, 2004</td><td class="patent-data-table-td ">Sima Products Corporation</td><td class="patent-data-table-td ">Digital audio recorder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6782144">US6782144</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 12, 2001</td><td class="patent-data-table-td patent-date-value">Aug 24, 2004</td><td class="patent-data-table-td ">Multiscan Corp.</td><td class="patent-data-table-td ">Document scanner, system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7343324">US7343324</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 21, 2001</td><td class="patent-data-table-td patent-date-value">Mar 11, 2008</td><td class="patent-data-table-td ">Contentguard Holdings Inc.</td><td class="patent-data-table-td ">Method, system, and computer readable medium for automatically publishing content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7392287">US7392287</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 27, 2001</td><td class="patent-data-table-td patent-date-value">Jun 24, 2008</td><td class="patent-data-table-td ">Hemisphere Ii Investment Lp</td><td class="patent-data-table-td ">Method and apparatus for sharing information using a handheld device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20010039497">US20010039497</a></td><td class="patent-data-table-td patent-date-value">Feb 27, 2001</td><td class="patent-data-table-td patent-date-value">Nov 8, 2001</td><td class="patent-data-table-td ">Hubbard Edward A.</td><td class="patent-data-table-td ">System and method for monitizing network connected user bases utilizing distributed processing systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020013832">US20020013832</a></td><td class="patent-data-table-td patent-date-value">Apr 13, 2001</td><td class="patent-data-table-td patent-date-value">Jan 31, 2002</td><td class="patent-data-table-td ">Hubbard Edward A.</td><td class="patent-data-table-td ">Software-based network attached storage services hosted on massively distributed parallel computing networks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020019844">US20020019844</a></td><td class="patent-data-table-td patent-date-value">Jan 12, 2001</td><td class="patent-data-table-td patent-date-value">Feb 14, 2002</td><td class="patent-data-table-td ">Kurowski Scott J.</td><td class="patent-data-table-td ">Method and system for network-distributed computing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020111210">US20020111210</a></td><td class="patent-data-table-td patent-date-value">Mar 27, 2001</td><td class="patent-data-table-td patent-date-value">Aug 15, 2002</td><td class="patent-data-table-td ">Luciano Robert Anthony</td><td class="patent-data-table-td ">Anonymous player identifiers in a gaming environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020116291">US20020116291</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 22, 2000</td><td class="patent-data-table-td patent-date-value">Aug 22, 2002</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Recommender system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20030055679">US20030055679</a></td><td class="patent-data-table-td patent-date-value">Apr 9, 1999</td><td class="patent-data-table-td patent-date-value">Mar 20, 2003</td><td class="patent-data-table-td ">Andrew H. Soll</td><td class="patent-data-table-td ">Enhanced medical treatment system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20030157976">US20030157976</a></td><td class="patent-data-table-td patent-date-value">Jan 23, 2001</td><td class="patent-data-table-td patent-date-value">Aug 21, 2003</td><td class="patent-data-table-td ">Burton Simon</td><td class="patent-data-table-td ">Multi-person parimutuel betting games based on sporting events</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8355583">US8355583</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 2008</td><td class="patent-data-table-td patent-date-value">Jan 15, 2013</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Image processing apparatus and apparatus for searching operator</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=lWixBgABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc709/defs709.htm&usg=AFQjCNFBXWYqUOVuuxerz7B8cqt9daJk7Q#C709S205000">709/205</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=lWixBgABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc709/defs709.htm&usg=AFQjCNFBXWYqUOVuuxerz7B8cqt9daJk7Q#C709S217000">709/217</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=lWixBgABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707SE17008">707/E17.008</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=lWixBgABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc370/defs370.htm&usg=AFQjCNEr5EDctcusna2HU7Iww2g4dx3BIw#C370S389000">370/389</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=lWixBgABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc250/defs250.htm&usg=AFQjCNE2JFZGWgKce_vsAhza5snqYsKAQg#C250S236000">250/236</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=lWixBgABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0015160000">G06F15/16</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=lWixBgABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06Q0010000000">G06Q10/00</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=lWixBgABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06Q10/10">G06Q10/10</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06Q10/10</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Nov 5, 2013</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIMS 1-56 ARE CANCELLED.AT THE TIME OF ISSUANCE AND PUBLICATION OF THIS CERTIFICATE, THE PATENT REMAINS SUBJECT TO PENDING REEXAMINATION CONTROL NUMBER 95/002,361 FILED SEP. 14, 2012.THE CLAIM CONTENT OF THE PATENT MAY BE SUBSEQUENTLY REVISED IF A REEXAMINATION CERTIFICATE ISSUES FROM THE REEXAMINATION PROCEEDING.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 22, 2013</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20120914</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 19, 2011</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20110225</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 9, 2010</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:ACACIA PATENT ACQUISITION, LLC;REEL/FRAME:024953/0599</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">WIRELESS RECOGNITION TECHNOLOGIES LLC, TEXAS</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100908</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 7, 2010</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">ACACIA PATENT ACQUISITION LLC, CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">LICENSE;ASSIGNOR:C SQUARED COMMUNICATIONS, LLC;REEL/FRAME:024942/0294</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100618</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 12, 2010</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">C SQUARED COMMUNICATIONS LLC,FLORIDA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:HEMISPHERE II INVESTMENT, L.P.;US-ASSIGNMENT DATABASE UPDATED:20100512;REEL/FRAME:24369/571</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100507</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:HEMISPHERE II INVESTMENT, L.P.;REEL/FRAME:024369/0571</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">C SQUARED COMMUNICATIONS LLC, FLORIDA</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 11, 2007</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">HEMISPHERE II INVESTMENT LP, FLORIDA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:RATCLIFF, RAYMOND F;REEL/FRAME:019949/0004</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20071001</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_4ff636b3d23669b7103f3b3a3a18b4cd.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U0tnnA8ziQptNuBakeAwlQGfSF7xQ\u0026id=lWixBgABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U2gMqjTshkwSlzc1yqfOBVSOWWHoA\u0026id=lWixBgABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U1bujJ2vvuHNqQ-zaTP3l3hHBqTFQ","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Method_and_apparatus_for_identifying_doc.pdf?id=lWixBgABERAJ\u0026output=pdf\u0026sig=ACfU3U3zuREEmL0jRl-PgwbWVHg7UG44mA"},"sample_url":"http://www.google.com/patents/reader?id=lWixBgABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>