<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7542610 - System and method for use of images with recognition analysis - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="System and method for use of images with recognition analysis"><meta name="DC.contributor" content="Salih Burak Gokturk" scheme="inventor"><meta name="DC.contributor" content="Munjal Shah" scheme="inventor"><meta name="DC.contributor" content="Azhar Khan" scheme="inventor"><meta name="DC.contributor" content="Like.Com" scheme="assignee"><meta name="DC.date" content="2006-10-3" scheme="dateSubmitted"><meta name="DC.description" content="An index is provided that holds information about each image content item in a collection of items, For each image content item, a first information item identifying the image content item and its location on a network, and at least one of (i) a second information item identifying a signature value of an object in the image content, or (ii) identification of a recognized object in the image content."><meta name="DC.date" content="2009-6-2" scheme="issued"><meta name="DC.relation" content="US:20010033690:A1" scheme="references"><meta name="DC.relation" content="US:20020097893:A1" scheme="references"><meta name="DC.relation" content="US:20020103813:A1" scheme="references"><meta name="DC.relation" content="US:20020107718:A1" scheme="references"><meta name="DC.relation" content="US:20020114522:A1" scheme="references"><meta name="DC.relation" content="US:20030028451:A1" scheme="references"><meta name="DC.relation" content="US:20030063779:A1" scheme="references"><meta name="DC.relation" content="US:20030202683:A1" scheme="references"><meta name="DC.relation" content="US:20040264810:A1" scheme="references"><meta name="DC.relation" content="US:20050078885:A1" scheme="references"><meta name="DC.relation" content="US:20050271304:A1" scheme="references"><meta name="DC.relation" content="US:20060133699:A1" scheme="references"><meta name="DC.relation" content="US:20060173560:A1" scheme="references"><meta name="DC.relation" content="US:20060227992:A1" scheme="references"><meta name="DC.relation" content="US:20060251292:A1" scheme="references"><meta name="DC.relation" content="US:20060251338:A1" scheme="references"><meta name="DC.relation" content="US:20060251339:A1" scheme="references"><meta name="DC.relation" content="US:20070003113:A1" scheme="references"><meta name="DC.relation" content="US:20070081744:A1" scheme="references"><meta name="DC.relation" content="US:20080080745:A1" scheme="references"><meta name="DC.relation" content="US:20080082426:A1" scheme="references"><meta name="DC.relation" content="US:20080212849:A1" scheme="references"><meta name="DC.relation" content="US:5781650" scheme="references"><meta name="DC.relation" content="US:5845639" scheme="references"><meta name="DC.relation" content="US:5982912" scheme="references"><meta name="DC.relation" content="US:6035055" scheme="references"><meta name="DC.relation" content="US:6173068" scheme="references"><meta name="DC.relation" content="US:6556713" scheme="references"><meta name="DC.relation" content="US:6785421" scheme="references"><meta name="DC.relation" content="US:6801641" scheme="references"><meta name="DC.relation" content="US:6819783" scheme="references"><meta name="DC.relation" content="US:6919892" scheme="references"><meta name="DC.relation" content="US:7006236" scheme="references"><meta name="DC.relation" content="US:7140550" scheme="references"><meta name="DC.relation" content="US:7203356" scheme="references"><meta name="DC.relation" content="US:7310431" scheme="references"><meta name="DC.relation" content="US:7340077" scheme="references"><meta name="DC.relation" content="US:7382903" scheme="references"><meta name="citation_reference" content="Final Office Action for U.S. Appl. No. 11/246,741, filed Aug. 11, 2008, 19 pages."><meta name="citation_reference" content="International Preliminary Report on Patentability and Written Opinion of the International Searching Authority in Application PCT/US2006/018016, Oct. 16, 2008, 12 pages."><meta name="citation_reference" content="International Preliminary Report on Patentability in Application PCT/US2006/038864, Nov. 27, 2008, 10 pages."><meta name="citation_reference" content="International Search Report and Written Opinion of the International Searching Authority in Application PCT/US06/18016, U.S.International Searching Authority, Jun. 17, 2008, 17 pages."><meta name="citation_reference" content="International Search Report and Written Opinion of the International Searching Authority in Application PCT/US06/38864, U.S.International Searching Authority, Oct. 14, 2008, 16 pages."><meta name="citation_reference" content="International Search Report and Written Opinion of the International Searching Authority in Application PCT/US07/83935, U.S.International Searching Authority, Aug. 18, 2008, 23 pages."><meta name="citation_reference" content="Non-Final Office Action dated Dec. 29, 2008 for U.S. Appl. No. 11/246,589, 19 Pages."><meta name="citation_reference" content="Non-Final Office Action dated Dec. 30, 2008 for U.S. Appl. No. 11/936,713, 15 Pages."><meta name="citation_reference" content="Non-Final Office Action dated Dec. 6, 2007 for U.S. Appl. No. 11/246,741, 31 Pages."><meta name="citation_reference" content="Non-Final Office Action dated Oct. 16, 2008 for U.S. Appl. No. 11/777,070, 10 Pages."><meta name="citation_reference" content="Non-Final Office Action dated Oct. 21, 2008 for U.S. Appl. No. 11/936,705, 18 Pages."><meta name="citation_reference" content="Non-Final Office Action dated Oct. 27, 2008 for U.S. Appl. No. 11/246,434, 11 Pages."><meta name="citation_reference" content="Non-Final Office Action dated Oct. 27, 2008 for U.S. Appl. No. 11/936,734, 7 Pages."><meta name="citation_reference" content="Non-Final Office Action for U.S. Appl. No. 11/246,742, filed Jun. 3, 2008, 16 Pages."><meta name="citation_reference" content="Notice of Allowance dated Dec. 22, 2008 for U.S. Appl. No. 11/246,742, 12 Pages."><meta name="citation_reference" content="Tu, Zhuowen et al., &quot;Image Parsing: Unifying Segmentation, Detection, and Recognition,&quot; Proceedings of the Ninth IEEE International Conference on Computer Vision (ICCV 2003), University of California, Los Angeles, Los Angeles, CA 90095, 7 pages."><meta name="citation_reference" content="U.S. Appl. No. 11/246,741, filed Oct. 7, 2005, Gokturk et al."><meta name="citation_reference" content="U.S. Appl. No. 11/936,694, filed Nov. 7, 2007, Gokturk et al."><meta name="citation_reference" content="U.S. Appl. No. 11/936,705, filed Nov. 7, 2007, Gokturk et al."><meta name="citation_reference" content="U.S. Appl. No. 11/936,713, filed Nov. 7, 2007, Gokturk et al."><meta name="citation_reference" content="U.S. Appl. No. 11/936,734, filed Nov. 7. 2007, Gokturk et al."><meta name="citation_reference" content="Yuille, A.L. et al, &quot;Signfinder: Using Color to detect, localize and identify informational signs,&quot; Proceedings International Conference on Computer Vision, ICCV, 1998, Smith-Kettlewell Eye Research Institute, 2318 Fillmore Street, San San Francisco, CA 94115, 9 pages."><meta name="citation_patent_number" content="US:7542610"><meta name="citation_patent_application_number" content="US:11/543,758"><link rel="canonical" href="http://www.google.com/patents/US7542610"/><meta property="og:url" content="http://www.google.com/patents/US7542610"/><meta name="title" content="Patent US7542610 - System and method for use of images with recognition analysis"/><meta name="description" content="An index is provided that holds information about each image content item in a collection of items, For each image content item, a first information item identifying the image content item and its location on a network, and at least one of (i) a second information item identifying a signature value of an object in the image content, or (ii) identification of a recognized object in the image content."/><meta property="og:title" content="Patent US7542610 - System and method for use of images with recognition analysis"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("00vsU7i2Is6nyASctoC4BQ"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("CAN"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("00vsU7i2Is6nyASctoC4BQ"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("CAN"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7542610?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7542610"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=YMnEBQABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7542610&amp;usg=AFQjCNF3iZSSB9Z5gVNpuCv27QZeGCcmGg" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7542610.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7542610.pdf"></a><a class="appbar-application-grant-link" data-label="Application" href="/patents/US20070081744"></a><a class="appbar-application-grant-link" data-selected="true" data-label="Grant" href="/patents/US7542610"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7542610" style="display:none"><span itemprop="description">An index is provided that holds information about each image content item in a collection of items, For each image content item, a first information item identifying the image content item and its location on a network, and at least one of (i) a second information item identifying a signature value of...</span><span itemprop="url">http://www.google.com/patents/US7542610?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7542610 - System and method for use of images with recognition analysis</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7542610 - System and method for use of images with recognition analysis" title="Patent US7542610 - System and method for use of images with recognition analysis"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7542610 B2</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 11/543,758</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Jun 2, 2009</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Oct 3, 2006</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">May 9, 2005</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US20070081744">US20070081744</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">11543758, </span><span class="patent-bibdata-value">543758, </span><span class="patent-bibdata-value">US 7542610 B2, </span><span class="patent-bibdata-value">US 7542610B2, </span><span class="patent-bibdata-value">US-B2-7542610, </span><span class="patent-bibdata-value">US7542610 B2, </span><span class="patent-bibdata-value">US7542610B2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Salih+Burak+Gokturk%22">Salih Burak Gokturk</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Munjal+Shah%22">Munjal Shah</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Azhar+Khan%22">Azhar Khan</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Like.Com%22">Like.Com</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7542610.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7542610.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7542610.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (38),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (22),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (47),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (19),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (6)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7542610&usg=AFQjCNGYJl7tH-NouAkD4MLdbvumn_7tpg">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7542610&usg=AFQjCNGKzHix7InsJ32jR0oOO6qauivUvw">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7542610B2%26KC%3DB2%26FT%3DD&usg=AFQjCNHKFsxQgP1ZIh45LvrdUAOF19fjVA">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT75394675" lang="EN" load-source="patent-office">System and method for use of images with recognition analysis</invention-title></span><br><span class="patent-number">US 7542610 B2</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA53249969" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">An index is provided that holds information about each image content item in a collection of items, For each image content item, a first information item identifying the image content item and its location on a network, and at least one of (i) a second information item identifying a signature value of an object in the image content, or (ii) identification of a recognized object in the image content.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(5)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7542610B2/US07542610-20090602-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7542610B2/US07542610-20090602-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7542610B2/US07542610-20090602-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7542610B2/US07542610-20090602-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7542610B2/US07542610-20090602-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7542610B2/US07542610-20090602-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7542610B2/US07542610-20090602-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7542610B2/US07542610-20090602-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7542610B2/US07542610-20090602-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7542610B2/US07542610-20090602-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(25)</span></span></div><div class="patent-text"><div mxw-id="PCLM13153721" lang="EN" load-source="patent-office" class="claims">
    <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
      <div class="claim-text">1. A method for enabling selection of image content items, the method comprising:
<div class="claim-text">using a processor to perform steps comprising:</div>
<div class="claim-text">performing image analysis on a collection of image content items to obtain information about each image content item;</div>
<div class="claim-text">wherein performing image analysis includes (i) identifying one or more objects in individual image content items, (ii) determining a category of each identified object, the category of each object being one of a plurality of possible object categories; (iii) identifying a set of features that are specific to the determined category of the identified object in each image content item, and (iv) determining information based on the set of features for the determined category that characterizes the identified object, the information characterizing the object to be separately identifiable from at least some other identified objects in the determined category;</div>
<div class="claim-text">storing the information obtained from the image analysis in one or more data stores;</div>
<div class="claim-text">identifying one or more criteria determined from a text or image content provided in connection with either a user or programmatically identified input; and</div>
<div class="claim-text">performing a search operation to identify one or more image content items that satisfy the one or more criteria of the input using the stored information in the one or more data stores;</div>
<div class="claim-text">wherein at least some of the collection of image content items correspond to images of merchandise objects;</div>
<div class="claim-text">wherein performing the search operation includes selecting one or more image content items of merchandise objects for display with a document in response to said input specifying a merchandise object in the document;</div>
<div class="claim-text">wherein selecting one or more images of merchandise objects for display with the document includes selecting the one or more images based on a determination that the one or more images of merchandise objects are similar to the specified merchandise object;</div>
<div class="claim-text">providing a link with each of the selected one or more images of the merchandise objects, wherein the link is selectable to enable a user to purchase the merchandise objects from a network site operated by the merchant.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
      <div class="claim-text">2. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing image analysis is performed programmatically and prior to identifying one or more criteria.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
      <div class="claim-text">3. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein storing the information includes storing the information in web based index that also associates information about each image content item with a location of the image content item.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
      <div class="claim-text">4. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein identifying one or more criteria includes programmatically analyzing a web document that is identified in the input to determine the one or more criteria from one or both of a text portion or image portion of the web document; and wherein the method further comprises presenting the identified one or more image content items to the user.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
      <div class="claim-text">5. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, identifying one or more criteria includes:
<div class="claim-text">detecting an image content item that is selected or being viewed by the user;</div>
<div class="claim-text">identifying text data associated with the image content item; and</div>
<div class="claim-text">forming at least a portion of the one or more criteria based on the text data associated with the image content item.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
      <div class="claim-text">6. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein identifying one or more criteria includes:
<div class="claim-text">detecting an input image content item from a web document;</div>
<div class="claim-text">identifying a given object of the input image content item, including a corresponding category of the given object, the corresponding category being associated with a specific set of features;</div>
<div class="claim-text">determining a signature value of the object in the input image content item based on a characterization of one or more features in the specific set of features for the corresponding category of the identified object;</div>
<div class="claim-text">forming at least one of the criteria from the signature value of the input image content item.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
      <div class="claim-text">7. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein
<div class="claim-text">performing image analysis on individual image content items in the collection by determining information based on the set of features for the determined category that characterizes the identified object includes determining a signature value for the identified object, the signature value being based at least in part on a quantitative characterization of at least some features in the set of features for the determined category of the identified object; and</div>
<div class="claim-text">performing a search operation to identify one or more image content items that includes using the signature of the one or more criteria to perform a similarity comparison with a signature value of other objects identified from image content items in the collection.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
      <div class="claim-text">8. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein at least some of the collection of image content items correspond to advertisement media, and wherein performing a search operation to identify one or more image content items includes selecting one or more advertisement media from the collection for display with a web document that is presented to the user.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
      <div class="claim-text">9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein selecting one or more advertisement media is performed in response to the user making a selection of a particular content on the web document.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
      <div class="claim-text">10. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein selecting one or more advertisement media is performed in response to detecting the text or image content provided in the web document.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
      <div class="claim-text">11. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing a search operation includes configuring the search operation based on a category of the one or more criteria.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
      <div class="claim-text">12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein configuring the search operation includes weighting how the one or more criteria compare to one or more corresponding features in the set of features for the category of the search.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
      <div class="claim-text">13. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<div class="claim-text">performing image analysis includes identifying one or more apparel items in each image content item,</div>
<div class="claim-text">determining the category of each object identified in each image content item includes identifying a category of the apparel item.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
      <div class="claim-text">14. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein identifying the set of features that are specific to the determined category includes identifying a buckle or zipper for one or more apparel items.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00015" num="00015" class="claim">
      <div class="claim-text">15. A computer system that operates to enable selection of image content items, the computer system comprising:
<div class="claim-text">an image analysis sub-system that is configured to analyze individual image content items that form part of a collection of image content items, the image analysis sub-system being configured to (i) identify one or more objects from individual image content items, (ii) determine one or more of a plurality of categories for each identified object, (iii) associate a set of features that are specific to each of the plurality of categories, so that each category is assigned a set of features that are different from the set of features for at least some of the other categories;</div>
<div class="claim-text">wherein for each identified object, the image analysis sub-system is configured to determine a signature value for the object by characterizing individual features in the set of features that are specific to the category of the identified object;</div>
<div class="claim-text">a search component that is configured to (i) identify, from an input that specifies or includes an image, a given object and a corresponding category of the given object, (ii) determine a criteria from the image of the input, wherein the criteria is based at least in part on a specific set of features that are associated with the corresponding category of the object identified from the object in the image of the input, and (iii) use the signature value determined for each identified object in the collection to select one or more image content items from the collection that satisfies the criteria;</div>
<div class="claim-text">wherein at least some of the collection of image content items correspond to images of merchandise objects;</div>
<div class="claim-text">wherein the search component is configured to:</div>
<div class="claim-text">select one or more image content items of merchandise objects for display with a document in response to said input specifying a merchandise object in the document, the one or more images being selected based on a determination that the one or more images of merchandise objects are similar to the specified merchandise object;</div>
<div class="claim-text">provide a link with each of the selected one or more images of the merchandise objects, wherein the link is selectable to enable a user to purchase the merchandise objects from a network site operated by the merchant.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
      <div class="claim-text">16. The computer system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising:
<div class="claim-text">a data store that stores information that identifies or corresponds to each image content item in the collection, and for each image content item, the signature value of one or more objects identified from each image content item in the collection.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
      <div class="claim-text">17. The computer system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the image analysis sub-system is configured to identify anyone of a plurality of apparel items as the object in individual image content items that form at least a portion of the collection.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
      <div class="claim-text">18. The computer system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the set of features for an item of apparel includes a shape of the apparel item at a specific region of the object.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
      <div class="claim-text">19. The computer system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the set of features for an item of apparel includes a style of the apparel item.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
      <div class="claim-text">20. The computer system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the set of features for an item of apparel includes at least one feature that indicates presence of a buckle, zipper or shoe heel.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00021" num="00021" class="claim">
      <div class="claim-text">21. The computer system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the input further includes a user selection to view a web page or resource.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00022" num="00022" class="claim">
      <div class="claim-text">22. The computer system of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the one or more components include a component that supplement the web page or resource being viewed by the user with the selected one or more image content items.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00023" num="00023" class="claim">
      <div class="claim-text">23. The computer system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the image analysis sub-system is configured to determine the signature value for each object of the individual image content items by quantitatively characterizing individual features in the set of features for the determined category of that object.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00024" num="00024" class="claim">
      <div class="claim-text">24. The computer system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the search component operates to determine a signature value of the given object in the image of the input based on a characterization of one or more features in the specific set of features for the corresponding category of the given object.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00025" num="00025" class="claim">
      <div class="claim-text">25. The computer system of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the search component operates to form at least one of the criteria from the signature value of the image of the input. </div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES18540759" lang="EN" load-source="patent-office" class="description">
    <heading>RELATED APPLICATIONS</heading> <p num="p-0002">This application claims benefit of priority to U.S. Provisional Patent Application No. 60/723,349, entitled REALITY BASED PHOTO ADVERTISING, filed Oct. 3, 2005; the aforementioned priority application being hereby incorporated by reference in its entirety.</p>
    <p num="p-0003">This application also claims benefit of priority to U.S. Provisional Patent Application No. 60/723,356, entitled PRIVACY CONCERNED PHOTO SHARING/GRADUAL PHOTO DISCOVERY, filed Oct. 3, 2005; the aforementioned priority application being hereby incorporated by reference in its entirety.</p>
    <p num="p-0004">This application is also a continuation-in-part of U.S. patent application Ser. No. 11/246,742, entitled SYSTEM AND METHOD FOR ENABLING THE USE OF CAPTURED IMAGES THROUGH RECOGNITION, filed on Oct. 7, 2005; U.S. patent application Ser. No. 11/246,741, entitled SYSTEM AND METHOD FOR ENABLING SEARCH AND RETRIEVAL FROM IMAGE FILES BASED ON RECOGNIZED INFORMATION, filed Oct. 7, 2005; U.S. patent application Ser. No. 11/246,589, entitled SYSTEM AND METHOD FOR RECOGNIZING OBJECTS FROM IMAGES AND IDENTIFYING RELEVANCY AMONGST IMAGES AND INFORMATION, filed on Oct. 7, 2005; U.S. patent application Ser. No. 11/246,434, entitled SYSTEM AND METHOD FOR PROVIDING OBJECTIFIED IMAGE RENDERINGS USING RECOGNITION INFORMATION FROM IMAGES, filed on Oct. 7, 2005; each of which claim priority to U.S. Provisional Patent Application No. 60/679,591, entitled METHOD FOR TAGGING IMAGES, filed May 9, 2005. All of the aforementioned priority applications are hereby incorporated by reference in their entirety.</p>
    <heading>TECHNICAL FIELD</heading> <p num="p-0005">The disclosed embodiments relate generally to the field of digital image processing. More particularly, the disclosed embodiments relate to a system and method for enabling the use of captured images.</p>
    <heading>BACKGROUND</heading> <p num="p-0006">Digital photography has become a consumer application of great significance. It has afforded individuals convenience in capturing and sharing digital images. Devices that capture digital images have become low-cost, and the ability to send pictures from one location to the other has been one of the driving forces in the drive for more network bandwidth.</p>
    <p num="p-0007">Due to the relative low cost of memory and the availability of devices and platforms from which digital images can be viewed, the average consumer maintains most digital images on computer-readable mediums, such as hard drives, CD-Roms, and flash memory. The use of file folders are the primary source of organization, although applications have been created to aid users in organizing and viewing digital images. Some search engines, such as GOOGLE, also enables users to search for images, primarily by matching text-based search input to text metadata or content associated with images.</p>
    <description-of-drawings> <heading>DESCRIPTION OF THE DRAWINGS</heading> <p num="p-0008"> <figref idrefs="DRAWINGS">FIG. 1</figref> illustrates a system for selecting images for presentation on an online document in connection with existing content, under an embodiment of the invention.</p>
      <p num="p-0009"> <figref idrefs="DRAWINGS">FIG. 2</figref> illustrates a method to use an index that stores information about images, under an embodiment of the invention.</p>
      <p num="p-0010"> <figref idrefs="DRAWINGS">FIG. 3</figref> illustrates a method for supplying a person with pertinent advertisement, under an embodiment of the invention.</p>
      <p num="p-0011"> <figref idrefs="DRAWINGS">FIG. 4A</figref> and <figref idrefs="DRAWINGS">FIG. 4B</figref> illustrate a result of segmentation on a merchandise object, as performed by one or more embodiments described herein.</p>
      <p num="p-0012"> <figref idrefs="DRAWINGS">FIG. 5</figref> illustrates a method for supplying a person with the ability to view merchandise and the same or similar merchandise through an on-the-fly image comparison, under an embodiment of the invention.</p>
    </description-of-drawings> <heading>DETAILED DESCRIPTION</heading> <p num="p-0013">Embodiments provide for a programmatic selection of image content for a document that is viewed by a person. One or more embodiments provide for an online environment, where the user views a document, and image content corresponding to pictures, advertisements, or other image content is displayed to the user to correspond to content from the document that the user views or interacts with. In one embodiment, an image analysis is performed on a collection of image content items to obtain information about each of the items. The analysis results in one or more of (i) determination of one or more objects in individual image content items, or (ii) determination of a signature value for each determined object. The information obtained from the image analysis in a data structure, such as an index. The data structure is made available for search operations that specify one or more criteria determined from a text or image content provided in connection with the document that is viewed by the user.</p>
    <p num="p-0014">In another embodiment, an index is provided that holds information about each image content item in a collection of items, For each image content item, a first information item identifying the image content item and its location on a network, and at least one of (i) a second information item identifying a signature value of an object in the image content, or (ii) identification of a recognized object in the image content.</p>
    <p num="p-0015">In another embodiment, a computer system is provided for selecting image content item for a document. The system includes a search component that communicates with one or more servers. Each of the one or more servers serve one or more web pages to terminals. The search component is configured to be responsive to an input identified from a user of one of the terminals interacting with a particular web page to generate a criteria for selecting image content item for a document. In response to the input containing an image, the search component analyzes the image to determine one or more objects in the image of the input, and uses the one or more objects determined from the analysis as a basis of the criteria for selecting the image content.</p>
    <p num="p-0016">As used herein, the term “image data” is intended to mean data that corresponds to or is based on discrete portions of a captured image. For example, with digital images, such as those provided in a JPEG format, the image data may correspond to data or information about pixels that form the image, or data or information determined from pixels of the image.</p>
    <p num="p-0017">The term signature value means one or more quantitative values that distinguish or like an appearance of an object from another object. The values may correspond to vectors or multi-dimensional values. In addition, a signature value may be one value, or a collection or aggregate of several other values (e.g. multiple feature vectors). Feature, or feature extraction are other terms that are generally used elsewhere and have the same meaning that we emphasize with the term signature.</p>
    <p num="p-0018">The terms “recognize”, or “recognition”, or variants thereof, in the context of an image or image data (e.g. “recognize an image”) is meant to means that a determination is made as to what the image correlates to, represents, identifies, means, and/or a context provided by the image. Recognition does not mean a determination of identity by name, unless stated so expressly, as name identification may require an additional step of correlation.</p>
    <p num="p-0019">As used herein, the terms “programmatic”, “programmatically” or variations thereof mean through execution of code, programming or other logic. A programmatic action may be performed with software, firmware or hardware, and generally without user-intervention, albeit not necessarily automatically, as the action may be manually triggered.</p>
    <p num="p-0020">One or more embodiments described herein may be implemented using programmatic elements, often referred to as modules or components, although other names may be used. Such programmatic elements may include a program, a subroutine, a portion of a program, or a software component or a hardware component capable of performing one or more stated tasks or functions. As used herein, a module or component, can exist on a hardware component independently of other modules/components or a module/component can be a shared element or process of other modules/components, programs or machines. A module or component may reside on one machine, such as on a client or on a server, or a module/component may be distributed amongst multiple machines, such as on multiple clients or server machines. Any system described may be implemented in whole or in part on a server, or as part of a network service. Alternatively, a system such as described herein may be implemented on a local computer or terminal, in whole or in part. In either case, implementation of system provided for in this application may require use of memory, processors and network resources (including data ports, and signal lines (optical, electrical etc.), unless stated otherwise.</p>
    <p num="p-0021">Embodiments described herein generally require the use of computers, including processing and memory resources. For example, systems described herein may be implemented on a server or network service. Such servers may connect and be used by users over networks such as the Internet, or by a combination of networks, such as cellular networks and the Internet. Alternatively, one or more embodiments described herein may be implemented locally, in whole or in part, on computing machines such as desktops, cellular phones, personal digital assistances or laptop computers. Thus, memory, processing and network resources may all be used in connection with the establishment, use or performance of any embodiment described herein (including with the performance of any method or with the implementation of any system).</p>
    <p num="p-0022">Furthermore, one or more embodiments described herein may be implemented through the use of instructions that are executable by one or more processors. These instructions may be carried on a computer-readable medium. Machines shown in figures below provide examples of processing resources and computer-readable mediums on which instructions for implementing embodiments of the invention can be carried and/or executed. In particular, the numerous machines shown with embodiments of the invention include processor(s) and various forms of memory for holding data and instructions. Examples of computer-readable mediums include permanent memory storage devices, such as hard drives on personal computers or servers. Other examples of computer storage mediums include portable storage units, such as CD or DVD units, flash memory (such as carried on many cell phones and personal digital assistants (PDAs)), and magnetic memory. Computers, terminals, network enabled devices (e.g. mobile devices such as cell phones) are all examples of machines and devices that utilize processors, memory, and instructions stored on computer-readable mediums.</p>
    <heading>Overview</heading> <p num="p-0023"> <figref idrefs="DRAWINGS">FIG. 1</figref> illustrates a system for selecting images for presentation on an online document in connection with existing content, under an embodiment of the invention. Under one implementation, an image selection system <b>100</b> such as shown by <figref idrefs="DRAWINGS">FIG. 1</figref> may be used to select image content for concurrent presentation with content that is existing on a web page. Alternatively, the image selection system <b>100</b> such as described may be used to replace, append or provide new image content based on a content that that the user views or interacts with. As used throughout any of the embodiments described herein, the image content that can be selected and rendered to the user may include any content with images, such as simple pictures (such as provided as JPEG or GIF) or documents or files that contain images as a portion (e.g. advertisement media with text and image).</p>
    <p num="p-0024">According to an embodiment, the image selection system <b>100</b> of <figref idrefs="DRAWINGS">FIG. 1</figref> includes a image analysis sub-system <b>110</b>, an index <b>120</b>, and a search component <b>130</b>. In one embodiment, the image selection system <b>100</b> may be made available as a service to operators of web sites and domains. To this end, the image selection system <b>100</b> may be made available to one or more servers <b>150</b>, each of which interact with terminals <b>160</b> operated by users. The servers <b>150</b> provide content in the form of web pages <b>155</b> or other online documents or resources. For simplicity, <figref idrefs="DRAWINGS">FIG. 1</figref> illustrates one server <b>150</b> and one terminal <b>160</b>, which may be considered representative of numerous servers or terminals. The image selection system <b>100</b> may identify image content that is selected responsively, or on-the-fly, in response to activities of the server <b>150</b> or the user in connection with the downloaded page <b>155</b>. Under one or more embodiments, such selection may be based on existing content that is either downloaded to the user, detected as being viewed by the user, or otherwise selected or subject to interaction by the user.</p>
    <p num="p-0025">One or more aggregation processes <b>112</b> or mechanisms may be used to procure image content in anyone of various forms. The processes <b>112</b> may locate image content items, such as digital images from numerous libraries, data bases, media collections, or sources for such media that are accessible over the Internet. Embodiments contemplate the use of licensed libraries of pictures, and repositories of pictures donated by the public. In addition, one or more embodiments contemplate use of images that, when selected for display by image selection system <b>100</b>, result in some monetary benefit to a proprietor of that system are of the server <b>150</b> that rendered image. In the latter case, the images may correspond to, for example, advertisements, or images available through an online service. Such images may be programmatically retrieved, a provided to a proprietor of system <b>100</b> as a library. Numerous other sources of images are contemplated. For example, a web crawler may be used to crawl domains on the Internet and to identify images by file type (e.g. JPEG). The aggregation processes <b>112</b> may store and make pictures available individually for the image analysis sub-system <b>110</b>. Thus, while specific embodiments described herein may reference use of images that can be identified or retrieved through use of the index <b>120</b>, embodiments provide that the index <b>120</b> may associate or reference content that incorporates such images. Examples of such content include advertisement media (images, text with slogan) or multimedia content.</p>
    <p num="p-0026">The image analysis sub-system <b>110</b> analyzes individual images as they are located from the aggregation processes <b>112</b> or otherwise provided to the system <b>100</b>. According to an embodiment, the image analysis sub-system <b>110</b> performs various kinds of analysis on individual images to identify information about the contents of the images. In one implementation, the image analysis sub-system <b>110</b> outputs the information it determines from individual images to the index <b>120</b>. For each image, the index <b>120</b> may store (i) an identifier <b>125</b> to the image and/or to the location of the image, (ii) information identified from data associated with the image, and (iii) information identified from performing recognition analysis on the image. Various other kinds of information may be stored. In one embodiment, information stored with each image includes object identification information <b>127</b> and a signature value <b>129</b> for identified objects. As will be described, the object identification information <b>127</b> identifies objects in a picture, with a level of specificity or characteristic information that may vary amongst implementations. The object identification information <b>127</b> may be derived from image analysis or text extraction (also described below). The signature value <b>129</b>, on the other hand, may be a quantitative expression of an object in the image. The signature value <b>129</b> may be the result of image analysis processes.</p>
    <p num="p-0027">As mentioned, one of the analysis performed by the image analysis sub-system <b>110</b> includes extracting text or other information (e.g. metadata) provided with the image to determine information about objects or content in the image. Image recognition processes may also be performed to recognize or detect objects from the image. In performing recognition to identify one or more objects in an image, the image analysis sub-system <b>110</b> may also identify one or more characteristics of the object, such as coloring or salient features. Recognized objects and features of an image include persons, items that appear in the image (e.g. shoes) or even text that appears in the image (such as on a sign). Thus, object identification information <b>127</b> may include more than mere identification of the object. The object identification information may also include descriptive information about the object. In one implementation, the object identification information <b>127</b> is provided as text data to the index <b>120</b>.</p>
    <p num="p-0028">In addition to object identification information <b>127</b>, the image recognition process may be performed to determine the signature value <b>129</b> of objects detected from individual images. In contrast to object identification information <b>127</b>, the signature value <b>129</b> may, under one implementation, be a numeric or quantitative description of an object. The signature value <b>129</b> can identify one object from another object of the same kind. To this extent, the signature value <b>129</b> may be used to determine when two objects are the same (e.g. same person's face, same model and type of shoes), or when two objects are similar (e.g. two people look alike, or two shoes have similar appearance). The latter case is referred to as a “similarity comparison”,</p>
    <p num="p-0029">Numerous techniques exist to determine objects in images, as well as to detect characteristics of determined objects, and obtaining signature values of objects in images. Some of these techniques are described in, for example, U.S. patent application Ser. No. 11/246,742, entitled SYSTEM AND METHOD FOR ENABLING THE USE OF CAPTURED IMAGES THROUGH RECOGNITION, filed on Oct. 7, 2005; which is hereby incorporated by reference in its entirety. Any of the priority documents may be used in their teachings for determining objects (including persons, apparel etc.) and obtaining signature values for such objects. In addition, sections provided below provide additional information about identifying objects from images for specific kinds or categories of objects, and formulating signature values by way of feature vectors.</p>
    <p num="p-0030">The following examples are illustrative of the possible information that can be outputted by the image analysis sub-system <b>110</b> when performing recognition on individual images (or content containing images). In the case where an object or feature that is recognized from an image corresponds to a face, the recognition may also include, as part of the object identification information <b>127</b>, one or more of the following: the coloring of the person's hair or skin, the gender of the person, the person's eye color, the race of the individual, and the age group of the individual. In the case where an object or feature that is recognized from an image corresponds to a shoe, the recognition may also include, as part of the object identification information <b>129</b>, one or more of the following: the color of shoe; whether the shoe is for a man, woman or child; the shoe style; the color of the shoe; the shape of the shoe at various places, such as the heal or tip; and other salient information such as whether the shoe contains a buckle or zipper, or the pattern of the shoe. As mentioned, under one implementation, of the object identification information <b>127</b> may be in the form of text data. For each recognized object in the examples provided, the signature value <b>129</b> provides a quantitative characterization of the characteristics of that object, to distinguish that object from other objects, or like the object to other object. With the face of a person, this may account for specific features in the person's face. With objects, such as shoes, the signature value may quantitatively reflect the shape, color, pattern and salient features of the shoe.</p>
    <p num="p-0031">According to an embodiment, the image analysis sub-system <b>110</b> is a programmatic component that performs its functions through execution of code or logic (i.e. programmatically). One or more alternative embodiments contemplate the image analysis sub-system <b>110</b> as including manual operators in addition to programmatic processes. In one implementation, object detection is performed by manual operators who view images individually and categorize or provide information about the contents of the images. In such an embodiment, the image analysis sub-system <b>110</b> may provide a human interface to display images from a collection, and accept and record input from operators viewing the image. Still and programmatic processes to analyze the contents of the images. In one embodiment, the result of programmatic object detection is displayed to a human operator along with the image, and the human operator may verify or correct the recognition. The human operator may also supplement or append the recognition, by, for example, detecting a salient feature that is missed by the programmatic element. In such an embodiment, one implementation provides for a human operator interface that displays numerous (i.e. tens or hundreds) of images at once, along with the object identification information <b>127</b>, and requires the human operator to verify or correct pictures individually, but as a cluster.</p>
    <p num="p-0032">From the output of the image analysis sub-system <b>110</b>, the index <b>120</b> may store entries that identify or locate individual images, as well as text or quantitative information derived from the image recognition processes. The index <b>120</b> may be made available to a search component <b>130</b>, which identifies criteria and uses the information stored in index <b>120</b> to select images. The search component is responsive to an input <b>152</b> that is provided from server <b>150</b>.</p>
    <p num="p-0033">Embodiments described herein provided for various kinds of input <b>152</b>. Input <b>152</b> may correspond to one or more of the following: (i) a selection of content that a user on the terminal <b>160</b> makes through some affirmative action, such as selecting and clicking with the mouse or other pointer device; (ii) a detection of content that a user is viewing or is interested in, such as by way of identifying content on a portion of the downloaded web page that the user is viewing; (iii) identification of any content up hearing on the web page; (iv) identification of a subject of a content that the user is viewing.</p>
    <p num="p-0034">In order to provide the input <b>152</b>, server <b>150</b> may be configured to include a programmatic component that can identify and communicate the input <b>152</b> to the image selection system <b>100</b>. In the case where input <b>152</b> corresponds to a selection of content by the user, the user may select an image, link or other data element that is associated with a subject or topic. The component <b>158</b> may communicate the selected topic or link to the system <b>100</b>. For example, if the user selects an image or a link to a movie star, the component <b>158</b> may communicate the name of the movie star to the system <b>100</b>. Alternatively, the component <b>158</b> may identify an associated topic of the selected item. For example, if the user selects to view a particular kind of sports car, the component <b>158</b> may include intelligence to identify either the make of the sports car, or just “expensive car”, or another topic that the component <b>158</b> is programmed to associate with the selected topic (e.g. a demographic may be identified by the selection).</p>
    <p num="p-0035">In the case where the content on the page <b>155</b> is detected, the programmatic component <b>158</b> may perform similar process to identify, for example, metadata associated with an image, embedded links in the page, or information pertaining to advertisements that appear on the page. One embodiment provides that for text content, programmatic component <b>158</b> performs a key word search to identify the contents of the document by keyword. From any of these processes, the programmatic component <b>158</b> may identify a subject, that is then communicated as part of input <b>152</b> to the system <b>100</b>. As will be described, what is returned is content, including images, that relate to the input <b>152</b>.</p>
    <p num="p-0036">In addition to implementations and examples provided above, numerous alternatives are also possible. In one embodiment, for example, text input manually entered by the user forms the basis for determining the subject for input <b>152</b>. For example, a search term that the user enters for a search engine may be used for input <b>152</b>, or results from the search. Alternatively, text the user enters through use of the web page <b>155</b> can be inspected for keywords, and then communicated as input <b>152</b> to system <b>100</b>. For example, the user may enter an email content that is then inspected for key words, or for its subject line. In either case, identification of the text results in the display of images from system <b>100</b> that are deemed pertinent in some way to the content that is or was existing on the web page <b>155</b>.</p>
    <p num="p-0037">From the input, the search component <b>130</b> performs a search using the index <b>120</b>. The search component may form a criteria <b>132</b> from the input <b>152</b>. One or more embodiments provide that the search component <b>130</b> can receive as input either images or text based data. Accordingly, search component <b>130</b> may include an image analysis component <b>136</b> to analyze image data as input <b>152</b>, and text component <b>138</b> to analyze text based input <b>152</b>. In one embodiment, the image analysis component <b>136</b> forms either a signature criteria or a text criteria. The type of criteria <b>132</b> and how its implemented to select images depends on implementation, as illustrated by the following usage scenarios. In the case where input <b>152</b> corresponds to an image, one embodiment provides that the image analysis component <b>136</b> recognizes or determines an object in the image, and then uses that determination in forming the search criteria <b>132</b> on the index <b>120</b>. As an alternative or addition, the image analysis component <b>136</b> determines the signature value of the object in the image, and uses that value as the criteria (or portion thereof). The signature value may be used when either an exact match to the object in the image input is needed, or when a similarity or likeness match is desired to that object.</p>
    <p num="p-0038">For example, the image input may be identified by user input (i.e. the user selects an image), or provided on a page or portion thereof that the user views. From the image input, the image analysis component <b>136</b> of the search component <b>130</b> determines one or more objects in the image. The image analysis component <b>136</b> then forms criteria based on the determined objects. In one implementation, this criteria may be text based.</p>
    <p num="p-0039">As an alternative or addition, the image analysis component <b>136</b> determines a signature from the image input. The criteria is then based on the signature and compared against other image signatures in the index <b>120</b>. In one embodiment, a similarity or likeness match may be performed to identify objects that are similar, or which have similar features. For example, the user may view a merchandise (e.g. rug) for sale, select the image, and direct the image to be compared against other similar images. The signature of the rug may then be used to form a criteria to perform a similarity match for other rugs, or for other merchandise carrying a pattern similar to the desired rug. Alternatively, the user may submit image to identify an identical product or design, so that he can compare pricing.</p>
    <p num="p-0040">In the case where input <b>152</b> corresponds to a text item, the criteria may be based more directly on the input <b>152</b>. The text component <b>138</b> of the search component <b>130</b> may translate, parse or otherwise process the input to form the criteria. Still further, the criteria may be based on both image and text, and carry a combination of any of the image analysis or text component described above.</p>
    <p num="p-0041">In either of the cases described, one or more embodiments provide that the criteria <b>132</b> returns either (i) a set of one or more images, or (ii) identification of the images or their locations. The search component may process a result <b>133</b> corresponding to the results from the index <b>120</b>. The system <b>100</b> may return a set of identification <b>145</b> to the selected images, or alternatively, the selected images themselves (with or without other content). The images are then provided in the web page <b>152</b>. Various examples of how images may be provided to supplement content, provide advertisement media, or provide merchandise objects.</p>
    <p num="p-0042">Alternatively, the system <b>100</b> may be provided separate or independent from the server <b>150</b>. For example, a user may simply copy content from the web page and visit a domain or site where the system <b>100</b> is provided. In such a case, the input <b>152</b> may be provided directly from the user, and the output of the identified images may form a new page.</p>
    <p num="p-0043">In another embodiment, the index <b>120</b> may specify image content that has not had image data analysis. For example, index <b>120</b> may include advertisement media. Information associated with each advertisement media may be determined from text associated with the media, or may be manually determined.</p>
    <p num="p-0044">Embodiments described with <figref idrefs="DRAWINGS">FIG. 1</figref> may be performed through use of one or more processors and storage components that are in communication with each other. In one embodiment, components that form the image selection system <b>100</b> may be distributed at different locations on a network, and even handled by different operators or domains. For example, while embodiments contemplate that the index <b>120</b> is operated in connection with the image analysis sub-system <b>110</b>, one or more alternative embodiments contemplate that the index <b>120</b> and the image sub-system <b>110</b> are operated independently, at different network locations or sites.</p>
    <p num="p-0045">With regard to any of components or elements of a system such as described, one or more embodiments contemplate use of servers, computers, or processing combinations that perform functions such as described with the search component <b>130</b> or the image recognition sub-system <b>110</b>. Furthermore, the index <b>120</b> and other components may incorporate storage mediums structured in the form of databases or other memory resources. A system such as described by <figref idrefs="DRAWINGS">FIG. 1</figref> may be distributed over multiple locations on a network such as the Internet, or provided on one domain or even on one server. For example, the index <b>120</b> and the search component <b>130</b> may be provided at different locations, and operated independently of one another by different operators.</p>
    <heading>Methodology</heading> <p num="p-0046"> <figref idrefs="DRAWINGS">FIG. 2</figref> illustrates a method to use an index that stores information about images, under an embodiment of the invention. A method such as described by <figref idrefs="DRAWINGS">FIG. 2</figref> may be performed using a system such as illustrated by <figref idrefs="DRAWINGS">FIG. 1</figref>. As such, reference to elements of <figref idrefs="DRAWINGS">FIG. 1</figref> is intended to illustrate suitable components for performing one or more steps or sub-steps being described.</p>
    <p num="p-0047">In a step <b>210</b>, content that is to be used for image selection is detected. As mentioned, one embodiment provides that the content is identified from a third-party server or domain that serves a web page to a terminal. Other embodiments provide that the content is specified by the user, interacting directly with the image selection system <b>100</b>.</p>
    <p num="p-0048">Step <b>220</b> provides that a criteria is determined from the content. Different sub-processes are possible in this step. In one embodiment, the content from step <b>210</b> is an image. In the case where the content is an image, two different processes may be operated by the search component <b>130</b>. One sub-process that can be performed is a determination of an object in the image. In step <b>232</b>, the image analysis component <b>136</b> of the search component <b>130</b> performs analysis to identify objects in the image. In step <b>234</b>, a criteria <b>132</b> is determined from the determination of the objects in the image. For example, the image may be analyzed to determine that there is “rug” or a “patterned rug”. Different levels of specificity are contemplated. For example, object determination may identify the rug as “Oriental” or “Asian” or predominantly of one color.</p>
    <p num="p-0049">Another sub-process that can be performed on the image input is the determination of the signature value. In one embodiment, the signature value is determined by detecting or determining the object(s) in the image in step <b>236</b>. From image data corresponding to the object, the object's signature value is determined in step <b>238</b>. Then in step <b>240</b>, a criteria may be formed based on the signature value in step <b>240</b>.</p>
    <p num="p-0050">The sub-processes may for image data input may be combined to yield a criteria that identifies the object and its signature. For example, an image of a carpet may yield object determination (“predominantly red rug”) and a signature value (identifying the pattern of the rug).</p>
    <p num="p-0051">Alternatively, a text based search criteria <b>132</b> may be received from the content. For example, text data may be correspond to key words in a document the person is viewing, or which corresponds to metadata that accompanies an image. The text data may form the basis of a criteria <b>132</b>.</p>
    <p num="p-0052">One criteria <b>132</b> may include a combination of all the processes described. For example, a user may view a web page for a carpet for sale. The text for the sale item may be analyzed and parsed to identify the word “carpet”. From the image, image recognition may yield a “red rug” and the signature value corresponds to a pattern of the rug. The criteria <b>132</b> may be formed from all three processes, so that it includes specification of carpet, red rug, and a signature value for the pattern.</p>
    <p num="p-0053">Once the criteria <b>132</b> is established from the input, step <b>260</b> provides that one or more images are selected for display to the user. The criteria <b>132</b> may correspond to an output of the image analysis component <b>136</b> and/or the text component <b>138</b>,</p>
    <p num="p-0054">The index may be scanned to return matching entries in step <b>270</b>. The entries may identify the images by location, or alternatively supply the images directly. The images may be formed in a new web page, or form a portion of an existing web page. Various implementations are contemplated by which resulting images of the search are returned to the user.</p>
    <heading>Advertisement Selection</heading> <p num="p-0055">The following illustrate various usage scenarios that correspond to one or more embodiments of the invention. Any of the methods described in the usage scenarios may be implemented using a system such as described with <figref idrefs="DRAWINGS">FIG. 1</figref>.</p>
    <p num="p-0056"> <figref idrefs="DRAWINGS">FIG. 3</figref> illustrates a method for supplying a person with pertinent advertisement, under an embodiment of the invention. In step <b>310</b>, a person is detected as viewing an online content comprising an image. Step <b>320</b> identifies the image portion of the content. Step <b>330</b> provides that the image content is analyzed using the image analysis component <b>136</b>. In one embodiment, a step <b>340</b> identifies an object of the image. As an alternative or additional step, step <b>350</b> determines a signature of the identified object. As another alternative or additional step, step <b>360</b> identifies text associated with the image. Each of the determinations about the image are used to formulate a query to the index <b>120</b>, which in the embodiment described, contains advertisement media. In step <b>370</b>, an advertisement media is selected for the user based at least in part on the image of the content he was viewing.</p>
    <heading>Merchandise Objects</heading> <p num="p-0057">As mentioned, embodiments described herein may apply to performing object determination, recognition, and similarity comparison on objects such as merchandise. Merchandise objects provide another example of an implementation for a system such as described with <figref idrefs="DRAWINGS">FIG. 1</figref>.</p>
    <p num="p-0058">In determining merchandise from a random image or content, one embodiment provides that text and metadata information associated with the image is used as clues to identify the object of the image. In one embodiment, pre-defined categories are identified, and based on information such as keywords describing the image, URL locating the image, or other information, a categorization of the object in the image is made. For instance, a website might have named the shoes as “men's footwear”. A corresponding pre-defined category may be labeled “men's shoes”. In one embodiment, a rule-based system can be used to map descriptive terms of an image to a predefined category. For instance, if the term “shoe”, and “for him” is identified in the descriptive text of the image, that item can be assumed to be in men's shoes categories. The rule based system may include a set of rules to perform such assignments.</p>
    <p num="p-0059">In another embodiment, the mappings can be done by a learning algorithm. For such an embodiment, a large collection of data is collected. A learning algorithm may be trained that can learn automatically the dependency of the words to categories. Optionally, the results of an automating category mapping algorithm can be verified and updated by human operators for accuracy.</p>
    <p num="p-0060">In an embodiment, segmentation is performed on the image. While categorization may assist segmentation, segmentation on the image data itself may be performed independently. The objective of the segmentation process is to separate the object(s) of interest from the background. For this, any foreground/background segmentation algorithm can be used. In one embodiment, the background can be assumed to be at the sides of the images, whereas the foreground can be assumed to be at the center. The intensity distribution of both foreground and background can be obtained from the center and side pixels respectively. As an example, a mixture of Gaussian models can be learnt for the foreground and background pixels. As a last step, these models can be applied to the whole image and each pixel can be classified as foreground and background. Optionally, the segmentation outputs of the algorithm can be verified by human operators for accuracy. <figref idrefs="DRAWINGS">FIG. 4A</figref> and <figref idrefs="DRAWINGS">FIG. 4B</figref> illustrate a result of segmentation on a merchandise object, as performed by one or more embodiments described herein.</p>
    <p num="p-0061">Following segmentation, a process of extraction may be performed. Once the object is segmented from the background, features that obtain the color, shape, boundary, and pattern of the foreground object are calculated. Such features may be referred to as “visual features”. Each of these features may be stored numerically as vectors and each item may be indexed. For such entries, the index <b>120</b> may include in part or whole a similarity database where the item's metadata is saved along with the visual features. In combination, one by one, or collectively, the various feature vectors for an object may comprise the signature value. The item's metadata is also saved as a metadata feature vector. In one embodiment, the metadata feature can be a mapping of the words to unique identifiers that are derived from a dictionary look-up. Inverse document frequency (IDF) of the word can be saved along with this unique identifier. The IDF indicates how frequent the word happens in documents, and hence how descriptive it is. For instance, if we are looking at shoe items, the word “shoe” is not very descriptive since it happens nearly in all the documents (items).</p>
    <p num="p-0062">The visual and metadata features can be indexed using various indexing algorithms. In one embodiment, a linear index can be used where each item is stored linearly in a file. In another embodiment, a tree based indexing algorithm can be used, where the nodes of the tree would keep clusters of similar looking items. This way, only that node needs to be loaded in the search time, and the search may be performed faster.</p>
    <p num="p-0063">In one embodiment, once all the items go through the steps of 1) Category mapping, 2) Segmentation 3) feature (signature) extraction and 4) indexing, the index database is saved, it is ready to be searched. The search can be initiated from another image. For instance, the user would tell find more examples (shoes) like another one that they liked. This would make a query to the search index, and a weighted distance matching is applied between the visual and metadata features of the query image and all the images in the index database. In one embodiment, this weighting can be a linear combination. In another embodiment, this weighting can be done based on non-linear transformations.</p>
    <p num="p-0064">Optionally, once the user gets the results he or she can provide additional feedback to get more accurate results. Different embodiments can be based on slider, color picker or based on choosing different key regions in the image. In the case of sliders, the user is allowed to change the weights of shape, color or pattern or style of the returned items. In the case of color picker, the user can choose a particular that he or she is interested in, and then the algorithm would match the query image's shape in that particular color. In the case of key regions, the user draws a rectangle on where he is interested in, and the system would run a query on local features of that kind. For instance, the user can draw a rectangle on the high heel, and the algorithm looks for high heels on all the images. The results are refined on run-time, as the users play with any of these feedback mechanisms.</p>
    <p num="p-0065">In one or more embodiments, when index <b>120</b> is used to store information about images of merchandise objects, the information may include URLs or other links to online merchants that provide the merchandise objects for sale. The URL or link may be returned with images when searches are performed by, for example, search component <b>130</b>. Thus, when, for example, a similarity search is performed, results of the similarity search include images that are active, and enable selection by the user to access a site of an online merchant where the merchandise object is provided.</p>
    <p num="p-0066"> <figref idrefs="DRAWINGS">FIG. 5</figref> illustrates a method for supplying a person with the ability to view merchandise and locate the same or similar merchandise through an on-the-fly image comparison, under an embodiment of the invention. In a step <b>510</b>, a user selects an image of a merchandise object, such as a shoe or a rug. Step <b>520</b> provides that image analysis is performed on the image of the selected merchandise. From the image analysis, a signature value is determined.</p>
    <p num="p-0067">In step <b>530</b>, a similarity operation may be performed by the search component <b>130</b> on the index <b>120</b>. The similarity operation may specify the merchandise object and the signature value, or alternatively the various feature vectors and other identifying information stored in the index <b>120</b>. In addition, the similarity operation may identify objects that are in images recorded in the index, with signature values that are deemed to be similar to the selected object. In one embodiment, two objects are deemed similar if the signature values are within a designated threshold.</p>
    <p num="p-0068">Another technique for performing such a similarity performance is taught in U.S. patent application Ser. No. 11/246,742, entitled SYSTEM AND METHOD FOR ENABLING THE USE OF CAPTURED IMAGES THROUGH RECOGNITION, filed on Oct. 7, 2005; which is incorporated by reference herein. Thus, the similarity comparison may return as a result all images containing an object with a feature vectors and/or signature values that are within a designated threshold that defines the similarity. The threshold may be one of design preference or dictated by the particular application.</p>
    <p num="p-0069">In step <b>540</b>, a result comprising one or more images with objects deemed to be similar in appearance is returned to the user. In one embodiment, the result includes, at least initially, only a single image that contains an image deemed most similar to the selected object. In another embodiment, a series, sequence or other plurality of images may be displayed. The images may be sorted or ranked by various factors, including proximity of similarity or other factors.</p>
    <p num="p-0070">In an implementation of an embodiment such as described with <figref idrefs="DRAWINGS">FIG. 5</figref>, a user may view an auction or e-commerce page that shows an object for sale. The user may select a feature, or alternatively access a site, that accepts the image as input and processes the image to determine a signature value. One or more implementations also provide that text associated with the merchandise (e.g. auction heading) may also be used to specify a category that the signature value is to apply to. Then a search of the index <b>120</b> is performed to identify either exact matches (e.g. the same item on sale at another auction or site) or an item that is deemed similar to the selected merchandise. For example, the user may like the item being viewed, but may want to see what else that is similar in appearance is offered at a particular auction site or on other e-commerce sites.</p>
    <heading>Additional Applications</heading> <p num="p-0071">In one embodiment, an algorithm is used to automatically generate images related to text content, such as articles. As described in provisional patent application No. 60/679,591, tags can be extracted from images, using information obtained from recognition, corresponding to objects in them, and text in them. Those tags and images are collected in a central server. All images are indexed using the tags inside them. In addition, an inverse index is created such that, given a tag, the inverse index provides all the images that contain that tag. In addition, a picture ranking algorithm, as described in provisional patent application No. 60/679,591 determines the most relevant images with that tag.</p>
    <p num="p-0072">In one embodiment, the tagging system can be used for an application called “photosense”. There are many articles provided with online source that can be supplemented with images. The article might be more valuable and more readable by the addition of relevant photos. As a first step, the system traverses the article, and find the key words. The key words can be found by looking at the frequency of words, and looking at the title. These words might be filtered by a proper noun dictionary if necessary. Once the most relevant words to the article are found, then the central server is connected and a search is applied on the relevant words. The most relevant search image results are returned, and they are automatically posted next to the images. In addition, an overlay on the images can be shown when the mouse is on the images. When the user presses on the overlay, the page might be directed to the web page of the actual product item, or full search page of the item from the central server. This way, photos are included to add value to the article, as well as, ads are displayed in images, and in a non-disturbing manner to the user. This system generates revenues based on advertisements.</p>
    <p num="p-0073">In any one person's library, there are photos of many people. There are photos of their direct family, photos of their extended family, photos of friends, photos of collegues, and photos of other people who attended the same event (wedding, soccer game, etc). Sharing photos today is labor intensive for the sender and requires tremendous patience and persistence by the receiver (reminding the other party to please send a copy of that photo). Having a large library of photos that is auto-tagged with who and what is in the photo by itself doesn't solve the problem. One cannot simply add these photos to a search engine and show anyone searching all of the photos that match his search, because it would violate the privacy of the owner (of the photos). In order to solve the sharing problem without violating the privacy rights, embodiments contemplate use of a photo sharing system called “gradual photo discovery”.</p>
    <p num="p-0074">As described in Provisional Patent Application No. 60/679,591, first a user adds his photos to the system. The system auto-tags the photo with who, what, and where the photo is. At this point the photo is marked private so no one can see it. A very blurred version of the photo is created for both the thumbnail view and the screen resolution view. However the tags (meta-data) about the photo is added to the global search index. Let us assume that this photo has the tags: “Burak”, “Ozge”, “Munjal”, and Las Vegas and is a photo that Burak added. Now when Munjal searches on the photo (assuming he is not aware that Burak is using the system and/or is not linked to Burak as a friend in the system) with the keywords Munjal and Las Vegas, the system returns a communication identifying one or more pictures that match the criteria. However, the returned image would not be shown to the user in its entirety, but in some degraded, or retracted fashion. For example, the results may be shown in a very blurred fashion. Munjal then select a feature to procure access to the image. For example, an icon or other feature may be presented to “Get” access on the blurred photo. This enables him to write the submitter (Burak) a message requesting access to the full image. The submitter may then elect to reply, grant access or deny access. If he says yes, Munjal can now see the full unblurred photo. The submitter may also have have the option of marking the photo “public” so that anyone can see it from that point forward. In the event that Munjal, has multiple search result hits that match his search but are blurred (private) he can request them all at once using a so-called “Get all” button and can write one note to all of the owners.</p>
    <p num="p-0075">Traditional photo sharing was always “push” based because of privacy reasons. The addition of meta-data allows to create this new “gradual photo discovery” system whereby one can first find a photo (using it's meta-data) and later see the photo (once you get permission using the Get request). Traditionally one could only find a photo once you have seen the photo not allowing this gradual trust based process to take place. An index or database such as shown with various embodiments can facilitate an embodiment for such gradual image sharing and privacy concern.</p>
    <p num="p-0076">As another example of an embodiment, one or more embodiments contemplate use of image content items in connection with a matchmaking or dating site. In one embodiment, a person can specify a celebrity, either by image or by name. The system <b>100</b> may determine the signature values of images in a collection corresponding to persons making themselves available on the date site. When the person specifies the celebrity, embodiments identify a signature value of the celebrity or a dress (or shoe) she is wearing. The signature value can be used as part of or the basis of the search term. The search component <b>130</b> can then perform a search of the index <b>120</b> for similar images and return people or merchandise of a particular type in appearance. As another example, a user can provide images of a person (ex-girlfriend, ex-boyfriend, etc), and the system can return pictures of other similar looking people. Still further, a person may view any random picture from, for example, a news item, and then select the image for a similarity search of other similar looking people.</p>
    <heading>Conclusion</heading> <p num="p-0077">As mentioned, it is contemplated for embodiments of the invention to extend to individual elements and concepts described herein, independently of other concepts, ideas or system, as well as for embodiments to include combinations of elements recited anywhere in this application. Although illustrative embodiments of the invention have been described in detail herein with reference to the accompanying drawings, it is to be understood that the invention is not limited to those precise embodiments. As such, many modifications and variations will be apparent to practitioners skilled in this art. Accordingly, it is intended that the scope of the invention be defined by the following claims and their equivalents. Furthermore, it is contemplated that a particular feature described either individually or as part of an embodiment can be combined with other individually described features, or parts of other embodiments, even if the other features and embodiments make no mentioned of the particular feature. This, the absence of describing combinations should not preclude the inventor from claiming rights to such combinations.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5781650">US5781650</a></td><td class="patent-data-table-td patent-date-value">Aug 28, 1997</td><td class="patent-data-table-td patent-date-value">Jul 14, 1998</td><td class="patent-data-table-td ">University Of Central Florida</td><td class="patent-data-table-td ">Automatic feature detection and age classification of human faces in digital images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5845639">US5845639</a></td><td class="patent-data-table-td patent-date-value">Nov 11, 1994</td><td class="patent-data-table-td patent-date-value">Dec 8, 1998</td><td class="patent-data-table-td ">Board Of Regents Of The University Of Washington</td><td class="patent-data-table-td ">Optical imaging methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5982912">US5982912</a></td><td class="patent-data-table-td patent-date-value">Mar 12, 1997</td><td class="patent-data-table-td patent-date-value">Nov 9, 1999</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Person identification apparatus and method using concentric templates and feature point candidates</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6035055">US6035055</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 3, 1997</td><td class="patent-data-table-td patent-date-value">Mar 7, 2000</td><td class="patent-data-table-td ">Hewlett-Packard Company</td><td class="patent-data-table-td ">Digital image management system in a distributed data access network system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6173068">US6173068</a></td><td class="patent-data-table-td patent-date-value">Mar 25, 1997</td><td class="patent-data-table-td patent-date-value">Jan 9, 2001</td><td class="patent-data-table-td ">Mikos, Ltd.</td><td class="patent-data-table-td ">Method and apparatus for recognizing and classifying individuals based on minutiae</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6556713">US6556713</a></td><td class="patent-data-table-td patent-date-value">Jul 30, 1998</td><td class="patent-data-table-td patent-date-value">Apr 29, 2003</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Image processing apparatus and method and storage medium</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6785421">US6785421</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 22, 2000</td><td class="patent-data-table-td patent-date-value">Aug 31, 2004</td><td class="patent-data-table-td ">Eastman Kodak Company</td><td class="patent-data-table-td ">Analyzing images to determine if one or more sets of materials correspond to the analyzed images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6801641">US6801641</a></td><td class="patent-data-table-td patent-date-value">Feb 6, 2002</td><td class="patent-data-table-td patent-date-value">Oct 5, 2004</td><td class="patent-data-table-td ">Wheeling Jesuit University</td><td class="patent-data-table-td ">Three dimensional face identification system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6819783">US6819783</a></td><td class="patent-data-table-td patent-date-value">Nov 14, 2003</td><td class="patent-data-table-td patent-date-value">Nov 16, 2004</td><td class="patent-data-table-td ">Centerframe, Llc</td><td class="patent-data-table-td ">Obtaining person-specific images in a public venue</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6919892">US6919892</a></td><td class="patent-data-table-td patent-date-value">Aug 14, 2002</td><td class="patent-data-table-td patent-date-value">Jul 19, 2005</td><td class="patent-data-table-td ">Avaworks, Incorporated</td><td class="patent-data-table-td ">Photo realistic talking head creation system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7006236">US7006236</a></td><td class="patent-data-table-td patent-date-value">Sep 17, 2002</td><td class="patent-data-table-td patent-date-value">Feb 28, 2006</td><td class="patent-data-table-td ">Canesta, Inc.</td><td class="patent-data-table-td ">Method and apparatus for approximating depth of an object&#39;s placement onto a monitored region with applications to virtual interface devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7140550">US7140550</a></td><td class="patent-data-table-td patent-date-value">Jul 20, 2005</td><td class="patent-data-table-td patent-date-value">Nov 28, 2006</td><td class="patent-data-table-td ">Diebold Self-Service Systems Division Of Diebold, Incorporated</td><td class="patent-data-table-td ">Multi-account card with magnetic stripe data and electronic ink display being changeable to correspond to a selected account</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7203356">US7203356</a></td><td class="patent-data-table-td patent-date-value">Apr 11, 2003</td><td class="patent-data-table-td patent-date-value">Apr 10, 2007</td><td class="patent-data-table-td ">Canesta, Inc.</td><td class="patent-data-table-td ">Subject segmentation and tracking using 3D sensing technology for video compression in multimedia applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7310431">US7310431</a></td><td class="patent-data-table-td patent-date-value">Apr 10, 2003</td><td class="patent-data-table-td patent-date-value">Dec 18, 2007</td><td class="patent-data-table-td ">Canesta, Inc.</td><td class="patent-data-table-td ">Optical methods for remotely measuring objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7340077">US7340077</a></td><td class="patent-data-table-td patent-date-value">Feb 18, 2003</td><td class="patent-data-table-td patent-date-value">Mar 4, 2008</td><td class="patent-data-table-td ">Canesta, Inc.</td><td class="patent-data-table-td ">Gesture recognition system using depth perceptive sensors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7382903">US7382903</a></td><td class="patent-data-table-td patent-date-value">Nov 19, 2003</td><td class="patent-data-table-td patent-date-value">Jun 3, 2008</td><td class="patent-data-table-td ">Eastman Kodak Company</td><td class="patent-data-table-td ">Method for selecting an emphasis image from an image collection based upon content recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20010033690">US20010033690</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 22, 2001</td><td class="patent-data-table-td patent-date-value">Oct 25, 2001</td><td class="patent-data-table-td ">Stephane Berche</td><td class="patent-data-table-td ">Method of recognizing and indexing documents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020097893">US20020097893</a></td><td class="patent-data-table-td patent-date-value">Sep 4, 2001</td><td class="patent-data-table-td patent-date-value">Jul 25, 2002</td><td class="patent-data-table-td ">Lee Seong-Deok</td><td class="patent-data-table-td ">Apparatus and method for generating object-labeled image in video sequence</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020103813">US20020103813</a></td><td class="patent-data-table-td patent-date-value">Nov 15, 2001</td><td class="patent-data-table-td patent-date-value">Aug 1, 2002</td><td class="patent-data-table-td ">Mark Frigon</td><td class="patent-data-table-td ">Method and apparatus for obtaining information relating to the existence of at least one object in an image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020107718">US20020107718</a></td><td class="patent-data-table-td patent-date-value">Feb 6, 2001</td><td class="patent-data-table-td patent-date-value">Aug 8, 2002</td><td class="patent-data-table-td ">Morrill Mark N.</td><td class="patent-data-table-td ">&quot;Host vendor driven multi-vendor search system for dynamic market preference tracking&quot;</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020114522">US20020114522</a></td><td class="patent-data-table-td patent-date-value">Dec 21, 2000</td><td class="patent-data-table-td patent-date-value">Aug 22, 2002</td><td class="patent-data-table-td ">Rene Seeber</td><td class="patent-data-table-td ">System and method for compiling images from a database and comparing the compiled images with known images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20030028451">US20030028451</a></td><td class="patent-data-table-td patent-date-value">Jul 26, 2002</td><td class="patent-data-table-td patent-date-value">Feb 6, 2003</td><td class="patent-data-table-td ">Ananian John Allen</td><td class="patent-data-table-td ">Personalized interactive digital catalog profiling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20030063779">US20030063779</a></td><td class="patent-data-table-td patent-date-value">Mar 29, 2002</td><td class="patent-data-table-td patent-date-value">Apr 3, 2003</td><td class="patent-data-table-td ">Jennifer Wrigley</td><td class="patent-data-table-td ">System for visual preference determination and predictive product selection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20030202683">US20030202683</a></td><td class="patent-data-table-td patent-date-value">Apr 30, 2002</td><td class="patent-data-table-td patent-date-value">Oct 30, 2003</td><td class="patent-data-table-td ">Yue Ma</td><td class="patent-data-table-td ">Vehicle navigation system that automatically translates roadside signs and objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20040264810">US20040264810</a></td><td class="patent-data-table-td patent-date-value">Jun 27, 2003</td><td class="patent-data-table-td patent-date-value">Dec 30, 2004</td><td class="patent-data-table-td ">Taugher Lawrence Nathaniel</td><td class="patent-data-table-td ">System and method for organizing images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20050078885">US20050078885</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 7, 2004</td><td class="patent-data-table-td patent-date-value">Apr 14, 2005</td><td class="patent-data-table-td ">Fuji Photo Film Co., Ltd.</td><td class="patent-data-table-td ">Image processing device and image processing method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20050271304">US20050271304</a></td><td class="patent-data-table-td patent-date-value">May 5, 2005</td><td class="patent-data-table-td patent-date-value">Dec 8, 2005</td><td class="patent-data-table-td ">Retterath Jamie E</td><td class="patent-data-table-td ">Methods and apparatus for automated true object-based image analysis and retrieval</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20060133699">US20060133699</a></td><td class="patent-data-table-td patent-date-value">Oct 7, 2005</td><td class="patent-data-table-td patent-date-value">Jun 22, 2006</td><td class="patent-data-table-td ">Bernard Widrow</td><td class="patent-data-table-td ">Cognitive memory and auto-associative neural network based search engine for computer and network located images and photographs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20060173560">US20060173560</a></td><td class="patent-data-table-td patent-date-value">Oct 7, 2005</td><td class="patent-data-table-td patent-date-value">Aug 3, 2006</td><td class="patent-data-table-td ">Bernard Widrow</td><td class="patent-data-table-td ">System and method for cognitive memory and auto-associative neural network based pattern recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20060227992">US20060227992</a></td><td class="patent-data-table-td patent-date-value">Apr 8, 2005</td><td class="patent-data-table-td patent-date-value">Oct 12, 2006</td><td class="patent-data-table-td ">Rathus Spencer A</td><td class="patent-data-table-td ">System and method for accessing electronic data via an image search engine</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20060251292">US20060251292</a></td><td class="patent-data-table-td patent-date-value">Oct 7, 2005</td><td class="patent-data-table-td patent-date-value">Nov 9, 2006</td><td class="patent-data-table-td ">Salih Burak Gokturk</td><td class="patent-data-table-td ">System and method for recognizing objects from images and identifying relevancy amongst images and information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20060251338">US20060251338</a></td><td class="patent-data-table-td patent-date-value">Oct 7, 2005</td><td class="patent-data-table-td patent-date-value">Nov 9, 2006</td><td class="patent-data-table-td ">Gokturk Salih B</td><td class="patent-data-table-td ">System and method for providing objectified image renderings using recognition information from images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20060251339">US20060251339</a></td><td class="patent-data-table-td patent-date-value">Oct 7, 2005</td><td class="patent-data-table-td patent-date-value">Nov 9, 2006</td><td class="patent-data-table-td ">Gokturk Salih B</td><td class="patent-data-table-td ">System and method for enabling the use of captured images through recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20070003113">US20070003113</a></td><td class="patent-data-table-td patent-date-value">Feb 5, 2004</td><td class="patent-data-table-td patent-date-value">Jan 4, 2007</td><td class="patent-data-table-td ">Goldberg David A</td><td class="patent-data-table-td ">Obtaining person-specific images in a public venue</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20070081744">US20070081744</a></td><td class="patent-data-table-td patent-date-value">Oct 3, 2006</td><td class="patent-data-table-td patent-date-value">Apr 12, 2007</td><td class="patent-data-table-td ">Gokturk Salih B</td><td class="patent-data-table-td ">System and method for use of images with recognition analysis</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20080080745">US20080080745</a></td><td class="patent-data-table-td patent-date-value">Jul 12, 2007</td><td class="patent-data-table-td patent-date-value">Apr 3, 2008</td><td class="patent-data-table-td ">Vincent Vanhoucke</td><td class="patent-data-table-td ">Computer-Implemented Method for Performing Similarity Searches</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20080082426">US20080082426</a></td><td class="patent-data-table-td patent-date-value">Jul 13, 2007</td><td class="patent-data-table-td patent-date-value">Apr 3, 2008</td><td class="patent-data-table-td ">Gokturk Salih B</td><td class="patent-data-table-td ">System and method for enabling image recognition and searching of remote content on display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20080212849">US20080212849</a></td><td class="patent-data-table-td patent-date-value">May 14, 2004</td><td class="patent-data-table-td patent-date-value">Sep 4, 2008</td><td class="patent-data-table-td ">Authenmetric Co., Ltd.</td><td class="patent-data-table-td ">Method and Apparatus For Facial Image Acquisition and Recognition</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Final Office Action for U.S. Appl. No. 11/246,741, filed Aug. 11, 2008, 19 pages.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">International Preliminary Report on Patentability and Written Opinion of the International Searching Authority in Application PCT/US2006/018016, Oct. 16, 2008, 12 pages.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">International Preliminary Report on Patentability in Application PCT/US2006/038864, Nov. 27, 2008, 10 pages.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">International Search Report and Written Opinion of the International Searching Authority in Application PCT/US06/18016, U.S.International Searching Authority, Jun. 17, 2008, 17 pages.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">International Search Report and Written Opinion of the International Searching Authority in Application PCT/US06/38864, U.S.International Searching Authority, Oct. 14, 2008, 16 pages.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">International Search Report and Written Opinion of the International Searching Authority in Application PCT/US07/83935, U.S.International Searching Authority, Aug. 18, 2008, 23 pages.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Non-Final Office Action dated Dec. 29, 2008 for U.S. Appl. No. 11/246,589, 19 Pages.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Non-Final Office Action dated Dec. 30, 2008 for U.S. Appl. No. 11/936,713, 15 Pages.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Non-Final Office Action dated Dec. 6, 2007 for U.S. Appl. No. 11/246,741, 31 Pages.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Non-Final Office Action dated Oct. 16, 2008 for U.S. Appl. No. 11/777,070, 10 Pages.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Non-Final Office Action dated Oct. 21, 2008 for U.S. Appl. No. 11/936,705, 18 Pages.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Non-Final Office Action dated Oct. 27, 2008 for U.S. Appl. No. 11/246,434, 11 Pages.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Non-Final Office Action dated Oct. 27, 2008 for U.S. Appl. No. 11/936,734, 7 Pages.</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Non-Final Office Action for U.S. Appl. No. 11/246,742, filed Jun. 3, 2008, 16 Pages.</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Notice of Allowance dated Dec. 22, 2008 for U.S. Appl. No. 11/246,742, 12 Pages.</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Tu, Zhuowen et al., "<a href='http://scholar.google.com/scholar?q="Image+Parsing%3A+Unifying+Segmentation%2C+Detection%2C+and+Recognition%2C"'>Image Parsing: Unifying Segmentation, Detection, and Recognition,</a>" Proceedings of the Ninth IEEE International Conference on Computer Vision (ICCV 2003), University of California, Los Angeles, Los Angeles, CA 90095, 7 pages.</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Appl. No. 11/246,741, filed Oct. 7, 2005, Gokturk et al.</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Appl. No. 11/936,694, filed Nov. 7, 2007, Gokturk et al.</td></tr><tr><td class="patent-data-table-td ">19</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Appl. No. 11/936,705, filed Nov. 7, 2007, Gokturk et al.</td></tr><tr><td class="patent-data-table-td ">20</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Appl. No. 11/936,713, filed Nov. 7, 2007, Gokturk et al.</td></tr><tr><td class="patent-data-table-td ">21</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Appl. No. 11/936,734, filed Nov. 7. 2007, Gokturk et al.</td></tr><tr><td class="patent-data-table-td ">22</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Yuille, A.L. et al, "<a href='http://scholar.google.com/scholar?q="Signfinder%3A+Using+Color+to+detect%2C+localize+and+identify+informational+signs%2C"'>Signfinder: Using Color to detect, localize and identify informational signs,</a>" Proceedings International Conference on Computer Vision, ICCV, 1998, Smith-Kettlewell Eye Research Institute, 2318 Fillmore Street, San San Francisco, CA 94115, 9 pages.</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7657100">US7657100</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 7, 2007</td><td class="patent-data-table-td patent-date-value">Feb 2, 2010</td><td class="patent-data-table-td ">Like.Com</td><td class="patent-data-table-td ">System and method for enabling image recognition and searching of images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7660468">US7660468</a></td><td class="patent-data-table-td patent-date-value">Nov 7, 2007</td><td class="patent-data-table-td patent-date-value">Feb 9, 2010</td><td class="patent-data-table-td ">Like.Com</td><td class="patent-data-table-td ">System and method for enabling image searching using manual enrichment, classification, and/or segmentation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7760917">US7760917</a></td><td class="patent-data-table-td patent-date-value">Jul 12, 2007</td><td class="patent-data-table-td patent-date-value">Jul 20, 2010</td><td class="patent-data-table-td ">Like.Com</td><td class="patent-data-table-td ">Computer-implemented method for performing similarity searches</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7783135">US7783135</a></td><td class="patent-data-table-td patent-date-value">Oct 7, 2005</td><td class="patent-data-table-td patent-date-value">Aug 24, 2010</td><td class="patent-data-table-td ">Like.Com</td><td class="patent-data-table-td ">System and method for providing objectified image renderings using recognition information from images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7809192">US7809192</a></td><td class="patent-data-table-td patent-date-value">Oct 7, 2005</td><td class="patent-data-table-td patent-date-value">Oct 5, 2010</td><td class="patent-data-table-td ">Like.Com</td><td class="patent-data-table-td ">System and method for recognizing objects from images and identifying relevancy amongst images and information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7809722">US7809722</a></td><td class="patent-data-table-td patent-date-value">Oct 7, 2005</td><td class="patent-data-table-td patent-date-value">Oct 5, 2010</td><td class="patent-data-table-td ">Like.Com</td><td class="patent-data-table-td ">System and method for enabling search and retrieval from image files based on recognized information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7983486">US7983486</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 29, 2007</td><td class="patent-data-table-td patent-date-value">Jul 19, 2011</td><td class="patent-data-table-td ">Seiko Epson Corporation</td><td class="patent-data-table-td ">Method and apparatus for automatic image categorization using image texture</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8111912">US8111912</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 15, 2008</td><td class="patent-data-table-td patent-date-value">Feb 7, 2012</td><td class="patent-data-table-td ">Yahoo! Inc.</td><td class="patent-data-table-td ">Cost-effective image metadata creation using near-duplicate image detection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8121618">US8121618</a></td><td class="patent-data-table-td patent-date-value">Feb 24, 2010</td><td class="patent-data-table-td patent-date-value">Feb 21, 2012</td><td class="patent-data-table-td ">Digimarc Corporation</td><td class="patent-data-table-td ">Intuitive computing methods and systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8189963">US8189963</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 13, 2007</td><td class="patent-data-table-td patent-date-value">May 29, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Matching advertisements to visual media objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8234168">US8234168</a></td><td class="patent-data-table-td patent-date-value">Apr 19, 2012</td><td class="patent-data-table-td patent-date-value">Jul 31, 2012</td><td class="patent-data-table-td ">Luminate, Inc.</td><td class="patent-data-table-td ">Image content and quality assurance system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8255495">US8255495</a></td><td class="patent-data-table-td patent-date-value">Mar 22, 2012</td><td class="patent-data-table-td patent-date-value">Aug 28, 2012</td><td class="patent-data-table-td ">Luminate, Inc.</td><td class="patent-data-table-td ">Digital image and content display systems and methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8285715">US8285715</a></td><td class="patent-data-table-td patent-date-value">Aug 17, 2009</td><td class="patent-data-table-td patent-date-value">Oct 9, 2012</td><td class="patent-data-table-td ">Ugmode, Inc.</td><td class="patent-data-table-td ">System and method for the structured display of items</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8311289">US8311289</a></td><td class="patent-data-table-td patent-date-value">Jun 21, 2010</td><td class="patent-data-table-td patent-date-value">Nov 13, 2012</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Computer-implemented method for performing similarity searches</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8311889">US8311889</a></td><td class="patent-data-table-td patent-date-value">Jul 10, 2012</td><td class="patent-data-table-td patent-date-value">Nov 13, 2012</td><td class="patent-data-table-td ">Luminate, Inc.</td><td class="patent-data-table-td ">Image content and quality assurance system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8315442">US8315442</a></td><td class="patent-data-table-td patent-date-value">Dec 28, 2009</td><td class="patent-data-table-td patent-date-value">Nov 20, 2012</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">System and method for enabling image searching using manual enrichment, classification, and/or segmentation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8320707">US8320707</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 15, 2011</td><td class="patent-data-table-td patent-date-value">Nov 27, 2012</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">System and method for use of images with recognition analysis</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8345982">US8345982</a></td><td class="patent-data-table-td patent-date-value">Dec 28, 2009</td><td class="patent-data-table-td patent-date-value">Jan 1, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">System and method for search portions of objects in images and features thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8385633">US8385633</a></td><td class="patent-data-table-td patent-date-value">Dec 7, 2010</td><td class="patent-data-table-td patent-date-value">Feb 26, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Techniques for enabling or establishing the use of face recognition algorithms</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8392538">US8392538</a></td><td class="patent-data-table-td patent-date-value">Aug 1, 2012</td><td class="patent-data-table-td patent-date-value">Mar 5, 2013</td><td class="patent-data-table-td ">Luminate, Inc.</td><td class="patent-data-table-td ">Digital image and content display systems and methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8406470">US8406470</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 19, 2011</td><td class="patent-data-table-td patent-date-value">Mar 26, 2013</td><td class="patent-data-table-td ">Mitsubishi Electric Research Laboratories, Inc.</td><td class="patent-data-table-td ">Object detection in depth images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8416981">US8416981</a></td><td class="patent-data-table-td patent-date-value">Jul 29, 2008</td><td class="patent-data-table-td patent-date-value">Apr 9, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">System and method for displaying contextual supplemental content based on image content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8495489">US8495489</a></td><td class="patent-data-table-td patent-date-value">May 16, 2012</td><td class="patent-data-table-td patent-date-value">Jul 23, 2013</td><td class="patent-data-table-td ">Luminate, Inc.</td><td class="patent-data-table-td ">System and method for creating and displaying image annotations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8503664">US8503664</a></td><td class="patent-data-table-td patent-date-value">Dec 20, 2010</td><td class="patent-data-table-td patent-date-value">Aug 6, 2013</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">Quality review of contacts between customers and customer service agents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8520979">US8520979</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 14, 2008</td><td class="patent-data-table-td patent-date-value">Aug 27, 2013</td><td class="patent-data-table-td ">Digimarc Corporation</td><td class="patent-data-table-td ">Methods and systems for content processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8542816">US8542816</a></td><td class="patent-data-table-td patent-date-value">Aug 14, 2008</td><td class="patent-data-table-td patent-date-value">Sep 24, 2013</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">Independent customer service agents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8571272">US8571272</a></td><td class="patent-data-table-td patent-date-value">Mar 12, 2007</td><td class="patent-data-table-td patent-date-value">Oct 29, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Techniques for enabling or establishing the use of face recognition algorithms</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8600035">US8600035</a></td><td class="patent-data-table-td patent-date-value">Aug 25, 2009</td><td class="patent-data-table-td patent-date-value">Dec 3, 2013</td><td class="patent-data-table-td ">Amazon Technologies, Inc.</td><td class="patent-data-table-td ">Systems and methods for customer contact</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8630493">US8630493</a></td><td class="patent-data-table-td patent-date-value">Dec 7, 2010</td><td class="patent-data-table-td patent-date-value">Jan 14, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Techniques for enabling or establishing the use of face recognition algorithms</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8630513">US8630513</a></td><td class="patent-data-table-td patent-date-value">Feb 10, 2012</td><td class="patent-data-table-td patent-date-value">Jan 14, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">System and method for providing objectified image renderings using recognition information from images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8649572">US8649572</a></td><td class="patent-data-table-td patent-date-value">Feb 27, 2009</td><td class="patent-data-table-td patent-date-value">Feb 11, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">System and method for enabling the use of captured images through recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8659764">US8659764</a></td><td class="patent-data-table-td patent-date-value">Feb 3, 2010</td><td class="patent-data-table-td patent-date-value">Feb 25, 2014</td><td class="patent-data-table-td ">Body Surface Translations, Inc.</td><td class="patent-data-table-td ">Estimating physical parameters using three dimensional representations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8670598">US8670598</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 19, 2009</td><td class="patent-data-table-td patent-date-value">Mar 11, 2014</td><td class="patent-data-table-td ">Robert Bosch Gmbh</td><td class="patent-data-table-td ">Device for creating and/or processing an object signature, monitoring device, method and computer program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8712862">US8712862</a></td><td class="patent-data-table-td patent-date-value">Sep 14, 2012</td><td class="patent-data-table-td patent-date-value">Apr 29, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">System and method for enabling image recognition and searching of remote content on display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8732025">US8732025</a></td><td class="patent-data-table-td patent-date-value">Jul 13, 2007</td><td class="patent-data-table-td patent-date-value">May 20, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">System and method for enabling image recognition and searching of remote content on display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8732030">US8732030</a></td><td class="patent-data-table-td patent-date-value">Feb 16, 2012</td><td class="patent-data-table-td patent-date-value">May 20, 2014</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">System and method for using image analysis and search in E-commerce</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8737678">US8737678</a></td><td class="patent-data-table-td patent-date-value">Nov 30, 2011</td><td class="patent-data-table-td patent-date-value">May 27, 2014</td><td class="patent-data-table-td ">Luminate, Inc.</td><td class="patent-data-table-td ">Platform for providing interactive applications on a digital content platform</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090123090">US20090123090</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 13, 2007</td><td class="patent-data-table-td patent-date-value">May 14, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Matching Advertisements to Visual Media Objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090182622">US20090182622</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 28, 2008</td><td class="patent-data-table-td patent-date-value">Jul 16, 2009</td><td class="patent-data-table-td ">Agarwal Amit D</td><td class="patent-data-table-td ">Enhancing and storing data for recall and use</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100046842">US20100046842</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 14, 2008</td><td class="patent-data-table-td patent-date-value">Feb 25, 2010</td><td class="patent-data-table-td ">Conwell William Y</td><td class="patent-data-table-td ">Methods and Systems for Content Processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110019871">US20110019871</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 19, 2009</td><td class="patent-data-table-td patent-date-value">Jan 27, 2011</td><td class="patent-data-table-td ">Robert Bosch Gmbh</td><td class="patent-data-table-td ">Device for creating and/or processing an object signature, monitoring device, method and computer program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110178871">US20110178871</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 20, 2010</td><td class="patent-data-table-td patent-date-value">Jul 21, 2011</td><td class="patent-data-table-td ">Yahoo! Inc.</td><td class="patent-data-table-td ">Image content based advertisement system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110194777">US20110194777</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 15, 2011</td><td class="patent-data-table-td patent-date-value">Aug 11, 2011</td><td class="patent-data-table-td ">Salih Burak Gokturk</td><td class="patent-data-table-td ">System and method for use of images with recognition analysis</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110320317">US20110320317</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 31, 2011</td><td class="patent-data-table-td patent-date-value">Dec 29, 2011</td><td class="patent-data-table-td ">Google Inc., A Delaware Corporation</td><td class="patent-data-table-td ">Image capture for purchases</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120269384">US20120269384</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 19, 2011</td><td class="patent-data-table-td patent-date-value">Oct 25, 2012</td><td class="patent-data-table-td ">Jones Michael J</td><td class="patent-data-table-td ">Object Detection in Depth Images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130129228">US20130129228</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 14, 2012</td><td class="patent-data-table-td patent-date-value">May 23, 2013</td><td class="patent-data-table-td ">Salih Burak Gokturk</td><td class="patent-data-table-td ">System and method for use of images with recognition analysis</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130159192">US20130159192</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 16, 2011</td><td class="patent-data-table-td patent-date-value">Jun 20, 2013</td><td class="patent-data-table-td ">Palo Alto Research Center Incorporated</td><td class="patent-data-table-td ">Privacy-preserving behavior targeting for digital coupons</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S209000">382/209</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S305000">382/305</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S224000">382/224</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc707/defs707.htm&usg=AFQjCNE7Q7Bg2eD2wcE_fXEcdOe7Yesevw#C707S999003">707/999.003</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06Q0030000000">G06Q30/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009600000">G06K9/60</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0007000000">G06F7/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009620000">G06K9/62</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=Y10S707/99933">Y10S707/99933</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30265">G06F17/30265</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30259">G06F17/30259</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/00664">G06K9/00664</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/00375">G06K9/00375</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=YMnEBQABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F17/30253">G06F17/30253</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06K9/00V2</span>, <span class="nested-value">G06K9/00H2</span>, <span class="nested-value">G06F17/30M1E</span>, <span class="nested-value">G06F17/30M2</span>, <span class="nested-value">G06F17/30M1S</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Dec 3, 2012</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 4, 2012</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 1-25 IS CONFIRMED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 28, 2012</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:LIKE.COM;REEL/FRAME:028862/0105</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20120731</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">GOOGLE INC., CALIFORNIA</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 18, 2010</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100317</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 11, 2009</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">LIKE.COM, CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:RIYA, INC.;REEL/FRAME:022245/0565</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080501</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">LIKE.COM,CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:RIYA, INC.;US-ASSIGNMENT DATABASE UPDATED:20100203;REEL/FRAME:22245/565</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:RIYA, INC.;US-ASSIGNMENT DATABASE UPDATED:20100209;REEL/FRAME:22245/565</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:RIYA, INC.;REEL/FRAME:22245/565</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 21, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">RIYA, INC., CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:GOKTURK, SALIH BURAK;SHAH, MUNJAL;KHAN, AZHAR;REEL/FRAME:018669/0990;SIGNING DATES FROM 20061204 TO 20061220</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U0YUxkENnuTtAoRj_Zw3u70clx67Q\u0026id=YMnEBQABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U2Vq9-UwTyG8NtX0zxyFNe_a7obgw\u0026id=YMnEBQABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U29Xag63v28sIEUkcsuYMnVXLehIg","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/System_and_method_for_use_of_images_with.pdf?id=YMnEBQABERAJ\u0026output=pdf\u0026sig=ACfU3U1bmtS2RuDOkrdJNN8VcOcg2c2M0g"},"sample_url":"http://www.google.com/patents/reader?id=YMnEBQABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>