<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US5812799 - Non-blocking load buffer and a multiple-priority memory system for real-time ... - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Non-blocking load buffer and a multiple-priority memory system for real-time multiprocessing"><meta name="DC.contributor" content="William K. Zuravleff" scheme="inventor"><meta name="DC.contributor" content="Mark Semmelmeyer" scheme="inventor"><meta name="DC.contributor" content="Timothy Robinson" scheme="inventor"><meta name="DC.contributor" content="Scott Furman" scheme="inventor"><meta name="DC.contributor" content="Microunity Systems Engineering, Inc." scheme="assignee"><meta name="DC.date" content="1995-6-7" scheme="dateSubmitted"><meta name="DC.description" content="A non-blocking load buffer for use in a high-speed microprocessor and memory system. The non-blocking load buffer interfaces a high-speed processor/cache bus, which connects a processor and a cache to the non-blocking load buffer, with a lower speed peripheral bus, which connects to peripheral devices. The non-blocking load buffer allows data to be retrieved from relatively low bandwidth peripheral devices directly from programmed I/O of the processor at the maximum rate of the peripherals so that the data may be processed and stored without unnecessarily idling the processor. I/O requests from several processors within a multiprocessor may simultaneously be buffered so that a plurality of non-blocking loads may be processed during the latency period of the device. As a result, a continuous maximum throughput from multiple I/O devices by the programmed I/O of the processor is achieved and the time required for completing tasks and processing data may be reduced. Also, a multiple priority non-blocking load buffer is provided for serving a multiprocessor running real-time processes of varying deadlines by prioritization-based scheduling of memory and peripheral accesses."><meta name="DC.date" content="1998-9-22" scheme="issued"><meta name="DC.relation" content="US:4833655" scheme="references"><meta name="DC.relation" content="US:5043981" scheme="references"><meta name="DC.relation" content="US:5179688" scheme="references"><meta name="DC.relation" content="US:5208490" scheme="references"><meta name="DC.relation" content="US:5249297" scheme="references"><meta name="DC.relation" content="US:5257356" scheme="references"><meta name="DC.relation" content="US:5289403" scheme="references"><meta name="DC.relation" content="US:5299158" scheme="references"><meta name="DC.relation" content="US:5317204" scheme="references"><meta name="DC.relation" content="US:5329176" scheme="references"><meta name="DC.relation" content="US:5343096" scheme="references"><meta name="DC.relation" content="US:5363485" scheme="references"><meta name="DC.relation" content="US:5367681" scheme="references"><meta name="DC.relation" content="US:5450564" scheme="references"><meta name="DC.relation" content="US:5459839" scheme="references"><meta name="DC.relation" content="US:5499346" scheme="references"><meta name="DC.relation" content="US:5507032" scheme="references"><meta name="DC.relation" content="US:5526508" scheme="references"><meta name="DC.relation" content="US:5535340" scheme="references"><meta name="DC.relation" content="US:5541912" scheme="references"><meta name="DC.relation" content="US:5542055" scheme="references"><meta name="DC.relation" content="US:5546546" scheme="references"><meta name="DC.relation" content="US:5548791" scheme="references"><meta name="DC.relation" content="US:5557744" scheme="references"><meta name="DC.relation" content="US:5568620" scheme="references"><meta name="DC.relation" content="US:5588125" scheme="references"><meta name="DC.relation" content="US:5623628" scheme="references"><meta name="DC.relation" content="US:5625778" scheme="references"><meta name="citation_reference" content="Doug Hunt, &quot;Advanced Performance Features of the 64-bit PA-8000&quot;, IEEE, pp. 123-128, 1995."><meta name="citation_reference" content="Doug Hunt, Advanced Performance Features of the 64 bit PA 8000 , IEEE, pp. 123 128, 1995."><meta name="citation_reference" content="Eric DeLano et al., &quot;A High Speed Superscaler PA-RISC Processor&quot;, IEEE, pp. 116-121, 1992."><meta name="citation_reference" content="Eric DeLano et al., A High Speed Superscaler PA RISC Processor , IEEE, pp. 116 121, 1992."><meta name="citation_reference" content="Jonathan Lotz et al., &quot;A CMOS RISC CPU Designed for Sustained High Performance on Large Applications&quot;, IEEE Journal of Solid-State Circuits, vol. 25, pp. 1190-1198, Oct. 1990."><meta name="citation_reference" content="Jonathan Lotz et al., A CMOS RISC CPU Designed for Sustained High Performance on Large Applications , IEEE Journal of Solid State Circuits, vol. 25, pp. 1190 1198, Oct. 1990."><meta name="citation_patent_number" content="US:5812799"><meta name="citation_patent_application_number" content="US:08/480,738"><link rel="canonical" href="http://www.google.com/patents/US5812799"/><meta property="og:url" content="http://www.google.com/patents/US5812799"/><meta name="title" content="Patent US5812799 - Non-blocking load buffer and a multiple-priority memory system for real-time multiprocessing"/><meta name="description" content="A non-blocking load buffer for use in a high-speed microprocessor and memory system. The non-blocking load buffer interfaces a high-speed processor/cache bus, which connects a processor and a cache to the non-blocking load buffer, with a lower speed peripheral bus, which connects to peripheral devices. The non-blocking load buffer allows data to be retrieved from relatively low bandwidth peripheral devices directly from programmed I/O of the processor at the maximum rate of the peripherals so that the data may be processed and stored without unnecessarily idling the processor. I/O requests from several processors within a multiprocessor may simultaneously be buffered so that a plurality of non-blocking loads may be processed during the latency period of the device. As a result, a continuous maximum throughput from multiple I/O devices by the programmed I/O of the processor is achieved and the time required for completing tasks and processing data may be reduced. Also, a multiple priority non-blocking load buffer is provided for serving a multiprocessor running real-time processes of varying deadlines by prioritization-based scheduling of memory and peripheral accesses."/><meta property="og:title" content="Patent US5812799 - Non-blocking load buffer and a multiple-priority memory system for real-time multiprocessing"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("0KrtU4fCAua-sQT3u4CwAw"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("NOR"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("0KrtU4fCAua-sQT3u4CwAw"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("NOR"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us5812799?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US5812799"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=1MtHBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS5812799&amp;usg=AFQjCNGqOFvrAPnz8zhOIXpScrFwA1a6gA" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US5812799.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US5812799.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US5812799" style="display:none"><span itemprop="description">A non-blocking load buffer for use in a high-speed microprocessor and memory system. The non-blocking load buffer interfaces a high-speed processor/cache bus, which connects a processor and a cache to the non-blocking load buffer, with a lower speed peripheral bus, which connects to peripheral devices....</span><span itemprop="url">http://www.google.com/patents/US5812799?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US5812799 - Non-blocking load buffer and a multiple-priority memory system for real-time multiprocessing</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US5812799 - Non-blocking load buffer and a multiple-priority memory system for real-time multiprocessing" title="Patent US5812799 - Non-blocking load buffer and a multiple-priority memory system for real-time multiprocessing"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US5812799 A</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 08/480,738</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Sep 22, 1998</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Jun 7, 1995</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Jun 7, 1995</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">08480738, </span><span class="patent-bibdata-value">480738, </span><span class="patent-bibdata-value">US 5812799 A, </span><span class="patent-bibdata-value">US 5812799A, </span><span class="patent-bibdata-value">US-A-5812799, </span><span class="patent-bibdata-value">US5812799 A, </span><span class="patent-bibdata-value">US5812799A</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22William+K.+Zuravleff%22">William K. Zuravleff</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Mark+Semmelmeyer%22">Mark Semmelmeyer</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Timothy+Robinson%22">Timothy Robinson</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Scott+Furman%22">Scott Furman</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Microunity+Systems+Engineering,+Inc.%22">Microunity Systems Engineering, Inc.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US5812799.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5812799.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5812799.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (28),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (6),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (32),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (10),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (6)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=1MtHBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/5812799&usg=AFQjCNEhajsTkVmxygyCddXLlAnQi34H4Q">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=1MtHBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D5812799&usg=AFQjCNG5vFA7jgXnP5ZJwJ13xPhmF5N-jg">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=1MtHBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D5812799A%26KC%3DA%26FT%3DD&usg=AFQjCNGkvCbJ51In3t63PKBgZeqN6pbtXg">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54339049" lang="EN" load-source="patent-office">Non-blocking load buffer and a multiple-priority memory system for real-time multiprocessing</invention-title></span><br><span class="patent-number">US 5812799 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA37817831" lang="EN" load-source="patent-office"> <div class="abstract">A non-blocking load buffer for use in a high-speed microprocessor and memory system. The non-blocking load buffer interfaces a high-speed processor/cache bus, which connects a processor and a cache to the non-blocking load buffer, with a lower speed peripheral bus, which connects to peripheral devices. The non-blocking load buffer allows data to be retrieved from relatively low bandwidth peripheral devices directly from programmed I/O of the processor at the maximum rate of the peripherals so that the data may be processed and stored without unnecessarily idling the processor. I/O requests from several processors within a multiprocessor may simultaneously be buffered so that a plurality of non-blocking loads may be processed during the latency period of the device. As a result, a continuous maximum throughput from multiple I/O devices by the programmed I/O of the processor is achieved and the time required for completing tasks and processing data may be reduced. Also, a multiple priority non-blocking load buffer is provided for serving a multiprocessor running real-time processes of varying deadlines by prioritization-based scheduling of memory and peripheral accesses.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(9)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5812799-1.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5812799-1.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5812799-2.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5812799-2.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5812799-3.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5812799-3.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5812799-4.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5812799-4.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5812799-5.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5812799-5.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5812799-6.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5812799-6.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5812799-7.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5812799-7.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5812799-8.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5812799-8.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5812799-9.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5812799-9.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(34)</span></span></div><div class="patent-text"><div mxw-id="PCLM5286271" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is:</claim-statement> <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1. A data processing system comprising:<div class="claim-text">one or more requesting processors that generate processor requests directed to one or more peripheral devices;</div> <div class="claim-text">a plurality of peripheral devices that accept the processor requests and</div> <div class="claim-text">a controller responsive to the processor requests that creates a plurality of separate pending queues corresponding to each one of the plurality of peripheral devices for queuing the processor requests directed to a particular peripheral device in entries of a corresponding separate pending queue, wherein at least two separate peripheral devices process the processor requests simultaneously, after retrieving such processor requests from their respective separate pending queues, wherein the one or more requesting processors include dependency checking logic that generate non-blocking processor requests.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2. The data processing system according to claim 1, wherein the one or more requesting processors include dependency checking logic that generate non-blocking processor requests.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3. The data processing system according to claim 1, wherein the non-blocking processor requests are generated by running real-time processes.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4. The data processing system according to claim 1, wherein the separate pending queues are prioritized such that the higher priority pending queues have a higher number of maximum entries than the lower priority pending queues.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5. The data processing system according to claim 1, wherein the one or more requesting processors generate the processor requests over a first bus, and wherein the peripheral devices accept the processor requests over a second bus that has a different bus bandwidth from the first bus.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6. The data processing system according to claim 1, wherein the controller marks a processor request as outstanding, when a corresponding peripheral device accepts such processor request.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7. The data processing system according to claim 6, wherein the controller places the processor requests in a return queue, after the peripheral devices respond to such outstanding processor requests.</div>
    </div>
    </div> <div class="claim"> <div num="8" class="claim">
      <div class="claim-text">8. A data processing system comprising:<div class="claim-text">one or more requesting processors that generate processor requests directed to one or more peripheral devices;</div> <div class="claim-text">a plurality of peripheral devices that accept the processor requests; and</div> <div class="claim-text">a controller responsive to the processor requests that creates a plurality of separate pending queues corresponding to each one of the plurality of peripheral devices for queuing the processor requests directed to a particular peripheral device in entries of a corresponding separate pending queue, wherein at least two separate peripheral devices process the processor requests simultaneously, after retrieving such processor requests from their respective separate pending queues;</div> <div class="claim-text">a shared memory device, wherein the controller generates the pending queues from an allocated free pool of entries on the shared memory device, wherein the controller variably adds entries to the pending queues from the allocated free pool of entries, only after the processor requests are generated; and wherein the entries include pointers that point to memory locations on the shared memory device.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9. The data processing system according to claim 8, wherein the processor requests include control information, addresses and data, and wherein the shared memory device is partitioned for storing the address and control information in a first memory array and for storing the data in a second separate memory array.</div>
    </div>
    </div> <div class="claim"> <div num="10" class="claim">
      <div class="claim-text">10. A data processing system comprising:<div class="claim-text">one or more requesting processors that generate non-blocking processor requests directed to one or more peripheral devices;</div> <div class="claim-text">a plurality of peripheral devices that accept the non-blocking processor requests;</div> <div class="claim-text">a shared memory device; and</div> <div class="claim-text">a memory controller responsive to the non-blocking processor requests that creates a plurality of separate pending queues on the shared memory device corresponding to each one of the plurality of peripheral devices for queuing the non-blocking processor requests directed to a particular peripheral device in entries of a corresponding separate pending queue, wherein at least two separate peripheral devices process the non-blocking processor requests simultaneously, after retrieving such non-blocking processor requests from their respective separate pending queues, wherein the entries include pointers that point to memory locations on the shared memory device.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" class="claim">
      <div class="claim-text">11. The data processing system according to claim 10, wherein the one or more requesting processors generate the non-blocking processor requests over a first bus, and wherein the peripheral devices accept the non-blocking processor requests over a second bus that has a different bus bandwidth from the first bus.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" class="claim">
      <div class="claim-text">12. The data processing system according to claim 10, wherein the non-blocking processor requests are prioritized in the pending queues such that the higher priority non-blocking processor requests are processed before the lower priority non-blocking processor requests.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" class="claim">
      <div class="claim-text">13. The data processing system according to claim 10, wherein the pending queues are prioritized such that the higher priority pending queues have a higher number of maximum entries than the lower priority pending queues.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" class="claim">
      <div class="claim-text">14. The data processing system according to claim 10, wherein the memory controller marks a non-blocking processor request as outstanding, when a corresponding peripheral device accepts such non-blocking processor request.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" class="claim">
      <div class="claim-text">15. The data processing system according to claim 10, wherein the memory controller generates the pending queues from an allocated free pool of entries on the shared memory device.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" class="claim">
      <div class="claim-text">16. The data processing system according to claim 14, wherein the memory controller places the accepted non-blocking processor requests in a return queue located on the shared memory device, after the peripheral devices respond to such outstanding non-blocking processor requests.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" class="claim">
      <div class="claim-text">17. The data processing system according to claim 15, wherein the memory controller variably adds entries to the pending queues, only after the non-blocking processor requests are generated.</div>
    </div>
    </div> <div class="claim"> <div num="18" class="claim">
      <div class="claim-text">18. A data processing system comprising:<div class="claim-text">one or more requesting processors that generate non-blocking processor requests directed to one or more peripheral devices;</div> <div class="claim-text">a plurality of peripheral devices that accept the non-blocking processor requests;</div> <div class="claim-text">a shared memory device; and</div> <div class="claim-text">a memory controller responsive to the non-blocking processor requests that creates a plurality of separate pending queues on the shared memory device corresponding to each one of the plurality of peripheral devices for queuing the non-blocking processor requests directed to a particular peripheral device in entries of a corresponding separate pending queue, wherein at least two separate peripheral devices process the non-blocking processor requests simultaneously, after retrieving such non-blocking processor requests from their respective separate pending queues, wherein the non-blocking processor requests include control information, addresses and data, and wherein the shared memory device is partitioned for storing the address and control information in a first memory array and for storing the data in a second separate memory array.</div> </div>
    </div>
    </div> <div class="claim"> <div num="19" class="claim">
      <div class="claim-text">19. A method for processing data comprising the steps of:<div class="claim-text">generating processor requests that are directed to a plurality of peripheral devices by one or more requesting processors;</div> <div class="claim-text">creating a plurality of separate pending queues that correspond to each one of the plurality of peripheral devices, for queuing the processor requests directed to a particular peripheral device in entries of a corresponding pending queue;</div> <div class="claim-text">processing two separate processor requests directed to corresponding peripheral devices simultaneously, after retrieving such processor requests from their respective separate pending queues;</div> <div class="claim-text">allocating a shared memory space for entries of the separate pending queues;</div> <div class="claim-text">freeing the entries of the processor requests in the pending queues, after such processor requests are accepted by the peripheral devices; and</div> <div class="claim-text">placing the outstanding processor requests in the entries of a return queue on the shared memory space, after the corresponding peripheral devices respond to such outstanding processor requests.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" class="claim">
      <div class="claim-text">20. The method for processing data according to claim 19 further including performing dependency checking on the processor requests, to generate non-blocking processor requests.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" class="claim">
      <div class="claim-text">21. The method for processing data according to claim 19 further including prioritizing the processor requests in the pending queues such that the higher priority processor requests are processed before the lower priority processor requests.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="22" class="claim">
      <div class="claim-text">22. The method for processing data according to claim 19 further including prioritizing the separate pending queues such that the higher priority pending queues have a higher number of maximum entries than the lower priority pending queues.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="23" class="claim">
      <div class="claim-text">23. The method for processing data according to claim 19 further including variably adding entries to the pending queues, only after processor requests directed to corresponding peripheral devices are generated.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="24" class="claim">
      <div class="claim-text">24. The method for processing data according to claim 19 further including marking a processor request as outstanding after such processor request is accepted by a corresponding peripheral device.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" class="claim">
      <div class="claim-text">25. The method for processing data according to claim 20 further including generating the non-blocking processor requests by running real-time processes.</div>
    </div>
    </div> <div class="claim"> <div num="26" class="claim">
      <div class="claim-text">26. A method for processing data comprising the steps of:<div class="claim-text">generating processor requests that are directed to a plurality of peripheral devices by one or more requesting processors;</div> <div class="claim-text">creating a plurality of separate pending queues that correspond to each one of the plurality of peripheral devices, for queuing the processor requests directed to a particular peripheral device in entries of a corresponding pending queue;</div> <div class="claim-text">processing two separate processor requests directed to corresponding peripheral devices simultaneously, after retrieving such processor requests from their respective separate pending queues; and</div> <div class="claim-text">freeing entries of the processor requests in the return queue, after transmitting corresponding responses from the peripheral devices to the requesting processors.</div> </div>
    </div>
    </div> <div class="claim"> <div num="27" class="claim">
      <div class="claim-text">27. A system for processing data comprising:<div class="claim-text">means for generating processor requests that are directed to a plurality of peripheral devices by one or more requesting processors;</div> <div class="claim-text">means for creating a plurality of separate pending queues that correspond to each one of the plurality of peripheral devices, for queuing the processor requests directed to a particular peripheral device in entries of a corresponding pending queue;</div> <div class="claim-text">means for processing at least two separate processor requests directed to corresponding peripheral devices simultaneously, after retrieving such processor requests from their respective separate pending queues; and</div> <div class="claim-text">means for freeing entries of the processor requests in the return queue, after transmitting corresponding responses from the peripheral devices to the requesting processors.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="28" class="claim">
      <div class="claim-text">28. The method for processing data according to claim 27 further including means for performing dependency checking on the processor request, to generate non-blocking processor requests.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="29" class="claim">
      <div class="claim-text">29. The system for processing data according to claim 27 further including means for prioritizing the processor requests in the pending queues such that the higher priority processor requests are processed before the lower priority processor requests.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="30" class="claim">
      <div class="claim-text">30. The system for processing data according to claim 27 further including means for prioritizing the separate pending queues such that the higher priority pending queues have a higher number of maximum entries than the lower priority pending queues.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="31" class="claim">
      <div class="claim-text">31. The system for processing data according to claim 27 further including means for variably adding entries to the pending queues, only if processor requests directed to corresponding peripheral devices are generated.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="32" class="claim">
      <div class="claim-text">32. The system for processing data according to claim 27 further including means for marking a processor request as outstanding after such processor request is accepted by a corresponding peripheral device.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="33" class="claim">
      <div class="claim-text">33. The system for processing data according to claim 28 further including means for generating the non-blocking processor requests by running real-time processes.</div>
    </div>
    </div> <div class="claim"> <div num="34" class="claim">
      <div class="claim-text">34. A system for processing data comprising:<div class="claim-text">means for generating processor requests that are directed to a plurality of peripheral devices by one or more requesting processors;</div> <div class="claim-text">means for creating a plurality of separate pending queues that correspond to each one of the plurality of peripheral devices, for queuing the processor requests directed to a particular peripheral device in entries of a corresponding pending queue;</div> <div class="claim-text">means for processing at least two separate processor requests directed to corresponding peripheral devices simultaneously, after retrieving such processor requests from their respective separate pending queues;</div> <div class="claim-text">means for allocating a shared memory space for entries of the separate pending queues;</div> <div class="claim-text">means for freeing the entries of the processor requests in the pending queues, after such processor requests are accepted by the peripheral devices; and</div> <div class="claim-text">means for placing the outstanding processor request in the entries of a return queue on the shared memory space, after the corresponding peripheral devices respond to such outstanding processor requests.</div> </div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES67178208" lang="EN" load-source="patent-office" class="description">
    <heading>FIELD OF THE INVENTION</heading> <p>The present invention relates to interfacing between a microprocessor, peripherals and/or memories. More particularly, the present invention relates to a data processing system for interfacing a high-speed data bus, which is connected to one or more processors and a cache, to a second, possibly lower speed peripheral bus, which is connected to a plurality of peripherals and memories, by a non-blocking load buffer so that a continuous maximum throughput from multiple I/O devices is provided via programmed I/O (memory-mapped I/O) of the processors and a plurality of non-blocking loads may be simultaneously performed. Also, the present invention is directed to a multiple-priority non-blocking load buffer which serves a multiprocessor for running real-time processes of varying deadlines by priority-based non-FIFO scheduling of memory and peripheral accesses.</p>
    <heading>BACKGROUND OF THE INVENTION</heading> <p>Conventionally, microprocessor and memory system applications retrieve data from relatively low bandwidth I/O devices, process the data by a processor and then store the processed data to the low bandwidth I/O devices. In typical microprocessor memory system applications, a processor and a cache are directly coupled by a bus to a plurality of peripherals such as I/O devices and memory devices. However, the processor may be unnecessarily idled due to the latency between the I/O devices and the processor, thus causing the processor to stall and, as a result, excessive time is required to complete tasks.</p>
    <p>In known processing systems, when an operation is performed on one of the peripherals such as a memory device, the time between performing such an operation and subsequent operations is dependent upon the latency period of the memory device. Thereby, the processor will be stalled during the entire duration of the memory transaction. One solution for improving the processing speed of known processing systems is to perform additional operations during the time of the latency period as long as there is no load-use dependency upon the additional operations. For example, if data is loaded into a first register by a first operation (1), where the first operation (1) corresponds to:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">load r1 r2!                                          (1)</pre>
    
    <p>and the first register is added to another operation by a second operation (2) where the second operation (2) corresponds to:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">add r3r1+r4                                          (2)</pre>
    
    <p>the operation (2) is load-use dependent and the operation (2) must wait for the latency period before being performed. FIGS. 1(a) and 1(b) illustrate a load-use dependent operation where the time for initiating the operation (2) must wait until the operation (1) is completed. Operation (2) is dependent upon the short latency period t<sub>1</sub> in FIG. 1(a) corresponding to a fast memory and the long latency period t<sub>2</sub> in FIG. 1(b) corresponds to a slower memory.</p>
    <p>A non-blocking cache and a non-blocking processor are known where a load operation is performed and additional operations other than loads may be subsequently performed during the latency period as long as the operation is not dependent upon the initial load operation. FIGS. 2(a) and 2(b) illustrate such operations. In operation (1), the first register is loaded. Next, operations (1.1) and (1.2) are to be executed. As long as operations (1.1) and (1.2) are not load dependent on another load, these operations may be performed during the latency period t<sub>2</sub> as illustrated in FIG. 2(a). However, if operation (1.1) is either load-dependent or a pending load, operations (1.1) and (1.2) must wait until the latency period t<sub>2</sub> ends before being performed.</p>
    <p>Also known is a Stall-On-Use (Hit Under Miss) operation for achieving cache miss optimizations as described in "A 200 MFLOP Precision Architecture Processor" at Hot Chips IV, 1993, William Jaffe et al. In this Hit Under Miss operation, when one miss is outstanding only certain other types of instructions may be executed before the system stalls. For example, during the handling of a load miss, execution proceeds until the target register is needed as an operand for another instruction or until another load miss occurs. However, this system is not capable of handling two misses being outstanding at the same time. For a store miss, execution proceeds until a load or sub-word store occurs to the missing line.</p>
    <p>This Hit Under Miss feature can improve the runtime performance of general-purpose computing applications. Examples of programs that benefit from the Hit Under Miss feature are SPEC benchmarks, SPICE circuit simulators and gcc C compilers. However, the Hit Under Miss feature does not sufficiently meet the high I/O bandwidth requirements for digital signal processing applications such as digital video, audio and RF processing.</p>
    <p>Known microprocessor and memory system applications use real-time processes which are programs having deadlines corresponding to times where data processing must be completed. For example, an audio waveform device driver process must supply audio samples at regular intervals to the output buffers of the audio device. When the driver software is late in delivering data for an audio waveform device driver, the generated audio may be interrupted by objectionable noises due to an output buffer underflowing.</p>
    <p>In order to analyze whether or not a real-time process can meet its deadlines under all conditions requires predictability of the worst-case performance of the real-time processing program. However, the sensitivity of the real-time processing program to its input data or its environment makes it impractical in many cases to exhaustively check the behavior of the process under all conditions. Therefore, the programmer must rely on some combination of analysis and empirical tests to verify that the real-time process will complete in the requisite time. The goals of real-time processing tend to be incompatible with computing platforms that have memory or peripheral systems in which the latency of the transactions is unpredictable because an analysis of whether the real-time deadlines can be met may not be possible or worst-case assumptions of memory performance are required. For example, performance estimates can be made by assuming that every memory transaction takes the maximum possible time. However, such an assumption may be so pessimistic that any useful estimate for the upper bound on the execution time of a real-time task cannot be made. Furthermore, even if the estimates are only slightly pessimistic, overly conservative decisions will be made for the hardware performance requirements so that a system results that is more expensive than necessary.</p>
    <p>Also, it is especially difficult to reliably predict real-time processing performance on known multiprocessors because the memory and peripherals are not typically multi-ported. Therefore simultaneous access by two or more processors to the same memory device must be serialized. Even if a device is capable of handling multiple transactions in parallel, the bus shared by all of the processors may still serialize the transactions to some degree.</p>
    <p>If memory requests are handled in a FIFO manner by a known multiprocessor, a memory transaction which arrives slightly later than another memory transaction may take a much longer amount of time to complete since the later arriving memory requests must wait until the earlier memory request is serviced. Due to this sensitivity, very small changes in the memory access patterns of a program can cause large changes in its performance. This situation grows worse as more processors share the same memory. For example, if ten processors attempt to access the same remote memory location simultaneously, the spread in memory latency among the processors might be 10:1 because as many as nine of these memory transactions may be buffered for later handling. In general, it is not possible to predict which of these processors will suffer the higher latencies and which of these processors will receive fast replies to their memory accesses. Very small changes to a program or its input data may cause the program to exhibit slight operation differences which perturb the timing of the memory transactions.</p>
    <p>Furthermore, types of memory which exhibit locality effects may exacerbate the above-described situation. For example, accesses to DRAMs are approximately two times faster if executed in page mode. To use page mode, a recent access must have been made to an address in the same memory segment (page). One of the most common access patterns is sequential accesses to consecutive locations in memory. These memory patterns tend to achieve high page locality, thus achieving high throughput and low latency. Known programs which attempt to take advantage of the benefits of page mode may be thwarted when a second program executing on another processor is allowed to interpose memory transactions on a different memory page. For instance, if ten processors, each with its own sequential memory access pattern, attempt to access the same DRAM bank simultaneously and each of the accesses is to a different memory page, the spread and memory latencies between the fastest and slowest responses might be more than 25:1.</p>
    <p>The present invention is directed to allowing a high rate of transfer to memory and I/O devices for tasks which have real-time requirements. The present invention is also directed to allowing the system to buffer I/O requests from several processors within a multiprocessor at once with a non-blocking load buffer. Furthermore, the present invention is directed to extending the basic non-blocking load buffer to service a data processing system running real-time processes of varying deadlines by using scheduling of memory and peripheral accesses which is not strictly FIFO scheduling.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>An object of the present invention is to reduce the effect of memory latency by overlapping a plurality of non-blocking loads during the latency period and to achieve a similar reduction on the effect of latency for non-blocking stores without requiring the additional temporary storage memory of a separate store buffer.</p>
    <p>Another object of the present invention is to provide a non-blocking load buffer which buffers I/O requests from one or more processors within a multiprocessor so that a plurality of I/O memory transactions, such as loads and stores, may be simultaneously performed and the high I/O bandwidth requirements for digital signal processing applications may be met.</p>
    <p>A still further object of the present invention is to provide a multiple-priority non-blocking load buffer for serving a multiprocessor running real-time processes of varying deadlines by using a priority-based method to schedule memory and peripheral accesses.</p>
    <p>The objects of the present invention are fulfilled by providing a data processing system comprising a first data bus for transferring data requests at a first speed, a second bus for transmitting I/O data of a second speed, and a non-blocking load buffer connected to the first and second data buses for holding the data requests and the I/O data so that a plurality of loads and stores may be performed simultaneously. It is possible for the speed of the second bus to be slower than the first speed of the first bus. As a result, data may be retrieved from relatively low bandwidth I/O devices and the retrieved data may be processed and stored to low bandwidth I/O devices without idling the system unnecessarily.</p>
    <p>In a further embodiment for a data processing system of the present invention, the first data bus is connected to a processor and the second data bus is connected to a plurality of peripherals and memories. The data processing system for this embodiment reduces the effect of latency of the peripherals and memories so that the application code, which uses programmed I/O, may meet its real-time constraints. Furthermore, the code may have reduced instruction scheduling constraints and a near maximum throughput may be achieved from the I/O devices of varying speeds.</p>
    <p>Another embodiment of the present invention is fulfilled by providing a non-blocking load buffer comprising a memory array for temporarily storing data, and a control block for simultaneously performing a plurality of data loads and stores. The non-blocking load buffer for this embodiment allows I/O requests from several processors to be performed at once.</p>
    <p>Further scope of applicability of the present invention will become apparent from the detailed description given hereafter. However, it should be understood that the detailed description and specific examples, while indicating preferred embodiments of the invention, are given by way of illustration only, since various changes and modifications within the spirit and scope of the invention will become apparent to those skilled in the art from this detailed description.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p>The present invention will become more fully understood from the detailed description given hereinbelow and the accompanying drawings which are given by way of illustration only, and thus are not limitative of the present invention and wherein:</p>
    <p>FIGS. 1(a) and 1(b) illustrate time dependency in a conventional load use operation;</p>
    <p>FIGS. 2(a) and 2(b) illustrate time dependency in a known Stall-On-Use operation;</p>
    <p>FIG. 2(c) illustrates time dependency in a non-blocking load buffer for an embodiment of the present invention;</p>
    <p>FIG. 3(a) illustrates unpredictable memory latency in a known multiprocessor system;</p>
    <p>FIG. 3(b) illustrates memory latency in a multiprocessor system having a multiple-priority version of the non-blocking load buffer;</p>
    <p>FIG. 4 illustrates a block diagram for the data processing system according to one embodiment of the present invention;</p>
    <p>FIG. 5 is a schematic illustration of the non-blocking load buffer for an embodiment of the present invention;</p>
    <p>FIG. 6 illustrates an example of the contents for the entries in the memory of the data processing system for an embodiment of the present invention;</p>
    <p>FIGS. 7(a) and 7(b) illustrate examples of the possible states for a non-blocking load through the memory of the data processing system for an embodiment of the present invention;</p>
    <p>FIGS. 7(c), 7(d), 7(e), 7(f), and 7(g) illustrate the progress of addresses through queues of the data processing system in an embodiment of the present invention;</p>
    <p>FIG. 8 illustrates the circuitry used for the non-blocking load buffer in an embodiment of the present invention;</p>
    <p>FIGS. 9(a) and 9(b) illustrate parallel pending queues which allow prioritization of data in the data processing system for an embodiment of the present invention;</p>
    <p>FIG. 10 illustrates a detailed block diagram of an embodiment of the non-blocking load buffer; and</p>
    <p>FIG. 11 illustrates a flow chart of the control of the pending queue in an embodiment of the present invention.</p>
    <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading> <p>The first embodiment of the present invention will be discussed with reference to FIG. 4. The data processing system includes a non-blocking load buffer 10, a cache 20, one or more processors 30<sub>0</sub>, 30<sub>1</sub>, . . . , 30<sub>m</sub>, a processor/cache bus 40 for connecting the non-blocking load buffer 10, the cache 20 and the processors 30<sub>0</sub>..m, a plurality of peripheral devices 50<sub>0</sub>, 50<sub>1</sub>, . . . , 50<sub>n</sub>, and a peripheral bus 60, which includes an output bus 61 and an input bus 62, for connecting the non-blocking load buffer 10 with the plurality of peripheral devices 50<sub>0</sub>..n. Some or all of the peripheral devices 50<sub>0</sub>..n may be memories, such as DRAM for example, which serve as a backing store for the cache 20 or as memory which the processors 30<sub>0</sub>..m access directly through the non-blocking load buffer 10, thus bypassing the cache 20.</p>
    <p>The processor/cache bus 40, which includes an input bus 41 and an output bus 42, transfers data from the cache 20 to registers of the processors 30<sub>0</sub>..m as loads via the output bus 42 or alternatively, from the registers of the processors 30<sub>0</sub>..m to the cache 20 as stores via the input bus 41. All I/O operations, which are programmed I/O, and any memory operations, even those which bypass the cache 20, are transmitted over the processor/cache bus 40 as well. For such operations the source or sink of the data is one of the peripheral devices 50<sub>0</sub>..n, such as a DRAM, for example.</p>
    <p>The bandwidth of the peripheral bus 60 should be chosen to meet the peak input or output bandwidth which is to be expected by the peripheral devices 50<sub>0</sub>..n. The bandwidth of the non-blocking load buffer 10 should therefore be chosen to at least equal the sum of the bandwidth of the processor/cache bus 40 and the bandwidth of the peripheral bus 60.</p>
    <p>The processors 30<sub>0</sub>..m may be designed with dependency-checking logic so that they will only stall when data requested by a previously executed load instruction has not yet arrived and that data is required as a source operand in a subsequent instruction. FIG. 2(c) illustrates the time dependency for the non-blocking load buffer 10 in an embodiment of the present invention. In FIG. 2(c), load instructions are executed at operations (1), (1.1), (1.2), . . . (1.n). In this example, operations (1), (1.1), (1.2), . . . (1.n) correspond to:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">load r1 r2!(1);</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">load r5 r2+24!(1.1);</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">load r6 r2+32!(1.2);</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">. . . function z1 (1.n)</pre>
    
    <p>In this example, operation (2) executes an instruction dependent on operation (1), operation (2.1) executes an instruction dependent on operation (1.1), operation (2.2) executes an instruction dependent on operation (1.2), . . . operation (2.n) executes an instruction dependent on operation (1.n). For instance, operations (2.1), (2.2), . . . (2.n) correspond to:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">add r3r1+r4(2);</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">add r7r5+r4(2.1);</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">add r8r6+r4(2.2);</pre>
    
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">. . . function z2(2.n).</pre>
    
    <p>n may be any positive integer. Operations (1.1), (1.2), . . . (1.n) occur as fast as possible and the time between these operations is dependent upon the processor cycle for each operation. As shown in FIG. 2(c), operation (2) waits only for a part of the latency period t<sub>2</sub> between operations (1) and (2), which is represented by t<sub>i</sub>, since the part of the latency period t<sub>2</sub> between operations (1) and (2) is overlapped with the operations (1.1), (1.2), . . . (1.n). Thereafter, operations (2.1) and (2.2) occur as fast as possible after operation (2) with the time limitation being the processor cycle. The operations (2.1) and (2.2) do not wait for the full latency period t<sub>2</sub> of operation (1.1) since the latency period t<sub>2</sub> is partially overlapped with operations (1.1), (1.2), . . . (1.n) and the latency period t<sub>2</sub> for operation (1). For values of n sufficiently large, t<sub>i</sub> may effectively be reduced to zero and the processor will not suffer any stalls due to memory or I/O latency. In this embodiment, there may be more than one outstanding load to slower memories or peripherals, as shown at instant t<sub>x</sub> in FIG. 2(c) for example, and requests may thereby be pipelined to provide continuous maximum throughput from the plurality of peripheral devices 50<sub>0</sub>..n from programmed I/O of the processor.</p>
    <p>The non-blocking load buffer 10 may accept both load and store requests from the processors 30<sub>0</sub>..m to the peripheral devices 50<sub>0</sub>..n. The non-blocking load buffer 10 may accept requests at the maximum rate issued by a processor for a period of time that is limited by the amount of intrinsic storage in the non-blocking load buffer 10. After accepting requests, the non-blocking load buffer 10 then forwards the requests to the peripheral devices 50<sub>0</sub>..n at a rate which the peripheral devices 50<sub>0</sub>..n may process them. However, the non-blocking load buffer 10 does not have to wait until all of the requests are accepted.</p>
    <p>FIG. 5 illustrates internal architecture for the non-blocking load buffer in an embodiment of the present invention. The internal architecture of the non-blocking load buffer 10 includes a plurality of variable-length pending queus 114<sub>0</sub>, 114<sub>1</sub>, . . . 114<sub>n</sub> which contain entries that represent the state of requests made by the processors 30<sub>0</sub>..m to the peripheral devices 50<sub>0</sub>..n. The pending queues 114<sub>0</sub>..n correspond to each of the peripheral devices 50<sub>0</sub>..n and are used to hold the address and control information for a load or a store before being transmitted to the corresponding peripheral device. If an entry in the pending queues 114<sub>0</sub>..n contains a store transaction, then that entry will also contain the data to be written to the corresponding peripheral device. Another component of the non-blocking load buffer 10 is a variable-length return queue 116 used to buffer the data returned by any of the peripheral devices 50<sub>0</sub>..n in response to a load until the data can be transmitted to the requesting processor. In addition, there is a free pool 112 that contains any unused request entries. Once a load request is sent from one of the pending queues 114<sub>0</sub>..n to its corresponding peripheral device, the load request is marked as "outstanding" until a return data value is sent by the peripheral and enqueued in the return queue 116. At any time, a given request entry is either outstanding, in one of the pending queues 114<sub>0</sub>..n, in the return queue 116, or is not in use and is therefore located in the free pool 112.</p>
    <p>FIG. 6 illustrates an example of a memory structure 200 for storing the information present in each entry of the non-blocking load buffer 10. E entries may be stored in the non-blocking load buffer 10 and each entry holds one of a plurality of requests 201<sub>1</sub>, 201<sub>2</sub>, . . . 201<sub>E</sub> which can either be a load or a store to any of the peripheral devices 50<sub>0</sub>..n. Each request 201 includes a control information 202, an address 204 and data 206. However, for a load operation, the data 206 is empty until data returns from the peripheral devices 50<sub>0</sub>..n. The control information 202 describes parameters such as whether the transaction is a load or a store, the size of data transferred, and a coherency flag that indicates whether all previous transactions must have been satisfied before the new transaction is permitted to be forwarded to the addressed peripheral device. The coherency flag enforces sequential consistency, which is occasionally a requirement for some I/O or memory operations. The data stored in the entries 201 need not be transferred among the queues of the non-blocking load buffer 10. Rather pointers to individual entries within the memory structure 200 may be exchanged between the pending queues 114<sub>0</sub>..n, the return queue 116 and the free pool 112 for example.</p>
    <p>FIGS. 7(a) and 7(b) illustrate examples of the operation of the non-blocking load buffer 10 which will be input to a peripheral device such as a RAM. Initially, all entries of the non-blocking load buffer 10 are unused and free. With reference to FIG. 7(a), when one of the processors 30<sub>0</sub>..m issues a non-blocking load, a free entry is chosen and the address of the load is stored in this entry. Additionally, space for the returning data is allocated. This entry now enters the pending queue, one of the queues 114<sub>0</sub>..n, corresponding to the addressed peripheral device. When the peripheral device is ready, the pending entry is delivered over the peripheral bus 60 and becomes an outstanding entry. When the outstanding entry returns from the peripheral device via the peripheral bus 60, the accompanying requested load data is written to the part of the entry that had been allocated for the returning data and the entry is placed in the return queue 116. The data is buffered until the one processor is ready to receive the data over the processor/cache bus 40 (i.e., until the one processor is not using any required cache or register file write ports). Finally, the data is read out and returned to the one processor over the processor/cache bus 40 and the entry again becomes free. In addition, a peripheral device may accept multiple requests before returning a response.</p>
    <p>FIG. 7(b) illustrates the progress of an entry during a store operation. The store operation is similar to the load operation but differs because data does not need to be returned to the processors 30<sub>0</sub>..m and this return step is, therefore, skipped. Once a store is accepted by the non-blocking load buffer 10, the store is treated by the issuing processor as if the data has already been transferred to the target peripheral device, one of the peripheral devices 50<sub>0</sub>..n.</p>
    <p>The non-blocking load buffer 10 performs similarly to a rate-matching FIFO but differs in several ways. For instance, one difference is that although memory transactions forwarded by the non-blocking load buffer 10 are issued to any particular peripheral device 50<sub>0</sub>..n in the same order that they were requested by any of the processors 30<sub>0</sub>..m, the transactions are not necessarily FIFO among different peripheral devices. This is similar to having a separate rate-matching FIFO for outputing to each of the peripheral devices 50<sub>0</sub>..n except that the storage memory for all the FIFOs is shared and is thereby more efficient. Also, the peripheral devices 50<sub>0</sub>..n are not required to return the results of a load request to the non-blocking load buffer 10 in the same order that the requests were transmitted.</p>
    <p>The non-blocking load buffer 10 is capable of keeping all of the peripheral devices 50<sub>0</sub>..n busy because an independent pending queue exists for each of the peripheral devices. A separate flow control signal from each of the peripheral devices indicates that this corresponding peripheral device is ready. Accordingly, a request for a fast peripheral device does not wait behind a request for a slow peripheral device.</p>
    <p>FIGS. 7(c), 7(d), 7(e), 7(f) and 7(g) illustrate examples of a non-blocking load progressing through the queues. After the queues are reset, entries 1, 2, 3 and 4 are in the free pool as shown in FIGS. 7(c). A non-blocking load is pending after the entry 1 is written into one of the pending queues as shown in FIG. 7(d). Next, FIG. 7(e) illustrates that entry 1 becomes outstanding after the pending load is accepted by its corresponding peripheral device. Subsequently, the load is returned from the peripheral device and entry 1 is now in the returned queue as illustrated in FIG. 7(f), and awaits transmission to the requesting processor (or to the cache if the access was part of a cache fill request). Finally, in FIG. 7(g), entry 1 is released to the free pool and, once again, all entries are free.</p>
    <p>FIG. 8 illustrates the circuitry used for the non-blocking load buffer in an embodiment of the present invention which implements variable depth FIFOs. This circuitry manipulates pointers (i.e. RAM addresses for example) to entries in a RAM 130 instead of storing the addresses of only the head and tail of a contiguous list of entries in the RAM 130. Furthermore, the circuitry does not require the pointers to be sequential. Therefore, records need not be contiguous and the RAM 130 can contain several interleaved data structures allowing more flexible allocation and de-allocation of entries within a FIFO so that it is possible to use the same storage RAM 130 for multiple FIFOs. The entire circuit in FIG. 8 represents a large FIFO of an unspecified data width which is controlled by a smaller FIFO represented by block 170 having pointer widths smaller than the records. In FIG. 8, a first control circuit 110, a second control circuit 120, the RAM 130 having separate read and write ports, a plurality of shift registers 150<sub>1</sub>, 150<sub>2</sub>, 150<sub>3</sub>, . . . 150<sub>n</sub> corresponding to each entry in the RAM 130 and a multiplexer 160 are illustrated. The control circuits 110 and 120 provide pointers to the entries of the RAM 130 rather than the entries themselves being queued. Block 170 is essentially a FIFO of pointers. A pointer may be enqueued in the FIFO of pointers in block 170 by asserting the "push" control line active and placing the pointer (SRAM 130 address) on the "push-addr" input lines to block 170. Similarly, a pointer value may be dequeued by asserting the "pop" control line on block 170 with the pointer value appearing on the "pop-addr" bus. Block 170 has a "full" output that when active indicates that no more pointers may be enqueued because all the registers 150<sub>1</sub>..p are occupied and the queue is at its maximum depth, p. Block 170 also has an "empty" output that indicates when no pointers remain enqueued.</p>
    <p>In a preferred embodiment, the RAM 130 is divided into two arrays, an address control array AC and a data array D. The address control array AC stores the control information 202 and the address 204 for each entry and the data array D stores the data 206 for each entry. Dividing the non-blocking load buffer into these two arrays AC and D realizes a more efficient chip layout than using a single array for both purposes.</p>
    <p>FIG. 10 illustrates the entire structure for a non-blocking load buffer in an embodiment of the present invention. Each of the pending queues 114<sub>0</sub>..n and the return queue 116 may be composed of copies of block 170 illustrated in FIG. 8 for example. The queue entries are stored in the AC and D arrays 501 and 502, with pointers to the array locations in the flip-flops that compose the pending queues 114<sub>0</sub>..n, the return queue 116 and the free pool 112. For the embodiment illustrated in FIG. 10, the write ports to the AC and D arrays are time-division multiplexed using multiplexers 301 and 302 to allow multiple concurrent accesses from the processors 30<sub>0</sub>..m, which issue load and store requests over bus 41, and the peripheral devices 50<sub>0</sub>..n, which return requested load data over bus 62. Similarly, the read ports are time-division multiplexed by multiplexer 303 between returning load data going back to the processors 30<sub>0</sub>..m via bus 42 and issuing of load and store requests to the peripherals 50<sub>0</sub>..n via bus 61. A processor request control unit 510 accepts the load and store requests from the processors 30<sub>0</sub>..m, a return queue control unit 520 controls the data returned by the peripheral devices 50<sub>0</sub>..n to the return queue 116 and a pending queue control unit 530 controls the selection of the pending queue 114<sub>0</sub>..n.</p>
    <p>When either a non-blocking load buffer or a conventional single FIFO load buffering operation is used, the uncertainty and memory latency is further request at a time as illustrated in FIG. 3(a) for example. In FIG. 3(a), processor A issues three non-blocking loads to a memory device after a first non-blocking load is issued by processor B but before a second non-blocking load is issued by processor B to the same memory device. The first memory transaction issued by processor B, labeled B1, will be executed immediately. However, the latency of the second transaction, labeled B2, by processor B, t<sub>B2</sub>, will be considerably longer than the latency of the first transaction, t<sub>B1</sub>, since the second transaction B2 will not be handled by the memory system until all of the requests A1, A2, and A3 by processor A have been processed.</p>
    <p>One consideration for overcoming all of these unpredictable memory latency effects is to have the programmer carefully design the times at which each real-time process attempts to access the memory which effectively has software serialize access to the memory banks instead of hardware. For instance, two processors may alternate memory accesses at regular intervals to match the throughput capabilities of the memory system. However, such programming places a heavy burden on both the program and the programmer because the programmer must be aware of the detailed temporal behavior of the memory access patterns by the program. The behavior of the programs is usually very complex and may change based on its input data, which is not always known. Therefore, it is impractical for the programmer to know the detailed temporal behavior of the memory access patterns by the program.</p>
    <p>Also, because the combinations of processors that are making the requests are even more difficult to predict and the combinations of programs running on the different processors might not always be the same on each occasion or even known, the detailed temporal behavior of the memory access patterns by the program may not be known. For example, two processes that run two different programs on two different processors may have been written by different programmers and the source code may not even be available for either or both programs. As a result, this programming approach for overcoming the unpredictable memory latency defeats the purpose of using a non-blocking load buffer, which was designed to simplify the burden on the programmer and the compiler by relaxing the scheduling constraints under which memory operations can be issued while still permitting efficient utilization of the processor.</p>
    <p>FIG. 9(a) illustrates another embodiment of the present invention for a multiple-priority version of the non-blocking load buffer. In this embodiment, the non-blocking load buffer is a variation of the basic non-blocking load buffer where multiple pending sub-queues 214<sub>0</sub>, 214<sub>1</sub>, . . . 214<sub>p</sub> exist as a component of each pending queue 114<sub>0</sub>..n. The outputs of the sub-queues 214<sub>0</sub>..p are then input to the sub-multiplexers 218 associated with one of the peripheral devices. Each output of the sub-multiplexers 218 are then input to the main multiplexer 118. Each of these pending sub-queues is assigned a unique priority level. In the simplest implementation of this multiple-priority version of the non-blocking load buffer, illustrated in FIG. 9(b), there are two pending sub-queues 214<sub>0</sub> and 214<sub>1</sub> for a peripheral device with sub-queue 214<sub>0</sub> being assigned a high priority and sub-queue 214<sub>1</sub> assigned a low priority. The multiple-priority non-blocking load buffer issues memory or peripheral transactions in a highest-priority-first manner. In other words, no transaction will be issued from a pending sub-queue for a given peripheral unless all of the higher priority sub-queues for that same peripheral are empty. For each peripheral device, requests are issued from the pending sub-queue in a FIFO manner for memory transactions of the same priority.</p>
    <p>The computational processes associated with the memory transactions that take place at each of these relative priority levels execute on the processors 30<sub>0</sub>..m. The priority levels are assigned by the processors based on the scheduling parameters of all the processes in the system. The priority level of the process is designed to reflect the relative importance of the memory accesses with respect to allowing all of the processes to meet their deadlines if possible. The priority levels may also be applicable in a uni-processor environment. For example, interrupt handlers may be provided to achieve low latency by using higher priority memory transactions than the transaction being interrupted. The priority level may be identified by adding a priority tag to the memory transaction. This priority tag is used to channel the memory transaction into the pending sub-queue with the matching priority level, thus the selection of the appropriate destination pending sub-queue for a non-blocking memory transaction is a function of both the address and the priority level of the access. The priority tag may be stored with the other control information 202 in the queue entry 201 corresponding to a given non-blocking memory access.</p>
    <p>FIG. 11 illustrates a flow chart for the functions performed by the pending queue control unit in an embodiment of the present invention for the multiple-priority version of the non-blocking load buffer. At step S10, the counter i for a pending queue is initialized to zero. At step S20, the counter is compared to the number n of pending queues 114. If the counter is not equal to the total number of pending queues, the counter is incremented at step S30 and a determination is made at step S40 of whether the peripheral corresponding to the counter is ready. If the peripheral device is determined not to be ready at step S40, the process returns to step S20. However, if the peripheral corresponding to the counter is ready at step S40, a pending queue is selected at step S50 with the highest priority that also contains at least one memory transaction. At step S60, memory transactions are issued in a FIFO order from the selected sub-queue and the process returns to step S20. If the counter is equal to the number of pending queues at S20, the process has been completed for all the pending queues and step S10 is returned to where the counter is initialized.</p>
    <p>FIG. 3(b) illustrates memory latency for the multiple-priority non-blocking load buffer. In this example, processor B's memory transactions are assigned a higher priority than processor A's memory transactions. Therefore, transaction B2 is delivered to the memory before transactions A2 and A3 even though the request to begin transaction B2 arrived at the non-blocking load buffer after requests A2 and A3. As a result, the latency for transaction B2, t<sub>B2</sub> is less in FIG. 3(b) than t<sub>B2</sub> in FIG. 3(a), which illustrates a non-blocking load buffer that does not offer the benefit of multiple-priority scheduling. Using the multiple-priority version of the non-blocking load buffer, Processor B spends less time stalled waiting for transaction B2 to complete as illustrated by the comparison in FIGS. 3(a) and 3(b).</p>
    <p>These priority levels may be heuristic in nature. For example, if using earliest deadline first (EDF) scheduling, a process should not be assigned a lower priority than the priority of any process which has a more distant deadline. In general, this priority level is not necessarily fixed for each process and the priority level may vary over time as the demands of the real-time process or of other processes change. As another example of a priority selection mechanism, if the load-use distance is known for a load instruction (as computed by a compiler), it can be used to set a priority for each individual memory access instruction. (Higher load-use distances result in lower priorities.) In general, non real-time processes (the processes having no deadlines) are typically given the lowest priority.</p>
    <p>Process priority may also be used to arbitrate for the limited resources within the non-blocking load buffer itself. For example, in one embodiment of the non-blocking load buffer, limited data storage memory is shared among all peripheral devices and processors. When the total number of slots among all of the non-blocking load buffer queues exceeds the number of non-blocking memory entries, it is possible for a process at a low priority to prevent a higher priority process from completely utilizing one or more of its pending sub-queues by using up these entries. The use of priority in allocating non-blocking memory entries can be used to eliminate this effect. For example, the maximum number of outstanding non-blocking memory transactions may be specified for each of the available priorities.</p>
    <p>Once an appropriate priority has been determined for a process, the priority of its memory transactions might be specified to the non-blocking load buffer by employing any of several techniques. For example, the priority of any given memory transaction might be determined by an operand to the instruction that performs the memory access or the priority can be associated with certain bit combinations in either the virtual address, the physical address of the memory or the physical address of the peripheral device accessed. Alternatively, the processors might be designed with programmable registers that set the priority of all memory accesses for each processor. Yet another possible technique is to store the memory access priority in the page table of a virtual memory system and thus make the memory access priority a characteristic of specific memory pages. When these various techniques are combined with conventional memory management and processor design techniques, memory priorities can be treated as privileged resources. As a result, the operating system reserves the highest priority levels for its own real-time tasks and therefore the user level programs are extremely limited in their ability to disrupt important system real-time tasks.</p>
    <p>To maintain memory coherency in a multiple-priority non-blocking load buffer, which is not always necessary, no load or store may be issued to any of the peripheral devices while a load or store to the same address is outstanding or while an earlier load or store to the same address is pending. The memory coherency may be maintained by time stamps combined with address comparison. However, memory coherency may also be maintained more simply by ensuring that requests of different priorities are sent to non-overlapping address segments within the same peripheral device.</p>
    <p>In the normal operation of programmed I/O activity, the processors 30<sub>0</sub>..m do not need to exactly schedule loads to achieve maximum throughput from the peripheral devices but can instead burst out requests to the limits of the non-blocking load storage and, if properly programmed, perform useful work while waiting for data to return. The non-blocking load buffer allows application code to use programmed I/O (memory mapped I/O) for achieving near maximum throughput from the I/O devices of varying speeds which reduces the effective latency of the I/O peripheral devices and relaxes the scheduling constraints on the programmed I/O. The non-blocking load buffer functions to rate-match the requests from the processor to the peripheral devices and back and to act upon the priority of requests from the processor to allow high priority requests to go ahead of low priority requests already buffered.</p>
    <p>The non-blocking load buffer uses queues of pointers to centralize storage to increase storage density, parallel queues to implement requests of different priority and memory segment descriptors to determine priority. Accordingly, I/O requests to a fast device need not wait behind requests to a slow device and requests from several processes of a signal processor running multiple processes are buffered so that the processor is not unnecessarily idled and the time to complete tasks is reduced. Because the multiple priority non-blocking load buffer has multiple pending sub-queues for each of the peripherals, a processor used in combination with this multiple priority non-blocking load buffer is able to run real-time processes of varying deadlines by use of non-FIFO scheduling of memory and peripheral accesses. Also, the multiple priority non-blocking load buffer simplifies the burden on the programmer and the compiler by relaxing the scheduling constraints under which memory operations can be issued while still permitting efficient utilization of the processor.</p>
    <p>The invention being thus described, it will be obvious that the same may be varied in many ways. Such variations are not to be regarded as a departure from the spirit and scope of the invention, and all such modifications as would be obvious to one skilled in the art are intended to be included within the scope of the following claims.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4833655">US4833655</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 28, 1985</td><td class="patent-data-table-td patent-date-value">May 23, 1989</td><td class="patent-data-table-td ">Wang Laboratories, Inc.</td><td class="patent-data-table-td ">FIFO memory with decreased fall-through delay</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5043981">US5043981</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 29, 1990</td><td class="patent-data-table-td patent-date-value">Aug 27, 1991</td><td class="patent-data-table-td ">Advanced Micro Devices, Inc.</td><td class="patent-data-table-td ">Method of and system for transferring multiple priority queues into multiple logical FIFOs using a single physical FIFO</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5179688">US5179688</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 30, 1987</td><td class="patent-data-table-td patent-date-value">Jan 12, 1993</td><td class="patent-data-table-td ">Tandem Computers Incorporated</td><td class="patent-data-table-td ">Queue system with uninterrupted transfer of data through intermediate locations to selected queue location</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5208490">US5208490</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 12, 1991</td><td class="patent-data-table-td patent-date-value">May 4, 1993</td><td class="patent-data-table-td ">Hewlett-Packard Company</td><td class="patent-data-table-td ">Functionally complete family of self-timed dynamic logic circuits</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5249297">US5249297</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 29, 1991</td><td class="patent-data-table-td patent-date-value">Sep 28, 1993</td><td class="patent-data-table-td ">Hewlett-Packard Company</td><td class="patent-data-table-td ">Methods and apparatus for carrying out transactions in a computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5257356">US5257356</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 28, 1991</td><td class="patent-data-table-td patent-date-value">Oct 26, 1993</td><td class="patent-data-table-td ">Hewlett-Packard Company</td><td class="patent-data-table-td ">Method of reducing wasted bus bandwidth due to slow responding slaves in a multiprocessor computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5289403">US5289403</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 8, 1991</td><td class="patent-data-table-td patent-date-value">Feb 22, 1994</td><td class="patent-data-table-td ">Hewlett-Packard Company</td><td class="patent-data-table-td ">Self-timed content addressable memory access mechanism with built-in margin test feature</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5299158">US5299158</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 16, 1992</td><td class="patent-data-table-td patent-date-value">Mar 29, 1994</td><td class="patent-data-table-td ">Hewlett-Packard Company</td><td class="patent-data-table-td ">Memory device with multiple read ports</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5317204">US5317204</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 19, 1992</td><td class="patent-data-table-td patent-date-value">May 31, 1994</td><td class="patent-data-table-td ">Hewlett-Packard Company</td><td class="patent-data-table-td ">Mitigating the adverse effects of charge sharing in dynamic logic circuits</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5329176">US5329176</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 29, 1993</td><td class="patent-data-table-td patent-date-value">Jul 12, 1994</td><td class="patent-data-table-td ">Hewlett-Packard Company</td><td class="patent-data-table-td ">Self-timed clocking system and method for self-timed dynamic logic circuits</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5343096">US5343096</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 19, 1992</td><td class="patent-data-table-td patent-date-value">Aug 30, 1994</td><td class="patent-data-table-td ">Hewlett-Packard Company</td><td class="patent-data-table-td ">System and method for tolerating dynamic circuit decay</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5363485">US5363485</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 1, 1992</td><td class="patent-data-table-td patent-date-value">Nov 8, 1994</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Bus interface having single and multiple channel FIFO devices using pending channel information stored in a circular queue for transfer of information therein</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5367681">US5367681</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 2, 1993</td><td class="patent-data-table-td patent-date-value">Nov 22, 1994</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Method and apparatus for routing messages to processes in a computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5450564">US5450564</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 3, 1993</td><td class="patent-data-table-td patent-date-value">Sep 12, 1995</td><td class="patent-data-table-td ">Unisys Corporation</td><td class="patent-data-table-td ">Method and apparatus for cache memory access with separate fetch and store queues</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5459839">US5459839</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 22, 1994</td><td class="patent-data-table-td patent-date-value">Oct 17, 1995</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and method for managing queue read and write pointers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5499346">US5499346</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 28, 1993</td><td class="patent-data-table-td patent-date-value">Mar 12, 1996</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Bus-to-bus bridge for a multiple bus information handling system that optimizes data transfers between a system bus and a peripheral bus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5507032">US5507032</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 2, 1993</td><td class="patent-data-table-td patent-date-value">Apr 9, 1996</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Multiprocessor I/O request control system forming device drive queue and processor interrupt queue from rows and cells of I/O request table and interrupt request table</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5526508">US5526508</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 16, 1994</td><td class="patent-data-table-td patent-date-value">Jun 11, 1996</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Cache line replacing system for simultaneously storing data into read and write buffers having multiplexer which controls by counter value for bypassing read buffer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5535340">US5535340</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 20, 1994</td><td class="patent-data-table-td patent-date-value">Jul 9, 1996</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Method and apparatus for maintaining transaction ordering and supporting deferred replies in a bus bridge</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5541912">US5541912</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 4, 1994</td><td class="patent-data-table-td patent-date-value">Jul 30, 1996</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">Dynamic queue length thresholds in a shared memory ATM switch</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5542055">US5542055</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 28, 1993</td><td class="patent-data-table-td patent-date-value">Jul 30, 1996</td><td class="patent-data-table-td ">International Business Machines Corp.</td><td class="patent-data-table-td ">System for counting the number of peripheral buses in each hierarch connected to primary bus for creating map of peripheral buses to locate peripheral devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5546546">US5546546</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 20, 1994</td><td class="patent-data-table-td patent-date-value">Aug 13, 1996</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Method and apparatus for maintaining transaction ordering and arbitrating in a bus bridge</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5548791">US5548791</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 25, 1994</td><td class="patent-data-table-td patent-date-value">Aug 20, 1996</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Input/output control system with plural channel paths to I/O devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5557744">US5557744</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 28, 1993</td><td class="patent-data-table-td patent-date-value">Sep 17, 1996</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Multiprocessor system including a transfer queue and an interrupt processing unit for controlling data transfer between a plurality of processors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5568620">US5568620</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 30, 1993</td><td class="patent-data-table-td patent-date-value">Oct 22, 1996</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Method and apparatus for performing bus transactions in a computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5588125">US5588125</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 24, 1995</td><td class="patent-data-table-td patent-date-value">Dec 24, 1996</td><td class="patent-data-table-td ">Ast Research, Inc.</td><td class="patent-data-table-td ">Method and apparatus for increasing bus bandwidth on a system bus by inhibiting interrupts while posted I/O write operations are pending</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5623628">US5623628</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 2, 1994</td><td class="patent-data-table-td patent-date-value">Apr 22, 1997</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Computer system and method for maintaining memory consistency in a pipelined, non-blocking caching bus request queue</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5625778">US5625778</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 3, 1995</td><td class="patent-data-table-td patent-date-value">Apr 29, 1997</td><td class="patent-data-table-td ">Apple Computer, Inc.</td><td class="patent-data-table-td ">Method and apparatus for presenting an access request from a computer system bus to a system resource with reduced latency</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Doug Hunt, "<a href='http://scholar.google.com/scholar?q="Advanced+Performance+Features+of+the+64-bit+PA-8000"'>Advanced Performance Features of the 64-bit PA-8000</a>", IEEE, pp. 123-128, 1995.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Doug Hunt, Advanced Performance Features of the 64 bit PA 8000 , IEEE, pp. 123 128, 1995.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Eric DeLano et al., "<a href='http://scholar.google.com/scholar?q="A+High+Speed+Superscaler+PA-RISC+Processor"'>A High Speed Superscaler PA-RISC Processor</a>", IEEE, pp. 116-121, 1992.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Eric DeLano et al., A High Speed Superscaler PA RISC Processor , IEEE, pp. 116 121, 1992.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Jonathan Lotz et al., "<a href='http://scholar.google.com/scholar?q="A+CMOS+RISC+CPU+Designed+for+Sustained+High+Performance+on+Large+Applications"'>A CMOS RISC CPU Designed for Sustained High Performance on Large Applications</a>", IEEE Journal of Solid-State Circuits, vol. 25, pp. 1190-1198, Oct. 1990.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Jonathan Lotz et al., A CMOS RISC CPU Designed for Sustained High Performance on Large Applications , IEEE Journal of Solid State Circuits, vol. 25, pp. 1190 1198, Oct. 1990.</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6112265">US6112265</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 7, 1997</td><td class="patent-data-table-td patent-date-value">Aug 29, 2000</td><td class="patent-data-table-td ">Intel Corportion</td><td class="patent-data-table-td ">System for issuing a command to a memory having a reorder module for priority commands and an arbiter tracking address of recently issued command</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6154815">US6154815</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 25, 1997</td><td class="patent-data-table-td patent-date-value">Nov 28, 2000</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Non-blocking hierarchical cache throttle</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6249520">US6249520</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 24, 1997</td><td class="patent-data-table-td patent-date-value">Jun 19, 2001</td><td class="patent-data-table-td ">Compaq Computer Corporation</td><td class="patent-data-table-td ">High-performance non-blocking switch with multiple channel ordering constraints</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6351783">US6351783</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 20, 1999</td><td class="patent-data-table-td patent-date-value">Feb 26, 2002</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Method and apparatus for isochronous data transport over an asynchronous bus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6571301">US6571301</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 5, 1999</td><td class="patent-data-table-td patent-date-value">May 27, 2003</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Multi processor system and FIFO circuit</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6684299">US6684299</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 28, 2001</td><td class="patent-data-table-td patent-date-value">Jan 27, 2004</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Method for operating a non-blocking hierarchical cache throttle</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6694149">US6694149</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 22, 1999</td><td class="patent-data-table-td patent-date-value">Feb 17, 2004</td><td class="patent-data-table-td ">Motorola, Inc.</td><td class="patent-data-table-td ">Method and apparatus for reducing power consumption in a network device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6697927">US6697927</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 28, 2001</td><td class="patent-data-table-td patent-date-value">Feb 24, 2004</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">Concurrent non-blocking FIFO array</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6802057">US6802057</a></td><td class="patent-data-table-td patent-date-value">May 3, 2000</td><td class="patent-data-table-td patent-date-value">Oct 5, 2004</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Automatic generation of fortran 90 interfaces to fortran 77 code</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6868476">US6868476</a></td><td class="patent-data-table-td patent-date-value">Aug 5, 2002</td><td class="patent-data-table-td patent-date-value">Mar 15, 2005</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Software controlled content addressable memory in a general purpose execution datapath</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6910107">US6910107</a></td><td class="patent-data-table-td patent-date-value">Aug 23, 2000</td><td class="patent-data-table-td patent-date-value">Jun 21, 2005</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Method and apparatus for invalidation of data in computer systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6957208">US6957208</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 2000</td><td class="patent-data-table-td patent-date-value">Oct 18, 2005</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Method, apparatus, and article of manufacture for performance analysis using semantic knowledge</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6986130">US6986130</a></td><td class="patent-data-table-td patent-date-value">Jul 28, 2000</td><td class="patent-data-table-td patent-date-value">Jan 10, 2006</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Methods and apparatus for compiling computer programs using partial function inlining</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7035989">US7035989</a></td><td class="patent-data-table-td patent-date-value">Feb 16, 2000</td><td class="patent-data-table-td patent-date-value">Apr 25, 2006</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Adaptive memory allocation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7096292">US7096292</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 28, 2002</td><td class="patent-data-table-td patent-date-value">Aug 22, 2006</td><td class="patent-data-table-td ">Cavium Acquisition Corp.</td><td class="patent-data-table-td ">On-chip inter-subsystem communication</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7213099">US7213099</a></td><td class="patent-data-table-td patent-date-value">Dec 30, 2003</td><td class="patent-data-table-td patent-date-value">May 1, 2007</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Method and apparatus utilizing non-uniformly distributed DRAM configurations and to detect in-range memory address matches</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7233598">US7233598</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 5, 2002</td><td class="patent-data-table-td patent-date-value">Jun 19, 2007</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">System and method for speculatively issuing memory requests while maintaining a specified packet order</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7240169">US7240169</a></td><td class="patent-data-table-td patent-date-value">Jan 18, 2002</td><td class="patent-data-table-td patent-date-value">Jul 3, 2007</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Protocol for coordinating the distribution of shared memory</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7243179">US7243179</a></td><td class="patent-data-table-td patent-date-value">Jun 6, 2006</td><td class="patent-data-table-td patent-date-value">Jul 10, 2007</td><td class="patent-data-table-td ">Cavium Networks, Inc.</td><td class="patent-data-table-td ">On-chip inter-subsystem communication</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7313658">US7313658</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 13, 2005</td><td class="patent-data-table-td patent-date-value">Dec 25, 2007</td><td class="patent-data-table-td ">Via Technologies, Inc.</td><td class="patent-data-table-td ">Microprocessor and method for utilizing disparity between bus clock and core clock frequencies to prioritize cache line fill bus access requests</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7386692">US7386692</a></td><td class="patent-data-table-td patent-date-value">Aug 20, 2004</td><td class="patent-data-table-td patent-date-value">Jun 10, 2008</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Method and apparatus for quantized deadline I/O scheduling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7406681">US7406681</a></td><td class="patent-data-table-td patent-date-value">Oct 12, 2000</td><td class="patent-data-table-td patent-date-value">Jul 29, 2008</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Automatic conversion of source code from 32-bit to 64-bit</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7436954">US7436954</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 2002</td><td class="patent-data-table-td patent-date-value">Oct 14, 2008</td><td class="patent-data-table-td ">Cavium Networks, Inc.</td><td class="patent-data-table-td ">Security system with an intelligent DMA controller</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7478179">US7478179</a></td><td class="patent-data-table-td patent-date-value">May 12, 2006</td><td class="patent-data-table-td patent-date-value">Jan 13, 2009</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Input/output priority inheritance wherein first I/O request is executed based on higher priority</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7653763">US7653763</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 2002</td><td class="patent-data-table-td patent-date-value">Jan 26, 2010</td><td class="patent-data-table-td ">Cavium Networks, Inc.</td><td class="patent-data-table-td ">Subsystem boot and peripheral data transfer architecture for a subsystem of a system-on- chip</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7657671">US7657671</a></td><td class="patent-data-table-td patent-date-value">Apr 19, 2006</td><td class="patent-data-table-td patent-date-value">Feb 2, 2010</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Adaptive resilvering I/O scheduling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7689814">US7689814</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 20, 2004</td><td class="patent-data-table-td patent-date-value">Mar 30, 2010</td><td class="patent-data-table-td ">Sony Computer Entertainment Inc.</td><td class="patent-data-table-td ">Methods and apparatus for disabling error countermeasures in a processing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7730287">US7730287</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 27, 2007</td><td class="patent-data-table-td patent-date-value">Jun 1, 2010</td><td class="patent-data-table-td ">Microunity Systems Engineering, Inc.</td><td class="patent-data-table-td ">Method and software for group floating-point arithmetic operations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8117426">US8117426</a></td><td class="patent-data-table-td patent-date-value">Jul 27, 2007</td><td class="patent-data-table-td patent-date-value">Feb 14, 2012</td><td class="patent-data-table-td ">Microunity Systems Engineering, Inc</td><td class="patent-data-table-td ">System and apparatus for group floating-point arithmetic operations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8151008">US8151008</a></td><td class="patent-data-table-td patent-date-value">Jul 2, 2008</td><td class="patent-data-table-td patent-date-value">Apr 3, 2012</td><td class="patent-data-table-td ">Cradle Ip, Llc</td><td class="patent-data-table-td ">Method and system for performing DMA in a multi-core system-on-chip using deadline-based scheduling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20140195699">US20140195699</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 8, 2013</td><td class="patent-data-table-td patent-date-value">Jul 10, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Maintaining i/o priority and i/o sorting</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2003019399A1?cl=en">WO2003019399A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 27, 2002</td><td class="patent-data-table-td patent-date-value">Mar 6, 2003</td><td class="patent-data-table-td ">Intel Corp</td><td class="patent-data-table-td ">A multiprocessor infrastructure for providing flexible bandwidth allocation via multiple instantiations of separate data buses, control buses and support mechanisms</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=1MtHBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc710/defs710.htm&usg=AFQjCNEn5ByvpUOuGF3R-upj_HDvCv7LsQ#C710S052000">710/52</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=1MtHBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc710/defs710.htm&usg=AFQjCNEn5ByvpUOuGF3R-upj_HDvCv7LsQ#C710S020000">710/20</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=1MtHBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc710/defs710.htm&usg=AFQjCNEn5ByvpUOuGF3R-upj_HDvCv7LsQ#C710S017000">710/17</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=1MtHBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc710/defs710.htm&usg=AFQjCNEn5ByvpUOuGF3R-upj_HDvCv7LsQ#C710S039000">710/39</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=1MtHBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc710/defs710.htm&usg=AFQjCNEn5ByvpUOuGF3R-upj_HDvCv7LsQ#C710S018000">710/18</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=1MtHBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc710/defs710.htm&usg=AFQjCNEn5ByvpUOuGF3R-upj_HDvCv7LsQ#C710S040000">710/40</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=1MtHBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc710/defs710.htm&usg=AFQjCNEn5ByvpUOuGF3R-upj_HDvCv7LsQ#C710S056000">710/56</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=1MtHBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0013420000">G06F13/42</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=1MtHBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F13/4243">G06F13/4243</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06F13/42C3S</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Mar 22, 2010</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 10, 2009</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 1-34 IS CONFIRMED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 13, 2007</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20061027</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 21, 2006</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 7, 2002</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 7, 1995</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">MICROUNITY SYSTEMS ENGINEERING, INC., CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:ZURAVLEFF, WILLIAM K.;SEMMELMEYER, MARK;ROBINSON, TIMOTHY;AND OTHERS;REEL/FRAME:007546/0810;SIGNING DATES FROM 19950601 TO 19950607</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U2_ijvqqnsZBdJ5NFQl9aCiUCNftQ\u0026id=1MtHBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U3WP3-GAdR0soDpdlaFJTf691yKog\u0026id=1MtHBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U1MxXTMKNiLwfuS6NqHfwgI3BD25Q","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Non_blocking_load_buffer_and_a_multiple.pdf?id=1MtHBAABERAJ\u0026output=pdf\u0026sig=ACfU3U2Lpk2nA7mzIucn11DkbhV0ewoWhg"},"sample_url":"http://www.google.com/patents/reader?id=1MtHBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>