<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US5799279 - Continuous speech recognition of text and commands - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Continuous speech recognition of text and commands"><meta name="DC.contributor" content="Joel M. Gould" scheme="inventor"><meta name="DC.contributor" content="Jonathan H. Young" scheme="inventor"><meta name="DC.contributor" content="Dragon Systems, Inc." scheme="assignee"><meta name="DC.date" content="1995-11-13" scheme="dateSubmitted"><meta name="DC.description" content="In a method for use in recognizing continuous speech, signals are accepted corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements corresponding to commands to be executed. The elements are recognized. The recognized elements are acted on in a manner which depends on whether they represent text or commands."><meta name="DC.date" content="1998-8-25" scheme="issued"><meta name="DC.relation" content="US:4376874" scheme="references"><meta name="DC.relation" content="US:4618984" scheme="references"><meta name="DC.relation" content="US:4624008" scheme="references"><meta name="DC.relation" content="US:4677569" scheme="references"><meta name="DC.relation" content="US:4688195" scheme="references"><meta name="DC.relation" content="US:4776016" scheme="references"><meta name="DC.relation" content="US:4783803" scheme="references"><meta name="DC.relation" content="US:4827520" scheme="references"><meta name="DC.relation" content="US:4829576" scheme="references"><meta name="DC.relation" content="US:4833712" scheme="references"><meta name="DC.relation" content="US:4866778" scheme="references"><meta name="DC.relation" content="US:4914704" scheme="references"><meta name="DC.relation" content="US:4931950" scheme="references"><meta name="DC.relation" content="US:4962535" scheme="references"><meta name="DC.relation" content="US:4980918" scheme="references"><meta name="DC.relation" content="US:4984177" scheme="references"><meta name="DC.relation" content="US:5027406" scheme="references"><meta name="DC.relation" content="US:5031217" scheme="references"><meta name="DC.relation" content="US:5033087" scheme="references"><meta name="DC.relation" content="US:5036538" scheme="references"><meta name="DC.relation" content="US:5086472" scheme="references"><meta name="DC.relation" content="US:5095508" scheme="references"><meta name="DC.relation" content="US:5127055" scheme="references"><meta name="DC.relation" content="US:5202952" scheme="references"><meta name="DC.relation" content="US:5231670" scheme="references"><meta name="DC.relation" content="US:5377303" scheme="references"><meta name="DC.relation" content="US:5425129" scheme="references"><meta name="DC.relation" content="US:5428707" scheme="references"><meta name="DC.relation" content="US:5526463" scheme="references"><meta name="citation_reference" content="Dale Evans, &quot;Talking to the Bug,&quot; Microcad News, pp. 58-61, Mar. 1989."><meta name="citation_reference" content="Dale Evans, Talking to the Bug, Microcad News, pp. 58 61, Mar. 1989."><meta name="citation_reference" content="Mandel, Mark A. et al., &quot;A Commercial Large-Vocabulary Discrete Speech Recognition System: DragonDictate,&quot; Language and Speech, vol. 35 (1, 2) (1992), pp. 237-246."><meta name="citation_reference" content="Mandel, Mark A. et al., A Commercial Large Vocabulary Discrete Speech Recognition System: DragonDictate, Language and Speech, vol. 35 (1, 2) (1992), pp. 237 246."><meta name="citation_reference" content="U. S. Patent Application, &quot;Apparatuses and Methods for Training and Operating Speech Recognition Systems&quot; Joel M. Gould et al., filed Feb. 1, 1995."><meta name="citation_reference" content="U. S. Patent Application, &quot;Speech Recognition,&quot; Serial No. 08/521,543, Gregory J. Gadbois, filed Aug. 30, 1995."><meta name="citation_reference" content="U. S. Patent Application, &quot;Speech Recognition,&quot; Serial No. 08/559,190, Gregory J. Gadbois, filed Nov. 13, 1995."><meta name="citation_reference" content="U. S. Patent Application, Apparatuses and Methods for Training and Operating Speech Recognition Systems Joel M. Gould et al., filed Feb. 1, 1995."><meta name="citation_reference" content="U. S. Patent Application, Speech Recognition, Serial No. 08/521,543, Gregory J. Gadbois, filed Aug. 30, 1995."><meta name="citation_reference" content="U. S. Patent Application, Speech Recognition, Serial No. 08/559,190, Gregory J. Gadbois, filed Nov. 13, 1995."><meta name="citation_reference" content="U.S. Patent Application &quot;Continuous Speech Recognition of Text and Commands,&quot; Serial No. 08/559,207, Joel M. Gould et al., filed Nov. 13, 1995."><meta name="citation_reference" content="U.S. Patent Application Continuous Speech Recognition of Text and Commands, Serial No. 08/559,207, Joel M. Gould et al., filed Nov. 13, 1995."><meta name="citation_patent_number" content="US:5799279"><meta name="citation_patent_application_number" content="US:08/559,207"><link rel="canonical" href="http://www.google.com/patents/US5799279"/><meta property="og:url" content="http://www.google.com/patents/US5799279"/><meta name="title" content="Patent US5799279 - Continuous speech recognition of text and commands"/><meta name="description" content="In a method for use in recognizing continuous speech, signals are accepted corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements corresponding to commands to be executed. The elements are recognized. The recognized elements are acted on in a manner which depends on whether they represent text or commands."/><meta property="og:title" content="Patent US5799279 - Continuous speech recognition of text and commands"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("RojtU8ndIZGhsQTo_4DIAg"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("FRA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("RojtU8ndIZGhsQTo_4DIAg"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("FRA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us5799279?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US5799279"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=OH9HBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS5799279&amp;usg=AFQjCNFewON2JphbWbdWlvCFWsdx0USovQ" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US5799279.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US5799279.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US5799279" style="display:none"><span itemprop="description">In a method for use in recognizing continuous speech, signals are accepted corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements corresponding to commands to be executed. The elements are recognized. The recognized elements are...</span><span itemprop="url">http://www.google.com/patents/US5799279?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US5799279 - Continuous speech recognition of text and commands</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US5799279 - Continuous speech recognition of text and commands" title="Patent US5799279 - Continuous speech recognition of text and commands"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US5799279 A</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 08/559,207</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Aug 25, 1998</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Nov 13, 1995</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Nov 13, 1995</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/DE69634239D1">DE69634239D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE69634239T2">DE69634239T2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0785540A2">EP0785540A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0785540A3">EP0785540A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0785540B1">EP0785540B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6088671">US6088671</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">08559207, </span><span class="patent-bibdata-value">559207, </span><span class="patent-bibdata-value">US 5799279 A, </span><span class="patent-bibdata-value">US 5799279A, </span><span class="patent-bibdata-value">US-A-5799279, </span><span class="patent-bibdata-value">US5799279 A, </span><span class="patent-bibdata-value">US5799279A</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Joel+M.+Gould%22">Joel M. Gould</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Jonathan+H.+Young%22">Jonathan H. Young</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Dragon+Systems,+Inc.%22">Dragon Systems, Inc.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US5799279.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5799279.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5799279.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (29),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (12),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (65),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (8),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (13)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=OH9HBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/5799279&usg=AFQjCNGMS02J5yueBJPEuDNsiAU1Nf581Q">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=OH9HBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D5799279&usg=AFQjCNGhHlrXbpRALcOLkqnboLxSTxzZsg">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=OH9HBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D5799279A%26KC%3DA%26FT%3DD&usg=AFQjCNF0AQQhf9I1PZq-9BFcn5x3sKsg5w">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54330237" lang="EN" load-source="patent-office">Continuous speech recognition of text and commands</invention-title></span><br><span class="patent-number">US 5799279 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA37805784" lang="EN" load-source="patent-office"> <div class="abstract">In a method for use in recognizing continuous speech, signals are accepted corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements corresponding to commands to be executed. The elements are recognized. The recognized elements are acted on in a manner which depends on whether they represent text or commands.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(20)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-1.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-1.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-2.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-2.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-3.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-3.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-4.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-4.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-5.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-5.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-6.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-6.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-7.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-7.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-8.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-8.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-9.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-9.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-10.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-10.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-11.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-11.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-12.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-12.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-13.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-13.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-14.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-14.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-15.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-15.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-16.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-16.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-17.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-17.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-18.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-18.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-19.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-19.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5799279-20.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5799279-20.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(22)</span></span></div><div class="patent-text"><div mxw-id="PCLM5273028" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is:</claim-statement> <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1. A method for use in recognizing continuous speech comprising<div class="claim-text">accepting signals corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements corresponding to commands to be executed, wherein a particular one of the speech elements may correspond to a text element in one context and to a command element in another context,</div> <div class="claim-text">recognizing the speech elements,</div> <div class="claim-text">when a recognized one of the speech elements may be either a command element or a text element, designating the recognized speech element as corresponding to a text element or to a command element based on a context in which the recognized speech element appears, and</div> <div class="claim-text">acting on the recognized speech elements in a manner which depends on whether the speech elements correspond to text elements or command elements.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2. The method of claim 1 in which a text element is acted on by providing the text element to a text processing application.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3. The method of claim 1 in which a command element acted upon by causing an application to perform a step.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4. The method of claim 1 in which the designating is based on natural characteristics of spoken text versus spoken commands.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5. The method of claim 1 in which the designating includes evaluating a likelihood that the recognized speech element is either a command element or a text element.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6. The method of claim 1 further including biasing the designating in favor of the recognized speech element being a text element or a command element.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7. The method of claim 6 in which the biasing includes determining if the recognized speech element reflects a command reject.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8. The method of claim 6 in which the biasing includes determining if the recognized speech element conforms to a command template.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9. The method of claim 6 in which the biasing includes comparing recognition scores of the recognized speech element as a command or as text.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" class="claim">
      <div class="claim-text">10. The method of claim 6 in which the biasing includes determining the length of silence between successive ones of the speech elements.</div>
    </div>
    </div> <div class="claim"> <div num="11" class="claim">
      <div class="claim-text">11. The method of 6 in which the biasing includes determining whether actions of the user imply that the recognized speech element cannot be text.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" class="claim">
      <div class="claim-text">12. The method of claim 1 in which the recognizing comprises, in parallel<div class="claim-text">recognizing the speech elements as if they were text, and</div> <div class="claim-text">recognizing the speech elements as if they were commands.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" class="claim">
      <div class="claim-text">13. The method of claim 12 further comprising temporarily stopping the recognizing of speech elements as if they were text (or commands) upon designating a speech element as being a command element (or a text element).</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" class="claim">
      <div class="claim-text">14. The method of claim 1 further comprising displaying to a user the results of the recognition.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" class="claim">
      <div class="claim-text">15. The method of claim 14 wherein the results are partial results.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" class="claim">
      <div class="claim-text">16. The method of claim 1 further comprising enabling the user to cause a re-recognition if a speech element is incorrectly recognized as a text element or a command element.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" class="claim">
      <div class="claim-text">17. The method of claim 16 in which<div class="claim-text">the user may cause a rerecognition if a command element is recognized as a text element, and</div> <div class="claim-text">in response to the re-recognition a text processing application may undo the inclusion of the text element in text being processed.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" class="claim">
      <div class="claim-text">18. The method of claim 1 in which prior to acting on a recognized command element, information associated with the command element is displayed to a user.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" class="claim">
      <div class="claim-text">19. The method of claim 1 further comprising<div class="claim-text">accepting from a user a direction to consider previous or subsequent speech elements as either text elements or command elements but not both.</div> </div>
    </div>
    </div> <div class="claim"> <div num="20" class="claim">
      <div class="claim-text">20. Software stored on a medium for use in recognizing speech comprising<div class="claim-text">instructions for accepting signals corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements corresponding to commands to be executed, wherein a particular one of the speech elements may correspond to a text element in one context and to a command element in another context,</div> <div class="claim-text">instructions for recognizing the speech elements,</div> <div class="claim-text">instructions for, when a recognized one of the speech element may be either a command element or a text element, designating the recognized speech element as corresponding to a text element or to a command element based on a context in which the speech element appears, and</div> <div class="claim-text">instructions for acting on the recognized speech elements in a manner which depends on whether the speech elements correspond to text elements or command elements.</div> </div>
    </div>
    </div> <div class="claim"> <div num="21" class="claim">
      <div class="claim-text">21. A method for use in recognizing speech comprising<div class="claim-text">accepting signals corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements corresponding to commands to be executed, wherein a particular one of the speech elements may correspond to a text element in one context and to a command element in another context,</div> <div class="claim-text">recognizing the speech elements,</div> <div class="claim-text">when a recognized one of the speech elements may be either a command element or a text element, biasing the recognizing in favor of the given element being text or a command,</div> <div class="claim-text">acting on the text by providing it to a text processing application,</div> <div class="claim-text">acting on the command by causing an application to perform a step, and</div> <div class="claim-text">displaying to a user the results of the recognition.</div> </div>
    </div>
    </div> <div class="claim"> <div num="22" class="claim">
      <div class="claim-text">22. A method for use in recognizing continuous speech comprising<div class="claim-text">accepting signals corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements corresponding to commands to be executed, wherein a particular one of the speech elements may correspond to a text element in one context and to a command element in another context;</div> <div class="claim-text">recognizing the speech elements;</div> <div class="claim-text">when a recognized one of the speech elements may be either a command element or a text element, designating the recognized speech element as corresponding to a text element or to a command element based on a context in which the recognized speech element appears, the designating including evaluating a likelihood that the recognized speech element is either a command element or a text element;</div> <div class="claim-text">acting on a recognized speech element that corresponds to a text element by providing the text element to a text processing application;</div> <div class="claim-text">acting on a recognized speech element that corresponds to a command element by causing an application to perform a step;</div> <div class="claim-text">displaying to a user results of the recognition; and</div> <div class="claim-text">enabling the user to cause a re-recognition if a speech element is incorrectly recognized as a text element or a command element.</div> </div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES67164965" lang="EN" load-source="patent-office" class="description">
    <heading>BACKGROUND</heading> <p>This invention relates to continuous speech recognition.</p>
    <p>Many speech recognition systems recognize spoken text in one mode and spoken commands in another mode. In one example, the dictation mode requires discrete speech while the command mode may be handled by continuous/discrete speech. In dictation mode, a user's discrete speech is recognized as, e.g., English words, and the recognized words are displayed to the user. The user may dictate any word that is within a vocabulary held in the system without having to follow any particular structure. This is called "free context" discrete speech. In command mode, the system recognizes either continuous or discrete speech and executes the commands. For example, if the user says "underline last three words," the system recognizes the command and then underlines the last three words that the user spoke in dictation mode. The user speaks commands as structured speech in accordance with a particular structure or template. For example, the user may say "underline last three words" but not "underline the last three words" or "please underline last three words." The user switches between command mode and dictation mode by speaking "Command Mode", double clicking on an icon representing the mode the user wants to switch into, or typing a switch mode command.</p>
    <heading>SUMMARY</heading> <p>In general, in one aspect, the invention features a method for use in recognizing continuous speech. Signals are accepted corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements corresponding to commands to be executed. The elements are recognized. The recognized elements are acted on in a manner which depends on whether they represent text or commands.</p>
    <p>Implementations of the invention may include one or more of the following. The text may be acted on by providing it to a text processing application. The commands may be acted upon by causing an application to perform a step. The recognizing may be based on natural characteristics of spoken text versus spoken commands. The recognizing may include evaluating the likelihood that a given element is either a command element or a text element. The recognizing may be biased in favor of a given element being text or a command. The biasing may include determining if a given one of the elements reflects a command reject or conforms to a command template; or comparing recognition scores of the given element as a command or as text; or determining the length of silence between successive ones of the elements or whether the actions of the user imply that a given one of the elements cannot be text.</p>
    <p>The recognizing may include, in parallel, recognizing the elements as if they were text, and recognizing the elements as if they were commands. The recognizing of elements as if they were text (or commands) may be temporarily stopped upon determining that an element is a command element (or a text element). The results of the recognition may be displayed to a user. The results may be partial results. The user may be enabled to cause a re-recognition if the element is incorrectly recognized as text or a command. The user may cause a re-recognition if a command element is recognized as a text element, and in response to the re-recognition a text processing application may undo the inclusion of the text element in text being processed. Prior to acting on a recognized command element, information associated with the command element may be displayed to a user; a direction may be accepted from the user to consider previous or subsequent elements as either text or commands but not both.</p>
    <p>The advantages of the invention may include one or more of the following. Recognizing spoken commands within dictated text allows users to intermittently execute commands that affect the text (e.g., underlining or bolding particular words) without requiring the user to switch between separate command and dictation modes. Moreover, user confusion is reduced because the user is not required to remember which mode the system is in.</p>
    <p>Other advantages and features will become apparent from the following description and from the claims.</p>
    <heading>DESCRIPTION</heading> <p>FIG. 1 is a block diagram of a speech recognition system.</p>
    <p>FIG. 2 is a block diagram of speech recognition software and application software.</p>
    <p>FIG. 3 is a block diagram of speech recognition software and vocabularies stored in memory.</p>
    <p>FIG. 4 is a flow chart of recognizing both commands and dictated text.</p>
    <p>FIG. 5 is a computer screen display of word processing commands.</p>
    <p>FIG. 6 is a computer screen display of examples of word processing commands.</p>
    <p>FIG. 7 is a block diagram of word processing commands.</p>
    <p>FIGS. 8a, 8b, 9a, and 9b are computer screen displays of partial results and command execution results.</p>
    <p>FIG. 10 is a another flow chart of recognizing both commands and dictated text.</p>
    <p>FIG. 11 is a block diagram of speech recognition software and vocabularies stored in memory.</p>
    <p>FIG. 12 is a flow chart of structured continuous command speech recognition.</p>
    <p>FIG. 13 is a block diagram of spreadsheet commands.</p>
    <p>FIG. 14 is a another flow chart of recognizing both commands and dictated text.</p>
    <p>FIGS. 15a-15d are computer screen displays depicting the process of correcting a misrecognized command.</p>
    <p>The system recognizes both continuously spoken commands and continuously dictated text by taking advantage of characteristics common to the natural speech of most users. For instance, users typically pause (e.g., 0.5 sec) before and after speaking a command. Similarly, following a pause, users begin commands by speaking action verbs (e.g., underline, bold, delete) and begin dictated text by speaking nouns. To take advantage of these and other characteristics, the system expects the user to pause before and after speaking a command and to follow a particular structure or template when speaking a command (e.g., all commands begin with action verbs). These requirements improve the accuracy with which the system distinguishes between dictated text and commands.</p>
    <p>Referring to FIG. 1, a typical speech recognition system 10 includes a microphone 12 for converting a user's speech into an analog data signal 14 and a sound card 16. The sound card includes a digital signal processor (DSP) 19 and an analog-to-digital (A/D) converter 17 for converting the analog data signal into a digital data signal 18 by sampling the analog data signal at about 11 Khz to generate 220 digital samples during a 20 msec time period. Each 20 ms time period corresponds to a separate speech frame. The DSP processes the samples corresponding to each speech frame to generate a group of parameters associated with the analog data signal during the 20 ms period. Generally, the parameters represent the amplitude of the speech at each of a set of frequency bands.</p>
    <p>The DSP also monitors the volume of the speech frames to detect user utterances. If the volume of three consecutive speech frames within a window of five consecutive speech frames exceeds a predetermined speech threshold, for example, 20 dB, then the DSP determines that the analog signal represents speech and the DSP begins sending a batch of, e.g., three, speech frames of data at a time via a digital data signal 23 to a central processing unit (CPU) 20. The DSP asserts an utterance signal (Utt) 22 to notify the CPU each time a batch of speech frames representing an utterance is sent via the digital data signal.</p>
    <p>When an interrupt handler 24 on the CPU receives assertions of Utt signal 22, the CPU's normal sequence of execution is interrupted. Interrupt signal 26 causes operating system software 28 to call a store routine 29. Store routine 29 stores the incoming batch of speech frames into a buffer 30. When fourteen consecutive speech frames within a window of nineteen consecutive speech frames fall below a predetermined silence threshold, e.g., 6 dB, then the DSP stops sending speech frames to the CPU and asserts an End<sub>--</sub> Utt signal 21. The End<sub>--</sub> Utt signal causes the store routine to organize the batches of previously stored speech frames into a speech packet 39 corresponding to the user utterance.</p>
    <p>Interrupt signal 26 also causes the operating system software to call monitor software 32. Monitor software 32 keeps a count 34 of the number of speech packets stored but not yet processed. An application 36, for example, a word processor, being executed by the CPU periodically checks for user input by examining the monitor software's count. If the count is zero, then there is no user input. If the count is not zero, then the application calls speech recognizer software 38 and passes a pointer 37 to the address location of the speech packet in buffer 30. The speech recognizer may be called directly by the application or may be called on behalf of the application by a separate program, such as DragonDictate from Dragon Systems of West Newton, Mass., in response to the application's request for input from the mouse or keyboard.</p>
    <p>For a more detailed description of how user utterances are received and stored within a speech recognition system, see U.S. Pat. No. 5,027,406, entitled "Method for Interactive Speech Recognition and Training" which is incorporated by reference.</p>
    <p>Referring to FIG. 2, to determine what words have been spoken speech recognition software 38 causes the CPU to retrieve speech frames within speech packet 39 from buffer 30 and compare the speech frames to speech models stored in one or more vocabularies 40. For a more detailed description of continuous speech recognition, see U.S. Pat. No. 5,202,952, entitled "Large-Vocabulary Continuous Speech prefiltering and processing System", which is incorporated by reference.</p>
    <p>The recognition software uses common script language interpreter software to communicate with the application 36 that called the recognition software. The common script language interpreter software enables the user to dictate directly to the application either by emulating the computer keyboard and converting the recognition results into application dependent keystrokes or by sending application dependent commands directly to the application using the system's application communication mechanism (e.g., Microsoft Windows uses Dynamic Data Exchange). The desired applications include, for example, word processors 44 (e.g., Word perfect or Microsoft Word), spreadsheets 46 (e.g., Lotus 1-2-3 or Excel), and games 48 (e.g., Solitaire).</p>
    <p>As an alternative to dictating directly to an application, the user dictates text to a speech recognizer window, and after dictating a document, the user transfers the document (manually or automatically) to the application.</p>
    <p>Referring to FIG. 3, when an application first calls the speech recognition software, it is loaded from a disk drive into the computer's local memory 42. One or more vocabularies, for example, common vocabulary 48 and Microsoft Office vocabulary 50, are also loaded from remote storage into memory 42. The vocabularies 48, 52, and 54 include all the words 48b, 50b, and 54b (text and commands), and corresponding speech models 48a, 50a, and 54a, that a user may speak.</p>
    <p>Spreading the speech models and words across different vocabularies allows the speech models and words to be grouped into vendor (e.g., Microsoft and Novell) dependent vocabularies which are only loaded into memory when an application corresponding to a particular vendor is executed for the first time after power-up. For example, many of the speech models and words in the Novell Perfectoffice vocabulary 54 represent words only spoken when a user is executing a Novell PerfectOffice application, e.g., Wordperfect. As a result, these speech models and words are only needed when the user executes a Novell application. To avoid wasting valuable memory space, the Novell PerfectOffice vocabulary 54 is only loaded into memory when needed (i.e., when the user executes a Novell application).</p>
    <p>Alternatively, the speech models and words may be grouped into application dependent vocabularies. For example, separate vocabularies may exist for Microsoft Word, Microsoft Excel, and Novell Wordperfect. As another alternative, only a single vocabulary including all words, and corresponding speech models, that a user may speak is loaded into local memory and used by the speech recognition software to recognize a user's speech.</p>
    <p>Referring to FIG. 4, once the vocabularies are stored in local memory an application calls the recognition software, in one method, the CPU compares speech frames representing the user's speech to speech models in the vocabularies to recognize (step 60) the user's speech. The CPU then determines (steps 62 and 64) whether the results represent a command or text. Commands include single words and phrases and sentences that are defined by templates (i.e., restriction rules). The templates define the words that may be said within command sentences and the order in which the words are spoken. The CPU compares (step 62) the recognition results to the possible command words and phrases and to command templates, and if the results match a command word or phrase or a command template (step 64), then the CPU sends (step 65a) the application that called the speech recognition software keystrokes or scripting language that cause the application to execute the command, and if the results do not match a command word or phrase or a command template, the CPU sends (step 65b) the application keystrokes or scripting language that cause the application to type the results as text.</p>
    <p>Referring to FIG. 5, while dictating text, the user may cause the computer to display a command browser 66 by keystroke, mouse selection, or utterance (e.g., speaking the phrase "What Can I Say" 68 into the microphone). The command browser displays possible commands for the application being executed. For example, a word processing application includes single command words, e.g.,  Bold! 70 and  Center! 72, command phrases, e.g.,  Close Document! 74 and  Cut This paragraph! 76, and flexible sentence commands, e.g.,  &lt;Action&gt; &lt;2 to 20&gt; &lt;Text Objects&gt;! 78 and  Move &lt;Direction&gt; &lt;2 to 20&gt; &lt;Text Objects&gt;! 80. Referring also to FIG. 6, the user may select a command shown in the command browser to display examples 82 of the selected command 80.</p>
    <p>Referring to FIG. 7, the command sentences, e.g., 78, 80, 84, and 88, are spoken in accordance with a template and without long, e.g., greater than 0.5 second, pauses between the words of the sentence. (The length of the pause may be adjusted to compensate for a particular user's speech impediment.) For example, command 80 requires the user to speak the fixed word "Move" 88 followed by a direction variable 90 (i.e., &lt;Direction&gt;: "Up", "Down", "Left", "Right", "Back", or "Forward"), a number variable 92 (i.e., &lt;2 to 20&gt;: "2", "3", "4", . . . or "20"), and, optionally (dashed line 94), a plural text object variable 96 (i.e., &lt;Text Objects&gt;: "Characters", "Words", "Lines", "Sentences", or "paragraphs"). If the user wants to move up two lines in previously dictated text, the user says "Move Up 2 Lines". The user may not say "Move Up 2", "Please Move Up 2 Lines", or "Move Up Last 2 Lines" because this speech does not follow the template for Move command 80.</p>
    <p>Referring back to FIG. 3, in addition to including words (and phrases) and corresponding speech models, the vocabularies include application (e.g., Microsoft Word 100 and Microsoft Excel 102) dependent command sentences 48c, 50c, and 54c available to the user and application dependent groups 48d, 50d, and 54d which are pointed to by the sentences and which point to groups of variable words in the command templates.</p>
    <p>Aside from pointing to groups of variable words, the groups define the application dependent keystrokes (or scripting language) for each word that may be spoken. For example, when the user speaks a command sentence beginning with "Capitalize" while executing Microsoft Word, the action group points to the word "Capitalize" and provides the following keystrokes:</p>
    <p>{Alt+O}et{Enter}.</p>
    <p>When executing Novell Wordperfect, the action group also points to the word "Capitalize" but provides the following keystrokes:</p>
    <p>{Alt+e}vi{RightO}.</p>
    <p>Each command sentence in the loaded vocabularies 48, 50, and 54 includes pointers to the different components of the sentence. For example, command sentence 102 includes a pointer to the fixed word Move 178 (and its corresponding speech model) and pointers to the groups, e.g., &lt;Direction&gt; 120, &lt;2 to 20&gt; 122, and &lt;Text Objects&gt; 124. The groups include pointers to the words in the groups (and the corresponding speech models), e.g., direction words 126, numbers 128, and text object words 130.</p>
    <p>The pointers allow components of each sentence to be spread across several stored vocabularies and shared by the sentences of different vocabularies. For example, the command sentence 136 ( Print Pages &lt;Number/1 to 99&gt; to &lt;Number/1 to 99&gt;!, FIG. 5) is stored in both the Microsoft Office vocabulary 50 (not shown) and the Novell Perfectoffice vocabulary 54 (not shown), while the speech models and words (i.e., numbers 1 to 99) are stored in Number vocabulary 138. To allow for "cross-vocabulary recognition", the pointers in vocabularies 48, 50, and 54 reference by name the vocabulary in which the words can be found. For example, the variable words 1, 2, 3, . . . 99 may be found in the Number vocabulary (e.g., &lt;Number/1 to 99&gt;). Once the vocabularies are copied into local memory, the named references are resolved and replaced with actual address locations of the words within the local memory.</p>
    <p>Through cross-vocabulary recognition, a word may be added to a variable group of words (e.g., &lt;1 to 99&gt;) in only one vocabulary instead of to each vocabulary including the group. Additionally, the variable group of words is not repeated across several vocabularies.</p>
    <p>While a user's speech is being recognized, the CPU sends keystrokes or scripting language to the application to cause the application to display partial results (i.e., recognized words within an utterance before the entire utterance has been considered) within the document being displayed on the display screen (or in a status window on the display screen). If the CPU determines that the user's speech is text and the partial results match the final results, then the CPU is finished. However, if the CPU determines that the user's speech is text but that the partial results do not match the final results, then the CPU sends keystrokes or scripting language to the application to correct the displayed text. Similarly, if the CPU determines that the user's speech was a command, then the CPU sends keystrokes or scripting language to the application to cause the application to delete the partial results from the screen and execute the command.</p>
    <p>For example, the application being executed by the system is a meeting scheduler (FIGS. 8a, 8b, 9a, and 9b). After the system displays partial results 302 "schedule this meeting in room 507" (FIG. 8a), the system determines that the utterance was a command and removes the text from the display screen (FIG. 8b) and executes the command by scheduling 304 the meeting in room 507. Similarly, after the system displays partial results 304 "underline last three words" (FIG. 9a), the system determines that the utterance was a command and removes the text from the display screen (FIG. 9b) and executes the command by underlining 306 the last three words.</p>
    <p>The partial results allow the user to see how the recognition is proceeding. If the speech recognition is not accurate the user can stop speaking and proceed by speaking more slowly or clearly or the user or a technician can use the partial results information to diagnose speech recognition system errors.</p>
    <p>One difficulty with recognizing both commands and text against the same set (i.e., one or more) of vocabularies is that language modeling information in the vocabularies may cause the CPU to recognize a user's spoken command as text rather than as a command. Typically, the speech models for dictated words include language modeling information about the way a user naturally speaks a given language. For example, the word "bold" is generally followed by a noun, e.g., "That was a bold presentation." On the other hand, command sentences are purposefully stilted or unnatural (e.g., beginning with action verbs instead of nouns) to distinguish them from text and improve speech recognition accuracy. For example, the command "bold" is generally followed by a direction (e.g., next, last), a number (e.g., 2, 3, 4), or a text object (e.g., character, paragraph), e.g., "Bold last paragraph." When a user's speech is recognized for commands and text against the same set of vocabularies, any language modeling information in the vocabularies tends to cause the system to favor the recognition of text over commands.</p>
    <p>Referring to FIGS. 10 and 11, one alternative is to execute two serial recognitions. The CPU begins by recognizing (step 140, FIG. 10) the user's speech using one or more dictated word vocabularies 150 (FIG. 11) including words (and corresponding speech models) that a user may say while dictating text. This vocabulary includes language modeling information but not command sentences. The CPU then recognizes (step 142) the user's speech using one or more command vocabularies 152 including only command words, phrases, and sentences (and corresponding speech models and groups). Each recognition (steps 140 and 142) assigns a score to each recognition based on how closely the user's speech matches the speech models corresponding to the word or words recognized. The scores of both recognitions are then compared and the CPU determines (step 144) whether the user's speech was a command or text.</p>
    <p>For command recognition, the CPU compares the initial speech frames to only a first group of possible speech models representing the first words of commands and does not compare the initial speech frames to every speech model in the command vocabularies. As an example, the initial speech frames are not compared to the direction variables, "Up", "Down", "Left", "Right", "Back", and "Forward." Limiting the number of speech models to which the speech frames are compared reduces the time for the comparison and increases the accuracy of the command recognition.</p>
    <p>Referring also to FIG. 12, for continuous speech recognition, the recognizer engine begins in state 1 and waits 200 until the user begins speaking. As the user begins to speak, the CPU recognizes the beginning of the user's first word and pre-filters the first group of speech models for those speech models having similar sounding beginnings, e.g., "Select", "Center", "Single Space", "Set Font", "Set Size." The pre-filtered speech models provide a possible command sentence list of, for example, twenty, possible command sentences that the user may be speaking.</p>
    <p>The recognizer continues by comparing the successive speech frames to the pre-filtered speech models but not to other speech models (e.g., "Bold"). The possible command list is ranked in the order of highest to lowest probability with the command including the speech model most closely matching the speech frames being of highest probability (best candidate). As the CPU continues to compare successive speech frames to the pre-filtered speech models, the CPU actively re-ranks the command list if the probabilities change.</p>
    <p>If the CPU determines that the speech frames substantially match speech models for one or more first words in one or more commands, the CPU uses the pointer in each command to the next command component (i.e., second word) to begin comparing the successive speech frames to groups of speech models representing possible second words. For example, if the speech recognition engine recognizes the word Copy 202 as one of the twenty possible first words spoken, then the speech recognizer engine uses the references in the &lt;Action&gt; command sentences 78 (FIG. 7) to begin comparing (state 2) the successive speech frames to the speech models representing the words in the &lt;Next or previous&gt; group 204, including, "previous", "Last", "Back", "Next", and "Forward", the fixed word "Selection" 206, the &lt;2 to 20&gt; group 208, and the &lt;Text Objects&gt; group 210. The speech recognizer may also identify the beginning of the second word to pre-filter the speech models representing possible second command sentence words.</p>
    <p>Because some words take longer to speak than others, the speech recognition engine simultaneously continues to compare the successive speech frames to longer pre-filtered speech models. Thus, as the speech recognizer compares (state 2) the successive speech frames to groups of speech models representing possible second words in the command sentences starting with the word "Copy", the speech recognizer continues to compare the successive speech models to longer speech models representing the words "Capitalize" 212 and "Quote" 214. The continued comparison may cause the CPU to list one of these other possibilities as a higher probability than "Copy" 202 followed by a second command sentence word.</p>
    <p>The command sentences are similar in grammar and limited in number to reduce the amount of user confusion and to permit the user to easily memorize the possible commands. The variables (e.g., &lt;Action&gt;, &lt;Style&gt;, &lt;Next or prev&gt;) in the command sentences provide the user with a wide variety of commands without introducing a large number of individual command sentences.</p>
    <p>Additional command sentences may be generated for other types of applications. For example, FIG. 13 displays possible command sentences for spreadsheet applications (e.g., Lotus 1-2-3 and Excel). The command sentence templates are generic across spreadsheet applications. However the keystrokes from the command recognizer software to the application are application dependent (i.e., the keystrokes required by Lotus 1-2-3 may be different from the keystrokes required by Excel).</p>
    <p>The CPU favors dictated text over similarly scored commands because it is easier for the user to delete misrecognized commands that are typed into a document than it is for a user to undo text that is misrecognized and executed as a command. For example, if the user dictates "belated fall flies," and the system recognizes the text "belated fall flies" and the command "delete all files", it is easier for the user to delete the typed command "delete all files" than it is for the user to regain all deleted files.</p>
    <p>In favoring text, the system first determines if there was a command reject. Command rejects include noise picked up by the system microphone. The speech frames may be identified as noise if they match speech models corresponding to background noise, telephone ringing, or other common noises, or the user utterance may be considered a command reject if the command recognition scores below an empirically tuned threshold. The user may be given the ability to vary the threshold to provide the user with some control over the precision of spoken commands. Other command rejects include insufficient volume or excessive volume, hardware errors, or buffer overflow errors. Several command rejects may also be considered text rejects as well.</p>
    <p>In favoring text, the system next determines if the user's speech conformed to a command template. User's speech that does not conform to a command template does not provide a valid recognized command. A user's speech does not conform to a template if the user does not speak permitted words in the predetermined order or if the user inserts pauses between the words of a command. For example, if the user says "bold last 3 (pause) words", the words "bold last 3" are considered one utterance while the word "words" is considered another utterance. Neither utterance conforms to a command template, and, thus, neither utterance provides a valid recognized command result.</p>
    <p>The system also favors text by comparing the recognition score of the command against the recognition score of the text. If the text score is higher, then the system recognizes the user's speech as text. If the text and command scores are equal or the command score is within an empirically tuned range of the text score, then the system favors text by recognizing the user's speech as text. For example, the empirically tuned range may be determined by multiplying the number of words in the utterance by an empirically determined number, e.g., 100. However, because the commands are stilted or unnatural, the recognition score of a correctly spoken command will generally greatly out score the text recognition.</p>
    <p>Dictated text is not favored where the user cannot be dictating text. For instance, if the user has pulled down a window menu, then a user's spoken utterance can only be a command and thus, command recognition is favored or only command recognition is executed.</p>
    <p>If the CPU determines that the user's speech is text, the CPU sends (step 146) keystrokes or scripting language representing the recognized words to the application that called the speech recognition software. If the CPU determines that the user's speech is a command, the CPU sends (step 148) keystrokes or scripting language commands to the application to cause the application to execute the command.</p>
    <p>Recognizing dictated text takes longer (e.g., 1.2 real time) than the recognizing commands (e.g., 0.1 real time). One reason for the increase in time is that the dictated text vocabulary is much larger than the command vocabulary. Recognizing the dictated text before recognizing commands takes advantage of the speaking time required by the user.</p>
    <p>Because the dictated text and command vocabularies are separate, they may be optimized for their respective purposes without reducing the accuracy of either recognition. As discussed, the dictated text vocabulary may include language modeling information. Similarly, the command vocabulary may include modeling information that optimizes command recognition. For instance, the word "sentence" may have a higher probability (i.e., receive a higher score) than the word "character".</p>
    <p>Another alternative is parallel speech recognition of both dictated text and commands. Referring to FIG. 14, the CPU simultaneously recognizes (steps 160 and 162) dictated text and commands by simultaneously comparing the speech frames of the user utterance against one or more dictated text vocabularies 150 and one or more command vocabularies 152. The CPU then compares (step 164) the results of both recognitions and determines (step 166) if the user utterance is a command or text. Again, the CPU favors text recognition over command recognition. If the CPU determines that the user utterance is a command, then the CPU sends (step 168) keystrokes or scripting language to the application that called the speech recognition software to cause the application to execute the recognized command. If the CPU determines that the user utterance is dictated text, then the CPU sends (step 170) keystrokes or scripting language to the application to cause the application to type the recognized text.</p>
    <p>If the first word of a user utterance is recognized as a first word of a command sentence, then the CPU may stop the dictated text recognition and complete only the command recognition. Similarly, if the first word of a user utterance is not recognized as a first word of a command sentence, then the CPU may stop the command recognition and complete only the dictated text recognition. Additional speech recognition optimizations and optimizations for distinguishing text from commands are also possible.</p>
    <heading>Command Correction</heading> <p>Referring to FIGS. 15a-15c, if the speech recognition system incorrectly recognizes a spoken command 310 as dictated text, the user may cause (by keystroke, mouse selection, or spoken command, e.g., "That was a command" 312, FIG. 15b) the CPU to re-execute the speech recognition software. The CPU then re-recognizes the user's previous utterance and generates keystrokes or scripting language commands to cause the application that called the speech recognition software to delete the previously typed text (FIG. 15c). Where a separate command vocabulary is available, the re-recognition is executed only against this vocabulary to increase the likelihood that the spoken command is correctly recognized. If the re-recognition provides a command result with a score that exceeds the empirically tuned threshold then the CPU generates keystrokes or scripting language commands that cause the application to execute the command (e.g., underlined text 314, FIG. 15c).</p>
    <p>Referring to FIG. 15d, if the re-recognition does not provide a command result or the score of the command result is insufficient, then the CPU displays the re-recognized text 310 within a command window 220 on the system's display screen. Alternatively, the command window is displayed each time the user selects re-recognition or, for each selection of re-recognition, the user determines when the command window is displayed. As previously described, there are many reasons why a command may be incorrectly recognized. For example, if the user does not speak a command in accordance with a command template, then the CPU cannot recognize the user's speech as a command. Similarly, if the user's environment is especially noisy or the user speaks too quickly or unclearly, then the CPU may not recognize the user's speech as a command. Displaying the re-recognized speech to the user allows the user to detect their own mistakes as well as environmental problems. This information may also be used to diagnose system problems.</p>
    <p>Recognition of a "dangerous" (i.e., difficult to undo) command may also cause the CPU to display the re-recognized command within the command window. For example, if the CPU recognizes the command "Delete all files", before executing this "dangerous" command, the CPU displays the command for the user. The CPU may also display low scoring commands for the user. If the user agrees that the displayed command in the command window is the command the user wants to execute, then the user requests (by keystroke, mouse selection, or utterance "OK") the execution of the command. If the user does not agree with the displayed command or the displayed text does not match a valid command, then the user may edit the previously spoken command, for example, by typing the correct command in the command window or by saying "Edit" followed again by the intended spoken command. The system then executes and recognized valid commands or again displays any recognized speech that does not conform to a command template.</p>
    <p>To avoid misrecognizing commands, the user may notify the system ahead of time that the user is going to speak a command. For example, the user may say "Simon Says" (or another unusual phrase) before speaking a command or hold in the control key when speaking a command. When the system recognizes "Simon Says" it does not type it as text but uses it as a notification that the next utterance is or the following words in the same utterance are a command. The command notification may be used to prevent the CPU from choosing recognized dictated text as the result or to compare the utterance only to a command vocabulary (where available) to further improve speech recognition accuracy. providing a command notification is particularly useful when the user is going to speak a command that the system regularly misrecognizes as text. For other easily recognized commands, the user may choose not to provide the notification.</p>
    <p>Instead of notifying the system that the user is going to speak a command, the system may be notified that the user is going to dictate text.</p>
    <p>Additionally, if the speech recognition system incorrectly recognizes dictated text as a command, the user may cause (by keystroke, mouse selection, or spoken command, e.g., "Type That") the CPU to re-execute the speech recognition software.</p>
    <p>Other embodiments are within the scope of the following claims.</p>
    <p>For example, instead of having a digital signal processor (DSP) process the samples corresponding to each speech frame to generate a group of parameters associated with the analog data signal during each 20 ms time period, the CPU includes front-end processing software that allows the CPU to generate the parameters.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4376874">US4376874</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 15, 1980</td><td class="patent-data-table-td patent-date-value">Mar 15, 1983</td><td class="patent-data-table-td ">Sperry Corporation</td><td class="patent-data-table-td ">Real time speech compaction/relay with silence detection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4618984">US4618984</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 8, 1983</td><td class="patent-data-table-td patent-date-value">Oct 21, 1986</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Adaptive automatic discrete utterance recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4624008">US4624008</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 9, 1983</td><td class="patent-data-table-td patent-date-value">Nov 18, 1986</td><td class="patent-data-table-td ">International Telephone And Telegraph Corporation</td><td class="patent-data-table-td ">Apparatus for automatic speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4677569">US4677569</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 27, 1983</td><td class="patent-data-table-td patent-date-value">Jun 30, 1987</td><td class="patent-data-table-td ">Casio Computer Co., Ltd.</td><td class="patent-data-table-td ">Computer controlled by voice input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4688195">US4688195</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 28, 1983</td><td class="patent-data-table-td patent-date-value">Aug 18, 1987</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Natural-language interface generating system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4776016">US4776016</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 21, 1985</td><td class="patent-data-table-td patent-date-value">Oct 4, 1988</td><td class="patent-data-table-td ">Position Orientation Systems, Inc.</td><td class="patent-data-table-td ">Voice control system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4783803">US4783803</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 12, 1985</td><td class="patent-data-table-td patent-date-value">Nov 8, 1988</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Speech recognition apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4827520">US4827520</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 16, 1987</td><td class="patent-data-table-td patent-date-value">May 2, 1989</td><td class="patent-data-table-td ">Prince Corporation</td><td class="patent-data-table-td ">Voice actuated control system for use in a vehicle</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4829576">US4829576</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 21, 1986</td><td class="patent-data-table-td patent-date-value">May 9, 1989</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Voice recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4833712">US4833712</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 29, 1985</td><td class="patent-data-table-td patent-date-value">May 23, 1989</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Automatic generation of simple Markov model stunted baseforms for words in a vocabulary</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4866778">US4866778</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 11, 1986</td><td class="patent-data-table-td patent-date-value">Sep 12, 1989</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Interactive speech recognition apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4914704">US4914704</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 30, 1984</td><td class="patent-data-table-td patent-date-value">Apr 3, 1990</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Text editor for speech input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4931950">US4931950</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 25, 1988</td><td class="patent-data-table-td patent-date-value">Jun 5, 1990</td><td class="patent-data-table-td ">Electric Power Research Institute</td><td class="patent-data-table-td ">Multimedia interface and method for computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4962535">US4962535</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 12, 1990</td><td class="patent-data-table-td patent-date-value">Oct 9, 1990</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Antitumor, anticarcinogenic agents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4980918">US4980918</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 9, 1985</td><td class="patent-data-table-td patent-date-value">Dec 25, 1990</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Speech recognition system with efficient storage and rapid assembly of phonological graphs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4984177">US4984177</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 1, 1989</td><td class="patent-data-table-td patent-date-value">Jan 8, 1991</td><td class="patent-data-table-td ">Advanced Products And Technologies, Inc.</td><td class="patent-data-table-td ">Voice language translator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5027406">US5027406</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 6, 1988</td><td class="patent-data-table-td patent-date-value">Jun 25, 1991</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Method for interactive speech recognition and training</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5031217">US5031217</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 21, 1989</td><td class="patent-data-table-td patent-date-value">Jul 9, 1991</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Speech recognition system using Markov models having independent label output sets</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5033087">US5033087</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 14, 1989</td><td class="patent-data-table-td patent-date-value">Jul 16, 1991</td><td class="patent-data-table-td ">International Business Machines Corp.</td><td class="patent-data-table-td ">Method and apparatus for the automatic determination of phonological rules as for a continuous speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5036538">US5036538</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 22, 1989</td><td class="patent-data-table-td patent-date-value">Jul 30, 1991</td><td class="patent-data-table-td ">Telephonics Corporation</td><td class="patent-data-table-td ">Multi-station voice recognition and processing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5086472">US5086472</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 12, 1990</td><td class="patent-data-table-td patent-date-value">Feb 4, 1992</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Continuous speech recognition apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5095508">US5095508</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 16, 1990</td><td class="patent-data-table-td patent-date-value">Mar 10, 1992</td><td class="patent-data-table-td ">Ricoh Company, Ltd.</td><td class="patent-data-table-td ">Identification of voice pattern</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5127055">US5127055</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 11, 1991</td><td class="patent-data-table-td patent-date-value">Jun 30, 1992</td><td class="patent-data-table-td ">Kurzweil Applied Intelligence, Inc.</td><td class="patent-data-table-td ">Speech recognition apparatus &amp; method having dynamic reference pattern adaptation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5202952">US5202952</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 22, 1990</td><td class="patent-data-table-td patent-date-value">Apr 13, 1993</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Large-vocabulary continuous speech prefiltering and processing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5231670">US5231670</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 19, 1992</td><td class="patent-data-table-td patent-date-value">Jul 27, 1993</td><td class="patent-data-table-td ">Kurzweil Applied Intelligence, Inc.</td><td class="patent-data-table-td ">Voice controlled system and method for generating text from a voice controlled input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5377303">US5377303</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 9, 1993</td><td class="patent-data-table-td patent-date-value">Dec 27, 1994</td><td class="patent-data-table-td ">Articulate Systems, Inc.</td><td class="patent-data-table-td ">Enable voice utterances to control window elements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5425129">US5425129</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 29, 1992</td><td class="patent-data-table-td patent-date-value">Jun 13, 1995</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method for word spotting in continuous speech</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5428707">US5428707</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 13, 1992</td><td class="patent-data-table-td patent-date-value">Jun 27, 1995</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Apparatus and methods for training speech recognition systems and their users and otherwise improving speech recognition performance</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5526463">US5526463</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 9, 1993</td><td class="patent-data-table-td patent-date-value">Jun 11, 1996</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">System for processing a succession of utterances spoken in continuous or discrete form</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Dale Evans, "<a href='http://scholar.google.com/scholar?q="Talking+to+the+Bug%2C"'>Talking to the Bug,</a>" Microcad News, pp. 58-61, Mar. 1989.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Dale Evans, Talking to the Bug, Microcad News, pp. 58 61, Mar. 1989.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mandel, Mark A. et al., "<a href='http://scholar.google.com/scholar?q="A+Commercial+Large-Vocabulary+Discrete+Speech+Recognition+System%3A+DragonDictate%2C"'>A Commercial Large-Vocabulary Discrete Speech Recognition System: DragonDictate,</a>" Language and Speech, vol. 35 (1, 2) (1992), pp. 237-246.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Mandel, Mark A. et al., A Commercial Large Vocabulary Discrete Speech Recognition System: DragonDictate, Language and Speech, vol. 35 (1, 2) (1992), pp. 237 246.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U. S. Patent Application, "<a href='http://scholar.google.com/scholar?q="Apparatuses+and+Methods+for+Training+and+Operating+Speech+Recognition+Systems"'>Apparatuses and Methods for Training and Operating Speech Recognition Systems</a>" Joel M. Gould et al., filed Feb. 1, 1995.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U. S. Patent Application, "<a href='http://scholar.google.com/scholar?q="Speech+Recognition%2C"'>Speech Recognition,</a>" Serial No. 08/521,543, Gregory J. Gadbois, filed Aug. 30, 1995.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U. S. Patent Application, "<a href='http://scholar.google.com/scholar?q="Speech+Recognition%2C"'>Speech Recognition,</a>" Serial No. 08/559,190, Gregory J. Gadbois, filed Nov. 13, 1995.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">U. S. Patent Application, Apparatuses and Methods for Training and Operating Speech Recognition Systems Joel M. Gould et al., filed Feb. 1, 1995.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">U. S. Patent Application, Speech Recognition, Serial No. 08/521,543, Gregory J. Gadbois, filed Aug. 30, 1995.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">U. S. Patent Application, Speech Recognition, Serial No. 08/559,190, Gregory J. Gadbois, filed Nov. 13, 1995.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Patent Application "<a href='http://scholar.google.com/scholar?q="Continuous+Speech+Recognition+of+Text+and+Commands%2C"'>Continuous Speech Recognition of Text and Commands,</a>" Serial No. 08/559,207, Joel M. Gould et al., filed Nov. 13, 1995.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">U.S. Patent Application Continuous Speech Recognition of Text and Commands, Serial No. 08/559,207, Joel M. Gould et al., filed Nov. 13, 1995.</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5970451">US5970451</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 14, 1998</td><td class="patent-data-table-td patent-date-value">Oct 19, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method for correcting frequently misrecognized words or command in speech application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5983186">US5983186</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 20, 1996</td><td class="patent-data-table-td patent-date-value">Nov 9, 1999</td><td class="patent-data-table-td ">Seiko Epson Corporation</td><td class="patent-data-table-td ">Voice-activated interactive speech recognition device and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6023676">US6023676</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 12, 1996</td><td class="patent-data-table-td patent-date-value">Feb 8, 2000</td><td class="patent-data-table-td ">Dspc Israel, Ltd.</td><td class="patent-data-table-td ">Keyword recognition system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6064959">US6064959</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 28, 1997</td><td class="patent-data-table-td patent-date-value">May 16, 2000</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Error correction in speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6088671">US6088671</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 17, 1998</td><td class="patent-data-table-td patent-date-value">Jul 11, 2000</td><td class="patent-data-table-td ">Dragon Systems</td><td class="patent-data-table-td ">Continuous speech recognition of text and commands</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6195637">US6195637</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 25, 1998</td><td class="patent-data-table-td patent-date-value">Feb 27, 2001</td><td class="patent-data-table-td ">International Business Machines Corp.</td><td class="patent-data-table-td ">Marking and deferring correction of misrecognition errors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6227863">US6227863</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 18, 1998</td><td class="patent-data-table-td patent-date-value">May 8, 2001</td><td class="patent-data-table-td ">Donald Spector</td><td class="patent-data-table-td ">Phonics training computer system for teaching spelling and reading</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6233559">US6233559</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 1, 1998</td><td class="patent-data-table-td patent-date-value">May 15, 2001</td><td class="patent-data-table-td ">Motorola, Inc.</td><td class="patent-data-table-td ">Speech control of multiple applications using applets</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6292779">US6292779</a></td><td class="patent-data-table-td patent-date-value">Mar 9, 1999</td><td class="patent-data-table-td patent-date-value">Sep 18, 2001</td><td class="patent-data-table-td ">Lernout &amp; Hauspie Speech Products N.V.</td><td class="patent-data-table-td ">System and method for modeless large vocabulary speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6385585">US6385585</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 8, 2000</td><td class="patent-data-table-td patent-date-value">May 7, 2002</td><td class="patent-data-table-td ">Telefonaktiebolaget Lm Ericsson (Publ)</td><td class="patent-data-table-td ">Embedded data in a coded voice channel</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6434524">US6434524</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 5, 1999</td><td class="patent-data-table-td patent-date-value">Aug 13, 2002</td><td class="patent-data-table-td ">One Voice Technologies, Inc.</td><td class="patent-data-table-td ">Object interactive user interface using speech recognition and natural language processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6434529">US6434529</a></td><td class="patent-data-table-td patent-date-value">Feb 16, 2000</td><td class="patent-data-table-td patent-date-value">Aug 13, 2002</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">System and method for referencing object instances and invoking methods on those object instances from within a speech recognition grammar</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6466654">US6466654</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 6, 2000</td><td class="patent-data-table-td patent-date-value">Oct 15, 2002</td><td class="patent-data-table-td ">Avaya Technology Corp.</td><td class="patent-data-table-td ">Personal virtual assistant with semantic tagging</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6581033">US6581033</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 19, 1999</td><td class="patent-data-table-td patent-date-value">Jun 17, 2003</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">System and method for correction of speech recognition mode errors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6598016">US6598016</a></td><td class="patent-data-table-td patent-date-value">Oct 20, 1998</td><td class="patent-data-table-td patent-date-value">Jul 22, 2003</td><td class="patent-data-table-td ">Tele Atlas North America, Inc.</td><td class="patent-data-table-td ">System for using speech recognition with map data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6601027">US6601027</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 15, 1998</td><td class="patent-data-table-td patent-date-value">Jul 29, 2003</td><td class="patent-data-table-td ">Scansoft, Inc.</td><td class="patent-data-table-td ">Position manipulation in speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6745165">US6745165</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 16, 1999</td><td class="patent-data-table-td patent-date-value">Jun 1, 2004</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and apparatus for recognizing from here to here voice command structures in a finite grammar speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6813603">US6813603</a></td><td class="patent-data-table-td patent-date-value">Jan 26, 2000</td><td class="patent-data-table-td patent-date-value">Nov 2, 2004</td><td class="patent-data-table-td ">Korteam International, Inc.</td><td class="patent-data-table-td ">System and method for user controlled insertion of standardized text in user selected fields while dictating text entries for completing a form</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6820056">US6820056</a></td><td class="patent-data-table-td patent-date-value">Nov 21, 2000</td><td class="patent-data-table-td patent-date-value">Nov 16, 2004</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Recognizing non-verbal sound commands in an interactive computer controlled speech word recognition display system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6847931">US6847931</a></td><td class="patent-data-table-td patent-date-value">Jan 29, 2002</td><td class="patent-data-table-td patent-date-value">Jan 25, 2005</td><td class="patent-data-table-td ">Lessac Technology, Inc.</td><td class="patent-data-table-td ">Expressive parsing in computerized conversion of text to speech</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6865533">US6865533</a></td><td class="patent-data-table-td patent-date-value">Dec 31, 2002</td><td class="patent-data-table-td patent-date-value">Mar 8, 2005</td><td class="patent-data-table-td ">Lessac Technology Inc.</td><td class="patent-data-table-td ">Text to speech</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6871179">US6871179</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 7, 1999</td><td class="patent-data-table-td patent-date-value">Mar 22, 2005</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and apparatus for executing voice commands having dictation as a parameter</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6882974">US6882974</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 28, 2002</td><td class="patent-data-table-td patent-date-value">Apr 19, 2005</td><td class="patent-data-table-td ">Sap Aktiengesellschaft</td><td class="patent-data-table-td ">Voice-control for a user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6912498">US6912498</a></td><td class="patent-data-table-td patent-date-value">May 2, 2001</td><td class="patent-data-table-td patent-date-value">Jun 28, 2005</td><td class="patent-data-table-td ">Scansoft, Inc.</td><td class="patent-data-table-td ">Error correction in speech recognition by correcting text around selected area</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6915258">US6915258</a></td><td class="patent-data-table-td patent-date-value">Apr 2, 2001</td><td class="patent-data-table-td patent-date-value">Jul 5, 2005</td><td class="patent-data-table-td ">Thanassis Vasilios Kontonassios</td><td class="patent-data-table-td ">Method and apparatus for displaying and manipulating account information using the human voice</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6944593">US6944593</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 11, 2002</td><td class="patent-data-table-td patent-date-value">Sep 13, 2005</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Speech input system, speech portal server, and speech input terminal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6963841">US6963841</a></td><td class="patent-data-table-td patent-date-value">Jan 9, 2003</td><td class="patent-data-table-td patent-date-value">Nov 8, 2005</td><td class="patent-data-table-td ">Lessac Technology, Inc.</td><td class="patent-data-table-td ">Speech training method with alternative proper pronunciation database</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7058579">US7058579</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 14, 2004</td><td class="patent-data-table-td patent-date-value">Jun 6, 2006</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Speech input system, speech portal server, and speech input terminal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7085716">US7085716</a></td><td class="patent-data-table-td patent-date-value">Oct 26, 2000</td><td class="patent-data-table-td patent-date-value">Aug 1, 2006</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Speech recognition using word-in-phrase command</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7092728">US7092728</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 2, 2001</td><td class="patent-data-table-td patent-date-value">Aug 15, 2006</td><td class="patent-data-table-td ">Cisco Technology, Inc.</td><td class="patent-data-table-td ">Unified messaging system configured for converting short message service messages to audible messages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7149702">US7149702</a></td><td class="patent-data-table-td patent-date-value">Dec 31, 2001</td><td class="patent-data-table-td patent-date-value">Dec 12, 2006</td><td class="patent-data-table-td ">Bellsouth Intellectual Property Corp.</td><td class="patent-data-table-td ">System and method for document delays associated with a project</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7219137">US7219137</a></td><td class="patent-data-table-td patent-date-value">Jun 28, 2001</td><td class="patent-data-table-td patent-date-value">May 15, 2007</td><td class="patent-data-table-td ">Bellsouth Intellectual Property Corp</td><td class="patent-data-table-td ">Technician wireline and wireless intranet access via systems interface to legacy systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7233899">US7233899</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 7, 2002</td><td class="patent-data-table-td patent-date-value">Jun 19, 2007</td><td class="patent-data-table-td ">Fain Vitaliy S</td><td class="patent-data-table-td ">Speech recognition system using normalized voiced segment spectrogram analysis</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7257531">US7257531</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 17, 2003</td><td class="patent-data-table-td patent-date-value">Aug 14, 2007</td><td class="patent-data-table-td ">Medcom Information Systems, Inc.</td><td class="patent-data-table-td ">Speech to text system using controlled vocabulary indices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7280964">US7280964</a></td><td class="patent-data-table-td patent-date-value">Dec 31, 2002</td><td class="patent-data-table-td patent-date-value">Oct 9, 2007</td><td class="patent-data-table-td ">Lessac Technologies, Inc.</td><td class="patent-data-table-td ">Method of recognizing spoken language with recognition of language color</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7286994">US7286994</a></td><td class="patent-data-table-td patent-date-value">Dec 26, 2000</td><td class="patent-data-table-td patent-date-value">Oct 23, 2007</td><td class="patent-data-table-td ">At&amp;T Bls Intellectual Property, Inc.</td><td class="patent-data-table-td ">System for facilitating technician sales referrals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7299186">US7299186</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 17, 2005</td><td class="patent-data-table-td patent-date-value">Nov 20, 2007</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Speech input system, speech portal server, and speech input terminal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7315818">US7315818</a></td><td class="patent-data-table-td patent-date-value">May 11, 2005</td><td class="patent-data-table-td patent-date-value">Jan 1, 2008</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Error correction in speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7369997">US7369997</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 1, 2001</td><td class="patent-data-table-td patent-date-value">May 6, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Controlling speech recognition functionality in a computing device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7380203">US7380203</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 14, 2002</td><td class="patent-data-table-td patent-date-value">May 27, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Natural input recognition tool</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7401144">US7401144</a></td><td class="patent-data-table-td patent-date-value">Jun 28, 2001</td><td class="patent-data-table-td patent-date-value">Jul 15, 2008</td><td class="patent-data-table-td ">At&amp;T Delaware Intellectual Property, Inc.</td><td class="patent-data-table-td ">Technician intranet access via systems interface to legacy systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7409344">US7409344</a></td><td class="patent-data-table-td patent-date-value">Mar 8, 2005</td><td class="patent-data-table-td patent-date-value">Aug 5, 2008</td><td class="patent-data-table-td ">Sap Aktiengesellschaft</td><td class="patent-data-table-td ">XML based architecture for controlling user interfaces with contextual voice commands</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7606712">US7606712</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 28, 2001</td><td class="patent-data-table-td patent-date-value">Oct 20, 2009</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">Speech recognition interface for voice actuation of legacy systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7624010">US7624010</a></td><td class="patent-data-table-td patent-date-value">Jul 31, 2001</td><td class="patent-data-table-td patent-date-value">Nov 24, 2009</td><td class="patent-data-table-td ">Eliza Corporation</td><td class="patent-data-table-td ">Method of and system for improving accuracy in a speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7660754">US7660754</a></td><td class="patent-data-table-td patent-date-value">Dec 26, 2000</td><td class="patent-data-table-td patent-date-value">Feb 9, 2010</td><td class="patent-data-table-td ">At&amp;T Delaware Intellectual Property Inc.</td><td class="patent-data-table-td ">Technician communications system with automated claims processing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7672851">US7672851</a></td><td class="patent-data-table-td patent-date-value">Mar 17, 2008</td><td class="patent-data-table-td patent-date-value">Mar 2, 2010</td><td class="patent-data-table-td ">Sap Ag</td><td class="patent-data-table-td ">Enhanced application of spoken input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7822613">US7822613</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 7, 2003</td><td class="patent-data-table-td patent-date-value">Oct 26, 2010</td><td class="patent-data-table-td ">Mitsubishi Denki Kabushiki Kaisha</td><td class="patent-data-table-td ">Vehicle-mounted control apparatus and program that causes computer to execute method of providing guidance on the operation of the vehicle-mounted control apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7966176">US7966176</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 22, 2009</td><td class="patent-data-table-td patent-date-value">Jun 21, 2011</td><td class="patent-data-table-td ">At&amp;T Intellectual Property I, L.P.</td><td class="patent-data-table-td ">System and method for independently recognizing and selecting actions and objects in a speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8202094">US8202094</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 5, 2004</td><td class="patent-data-table-td patent-date-value">Jun 19, 2012</td><td class="patent-data-table-td ">Radmila Solutions, L.L.C.</td><td class="patent-data-table-td ">System and method for training users with audible answers to spoken questions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8315856">US8315856</a></td><td class="patent-data-table-td patent-date-value">Oct 23, 2008</td><td class="patent-data-table-td patent-date-value">Nov 20, 2012</td><td class="patent-data-table-td ">Red Shift Company, Llc</td><td class="patent-data-table-td ">Identify features of speech based on events in a signal representing spoken sounds</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8473295">US8473295</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 21, 2005</td><td class="patent-data-table-td patent-date-value">Jun 25, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Redictation of misrecognized words using a list of alternatives</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8478590">US8478590</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 2011</td><td class="patent-data-table-td patent-date-value">Jul 2, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Word-level correction of speech input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8494852">US8494852</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 27, 2010</td><td class="patent-data-table-td patent-date-value">Jul 23, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Word-level correction of speech input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8571860">US8571860</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 25, 2013</td><td class="patent-data-table-td patent-date-value">Oct 29, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Speech recognition with parallel recognition tasks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8656426">US8656426</a></td><td class="patent-data-table-td patent-date-value">Apr 12, 2010</td><td class="patent-data-table-td patent-date-value">Feb 18, 2014</td><td class="patent-data-table-td ">Cisco Technology Inc.</td><td class="patent-data-table-td ">Advertisement selection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8751232">US8751232</a></td><td class="patent-data-table-td patent-date-value">Feb 6, 2013</td><td class="patent-data-table-td patent-date-value">Jun 10, 2014</td><td class="patent-data-table-td ">At&amp;T Intellectual Property I, L.P.</td><td class="patent-data-table-td ">System and method for targeted tuning of a speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8755912">US8755912</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 24, 2013</td><td class="patent-data-table-td patent-date-value">Jun 17, 2014</td><td class="patent-data-table-td ">Stephen S. Miller</td><td class="patent-data-table-td ">Apparatus for remotely controlling computers and other electronic appliances/devices using a combination of voice commands and finger movements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8788271">US8788271</a></td><td class="patent-data-table-td patent-date-value">Dec 22, 2004</td><td class="patent-data-table-td patent-date-value">Jul 22, 2014</td><td class="patent-data-table-td ">Sap Aktiengesellschaft</td><td class="patent-data-table-td ">Controlling user interfaces with contextual voice commands</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110166851">US20110166851</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 27, 2010</td><td class="patent-data-table-td patent-date-value">Jul 7, 2011</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Word-Level Correction of Speech Input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120022868">US20120022868</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 2011</td><td class="patent-data-table-td patent-date-value">Jan 26, 2012</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Word-Level Correction of Speech Input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130135240">US20130135240</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 24, 2013</td><td class="patent-data-table-td patent-date-value">May 30, 2013</td><td class="patent-data-table-td ">Stephen S. Miller</td><td class="patent-data-table-td ">Apparatus for remotely controlling computers and other electronic appliances/devices using a combination of voice commands and finger movements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130138440">US20130138440</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 25, 2013</td><td class="patent-data-table-td patent-date-value">May 30, 2013</td><td class="patent-data-table-td ">Brian Strope</td><td class="patent-data-table-td ">Speech recognition with parallel recognition tasks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101971250B?cl=en">CN101971250B</a></td><td class="patent-data-table-td patent-date-value">Sep 15, 2008</td><td class="patent-data-table-td patent-date-value">May 9, 2012</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mobile electronic device with active speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2002035519A1?cl=en">WO2002035519A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 26, 2001</td><td class="patent-data-table-td patent-date-value">May 2, 2002</td><td class="patent-data-table-td ">Even Stijn Van</td><td class="patent-data-table-td ">Speech recognition using word-in-phrase command</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2003071519A2?cl=en">WO2003071519A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 14, 2003</td><td class="patent-data-table-td patent-date-value">Aug 28, 2003</td><td class="patent-data-table-td ">Frankie James</td><td class="patent-data-table-td ">Voice-controlled data entry</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=OH9HBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S275000">704/275</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OH9HBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S231000">704/231</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OH9HBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704SE15044">704/E15.044</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=OH9HBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0015260000">G10L15/26</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OH9HBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0015220000">G10L15/22</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=OH9HBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L15/22">G10L15/22</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OH9HBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L15/26">G10L15/26</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G10L15/26C</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Aug 16, 2011</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 1-3, 5-9, 11, 12, 14-18 AND 20-22 IS CONFIRMED. CLAIMS 4, 10, 13 AND 19WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 21, 2010</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100830</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 10, 2010</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 24, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG. STAMFORD BRANCH, CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:018160/0909</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20060331</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:018160/0909</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG. STAMFORD BRANCH,CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:18160/909</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 7, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG, STAMFORD BRANCH, CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:017435/0199</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20060331</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG, STAMFORD BRANCH,CONNECTICUT</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 27, 2006</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 6, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">NUANCE COMMUNICATIONS, INC., MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:SCANSOFT, INC.;REEL/FRAME:016851/0772</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20051017</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jul 3, 2002</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">L &amp; H HOLDINGS USA INC., MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:DRAGON SYSTEMS, INC.;REEL/FRAME:013056/0626</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20000607</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">L &amp; H HOLDINGS USA INC. 52 THIRD AVENUE BURLINGTON</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">L &amp; H HOLDINGS USA INC. 52 THIRD AVENUEBURLINGTON,</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:DRAGON SYSTEMS, INC. /AR;REEL/FRAME:013056/0626</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 27, 2002</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SCANSOFT, INC., MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:L &amp; H HOLDING USA, INC.;REEL/FRAME:013036/0466</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20011212</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SCANSOFT, INC. 9 CENTENNIAL DRIVE PEABODY MASSACHU</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SCANSOFT, INC. 9 CENTENNIAL DRIVEPEABODY, MASSACHU</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:L &amp; H HOLDING USA, INC. /AR;REEL/FRAME:013036/0466</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 11, 2002</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 11, 2002</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 12, 2002</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 27, 1996</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">DRAGON SYSTEMS, INC., MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:GOULD, JOEL M.;YOUNG, JONATHAN H.;REEL/FRAME:007862/0789</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19960315</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U1n5agme33TljJmbivh055LPqR2mw\u0026id=OH9HBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U0tntwTEyGMD-xqR3gr19z2P9cn4g\u0026id=OH9HBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U2oayOutoP6tYJfAKEa3DuyoShvPA","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Continuous_speech_recognition_of_text_an.pdf?id=OH9HBAABERAJ\u0026output=pdf\u0026sig=ACfU3U07XNnkOi8XsOlX7EgvrvOY_zTj2A"},"sample_url":"http://www.google.com/patents/reader?id=OH9HBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>