<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US5794189 - Continuous speech recognition - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Continuous speech recognition"><meta name="DC.contributor" content="Joel M. Gould" scheme="inventor"><meta name="DC.contributor" content="Dragon Systems, Inc." scheme="assignee"><meta name="DC.date" content="1995-11-13" scheme="dateSubmitted"><meta name="DC.description" content="A method for use in recognizing speech in which signals are accepted corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements to be executed. The elements are recognized. Modification procedures are executed in response to recognized predetermined ones of the command elements. The modification procedures include refraining from training speech models when the modification procedures do not correct a speech recognition error. In another aspect, the modification procedures include simultaneously modifying previously recognized ones of the text elements."><meta name="DC.date" content="1998-8-11" scheme="issued"><meta name="DC.relation" content="US:4688195" scheme="references"><meta name="DC.relation" content="US:4776016" scheme="references"><meta name="DC.relation" content="US:4783803" scheme="references"><meta name="DC.relation" content="US:4827520" scheme="references"><meta name="DC.relation" content="US:4829576" scheme="references"><meta name="DC.relation" content="US:4833712" scheme="references"><meta name="DC.relation" content="US:4837831" scheme="references"><meta name="DC.relation" content="US:4866778" scheme="references"><meta name="DC.relation" content="US:4903305" scheme="references"><meta name="DC.relation" content="US:4914703" scheme="references"><meta name="DC.relation" content="US:4914704" scheme="references"><meta name="DC.relation" content="US:4931950" scheme="references"><meta name="DC.relation" content="US:4962535" scheme="references"><meta name="DC.relation" content="US:4984177" scheme="references"><meta name="DC.relation" content="US:5027406" scheme="references"><meta name="DC.relation" content="US:5036538" scheme="references"><meta name="DC.relation" content="US:5086472" scheme="references"><meta name="DC.relation" content="US:5095508" scheme="references"><meta name="DC.relation" content="US:5127055" scheme="references"><meta name="DC.relation" content="US:5202952" scheme="references"><meta name="DC.relation" content="US:5231670" scheme="references"><meta name="DC.relation" content="US:5377303" scheme="references"><meta name="DC.relation" content="US:5428707" scheme="references"><meta name="citation_reference" content="Dale Evans, &quot;Talking to the Bug,&quot; Microcad News, pp. 58-61, Mar. 1989."><meta name="citation_reference" content="Dale Evans, Talking to the Bug, Microcad News, pp. 58 61, Mar. 1989."><meta name="citation_reference" content="Itou K et al, Contin. Speech Recog. by Context Dependent Phonetic HMM, ICASSP, pp. I 21 to I 24, Mar. 1992."><meta name="citation_reference" content="Itou K et al, Contin. Speech Recog. by Context-Dependent Phonetic HMM, ICASSP, pp. I-21 to I-24, Mar. 1992."><meta name="citation_reference" content="Mandel, Mark A. et al., &quot;A Commercial Large-Vocabulary Discrete Speech Recognition System: DragonDictate,&quot;Language and Speech, vol. 35 (1, 2) (1992), pp. 237-246."><meta name="citation_reference" content="Mandel, Mark A. et al., A Commercial Large Vocabulary Discrete Speech Recognition System: DragonDictate, Language and Speech, vol. 35 (1, 2) (1992), pp. 237 246."><meta name="citation_reference" content="U.S. Patent Application &quot;Continuous Speech Recognition of Text and Commands,&quot; Ser. No. 08/559,207, Joel M. Gould et al., filed Nov. 13, 1995."><meta name="citation_reference" content="U.S. Patent Application Continuous Speech Recognition of Text and Commands, Ser. No. 08/559,207, Joel M. Gould et al., filed Nov. 13, 1995."><meta name="citation_reference" content="U.S. Patent Application, &quot;Apparatuses and Methods for Training ad Operating Speech Recognition Systems&quot; Joel. M. Gould et al., filed Feb. 1, 1995."><meta name="citation_reference" content="U.S. Patent Application, &quot;Speech Recognition,&quot; Ser. No. 08/521,543, Gregory J. Gadbois, filed Aug. 30, 1995."><meta name="citation_reference" content="U.S. Patent Application, &quot;Speech Recognition,&quot; Ser. No. 08/559,190, Gregory J. Gadbois, filed Nov. 13, 1995."><meta name="citation_reference" content="U.S. Patent Application, Apparatuses and Methods for Training ad Operating Speech Recognition Systems Joel. M. Gould et al., filed Feb. 1, 1995."><meta name="citation_reference" content="U.S. Patent Application, Speech Recognition, Ser. No. 08/521,543, Gregory J. Gadbois, filed Aug. 30, 1995."><meta name="citation_reference" content="U.S. Patent Application, Speech Recognition, Ser. No. 08/559,190, Gregory J. Gadbois, filed Nov. 13, 1995."><meta name="citation_patent_number" content="US:5794189"><meta name="citation_patent_application_number" content="US:08/556,280"><link rel="canonical" href="http://www.google.com/patents/US5794189"/><meta property="og:url" content="http://www.google.com/patents/US5794189"/><meta name="title" content="Patent US5794189 - Continuous speech recognition"/><meta name="description" content="A method for use in recognizing speech in which signals are accepted corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements to be executed. The elements are recognized. Modification procedures are executed in response to recognized predetermined ones of the command elements. The modification procedures include refraining from training speech models when the modification procedures do not correct a speech recognition error. In another aspect, the modification procedures include simultaneously modifying previously recognized ones of the text elements."/><meta property="og:title" content="Patent US5794189 - Continuous speech recognition"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("UaHtU5vHK62vsQTXgYGwDw"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("USA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("UaHtU5vHK62vsQTXgYGwDw"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("USA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us5794189?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US5794189"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=j5FHBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS5794189&amp;usg=AFQjCNG-sz7NK9BVPMMzXLhH8icyXkq23w" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US5794189.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US5794189.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US5794189" style="display:none"><span itemprop="description">A method for use in recognizing speech in which signals are accepted corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements to be executed. The elements are recognized. Modification procedures are executed in response to recognized...</span><span itemprop="url">http://www.google.com/patents/US5794189?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US5794189 - Continuous speech recognition</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US5794189 - Continuous speech recognition" title="Patent US5794189 - Continuous speech recognition"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US5794189 A</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 08/556,280</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Aug 11, 1998</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Nov 13, 1995</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Nov 13, 1995</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/DE69632517D1">DE69632517D1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0773532A2">EP0773532A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0773532A3">EP0773532A3</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0773532B1">EP0773532B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP1341156A1">EP1341156A1</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">08556280, </span><span class="patent-bibdata-value">556280, </span><span class="patent-bibdata-value">US 5794189 A, </span><span class="patent-bibdata-value">US 5794189A, </span><span class="patent-bibdata-value">US-A-5794189, </span><span class="patent-bibdata-value">US5794189 A, </span><span class="patent-bibdata-value">US5794189A</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Joel+M.+Gould%22">Joel M. Gould</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Dragon+Systems,+Inc.%22">Dragon Systems, Inc.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US5794189.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5794189.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5794189.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (23),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (14),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (49),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (11),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (12)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=j5FHBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/5794189&usg=AFQjCNH50X3IojGG19mi71RB0e4LaBs6Xw">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=j5FHBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D5794189&usg=AFQjCNGMwJSi5L7mpeAzsKjaxZ3_LVu_GA">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=j5FHBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D5794189A%26KC%3DA%26FT%3DD&usg=AFQjCNGoa30R2Iih30VgCW_O82xN5CT3nA">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54334851" lang="EN" load-source="patent-office">Continuous speech recognition</invention-title></span><br><span class="patent-number">US 5794189 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA37801043" lang="EN" load-source="patent-office"> <div class="abstract">A method for use in recognizing speech in which signals are accepted corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements to be executed. The elements are recognized. Modification procedures are executed in response to recognized predetermined ones of the command elements. The modification procedures include refraining from training speech models when the modification procedures do not correct a speech recognition error. In another aspect, the modification procedures include simultaneously modifying previously recognized ones of the text elements.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(24)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-1.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-1.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-2.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-2.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-3.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-3.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-4.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-4.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-5.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-5.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-6.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-6.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-7.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-7.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-8.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-8.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-9.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-9.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-10.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-10.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-11.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-11.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-12.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-12.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-13.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-13.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-14.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-14.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-15.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-15.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-16.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-16.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-17.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-17.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-18.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-18.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-19.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-19.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-20.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-20.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-21.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-21.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-22.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-22.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-23.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-23.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5794189-24.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5794189-24.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(33)</span></span></div><div class="patent-text"><div mxw-id="PCLM5267793" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is:</claim-statement> <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1. A method for use in recognizing speech comprising:<div class="claim-text">accepting signals corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements to be executed,</div> <div class="claim-text">recognizing the elements, and</div> <div class="claim-text">executing modification procedures in response to recognized predetermined ones of the command elements, including:<div class="claim-text">simultaneously modifying previously recognized ones of the text elements in response to a single one of the recognized predetermined ones of the command elements.</div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2. The method of claim 1, wherein<div class="claim-text">executing modification procedures includes executing modification procedures that modify the set of elements in response to recognized predetermined ones of the command elements that include a first modification command element that corrects a speech recognition error and a second modification command element that modifies the set of elements without correcting a speech recognition error, including:</div> <div class="claim-text">training speech models when the modification procedures are in response to the first modification command element, and</div> <div class="claim-text">refraining from training speech models when the modification procedures are in response to the second modification command element.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3. The method of claim 1 in which simultaneously modifying previously recognized text elements includes simultaneously modifying text element boundaries of the previously recognized ones of the text elements.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4. The method of claim 3 in which the text element boundaries were misrecognized.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5. The method of claim 2 in which the command indicates that the user wishes to delete a recognized text element.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6. The method of claim 5 in which the text element is the most recently recognized text element.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7. The method of claim 5 in which the command comprises "scratch that".</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8. The method of claim 5 in which the command is followed by a pause and the most recently recognized text element is then deleted.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9. The method of claim 5 in which the command is followed by an utterance corresponding to a substitute text element and the substitute text element is then substituted for the most recently recognized text element.</div>
    </div>
    </div> <div class="claim"> <div num="10" class="claim">
      <div class="claim-text">10. A method for use in recognizing speech comprising:<div class="claim-text">accepting signals corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements to be executed,</div> <div class="claim-text">recognizing the elements by producing a set of elements associated with the signals, and</div> <div class="claim-text">executing modification procedures that modify the set of elements in response to recognized predetermined ones of the command elements;</div> <div class="claim-text">wherein:<div class="claim-text">the command elements include an utterance representing a selected recognized text element to be corrected, and</div> <div class="claim-text">the modification procedures include matching the selected recognized text element against previously recognized text elements.</div> </div> </div>
    </div>
    </div> <div class="claim"> <div num="11" class="claim">
      <div class="claim-text">11. A method for use in recognizing speech comprising:<div class="claim-text">accepting signals corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements to be executed,</div> <div class="claim-text">recognizing the elements by producing a set of elements associated with the signals, and</div> <div class="claim-text">executing modification procedures that modify the get of elements in response to recognized predetermined ones of the command elements;</div> <div class="claim-text">wherein:<div class="claim-text">the modification procedures include parsing previously recognized text elements and building a tree structure that represents the ordered relationship among the previously recognized text elements.</div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" class="claim">
      <div class="claim-text">12. The method of claim 11 in which executing the modification procedures includes:<div class="claim-text">detecting a speech recognition error, and</div> <div class="claim-text">training speech models in response to the detected speech recognition error.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" class="claim">
      <div class="claim-text">13. The method of claim 12 in which detecting further includes:<div class="claim-text">determining whether speech frames or speech models corresponding to a speech recognition modification match at least a portion of the speech frames or speech models corresponding to previous utterances.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" class="claim">
      <div class="claim-text">14. The method of claim 13, further including:<div class="claim-text">selecting matching speech frames or speech models.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" class="claim">
      <div class="claim-text">15. The method of claim 11 in which the predetermined ones of the command elements include a select command.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" class="claim">
      <div class="claim-text">16. The method of claim 15 in which the command elements include an utterance representing a selected recognized text element to be corrected.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" class="claim">
      <div class="claim-text">17. The method of claim 11 in which the tree structure reflects multiple occurrences of a given previously recognized one of the text elements.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" class="claim">
      <div class="claim-text">18. The method of claim 15 in which the utterance represents a sequence of multiple selected recognized text elements.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" class="claim">
      <div class="claim-text">19. The method of claim 11 in which the modification procedures include<div class="claim-text">modifying one of the recognized text elements.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" class="claim">
      <div class="claim-text">20. The method of claim 19 in which the modifying is based on correction information provided by a user.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" class="claim">
      <div class="claim-text">21. The method of claim 20 in which the correction information is provided by the user speaking substitute text elements.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="22" class="claim">
      <div class="claim-text">22. The method of claim 21 in which the correction information includes correction of boundaries between text elements.</div>
    </div>
    </div> <div class="claim"> <div num="23" class="claim">
      <div class="claim-text">23. A method for use in recognizing speech comprising:<div class="claim-text">accepting signals corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements to be executed,</div> <div class="claim-text">recognizing the elements by producing a set of elements associated with the signals, and</div> <div class="claim-text">executing modification procedures that modify the set of elements in response to recognized predetermined ones of the command elements;</div> <div class="claim-text">wherein<div class="claim-text">the modification procedures include building a tree structure grouping speech frames corresponding to possible text elements in branches of the tree.</div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="24" class="claim">
      <div class="claim-text">24. The method of claim 23 in which the modification procedures include modifying one or more of the most recently recognized text elements.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" class="claim">
      <div class="claim-text">25. The method of claim 24 in which the predetermined ones of the command elements include a command indicating that a short term correction is to be made.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="26" class="claim">
      <div class="claim-text">26. The method of claim 25 in which the command comprises "oops".</div>
    </div>
    </div> <div class="claim-dependent"> <div num="27" class="claim">
      <div class="claim-text">27. The method of claim 24 in which the modification procedures include interaction with a user with respect to modifications to be made.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="28" class="claim">
      <div class="claim-text">28. The method of claim 27 in which the interaction includes a display window in which proposed modifications are indicated.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="29" class="claim">
      <div class="claim-text">29. The method of claim 27 in which the interaction includes a user uttering the spelling of a word to be corrected.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="30" class="claim">
      <div class="claim-text">30. The method of claim 23 in which the modification procedures include re-recognizing the most recently recognized text elements using the speech frames of the tree structure.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="31" class="claim">
      <div class="claim-text">31. The method of claim 23 in which the tree is used to determine, text element by text element, a match between a correction utterance and the originally recognized text elements.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="32" class="claim">
      <div class="claim-text">32. The method of claim 31 in which the modification procedures include, after determining a match, re-recognizing subsequent speech frames of an original utterance.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="33" class="claim">
      <div class="claim-text">33. The method of claim 2 in which, if no match is determined, the recognized correction utterance is displayed to the user.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES67159729" lang="EN" load-source="patent-office" class="description">
    <heading>BACKGROUND</heading> <p>This invention relates to continuous speech recognition.</p>
    <p>Many speech recognition systems, including DragonDictate from Dragon Systems of West Newton, Mass., store data representing a user's speech (i.e., speech frames) for a short list of words,, e.g., 32, just spoken by the user. If a user determines that a word was incorrectly recognized, the user calls up (by keystroke, mouse selection, or utterance) a correction window on a display screen. The correction window displays the short list of words or a portion of the short list of words, and the user selects the misrecognized word for correction. Selecting a word causes the speech recognition system to re-recognize the word by comparing the stored speech frames associated with the word to a vocabulary of speech models. The comparison provides a choice list of words that may have been spoken by the user and the system displays the choice list for the user. The user then selects the correct word from the choice list or the user verbally spells the correct word in the correction window. In either case, the system replaces the incorrect word with the correct word and adapts (i.e., trains) the speech models represent the correct word using associated speech frames.</p>
    <p>For more information on training speech models, see U.S. Pat. No. 5,027,406, entitled "Method for Interactive Speech Recognition and Training", and U.S. patent application Ser. No. 08/382,752, entitled "Apparatuses and Methods for Training and Operating Speech Recognition Systems", which are incorporated by reference. For more information on choice lists and alphabetic prefiltering see U.S. Pat. No. 4,783,803, entitled "Speech Recognition Apparatus and Method", U.S. Pat. No. 4,866,778, entitled "Interactive Speech Recognition Apparatus", and U.S. Pat. No. 5,027,406, entitled "Method for Interactive Speech Recognition and Training", which are incorporated by reference.</p>
    <p>Aside from correcting speech recognition errors, users often change their mind regarding previously entered text and want to replace one or more previously entered words with different words. To do this editing, users frequently call up the correction window, select a previously entered word, and then type or speak a different word. The system replaces the previously entered word with the different word, and, because training is continuous, the system also adapts the speech models associated with the different word with the speech frames from the original utterance. This "misadaptation" may degrade the integrity of the speech models for the different word and reduce speech recognition accuracy.</p>
    <p>For example, the user may have entered "It was a rainy day" and may want the text to read "It was a cold day." If the user calls up the correction window, selects the word "rainy" and types in or speaks the word "cold", the system replaces the word "rainy" with the word "cold" and misadapts the speech models for "cold" with the speech models for "rainy".</p>
    <p>If the speech recognition system misrecognizes one or more word boundaries, then the user may need to correct two or more words. For example, if the user says "let's recognize speech" and the system recognizes "let's wreck a nice beach," then the user needs to change "wreck a nice beach" to "recognize speech." The user may call up the correction window and change each word individually using the choice list for each word. For example, the user may call up the correction window and select "wreck" as the word to be changed and choose "recognize" from the choice list (if available) or enter (by keystroke or utterance: word or spelling) "recognize" into the correction window. The user may then select and reject (i.e., delete) "all" and then "nice", and lastly the user may select "beach" and choose "speech" from the choice list or enter "speech" into the correction window.</p>
    <p>Alternatively, after the user has called up the correction window and chosen "recognize", some speech recognition systems permit the user to enter a space after "recognize" to indicate to the system that another word correction follows. The system re-recognizes the speech frames following the newly entered word "recognize" and provides a hypothesis (e.g., "speech") and a corresponding choice list for the user. The user chooses either the hypothesis or a word from the choice list and may again follow that word with a space to cause the system to re-recognize a next word.</p>
    <p>Other speech recognition systems have large storage capabilities that store all speech frames associated with user utterances and record all user utterances. The user may select a previously spoken word to have the system play back the user's original utterance. If the utterance does not match the recognized word (i.e., the system misrecognized the word), then the user may call up a correction window and type or speak the correct word to have the system make the correction and train the speech models for the corrected word. This may reduce speech model misadaptation by requiring the user to determine whether the system actually misrecognized the word before speech models are trained.</p>
    <heading>SUMMARY</heading> <p>In general, in one aspect, the invention features a method for use in recognizing speech. Signals are accepted corresponding to interspersed speech elements including text elements corresponding to text to be recognized and command elements to be executed. The elements are recognized. Modification procedures are executed in response to recognized predetermined ones of the command elements. The modification procedures include refraining from training speech models when the modification procedures do not correct a speech recognition error.</p>
    <p>In general, in another aspect, the modification procedures include simultaneously modifying previously recognized ones of the text elements.</p>
    <p>Implementations of the invention may include one or more of the following features. Text element boundaries (e.g., misrecognized boundaries) of the previously recognized ones of the text elements may be modified. Executing the modification procedures may include detecting a speech recognition error, and training speech models in response to the detected speech recognition error. The detecting may include determining whether speech frames or speech models corresponding to a speech recognition modification match at least a portion of the speech frames or speech models corresponding to previous utterances. Matching speech frames or speech models may be selected. The predetermined command elements may include a select command and an utterance representing a selected recognized text element to be corrected. The selected recognized text element may be matched against previously recognized text elements. Previously recognized text elements may be parsed and a tree structure may be built that represents the ordered relationship among the previously recognized text elements. The tree structure may reflect multiple occurrences of a given previously recognized one of the text elements. The utterance may represent a sequence of multiple selected recognized text elements. One of the recognized text elements may be modified based on correction information provided by a user speaking substitute text. The correction information may include correction of boundaries between text elements. The method of claim 1 in which the modification procedures include modifying one or more of the most recently recognized text elements.</p>
    <p>The predetermined command elements may include a command (e.g., "oops") indicating that a short term correction is to be made. The modification procedures may include interaction with a user with respect to modifications to be made. The interaction may include a display window in which proposed modifications are indicated. The interaction may include a user uttering the spelling of a word to be corrected. The modification procedures may include building a tree structure grouping speech frames corresponding to possible text elements in branches of the tree. The most recently recognized text elements may be re-recognized using the speech frames of the tree structure. The tree may be used to determine, text element by text element, a match between a correction utterance and the originally recognized text elements. The modification procedures may include, after determining a match, re-recognizing subsequent speech frames of an original utterance. If no match is determined, the recognized correction utterance may be displayed to the user. The command may indicate that the user wishes to delete a recognized text element. The text element may be the most recently recognized text element.</p>
    <p>The predetermined command may be "scratch that". The command may be followed by a pause and the most recently recognized text element may then be deleted. The command may be followed by an utterance corresponding to a substitute text element and the substitute text element is then substituted for the most recently recognized text element.</p>
    <p>The advantages of the invention may include one or more of the following. Providing the user with a variety of editing/correcting techniques allows the user to choose how they will edit or correct previously entered text. The technique chosen may depend upon the edit or correction to be made or the user may choose the technique with which they are most comfortable. The different techniques also allow users flexibility as to when changes or corrections are made. For example, the user may edit continuously while dictating text or the user may dictate an entire document before going back to make changes or corrections. Furthermore, the user's cognitive overhead for correcting and editing previously entered text is reduced. For instance, speech models may be trained only when the speech recognition system, not the user, determines that a word or series of words has been misrecognized. Similarly, in response to a user's correction, the system may automatically modify word boundaries to simultaneously change a first number of words into a second number of different words.</p>
    <p>Other advantages and features will become apparent from the following description and from the claims.</p>
    <heading>DESCRIPTION</heading> <p>FIG. 1 is a block diagram of a speech recognition system.</p>
    <p>FIG. 2 is a block diagram of speech recognition software and application software.</p>
    <p>FIG. 3 is a block diagram of speech recognition software and vocabularies stored in memory.</p>
    <p>FIG. 4 is computer screen display of word processing command words and sentences.</p>
    <p>FIG. 5 is a flow chart depicting a long term editing feature.</p>
    <p>FIGS. 6 and 7 are block diagrams of long term editing feature tree structures.</p>
    <p>FIGS. 8a-8f are computer screen displays depicting the long term editing feature.</p>
    <p>FIG. 9 is a flow chart depicting a short term error correction feature.</p>
    <p>FIGS. 10a-10e are computer screen displays depicting a short term speech recognition error correction feature.</p>
    <p>FIG. 11 is a computer screen display of a correction window and a spelling window.</p>
    <p>FIGS. 12 and 13 are block diagrams of short term error correction feature tree structures.</p>
    <p>FIG. 14 is a flow chart depicting a scratch that editing feature.</p>
    <p>FIGS. 15a-15d show user interface screens.</p>
    <p>The speech recognition system includes several correction/editing features. Using one correction feature termed "short term error correction," the user calls up (by keystroke, mouse selection, or utterance, e.g., "oops") a correction window and enters (by keystroke or utterance) one or more previously spoken words to correct a recently misrecognized utterance. The system compares speech models (for typed words) or speech frames (for spoken words) associated with the correction against the speech frames of a predetermined number, e.g., three, of the user's previous utterances. If the comparison locates speech frames corresponding to a portion of one of the user's previous three utterances that substantially match the speech models or frames of user's correction, then the system modifies the original recognition to include the correction. The modification of the original utterance includes re-recognizing the speech frames around the correction. As a result, a user may simultaneously correct one word, a series of words, or an entire utterance, including correcting misrecognized word boundaries. The speech frames from the original utterance are also used to train (i.e., adapt) the speech models for the correction.</p>
    <p>If the comparison does not locate speech frames corresponding to a portion of one of the user's previous three utterances that substantially match the user's correction, then the system notifies the user that the correction cannot be made. For example, if the user erroneously enters one or more different words as a correction, the comparison will not locate corresponding speech frames in one of the user's previous three utterances. This reduces the possibility that speech models may be misadapted.</p>
    <p>Another editing feature, termed "long term editing," allows the user to select and modify previously entered text. After selecting text through keystrokes or mouse selection or by speaking the words to be selected, the user modifies the selected text by typing or speaking replacement words. The user may simultaneously modify one word, a series of words, or an entire utterance, including correcting misrecognized word boundaries. Because the user may use long term editing to edit previously entered text or to correct speech recognition errors, the system does not automatically train the speech models for the modifications which substantially prevents misadaptation of speech models. The user may, however, request that the system train the speech models for a modification.</p>
    <p>A correction/editing feature, termed "scratch that and repeat", allows the user to quickly and easily delete or delete and replace his or her most recent utterance. After speaking an utterance, if the user determines that the system did not correctly recognize the previous utterance, the user selects (by keystroke, mouse selection, or utterance, e.g., "scratch that") a scratch command and repeats the utterance. The system replaces the words recognized from the original utterance with words recognized from the second utterance. If the user wants to delete the words of the previous utterance, the user enters the scratch that command alone (e.g., followed by silence), and if the user wants to edit the words of the previous utterance, the user speaks "scratch that" followed by new text. In any case, the system does not train speech models in accordance with any replacement text which reduces the possibility of misadaptation of speech models.</p>
    <p>Referring to FIG. 1, a typical speech recognition system 10 includes a microphone 12 for converting a user's speech into an analog data signal 14 and a sound card 16. The sound card includes a digital signal processor (DSP) 19 and an analog-to-digital (A/D) converter 17 for converting the analog data signal into a digital data signal 18 by sampling the analog data signal at about 11 Khz to generate 220 digital samples during a 20 msec time period. Each 20 ms time period corresponds to a separate speech frame. The DSP processes the samples corresponding to each speech frame to generate a group of parameters associated with the analog data signal during the 20 ms period. Generally, the parameters represent the amplitude of the speech at each of a set of frequency bands.</p>
    <p>The DSP also monitors the volume of the speech frames to detect user utterances. If the volume of three consecutive speech frames within a window of five consecutive speech frames (i.e., three of the last five speech frames) exceeds a predetermined speech threshold, for example, 20 dB, then the DSP determines that the analog signal represents speech and the DSP begins sending several, e.g., three, speech frames of data at a time (i.e., a batch) via a digital data signal 23 to a central processing unit (CPU) 20. The DSP asserts an utterance signal (Utt) 22 to notify the CPU each time a batch of speech frames representing an utterance is sent via the digital data signal.</p>
    <p>When an interrupt handler 24 on the CPU receives assertions of Utt signal 22, the CPU's normal sequence of execution is interrupted. Interrupt signal 26 causes operating system software 28 to call a store routine 29. Store routine 29 stores the incoming batch of speech frames into a buffer 30. When fourteen consecutive speech frames within a window of nineteen consecutive speech frames fall below a predetermined silence threshold, e.g., 6 dB, then the DSP stops sending speech frames to the CPU and asserts an End<sub>--</sub> Utt signal 21. The End<sub>--</sub> Utt signal causes the store routine to organize the batches of previously stored speech frames into a speech packet 39 corresponding to the user utterance.</p>
    <p>Interrupt signal 26 also causes the operating system software to call monitor software 32. Monitor software 32 keeps a count 34 of the number of speech packets stored but not yet processed. An application 36, for example, a word processor, being executed by the CPU periodically checks for user input by examining the monitor software's count. If the count is zero, then there is no user input. If the count is not zero, then the application calls speech recognizer software 38 and passes a pointer 37 to the address location of the speech packet in buffer 30. The speech recognizer may be called directly by the application or may be called on behalf of the application by a separate program, such as DragonDictate from Dragon Systems of West Newton, Mass., in response to the application's request for input from the mouse or keyboard.</p>
    <p>For a more detailed description of how user utterances are received and stored within a speech recognition system, see U.S. Pat. No. 5,027,406, entitled "Method for Interactive Speech Recognition and Training" which is incorporated by reference.</p>
    <p>Referring to FIG. 2, to determine what words have been spoken speech recognition software 38 causes the CPU to retrieve speech frames within speech packet 39 from buffer 30 and compare the speech frames (i.e., the user's speech) to speech models stored in one or more vocabularies 40. For a more detailed description of continuous speech recognition, see U.S. Pat. No. 5,202,952, entitled "Large-Vocabulary Continuous Speech Prefiltering and Processing System", which is incorporated by reference.</p>
    <p>The recognition software uses common script language interpreter software to communicate with the application 36 that called the recognition software. The common script language interpreter software enables the user to dictate directly to the application either by emulating the computer keyboard and converting the recognition results into application dependent keystrokes or by sending application dependent commands directly to the application using the system's application communication mechanism (e.g., Microsoft Windows uses Dynamic Data Exchange). The desired applications include, for example, word processors 44 (e.g., Word Perfect or Microsoft Word), spreadsheets 46 (e.g., Lotus 1-2-3 or Excel), and games 48 (e.g., Solitaire).</p>
    <p>As an alternative to dictating directly to an application, the user dictates text to a speech recognizer window, and after dictating a document, the user transfers the document (manually or automatically) to the application.</p>
    <p>Referring to FIG. 3, when an application first calls the speech recognition software, it is loaded from remote storage (e.g., a disk drive) into the computer's local memory 42. One or more vocabularies, for example, common vocabulary 48 and Microsoft Office vocabulary 50, are also loaded from remote storage into memory 42. The vocabularies 48, 50, and 54 include all words 48b, 50b, and 54b (text and commands), and corresponding speech models 48a, 50a, and 54a, that a user may speak.</p>
    <p>Spreading the speech models and words across different vocabularies allows the speech models and words to be grouped into vendor (e.g., Microsoft and Novell) dependent vocabularies which are only loaded into memory when an application corresponding to a particular vendor is executed for the first time after power-up. For example, many of the speech models and words in the Novell PerfectOffice vocabulary 54 represent words only spoken when a user is executing a Novell PerfectOffice application, e.g., WordPerfect. As a result, these speech models and words are only needed when the user executes a Novell application. To avoid wasting valuable memory space, the Novell PerfectOffice vocabulary 54 is only loaded into memory when needed (i.e., when the user executes a Novell application).</p>
    <p>Alternatively, the speech models and words are grouped into application dependent vocabularies. For example, separate vocabularies may exist for Microsoft Word, Microsoft Excel, and Novell WordPerfect. Similarly, the speech models and words corresponding to commands may be grouped into one set of vocabularies while the speech models and words corresponding to text may grouped into another set of vocabularies. As another alternative, only a single vocabulary including all words, and corresponding speech models, that a user may speak is loaded into local memory and used by the speech recognition software to recognize a user's speech.</p>
    <p>Referring to FIG. 4, once the vocabularies are loaded and an application calls the recognition software, the CPU compares speech frames representing the user's speech to speech models in the vocabularies to recognize (step 60) the user's speech. The CPU then determines (steps 62 and 64) whether the results represent a command or text. Commands include single words and phrases and sentences that are defined by templates (i.e., restriction rules). The templates define the words that may be said within command sentences and the order in which the words are spoken. The CPU compares (step 62) the recognition results to the possible command words and phrases and to command templates, and if the results match a command word or phrase or a command template (step 64), then the CPU sends (step 65a) the application that called the speech recognition software keystrokes or scripting language that cause the application to execute the command, and if the results do not match a command word or phrase or a command template, the CPU sends (step 65b) the application keystrokes or scripting language that cause the application to type the results as text.</p>
    <p>For more information on this and other methods of distinguishing between text and commands, see U.S. patent application Ser. No. 08/559,207, entitled "Continuous Speech Recognition of Text and Commands", filed the same day and assigned to the same assignee as this application, which is incorporated by reference.</p>
    <p>Referring back to FIG. 3, in addition to including words 51 (and phrases) and corresponding speech models 53, the vocabularies include application (e.g., Microsoft Word 100 and Microsoft Excel 102) dependent command sentences 48c, 50c, and 54c available to the user and application dependent groups 48d, 50d, and 54d which are pointed to by the sentences and which point to groups of variable words in the command templates.</p>
    <heading>Long Term Editing</heading> <p>The long term editing feature provides the user with the flexibility to edit text that was just entered (correctly or incorrectly) into an open document or to open an old document and edit text entered at an earlier time. Referring to FIG. 5, the system first determines (step 130) whether the user has spoken, and if so, the system recognizes (step 132) the user's speech. The system then determines (step 134) whether the user said "select". If the user did not say "select", the system determines (step 136) whether any text is selected. If text was selected, the system replaces (step 138) the selected text with the newly recognized text on a display screen 135 (FIG. 1). If no other text is selected, the system enters (step 140) the newly recognized text on the display screen.</p>
    <p>If the system determines (step 134) that the user did say "select", then the system determines (step 142) whether "select" is followed by a pause. If "select" is followed by a pause, then the system enters (step 140) the word "select" on the display screen. If "select" is not followed by a pause, then the system reads (step 144) data stored in a display screen buffer 143 (FIG. 1). This data represents the succession of words displayed on the display screen and may be read through a standard edit control request to the operating system or through an application program interface (API) corresponding to the application being executed, for example, Microsoft Word or Novell Wordperfect.</p>
    <p>The system parses (step 146) the stored data and maps each word into indices in one or more vocabularies consisting of, for example, 180,000, words. As an example, "hello there." is parsed into three words, "hello" "there" and "period", while "New York", a phrase, is parsed into one "word". If the data represents a word that is not in the one or more vocabularies, then the system does not index the word or the system indexes the word after generating an estimated pronunciation using known text-to-speech synthesis rules.</p>
    <p>Using the parsed words, the system builds (step 148) a tree structure that describes the connection between the words being displayed. Referring to FIG. 6, if the display screen displays "This is a test of speech", then the system builds a tree structure 149 beginning with the word "select" 150 that indicates (arrows 151) that the word "select" must be followed by at least one of the words being displayed: "This", "is", "a", "test", "of", or "speech". For example, according to tree structure 149, if "This" follows "select", then "is" must be next, if "is" follows "select", then "a" must be next, if "a" follows "select" then "test" must be next, if "test" follows "select" then "of" must be next, if "of" follows "select" then "speech" must be next, and if "speech" follows "select", then silence must follow. The tree structure also accounts for repeated words. Referring to FIG. 7, if the display screen displays "This is a test of this test", then the system builds a tree structure 152 that indicates (arrows 154) that the word "test" may follow the words "a" or "this".</p>
    <p>As an alternative to executing steps 144, 146, and 148 after the select command is recognized, the system may execute these steps before the select command is issued by the user (e.g., when a document is first opened and each time the words on the display screen change) or the system may execute these steps when the select command is partially recognized (e.g., when the user says "select").</p>
    <p>Referring also to FIGS. 8a-8c, to select one or more words in previously entered text 300, the user's speech following "select" 302 (i.e., partial speech recognition results are shown) must match one or more words in the previously entered text (e.g., "test" 304). Thus, the system compares (step 156) the words of the newly recognized text (e.g., "test") to the tree structure to determine (step 158) whether the words of the newly recognized text match at least a portion of the tree structure. If a match is not found, then the system enters (step 159) "select" and the remaining newly recognized text on the display screen. If a match is found, then the system highlights (step 160) the matching text 306 (FIG. 8c) and waits (steps 162 and 164) for the user to accept or reject the selection.</p>
    <p>If the user agrees with the system's selection, then the user accepts (step 164) the selection, and the system selects (step 166) the matching text and waits (step 130) for user input. If the user types or speaks new text (e.g., "text"), the system replaces (steps 130-138) the selected text with the new text (e.g., "text" 308, FIG. 8d).</p>
    <p>If the user does not agree with the system's selection, then the user may request (step 162) (by keystroke, mouse selection, or utterance, e.g., "try again" 310, shown as partial results on the display screen in FIG. 8e) that the system re-compare (step 156) the newly recognized text to the tree structure. If the words of the newly recognized speech are displayed at several locations on the display screen, then the newly recognized speech matches multiple portions of the tree structure. For example, if the screen displays "This is a test of continuous speech . . . Somewhere in this test is an error . . . " (FIG. 8f) and the user says "select test", then "test" matches two portions of the tree structure. Originally, the system selects the text 308 that is displayed before (or after) and closest to the top of the display screen (or closest to the current cursor position). If the user requests a re-compare, then the system selects the next closest match 312 and highlights that match.</p>
    <p>If the newly recognized text is not displayed elsewhere on the display screen and the user requests a re-compare, then the system selects the next best match (i.e., other text that substantially matches the newly recognized text).</p>
    <p>Instead of requesting a re-compare, the user may reject the selected text (by keystroke, mouse selection, or utterance, e.g., "abort", step 164) and exit out of the long term editing feature.</p>
    <p>As an example, if the displayed text is "This is a test of speech" and the user says "select test" ("select a test" or "select a test of") then the system determines that "test" ("a test" or "a test of") matches a portion of the tree structure 149 (FIG. 6) and selects (i.e., highlights) "test" ("a test" or "a test of") on the display screen. If the user disagrees with the selection, then the user may request that the system re-compare the newly recognized text against the tree structure or the user may exit out of the selection. If the user agrees with the selection, then the system selects (166) the matching text. If a match is not found, then the system determines that the user was dictating text and not issuing the select command and enters (step 159) "select" and the recognized text on the display screen. For example, if the displayed text is "This is a test of speech" and the user says "select this test", the system determines that the recognized text does not match the tree structure and types "select this test" on the display screen.</p>
    <p>Because the long term editing feature does not compare speech frames or models of a user's text selection to speech frames or models of the previously entered text, the system need not save speech frames for entire documents and the user has the flexibility to edit newly entered text in an already open document or to open an old document and edit text within that document. The system also does not adapt speech models for edited text when the long term editing feature is used because the user's edits may or may not correct speech recognition errors. This substantially prevents misadaptation. Furthermore, because the user can simultaneously replace multiple pre-existing words with multiple new words, the user may use the long term editing feature to change misrecognized word boundaries.</p>
    <heading>Short Term Speech Recognition Error Correction</heading> <p>The short term error correction feature allows the user to correct speech recognition errors in a predetermined number (e.g., three) of the user's last utterances. The correction may simultaneously modify one or more words and correct misrecognized word boundaries as well as train the speech models for any misrecognized word or words. The system only modifies a previous utterance and trains speech models if the user's correction substantially matches speech frames corresponding to at least a portion of the previous utterance. This substantially prevents misadaptation of speech models by preventing the user from replacing previously entered text with new words using the short term error correction feature.</p>
    <p>Referring to FIGS. 9 and 10a-10e, when a user determines that a speech recognition error 320 has occurred within the last three utterances, the user may say "Oops" 322 (FIG. 10b) or type keystrokes or make a mouse selection of a correction window icon. When the system determines (step 178) that the user has issued the oops command, the system displays (step 180) a correction window 182 (FIG. 10c) on display screen 136 and displays (step 183) the last utterance 184 in a correction sub-window 186. The system then determines (step 188) whether the user has input (by keystroke or utterance) corrected text (e.g., "This" 324, FIG. 10d). For example, if the user said "This ability to talk fast" and the system recognized "Disability to talk fast", the user may say "oops" and then repeat or type "This" (or "This ability" or "This ability to talk", etc.).</p>
    <p>If the system determines (step 190) that the user spoke the corrected text, then the system recognizes (step 192) the user's speech. Instead of providing words as corrected text, the user may enter (by keystroke, mouse selection, or utterance, e.g., "spell that", FIG. 11) a spelling command followed by the letters of the words in the corrected text. After determining that the user entered the spelling command, the system displays a spelling window 194. The system then recognizes the letters 196 spoken or typed by the user and provides a choice list 197 corresponding to the recognized letters. For more information regarding the spelling command and speech recognition of letters, see U.S. patent application Ser. No. 08/521,543, entitled "Speech Recognition", filed Aug. 30, 1995, and U.S. patent application Ser. No. 08/559,190, entitled "Speech Recognition", filed the same day and assigned to the same assignee as this application.</p>
    <p>Referring also to FIG. 12, whether the user types or speaks the corrected text, the system builds (step 198) a tree structure (e.g., 200) for each of the last three utterances using the speech frames corresponding to these utterances and the speech frames (if spoken) or speech models (if typed) corresponding to the corrected text. The system then re-recognizes (step 202) each of the last three utterances against the corresponding tree structure to determine (step 204) if at least a portion of the speech frames in the corresponding utterance substantially match the speech frames or models corresponding to the corrected text. Each state 210-220 in the tree structure includes one or more speech frames corresponding to a previously recognized word in the utterance, the remaining speech frames in the utterance, and the speech frames or models corresponding to a first recognized word in the corrected text.</p>
    <p>For example, if the user says "Let's recognize speech" and the system recognizes "Let's wreck a nice beach", the user may say "loops" to call up the correction window and say "recognize" as the corrected text. State 210 includes all of the speech frames of the utterance and the speech frames corresponding to "recognize", while state 216 includes only the speech frames corresponding to "nice", the remaining speech frames of the utterance (e.g., "beach"), and the speech frames corresponding to "recognize". State 220 includes only the speech frames corresponding to "recognize" to prevent the system from reaching final state 222 before at least a portion of the speech frames in the utterance are found to substantially match the speech frames corresponding to "recognize".</p>
    <p>If the system determines that the initial speech frames of the utterance best match the speech models in the system vocabulary for the word "let's", then the system determines whether the next speech frames best match "wreck" or "recognize". If the system determines that the speech frames best match "wreck", the system determines whether the next speech frames best match "a" or "recognize". The system makes this determination for each of the originally recognized words in the utterance.</p>
    <p>During re-recognition, the system determines which path (from state 210 to 222) has the highest speech recognition score. Initially, the system is likely to reach state 220 after re-recognizing the original utterance as it originally did, i.e., "let's wreck a nice beach". After reaching state 220, however, the system cannot match any remaining speech frames to "recognize" and reach final state 222. Thus, the score for this path is very low and the system disregards this path as a possibility. In this example, the highest scoring path is "let's recognize speech" (as opposed to other possible paths: "let's wreck recognize" or "let's wreck a recognize" ).</p>
    <p>If a match for the first word of the corrected text is found, then the system transitions to final state 222 and re-recognizes the remaining speech frames of the user utterance against the entire system vocabulary. The system then displays (step 224) the proposed text correction in the correction sub-window and determines (steps 226 and 228) whether the user has provided additional corrected text (step 226) or accepted or rejected (step 228) the correction. The user may disagree with the proposed correction and input (by keystroke or utterance) additional corrected text. For instance, instead of saying "oops recognize", the user may say "oops recognize speech". The user may also reject the correction to exit out of the correction window. If the user agrees with the correction, the system modifies (step 230) the displayed text (i.e., change "Disability" 320, FIG. 10d, to "This ability" 326, FIG. 10e) and trains the speech models of the correctly recognized words against the speech frames of the original user utterance.</p>
    <p>If no match is found (step 204) or if the score of the match is below an empirically tuned threshold, then the system notifies (step 232) the user and displays the recognized corrected text in the correction sub-window and again waits (steps 226 and 228) for user input. Displaying the corrected text allows the user to determine if he or she made an error by providing different text instead of corrected text (i.e., a repeat of the original utterance). If the user made an error, the user may try again by speaking or typing corrected text. If the user did not make an error, but the system did not find a match or found an incorrect match, then the user may input additional corrected text to improve the likelihood that a correct match will be found.</p>
    <p>For example, instead of providing a single word "recognize" as the corrected text, the user provides multiple words "recognize speech" as the corrected text. Referring to FIG. 13, the resulting tree structure 234 generated by the system adds a state 236 that includes the speech frames or models of the second word in the corrected text (e.g., "speech"). A similar state is added for each additional word in the corrected text. After matching the first word in the corrected text to one or more speech frames in the user utterance, to reach final state 238, the system must match one or more following speech frames of the utterance to speech frames or models corresponding to each additional word in the corrected text. Additional words increase the accuracy with which speech frames from the original utterance are matched with speech frames or models from the correction.</p>
    <p>The empirically tuned threshold substantially prevents the user from entering new text as corrected text which reduces the possibility that speech models corresponding to correctly recognized words will be misadapted. Because the corrected text may include multiple words, the user may correct multiple word misrecognitions and word boundary misrecognitions simultaneously. Limiting the number of previous utterances that may be corrected limits the number of speech frames that the system must store.</p>
    <heading>Scratch That and Repeat</heading> <p>The scratch that command allows the user to quickly and easily delete or delete and replace their last utterance. Referring to FIG. 14, if the system determines (step 212) that the user entered the scratch that command (i.e., keystroke, mouse selection of a scratch that icon, or utterance, e.g., "scratch that"), the system deletes (step 214) the last utterance. If the user speaks an additional utterance after the scratch that command, then the system recognizes the additional utterance and displays it on the display screen in place of the deleted utterance.</p>
    <p>Referring to FIGS. 15a-15d, for example, if the user says "I will like to dictate" 330 (FIG. 15a) or if the user says "I would like to dictate" but the system recognizes "I will like to dictate" 330, then the user may say "scratch that" 332 (FIG. 15b) to delete that utterance (FIG. 15c). If the user made a mistake, then the user can speak the new correct text "I would like to dictate" 334 (FIG. 15d), and if the user spoke correctly but the system misrecognized the utterance, then the user can repeat the utterance "I would like to dictate" 334. In either case, the system recognizes the speech and displays it on the display screen.</p>
    <p>Because the user may use the scratch that command to edit previous text or correct speech recognition errors, the system does not adapt speech models when the user enters the scratch that command. This substantially prevents misadaptation of speech models.</p>
    <p>Other embodiments are within the scope of the following claims.</p>
    <p>For example, instead of having a digital signal processor (DSP) process the samples corresponding to each speech frame to generate a group of parameters associated with the analog data signal during each 20 ms time period, the CPU includes front-end processing software that allows the CPU to generate the parameters.</p>
    <p>As another example, speech models may be selectively trained when the long term editing feature and/or the scratch that command are used. For example, the user may be given control over when speech models are adapted. With such control, the user may decide when a speech recognition error has occurred and have the system train speech models in accordance with that determination. As another example, the system may be given control over when speech models are adapted. If the system determines that the user corrected a speech recognition error, then the system trains the speech models accordingly.</p>
    <p>Many optimizations to improve speech recognition performance are possible. For example, typed text cannot cause speech recognition errors, and, as a result, during short term error correction re-recognition (step 202, FIG. 10) when the system is re-recognizing the remaining speech frames against the system vocabulary (state 222, FIG. 12), the system may increase the speech recognition score for words matching text that the user entered through keystrokes.</p>
    <heading>Pseudo-Code</heading> <p>Following is pseudo-code derived from C Programming Language Code that describes the process for Long Term Editing and Short Term Speech Recognition Error Correction:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________Long Term Editingstart: wait for start of speech start recognition of speech if first word of the recognition is "select"  build-the-select-grammar  recognize the utterance against the select-grammar  if the recognition matches the select-grammar   search-for-the-indicated-words   remember the utterance and recognition results aslast-select-result   goto start  otherwise,   interpret recognition as text   type-text-on-the-screen   delete the last-select-result   goto start otherwise, if the recognition matches "try again" and there is alast-select-result  search-for-the-indicated-words in the last-select-result  if the words found by the search are not the exact sameoccurrences which were first selected by thistranscription of the results   goto start  otherwise,   change the last-select-result to the next best unusedtranscription of the utterance saved inlast-select-result   if there are no more unused transcriptions inlast-select-result    goto start   otherwise,    search-for-the-indicated-words in the next besttranscription    goto start otherwise,  continue recognition  type-text-on-the-screen  delete the last-select-result  goto startsearch-for-the-indicated-words: set the current word to be the word on the screen justbefore the selectionloop: if the text on the screen starting with the current wordmatches the indicated words  set the selection to text on the screen just comparedagainst  return from subroutine otherwise,  if the current word is the first word on the screen   set the current word to be the last word on the screen  otherwise,   change the current word to be the word on the screenbefore the current word  then,  if the current word is the first word in the selection   return from subroutine  otherwise,   goto looptype-text-on-the-screen: if words are selected on the screen  delete the words which are selected  leave the insertion point at the point where words weredeleted  type the text at the current insertion point otherwise,  type the text at the current insertion pointbuild-the-select-grammar: create a state with the word "select" create a large state which will hold all the words add a transition from the word "select" to the large state set the last-small-state variable to null set the last-word-in-large-state variable to null read the screen into a buffer parse the buffer into a series of words for each word in the buffer  look the word up in the dictionary to get a speech model  if the word is not in the dictionary    try to create a speech model for this word by generating a pronunciation using text to speech synthesis rules    if no speech model can be created for this wordskip this wordset the last-small-state variable to nullset the last-word-in-large-state variable to nullcontinue with the next word in the buffer  then,  create a small state containing only this word  if the last-small-state variable is not null   add a transition from the last-small-state to this new state  set the last-small-state variable to be this newly created small state  if the last-word-in-large-state variable is not null   add a transition from the last-word-in-large-state to this new state  if the word is not in the large state   add the word to the large state   set the last-word-in-large-state variable to this new word   continue with the next word in the buffer  otherwise,   set the last-word-in-large-state variable to the existing occurrence of the word in the large buffer  continue with the next word in the buffer if there are no more words in the buffer  return from subroutineShort Term Speech Recognition Error Correctionstart: wait for speech recognize the speech remember the utterance in a four element  first-in-first-out (FIFO) queue if utterance is not "oops"  perform the indicated command or type the recognized    text  goto to startotherwise,  concatenate the results from the last four utterances in    the FIFO queue into a single long string  display a correction dialog box with two fields, thefirst field should be blank and the secondfield should contain the concatenated results  goto looploop:  wait for speech or another user action  if more than 2 seconds have elapsed since the contents   of the first field in the dialog have changed  recompute-the-correction  goto loop otherwise, if speech is detected and the speech recognized "press    OK" or the user clicks the mouse on the OK    button, or the user presses the enter key  if the contents of the first field in the dialog have    changed since the correction was last    recomputed   recompute-the-correction  then,  if there is a corrected utterance   update-the-original-document  then,  destroy the correction dialog  goto start otherwise, if speech is detected and the speech recognized "press    Cancel" or the user clicks the mouse on the    Cancel button, or the user presses the escape    key  destroy the correction dialog  goto start otherwise, if speech is detected  recognize the speech  enter the recognized text into the first field of the    dialog  record that the first field of the dialog has changed  goto loop otherwise, if the user starts typing  enter the typed keystrokes into the first field of the    dialog    record that the first field of the dialog has changed    goto loop   otherwise,    goto loop update-the-original-document:  find the corrected utterance in the original document  remove the original text of the corrected utterance  replace the original text with the corrected text  return from subroutine recompute-the-correction:  read the contents of the first field of the dialog into a    buffer  parse the buffer into a series of words  for each word in the buffer   look the word up in the dictionary to get a speech model   if the word is not in the dictionary    try to create a speech model for this word bygenerating a pronunciation using text to speechsynthesis rules    if no speech model can be created for this worddisplay an "unknown word" error to the userreturn from subroutine   otherwise,    remember these words as the target words  then,  for each utterance in the FIFO queue   compute-a-possible-correction for this utterance and thetarget words   record the score of this possible correction and thecorrection itself  then,  compute the maximum score of all computed possiblecorrections  if the maximum score is zero   display "utterance can not be corrected" error to theuser   return from subroutine  otherwise,  remember the highest scoring computed possiblecorrection as the corrected utterance  concatenate the results from the last four utterances inthe FIFO queue into a single long string  replace the results for the corrected utterance with thecomputed possible correction  replace the second field with the corrected concatenatedstring  highlight the words in the corrected results whichcorrespond to the words in the first field ofthe dialog box  return from subroutinecompute-a-possible-correction: create-a-correction-grammar using the utterance and thetarget words recognize the utterance against the correction grammar look in the results for the target words if the target words do not appear in the results  return 0 otherwise,  record the results of the recognition as a possiblecorrection  return the score from the recognitioncreate-a-correction-grammar: set the last-target-word to NULL for every target word  create a small state containing the next target word  if the last-target-word is not NULL   add a transition from the last-target-word to this new    small state  set the last-target-word equal to the current target    word then, add a transition from the last-target-word to the state of    all words in the vocabulary set the last-original-word to NULL for every word in the original recognition results  create a small state containing the next word in the    original results  if the last-original-word is not NULL   add a transition from the last-original-word to this    new small state  then,  if the current word in the original recognition results    is not the same as the first target word   add the first target word to this state  then,  if there is only one target word   add a transition from the first target word in this    new small state to the state of all words in    the vocabulary  otherwise,  add a transition from the first target word in this    new small state to the small state created    earlier which contains the second target word  then,  set the last-original-word equal to the current word inthe original results  then,  add a transition from the last-original-word to the smallstate created earlier which contains the firsttarget word  return from subroutine______________________________________</pre>
    
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4688195">US4688195</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 28, 1983</td><td class="patent-data-table-td patent-date-value">Aug 18, 1987</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Natural-language interface generating system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4776016">US4776016</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 21, 1985</td><td class="patent-data-table-td patent-date-value">Oct 4, 1988</td><td class="patent-data-table-td ">Position Orientation Systems, Inc.</td><td class="patent-data-table-td ">Voice control system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4783803">US4783803</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 12, 1985</td><td class="patent-data-table-td patent-date-value">Nov 8, 1988</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Speech recognition apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4827520">US4827520</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 16, 1987</td><td class="patent-data-table-td patent-date-value">May 2, 1989</td><td class="patent-data-table-td ">Prince Corporation</td><td class="patent-data-table-td ">Voice actuated control system for use in a vehicle</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4829576">US4829576</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 21, 1986</td><td class="patent-data-table-td patent-date-value">May 9, 1989</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Voice recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4833712">US4833712</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 29, 1985</td><td class="patent-data-table-td patent-date-value">May 23, 1989</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Automatic generation of simple Markov model stunted baseforms for words in a vocabulary</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4837831">US4837831</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 15, 1986</td><td class="patent-data-table-td patent-date-value">Jun 6, 1989</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Method for creating and using multiple-word sound models in speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4866778">US4866778</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 11, 1986</td><td class="patent-data-table-td patent-date-value">Sep 12, 1989</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Interactive speech recognition apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4903305">US4903305</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 23, 1989</td><td class="patent-data-table-td patent-date-value">Feb 20, 1990</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Method for representing word models for use in speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4914703">US4914703</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 5, 1986</td><td class="patent-data-table-td patent-date-value">Apr 3, 1990</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Method for deriving acoustic models for use in speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4914704">US4914704</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 30, 1984</td><td class="patent-data-table-td patent-date-value">Apr 3, 1990</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Text editor for speech input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4931950">US4931950</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 25, 1988</td><td class="patent-data-table-td patent-date-value">Jun 5, 1990</td><td class="patent-data-table-td ">Electric Power Research Institute</td><td class="patent-data-table-td ">Multimedia interface and method for computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4962535">US4962535</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 12, 1990</td><td class="patent-data-table-td patent-date-value">Oct 9, 1990</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Antitumor, anticarcinogenic agents</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4984177">US4984177</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 1, 1989</td><td class="patent-data-table-td patent-date-value">Jan 8, 1991</td><td class="patent-data-table-td ">Advanced Products And Technologies, Inc.</td><td class="patent-data-table-td ">Voice language translator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5027406">US5027406</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 6, 1988</td><td class="patent-data-table-td patent-date-value">Jun 25, 1991</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Method for interactive speech recognition and training</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5036538">US5036538</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 22, 1989</td><td class="patent-data-table-td patent-date-value">Jul 30, 1991</td><td class="patent-data-table-td ">Telephonics Corporation</td><td class="patent-data-table-td ">Multi-station voice recognition and processing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5086472">US5086472</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 12, 1990</td><td class="patent-data-table-td patent-date-value">Feb 4, 1992</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Continuous speech recognition apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5095508">US5095508</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 16, 1990</td><td class="patent-data-table-td patent-date-value">Mar 10, 1992</td><td class="patent-data-table-td ">Ricoh Company, Ltd.</td><td class="patent-data-table-td ">Identification of voice pattern</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5127055">US5127055</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 11, 1991</td><td class="patent-data-table-td patent-date-value">Jun 30, 1992</td><td class="patent-data-table-td ">Kurzweil Applied Intelligence, Inc.</td><td class="patent-data-table-td ">Speech recognition apparatus &amp; method having dynamic reference pattern adaptation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5202952">US5202952</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 22, 1990</td><td class="patent-data-table-td patent-date-value">Apr 13, 1993</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Large-vocabulary continuous speech prefiltering and processing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5231670">US5231670</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 19, 1992</td><td class="patent-data-table-td patent-date-value">Jul 27, 1993</td><td class="patent-data-table-td ">Kurzweil Applied Intelligence, Inc.</td><td class="patent-data-table-td ">Voice controlled system and method for generating text from a voice controlled input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5377303">US5377303</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 9, 1993</td><td class="patent-data-table-td patent-date-value">Dec 27, 1994</td><td class="patent-data-table-td ">Articulate Systems, Inc.</td><td class="patent-data-table-td ">Enable voice utterances to control window elements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5428707">US5428707</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 13, 1992</td><td class="patent-data-table-td patent-date-value">Jun 27, 1995</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Apparatus and methods for training speech recognition systems and their users and otherwise improving speech recognition performance</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Dale Evans, "<a href='http://scholar.google.com/scholar?q="Talking+to+the+Bug%2C"'>Talking to the Bug,</a>" Microcad News, pp. 58-61, Mar. 1989.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Dale Evans, Talking to the Bug, Microcad News, pp. 58 61, Mar. 1989.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Itou K et al, Contin. Speech Recog. by Context Dependent Phonetic HMM, ICASSP, pp. I 21 to I 24, Mar. 1992.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Itou K et al, Contin. Speech Recog. by Context-Dependent Phonetic HMM, ICASSP, pp. I-21 to I-24, Mar. 1992.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Mandel, Mark A. et al., "<a href='http://scholar.google.com/scholar?q="A+Commercial+Large-Vocabulary+Discrete+Speech+Recognition+System%3A+DragonDictate%2C"'>A Commercial Large-Vocabulary Discrete Speech Recognition System: DragonDictate,</a>"Language and Speech, vol. 35 (1, 2) (1992), pp. 237-246.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Mandel, Mark A. et al., A Commercial Large Vocabulary Discrete Speech Recognition System: DragonDictate, Language and Speech, vol. 35 (1, 2) (1992), pp. 237 246.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Patent Application "<a href='http://scholar.google.com/scholar?q="Continuous+Speech+Recognition+of+Text+and+Commands%2C"'>Continuous Speech Recognition of Text and Commands,</a>" Ser. No. 08/559,207, Joel M. Gould et al., filed Nov. 13, 1995.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">U.S. Patent Application Continuous Speech Recognition of Text and Commands, Ser. No. 08/559,207, Joel M. Gould et al., filed Nov. 13, 1995.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Patent Application, "<a href='http://scholar.google.com/scholar?q="Apparatuses+and+Methods+for+Training+ad+Operating+Speech+Recognition+Systems"'>Apparatuses and Methods for Training ad Operating Speech Recognition Systems</a>" Joel. M. Gould et al., filed Feb. 1, 1995.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Patent Application, "<a href='http://scholar.google.com/scholar?q="Speech+Recognition%2C"'>Speech Recognition,</a>" Ser. No. 08/521,543, Gregory J. Gadbois, filed Aug. 30, 1995.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">U.S. Patent Application, "<a href='http://scholar.google.com/scholar?q="Speech+Recognition%2C"'>Speech Recognition,</a>" Ser. No. 08/559,190, Gregory J. Gadbois, filed Nov. 13, 1995.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">U.S. Patent Application, Apparatuses and Methods for Training ad Operating Speech Recognition Systems Joel. M. Gould et al., filed Feb. 1, 1995.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">U.S. Patent Application, Speech Recognition, Ser. No. 08/521,543, Gregory J. Gadbois, filed Aug. 30, 1995.</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">U.S. Patent Application, Speech Recognition, Ser. No. 08/559,190, Gregory J. Gadbois, filed Nov. 13, 1995.</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6023678">US6023678</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 27, 1998</td><td class="patent-data-table-td patent-date-value">Feb 8, 2000</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Using TTS to fill in for missing dictation audio</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6092044">US6092044</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 28, 1997</td><td class="patent-data-table-td patent-date-value">Jul 18, 2000</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Pronunciation generation in speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6112173">US6112173</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 1, 1998</td><td class="patent-data-table-td patent-date-value">Aug 29, 2000</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Pattern recognition device using tree structure data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6167377">US6167377</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 28, 1997</td><td class="patent-data-table-td patent-date-value">Dec 26, 2000</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Speech recognition language models</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6195635">US6195635</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 13, 1998</td><td class="patent-data-table-td patent-date-value">Feb 27, 2001</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">User-cued speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6195636">US6195636</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 19, 1999</td><td class="patent-data-table-td patent-date-value">Feb 27, 2001</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Speech recognition over packet networks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6210170">US6210170</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 15, 1998</td><td class="patent-data-table-td patent-date-value">Apr 3, 2001</td><td class="patent-data-table-td ">Steven M. Sorensen</td><td class="patent-data-table-td ">Method for teaching in a screen-saver environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6257740">US6257740</a></td><td class="patent-data-table-td patent-date-value">Feb 11, 2000</td><td class="patent-data-table-td patent-date-value">Jul 10, 2001</td><td class="patent-data-table-td ">James W Gibboney, Jr.</td><td class="patent-data-table-td ">Lamp for use in light strings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6347300">US6347300</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 29, 2000</td><td class="patent-data-table-td patent-date-value">Feb 12, 2002</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Speech correction apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6385582">US6385582</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 2, 2000</td><td class="patent-data-table-td patent-date-value">May 7, 2002</td><td class="patent-data-table-td ">Pioneer Corporation</td><td class="patent-data-table-td ">Man-machine system equipped with speech recognition device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6400289">US6400289</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 14, 2000</td><td class="patent-data-table-td patent-date-value">Jun 4, 2002</td><td class="patent-data-table-td ">Hughes Electronics Corporation</td><td class="patent-data-table-td ">System and method for performing lossless data compression and decompression</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6453292">US6453292</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 28, 1998</td><td class="patent-data-table-td patent-date-value">Sep 17, 2002</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Command boundary identifier for conversational natural language</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6492917">US6492917</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 31, 2001</td><td class="patent-data-table-td patent-date-value">Dec 10, 2002</td><td class="patent-data-table-td ">Hughes Electronics Corporation</td><td class="patent-data-table-td ">System and method for implementation of the YK lossless data compression algorithm using a modular computational architecture</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6581033">US6581033</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 19, 1999</td><td class="patent-data-table-td patent-date-value">Jun 17, 2003</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">System and method for correction of speech recognition mode errors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6601027">US6601027</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 15, 1998</td><td class="patent-data-table-td patent-date-value">Jul 29, 2003</td><td class="patent-data-table-td ">Scansoft, Inc.</td><td class="patent-data-table-td ">Position manipulation in speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6839668">US6839668</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 13, 2001</td><td class="patent-data-table-td patent-date-value">Jan 4, 2005</td><td class="patent-data-table-td ">Koninklijke Philips Electronics N.V.</td><td class="patent-data-table-td ">Store speech, select vocabulary to recognize word</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6912498">US6912498</a></td><td class="patent-data-table-td patent-date-value">May 2, 2001</td><td class="patent-data-table-td patent-date-value">Jun 28, 2005</td><td class="patent-data-table-td ">Scansoft, Inc.</td><td class="patent-data-table-td ">Error correction in speech recognition by correcting text around selected area</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6934682">US6934682</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 1, 2001</td><td class="patent-data-table-td patent-date-value">Aug 23, 2005</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Processing speech recognition errors in an embedded speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6973428">US6973428</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 24, 2001</td><td class="patent-data-table-td patent-date-value">Dec 6, 2005</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and method for searching, analyzing and displaying text transcripts of speech after imperfect speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6990445">US6990445</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 17, 2001</td><td class="patent-data-table-td patent-date-value">Jan 24, 2006</td><td class="patent-data-table-td ">Xl8 Systems, Inc.</td><td class="patent-data-table-td ">System and method for speech recognition and transcription</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7027985">US7027985</a></td><td class="patent-data-table-td patent-date-value">Sep 4, 2001</td><td class="patent-data-table-td patent-date-value">Apr 11, 2006</td><td class="patent-data-table-td ">Koninklijke Philips Electronics, N.V.</td><td class="patent-data-table-td ">Speech recognition method with a replace command</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7085716">US7085716</a></td><td class="patent-data-table-td patent-date-value">Oct 26, 2000</td><td class="patent-data-table-td patent-date-value">Aug 1, 2006</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Speech recognition using word-in-phrase command</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7149970">US7149970</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 23, 2000</td><td class="patent-data-table-td patent-date-value">Dec 12, 2006</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for filtering and selecting from a candidate list generated by a stochastic input method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7315818">US7315818</a></td><td class="patent-data-table-td patent-date-value">May 11, 2005</td><td class="patent-data-table-td patent-date-value">Jan 1, 2008</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Error correction in speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7444286">US7444286</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 5, 2004</td><td class="patent-data-table-td patent-date-value">Oct 28, 2008</td><td class="patent-data-table-td ">Roth Daniel L</td><td class="patent-data-table-td ">Speech recognition using re-utterance recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7467089">US7467089</a></td><td class="patent-data-table-td patent-date-value">Dec 5, 2004</td><td class="patent-data-table-td patent-date-value">Dec 16, 2008</td><td class="patent-data-table-td ">Roth Daniel L</td><td class="patent-data-table-td ">Combined speech and handwriting recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7505911">US7505911</a></td><td class="patent-data-table-td patent-date-value">Dec 5, 2004</td><td class="patent-data-table-td patent-date-value">Mar 17, 2009</td><td class="patent-data-table-td ">Roth Daniel L</td><td class="patent-data-table-td ">Combined speech recognition and sound recording</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7526431">US7526431</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 2004</td><td class="patent-data-table-td patent-date-value">Apr 28, 2009</td><td class="patent-data-table-td ">Voice Signal Technologies, Inc.</td><td class="patent-data-table-td ">Speech recognition using ambiguous or phone key spelling and/or filtering</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7809574">US7809574</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 2004</td><td class="patent-data-table-td patent-date-value">Oct 5, 2010</td><td class="patent-data-table-td ">Voice Signal Technologies Inc.</td><td class="patent-data-table-td ">Word recognition using choice lists</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7974605">US7974605</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 26, 2007</td><td class="patent-data-table-td patent-date-value">Jul 5, 2011</td><td class="patent-data-table-td ">Gintz Richard A</td><td class="patent-data-table-td ">Personal communications processing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8447602">US8447602</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 22, 2004</td><td class="patent-data-table-td patent-date-value">May 21, 2013</td><td class="patent-data-table-td ">Nuance Communications Austria Gmbh</td><td class="patent-data-table-td ">System for speech recognition and correction, correction device and method for creating a lexicon of alternatives</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8478590">US8478590</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 2011</td><td class="patent-data-table-td patent-date-value">Jul 2, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Word-level correction of speech input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8494852">US8494852</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 27, 2010</td><td class="patent-data-table-td patent-date-value">Jul 23, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Word-level correction of speech input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8543404">US8543404</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 7, 2008</td><td class="patent-data-table-td patent-date-value">Sep 24, 2013</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Proactive completion of input fields for automated voice enablement of a web page</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20070276586">US20070276586</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 25, 2007</td><td class="patent-data-table-td patent-date-value">Nov 29, 2007</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Method of setting a navigation terminal for a destination and an apparatus therefor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090254347">US20090254347</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 7, 2008</td><td class="patent-data-table-td patent-date-value">Oct 8, 2009</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Proactive completion of input fields for automated voice enablement of a web page</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110166851">US20110166851</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 27, 2010</td><td class="patent-data-table-td patent-date-value">Jul 7, 2011</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Word-Level Correction of Speech Input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120022868">US20120022868</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 2011</td><td class="patent-data-table-td patent-date-value">Jan 26, 2012</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Word-Level Correction of Speech Input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN100458913C?cl=en">CN100458913C</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 24, 2005</td><td class="patent-data-table-td patent-date-value">Feb 4, 2009</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Phonic proving method for speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001084535A2?cl=en">WO2001084535A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 2, 2001</td><td class="patent-data-table-td patent-date-value">Nov 8, 2001</td><td class="patent-data-table-td ">David Abrahams</td><td class="patent-data-table-td ">Error correction in speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2002021510A1?cl=en">WO2002021510A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 24, 2001</td><td class="patent-data-table-td patent-date-value">Mar 14, 2002</td><td class="patent-data-table-td ">Koninkl Philips Electronics Nv</td><td class="patent-data-table-td ">Speech recognition method with a replace command</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2002035519A1?cl=en">WO2002035519A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 26, 2001</td><td class="patent-data-table-td patent-date-value">May 2, 2002</td><td class="patent-data-table-td ">Even Stijn Van</td><td class="patent-data-table-td ">Speech recognition using word-in-phrase command</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2005006307A1?cl=en">WO2005006307A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 9, 2004</td><td class="patent-data-table-td patent-date-value">Jan 20, 2005</td><td class="patent-data-table-td ">Joshua D Ky</td><td class="patent-data-table-td ">System and method for speech recognition and transcription</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2007047587A2?cl=en">WO2007047587A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 13, 2006</td><td class="patent-data-table-td patent-date-value">Apr 26, 2007</td><td class="patent-data-table-td ">Cheng Yan Ming</td><td class="patent-data-table-td ">Method and device for recognizing human intent</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2011102842A1?cl=en">WO2011102842A1</a></td><td class="patent-data-table-td patent-date-value">Feb 22, 2010</td><td class="patent-data-table-td patent-date-value">Aug 25, 2011</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Online maximum-likelihood mean and variance normalization for speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2011149465A1?cl=en">WO2011149465A1</a></td><td class="patent-data-table-td patent-date-value">May 27, 2010</td><td class="patent-data-table-td patent-date-value">Dec 1, 2011</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Efficient exploitation of model complementariness by low confidence re-scoring in automatic speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2013169232A1?cl=en">WO2013169232A1</a></td><td class="patent-data-table-td patent-date-value">May 8, 2012</td><td class="patent-data-table-td patent-date-value">Nov 14, 2013</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Differential acoustic model representation and linear transform-based adaptation for efficient user profile update techniques in automatic speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2014003748A1?cl=en">WO2014003748A1</a></td><td class="patent-data-table-td patent-date-value">Jun 28, 2012</td><td class="patent-data-table-td patent-date-value">Jan 3, 2014</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Meta-data inputs to front end processing for automatic speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2014018004A1?cl=en">WO2014018004A1</a></td><td class="patent-data-table-td patent-date-value">Jul 24, 2012</td><td class="patent-data-table-td patent-date-value">Jan 30, 2014</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Feature normalization inputs to front end processing for automatic speech recognition</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=j5FHBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S231000">704/231</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=j5FHBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S232000">704/232</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=j5FHBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S257000">704/257</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=j5FHBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S251000">704/251</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=j5FHBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704SE15044">704/E15.044</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=j5FHBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S258000">704/258</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=j5FHBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0015220000">G10L15/22</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=j5FHBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0015260000">G10L15/26</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=j5FHBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L2015/0631">G10L2015/0631</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=j5FHBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L15/22">G10L15/22</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G10L15/22</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Jan 21, 2010</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 13, 2009</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIMS 3 AND 10 ARE CANCELLED. CLAIM 1 IS DETERMINED TO BE PATENTABLE AS AMENDED. CLAIMS 2 AND 5-9,DEPENDENT ON AN AMENDED CLAIM, ARE DETERMINED TO BE PATENTABLE. CLAIMS 4 AND 11-33 WERE NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 8, 2009</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20070314</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 24, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG. STAMFORD BRANCH, CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:018160/0909</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20060331</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:018160/0909</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG. STAMFORD BRANCH,CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:18160/909</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 7, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG, STAMFORD BRANCH, CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:017435/0199</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20060331</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG, STAMFORD BRANCH,CONNECTICUT</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 13, 2006</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 6, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">NUANCE COMMUNICATIONS, INC., MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:SCANSOFT, INC.;REEL/FRAME:016851/0772</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20051017</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 8, 2002</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">L &amp; H HOLDINGS USA, INC., MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">MERGER;ASSIGNOR:DRAGON SYSTEMS, INC.;REEL/FRAME:013362/0732</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20000607</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SCANSOFT, INC., MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:L &amp; H HOLDINGS USA, INC.;REEL/FRAME:013362/0739</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20011212</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 11, 2002</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 11, 2002</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 5, 2002</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 15, 1996</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">DRAGON SYSTEMS, INC., MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GOULD, JOEL M.;REEL/FRAME:007820/0248</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19960208</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U2Kmq2RfOaYkpKsFyRLrfG4tOBwCg\u0026id=j5FHBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U3ABeKREaOfxE7isatuSksTsQx_Lw\u0026id=j5FHBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U1km3_H04uAQZ57F5NZQ3aZCl_NOQ","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Continuous_speech_recognition.pdf?id=j5FHBAABERAJ\u0026output=pdf\u0026sig=ACfU3U2QZROsoKDC5IDa3pRMNIyh3cDRZg"},"sample_url":"http://www.google.com/patents/reader?id=j5FHBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>