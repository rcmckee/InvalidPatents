<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US5701392 - Depth-first algebraic-codebook search for fast coding of speech - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Depth-first algebraic-codebook search for fast coding of speech"><meta name="DC.contributor" content="Jean-Pierre Adoul" scheme="inventor"><meta name="DC.contributor" content="Claude Laflamme" scheme="inventor"><meta name="DC.contributor" content="Universite De Sherbrooke" scheme="assignee"><meta name="DC.date" content="1995-7-31" scheme="dateSubmitted"><meta name="DC.description" content="A codebook is searched in view of encoding a sound signal. This codebook consists of a set of codevectors each of 40 positions and comprising N non-zero-amplitude pulses assignable to predetermined valid positions. To reduce the search complexity, a depth-first search is used which involves a tree structure with levels ordered from 1 through M. A path-building operation takes place at each level whereby a candidate path from the previous level is extended by choosing a predetermined number of new pulses and selecting valid positions for said new pulses in accordance with a given pulse-order rule and a given selection criterion. A path originated at the first level and extended by the path-building operations of subsequent levels determines the respective positions of the N non-zero-amplitude pulse of a candidate codevector. Use of a signal-based pulse-position likelihood estimate during the first few levels enable initial pulse-screening to start the search on favorable conditions. A selection criterion based on maximizing a ratio is used to assess the progress and to choose the best one among competing candidate codevectors."><meta name="DC.date" content="1997-12-23" scheme="issued"><meta name="DC.relation" content="EP:0138061:A1" scheme="references"><meta name="DC.relation" content="EP:0149724:A1" scheme="references"><meta name="DC.relation" content="EP:0342687:A2" scheme="references"><meta name="DC.relation" content="EP:0446817:A2" scheme="references"><meta name="DC.relation" content="EP:0514912:A2" scheme="references"><meta name="DC.relation" content="EP:0532225:A2" scheme="references"><meta name="DC.relation" content="EP:0545386:A2" scheme="references"><meta name="DC.relation" content="US:4401855" scheme="references"><meta name="DC.relation" content="US:4486899" scheme="references"><meta name="DC.relation" content="US:4520499" scheme="references"><meta name="DC.relation" content="US:4594687" scheme="references"><meta name="DC.relation" content="US:4625286" scheme="references"><meta name="DC.relation" content="US:4669120" scheme="references"><meta name="DC.relation" content="US:4677671" scheme="references"><meta name="DC.relation" content="US:4680797" scheme="references"><meta name="DC.relation" content="US:4710959" scheme="references"><meta name="DC.relation" content="US:4720861" scheme="references"><meta name="DC.relation" content="US:4724535" scheme="references"><meta name="DC.relation" content="US:4742550" scheme="references"><meta name="DC.relation" content="US:4764963" scheme="references"><meta name="DC.relation" content="US:4771465" scheme="references"><meta name="DC.relation" content="US:4797925" scheme="references"><meta name="DC.relation" content="US:4797926" scheme="references"><meta name="DC.relation" content="US:4799261" scheme="references"><meta name="DC.relation" content="US:4811398" scheme="references"><meta name="DC.relation" content="US:4815134" scheme="references"><meta name="DC.relation" content="US:4817157" scheme="references"><meta name="DC.relation" content="US:4821324" scheme="references"><meta name="DC.relation" content="US:4858115" scheme="references"><meta name="DC.relation" content="US:4860355" scheme="references"><meta name="DC.relation" content="US:4864620" scheme="references"><meta name="DC.relation" content="US:4868867" scheme="references"><meta name="DC.relation" content="US:4873723" scheme="references"><meta name="DC.relation" content="US:4964169" scheme="references"><meta name="DC.relation" content="US:4991214" scheme="references"><meta name="DC.relation" content="US:5097508" scheme="references"><meta name="DC.relation" content="US:5193140" scheme="references"><meta name="DC.relation" content="US:5293449" scheme="references"><meta name="DC.relation" content="US:5307441" scheme="references"><meta name="DC.relation" content="US:5457783" scheme="references"><meta name="DC.relation" content="US:5667340" scheme="references"><meta name="DC.relation" content="WO:1990000381:A1" scheme="references"><meta name="DC.relation" content="WO:1991013432:A1" scheme="references"><meta name="citation_reference" content="&quot;8 kbits/s Speech Coder with Pitch Adaptive Vector Quantizer&quot; S. Iai and K. Irie, ICASSP 1986, Tokyo, vol. 3, Apr. 1986, pp. 1697-1700."><meta name="citation_reference" content="&quot;A comparison of some algebraic structures for CELP coding of speech&quot; J-P. Adoul et al. ICASSP 87 Proceedings, Apr. 6-9, 1987, Dallas, Texas pp. 1953-1956."><meta name="citation_reference" content="&quot;A robust 16 Kbits/s vector adaptive predictive coder for mobile communications&quot; A. Le Guyader et al. ICASSP 86 Proceedings, Apr. 7-11, 1986, Tokyo, Japan pp. 857-860."><meta name="citation_reference" content="&quot;Algorithme de quantification vectorielle spherique a partir du reseau de Gosset d&#39;ordre&quot; C. Lamblin et J.P. Adoul , Annales des Telecommunications, 1988, vol. 43, No. 1-2, pp. 172-186."><meta name="citation_reference" content="&quot;Coding of Speech at 8 kbit/s using Conjugate-structure Algebraic-code-excited Linear-Predictive (CS-ACELP) Coding&quot; Study Group 15 contrib., Int. Telecom. Union Jun. 1995, pp. 1-43."><meta name="citation_reference" content="&quot;Fast CELP coding based on algebraic codes&quot; J-P. Adoul et al. ICASSP 87 Proceedings Apr. 6-9, 1987, Dallas Texas pp. 1957-1960."><meta name="citation_reference" content="&quot;Fast Methods for Code Search in CELP&quot; M.E. Ahmed and M.I. Al-Suwaiyel, IEEE Transactions on Speech and Audio Processing, 1993, vol. 1, No. 3, New York, pp. 315-325."><meta name="citation_reference" content="&quot;Multipulse excitation codebook design and fast search methods for CELP speech coding&quot; F.F. Tzeng IEEE Glob. Telecom. Conf. &amp; Exhib., No. 28-Dec. 1, 88 Hollywood Fla, pp. 0590-0594."><meta name="citation_reference" content="8 kbits/s Speech Coder with Pitch Adaptive Vector Quantizer S. Iai and K. Irie, ICASSP 1986, Tokyo, vol. 3, Apr. 1986, pp. 1697 1700."><meta name="citation_reference" content="A comparison of some algebraic structures for CELP coding of speech J P. Adoul et al. ICASSP 87 Proceedings, Apr. 6 9, 1987, Dallas, Texas pp. 1953 1956."><meta name="citation_reference" content="A robust 16 Kbits/s vector adaptive predictive coder for mobile communications A. Le Guyader et al. ICASSP 86 Proceedings, Apr. 7 11, 1986, Tokyo, Japan pp. 857 860."><meta name="citation_reference" content="Abstract of &quot;Low delay speech coding&quot;, Cuperman, et al., Journal Speech Communication, vol. 12, No. 2, Netherlands, Jun. 1993, pp. 193-204."><meta name="citation_reference" content="Abstract of Low delay speech coding , Cuperman, et al., Journal Speech Communication, vol. 12, No. 2, Netherlands, Jun. 1993, pp. 193 204."><meta name="citation_reference" content="Algorithme de quantification vectorielle sph e rique a partir du r e seau de Gosset d ordre C. Lamblin et J.P. Adoul , Annales des T e l e communications, 1988, vol. 43, No. 1 2, pp. 172 186."><meta name="citation_reference" content="Coding of Speech at 8 kbit/s using Conjugate structure Algebraic code excited Linear Predictive (CS ACELP) Coding Study Group 15 contrib., Int. Telecom. Union Jun. 1995, pp. 1 43."><meta name="citation_reference" content="Fast CELP coding based on algebraic codes J P. Adoul et al. ICASSP 87 Proceedings Apr. 6 9, 1987, Dallas Texas pp. 1957 1960."><meta name="citation_reference" content="Fast Methods for Code Search in CELP M.E. Ahmed and M.I. Al Suwaiyel, IEEE Transactions on Speech and Audio Processing, 1993, vol. 1, No. 3, New York, pp. 315 325."><meta name="citation_reference" content="Leflamme, et al., &quot;On Reducing Computational Complexity of Codebook Search in CELP Coder Through the Use of Algebraic Codes&quot;, Proceedings of the IEEE ICASSP 1990, pp. 177-180."><meta name="citation_reference" content="Leflamme, et al., On Reducing Computational Complexity of Codebook Search in CELP Coder Through the Use of Algebraic Codes , Proceedings of the IEEE ICASSP 1990, pp. 177 180."><meta name="citation_reference" content="Multipulse excitation codebook design and fast search methods for CELP speech coding F.F. Tzeng IEEE Glob. Telecom. Conf. &amp; Exhib., No. 28 Dec. 1, 88 Hollywood Fla, pp. 0590 0594."><meta name="citation_patent_number" content="US:5701392"><meta name="citation_patent_application_number" content="US:08/509,525"><link rel="canonical" href="http://www.google.com/patents/US5701392"/><meta property="og:url" content="http://www.google.com/patents/US5701392"/><meta name="title" content="Patent US5701392 - Depth-first algebraic-codebook search for fast coding of speech"/><meta name="description" content="A codebook is searched in view of encoding a sound signal. This codebook consists of a set of codevectors each of 40 positions and comprising N non-zero-amplitude pulses assignable to predetermined valid positions. To reduce the search complexity, a depth-first search is used which involves a tree structure with levels ordered from 1 through M. A path-building operation takes place at each level whereby a candidate path from the previous level is extended by choosing a predetermined number of new pulses and selecting valid positions for said new pulses in accordance with a given pulse-order rule and a given selection criterion. A path originated at the first level and extended by the path-building operations of subsequent levels determines the respective positions of the N non-zero-amplitude pulse of a candidate codevector. Use of a signal-based pulse-position likelihood estimate during the first few levels enable initial pulse-screening to start the search on favorable conditions. A selection criterion based on maximizing a ratio is used to assess the progress and to choose the best one among competing candidate codevectors."/><meta property="og:title" content="Patent US5701392 - Depth-first algebraic-codebook search for fast coding of speech"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("XzDuU-KoGYfesAT3s4CoCg"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("DEU"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("XzDuU-KoGYfesAT3s4CoCg"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("DEU"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us5701392?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US5701392"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=hdBEBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS5701392&amp;usg=AFQjCNFtWplW1mpvL8lfC-kJNfdYcLeoIA" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US5701392.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US5701392.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US5701392" style="display:none"><span itemprop="description">A codebook is searched in view of encoding a sound signal. This codebook consists of a set of codevectors each of 40 positions and comprising N non-zero-amplitude pulses assignable to predetermined valid positions. To reduce the search complexity, a depth-first search is used which involves a tree structure...</span><span itemprop="url">http://www.google.com/patents/US5701392?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US5701392 - Depth-first algebraic-codebook search for fast coding of speech</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US5701392 - Depth-first algebraic-codebook search for fast coding of speech" title="Patent US5701392 - Depth-first algebraic-codebook search for fast coding of speech"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US5701392 A</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 08/509,525</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Dec 23, 1997</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Jul 31, 1995</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Feb 23, 1990</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/CA2213740A1">CA2213740A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/CA2213740C">CA2213740C</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN1114900C">CN1114900C</a>, </span><span class="patent-bibdata-value"><a href="/patents/CN1181151A">CN1181151A</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE19609170A1">DE19609170A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/DE19609170B4">DE19609170B4</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0813736A1">EP0813736A1</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0813736B1">EP0813736B1</a>, </span><span class="patent-bibdata-value"><a href="/patents/WO1996028810A1">WO1996028810A1</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">08509525, </span><span class="patent-bibdata-value">509525, </span><span class="patent-bibdata-value">US 5701392 A, </span><span class="patent-bibdata-value">US 5701392A, </span><span class="patent-bibdata-value">US-A-5701392, </span><span class="patent-bibdata-value">US5701392 A, </span><span class="patent-bibdata-value">US5701392A</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Jean-Pierre+Adoul%22">Jean-Pierre Adoul</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Claude+Laflamme%22">Claude Laflamme</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Universite+De+Sherbrooke%22">Universite De Sherbrooke</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US5701392.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5701392.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US5701392.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (43),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (20),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (72),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (16),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (7)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/5701392&usg=AFQjCNG5Rz2H_GdsV2WkVEhsE9igCfCuUw">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D5701392&usg=AFQjCNH0DX8pouR54o_dsm820ngHthLHUg">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D5701392A%26KC%3DA%26FT%3DD&usg=AFQjCNHYr2fRfHaV7sYAZpZU0Yb8XGwDXQ">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54243472" lang="EN" load-source="patent-office">Depth-first algebraic-codebook search for fast coding of speech</invention-title></span><br><span class="patent-number">US 5701392 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA37709625" lang="EN" load-source="patent-office"> <div class="abstract">A codebook is searched in view of encoding a sound signal. This codebook consists of a set of codevectors each of 40 positions and comprising N non-zero-amplitude pulses assignable to predetermined valid positions. To reduce the search complexity, a depth-first search is used which involves a tree structure with levels ordered from 1 through M. A path-building operation takes place at each level whereby a candidate path from the previous level is extended by choosing a predetermined number of new pulses and selecting valid positions for said new pulses in accordance with a given pulse-order rule and a given selection criterion. A path originated at the first level and extended by the path-building operations of subsequent levels determines the respective positions of the N non-zero-amplitude pulse of a candidate codevector. Use of a signal-based pulse-position likelihood estimate during the first few levels enable initial pulse-screening to start the search on favorable conditions. A selection criterion based on maximizing a ratio is used to assess the progress and to choose the best one among competing candidate codevectors.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(6)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5701392-2.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5701392-2.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5701392-3.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5701392-3.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5701392-4.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5701392-4.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5701392-5.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5701392-5.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5701392-6.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5701392-6.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US5701392-7.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US5701392-7.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(31)</span></span></div><div class="patent-text"><div mxw-id="PCLM59457275" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is:</claim-statement> <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1. A method of encoding a sound signal, comprising the steps of:<div class="claim-text">providing a codebook circuit for forming a codebook including a set of codevectors A<sub>k</sub> each defining a plurality of different positions p and comprising N non-zero-amplitude pulses each assignable to predetermined valid positions p of the codevector;</div> <div class="claim-text">providing a device for conducting in said codebook a depth-first search involving a tree structure defining a number M of ordered levels, each level m being associated with a predetermined number N<sub>m</sub> of non-zero-amplitude pulses, N<sub>m</sub> ≧1, wherein the sum of said predetermined numbers associated with all said M levels is equal to the number N of the non-zero-amplitude pulses comprised in said codevectors, each level m of the tree structure being further associated with a path building operation, with a given pulse-order rule and with a given selection criterion;</div> <div class="claim-text">wherein:<div class="claim-text">in a level 1 of the tree structure, the associated path-building operation comprises the following substeps:<div class="claim-text">choosing a number N<sub>1</sub> of said N non-zero-amplitude pulses in relation to the associated pulse-order rule;</div> </div> <div class="claim-text">selecting at least one of the valid positions p of said N<sub>1</sub> non-zero-amplitude pulses in relation to the associated selection criterion to define at least one level-1 candidate path;</div> <div class="claim-text">in a level m of the tree structure, the associated path-building operation defines recursively a level-m candidate path by extending a level-(m-1) candidate path through the following substeps:<div class="claim-text">choosing N<sub>m</sub> of said non-zero-amplitude pulses not previously chosen in the course of building said level-(m-1) path in relation to the associated pulse-order rule;</div> <div class="claim-text">selecting at least one of the valid positions p of said N<sub>m</sub> non-zero-amplitude pulses in relation to the associated selection criterion to form at least one level-m candidate path; and</div> <div class="claim-text">wherein a level-M candidate path originated at a level-1 and extended during the path-building operations associated with subsequent levels of the tree structure determines the respective positions p of the N non-zero-amplitude pulses of a codevector and thereby defines a candidate codevector A<sub>k</sub>.</div> </div> </div> </div>
    </div>
    </div> <div class="claim"> <div num="2" class="claim">
      <div class="claim-text">2. A method of encoding a sound signal, comprising the steps of:<div class="claim-text">providing a codebook circuit for forming a codebook including a set of codevectors A<sub>k</sub> each defining a plurality of different positions p and comprising N non-zero-amplitude pulses each assignable to predetermined valid positions p of the codevector;</div> <div class="claim-text">providing a device for conducting in said codebook a depth-first search involving (a) a partition of the N non-zero-amplitude pulses into a number M of subsets each comprising at least one non-zero-amplitude pulse, and (b) a tree structure including nodes representative of the valid positions p of the N non-zero-amplitude pulses and defining a plurality of search levels each associated to one of the M subsets, each search level being further associated to a given pulse-ordering rule and to a given selection criterion;</div> <div class="claim-text">said depth-first search conducting step itself comprising the steps of:<div class="claim-text">in a first search level of the tree structure,<div class="claim-text">choosing at least one of said N non-zero-amplitude pulses in relation to ,the associated pulse-ordering rule to form the associated subset;</div> <div class="claim-text">selecting at least one of the valid positions p of said at least one non-zero-amplitude pulse in relation to the associated selection criterion to define at least one path through the nodes of the tree structure;</div> <div class="claim-text">in each subsequent search level of the tree structure,</div> </div> <div class="claim-text">choosing at least one of said non-zero-amplitude pulses not previously chosen in relation to the associated pulse-ordering rule to form the associated subset; and<div class="claim-text">selecting at least one of the valid positions p of said at least one non-zero-amplitude pulse of the associated subset in relation to the associated selection criterion to extend said at least one path through the nodes of the tree structure;</div> </div> <div class="claim-text">wherein each path defined at the first search level and extended during the subsequent search levels determines the respective positions p of the N non-zero-amplitude pulses of a codevector A<sub>k</sub> constituting a candidate codevector in view of encoding the sound signal.</div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3. A sound signal encoding method as recited in claim 2, wherein said at least one path comprises a plurality of paths, wherein said search levels of the tree structure include a last search level, and wherein said depth-first search conducting step comprises, in the last search level of the tree structure, the step of selecting in relation to the associated selection criterion one of the candidate codevectors A<sub>k</sub> defined by said paths in view of encoding the sound signal.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4. A sound signal encoding method as recited in claim 2, further comprising the step of deriving the predetermined valid positions p of the N non-zero-amplitude pulses in accordance with at least one interleaved single-pulse permutation design.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5. A sound signal encoding method as recited in claim 2, wherein, in each said subsequent search level of the tree structure, the selecting step comprises:<div class="claim-text">calculating a given mathematical ratio for each path defined by the pulse position(s) p selected in the former search level(s) and extended by each valid position p of said at least one pulse of the subset associated to said subsequent search level; and</div> <div class="claim-text">retaining the extended path defined by the pulse positions p that maximize said given ratio.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6. A sound signal encoding method as recited in claim 2, wherein, at the first search level of the tree structure, the choosing and selecting steps are carried out by:<div class="claim-text">calculating a pulse-position likelihood-estimate vector in relation to the sound signal; and</div> <div class="claim-text">selecting said at least one non-zero-amplitude pulse of the associated subset and said at least one valid position p thereof in relation to said pulse-position likelihood-estimate vector.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" class="claim">
      <div class="claim-text">7. A sound signal encoding method as recited in claim 6, wherein the step of calculating the pulse-position likelihood-estimate vector comprises the steps of:<div class="claim-text">processing the sound signal to produce a target signal X, a backward-filtered target signal D and a pitch-removed residual signal R'; and</div> <div class="claim-text">calculating the pulse-position likelihood-estimate vector B in response to at least one of said target signal X, backward-filtered target signal D and pitch-removed residual signal R'.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" class="claim">
      <div class="claim-text">8. A sound signal encoding method as recited in claim 7, wherein the step of calculating the pulse-position likelihood-estimate vector B in response to at least one of said target signal X, backward-filtered target signal D and pitch-removed residual signal R' comprises:<div class="claim-text">summing the backward-filtered target signal D in normalized form: ##EQU17## to the pitch-removed residual signal R' in normalized form: ##EQU18## to thereby obtain a pulse-position likelihood-estimate vector B of the form: ##EQU19## where β is a fixed constant.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9. A sound signal encoding method as recited in claim 8, wherein β is a fixed constant having a value situated between 0 and 1.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" class="claim">
      <div class="claim-text">10. A sound signal encoding method as recited in claim 9, wherein β is a fixed constant having a value of 1/2.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" class="claim">
      <div class="claim-text">11. A sound signal encoding method as recited in claim 2, wherein said N non-zero-amplitude pulses have respective indexes, and wherein, in each said subsequent search level of the tree structure, the step of choosing at least one of said non-zero-amplitude pulses not previously chosen in relation to the associated pulse-ordering function comprises laying out the indexes of the pulses not previously chosen on a circle and choosing said at least one non-zero-amplitude pulse in accordance with a clockwise sequence of the indexes starting at the right of the last non-zero-amplitude pulse selected in the former search level of the tree structure.</div>
    </div>
    </div> <div class="claim"> <div num="12" class="claim">
      <div class="claim-text">12. A system for encoding a sound signal, comprising:<div class="claim-text">a codebook including a set of codevectors A<sub>k</sub> each defining a plurality of different positions p and comprising N non-zero-amplitude pulses each assignable to predetermined valid positions p of the codevector;</div> <div class="claim-text">a device for conducting in said codebook a depth-first search involving (a) a partition of the N non-zero-amplitude pulses into a number M of subsets each comprising at least one non-zero-amplitude pulse, and (b) a tree structure including nodes representative of the valid positions p of the N non-zero-amplitude pulses and defining a plurality of search levels each associated to one of the M subsets, each search level being further associated to a given pulse-ordering rule and to a given selection criterion;</div> <div class="claim-text">said depth-first codebook search conducting device comprising:<div class="claim-text">for a first search level of the tree structure,<div class="claim-text">first means for choosing at least one of said N non-zero-amplitude pulses in relation to the associated pulse-ordering rule to form the associated subset;</div> <div class="claim-text">first means for selecting at least one of the valid positions p of said at least one non-zero-amplitude pulse in relation to the associated selection criterion to define at least one path through the nodes of the tree structure;</div> </div> <div class="claim-text">for each subsequent search level of the tree structure,<div class="claim-text">second means for choosing at least one of said non-zero-amplitude pulses not previously chosen in relation to the associated pulse-ordering function to form the associated subset; and</div> <div class="claim-text">second means for selecting, in said subsequent search level, at least one of the valid positions p of said at least one non-zero-amplitude pulse of the associated subset in relation to the associated selection criterion to extend said at least one path through the nodes of the tree structure;</div> <div class="claim-text">wherein each path defined at the first search level and extended during the subsequent search levels determines the respective positions p of the N non-zero-amplitude pulses of a codevector A<sub>k</sub> constituting a candidate codevector in view of encoding the sound signal.</div> </div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" class="claim">
      <div class="claim-text">13. A sound signal encoding system as recited in claim 12, wherein said at least one path comprises a plurality of paths, wherein said search levels of the tree structure include a last search level, and wherein said device comprises means for selecting, in the last search level of the tree structure and in relation to the associated selection criterion, one of the candidate codevectors A<sub>k</sub> defined by said paths in view of encoding the sound signal.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" class="claim">
      <div class="claim-text">14. A sound signal encoding system as recited in claim 12, further comprising means for deriving the predetermined valid positions p of the N non-zero-amplitude pulses in accordance with at least one interleaved single-pulse permutation design.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" class="claim">
      <div class="claim-text">15. A sound signal encoding system as recited in claim 12, wherein said second selecting means comprises:<div class="claim-text">means for calculating a given mathematical ratio for each path defined by the pulse position(s) p selected in the former search level(s) and extended by each valid position p of said at least one pulse of the subset associated to said subsequent search level; and</div> <div class="claim-text">means for retaining the extended path defined by the pulse positions p that maximize said given ratio.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" class="claim">
      <div class="claim-text">16. A sound signal encoding system as recited in claim 12, wherein the first choosing means and the first selecting means comprise:<div class="claim-text">means for calculating a pulse-position likelihood-estimate vector in relation to the sound signal; and</div> <div class="claim-text">means for selecting said at least one non-zero-amplitude pulse of the associated subset and said at least one valid position p thereof in relation to said pulse-position likelihood-estimate vector.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" class="claim">
      <div class="claim-text">17. A sound signal encoding system as recited in claim 16, wherein said means for calculating the pulse-position likelihood-estimate vector comprises:<div class="claim-text">means for processing the sound signal to produce a target signal X, a backward-filtered target signal D and a pitch-removed residual signal R'; and</div> <div class="claim-text">means for calculating the pulse-position likelihood-estimate vector B in response to at least one of said target signal X, backward-filtered target signal .D and pitch-removed residual signal R'.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" class="claim">
      <div class="claim-text">18. A sound signal encoding system as recited in claim 17, wherein said means for calculating the pulse-position likelihood-estimate vector B in response to at least one of said target signal X, backward-filtered target signal D and pitch-removed residual signal R' comprises:<div class="claim-text">means for summing the backward-filtered target signal D in normalized form: ##EQU20## to the pitch-removed residual signal R' in normalized form: ##EQU21## to thereby obtain a pulse-position likelihood-estimate vector B of the form: ##EQU22## where β is a fixed constant.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" class="claim">
      <div class="claim-text">19. A sound signal encoding system as recited in claim 18, wherein β is a fixed constant having a value situated between 0 and 1.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" class="claim">
      <div class="claim-text">20. A sound signal encoding system as recited in claim 19, wherein β is a fixed constant having a value of 1/2.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" class="claim">
      <div class="claim-text">21. A sound signal encoding system as recited in claim 12, wherein said N non-zero-amplitude pulses have respective indexes, and wherein said second choosing means comprises:<div class="claim-text">means for laying out the indexes of the pulses not previously chosen on a circle; and</div> <div class="claim-text">means for choosing said at least one non-zero-amplitude pulse in accordance with a clockwise sequence of the indexes starting at the right of the last non-zero-amplitude pulse selected in the former search level of the tree structure.</div> </div>
    </div>
    </div> <div class="claim"> <div num="22" class="claim">
      <div class="claim-text">22. A cellular communication system for servicing a large geographical area divided into a plurality of cells, comprising:<div class="claim-text">mobile transmitter/receiver units;</div> <div class="claim-text">cellular base stations respectively situated in said cells;</div> <div class="claim-text">means for controlling communication between the cellular base stations;</div> <div class="claim-text">a bidirectional wireless communication sub-system between each mobile unit situated in one cell and the cellular base station of said one cell, said bidirectional wireless communication sub-system comprising in both the mobile unit and the cellular base station (a) a transmitter including means for encoding a speech signal and means for transmitting the encoded speech signal, and (b) a receiver including means for receiving a transmitted encoded speech signal and means for decoding the received encoded speech signal;<div class="claim-text">wherein said speech signal encoding means comprises a device for conducting a depth-first search in a codebook in view of encoding a sound signal, wherein:</div> </div> <div class="claim-text">said codebook comprises a set of codevectors A<sub>k</sub> each defining a plurality of different positions p and comprising N non-zero-amplitude pulses each assignable to predetermined valid positions p of the codevector;</div> <div class="claim-text">said depth-first search involves (a) a partition of the N non-zero-amplitude pulses into a number M of subsets each comprising at least one non-zero-amplitude pulse, and (b) a tree structure including nodes representative of the valid positions p of the N non-zero-amplitude pulses and defining a plurality of search levels each associated to one of the M subsets, each search level being further associated to a given pulse-ordering rule and to a given selection criterion;</div> <div class="claim-text">said depth-first codebook search conducting device comprising:<div class="claim-text">for a first search level of the tree structure,<div class="claim-text">first means for choosing at least one of said N non-zero-amplitude pulses in relation to the associated pulse-ordering rule to form the associated subset;</div> <div class="claim-text">first means for selecting at least one of the valid positions p of said at least one non-zero-amplitude pulse in relation to the associated selection criterion to define at least one path through the nodes of the tree structure;</div> </div> <div class="claim-text">for each subsequent search level of the tree structure,<div class="claim-text">second means for choosing at least one of said non-zero-amplitude pulses not previously chosen in relation to the associated pulse-ordering function to form the associated subset; and</div> <div class="claim-text">second means for selecting, in said subsequent search level, at least one of the valid positions p of said at least one non-zero-amplitude pulse of the associated subset in relation to the associated selection criterion to extend said at least one path through the nodes of the tree structure;</div> <div class="claim-text">wherein each path defined at the first search level and extended during the subsequent search levels determines the respective positions p of the N non-zero-amplitude pulses of a codevector A<sub>k</sub> constituting a candidate codevector in view of encoding the sound signal.</div> </div> </div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="23" class="claim">
      <div class="claim-text">23. The cellular communication system of claim 22, wherein said at least one path comprises a plurality of paths, wherein said search levels of the tree structure include a last search level, and wherein said device comprises means for selecting, in the last search level of the tree structure and in relation to the associated selection criterion, one of the candidate codevectors A<sub>k</sub> defined by said paths in view of encoding the sound signal.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="24" class="claim">
      <div class="claim-text">24. The cellular communication system of claim 22, further comprising means for deriving the predetermined valid positions p of the N non-zero-amplitude pulses in accordance with at least one interleaved single-pulse permutation design.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" class="claim">
      <div class="claim-text">25. The cellular communication system of claim 22, wherein said second selecting means comprises:<div class="claim-text">means for calculating a given mathematical ratio for each path defined by the pulse position(s) p selected in the former search level(s) and extended by each valid position p of said at least one pulse of the subset associated to said subsequent search level; and</div> <div class="claim-text">means for retaining the extended path defined by the pulse positions p that maximize said given ratio.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="26" class="claim">
      <div class="claim-text">26. The cellular communication system of claim 22, wherein the first choosing means and the first selecting means comprise:<div class="claim-text">means for calculating a pulse-position likelihood-estimate vector in relation to the sound signal; and</div> <div class="claim-text">means for selecting said at least one non-zero-amplitude pulse of the associated subset and said at least one valid position p thereof in relation to said pulse-position likelihood-estimate vector.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="27" class="claim">
      <div class="claim-text">27. The cellular communication system of claim 26, wherein said means for calculating the pulse-position likelihood-estimate vector comprises:<div class="claim-text">means for processing the sound signal to produce a target signal X, a backward-filtered target signal D and a pitch-removed residual signal R'; and</div> <div class="claim-text">means for calculating the pulse-position likelihood-estimate vector B in response to at least one of said target signal X, backward-filtered target signal D and pitch-removed residual signal R'.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="28" class="claim">
      <div class="claim-text">28. The cellular communication system of claim 27, wherein said means for calculating the pulse-position likelihood-estimate vector B in response to at least one of said target signal X, backward-filtered target signal D and pitch-removed residual signal R' comprises:<div class="claim-text">means for summing the backward-filtered target signal D in normalized form: ##EQU23## to the pitch-removed residual signal R' in normalized form: ##EQU24## to thereby obtain an amplitude estimate vector B of the form: ##EQU25## where β is a fixed constant.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="29" class="claim">
      <div class="claim-text">29. The cellular communication system of claim 28, wherein β is a fixed constant having a value situated between 0 and 1.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="30" class="claim">
      <div class="claim-text">30. The cellular communication system of claim 29, wherein β is a fixed constant having a value of 1/2.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="31" class="claim">
      <div class="claim-text">31. The cellular communication system of claim 22, wherein said N non-zero-amplitude pulses have respective indexes, and wherein said second choosing means comprises:<div class="claim-text">means for laying out the indexes of the pulses not previously chosen on a circle; and</div> <div class="claim-text">means for choosing said at least one non-zero-amplitude pulse in accordance with a clockwise sequence of the indexes starting at the right of the last non-zero-amplitude pulse selected in the former search level of the tree structure.</div> </div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES67056030" lang="EN" load-source="patent-office" class="description">
    <heading>RELATED U.S. PATENT APPLICATION</heading> <p>This is a Continuation-In-Part of U.S. patent application Ser. No. 08/401,785 filed on Mar. 10, 1995 for an invention entitled "DEPTH-FIRST ALGEBRAIC-CODEBOOK SEARCH FOR FAST CODING OF SPEECH" which is a continuation in part of application Ser. No. 07/927,528 filed as PCT/CA90/00381 Nov. 6, 1990, U.S. Pat. No. 5,444,816.</p>
    <heading>BACKGROUND OF THE INVENTION</heading> <p>1. Field of the Invention</p>
    <p>The present invention relates to an improved technique for digitally encoding a sound signal, in particular but not exclusively a speech signal, in view of transmitting and synthesizing this sound signal.</p>
    <p>2. Brief Description of the Prior Art</p>
    <p>The demand for efficient digital speech encoding techniques with a good subjective quality/bit rate tradeoff is increasing for numerous applications such as voice transmission over satellites, landmobile, digital radio or packed network, voice storage, voice response and wireless telephony.</p>
    <p>One of the best prior art techniques capable of achieving a good quality/bit rate tradeoff is the so called Code Excited Linear Prediction (CELP) technique. According to this technique, the speech signal is sampled and processed in successive blocks of L samples (i.e. vectors), where L is some predetermined number. The CELP technique makes use of a codebook.</p>
    <p>A codebook, in the CELP context, is an indexed set of L-sample-long sequences which will be referred to as L-dimensional codevectors. The codebook comprises an index k ranging from 1 to M, where M represents the size of the codebook sometimes expressed as a number of bits b:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">M=2<sup>b</sup> </pre>
    
    <p>A codebook can be stored in a physical memory (e.g. a look-up table), or can refer to a mechanism for relating the index to a corresponding codevector (e.g. a formula).</p>
    <p>To synthesize speech according to the CELP technique, each block of speech samples is synthesized by filtering the appropriate codevector from the codebook through time varying filters modeling the spectral characteristics of the speech signal. At the encoder end, the synthetic output is computed for all or a subset of the codevectors from the codebook (codebook search). The retained codevector is the one producing the synthetic output which is the closest to the original speech signal according to a perceptually weighted distortion measure.</p>
    <p>A first type of codebooks are the so called "stochastic" codebooks. A drawback of these codebooks is that they often involve substantial physical storage. They are stochastic, i.e. random in the sense that the path from the index to the associated codevector involves look-up tables which are the result of randomly generated numbers or statistical techniques applied to large speech training sets. The size of stochastic codebooks tends to be limited by storage and/or search complexity.</p>
    <p>A second type of codebooks are the algebraic codebooks. By contrast with the stochastic codebooks, algebraic codebooks are not random and require no substantial storage. An algebraic codebook is a set of indexed codevectors of which the amplitudes and positions of the pulses of the k<sup>th</sup> codevector can be derived from a corresponding index k through a rule requiring no, or minimal, physical storage. Therefore, the size of algebraic codebooks is not limited by storage requirements. Algebraic codebooks can also be designed for efficient search.</p>
    <heading>OBJECTS OF THE INVENTION</heading> <p>An object of the present invention is therefore to provide a method and device for drastically reducing the complexity of the codebook search upon encoding a sound signal, these method and device being applicable to a large class of codebooks.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>More particularly, in accordance with the present invention, there is provided a method of conducting a depth-first search in a codebook in view of encoding a sound signal, wherein:</p>
    <p>the codebook comprises a set of codevectors A<sub>k</sub> each defining a plurality of different positions p and comprising N non-zero-amplitude pulses each assignable to predetermined valid positions p of the codevector;</p>
    <p>the depth-first search involves a tree structure defining a number M of ordered levels, each level m being associated with a predetermined number N<sub>m</sub> of at least one non-zero-amplitude pulse, wherein the sum of the predetermined numbers associated with all the M levels is equal to the number N of the non-zero-amplitude pulses comprised in the codevectors, each level m of the tree structure being further associated with a path building operation, with a given pulse-order rule and with a given selection criterion;</p>
    <p>the depth-first codebook search conducting method comprising the steps of:</p>
    <p>in a level 1 of the tree structure, the association path-building operation consists of:</p>
    <p>choosing N<sub>1</sub> of the N non-zero-amplitude pulses in relation to the associated pulse-order rule;</p>
    <p>selecting at least one of the valid positions p of the N<sub>1</sub> non-zero-amplitude pulses in relation to the associated selection criterion to define at least one level-1 candidate path;</p>
    <p>in a level m of the tree structure, the associated path-building operation defines recursively a level-m candidate path by extending a level-(m-1) candidate path through the following substeps:</p>
    <p>choosing N<sub>m</sub> of the non-zero-amplitude pulses not previously chosen in the course of building the level-(m-1) path in relation to the associated pulse-order rule;</p>
    <p>selecting at least one of the valid positions p of the N<sub>m</sub> non-zero-amplitude pulse in relation to the associated selection criterion to form at least one level-m candidate path;</p>
    <p>wherein a level-M candidate path originated at a level-1 and extended during the path-building operations associated with subsequent levels of the tree structure determines the respective positions p of the N non-zero-amplitude pulses of a codevector and thereby defines a candidate codevector A<sub>k</sub>.</p>
    <p>The present invention also relates to a device for conducting a depth-first search in a codebook in view of encoding a sound signal, wherein:</p>
    <p>the codebook comprises a set of codevectors A<sub>k</sub> each defining a plurality of different positions p and comprising N non-zero-amplitude pulses each assignable to predetermined valid positions p of the codevector;</p>
    <p>the depth-first search involves (a) a partition of the N non-zero-amplitude pulses into a number M of subsets each comprising at least one non-zero-amplitude pulse, and (b) a tree structure including nodes representative of the valid positions p of the N non-zero-amplitude pulses and defining a plurality of search levels each associated to one of the M subsets, each search level being further associated to a given pulse-ordering rule and to a given selection criterion;</p>
    <p>the depth-first codebook search conducting device comprising:</p>
    <p>for a first search level of the tree structure,</p>
    <p>first means for choosing at least one of the N non-zero-amplitude pulses in relation to the associated pulse-ordering rule to form the associated subset;</p>
    <p>first means for selecting at least one of the valid positions p of the at least one non-zero-amplitude pulse in relation to the associated selection criterion to define at least one path through the nodes of the tree structure;</p>
    <p>for each subsequent search level of the tree structure,</p>
    <p>second means for choosing at least one of the non-zero-amplitude pulses not previously chosen in relation to the associated pulse-ordering function to form the associated subset; and</p>
    <p>second means for selecting, in the subsequent search level, at least one of the valid positions p of the at least one non-zero-amplitude pulse of the associated subset in relation to the associated selection criterion to extend the at least one path through the nodes of the tree structure; wherein each path defined at the first search level and extended during the subsequent search levels determines the respective positions p of the N non-zero-amplitude pulses of a codevector A<sub>k</sub> constituting a candidate codevector in view of encoding the sound signal.</p>
    <p>The subject invention further relates to a cellular communication system for servicing a large geographical area divided into a plurality of cells, comprising:</p>
    <p>mobile transmitter/receiver units;</p>
    <p>cellular base stations respectively situated in the cells;</p>
    <p>means for controlling communication between the cellular base stations;</p>
    <p>a bidirectional wireless communication sub-system between each mobile unit situated in one cell and the cellular base station of the one cell, the bidirectional wireless communication sub-system comprising in both the mobile unit and the cellular base station (a) a transmitter including means for encoding a speech signal and means for transmitting the encoded speech signal, and (b) a receiver including means for receiving a transmitted encoded speech signal and means for decoding the received encoded speech signal;</p>
    <p>wherein the speech signal encoding means comprises a device for conducting a depth-first search in a codebook in view of encoding a sound signal, wherein:</p>
    <p>the codebook comprises a set of codevectors A<sub>k</sub> each defining a plurality of different positions p and comprising N non-zero-amplitude pulses each assignable to predetermined valid positions p of the codevector;</p>
    <p>the depth-first search involves (a) a partition of the N non-zero-amplitude pulses into a number M of subsets each comprising at least one non-zero-amplitude pulse, and (b) a tree structure including nodes representative of the valid positions p of the N non-zero-amplitude pulses and defining a plurality of search levels each associated to one of the M subsets, each search level being further associated to a given pulse-ordering rule and to a given selection criterion;</p>
    <p>the depth-first codebook search conducting device comprising:</p>
    <p>for a first search level of the tree structure,</p>
    <p>first means for choosing at least one of the N non-zero-amplitude pulses in relation to the associated pulse-ordering rule to form the associated subset;</p>
    <p>first means for selecting at least one of the valid positions p of the at least one non-zero-amplitude pulse in relation to the associated selection criterion to define at least one path through the nodes of the tree structure;</p>
    <p>for each subsequent search level of the tree structure,</p>
    <p>second means for choosing at least one of the non-zero-amplitude pulses not previously chosen in relation to the associated pulse-ordering function to form the associated subset; and</p>
    <p>second means for selecting, in the subsequent search level, at least one of the valid positions p of the at least one non-zero-amplitude pulse of the associated subset in relation to the associated selection criterion to extend the at least one path through the nodes of the tree structure;</p>
    <p>wherein each path defined at the first search level and extended during the subsequent search levels determines the respective positions p of the N non-zero-amplitude pulses of a codevector A<sub>k</sub> constituting a candidate codevector in view of encoding the sound signal.</p>
    <p>The objects, advantages and other features of the present invention will become more apparent upon reading of the following non restrictive description of preferred embodiments thereof, given by way of example only with reference to the accompanying drawings.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p>In the appended drawings:</p>
    <p>FIG. 1 is a schematic block diagram of a preferred embodiment of an encoding system in accordance with the present invention, comprising a pulse-position likelihood-estimator and an optimizing controller;</p>
    <p>FIG. 2 is a schematic block diagram of a decoding system associated to the encoding system of FIG. 1;</p>
    <p>FIG. 3 is a schematic representation of a plurality of nested loops used by the optimizing controller of the encoding system of FIG. 1 for computing optimum codevectors;</p>
    <p>FIG. 4a shows a tree structure to illustrate by way of an example some features of the "nested-loop search" technique of FIG. 3;</p>
    <p>FIG. 4b shows the tree structure of FIG. 4a when the processing at lower levels is conditioned on the performance exceeding some given threshold; this is a faster method of exploring the tree by focusing only on the most promising regions of that tree;</p>
    <p>FIG. 5 illustrates how the depth-first search technique is proceeding through a tree structure to some combinations of pulse positions; the example relates to a ten-pulse codebook of forty-positions codevectors designed according to an interleaved single-pulse permutations;</p>
    <p>FIG. 6 is a schematic flow chart showing operation of the pulse-position likelihood-estimator and an optimizing controller of FIG. 1; and</p>
    <p>FIG. 7 is a schematic block diagram illustrating the infrastructure of a typical cellular communication system.</p>
    <heading>DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS</heading> <p>Although application of the depth-first codebook searching method and device according to the invention to a cellular communication system is disclosed as a non limitative example in the present specification, it should be kept in mind that these method and device can be used with the same advantages in many other types of communication systems in which sound signal encoding is required.</p>
    <p>In a cellular communication system such as 1 (FIG. 7), a telecommunications service is provided over a large geographic area by dividing that large area into a number of smaller cells. Each cell has a cellular base station 2 for providing radio signalling channels, and audio and data channels.</p>
    <p>The radio signalling channels are utilized to page mobile radio telephones (mobile transmitter/receiver units) such as 3 within the limits of the cellular base station's coverage area (cell), and to place calls to other radio telephones 3 either inside or outside the base station's cell, or onto another network such as the Public Switched Telephone Network (PSTN) 4.</p>
    <p>Once a radio telephone 3 has successfully placed or received a call, an audio or data channel is set up with the cellular base station 2 corresponding to the cell in which the radio telephone 3 is situated, and communication between the base station 2 and radio telephone 3 occurs over that audio or data channel. The radio telephone 3 may also receive control or timing information over the signalling channel whilst a call is in progress.</p>
    <p>If a radio telephone 3 leaves a cell during a call and enters another cell, the radio telephone hands over the call to an available audio or data channel in the new cell. Similarly, if no call is in progress a control message is sent over the signalling channel such that the radio telephone 3 logs onto the base station 2 associated with the new cell. In this manner mobile communication over a wide geographical area is possible.</p>
    <p>The cellular communication system 1 further comprises a terminal 5 to control communication between the cellular base stations 2 and the PSTN 4, for example during a communication between a radio telephone 3 and the PSTN 4, or between a radio telephone 3 in a first cell and a radio telephone 3 in a second cell.</p>
    <p>Of course, a bidirectional wireless radio communication sub-system is required to establish communication between each radio telephone 3 situated in one cell and the cellular base station 2 of that cell. Such a bidirectional wireless radio communication system typically comprises in both the radio telephone 3 and the cellular base station 2 (a) a transmitter for encoding the speech signal and for transmitting the encoded speech signal through an antenna such as 6 or 7, and (b) a receiver for receiving a transmitted encoded speech signal through the same antenna 6 or 7 and for decoding the received encoded speech signal. As well known to those of ordinary skill in the art, voice encoding is required in order to reduce the bandwidth necessary to transmit speech across the bidirectional wireless radio communication system, i.e. between a radio telephone 3 and a base station 2.</p>
    <p>The aim of the present invention is to provide an efficient digital speech encoding technique with a good subjective quality/bit rate tradeoff for example for bidirectional transmission of speech signals between a cellular base station 2 and a radio telephone 3 through an audio or data channel. FIG. 1 is a schematic block diagram of a digital speech encoding device suitable for carrying out this efficient technique.</p>
    <p>The speech encoding system of FIG. 1 is the same encoding device as illustrated in FIG. 1 of U.S. parent patent application Ser. No. 07/927,528 to which a pulse position estimator 112 in accordance with the present invention has been added. U.S. parent patent application Ser. No. 07/927,528 was filed on Sep. 10, 1992 for an invention entitled "DYNAMIC CODEBOOK FOR EFFICIENT SPEECH CODING BASED ON ALGEBRAIC CODES".</p>
    <p>The analog input speech signal is sampled and block processed. It should be understood that the present invention is not limited to an application to speech signal. Encoding of other types of sound signal can also be contemplated.</p>
    <p>In the illustrated example, the block of input sample speech S (FIG. 1) comprises L consecutive samples. In the CELP literature, L is designated as the "subframe" length and is typically situated between 20 and 80. Also, the blocks of L-samples are referred to as L-dimensional vectors. Various L-dimensional vectors are produced in the course of the encoding procedure. A list of these vectors which appear on FIGS. 1 and 2, as well as a list of transmitted parameters is given hereinbelow:</p>
    <p>List of the main L-dimensional vectors:</p>
    <p>S Input speech vector;</p>
    <p>R' Pitch-removed residual vector;</p>
    <p>X Target vector;</p>
    <p>D Backward-filtered target vector;</p>
    <p>A<sub>k</sub> Codevector of index k from the algebraic codebook; and</p>
    <p>C<sub>k</sub> Innovation vector (filtered codevector).</p>
    <p>List of transmitted parameters:</p>
    <p>k Codevector index (input of the algebraic codebook);</p>
    <p>g Gain;</p>
    <p>STP Short term prediction parameters (defining A(z)); and</p>
    <p>LTP Long term prediction parameters (defining a pitch gain b and a pitch delay T).</p>
    <p>Decoding Principle</p>
    <p>It is believed preferable to describe first the speech decoding device of FIG. 2 illustrating the various steps carried out between the digital input (input of demultiplexer 205) and the output sampled speech (output of synthesis filter 204).</p>
    <p>The demultiplexer 205 extracts four different parameters from the binary information received from a digital input channel, namely the index k, the gain g, the short term prediction parameters STP, and the long term prediction parameters LTP. The current L-dimensional vector S of speech signal is synthesized on the basis of these four parameters as will be explained in the following description.</p>
    <p>The speech decoding device of FIG. 2 comprises a dynamic codebook 208 composed of an algebraic code generator 201 and an adaptive prefilter 202, an amplifier 206, an adder 207, a long term predictor 203, and a synthesis filter 204.</p>
    <p>In a first step, the algebraic code generator 201 produces a codevector A<sub>k</sub> in response to the index k.</p>
    <p>In a second step, the codevector A<sub>k</sub> is processed through an adaptive prefilter 202 supplied with the long term prediction parameters LTP to produce an output innovation vector C<sub>k</sub>. The purpose of the adaptive prefilter 202 is to dynamically control the frequency content of the output innovation vector C<sub>k</sub> so as to enhance speech quality, i.e. to reduce the audible distortion caused by frequencies annoying the human ear. Typical transfer functions F(z) for the adaptive prefilter 202 are given below: ##EQU1## F<sub>a</sub> (z) is a formant prefilter in which 0&lt;γ<sub>1</sub> &lt;γ<sub>2</sub> &lt;1 are constants. This prefilter enhances the formant regions and works very effectively especially at coding rate below 5 kbit/s.</p>
    <p>F<sub>b</sub> (z) is a pitch prefilter where T is the time varying pitch delay and b<sub>0</sub> is either constant or equal to the quantized long term pitch prediction parameter from the current or previous subframes. F<sub>b</sub> (z) is very effective to enhance pitch harmonic frequencies at all rates. Therefore, F(z) typically includes a pitch prefilter sometimes combined with a formant prefilter, namely, F(z)=F<sub>a</sub> (z)F<sub>b</sub> (z). Other forms of prefilter can also be applied profitably.</p>
    <p>In accordance with the CELP technique, the output sampled speech signal S is obtained by first scaling the innovation vector C<sub>k</sub> from the codebook 208 by the gain g through the amplifier 206. The adder 207 then adds the scaled waveform gC<sub>k</sub> to the output E (the long term prediction component of the signal excitation of the synthesis filter 204) of a long term predictor 203 supplied with the LTP parameters, placed in a feedback loop and having a transfer function B(z) defined as follows:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">B(z)=bz<sup>-T</sup> </pre>
    
    <p>where b and T are the above defined pitch gain and delay, respectively.</p>
    <p>The predictor 203 is a filter having a transfer function in accordance to the last received LTP parameters b and T to model the pitch periodicity of speech. It introduces the appropriate pitch gain b and delay T of samples. The composite signal E+gC<sub>k</sub> constitutes the signal excitation of the synthesis filter 204 which has a transfer function 1/A(z). The filter 204 provides the correct spectrum shaping in accordance with the last received STP parameters. More specifically, the filter 204 models the resonant frequencies (formants) of speech. The output block S is the synthesized sampled speech signal which can be converted into an analog signal with proper anti-aliasing filtering in accordance with a technique well known in the art.</p>
    <p>There are many ways to design an algebraic codebook 208. In the present invention, the algebraic codebook 208 is composed of codevectors having N non-zero-amplitude pulses (or non-zero pulses for short).</p>
    <p>Let us call P<sub>i</sub> and S<sub>pi</sub> the position and amplitude of the i<sup>th</sup> non-zero pulse, respectively. We will assume that the amplitude S<sub>pi</sub> is known either because the i<sup>th</sup> amplitude is fixed or because there exists some method for selecting S<sub>pi</sub> prior to the codevector search.</p>
    <p>Let us call "track i", denoted T<sub>i</sub> the set of positions that P<sub>i</sub> can occupy between 1 and L. Some typical sets of tracks are given below assuming L=40. The first example is a design introduced in the above mentioned U.S. patent application Ser. No. 927,528 and referred to as "Interleaved Single Pulse Permutations" (ISPP). In the first design example, denoted ISPP(40,5), a set of 40 positions is partitioned in 5 interleaved tracks of 40/5=8 valid positions each. Three bits are required to specify the 8=2<sup>3</sup> valid positions of a given pulse. Therefore, a total of 5×3=15 coding bits are required to specify pulse positions for this particular algebraic codebook structure.</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________Design 1: ISPP(40,5)        Tracks (valid positions for the i<sup>th</sup>i            pulse)______________________________________1            T1 = {1, 6, 11, 16, 21, 26, 31, 36}2            T2 = {2, 7, 12, 17, 22, 27, 32, 37}3            T3 = {3, 8, 13, 18, 23, 28, 33, 38}4            T4 = {4, 9, 14, 19, 24, 29, 34, 39}5            T5 = {5, 10, 15, 20, 25, 30, 35, 40}______________________________________</pre>
    
    <p>This ISPP is complete in the sense that any of the 40 positions is related to one and only one track. There are many ways to derive a codebook structure from one, or more, ISPP to accommodate particular requirements in terms of number of pulses or coding bits. For instance, a four-pulse codebook can be derived from ISPP(40,5) by simply ignoring track 5, or by considering the union of tracks 4 and 5 as a single track. Design examples 2 and 3 provide other instances of complete ISPP designs.</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________i           Tracks (valid positions for the i<sup>th</sup> pulse)______________________________________Design 2: ISPP(40,10)1           T1 = {1, 11, 21, 31}2           T2 = {2, 12, 22, 32}3           T3 = {3, 13, 23, 33}. . .       . . .9           T9 = {9, 19, 29, 39}10          T10 = {10, 20, 30, 40}______________________________________Design 3: ISPP(48,12)1           T1 = {1, 13, 25, 37}2           T2 = {2, 14, 26, 38}3           T3 = {3, 15, 27, 39}4           T4 = {4, 16, 28, 40}5           T5 = {5, 17, 29, 41}. . .       . . .11          T9 = {11, 23, 35, 47}12          T10 = {12, 24, 36, 48}______________________________________</pre>
    
    <p>Note that in design 3, the last pulse position of tracks T5 through T12 fall outside the subframe length L=40. In such a case the last pulse is simply ignored.</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________Design 4: Sum of two ISPP(40,1)i           Tracks (valid positions for the i<sup>th</sup> pulse)______________________________________1           T1 = {1, 2, 3, 4, 5, 6, 7, . . . , 39, 40}2           T2 = {1, 2, 3, 4, 5, 6, 7, . . . , 39, 40}______________________________________</pre>
    
    <p>In design example 4, tracks T1 and T2 allow for any of the 40 positions. Note that the positions of tracks T1 and T2 overlap. When more than one pulse occupy the same location their amplitudes are simply added together.</p>
    <p>A great variety of codebooks can be built around the general theme of ISPP designs.</p>
    <p>Encoding Principle</p>
    <p>The sampled speech signal S is encoded on a block by block basis by the encoding system of FIG. 1 which is broken down into 11 modules numbered from 102 to 112. The function and operation of most of these modules are unchanged with respect to the description of U.S. parent patent application Ser. No. 07/927,528. Therefore, although the following description will at least briefly explain the function and operation of each module, it will focus on the matter which is new with respect to the disclosure of U.S. parent patent application Ser. No. 07/927,528.</p>
    <p>For each block of L samples of speech signal, a set of Linear Predictive Coding (LPC) parameters, called short term prediction (STP) parameters, is produced in accordance with a prior art technique through an LPC spectrum analyzer 102. More specifically, the analyzer 102 models the spectral characteristics of each block S of L samples.</p>
    <p>The input block S of L-sample is whitened by a whitening filter 103 having the following transfer function based on the current values of the STP parameters: ##EQU2## where a<sub>0</sub> =1, and z is the usual variable of the so-called z-transform. As illustrated in FIG. 1, the whitening filter 103 produces a residual vector R.</p>
    <p>A pitch extractor 104 is used to compute and quantize the LTP parameters, namely the pitch delay T and the pitch gain g. The initial state of the extractor 104 is also set to a value FS from an initial state extractor 110. A detailed procedure for computing and quantizing the LTP parameters is described in U.S. parent patent application Ser. No. 07/927,528 and is believed to be well known to those of ordinary skill in the art. Accordingly, it will not be further elaborated in the present disclosure.</p>
    <p>A filter responses characterizer 105 (FIG. 1) is supplied with the STP and LTP parameters to compute a filter responses characterization FRC for use in the later steps. The FRC information consists of the following three components where n=1, 2, . . . L.</p>
    <p>f (n): response of F(z).</p>
    <p>Note that F (z ) generally includes the pitch prefilter.</p>
    <p>h(n): response of ##EQU3##  to f(n) where γ is a perceptual factor. More generally, h(n) is the impulse response of F(z)W(z)/A(z) which is the cascade of prefilter F(z), perceptual weighting filter W(z) and synthesis filter 1/A(Z). Note that F(z) and 1/A(z) are the same filters as used at the decoder.</p>
    <p>U(i,j): autocorrelation of h(n) according to the following expression: ##EQU4##  for 1≦i≦L and i≦j≦L; h(n)=1 for n&lt;1.</p>
    <p>The long term predictor 106 is supplied with the past excitation signal (i.e., E+gCk of the previous subframe) to form the new E component using the proper pitch delay T and gain b.</p>
    <p>The initial state of the perceptual filter 107 is set to the value FS supplied from the initial state extractor 110. The pitch removed residual vector R'=R-E calculated by a subtractor 121 (FIG. 1) is then supplied to the perceptual filter 107 to obtain at the output of the latter filter a target vector X. As illustrated in FIG. 1, the STP parameters are applied to the filter 107 to vary its transfer function in relation to these parameters. Basically, X=R'-P where P represents the contribution of the long term prediction (LTP) including "ringing" from the past excitations. The MSE criterion which applies to A can now be stated in the following matrix notations: ##EQU5## where H is an L×L lower triangular Toeplitz matrix formed from the h(n) response as follows. The term h(0) occupies the matrix diagonal and h(1), h(2), . . . and h(L-1) occupy the respective lower diagonals.</p>
    <p>A backward filtering step is performed by the filter 108 of FIG. 1. Setting to zero the derivative of the above equation with respect to the gain g yields to the optimum gain as follows: ##EQU6## With this value for g, the minimization becomes: ##EQU7##</p>
    <p>The objective is to find the particular index k for which the minimization is achieved. Note that because ∥X∥<sup>2</sup> is a fixed quantity, the same index can be found by maximizing the following quantity: ##EQU8## where D=(XH) and α<sub>k</sub> <sup>2</sup> =∥A<sub>k</sub> H<sup>T</sup> ∥<sup>2</sup>.</p>
    <p>In the backward filter 108, a backward filtered target vector D=(XH) is computed. The term "backward filtering" for this operation comes from the interpretation of (XH) as the filtering of time-reversed X.</p>
    <p>The purpose of the optimizing controller 109 is to search the codevectors available in the algebraic codebook to select the best codevector for encoding the current L-sample block. The basic criterion for selecting the best codevector among a set of codevectors each having N non-zero-amplitude pulses is given in the form of a ratio to be maximized: ##EQU9## and where A<sub>k</sub> has N non-zero amplitude pulses. The numerator in the above equation is the square of</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">DA<sub>k</sub> <sup>T</sup> =ΣD<sub>Pi</sub> S<sub>Pi</sub> </pre>
    
    <p>where D is the backward-filtered target vector and A<sub>k</sub> is the algebraic codevector having N non zero pulses of amplitudes S<sub>pi</sub>.</p>
    <p>The denominator is an energy term which can be expressed ##EQU10## where U(p<sub>i</sub>,p<sub>j</sub>) is the correlation associated with two unit-amplitude pulses, one at location p<sub>i</sub> and the other at location p<sub>j</sub>. This matrix is computed in accordance with the above equation in the filter response characterizer module 105 and included in the set of parameters referred to as FRC in the block diagram of FIG. 1.</p>
    <p>A fast method for computing this denominator involves the N-nested loops illustrated in FIG. 4 in which the trim lined notation S(i) and SS(i,j) is used in the place of the respective quantities "S<sub>pi</sub> " and "S<sub>pi</sub> S<sub>pj</sub> ". Computation of the denominator α<sub>k</sub> <sup>2</sup> is the most time consuming process. The computations contributing to α<sub>k</sub> <sup>2</sup> which are performed in each loop of FIG. 4 can be written on separate lines from the outermost loop to the innermost loop as follows: ##EQU11## where p<sub>i</sub> is the position of the i<sup>th</sup> non-zero pulse.</p>
    <p>The previous equation can be simplified if some pre-computing is performed by the optimizing controller 109 to transform the matrix U(i,j) supplied by the filter response characterizer 105 into a matrix U'(i,j) in accordance with the following relation:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">U'(j,k)=S<sub>j</sub> S<sub>k</sub> U(j,k)</pre>
    
    <p>where S<sub>k</sub> is the amplitude selected for an individual pulse at position k following quantization of the corresponding amplitude estimate (to be described in the following description). The factor 2 will be ignored in the rest of the discussion in order to streamline the equations.</p>
    <p>With the new matrix U'(j,k) , the computation (see FIG. 3) for each loop of the fast algorithm can be written on a separate line, from outermost to innermost loops, as follows: ##EQU12##</p>
    <p>FIGS. 4a and 4b shows two examples of a tree structure to illustrate some features of the "nested-loop search" technique just described and illustrated in FIG. 3, in order to contrast it with the present invention. The terminal nodes at the bottom of the tree of FIG. 4a illustrate all possible combinations of pulse positions for a five-pulse example (N=5) wherein each pulse can assume one of four possible positions. The exhaustive "nested-loop search" technique proceeds through the tree nodes basically from left to right as indicated. One drawback of the "nested-loop search" approach is that the search complexity increases as a function of the number of pulses N. To be able to process codebooks having a larger number N of pulses, one must settle for a partial search of the codebook. FIG. 4b illustrates the same tree wherein a faster search is achieved by focusing only on the most promising region of the tree. More precisely, proceeding to lower levels is not systematic but conditioned on performance exceeding some given thresholds.</p>
    <p>Depth-First Search</p>
    <p>Let's now turn our attention to the alternate faster technique constituting the object of the present invention and performed by the pulse-position likelihood-estimator 112 and the optimizing controller 109 of FIG. 1. The general features of this technique will be first described. Thereafter, a number of typical illustrative embodiments of the faster technique will be described.</p>
    <p>The goal of the search is to determine the codevector with the best set of N pulse positions assuming amplitudes of the pulses are either fixed or have been selected by some signal-based mechanism prior to the search such as described in co-pending U.S. patent application Ser. No. 08/383,968 filed on Feb. 6, 1995. The basic selection criterion is the maximization of the above mentioned ratio Q<sub>k</sub>.</p>
    <p>In order to reduce the search complexity, the pulses positions are determined N<sub>M</sub> pulses at a time. More precisely, the N available pulses are partitioned (step 601 of FIG. 6) into M non-empty subsets of N<sub>m</sub> pulses respectively such that N<sub>1</sub> +N<sub>2</sub> . . . +N<sub>m</sub> . . . +N<sub>M</sub> =N. A particular choice of positions for the first J=N<sub>1</sub> +N<sub>2</sub> . . . +N<sub>m-1</sub> pulses considered is called a level-m path or a path of length J. The basic criterion for a path of J pulse positions is the ratio Q<sub>k</sub> (J) when only the J relevant pulses are considered.</p>
    <p>The search begins with subset #1 and proceeds with subsequent subsets according to a tree structure whereby subset m is searched at the m<sup>th</sup> level of the tree.</p>
    <p>The purpose of the search at level 1 is to consider the N<sub>1</sub> pulses of subset #1 and their valid positions in order to determine one, or a number of, candidate path(s) of length N<sub>1</sub> which are the tree nodes at level 1.</p>
    <p>The path at each terminating node of level m-1 is extended to length N<sub>1</sub> +N<sub>2</sub> . . . +N<sub>m</sub> at level m by considering N<sub>m</sub> new pulses and their valid positions. One, or a number of, candidate extended path(s) are determined to constitute level-m nodes.</p>
    <p>The best codevector corresponds to that path of length N which maximizes the criterion Q<sub>k</sub> (N) with respect to all level-M nodes.</p>
    <p>Whereas, in the above mentioned U.S. patent application Ser. No. 927,528, the pulses (or tracks) are explored in a pre-established order (i=1,2, . . . N) they are considered in various orders in the present invention. In fact, they can be considered according to which order is deemed the most promising under the particular circumstances at any one time during the search. To this end, a new chronological index n (n=1, 2, . . . N) is used and the ID(identification)-number of the n<sup>th</sup> pulse considered in the search is given by the "pulse-order function": i=i(n). For instance at some particular time, the search path, for a 5-pulse codebook, might proceed according to the following pulse-order function:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________n =     1 2 3 4 5       chronological indexi =     4 3 1 5 2       pulse (or track) ID______________________________________</pre>
    
    <p>In order to guess intelligently which pulse order is more promising at any one time, the present invention introduces a "pulse-position likelihood-estimate vector" B, which is based on speech-related signals. The p<sup>th</sup> component B<sub>p</sub> of this estimate vector B characterizes the probability of a pulse occupying position p (p=1, 2, . . . L) in the best codevector we are searching for. This best codevector is still unknown and it is the purpose of the present invention to disclose how some properties of this best codevector can be inferred from speech-related signals.</p>
    <p>The estimate vector B can be used as follows.</p>
    <p>Firstly, the estimate vector B serves as a basis to determine for which tracks i or j it is easier to guess the pulse position. The track for which the pulse position is easier to guess should be processed first. This property is often used in the pulse ordering rule for choosing the N<sub>m</sub> pulses at the first levels of the tree structure.</p>
    <p>Secondly, for a given track, the estimate vector B indicates the relative probability of each valid position. This property is used advantageously as a selection criterion in the first few levels of the tree structure in place of the basic selection criterion Q<sub>k</sub> (j) which anyhow, in the first few levels operates on too few pulses to provide reliable performance in selecting valid positions.</p>
    <p>The preferred method for obtaining the pulse-position likelihood-estimate vector B from speech-related signals consists of calculating the sum of the normalized backward-filtered target vector D: ##EQU13## and the normalized pitch-removed residual signal R': ##EQU14## to obtain the pulse-position likelihood-estimate vector B: ##EQU15## where β is a fixed constant with a typical value of 1/2 (β is chosen between 0 and 1 depending on the percentage of non-zero pulses used in the algebraic code).</p>
    <p>It should be pointed out here that the same estimate vector B is used in a different context and for a different purpose in copending U.S. patent application Ser. No. 08/383,968 filed on Feb. 6, 1995 for an invention entitled "ALGEBRAIC CODEBOOK WITH SIGNAL-SELECTED PULSE AMPLITUDES FOR FAST CODING OF SPEECH", which discloses a method of selecting apriori a near-optimal combination of pulse amplitudes. This is useful in the context of an algebraic codebook design where non-zero pulse amplitudes may assume one of q values, where q&gt;1. This observation confirms that the discovery of good estimators such as B which can be inferred from the signal itself is of deep significance to efficient speech coding. In actual fact, beyond being estimators for either positions or amplitudes they are estimators for the codevector A<sub>k</sub> itself. Therefore any search technique which combines both the principles of said copending U.S. patent application Ser. No. 08/383,968 and of the present application is clearly within the nature and spirit of the present invention. The following is an example of a typical combined technique within the spirit of the invention. It was pointed out earlier in the present disclosure that when two or more pulses from overlapping tracks share the same position in the frame they should be added. This position-amplitude tradeoff can be jointly optimized by a trellis-like search.</p>
    <p>For convenience, both the constants and variables already defined are listed hereinbelow.</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________List of ConstantsConstant Example      Name/meaning______________________________________L        40           Frame length (Number of                 positions);N        10           Number of pulses;L<sub>i</sub>  4            Number of possible                 positions in track i;M         5           Number of levels;N<sub>m</sub>   2           Number of pulses associated                 with level m;S<sub>p</sub>  -1           Amplitude at position p;p<sub>i</sub>  13           Position of i<sup>th</sup> pulse;p<sub>i</sub>(n)    19           Position of n<sup>th</sup> processed                 pulse.______________________________________List of variablesINDEX    RANGE        NORMAL USAGE______________________________________p        1-L          Position index within                 frame;i        1-N          Pulse index;m        1-M          Subset index;n        1-N          Processing-order index;i(n)     1-N          Index of the n<sup>th</sup> processed                 pulse;p<sub>i</sub>(n)    1-L          Position of n<sup>th</sup> processed                 pulse;S<sub>p</sub>  {±1}      Amplitude at position p;                 andSp<sub>i</sub>(n)    {±1}      Amplitude at position                 occupied by the n<sup>th</sup> pulse.______________________________________</pre>
    
    <heading>Examples of Depth-First Searches</heading> <p>Let us now consider a number of typical examples of depth-first searches.</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________SEARCH TECHNIQUE #1Algebraic CodebookL = 40; N = 5ISPP(40,5)       (i.e.: L<sub>1</sub> = L<sub>2</sub> = . . . L<sub>5</sub> = 8).Search procedure:Level   Number of            Candidate  Pulse-order                               Selectionm       pulses, N<sub>m</sub>            paths      rule    Criterion______________________________________1       1        10         R1, R2  B2       2        2          R2      Q<sub>k</sub> (2)3       2        2          R2      Q<sub>k</sub> (4)______________________________________</pre>
    
    <p>Rule R1:</p>
    <p>The 10 ways to choose a first pulse position P<sub>i</sub>(1) for the level-1 path-building operation is to consider each of the 5 tracks in turn, and for each track select in turn one of the two positions that maximize B<sub>p</sub> for the track under consideration.</p>
    <p>Rule R2:</p>
    <p>Rule 2 defines the pulse-order function to be used for four pulses considered at levels 2 and 3 as follows. Lay out the four remaining indices on a circle and re-number them in a clockwise fashion starting at the right of the i(1) pulse (i.e., the pulse number of the particular level-1 node considered).</p>
    <p>We now turn to a second instance of the depth-first codebook search called Search technique #2 which will clearly exemplify the depth first principle.</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________SEARCH TECHNIQUE #2Algebraic CodebookL = 40; N = 10ISPP(40,10)      (i.e.: L<sub>1</sub> = L<sub>2</sub> = . . . L<sub>10</sub> = 4)Search procedure:level   Number of            Candidate  Pulse-order                               Selectionm       pulses, N<sub>m</sub>            paths      rule    Criterion______________________________________1       2        9          R3      B2       2        1          R4      Q<sub>k</sub> (4)3       2        1          R4      Q<sub>k</sub> (6)4       2        1          R4      Q<sub>k</sub> (8)5       2        1          R4       Q<sub>k</sub> (10)______________________________________</pre>
    
    <p>Rule R3:</p>
    <p>Choose pulse i(1) and select its position according to the maximum of B<sub>p</sub> over all p. For i(2), choose in turn each of the remaining 9 pulses. The selection criterion for a given i(2) consists of selecting the position which maximizes B<sub>p</sub> within its track.</p>
    <p>Rule R4:</p>
    <p>At the end of level 1. The entire pulse order function is determined by laying out the eight remaining indexes n on a circle and re-numbering them in a clockwise fashion starting at the right of i(2).</p>
    <p>Search technique #2 is illustrated in FIGS. 5 and 6. FIG. 5 illustrates the tree structure of the depth-first search technique #2 applied to a 10 pulse codebook of 40 positions codevectors designed according to an interleaved single-pulse permutations. The corresponding flow chart is illustrated in FIG. 6.</p>
    <p>The L=40 positions are partitioned into 10 tracks each associated to one of the N=10 non-zero-amplitude pulses of the codevectors. The ten tracks are interleaved in accordance with N interleaved single-pulse permutations.</p>
    <p>Step 601</p>
    <p>The above described pulse-position likelihood-estimate vector B is calculated.</p>
    <p>Step 602</p>
    <p>The position p of the maximum absolute value of the estimated B<sub>p</sub> is calculated.</p>
    <p>Step 603 (start level-1 path building operations)</p>
    <p>Choose pulse (i.e., track) i(1) and select ita valid position so that it conforms to the position found in step 602 (see 501 in FIG. 5).</p>
    <p>Step 604 (end level-1 path-building operations)</p>
    <p>For i(2), choose in turn each of the remaining 9 pulses. The selection criterion for a given i(2) consists of selecting the position which maximizes B<sub>p</sub> within the track of said given i(2). Thus, 9 distinct level-1 candidate paths are originated (see 502 in FIG. 5). Each of said level-1 candidate path is thereafter extended through subsequent levels of the tree structure to form 9 distinct candidate codevectors. Clearly, the purpose of level-1 is to pick nine good starting pairs of pulses based on the B estimate. For this reason, level-a path building operations are called "signal-based pulse screening" in FIG. 5.</p>
    <p>Step 605 (Rule R4)</p>
    <p>To save computation time, the pulse order to be used in the subsequent 4 levels is preset. Namely, the pulse order function i(n) for n=3, 4, . . . 10 is determined by laying out the eight remaining indexes n on a circle and re-numbering them in a clockwise fashion starting at the right of i(2). In accordance with this order, the pulses i(3) and i(4) are chosen for level-2, pulses i(5) and i(6) are already chosen for level-3, and so on.</p>
    <p>Steps 606, 607, 608, 609, (Levels 2 through 5)</p>
    <p>Levels 2 through 5 are designed for efficiency and follow identical procedures. Namely, an exhaustive search is applied to all sixteen combinations of the four positions of the two pulses considered (see 503 in FIG. 5) according to the associated selection criterion Q<sub>k</sub> (2m), where m=2, 3, 4, 5 is the level number.</p>
    <p>Because only a single candidate path results from each path building operation(see 504 in FIG. 5) associated with levels 2 through 5 (i.e., branching factor of 1), the complexity of the search grows only essentially linearly with the total number of pulses. For this reason the search performed in levels 2 through 5 can be accurately characterized as a depth-first search. Tree search techniques varies greatly in structures, criteria and problem domains, however, in the field of artificial intelligence it is customary to contrast two broad classes of search philosophy, namely, "breadth-first searches" and "depth-first searches".</p>
    <p>Step 610</p>
    <p>The 9 distinct level-1 candidate paths originated in step 604 and extended through levels 2 through 5 (i.e., step 605 through 609) constitute 9 candidate codevectors A<sub>k</sub> (see 505 in FIG. 5).</p>
    <p>The purpose of step 610 is to compare the 9 candidate codevectors A<sub>k</sub> and select the best one according to the selection criterion associated with the last level, namely Q<sub>k</sub> (10).</p>
    <p>We continue with a third instance of the depth-first codebook search called "Search technique #3" with the purpose of illustrating a case where more than one pulses are allowed to occupy the same position.</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________SEARCH TECHNIQUE #3, 10 pulses or lessAlgebraic CodebookL = 40; N = 10Number of distinct pulses ≦ 10Sum of two ISPP(40,5)(i.e.: L<sub>1</sub> = L<sub>2</sub> = . . . L<sub>5</sub> = 8; L<sub>6</sub> = L<sub>7</sub> = . . .L<sub>10</sub> = 8).Search procedure:level   Number of            Candidate  Pulse-order                               Selectionm       pulses, N<sub>m</sub>            paths      rule    Criterion______________________________________1       2        50         R5      B2       2        2          R6      Q<sub>k</sub> (4)3       2        2          R6      Q<sub>k</sub> (6)4       2        1          R6      Q<sub>k</sub> (8)5       2        1          R6       Q<sub>k</sub> (10)______________________________________</pre>
    
    <p>Rule R5:</p>
    <p>Note that two pulses can occupy the same position therefore their amplitude add together to give a double-amplitude pulse. Rule R5 determines the way in which the first two pulse positions are selected in order to provide the set of level-1 candidate paths. The ##EQU16## nodes of level-1 candidate paths correspond to one double-amplitude pulse at each of the position maximizing B<sub>p</sub> in the five distinct tracks, and, all combinations of two pulse positions from the pool of 10 pulse positions selected by picking the two positions maximizing B<sub>p</sub> in each of the five distinct tracks.</p>
    <p>Rule R6: Similar to Rule R4.</p>
    <p>Although preferred embodiments of the present invention have been described in detail herein above, these embodiments can be modified at will, within the scope of the appended claims, without departing from the nature and spirit of the invention. Also the invention is not limited to the treatment of a speech signal; other types of sound signal such as audio can be processed. Such modifications, which retain the basic principle, are obviously within the scope of the subject invention.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4401855">US4401855</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 28, 1980</td><td class="patent-data-table-td patent-date-value">Aug 30, 1983</td><td class="patent-data-table-td ">The Regents Of The University Of California</td><td class="patent-data-table-td ">Apparatus for the linear predictive coding of human speech</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4486899">US4486899</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 16, 1982</td><td class="patent-data-table-td patent-date-value">Dec 4, 1984</td><td class="patent-data-table-td ">Nippon Electric Co., Ltd.</td><td class="patent-data-table-td ">System for extraction of pole parameter values</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4520499">US4520499</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 25, 1982</td><td class="patent-data-table-td patent-date-value">May 28, 1985</td><td class="patent-data-table-td ">Milton Bradley Company</td><td class="patent-data-table-td ">Combination speech synthesis and recognition apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4594687">US4594687</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 26, 1983</td><td class="patent-data-table-td patent-date-value">Jun 10, 1986</td><td class="patent-data-table-td ">Nippon Telegraph &amp; Telephone Corporation</td><td class="patent-data-table-td ">Address arithmetic circuit of a memory unit utilized in a processing system of digitalized analogue signals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4625286">US4625286</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 3, 1982</td><td class="patent-data-table-td patent-date-value">Nov 25, 1986</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Time encoding of LPC roots</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4669120">US4669120</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 2, 1984</td><td class="patent-data-table-td patent-date-value">May 26, 1987</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Low bit-rate speech coding with decision of a location of each exciting pulse of a train concurrently with optimum amplitudes of pulses</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4677671">US4677671</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 18, 1983</td><td class="patent-data-table-td patent-date-value">Jun 30, 1987</td><td class="patent-data-table-td ">International Business Machines Corp.</td><td class="patent-data-table-td ">Method and device for coding a voice signal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4680797">US4680797</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 26, 1984</td><td class="patent-data-table-td patent-date-value">Jul 14, 1987</td><td class="patent-data-table-td ">The United States Of America As Represented By The Secretary Of The Air Force</td><td class="patent-data-table-td ">Secure digital speech communication</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4710959">US4710959</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 29, 1982</td><td class="patent-data-table-td patent-date-value">Dec 1, 1987</td><td class="patent-data-table-td ">Massachusetts Institute Of Technology</td><td class="patent-data-table-td ">Voice encoder and synthesizer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4720861">US4720861</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 24, 1985</td><td class="patent-data-table-td patent-date-value">Jan 19, 1988</td><td class="patent-data-table-td ">Itt Defense Communications A Division Of Itt Corporation</td><td class="patent-data-table-td ">Digital speech coding circuit</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4724535">US4724535</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 16, 1985</td><td class="patent-data-table-td patent-date-value">Feb 9, 1988</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Low bit-rate pattern coding with recursive orthogonal decision of parameters</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4742550">US4742550</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 17, 1984</td><td class="patent-data-table-td patent-date-value">May 3, 1988</td><td class="patent-data-table-td ">Motorola, Inc.</td><td class="patent-data-table-td ">Residual excited linear predictive coder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4764963">US4764963</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 12, 1987</td><td class="patent-data-table-td patent-date-value">Aug 16, 1988</td><td class="patent-data-table-td ">American Telephone And Telegraph Company, At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Speech pattern compression arrangement utilizing speech event identification</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4771465">US4771465</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 11, 1986</td><td class="patent-data-table-td patent-date-value">Sep 13, 1988</td><td class="patent-data-table-td ">American Telephone And Telegraph Company, At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Processing system for synthesizing voice from encoded information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4797925">US4797925</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 26, 1986</td><td class="patent-data-table-td patent-date-value">Jan 10, 1989</td><td class="patent-data-table-td ">Bell Communications Research, Inc.</td><td class="patent-data-table-td ">Method for coding speech at low bit rates</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4797926">US4797926</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 11, 1986</td><td class="patent-data-table-td patent-date-value">Jan 10, 1989</td><td class="patent-data-table-td ">American Telephone And Telegraph Company, At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Digital speech vocoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4799261">US4799261</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 8, 1987</td><td class="patent-data-table-td patent-date-value">Jan 17, 1989</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Low data rate speech encoding employing syllable duration patterns</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4811398">US4811398</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 24, 1986</td><td class="patent-data-table-td patent-date-value">Mar 7, 1989</td><td class="patent-data-table-td ">Cselt-Centro Studi E Laboratori Telecomunicazioni S.P.A.</td><td class="patent-data-table-td ">Method of and device for speech signal coding and decoding by subband analysis and vector quantization with dynamic bit allocation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4815134">US4815134</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 8, 1987</td><td class="patent-data-table-td patent-date-value">Mar 21, 1989</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Very low rate speech encoder and decoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4817157">US4817157</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 7, 1988</td><td class="patent-data-table-td patent-date-value">Mar 28, 1989</td><td class="patent-data-table-td ">Motorola, Inc.</td><td class="patent-data-table-td ">Digital speech coder having improved vector excitation source</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4821324">US4821324</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 24, 1985</td><td class="patent-data-table-td patent-date-value">Apr 11, 1989</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Low bit-rate pattern encoding and decoding capable of reducing an information transmission rate</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4858115">US4858115</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 31, 1985</td><td class="patent-data-table-td patent-date-value">Aug 15, 1989</td><td class="patent-data-table-td ">Unisys Corporation</td><td class="patent-data-table-td ">Loop control mechanism for scientific processor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4860355">US4860355</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 15, 1987</td><td class="patent-data-table-td patent-date-value">Aug 22, 1989</td><td class="patent-data-table-td ">Cselt Centro Studi E Laboratori Telecomunicazioni S.P.A.</td><td class="patent-data-table-td ">Method of and device for speech signal coding and decoding by parameter extraction and vector quantization techniques</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4864620">US4864620</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 3, 1988</td><td class="patent-data-table-td patent-date-value">Sep 5, 1989</td><td class="patent-data-table-td ">The Dsp Group, Inc.</td><td class="patent-data-table-td ">Method for performing time-scale modification of speech information or speech signals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4868867">US4868867</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 6, 1987</td><td class="patent-data-table-td patent-date-value">Sep 19, 1989</td><td class="patent-data-table-td ">Voicecraft Inc.</td><td class="patent-data-table-td ">Vector excitation speech or audio coder for transmission or storage</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4873723">US4873723</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 16, 1987</td><td class="patent-data-table-td patent-date-value">Oct 10, 1989</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Method and apparatus for multi-pulse speech coding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4964169">US4964169</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 15, 1989</td><td class="patent-data-table-td patent-date-value">Oct 16, 1990</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Method and apparatus for speech coding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4991214">US4991214</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 26, 1988</td><td class="patent-data-table-td patent-date-value">Feb 5, 1991</td><td class="patent-data-table-td ">British Telecommunications Public Limited Company</td><td class="patent-data-table-td ">Speech coding using sparse vector codebook and cyclic shift techniques</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5097508">US5097508</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 31, 1989</td><td class="patent-data-table-td patent-date-value">Mar 17, 1992</td><td class="patent-data-table-td ">Codex Corporation</td><td class="patent-data-table-td ">Digital speech coder having improved long term lag parameter determination</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5193140">US5193140</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 30, 1990</td><td class="patent-data-table-td patent-date-value">Mar 9, 1993</td><td class="patent-data-table-td ">Telefonaktiebolaget L M Ericsson</td><td class="patent-data-table-td ">Excitation pulse positioning method in a linear predictive speech coder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5293449">US5293449</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 29, 1992</td><td class="patent-data-table-td patent-date-value">Mar 8, 1994</td><td class="patent-data-table-td ">Comsat Corporation</td><td class="patent-data-table-td ">Analysis-by-synthesis 2,4 kbps linear predictive speech codec</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5307441">US5307441</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 29, 1989</td><td class="patent-data-table-td patent-date-value">Apr 26, 1994</td><td class="patent-data-table-td ">Comsat Corporation</td><td class="patent-data-table-td ">Wear-toll quality 4.8 kbps speech codec</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5457783">US5457783</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 7, 1992</td><td class="patent-data-table-td patent-date-value">Oct 10, 1995</td><td class="patent-data-table-td ">Pacific Communication Sciences, Inc.</td><td class="patent-data-table-td ">Adaptive speech coder having code excited linear prediction</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5667340">US5667340</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 5, 1995</td><td class="patent-data-table-td patent-date-value">Sep 16, 1997</td><td class="patent-data-table-td ">Sandoz Ltd.</td><td class="patent-data-table-td ">Cementitious composition for underwater use and a method for placing the composition underwater</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0138061A1?cl=en">EP0138061A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 12, 1984</td><td class="patent-data-table-td patent-date-value">Apr 24, 1985</td><td class="patent-data-table-td ">Siemens Aktiengesellschaft</td><td class="patent-data-table-td ">Method of determining speech spectra with an application to automatic speech recognition and speech coding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0149724A1?cl=en">EP0149724A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 8, 1984</td><td class="patent-data-table-td patent-date-value">Jul 31, 1985</td><td class="patent-data-table-td ">Northern Telecom Limited</td><td class="patent-data-table-td ">Method and apparatus for coding digital signals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0342687A2?cl=en">EP0342687A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 19, 1989</td><td class="patent-data-table-td patent-date-value">Nov 23, 1989</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Coded speech communication system having code books for synthesizing small-amplitude components</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0446817A2?cl=en">EP0446817A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 8, 1991</td><td class="patent-data-table-td patent-date-value">Sep 18, 1991</td><td class="patent-data-table-td ">Gte Laboratories Incorporated</td><td class="patent-data-table-td ">Method for reducing the search complexity in analysis-by-synthesis coding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0514912A2?cl=en">EP0514912A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 21, 1992</td><td class="patent-data-table-td patent-date-value">Nov 25, 1992</td><td class="patent-data-table-td ">Nippon Telegraph And Telephone Corporation</td><td class="patent-data-table-td ">Speech coding and decoding methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0532225A2?cl=en">EP0532225A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 3, 1992</td><td class="patent-data-table-td patent-date-value">Mar 17, 1993</td><td class="patent-data-table-td ">AT&amp;amp;T Corp.</td><td class="patent-data-table-td ">Method and apparatus for speech coding and decoding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0545386A2?cl=en">EP0545386A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 2, 1992</td><td class="patent-data-table-td patent-date-value">Jun 9, 1993</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Method for speech coding and voice-coder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO1990000381A1?cl=en">WO1990000381A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 10, 1989</td><td class="patent-data-table-td patent-date-value">Jan 25, 1990</td><td class="patent-data-table-td ">Kci Medical United Kingdom Lim</td><td class="patent-data-table-td ">Improved fluidized bead bed</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO1991013432A1?cl=en">WO1991013432A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 6, 1990</td><td class="patent-data-table-td patent-date-value">Sep 5, 1991</td><td class="patent-data-table-td ">Univ Sherbrooke</td><td class="patent-data-table-td ">Dynamic codebook for efficient speech coding based on algebraic codes</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="8+kbits%2Fs+Speech+Coder+with+Pitch+Adaptive+Vector+Quantizer"'>8 kbits/s Speech Coder with Pitch Adaptive Vector Quantizer</a>" S. Iai and K. Irie, ICASSP 1986, Tokyo, vol. 3, Apr. 1986, pp. 1697-1700.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="A+comparison+of+some+algebraic+structures+for+CELP+coding+of+speech"'>A comparison of some algebraic structures for CELP coding of speech</a>" J-P. Adoul et al. ICASSP 87 Proceedings, Apr. 6-9, 1987, Dallas, Texas pp. 1953-1956.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="A+robust+16+Kbits%2Fs+vector+adaptive+predictive+coder+for+mobile+communications"'>A robust 16 Kbits/s vector adaptive predictive coder for mobile communications</a>" A. Le Guyader et al. ICASSP 86 Proceedings, Apr. 7-11, 1986, Tokyo, Japan pp. 857-860.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Algorithme+de+quantification+vectorielle+spherique+a+partir+du+reseau+de+Gosset+d%27ordre"'>Algorithme de quantification vectorielle spherique a partir du reseau de Gosset d'ordre</a>" C. Lamblin et J.P. Adoul , Annales des Telecommunications, 1988, vol. 43, No. 1-2, pp. 172-186.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Coding+of+Speech+at+8+kbit%2Fs+using+Conjugate-structure+Algebraic-code-excited+Linear-Predictive+%28CS-ACELP%29+Coding"'>Coding of Speech at 8 kbit/s using Conjugate-structure Algebraic-code-excited Linear-Predictive (CS-ACELP) Coding</a>" Study Group 15 contrib., Int. Telecom. Union Jun. 1995, pp. 1-43.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Fast+CELP+coding+based+on+algebraic+codes"'>Fast CELP coding based on algebraic codes</a>" J-P. Adoul et al. ICASSP 87 Proceedings Apr. 6-9, 1987, Dallas Texas pp. 1957-1960.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Fast+Methods+for+Code+Search+in+CELP"'>Fast Methods for Code Search in CELP</a>" M.E. Ahmed and M.I. Al-Suwaiyel, IEEE Transactions on Speech and Audio Processing, 1993, vol. 1, No. 3, New York, pp. 315-325.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">"<a href='http://scholar.google.com/scholar?q="Multipulse+excitation+codebook+design+and+fast+search+methods+for+CELP+speech+coding"'>Multipulse excitation codebook design and fast search methods for CELP speech coding</a>" F.F. Tzeng IEEE Glob. Telecom. Conf. &amp; Exhib., No. 28-Dec. 1, 88 Hollywood Fla, pp. 0590-0594.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">8 kbits/s Speech Coder with Pitch Adaptive Vector Quantizer S. Iai and K. Irie, ICASSP 1986, Tokyo, vol. 3, Apr. 1986, pp. 1697 1700.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">A comparison of some algebraic structures for CELP coding of speech J P. Adoul et al. ICASSP 87 Proceedings, Apr. 6 9, 1987, Dallas, Texas pp. 1953 1956.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">A robust 16 Kbits/s vector adaptive predictive coder for mobile communications A. Le Guyader et al. ICASSP 86 Proceedings, Apr. 7 11, 1986, Tokyo, Japan pp. 857 860.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Abstract of "<a href='http://scholar.google.com/scholar?q="Low+delay+speech+coding"'>Low delay speech coding</a>", Cuperman, et al., Journal Speech Communication, vol. 12, No. 2, Netherlands, Jun. 1993, pp. 193-204.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Abstract of Low delay speech coding , Cuperman, et al., Journal Speech Communication, vol. 12, No. 2, Netherlands, Jun. 1993, pp. 193 204.</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Algorithme de quantification vectorielle sph e rique a partir du r e seau de Gosset d ordre C. Lamblin et J.P. Adoul , Annales des T e l e communications, 1988, vol. 43, No. 1 2, pp. 172 186.</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Coding of Speech at 8 kbit/s using Conjugate structure Algebraic code excited Linear Predictive (CS ACELP) Coding Study Group 15 contrib., Int. Telecom. Union Jun. 1995, pp. 1 43.</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Fast CELP coding based on algebraic codes J P. Adoul et al. ICASSP 87 Proceedings Apr. 6 9, 1987, Dallas Texas pp. 1957 1960.</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Fast Methods for Code Search in CELP M.E. Ahmed and M.I. Al Suwaiyel, IEEE Transactions on Speech and Audio Processing, 1993, vol. 1, No. 3, New York, pp. 315 325.</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Leflamme, et al., "<a href='http://scholar.google.com/scholar?q="On+Reducing+Computational+Complexity+of+Codebook+Search+in+CELP+Coder+Through+the+Use+of+Algebraic+Codes"'>On Reducing Computational Complexity of Codebook Search in CELP Coder Through the Use of Algebraic Codes</a>", Proceedings of the IEEE ICASSP 1990, pp. 177-180.</td></tr><tr><td class="patent-data-table-td ">19</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Leflamme, et al., On Reducing Computational Complexity of Codebook Search in CELP Coder Through the Use of Algebraic Codes , Proceedings of the IEEE ICASSP 1990, pp. 177 180.</td></tr><tr><td class="patent-data-table-td ">20</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Multipulse excitation codebook design and fast search methods for CELP speech coding F.F. Tzeng IEEE Glob. Telecom. Conf. &amp; Exhib., No. 28 Dec. 1, 88 Hollywood Fla, pp. 0590 0594.</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5893061">US5893061</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 6, 1996</td><td class="patent-data-table-td patent-date-value">Apr 6, 1999</td><td class="patent-data-table-td ">Nokia Mobile Phones, Ltd.</td><td class="patent-data-table-td ">Method of synthesizing a block of a speech signal in a celp-type coder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6041298">US6041298</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 8, 1997</td><td class="patent-data-table-td patent-date-value">Mar 21, 2000</td><td class="patent-data-table-td ">Nokia Mobile Phones, Ltd.</td><td class="patent-data-table-td ">Method for synthesizing a frame of a speech signal with a computed stochastic excitation part</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6161086">US6161086</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 15, 1998</td><td class="patent-data-table-td patent-date-value">Dec 12, 2000</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Low-complexity speech coding with backward and inverse filtered target matching and a tree structured mutitap adaptive codebook search</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6295520">US6295520</a></td><td class="patent-data-table-td patent-date-value">Mar 15, 1999</td><td class="patent-data-table-td patent-date-value">Sep 25, 2001</td><td class="patent-data-table-td ">Tritech Microelectronics Ltd.</td><td class="patent-data-table-td ">Multi-pulse synthesis simplification in analysis-by-synthesis coders</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6385576">US6385576</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 23, 1998</td><td class="patent-data-table-td patent-date-value">May 7, 2002</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Speech encoding/decoding method using reduced subframe pulse positions having density related to pitch</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6401062">US6401062</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 1, 1999</td><td class="patent-data-table-td patent-date-value">Jun 4, 2002</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Apparatus for encoding and apparatus for decoding speech and musical signals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6556966">US6556966</a></td><td class="patent-data-table-td patent-date-value">Sep 15, 2000</td><td class="patent-data-table-td patent-date-value">Apr 29, 2003</td><td class="patent-data-table-td ">Conexant Systems, Inc.</td><td class="patent-data-table-td ">Codebook structure for changeable pulse multimode speech coding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6594626">US6594626</a></td><td class="patent-data-table-td patent-date-value">Jan 8, 2002</td><td class="patent-data-table-td patent-date-value">Jul 15, 2003</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Voice encoding and voice decoding using an adaptive codebook and an algebraic codebook</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6694292">US6694292</a></td><td class="patent-data-table-td patent-date-value">Mar 14, 2002</td><td class="patent-data-table-td patent-date-value">Feb 17, 2004</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Apparatus for encoding and apparatus for decoding speech and musical signals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6714907">US6714907</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 15, 2001</td><td class="patent-data-table-td patent-date-value">Mar 30, 2004</td><td class="patent-data-table-td ">Mindspeed Technologies, Inc.</td><td class="patent-data-table-td ">Codebook structure and search for speech coding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6738733">US6738733</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 1999</td><td class="patent-data-table-td patent-date-value">May 18, 2004</td><td class="patent-data-table-td ">Stmicroelectronics Asia Pacific Pte Ltd.</td><td class="patent-data-table-td ">G.723.1 audio encoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6795805">US6795805</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 1999</td><td class="patent-data-table-td patent-date-value">Sep 21, 2004</td><td class="patent-data-table-td ">Voiceage Corporation</td><td class="patent-data-table-td ">Periodicity enhancement in decoding wideband signals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6807524">US6807524</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 1999</td><td class="patent-data-table-td patent-date-value">Oct 19, 2004</td><td class="patent-data-table-td ">Voiceage Corporation</td><td class="patent-data-table-td ">Perceptual weighting device and method for efficient coding of wideband signals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6978235">US6978235</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 30, 1999</td><td class="patent-data-table-td patent-date-value">Dec 20, 2005</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Speech coding apparatus and speech decoding apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7089179">US7089179</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 31, 1999</td><td class="patent-data-table-td patent-date-value">Aug 8, 2006</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Voice coding method, voice coding apparatus, and voice decoding apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7151802">US7151802</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 1999</td><td class="patent-data-table-td patent-date-value">Dec 19, 2006</td><td class="patent-data-table-td ">Voiceage Corporation</td><td class="patent-data-table-td ">High frequency content recovering method and device for over-sampled synthesized wideband signal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7191122">US7191122</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 22, 2005</td><td class="patent-data-table-td patent-date-value">Mar 13, 2007</td><td class="patent-data-table-td ">Mindspeed Technologies, Inc.</td><td class="patent-data-table-td ">Speech compression system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7191123">US7191123</a></td><td class="patent-data-table-td patent-date-value">Nov 17, 2000</td><td class="patent-data-table-td patent-date-value">Mar 13, 2007</td><td class="patent-data-table-td ">Voiceage Corporation</td><td class="patent-data-table-td ">Gain-smoothing in wideband speech and audio signal decoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7206739">US7206739</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 23, 2002</td><td class="patent-data-table-td patent-date-value">Apr 17, 2007</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Excitation codebook search method in a speech coding system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7249014">US7249014</a></td><td class="patent-data-table-td patent-date-value">Mar 13, 2003</td><td class="patent-data-table-td patent-date-value">Jul 24, 2007</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Apparatus, methods and articles incorporating a fast algebraic codebook search technique</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7260521">US7260521</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 1999</td><td class="patent-data-table-td patent-date-value">Aug 21, 2007</td><td class="patent-data-table-td ">Voiceage Corporation</td><td class="patent-data-table-td ">Method and device for adaptive bandwidth pitch search in coding wideband signals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7280959">US7280959</a></td><td class="patent-data-table-td patent-date-value">Nov 22, 2001</td><td class="patent-data-table-td patent-date-value">Oct 9, 2007</td><td class="patent-data-table-td ">Voiceage Corporation</td><td class="patent-data-table-td ">Indexing pulse positions and signs in algebraic codebooks for coding of wideband signals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7289952">US7289952</a></td><td class="patent-data-table-td patent-date-value">May 7, 2001</td><td class="patent-data-table-td patent-date-value">Oct 30, 2007</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Excitation vector generator, speech coder and speech decoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7373295">US7373295</a></td><td class="patent-data-table-td patent-date-value">Jul 9, 2003</td><td class="patent-data-table-td patent-date-value">May 13, 2008</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Speech coder and speech decoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7398205">US7398205</a></td><td class="patent-data-table-td patent-date-value">Jun 2, 2006</td><td class="patent-data-table-td patent-date-value">Jul 8, 2008</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Code excited linear prediction speech decoder and method thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7496504">US7496504</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 2003</td><td class="patent-data-table-td patent-date-value">Feb 24, 2009</td><td class="patent-data-table-td ">Electronics And Telecommunications Research Institute</td><td class="patent-data-table-td ">Method and apparatus for searching for combined fixed codebook in CELP speech codec</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7499854">US7499854</a></td><td class="patent-data-table-td patent-date-value">Nov 18, 2005</td><td class="patent-data-table-td patent-date-value">Mar 3, 2009</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Speech coder and speech decoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7519533">US7519533</a></td><td class="patent-data-table-td patent-date-value">Mar 8, 2007</td><td class="patent-data-table-td patent-date-value">Apr 14, 2009</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Fixed codebook searching apparatus and fixed codebook searching method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7533016">US7533016</a></td><td class="patent-data-table-td patent-date-value">Jul 12, 2007</td><td class="patent-data-table-td patent-date-value">May 12, 2009</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Speech coder and speech decoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7546239">US7546239</a></td><td class="patent-data-table-td patent-date-value">Aug 24, 2006</td><td class="patent-data-table-td patent-date-value">Jun 9, 2009</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Speech coder and speech decoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7587316">US7587316</a></td><td class="patent-data-table-td patent-date-value">May 11, 2005</td><td class="patent-data-table-td patent-date-value">Sep 8, 2009</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Noise canceller</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7590527">US7590527</a></td><td class="patent-data-table-td patent-date-value">May 10, 2005</td><td class="patent-data-table-td patent-date-value">Sep 15, 2009</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Speech coder using an orthogonal search and an orthogonal search method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7593852">US7593852</a></td><td class="patent-data-table-td patent-date-value">Jan 30, 2007</td><td class="patent-data-table-td patent-date-value">Sep 22, 2009</td><td class="patent-data-table-td ">Mindspeed Technologies, Inc.</td><td class="patent-data-table-td ">Speech compression system and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7596493">US7596493</a></td><td class="patent-data-table-td patent-date-value">Dec 19, 2005</td><td class="patent-data-table-td patent-date-value">Sep 29, 2009</td><td class="patent-data-table-td ">Stmicroelectronics Asia Pacific Pte Ltd.</td><td class="patent-data-table-td ">System and method for supporting multiple speech codecs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7672837">US7672837</a></td><td class="patent-data-table-td patent-date-value">Aug 4, 2006</td><td class="patent-data-table-td patent-date-value">Mar 2, 2010</td><td class="patent-data-table-td ">Voiceage Corporation</td><td class="patent-data-table-td ">Method and device for adaptive bandwidth pitch search in coding wideband signals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7693710">US7693710</a></td><td class="patent-data-table-td patent-date-value">May 30, 2003</td><td class="patent-data-table-td patent-date-value">Apr 6, 2010</td><td class="patent-data-table-td ">Voiceage Corporation</td><td class="patent-data-table-td ">Method and device for efficient frame erasure concealment in linear predictive based speech codecs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7698132">US7698132</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 17, 2002</td><td class="patent-data-table-td patent-date-value">Apr 13, 2010</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Sub-sampled excitation waveform codebooks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7739108">US7739108</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 17, 2003</td><td class="patent-data-table-td patent-date-value">Jun 15, 2010</td><td class="patent-data-table-td ">Electronics And Telecommunications Research Institute</td><td class="patent-data-table-td ">Method for searching fixed codebook based upon global pulse replacement</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7809557">US7809557</a></td><td class="patent-data-table-td patent-date-value">Jun 6, 2008</td><td class="patent-data-table-td patent-date-value">Oct 5, 2010</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Vector quantization apparatus and method for updating decoded vector storage</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7925501">US7925501</a></td><td class="patent-data-table-td patent-date-value">Jan 29, 2009</td><td class="patent-data-table-td patent-date-value">Apr 12, 2011</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Speech coder using an orthogonal search and an orthogonal search method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7949521">US7949521</a></td><td class="patent-data-table-td patent-date-value">Feb 25, 2009</td><td class="patent-data-table-td patent-date-value">May 24, 2011</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Fixed codebook searching apparatus and fixed codebook searching method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7957962">US7957962</a></td><td class="patent-data-table-td patent-date-value">Feb 25, 2009</td><td class="patent-data-table-td patent-date-value">Jun 7, 2011</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Fixed codebook searching apparatus and fixed codebook searching method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8036885">US8036885</a></td><td class="patent-data-table-td patent-date-value">Nov 17, 2009</td><td class="patent-data-table-td patent-date-value">Oct 11, 2011</td><td class="patent-data-table-td ">Voiceage Corp.</td><td class="patent-data-table-td ">Method and device for adaptive bandwidth pitch search in coding wideband signals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8036887">US8036887</a></td><td class="patent-data-table-td patent-date-value">May 17, 2010</td><td class="patent-data-table-td patent-date-value">Oct 11, 2011</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">CELP speech decoder modifying an input vector with a fixed waveform to transform a waveform of the input vector</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8086450">US8086450</a></td><td class="patent-data-table-td patent-date-value">Aug 27, 2010</td><td class="patent-data-table-td patent-date-value">Dec 27, 2011</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Excitation vector generator, speech coder and speech decoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8160871">US8160871</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 31, 2010</td><td class="patent-data-table-td patent-date-value">Apr 17, 2012</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Speech coding method and apparatus which codes spectrum parameters and an excitation signal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8185385">US8185385</a></td><td class="patent-data-table-td patent-date-value">Apr 26, 2010</td><td class="patent-data-table-td patent-date-value">May 22, 2012</td><td class="patent-data-table-td ">Electronics And Telecommunications Research Institute</td><td class="patent-data-table-td ">Method for searching fixed codebook based upon global pulse replacement</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8224657">US8224657</a></td><td class="patent-data-table-td patent-date-value">Jun 27, 2003</td><td class="patent-data-table-td patent-date-value">Jul 17, 2012</td><td class="patent-data-table-td ">Nokia Corporation</td><td class="patent-data-table-td ">Method and device for efficient in-band dim-and-burst signaling and half-rate max operation in variable bit-rate wideband speech coding for CDMA wireless systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8249866">US8249866</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2010</td><td class="patent-data-table-td patent-date-value">Aug 21, 2012</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Speech decoding method and apparatus which generates an excitation signal and a synthesis filter</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8255207">US8255207</a></td><td class="patent-data-table-td patent-date-value">Dec 27, 2006</td><td class="patent-data-table-td patent-date-value">Aug 28, 2012</td><td class="patent-data-table-td ">Voiceage Corporation</td><td class="patent-data-table-td ">Method and device for efficient frame erasure concealment in speech codecs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8260621">US8260621</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2010</td><td class="patent-data-table-td patent-date-value">Sep 4, 2012</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Speech coding method and apparatus for coding an input speech signal based on whether the input speech signal is wideband or narrowband</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8315861">US8315861</a></td><td class="patent-data-table-td patent-date-value">Mar 12, 2012</td><td class="patent-data-table-td patent-date-value">Nov 20, 2012</td><td class="patent-data-table-td ">Kabushiki Kaisha Toshiba</td><td class="patent-data-table-td ">Wideband speech decoding apparatus for producing excitation signal, synthesis filter, lower-band speech signal, and higher-band speech signal, and for decoding coded narrowband speech</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8332214">US8332214</a></td><td class="patent-data-table-td patent-date-value">Jan 21, 2009</td><td class="patent-data-table-td patent-date-value">Dec 11, 2012</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Speech coder and speech decoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8352253">US8352253</a></td><td class="patent-data-table-td patent-date-value">May 20, 2010</td><td class="patent-data-table-td patent-date-value">Jan 8, 2013</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Speech coder and speech decoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8352254">US8352254</a></td><td class="patent-data-table-td patent-date-value">Dec 8, 2006</td><td class="patent-data-table-td patent-date-value">Jan 8, 2013</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Fixed code book search device and fixed code book search method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8370137">US8370137</a></td><td class="patent-data-table-td patent-date-value">Nov 22, 2011</td><td class="patent-data-table-td patent-date-value">Feb 5, 2013</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Noise estimating apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8452590">US8452590</a></td><td class="patent-data-table-td patent-date-value">Apr 25, 2011</td><td class="patent-data-table-td patent-date-value">May 28, 2013</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Fixed codebook searching apparatus and fixed codebook searching method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8515743">US8515743</a></td><td class="patent-data-table-td patent-date-value">Jun 4, 2009</td><td class="patent-data-table-td patent-date-value">Aug 20, 2013</td><td class="patent-data-table-td ">Huawei Technologies Co., Ltd</td><td class="patent-data-table-td ">Method and apparatus for searching fixed codebook</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8560306">US8560306</a></td><td class="patent-data-table-td patent-date-value">Jul 13, 2006</td><td class="patent-data-table-td patent-date-value">Oct 15, 2013</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Method and apparatus to search fixed codebook using tracks of a trellis structure with each track being a union of tracks of an algebraic codebook</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8566106">US8566106</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 11, 2008</td><td class="patent-data-table-td patent-date-value">Oct 22, 2013</td><td class="patent-data-table-td ">Voiceage Corporation</td><td class="patent-data-table-td ">Method and device for fast algebraic codebook search in speech and audio coding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8600739">US8600739</a></td><td class="patent-data-table-td patent-date-value">Jun 9, 2009</td><td class="patent-data-table-td patent-date-value">Dec 3, 2013</td><td class="patent-data-table-td ">Huawei Technologies Co., Ltd.</td><td class="patent-data-table-td ">Coding method, encoder, and computer readable medium that uses one of multiple codebooks based on a type of input signal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8620649">US8620649</a></td><td class="patent-data-table-td patent-date-value">Sep 23, 2008</td><td class="patent-data-table-td patent-date-value">Dec 31, 2013</td><td class="patent-data-table-td ">O&#39;hearn Audio Llc</td><td class="patent-data-table-td ">Speech coding system and method using bi-directional mirror-image predicted pulses</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100280831">US20100280831</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 11, 2008</td><td class="patent-data-table-td patent-date-value">Nov 4, 2010</td><td class="patent-data-table-td ">Redwan Salami</td><td class="patent-data-table-td ">Method and Device for Fast Algebraic Codebook Search in Speech and Audio Coding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/CN101223580B?cl=en">CN101223580B</a></td><td class="patent-data-table-td patent-date-value">Jul 13, 2006</td><td class="patent-data-table-td patent-date-value">Apr 18, 2012</td><td class="patent-data-table-td ">三星电子株式会社</td><td class="patent-data-table-td ">Method and apparatus for searching fixed codebook</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE19609170A1?cl=en">DE19609170A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 9, 1996</td><td class="patent-data-table-td patent-date-value">Sep 19, 1996</td><td class="patent-data-table-td ">Univ Sherbrooke</td><td class="patent-data-table-td ">Verfahren zur Durchführung einer &quot;Tiefe-Zuerst&quot;-Suche in einem Codebuch zur Codierung eines Geräusch- bzw. Klangsignals, Vorrichtung zur Durchführung dieses Verfahrens sowie zellulares Kommunikationssystem mit einer derartigen Vorrichtung</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE19609170B4?cl=en">DE19609170B4</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 9, 1996</td><td class="patent-data-table-td patent-date-value">Nov 11, 2004</td><td class="patent-data-table-td ">Université de Sherbrooke, Sherbrooke</td><td class="patent-data-table-td ">Verfahren zur Durchführung einer &quot;Tiefe-Zuerst&quot;-Suche in einem Codebuch zur Codierung eines Geräusch- bzw. Klangsignales, Vorrichtung zur Durchführung dieses Verfahrens sowie zellulares Kommunikationssystem mit einer derartigen Vorrichtung</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1677287A1?cl=en">EP1677287A1</a></td><td class="patent-data-table-td patent-date-value">Dec 19, 2005</td><td class="patent-data-table-td patent-date-value">Jul 5, 2006</td><td class="patent-data-table-td ">STMicroelectronics Asia Pacific Pte Ltd.</td><td class="patent-data-table-td ">A system and method for supporting dual speech codecs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2001020595A1?cl=en">WO2001020595A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 14, 1999</td><td class="patent-data-table-td patent-date-value">Mar 22, 2001</td><td class="patent-data-table-td ">Fujitsu Ltd</td><td class="patent-data-table-td ">Voice encoder/decoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2002025638A2?cl=en">WO2002025638A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 17, 2001</td><td class="patent-data-table-td patent-date-value">Mar 28, 2002</td><td class="patent-data-table-td ">Conexant Systems Inc</td><td class="patent-data-table-td ">Codebook structure and search for speech coding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2002043053A1?cl=en">WO2002043053A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 22, 2001</td><td class="patent-data-table-td patent-date-value">May 30, 2002</td><td class="patent-data-table-td ">Voiceage Corp</td><td class="patent-data-table-td ">Indexing pulse positions and signs in algebraic codebooks for coding of wideband signals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2002071396A1?cl=en">WO2002071396A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 22, 2002</td><td class="patent-data-table-td patent-date-value">Sep 12, 2002</td><td class="patent-data-table-td ">Conexant Systems Inc</td><td class="patent-data-table-td ">Codebook structure and search for speech coding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2007027005A1?cl=en">WO2007027005A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 13, 2006</td><td class="patent-data-table-td patent-date-value">Mar 8, 2007</td><td class="patent-data-table-td ">Samsung Electronics Co Ltd</td><td class="patent-data-table-td ">Method and apparatus for searching fixed codebook</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S219000">704/219</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S223000">704/223</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704SE19035">704/E19.035</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704SE19032">704/E19.032</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S200100">704/200.1</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S262000">704/262</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0019080000">G10L19/08</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0019120000">G10L19/12</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0019000000">G10L19/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0019100000">G10L19/10</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L19/10">G10L19/10</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L19/00">G10L19/00</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L19/12">G10L19/12</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=hdBEBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L25/06">G10L25/06</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G10L19/12</span>, <span class="nested-value">G10L19/10</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Jun 17, 2009</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 15, 2006</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 1-31 IS CONFIRMED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 17, 2005</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 26, 2004</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20040913</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 8, 2001</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 22, 2000</td><td class="patent-data-table-td ">CC</td><td class="patent-data-table-td ">Certificate of correction</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 2, 1995</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">UNIVERSITE DE SHERBROOKE, CANADA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:ADOUL, JEAN-PIERRE;LAFLAMME, CLAUDE;REEL/FRAME:007871/0763</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">19951003</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U08P57K3_wP3uy6WIi2nhkgZ0xGEA\u0026id=hdBEBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U3wQQYBN6CGX1_dhJcnOk5sGoCbXQ\u0026id=hdBEBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U3gX0tolGCqnODAlwRPUnokryV5JA","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Depth_first_algebraic_codebook_search_fo.pdf?id=hdBEBAABERAJ\u0026output=pdf\u0026sig=ACfU3U06iXQW8biJBl8pvfiC3ezU8M7rPw"},"sample_url":"http://www.google.com/patents/reader?id=hdBEBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>