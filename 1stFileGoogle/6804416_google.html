<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6804416 - Method and system for aligning geometric object models with images - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Method and system for aligning geometric object models with images"><meta name="DC.contributor" content="Ivan Bachelder" scheme="inventor"><meta name="DC.contributor" content="Lowell Jacobson" scheme="inventor"><meta name="DC.contributor" content="Jay Negro" scheme="inventor"><meta name="DC.contributor" content="Leonid Segal" scheme="inventor"><meta name="DC.contributor" content="Cognex Corporation" scheme="assignee"><meta name="DC.date" content="2001-3-16" scheme="dateSubmitted"><meta name="DC.description" content="A method and system is provided for aligning a geometric object model with a pixel image. A geometric object model representing an object is obtained first. Using the geometric object model of the object, a revised geometric object model is created. An alignment tool is then trained based on the revised geometric object model. The alignment model can be applied to a pixel image of the object so that the pose of the object can be computed by aligning the revised geometric object model with the image."><meta name="DC.date" content="2004-10-12" scheme="issued"><meta name="DC.relation" content="US:4980971" scheme="references"><meta name="DC.relation" content="US:5060276" scheme="references"><meta name="DC.relation" content="US:5111516" scheme="references"><meta name="DC.relation" content="US:5113565" scheme="references"><meta name="DC.relation" content="US:5226095" scheme="references"><meta name="DC.relation" content="US:5268999" scheme="references"><meta name="DC.relation" content="US:5343028" scheme="references"><meta name="DC.relation" content="US:5371690" scheme="references"><meta name="DC.relation" content="US:5471541" scheme="references"><meta name="DC.relation" content="US:5495537" scheme="references"><meta name="DC.relation" content="US:5497451" scheme="references"><meta name="DC.relation" content="US:5500906" scheme="references"><meta name="DC.relation" content="US:5545887" scheme="references"><meta name="DC.relation" content="US:5602937" scheme="references"><meta name="DC.relation" content="US:5621807" scheme="references"><meta name="DC.relation" content="US:5625715" scheme="references"><meta name="DC.relation" content="US:5627912" scheme="references"><meta name="DC.relation" content="US:5627915" scheme="references"><meta name="DC.relation" content="US:5663809" scheme="references"><meta name="DC.relation" content="US:5828769" scheme="references"><meta name="DC.relation" content="US:5850469" scheme="references"><meta name="DC.relation" content="US:5974169" scheme="references"><meta name="DC.relation" content="US:6078700" scheme="references"><meta name="DC.relation" content="US:6151406" scheme="references"><meta name="DC.relation" content="US:6324298" scheme="references"><meta name="DC.relation" content="US:6421458" scheme="references"><meta name="DC.relation" content="US:6462751" scheme="references"><meta name="citation_reference" content="Andre Fischer, Thomas Kolbe and Felicitas Lang, On the Use of Geometric and Semantic Models for Component-Based Building Reconstruction: Institue for Photogrammetry, University of Bonn, pp. 101-119, 1999."><meta name="citation_reference" content="Anthony Hoogs and Ruzena Bajcsy, Model-based Learning of Segmentations, pp. 494-499, IEEE, 1996."><meta name="citation_reference" content="Cognex Corporation, Cognex 3000/4000/5000 Vision Tool, Revision 7.6, Chapter 4, Caliper Tool, 1996."><meta name="citation_reference" content="Cognex Corporation, Cognex 3000/4000/5000 Vision Tool, Revision 7.6, Chapter 5, Inspection, 1996."><meta name="citation_reference" content="Cognex Corporation, Cognex 3000/4400 SMD Tools Release 5.2, SMD 2, 1994."><meta name="citation_reference" content="Cognex Corporation, Cognex 4000/5000 SMD Placement Guidance Package, User&#39;s Manual Release 3.8.00, Chapter 15, MFOV-LL Device Inspection, 1998."><meta name="citation_reference" content="Cognex Corporation, Cognex 4000/5000 SMD Placement Guidance Package, User&#39;s Manual Release 3.8.00, Chapter 7, Rectilinear Device Inspection, 1998."><meta name="citation_reference" content="Cognex Corporation, Cognex 4000/5000 SMD Placement Guidance Package, User&#39;s Manual Release 3.8.00, Chapter 8, Large-Leaded Device Inspection, 1998."><meta name="citation_reference" content="Cognex Corporation, Cognex MVS-8000 Series, GDE User&#39;s Guide, Revision 1.1, Apr. 7, 2000."><meta name="citation_reference" content="D.W. Jacobs, The Use of Grouping in Visual Object Recognition, MIT Artifical Intelligence Laboratory, Office of Naval Research, pp. 1-162, Oct. 1988."><meta name="citation_reference" content="George Vosselman, Interactive Aligment of Parameterised Object Models to Images: Delft University of Technology, Faculty of Civil Engineering and Geosciences, 1998."><meta name="citation_reference" content="Shimon Ullman, Aligning pictorial descriptions: An approach to object recognition, Cognition, vol. 32, No. 3, pp. 193-254, Aug. 1989."><meta name="citation_reference" content="Timothy S. Newman, Anil K. Jain and H.R. Keshavan, 3D CAD-Based Inspection I: Coarse Verification, pp. 49-52, IEEE, 1992."><meta name="citation_patent_number" content="US:6804416"><meta name="citation_patent_application_number" content="US:09/809,026"><link rel="canonical" href="http://www.google.com/patents/US6804416"/><meta property="og:url" content="http://www.google.com/patents/US6804416"/><meta name="title" content="Patent US6804416 - Method and system for aligning geometric object models with images"/><meta name="description" content="A method and system is provided for aligning a geometric object model with a pixel image. A geometric object model representing an object is obtained first. Using the geometric object model of the object, a revised geometric object model is created. An alignment tool is then trained based on the revised geometric object model. The alignment model can be applied to a pixel image of the object so that the pose of the object can be computed by aligning the revised geometric object model with the image."/><meta property="og:title" content="Patent US6804416 - Method and system for aligning geometric object models with images"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("j0nsU8H-IIHygwTalICACA"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("BRA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("j0nsU8H-IIHygwTalICACA"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("BRA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6804416?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6804416"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=myxpBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6804416&amp;usg=AFQjCNEGtlaYuFxCI0dxIbRUrD5WxBD4fg" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6804416.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6804416.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6804416" style="display:none"><span itemprop="description">A method and system is provided for aligning a geometric object model with a pixel image. A geometric object model representing an object is obtained first. Using the geometric object model of the object, a revised geometric object model is created. An alignment tool is then trained based on the revised...</span><span itemprop="url">http://www.google.com/patents/US6804416?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6804416 - Method and system for aligning geometric object models with images</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6804416 - Method and system for aligning geometric object models with images" title="Patent US6804416 - Method and system for aligning geometric object models with images"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6804416 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/809,026</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Oct 12, 2004</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Mar 16, 2001</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Mar 16, 2001</td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">09809026, </span><span class="patent-bibdata-value">809026, </span><span class="patent-bibdata-value">US 6804416 B1, </span><span class="patent-bibdata-value">US 6804416B1, </span><span class="patent-bibdata-value">US-B1-6804416, </span><span class="patent-bibdata-value">US6804416 B1, </span><span class="patent-bibdata-value">US6804416B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Ivan+Bachelder%22">Ivan Bachelder</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Lowell+Jacobson%22">Lowell Jacobson</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Jay+Negro%22">Jay Negro</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Leonid+Segal%22">Leonid Segal</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Cognex+Corporation%22">Cognex Corporation</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6804416.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6804416.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6804416.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (27),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (13),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (8),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (6),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (5)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=myxpBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6804416&usg=AFQjCNEP75w46_IP5Wo_LG5TcMD8VEuVPQ">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=myxpBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6804416&usg=AFQjCNG3IXwAfKJSCQRAirjOk3JaJGC0NQ">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=myxpBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6804416B1%26KC%3DB1%26FT%3DD&usg=AFQjCNEnqjl050_1Nhvviv2-frMIU0G6sw">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55335495" lang="EN" load-source="patent-office">Method and system for aligning geometric object models with images</invention-title></span><br><span class="patent-number">US 6804416 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA50733649" lang="EN" load-source="patent-office"> <div class="abstract">A method and system is provided for aligning a geometric object model with a pixel image. A geometric object model representing an object is obtained first. Using the geometric object model of the object, a revised geometric object model is created. An alignment tool is then trained based on the revised geometric object model. The alignment model can be applied to a pixel image of the object so that the pose of the object can be computed by aligning the revised geometric object model with the image.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(10)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6804416B1/US06804416-20041012-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6804416B1/US06804416-20041012-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6804416B1/US06804416-20041012-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6804416B1/US06804416-20041012-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6804416B1/US06804416-20041012-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6804416B1/US06804416-20041012-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6804416B1/US06804416-20041012-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6804416B1/US06804416-20041012-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6804416B1/US06804416-20041012-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6804416B1/US06804416-20041012-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6804416B1/US06804416-20041012-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6804416B1/US06804416-20041012-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6804416B1/US06804416-20041012-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6804416B1/US06804416-20041012-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6804416B1/US06804416-20041012-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6804416B1/US06804416-20041012-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6804416B1/US06804416-20041012-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6804416B1/US06804416-20041012-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6804416B1/US06804416-20041012-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6804416B1/US06804416-20041012-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(25)</span></span></div><div class="patent-text"><div mxw-id="PCLM8746579" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is: </claim-statement> <div class="claim"> <div num="1" id="US-6804416-B1-CLM-00001" class="claim">
      <div class="claim-text">1. A method for aligning a geometric object model with a pixel image, said method comprising:</div>
      <div class="claim-text">obtaining a geometric object model representing an object; </div>
      <div class="claim-text">converting the geometric object model into a set of geometric elements; </div>
      <div class="claim-text">selecting from the set of geometric elements a set of salient geometric elements by determining the acceptability of each of said geometric elements according to at least one criterion; </div>
      <div class="claim-text">creating a revised geometric object model using the set of salient geometric elements; </div>
      <div class="claim-text">training at least one alignment model based on the revised geometric object model; </div>
      <div class="claim-text">acquiring a pixel image object representation; and </div>
      <div class="claim-text">aligning said revised geometric object model with said pixel image object representation using said at least one alignment model. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" id="US-6804416-B1-CLM-00002" class="claim">
      <div class="claim-text">2. The method of <claim-ref idref="US-6804416-B1-CLM-00001">claim 1</claim-ref>, wherein obtaining a geometric object model includes obtaining a CAD model, and wherein converting the geometric object model into a set of geometric elements includes removing parts of the CAD model.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" id="US-6804416-B1-CLM-00003" class="claim">
      <div class="claim-text">3. The method of <claim-ref idref="US-6804416-B1-CLM-00001">claim 1</claim-ref>, wherein said converting the geometric object model into a set of geometric elements includes converting the geometric object model into a set of lines.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" id="US-6804416-B1-CLM-00004" class="claim">
      <div class="claim-text">4. The method of <claim-ref idref="US-6804416-B1-CLM-00001">claim 1</claim-ref>, wherein said converting the geometric object model into a set of geometric elements includes converting the geometric object model into a set of arcs.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" id="US-6804416-B1-CLM-00005" class="claim">
      <div class="claim-text">5. The method of <claim-ref idref="US-6804416-B1-CLM-00001">claim 1</claim-ref>, wherein said geometric elements comprise circular and elliptical arcs and lines.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" id="US-6804416-B1-CLM-00006" class="claim">
      <div class="claim-text">6. The method of <claim-ref idref="US-6804416-B1-CLM-00001">claim 1</claim-ref>, wherein the at least one criterion includes:</div>
      <div class="claim-text">visibility of a geometric element in said pixel image object representation. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" id="US-6804416-B1-CLM-00007" class="claim">
      <div class="claim-text">7. The method of <claim-ref idref="US-6804416-B1-CLM-00001">claim 1</claim-ref>, wherein the the at least one criterion includes:</div>
      <div class="claim-text">non-occlusion of a geometric element in said pixel image object representation. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" id="US-6804416-B1-CLM-00008" class="claim">
      <div class="claim-text">8. The method of <claim-ref idref="US-6804416-B1-CLM-00001">claim 1</claim-ref>, wherein the at least one criterion includes:</div>
      <div class="claim-text">detectability of geometric element in said pixel image object representation. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" id="US-6804416-B1-CLM-00009" class="claim">
      <div class="claim-text">9. The method of <claim-ref idref="US-6804416-B1-CLM-00001">claim 1</claim-ref>, wherein the at least one criterion includes</div>
      <div class="claim-text">whether a geometric elements is selected by a human operator using a graphical interface. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" id="US-6804416-B1-CLM-00010" class="claim">
      <div class="claim-text">10. The method of <claim-ref idref="US-6804416-B1-CLM-00001">claim 1</claim-ref>, wherein said aligning includes:</div>
      <div class="claim-text">extracting a set of image features from said pixel image representation of said object, each of said image features corresponding to a different salient geometric element included in said revised geometric object model; </div>
      <div class="claim-text">matching said set of image features with corresponding salient geometric elements included in said revised geometric object model to produce a set of matched pairs between said set of image features and said corresponding salient geometric elements included in said revised geometric object model; and </div>
      <div class="claim-text">estimating the pose of said object in said pixel image representation based on said set of matched pairs. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" id="US-6804416-B1-CLM-00011" class="claim">
      <div class="claim-text">11. The method of <claim-ref idref="US-6804416-B1-CLM-00010">claim 10</claim-ref>, wherein matching said set of image features with corresponding salient geometric elements included in said revised geometric object model includes:</div>
      <div class="claim-text">generating alignment performance information. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" id="US-6804416-B1-CLM-00012" class="claim">
      <div class="claim-text">12. The method of <claim-ref idref="US-6804416-B1-CLM-00001">claim 1</claim-ref>, wherein aligning includes generating alignment performance information, and the method further includes:</div>
      <div class="claim-text">using the alignment performance information and the revised object model to create a further revised geometric object model having a different set of geometric elements; and </div>
      <div class="claim-text">using the further revised geometric object model to create a revised alignment model. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" id="US-6804416-B1-CLM-00013" class="claim">
      <div class="claim-text">13. The method of <claim-ref idref="US-6804416-B1-CLM-00012">claim 12</claim-ref>, wherein using the alignment performance information to create a further revised geometric object model having a different set of geometric elements includes:</div>
      <div class="claim-text">using the alignment performance information to remove geometric elements from the revised object model. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" id="US-6804416-B1-CLM-00014" class="claim">
      <div class="claim-text">14. The method of <claim-ref idref="US-6804416-B1-CLM-00013">claim 13</claim-ref>, wherein using the alignment performance information to remove geometric elements from the revised object model includes:</div>
      <div class="claim-text">removing geometric elements that are not adequately detected in an image during alignment. </div>
    </div>
    </div> <div class="claim"> <div num="15" id="US-6804416-B1-CLM-00015" class="claim">
      <div class="claim-text">15. A system for estimating the pose of an object in a pixel image, said system comprising:</div>
      <div class="claim-text">a storage mechanism storing at least one geometric object model; </div>
      <div class="claim-text">a converter to convert the at least one geometric object model to a set of geometric elements; </div>
      <div class="claim-text">a selector to select a set of salient geometric elements from the set of geometric elements, the selector having a determiner to determine the acceptability of each of said geometric elements according to at least one criterion so as to provide the set of salient geometric elements; </div>
      <div class="claim-text">a revised geometric object model builder to build a revised geometric object model using the set of salient geometric elements; </div>
      <div class="claim-text">a training unit to train at least one alignment tool based on the revised geometric object model; </div>
      <div class="claim-text">an image acquisition mechanism to obtain a pixel image object representation; and </div>
      <div class="claim-text">an alignment system to align said revised geometric object model with said pixel image object representation using said at least one alignment tool. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" id="US-6804416-B1-CLM-00016" class="claim">
      <div class="claim-text">16. The system of <claim-ref idref="US-6804416-B1-CLM-00015">claim 15</claim-ref>, wherein said geometric object model includes a CAD model, and wherein the converter converts the geometric object model into a set of geometric elements be removing parts of the CAD model.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" id="US-6804416-B1-CLM-00017" class="claim">
      <div class="claim-text">17. The system of <claim-ref idref="US-6804416-B1-CLM-00015">claim 15</claim-ref>, wherein said at least one criterion includes the visibility of said geometric elements in said pixel image representation.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" id="US-6804416-B1-CLM-00018" class="claim">
      <div class="claim-text">18. The system of <claim-ref idref="US-6804416-B1-CLM-00015">claim 15</claim-ref>, wherein said at least one criterion includes the non-occlusiveness of said geometric elements in said pixel image representation.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" id="US-6804416-B1-CLM-00019" class="claim">
      <div class="claim-text">19. The system of <claim-ref idref="US-6804416-B1-CLM-00015">claim 15</claim-ref>, wherein said at least one criterion includes the detectability of said geometric elements in said pixel image representation.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" id="US-6804416-B1-CLM-00020" class="claim">
      <div class="claim-text">20. The system of <claim-ref idref="US-6804416-B1-CLM-00015">claim 15</claim-ref>, wherein said at least one criterion includes whether said geometric elements are selected by a human operator through a graphical editor.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" id="US-6804416-B1-CLM-00021" class="claim">
      <div class="claim-text">21. The system of <claim-ref idref="US-6804416-B1-CLM-00015">claim 15</claim-ref>, wherein said alignment system includes:</div>
      <div class="claim-text">an extractor to extract a set of image features from said pixel image representation of said object, each of said image features corresponding to a different salient geometric element included in said revised geometric object model; </div>
      <div class="claim-text">a matcher to match said set of image features with corresponding salient geometric elements included in said revised geometric object model to produce a set of matched pairs between said set of image features and said corresponding salient geometric elements included in said revised geometric object model; and </div>
      <div class="claim-text">an estimator to estimate the pose of said object in said pixel image representation based on said set of matched pairs. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="22" id="US-6804416-B1-CLM-00022" class="claim">
      <div class="claim-text">22. The system of <claim-ref idref="US-6804416-B1-CLM-00021">claim 21</claim-ref>, wherein the matcher provides feedback information to the selector regarding geometric elements in the revised geometric object model that have not adequately matched to said set of image features.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="23" id="US-6804416-B1-CLM-00023" class="claim">
      <div class="claim-text">23. The system of <claim-ref idref="US-6804416-B1-CLM-00015">claim 15</claim-ref>, wherein the alignment system provides feedback information to the selector regarding geometric elements in the revised geometric object model that have not adequately matched to said set of image features.</div>
    </div>
    </div> <div class="claim"> <div num="24" id="US-6804416-B1-CLM-00024" class="claim">
      <div class="claim-text">24. A medium having information recorded thereon, such that when said information is read and executed by a computer, the computer is caused to:</div>
      <div class="claim-text">obtain a geometric object model representing an object; </div>
      <div class="claim-text">convert the geometric object model into a set of geometric elements; </div>
      <div class="claim-text">select from the set of geometric elements a set of salient geometric elements by determining the acceptability of each of said geometric elements according to at least one criterion; </div>
      <div class="claim-text">create a revised geometric object model using the set of salient geometric elements; </div>
      <div class="claim-text">train at least one alignment model based on the revised geometric object model; </div>
      <div class="claim-text">acquire a pixel image object representation; and </div>
      <div class="claim-text">align said revised geometric object model with said pixel image object representation using said at least one alignment model. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" id="US-6804416-B1-CLM-00025" class="claim">
      <div class="claim-text">25. The medium of <claim-ref idref="US-6804416-B1-CLM-00024">claim 24</claim-ref>, wherein said information recorded on said medium further causing said computer to:</div>
      <div class="claim-text">extract a set of image features from said pixel image representation of said object, each of said image features corresponding to a different salient geometric element included in said revised geometric object model; </div>
      <div class="claim-text">match said set of image features with corresponding salient geometric elements included in said revised geometric object model to produce a set of matched pairs between said set of image features and said corresponding salient geometric elements included in said revised geometric object model; and </div>
      <div class="claim-text">estimate the pose of said object in said pixel image representation based on said set of matched pairs.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES54316812" lang="EN" load-source="patent-office" class="description">
    <heading>RESERVATION OF COPYRIGHT</heading> <p>This patent document contains information subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent, as it appears in the U.S. Patent and Trademark Office files or records, but otherwise reserves all copyright rights whatsoever.</p>
    <heading>BACKGROUND</heading> <p>1. Field of the Invention</p>
    <p>The present invention, in certain respects, relates to the field of machine vision. In other respects, the present invention relates to a method and system that aligns a geometric object model with an image.</p>
    <p>2. Description of Background Information</p>
    <p>Various machine vision systems perform alignment as a first step for various tasks. For example, a machine vision inspection system will identify the correct alignment of an object before the object appearing in an image is inspected. Another example is that a robot aligns its field of view before it decides the direction in which to proceed and identifies those what obstacles to avoid. Such alignment often requires training based on an image of the object to be recognized, together with, for instance, a specification of the origin of the object as well as the dimension information of the object. While such image based training for alignment can be effective, there are certain situations where it is impractical.</p>
    <p>Training an alignment tool based on object images is tedious and time consuming. This becomes especially a problem for manufacturing processes, where there may be a wide variety of products or objects that need to be inspected using machine vision inspection. Furthermore, product designs may frequently change. Even a minor revision to an object, for example, its shape, may require retraining.</p>
    <p>Parameterized geometric object models may be used, instead of using object images, to train machine vision inspection systems. For purposes of the disclosure herein, a geometric model of an object comprises parameter representations of the geometry of the object. For example, parameterized geometric models are created and employed in machine vision inspection systems when aligning fiducial marks on electronic components. For a simple object, its geometric model may be created by manually entering the shape and dimension information about the object. For more complicated objects, however, creating a geometric object model may be even more difficult than training using images. That is, the complexity associated with creating a geometric model for a complicated object may outweigh the advantage of using a geometric object model for training an alignment tool.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>There is a need to make direct use of available geometric object models for alignment purposes in machine vision systems so that the training process can be more efficient and such trained alignment tools can effectively align a geometric object model with an image.</p>
    <p>The present invention is provided to improve upon techniques for obtaining a geometric object model used in a machine vision application and for performing alignment with such models. Improved methods and features are presented that address the need identified above and provide methods and apparatuses for aligning an object model with an image. Aspects of the present invention can improve both the efficiency of a training phase as well as the effectiveness of an on-line alignment phase. Existing geometric object models, such as CAD models, are utilized directly but revised with respect to the needs for alignment. A revised geometric object model is constructed especially for alignment purpose. It retains only the geometric features, selected from the original geometric object model, that are considered salient and useful with respect to the underlying task of aligning a geometric object model with an image. An alignment tool can be trained based on a revised geometric object model. Both a revised geometric object model and its correspondingly trained alignment tool may be further refined using the feedback information from an on-line alignment operation so that the revised model and the corresponding alignment tool can be adjusted to better fit the real scenarios of different machine vision applications.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p>The present invention is further described in the detailed description which follows, by reference to the noted drawings by way of non-limiting exemplary embodiments, in which like reference numerals represent similar parts throughout the several views of the drawings, and wherein:</p>
    <p>FIG. 1 shows a high level block diagram of an illustrative embodiment of a system that aligns an image with a geometric model using an alignment tool trained using existing geometric object models;</p>
    <p>FIG. 2 shows a more detailed block diagram of an embodiment of the present invention;</p>
    <p>FIG. 3A shows an exemplary CAD model for an electronics enclosure face plate;</p>
    <p>FIG. 3B shows an image of exemplary geometric elements converted from the CAD model illustrated in FIG. 3A;</p>
    <p>FIG. 4 is a block diagram for an embodiment of the present invention, in which the a set of functional geometric elements are selected from a set of geometric elements;</p>
    <p>FIG. 5 shows an exemplary set of functional geometric elements chosen from a set of geometric elements converted from the illustrated CAD model;</p>
    <p>FIG. 6 is a flowchart of an embodiment of the present invention, in which an alignment model is trained based on a geometric object model;</p>
    <p>FIG. 7 is a flowchart of an embodiment of the present invention, in which a revised geometric object model and its corresponding alignment tool are used in on-line alignment; and</p>
    <p>FIG. 8 is a flowchart of an embodiment of the present invention, in which a revised geometric object model and its corresponding alignment tool may be further refined based the feedback information from the on-line alignment performance.</p>
    <heading>DETAILED DESCRIPTION</heading> <p>A variety of geometric object models are available in CAD format. Accordingly, geometric object model information available from CAD data may be used to train an alignment tool. This approach is beneficial for manufacturing applications where CAD models are often established during the design phase and used in other phases of the manufacturing process.</p>
    <p>A CAD model encodes the physical features of an object. Examples of such physical features include a 90 degree angle between two lines or a circular planar with certain radius and certain orientation. Such a CAD model may, e.g., be represented in DXF format. While such features are well defined in the space they physically appear, they are not necessarily apparent in a visual image of the object. For instance, a 90 degree angle between two straight lines of an object does not necessarily appear to be a 90 degree angle in an image of the two lines. The degree of the angle between the two lines seen in their image depends on the perspective of the object in the image. As another example, a physically circular planar may appear to be an oval-shaped object in its image. That is, a CAD model sometimes may not be suitable for machine vision applications because it may not directly provide what is needed for training an alignment tool.</p>
    <p>A method and apparatus is described below that directly utilizes an existing geometric model of an object to train an alignment tool for the object and that applies the trained alignment tool to determine the pose, which includes the position and the orientation, of the object in an image. For example, the method and apparatus can take a CAD model for a chair, as input, to train an alignment tool that can be used to align the CAD model with the chair appearing in a two dimensional image or 2D image and to determine the pose (position and orientation) of the chair in the image based on the alignment. Particularly, the method and apparatus described below allows a user to rapidly filter, via one of or a combination of tools, for example, manual, graphical, and automatic tools, those features of an existing geometric model description that are salient and useful with respect to alignment.</p>
    <p>FIG. 1 shows a high level block diagram of a geometric model based alignment system <b>100</b>. The illustrated system <b>100</b> comprises an object model storage mechanism <b>130</b>, a training system <b>120</b>, an alignment tool storage mechanism <b>140</b>, an image acquisition system <b>150</b>, and an alignment system <b>160</b>. In system <b>100</b>, an existing geometric object model (e.g., a CAD model for a face plate, as illustrated in FIG. <b>3</b>A), stored in object model storage mechanism <b>130</b>, can be retrieved and used by training system <b>120</b> to train an alignment tool. An alignment tool trained by training system <b>120</b> is stored in alignment tool storage mechanism <b>140</b>. The correspondence between geometric object models, stored in object model storage mechanism <b>130</b>, and alignment tools, stored in alignment tool storage mechanism <b>140</b>, may be established using some identification scheme. Such a trained alignment can be retrieved, from alignment tool storage mechanism <b>140</b>, and used by alignment system <b>160</b> to align an image of an object, acquired by image acquisition system <b>150</b>, with a geometric object model of the object, retrieved from object model storage mechanism <b>130</b>.</p>
    <p>Training system <b>120</b>, in system <b>100</b>, may revise the existing geometric object model to produce a revised geometric object model that may better facilitate the alignment of the revised object model with an image of the underlying object (e.g., removing parts of a 3D CAD model that will not be visible in an image of the object). For example, FIG. 5 illustrates a revised object model of the CAD model shown in FIG. <b>3</b>A. The geometric features retained in the revised model, displayed in FIG. 5, may be chosen from the geometric features, displayed in FIG. 3A, according to the usefulness of those features for alignment purposes.</p>
    <p>Once it is generated, the revised geometric object model is stored in object model storage mechanism <b>130</b>. Based on the revised object model, training system <b>120</b> then trains an alignment tool for aligning the revised model with an image of the underlying object and stores the trained tool in alignment tool storage mechanism <b>140</b>. The correspondence between the revised object model and the accordingly trained alignment tool may be established through some identification scheme. During alignment, the alignment tool, trained based on a revised object model, can be used, in combination with the use of the revised object model, by alignment system <b>160</b> to align the revised object model with an image of the underlying object.</p>
    <p>A machine vision system that performs alignment may comprise two phases: an off-line training phase and an on-line alignment phase. In an off-line training phase, alignment models are trained. In an on-line alignment phase, trained alignment models are applied to align images with object models. In FIG. 1, training system <b>120</b>, together with object model storage mechanism <b>130</b> and alignment model storage mechanism <b>140</b> forms an off-line training phase. Alignment system <b>160</b>, together with image acquisition system <b>150</b>, object model storage mechanism, and alignment model storage mechanism <b>140</b> forms an alignment phase. An alignment model is generated in an off-line training phase and used in an on-line alignment phase. There is an additional feedback connection, in system <b>100</b>, from the on-line alignment phase to off-line training phase (via the link between alignment system <b>160</b> and training system <b>120</b>). With this feedback, training system <b>120</b> may further refine a revised object model as well as its corresponding alignment model based on the alignment performance, fed back from alignment system <b>160</b>.</p>
    <p>In FIG. 1, object model storage mechanism <b>130</b> stores and retrieves object models. Object model storage mechanism <b>130</b> may reside at more than one site, connected by, for example, a network. Each site may store geometric models for different objects on a different type of medium, may employ different management scheme, and may retrieve stored object models in a different manner.</p>
    <p>Geometric object models stored in object model storage mechanism <b>130</b> may include CAD models that encode the physical features of various objects. Additionally, object model storage mechanism <b>130</b> may include other types of geometric object models that may encode different kinds of geometric features of various objects. A particular kind, for example, may be a revised CAD model that, although still a geometric object model, may describe those geometric features that are visible in a 2D image. For example, a CAD model for a watch may encode the physical features on both the front and the back of the watch. When the watch is imaged, for example in 2D space, in its frontal view, the physical features on the back of the watch will not be visible in the 2D image. In this case, a revised model for the watch that retains only the physical features corresponding to the front of the watch may be derived, from the original CAD model, to produce a revised geometric model for the watch that may be practically more useful, especially for aligning a 2D image.</p>
    <p>A revised object model simplifies the original geometric model. It also explicitly defines geometric features seen in the object image, and it can be revised so as to more accurately represent a given geometric feature of the object itself, e.g., the object's image inaccurately depicts the given feature. This information is very useful in training an alignment tool because it eliminates the unnecessary burden associated with those geometric features that are actually not relevant to the alignment. Therefore, a revised object model usually leads to a more effective alignment model.</p>
    <p>In FIG. 1, to train an alignment tool, training system <b>120</b> first retrieves an existing geometric object model for an object, such as a CAD models, from object model storage mechanism <b>130</b>. From the existing geometric object model, training system <b>120</b> derives a revised geometric object model and then trains an alignment model for the underlying object based on the revised geometric object model. The revised object model may be stored in object model storage mechanism <b>130</b> and the alignment model may be stored in alignment model storage mechanism <b>140</b>, respectively. The correspondence between the revised object model and the alignment model may be established. The revised object model may also be stored together with the alignment model.</p>
    <p>Since the revised object model may describe what geometric feature should be seen in an image of the object, the alignment model for the object may be trained accordingly to detect those geometric features from a pixel image of the object and to then use those features detected from an image to match with the corresponding geometric features in the revised object model so that the object image and the object model can be aligned. The pose of the object in the pixel image may also be automatically computed once the alignment is achieved.</p>
    <p>The revised models and alignment models for alignment, generated and trained off-line by training system <b>120</b>, are stored for their future use in an on-line alignment phase.</p>
    <p>During the process of alignment, a 2D image for an object, on which alignment is to be performed, is first acquired by image acquisition system <b>150</b>. Taking the 2D image as an input, alignment system <b>160</b> retrieves the alignment model, trained for the object during the off-line training phase, from alignment tool storage mechanism <b>140</b>. Alignment system <b>160</b> then performs the alignment and computes the pose of the object in the input image.</p>
    <p>Alignment system <b>160</b> may also generate some alignment performance related information. Such performance related information may be later used by training system <b>120</b> to refine a previously generated revised object model and subsequently to retrain the alignment model that is trained using the previously generated revised object model. This is a feedback interaction between the off-line training system <b>120</b> and the alignment system <b>160</b>. The need for refining both the revised object model and the corresponding alignment tool may arise due to different reasons. For example, it is possible that some of the geometric features described in a revised object model as visible in an image may consistently not be detected from an image by alignment system <b>160</b>, due to, for example, the poor lighting condition in the image. In this case, it may be desirable to remove these geometric features from the revised object model and then re-train an alignment tool, corresponding to the refined model, that does not attempt to detect these features during alignment.</p>
    <p>FIG. 2 shows a more detailed block diagram of system <b>100</b>, in which both training system <b>120</b> and alignment system <b>160</b> comprise more components. Training system <b>120</b> may include a converter <b>220</b> that converts a set of physical features described in a geometric object model into a corresponding set of geometric elements (GEs). Examples of GEs include lines or arcs. FIG. 3A shows an example of a nominal CAD model for a face place, rendered in a 2D image. The nominal CAD model, shown in FIG. 3A, can be converted to a set of GEs (e.g., arcs and lines). FIG. 3B shows a rendered image of the GEs converted from the nominal CAD model shown in FIG. <b>3</b>A.</p>
    <p>Based on the GEs converted by converter <b>220</b>, a revised model generation unit <b>230</b> creates a revised model of the object, which may be defined by, for example, a set of salient geometric elements (SGEs). An SGE of an object is a GE of the object that satisfies some criteria. Examples of such criteria include the visibility of a GE in an image of the object or whether the GE can be detected from an image. The SGEs may also be chosen, manually by a user, according to some subjective criteria. For example, a user may select SGEs based on how relevant or effective a GE is in terms of alignment. A user may consider that a curve in alignment is more useful than a straight line. In this case, the user may select only the GEs that represent curves as SGEs. Yet another example is the degree of effectiveness of a GE in alignment. For instance, a longer straight line may be considered more effective than a short straight line because a longer straight line may constrain the alignment more effectively. In this case, a GE that represents a very long straight line may be more likely selected as an SGE.</p>
    <p>Revised model generation unit <b>230</b> creates a revised object model, via different means, including, for example, manual, graphical, automatic, or a combination of these approaches. A revised object model, once created, is stored in object model storage mechanism <b>130</b>.</p>
    <p>Using a revised object model, alignment training unit <b>240</b> trains an alignment model for the corresponding object. The training may be achieved using different approaches. One example of such approaches is to train, based on a synthetic image rendered from the SGEs, a search model or a rotation-scale invariant search method. Another alternative is to directly train a rotation-scale invariant search method by synthesizing the geometric search features from SGEs. Such a trained alignment model is then stored in alignment tool storage mechanism <b>140</b> for future use. Before the alignment model is trained, features are converted from physical unites (e.g., microns) to image pixels or fractions thereof. A viewer/editor <b>221</b> may be provided to allow a user to view and edit any one of the representations in the training system.</p>
    <p>On the right side of FIG. 2, alignment system <b>160</b> performs alignment between a revised object model and an image of the corresponding object, by applying an alignment tool trained for the object. To do so, alignment system <b>160</b> first obtains an image of the object from image acquisition mechanism <b>150</b> and then retrieves the alignment tool, that has been trained for the particular object, from alignment tool storage mechanism <b>140</b>.</p>
    <p>The alignment process may comprise different tasks. Alignment system <b>160</b> may comprise individual system components that perform those distinctly different tasks. In FIG. 2, alignment system <b>160</b> comprises a feature extraction unit <b>250</b>, a matching unit <b>260</b>, and a pose estimation unit <b>270</b>. Their functionalities are described below.</p>
    <p>Based on an input image for an object, feature extraction unit <b>250</b> detects a set of image features, from the input image, that correspond to the SGEs specified in the revised geometric model of the object. To compute the pose of the object in the input image, it is necessary to align the image features, detected from the input image, with the SGEs, specified in the revised object model. It should be noted that these two feature sets may or may not have equal number of elements, depending on whether all the SGEs can be detected from the input image. As discussed earlier, it is possible that some of the SGEs may not be detected from the input image due to reasons such as poor illumination of the image.</p>
    <p>Matching unit <b>260</b> aligns the two feature sets by matching each image feature with its corresponding SGE. The matching operation generates pairs of image feature and its corresponding SGE. Based on these pairs produced by matching unit <b>260</b>, pose estimation unit <b>270</b> computes the pose of the object.</p>
    <p>As mentioned earlier, SGEs, generated by revised model generation unit <b>230</b>, correspond to GEs that satisfy some criteria. FIG. 4 illustrates an exemplary mechanism <b>400</b> to select SGEs. An acceptability determiner <b>450</b> determines whether each of the GEs, generated by converter <b>220</b>, will be accepted as a SGE. The SGEs selected by acceptability determiner <b>450</b> are fed to revised model former <b>460</b> to construct a revised object model. The revised model is then saved in object model storage mechanism <b>130</b>. FIG. 5 shows an exemplary revised object model derived based on the set of GEs shown in FIG. <b>3</b>B. The exemplary revised object model is illustrated, in FIG. 5, as rendered SGEs in a 2D image.</p>
    <p>In the exemplary embodiment illustrated in FIG. 4, there are four determiners, <b>410</b>, <b>420</b>, <b>430</b>, and <b>440</b>, each of which may determine the acceptability of a GE according to one single criterion.</p>
    <p>Visibility determiner <b>420</b> automatically decides the acceptability of a GE by examining whether it is visible in an image. For example, whether a GE is visible in a 2D image may be automatically determined once the perspective of the object is known.</p>
    <p>Detectability determiner <b>440</b> decides whether a GE is actually detectable from an image. Different from previously described determiners in FIG. 4, determiner <b>440</b> may take the alignment performance information generated by an alignment tool as input. The performance information may record the geometric features that have not been detected in the previous alignment operations. Using such information, determiner <b>440</b> may retrieve the corresponding revised object model and refine it so that the SGEs that have not been detected can be removed from the revised object model to produce a refined revised model for the object. Accordingly, the alignment tool previously trained for this particular object may also be refined based on the refined model.</p>
    <p>Manual determiner <b>410</b> is a mechanism, which allows a user to manually select SGEs from a set of GEs. The manual selection may be based on the subjective evaluation of the GEs with respect to a set of criteria, which may include any individual, any combination of the exemplary criteria discussed so far. For example, a user may select a GE as a SGE because the GE is not only visible but also non-occluded and detectable. The manual selection of SGEs may be accomplished through a graphical interface. For instance, an image that contains rendered GEs may be displayed in a window of the graphical interface. An interactive tool may be active in the display window that allows a user to select SGEs by simply clicking on the corresponding GEs.</p>
    <p>FIG. 6 is a flowchart of the training phase of the present invention. An existing geometric object model is first obtained at act <b>610</b>. The model is retrieved from object model storage mechanism <b>130</b> for the purpose of training an alignment model. The geometric object model is then converted to a set of GEs at act <b>620</b>. Based on the set of GEs, a set of SGEs are selected and used to construct a revised object model at act <b>630</b>. Once generated, the revised object model is stored at act <b>640</b> in object model storage mechanism <b>130</b> and is used to train an alignment model for the object at act <b>650</b>. Such trained alignment model for the object is then stored at act <b>660</b> in alignment model storage mechanism <b>140</b> for its use during on-line alignment phase.</p>
    <p>FIG. 7 shows a flowchart of an exemplary alignment phase of the present invention. An image for an object is acquired at act <b>710</b>. To estimate the pose of the object from the image, the first step is to align a revised geometric model of the object with the image, using an alignment model previously trained for the object. Therefore, the revised geometric model for the object is retrieved at act <b>720</b> and an alignment tool trained for the same object is retrieved at act <b>730</b>. Using the alignment model, a set of image features corresponding to the set of SGEs in the model are extracted at act <b>740</b>.</p>
    <p>The image features extracted at act <b>740</b> are then matched, at act <b>750</b>, with corresponding SGEs from the revised object model. The pairs of matched image features and SGEs will then be used, at act <b>760</b>, to compute the pose of the object in the image.</p>
    <p>As discussed earlier, in referring to FIG. 2, there may be a feedback interaction between the on-line alignment system <b>160</b> and the training system <b>120</b>. As depicted in FIG. 7, the on-line alignment process may store, at act <b>770</b>, the alignment performance information. Such information may describe which SGEs are not detected so that the training system <b>120</b> may be activated later to refine the revised object model and the corresponding alignment model <b>1</b> for the object based on that information. An exemplary flowchart of the refinement process of the present invention is shown in FIG. <b>8</b>.</p>
    <p>The alignment performance information is obtained, by training unit <b>120</b>, at act <b>810</b>. A previously generated revised object model, associated with the obtained alignment performance information, is retrieved at act <b>820</b> from object model storage mechanism <b>130</b>. Accordingly, the alignment model, previously trained based on the retrieved revised object model, is retrieved at act <b>830</b> from alignment model storage mechanism <b>140</b>. Using the alignment performance information, the previously generated revised object model is refined at act <b>840</b> and stored back to object model storage mechanism <b>130</b> at act <b>850</b> to replace the previous revised object model.</p>
    <p>If the alignment performance information describes a set of SGEs that the previously trained alignment model has not been able to detect from images during on-line alignment, such SGEs may be removed from the refined model. Based on the refined object model, the retrieved alignment model, that has been previously trained using previously generated revised object model, is refined or re-trained at act <b>860</b>. The refined alignment model is stored back at act <b>870</b> in alignment tool storage mechanism <b>140</b> in place of the previously trained alignment model.</p>
    <p>The processing described above may be performed by a general-purpose computer alone or in connection with a specialized image processing computer. Such processing may be performed by a single platform or by a distributed processing platform. In addition, such processing and functionality can be implemented in the form of special purpose hardware or in the form of software being run by a general-purpose computer. Any data handled in such processing or created as a result of such processing can be stored in any memory as is conventional in the art. By way of example, such data may be stored in a temporary memory, such as in the RAM of a given computer system or subsystem. In addition, or in the alternative, such data may be stored in longer-term storage devices, for example, magnetic disks, rewritable optical disks, and so on. For purposes of the disclosure herein, a computer-readable media may comprise any form of data storage mechanism, including such existing memory technologies as well as hardware or circuit representations of such structures and of such data.</p>
    <p>While the invention has been described with reference to the certain illustrated embodiments, the words that have been used herein are words of description, rather than words of limitation. Changes may be made, within the purview of the appended claims, without departing from the scope and spirit of the invention in its aspects. Although the invention has been described herein with reference to particular structures, acts, and materials, the invention is not to be limited to the particulars disclosed, but rather extends to all equivalent structures, acts, and, materials, such as are within the scope of the appended claims.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4980971">US4980971</a></td><td class="patent-data-table-td patent-date-value">Dec 14, 1989</td><td class="patent-data-table-td patent-date-value">Jan 1, 1991</td><td class="patent-data-table-td ">At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Using cameras</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5060276">US5060276</a></td><td class="patent-data-table-td patent-date-value">May 31, 1989</td><td class="patent-data-table-td patent-date-value">Oct 22, 1991</td><td class="patent-data-table-td ">At&amp;T Bell Laboratories</td><td class="patent-data-table-td ">Technique for object orientation detection using a feed-forward neural network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5111516">US5111516</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 11, 1990</td><td class="patent-data-table-td patent-date-value">May 5, 1992</td><td class="patent-data-table-td ">Kabushiki Kaisha Toyota Chuo Kenkyusho</td><td class="patent-data-table-td ">Apparatus for visual recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5113565">US5113565</a></td><td class="patent-data-table-td patent-date-value">Jul 6, 1990</td><td class="patent-data-table-td patent-date-value">May 19, 1992</td><td class="patent-data-table-td ">International Business Machines Corp.</td><td class="patent-data-table-td ">Apparatus and method for inspection and alignment of semiconductor chips and conductive lead frames</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5226095">US5226095</a></td><td class="patent-data-table-td patent-date-value">May 26, 1992</td><td class="patent-data-table-td patent-date-value">Jul 6, 1993</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Method of detecting the position of an object pattern in an image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5268999">US5268999</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 20, 1992</td><td class="patent-data-table-td patent-date-value">Dec 7, 1993</td><td class="patent-data-table-td ">Ricoh Company, Ltd.</td><td class="patent-data-table-td ">Modeling method and system using solid data having functional structure and normal projection drawing dimensional format</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5343028">US5343028</a></td><td class="patent-data-table-td patent-date-value">Aug 10, 1992</td><td class="patent-data-table-td patent-date-value">Aug 30, 1994</td><td class="patent-data-table-td ">United Parcel Service Of America, Inc.</td><td class="patent-data-table-td ">Method and apparatus for detecting and decoding bar code symbols using two-dimensional digital pixel images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5371690">US5371690</a></td><td class="patent-data-table-td patent-date-value">Jan 17, 1992</td><td class="patent-data-table-td patent-date-value">Dec 6, 1994</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and apparatus for inspection of surface mounted devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5471541">US5471541</a></td><td class="patent-data-table-td patent-date-value">Nov 16, 1993</td><td class="patent-data-table-td patent-date-value">Nov 28, 1995</td><td class="patent-data-table-td ">National Research Council Of Canada</td><td class="patent-data-table-td ">System for determining the pose of an object which utilizes range profiles and synethic profiles derived from a model</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5495537">US5495537</a></td><td class="patent-data-table-td patent-date-value">Jun 1, 1994</td><td class="patent-data-table-td patent-date-value">Feb 27, 1996</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Methods and apparatus for machine vision template matching of images predominantly having generally diagonal and elongate features</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5497451">US5497451</a></td><td class="patent-data-table-td patent-date-value">Jan 22, 1992</td><td class="patent-data-table-td patent-date-value">Mar 5, 1996</td><td class="patent-data-table-td ">Holmes; David</td><td class="patent-data-table-td ">Computerized method for decomposing a geometric model of surface or volume into finite elements</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5500906">US5500906</a></td><td class="patent-data-table-td patent-date-value">Jan 14, 1994</td><td class="patent-data-table-td patent-date-value">Mar 19, 1996</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Locating curvilinear objects using feathered fiducials</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5545887">US5545887</a></td><td class="patent-data-table-td patent-date-value">Oct 25, 1994</td><td class="patent-data-table-td patent-date-value">Aug 13, 1996</td><td class="patent-data-table-td ">United Parcel Service Of America, Inc.</td><td class="patent-data-table-td ">Method and apparatus for decoding bar code symbols using subpixel scan lines</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5602937">US5602937</a></td><td class="patent-data-table-td patent-date-value">Jun 1, 1994</td><td class="patent-data-table-td patent-date-value">Feb 11, 1997</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Methods and apparatus for machine vision high accuracy searching</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5621807">US5621807</a></td><td class="patent-data-table-td patent-date-value">Oct 13, 1994</td><td class="patent-data-table-td patent-date-value">Apr 15, 1997</td><td class="patent-data-table-td ">Dornier Gmbh</td><td class="patent-data-table-td ">Intelligent range image camera for object measurement</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5625715">US5625715</a></td><td class="patent-data-table-td patent-date-value">Oct 21, 1993</td><td class="patent-data-table-td patent-date-value">Apr 29, 1997</td><td class="patent-data-table-td ">U.S. Philips Corporation</td><td class="patent-data-table-td ">Method and apparatus for encoding pictures including a moving object</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5627912">US5627912</a></td><td class="patent-data-table-td patent-date-value">Dec 6, 1995</td><td class="patent-data-table-td patent-date-value">May 6, 1997</td><td class="patent-data-table-td ">Yozan Inc.</td><td class="patent-data-table-td ">Inspection method of inclination of an IC</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5627915">US5627915</a></td><td class="patent-data-table-td patent-date-value">Jan 31, 1995</td><td class="patent-data-table-td patent-date-value">May 6, 1997</td><td class="patent-data-table-td ">Princeton Video Image, Inc.</td><td class="patent-data-table-td ">Pattern recognition system employing unlike templates to detect objects having distinctive features in a video field</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5663809">US5663809</a></td><td class="patent-data-table-td patent-date-value">May 16, 1995</td><td class="patent-data-table-td patent-date-value">Sep 2, 1997</td><td class="patent-data-table-td ">Sharp Kabushiki Kaisha</td><td class="patent-data-table-td ">Image processing apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5828769">US5828769</a></td><td class="patent-data-table-td patent-date-value">Oct 23, 1996</td><td class="patent-data-table-td patent-date-value">Oct 27, 1998</td><td class="patent-data-table-td ">Autodesk, Inc.</td><td class="patent-data-table-td ">Method and apparatus for recognition of objects via position and orientation consensus of local image encoding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5850469">US5850469</a></td><td class="patent-data-table-td patent-date-value">Nov 4, 1996</td><td class="patent-data-table-td patent-date-value">Dec 15, 1998</td><td class="patent-data-table-td ">General Electric Company</td><td class="patent-data-table-td ">Real time tracking of camera pose</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5974169">US5974169</a></td><td class="patent-data-table-td patent-date-value">Mar 20, 1997</td><td class="patent-data-table-td patent-date-value">Oct 26, 1999</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Machine vision methods for determining characteristics of an object using boundary points and bounding regions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6078700">US6078700</a></td><td class="patent-data-table-td patent-date-value">Mar 13, 1997</td><td class="patent-data-table-td patent-date-value">Jun 20, 2000</td><td class="patent-data-table-td ">Sarachik; Karen B.</td><td class="patent-data-table-td ">Method and apparatus for location and inspecting a two-dimensional image including co-linear features</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6151406">US6151406</a></td><td class="patent-data-table-td patent-date-value">Oct 9, 1997</td><td class="patent-data-table-td patent-date-value">Nov 21, 2000</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Method and apparatus for locating ball grid array packages from two-dimensional image data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6324298">US6324298</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 13, 1999</td><td class="patent-data-table-td patent-date-value">Nov 27, 2001</td><td class="patent-data-table-td ">August Technology Corp.</td><td class="patent-data-table-td ">Automated wafer defect inspection system and a process of performing such inspection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6421458">US6421458</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 28, 1998</td><td class="patent-data-table-td patent-date-value">Jul 16, 2002</td><td class="patent-data-table-td ">Cognex Corporation</td><td class="patent-data-table-td ">Automated inspection of objects undergoing general affine transformation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6462751">US6462751</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 29, 1999</td><td class="patent-data-table-td patent-date-value">Oct 8, 2002</td><td class="patent-data-table-td ">Autodesk, Inc.</td><td class="patent-data-table-td ">Framework for objects having authorable behaviors and appearances</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Andre Fischer, Thomas Kolbe and Felicitas Lang, On the Use of Geometric and Semantic Models for Component-Based Building Reconstruction: Institue for Photogrammetry, University of Bonn, pp. 101-119, 1999.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Anthony Hoogs and Ruzena Bajcsy, Model-based Learning of Segmentations, pp. 494-499, IEEE, 1996.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, Cognex 3000/4000/5000 Vision Tool, Revision 7.6, Chapter 4, Caliper Tool, 1996.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, Cognex 3000/4000/5000 Vision Tool, Revision 7.6, Chapter 5, Inspection, 1996.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, Cognex 3000/4400 SMD Tools Release 5.2, SMD 2, 1994.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, Cognex 4000/5000 SMD Placement Guidance Package, User's Manual Release 3.8.00, Chapter 15, MFOV-LL Device Inspection, 1998.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, Cognex 4000/5000 SMD Placement Guidance Package, User's Manual Release 3.8.00, Chapter 7, Rectilinear Device Inspection, 1998.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, Cognex 4000/5000 SMD Placement Guidance Package, User's Manual Release 3.8.00, Chapter 8, Large-Leaded Device Inspection, 1998.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Cognex Corporation, Cognex MVS-8000 Series, GDE User's Guide, Revision 1.1, Apr. 7, 2000.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">D.W. Jacobs, The Use of Grouping in Visual Object Recognition, MIT Artifical Intelligence Laboratory, Office of Naval Research, pp. 1-162, Oct. 1988.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">George Vosselman, Interactive Aligment of Parameterised Object Models to Images: Delft University of Technology, Faculty of Civil Engineering and Geosciences, 1998.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Shimon Ullman, Aligning pictorial descriptions: An approach to object recognition, Cognition, vol. 32, No. 3, pp. 193-254, Aug. 1989.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Timothy S. Newman, Anil K. Jain and H.R. Keshavan, 3D CAD-Based Inspection I: Coarse Verification, pp. 49-52, IEEE, 1992.</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7336814">US7336814</a></td><td class="patent-data-table-td patent-date-value">Jul 14, 2005</td><td class="patent-data-table-td patent-date-value">Feb 26, 2008</td><td class="patent-data-table-td ">Braintech Canada, Inc.</td><td class="patent-data-table-td ">Method and apparatus for machine-vision</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7583272">US7583272</a></td><td class="patent-data-table-td patent-date-value">Nov 29, 2005</td><td class="patent-data-table-td patent-date-value">Sep 1, 2009</td><td class="patent-data-table-td ">Purdue Research Foundation</td><td class="patent-data-table-td ">Methods for retrieving shapes and drawings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7957583">US7957583</a></td><td class="patent-data-table-td patent-date-value">Aug 2, 2007</td><td class="patent-data-table-td patent-date-value">Jun 7, 2011</td><td class="patent-data-table-td ">Roboticvisiontech Llc</td><td class="patent-data-table-td ">System and method of three-dimensional pose estimation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8031906">US8031906</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 2, 2009</td><td class="patent-data-table-td patent-date-value">Oct 4, 2011</td><td class="patent-data-table-td ">Honda Motor Co., Ltd.</td><td class="patent-data-table-td ">Target orientation estimation using depth sensing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8095237">US8095237</a></td><td class="patent-data-table-td patent-date-value">Aug 6, 2003</td><td class="patent-data-table-td patent-date-value">Jan 10, 2012</td><td class="patent-data-table-td ">Roboticvisiontech Llc</td><td class="patent-data-table-td ">Method and apparatus for single image 3D vision guided robotics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8437535">US8437535</a></td><td class="patent-data-table-td patent-date-value">Sep 19, 2007</td><td class="patent-data-table-td patent-date-value">May 7, 2013</td><td class="patent-data-table-td ">Roboticvisiontech Llc</td><td class="patent-data-table-td ">System and method of determining object pose</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8559699">US8559699</a></td><td class="patent-data-table-td patent-date-value">Oct 10, 2008</td><td class="patent-data-table-td patent-date-value">Oct 15, 2013</td><td class="patent-data-table-td ">Roboticvisiontech Llc</td><td class="patent-data-table-td ">Methods and apparatus to facilitate operations in image based systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120249784">US20120249784</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 1, 2011</td><td class="patent-data-table-td patent-date-value">Oct 4, 2012</td><td class="patent-data-table-td ">Lockheed Martin Corporation</td><td class="patent-data-table-td ">Method and apparatus for digital video latency reduction by real-time warping</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=myxpBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S294000">382/294</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=myxpBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S159000">382/159</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=myxpBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc345/defs345.htm&usg=AFQjCNF0b52M2HqQQp5rThx3mQ75nwjbGg#C345S651000">345/651</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=myxpBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06K0009620000">G06K9/62</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=myxpBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06K9/6253">G06K9/6253</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06K9/62B5</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Feb 26, 2013</td><td class="patent-data-table-td ">FPB1</td><td class="patent-data-table-td ">Expired due to reexamination which canceled all claims</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 6, 2012</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 23, 2009</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090424</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 7, 2008</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 28, 2001</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COGNEX CORPORATION, MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BACHELDER, IVAN;JACOBSON, LOWELL;NEGRO, JAY;AND OTHERS;REEL/FRAME:011956/0449;SIGNING DATES FROM 20010618 TO 20010625</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COGNEX CORPORATION ONE VISION DRIVENATICK, MASSACH</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BACHELDER, IVAN /AR;REEL/FRAME:011956/0449;SIGNING DATESFROM 20010618 TO 20010625</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U0XfTmbRzAfElixW3tnSbhjPahKgA\u0026id=myxpBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U37Lpi2GCtALEGLKzu6ASR3iNdt4w\u0026id=myxpBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U09mXvlo9TeMbarhi3zWaQBw_D2MQ","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Method_and_system_for_aligning_geometric.pdf?id=myxpBAABERAJ\u0026output=pdf\u0026sig=ACfU3U28tSWtjKcRIvzkuPUxNdzuC5C1zA"},"sample_url":"http://www.google.com/patents/reader?id=myxpBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>