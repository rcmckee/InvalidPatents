<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US8373799 - Visual effects for video calls - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_4ff636b3d23669b7103f3b3a3a18b4cd/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_4ff636b3d23669b7103f3b3a3a18b4cd__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Visual effects for video calls"><meta name="DC.contributor" content="Erika Reponen" scheme="inventor"><meta name="DC.contributor" content="Jarmo KAUKO" scheme="inventor"><meta name="DC.contributor" content="Nokia Corporation" scheme="assignee"><meta name="DC.date" content="2006-12-29" scheme="dateSubmitted"><meta name="DC.description" content="An apparatus including a display, an input unit and a processor connected to the display and input unit, the processor being configured to recognize an input and embed at least one effect, in response to the input, into a video feed transmitted from the apparatus."><meta name="DC.date" content="2013-2-12" scheme="issued"><meta name="DC.relation" content="US:20080111822:A1" scheme="references"><meta name="DC.relation" content="US:7266289" scheme="references"><meta name="citation_patent_number" content="US:8373799"><meta name="citation_patent_application_number" content="US:11/617,940"><link rel="canonical" href="http://www.google.com/patents/US8373799"/><meta property="og:url" content="http://www.google.com/patents/US8373799"/><meta name="title" content="Patent US8373799 - Visual effects for video calls"/><meta name="description" content="An apparatus including a display, an input unit and a processor connected to the display and input unit, the processor being configured to recognize an input and embed at least one effect, in response to the input, into a video feed transmitted from the apparatus."/><meta property="og:title" content="Patent US8373799 - Visual effects for video calls"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("weLoU7npHJSnyAS17YK4BQ"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407291699.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("LUX"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("weLoU7npHJSnyAS17YK4BQ"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407291699.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("LUX"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us8373799?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US8373799"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=s9zDBwABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS8373799&amp;usg=AFQjCNElnCWaMTQoMc6nST6U1UBzqYp3RA" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US8373799.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US8373799.pdf"></a><a class="appbar-application-grant-link" data-label="Application" href="/patents/US20080158334"></a><a class="appbar-application-grant-link" data-selected="true" data-label="Grant" href="/patents/US8373799"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US8373799" style="display:none"><span itemprop="description">An apparatus including a display, an input unit and a processor connected to the display and input unit, the processor being configured to recognize an input and embed at least one effect, in response to the input, into a video feed transmitted from the apparatus....</span><span itemprop="url">http://www.google.com/patents/US8373799?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US8373799 - Visual effects for video calls</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US8373799 - Visual effects for video calls" title="Patent US8373799 - Visual effects for video calls"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US8373799 B2</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 11/617,940</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Feb 12, 2013</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Dec 29, 2006</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Dec 29, 2006</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US20080158334">US20080158334</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">11617940, </span><span class="patent-bibdata-value">617940, </span><span class="patent-bibdata-value">US 8373799 B2, </span><span class="patent-bibdata-value">US 8373799B2, </span><span class="patent-bibdata-value">US-B2-8373799, </span><span class="patent-bibdata-value">US8373799 B2, </span><span class="patent-bibdata-value">US8373799B2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Erika+Reponen%22">Erika Reponen</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Jarmo+KAUKO%22">Jarmo KAUKO</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Nokia+Corporation%22">Nokia Corporation</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US8373799.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8373799.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US8373799.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (2),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (11),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (3)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/8373799&usg=AFQjCNGhEyHkdxBIQBd7y_EBPnbd5ScXmA">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D8373799&usg=AFQjCNHxOKVH_jh-A48ibFUduN4SG_HOXw">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D8373799B2%26KC%3DB2%26FT%3DD&usg=AFQjCNFuldqQEeHoY-BiSk3yTzfck_WtZQ">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT120691877" lang="EN" load-source="patent-office">Visual effects for video calls</invention-title></span><br><span class="patent-number">US 8373799 B2</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA106613445" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">An apparatus including a display, an input unit and a processor connected to the display and input unit, the processor being configured to recognize an input and embed at least one effect, in response to the input, into a video feed transmitted from the apparatus.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(19)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US8373799B2/US08373799-20130212-D00018.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US8373799B2/US08373799-20130212-D00018.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(24)</span></span></div><div class="patent-text"><div mxw-id="PCLM49736373" lang="EN" load-source="patent-office" class="claims">
    <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
      <div class="claim-text">1. An apparatus comprising:
<div class="claim-text">a processor; and</div>
<div class="claim-text">a memory including computer program code, the memory and the computer program code configured to, with the processor, cause the apparatus to perform at least the following:</div>
<div class="claim-text">recognize at least one of a motion, gesture, keyboard emoticon input, touch-drawing input, emoticon selection input or spoken word provided via an input unit as user input provided by a user of a first communication device during a video call, video conference or video cast between the first communication device and one or more second communication devices;</div>
<div class="claim-text">embed at least one corresponding effect, in response to the user input, into a video feed associated with the video call, video conference or video cast; and</div>
<div class="claim-text">enable transmission of the video feed with the embedded effect from the first communication device to the one or more second communication devices.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
      <div class="claim-text">2. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one effect includes at least one of a photograph, text, graphic, image, animation, cartoon, logo or sound.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
      <div class="claim-text">3. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the user input is provided via an input unit including at least one of a keyboard, motion detector, touch-enabled screen or speech recognition unit.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
      <div class="claim-text">4. The apparatus of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the apparatus is caused to recognize text input by a user of the apparatus as the keyboard emoticon input and embed a corresponding emoticon effect into the video feed.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
      <div class="claim-text">5. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the apparatus is or forms part of the first communication device, wherein the first communication device is a mobile communication device, a phone, a PDA, a personal communicator, a tablet computer, a laptop, a computer, a desktop computer, a television or a television set top box.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
      <div class="claim-text">6. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the apparatus is further configured to receive the video feed,
<div class="claim-text">wherein
<div class="claim-text">the user input is provided at a particular time during reception of the video feed; and</div>
<div class="claim-text">the effect is embedded such that the timing of the embedded effect in the video feed corresponds to the particular time.</div>
</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
      <div class="claim-text">7. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the motion or gesture of the user comprises one or more of a non-verbal gesture, a facial expression, a body movement, a smile, a frown, a shoulder shrug, or a winking gesture.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00008" num="00008" class="claim">
      <div class="claim-text">8. A method comprising:
<div class="claim-text">recognizing at least one of a motion, gesture, keyboard emoticon input, touch-drawing input, emoticon selection input or spoken word provided via an input unit as user input provided by a user of a first communication device during a video call, video conference or video cast between the first communication device and one or more second communication devices;</div>
<div class="claim-text">embedding at least one corresponding effect, in response to the user input, into a video feed associated with the video call, video conference or video cast;</div>
<div class="claim-text">enabling transmission of the video feed with the embedded effect from the first communication device to the one or more second communication devices.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00009" num="00009" class="claim">
      <div class="claim-text">9. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the at least one effect includes at least one of a photograph, text, graphic, image, animation, cartoon, logo or sound.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00010" num="00010" class="claim">
      <div class="claim-text">10. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the user input includes text entered by a user of the first communication device.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00011" num="00011" class="claim">
      <div class="claim-text">11. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the method comprises:
<div class="claim-text">receiving the video feed in at least a second apparatus so that the at least one effect is presented to a user of the at least second apparatus in the received video feed.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
      <div class="claim-text">12. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the at least one effect is presented in the at least second apparatus so as not to obstruct a face of a participant in the video call, conference or cast.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
      <div class="claim-text">13. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the at least one effect is presented in the at least second apparatus so that the at least one effect is presented in a predefined location.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00014" num="00014" class="claim">
      <div class="claim-text">14. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the at least one effect is presented in the at least second apparatus so that the at least one effect is presented in random places on a display of the at least second apparatus.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
      <div class="claim-text">15. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising selecting the at least one effect from a set of effects so that the at least one effect corresponds to the user input.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
      <div class="claim-text">16. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising selecting the at least one effect from a set of effects so that the at least one effect is randomly chosen from the set.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
      <div class="claim-text">17. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the at least one effect includes characters or symbols representing, emoticons input into the first apparatus by a user of the first apparatus.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
      <div class="claim-text">18. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the at least one effect is a misrepresentation of the user input.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
      <div class="claim-text">19. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein at least one of the apparatus is a mobile communication device, a phone, a PDA, a personal communicator, a tablet computer, a laptop, a computer, a desktop computer, a television or a television set top box.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
      <div class="claim-text">20. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the at least one effect is presented in the at least second apparatus so that the at least one effect follows a movement of a feature presented in the video feed.</div>
    </div>
    </div> <div class="claim"> <div id="CLM-00021" num="00021" class="claim">
      <div class="claim-text">21. A computer program product comprising:
<div class="claim-text">a computer useable medium having computer readable code embodied therein, the computer readable code being configured to, upon execution, cause an apparatus to at least:</div>
<div class="claim-text">recognize at least one of a motion, gesture, keyboard emoticon input, touch-drawing input, emoticon selection input or spoken word provided via an input unit as user input provided by a user of a first communication device during a video call, video conference or video cast between the first communication device and one or more second communication devices;</div>
<div class="claim-text">embed at least one corresponding effect, in response to the user input, into a video feed associated with the video call, video conference or video cast;</div>
<div class="claim-text">enable transmission of the video feed with the embedded effect from the first communication device to the one or more second communication devices.</div>
</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00022" num="00022" class="claim">
      <div class="claim-text">22. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the at least one effect includes at least one of a photograph, text, graphic, image, animation, cartoon, logo or sound.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00023" num="00023" class="claim">
      <div class="claim-text">23. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the user input includes text entered by a user of the first communication device the text corresponding to emoticon input to be embedded as the effect.</div>
    </div>
    </div> <div class="claim-dependent"> <div id="CLM-00024" num="00024" class="claim">
      <div class="claim-text">24. The computer program product of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the computer readable program code is further configured to, upon execution, cause a second apparatus to receive the video feed so that the at least one effect is presented to a user of the at least second apparatus in the received video feed. </div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES56807305" lang="EN" load-source="patent-office" class="description">
    <heading>BACKGROUND</heading> <p num="p-0002">1. Field</p>
    <p num="p-0003">The disclosed embodiments relate to user interfaces and, more particularly, to user interfaces for communicating with other devices.</p>
    <p num="p-0004">2. Brief Description of Related Developments</p>
    <p num="p-0005">When people speak with each other non-verbal communication such as body movement, hand and face gestures play a large part in the conversation. Hand and face gestures may indicate to others an emotional state of the speaker. For example, when the speaker has a frown on his/her face it generally means that the speaker is sad or upset. When the speaker is smiling it generally means the speaker is in a happy mood. Likewise, when the speaker is frantically waving his/her arms when speaking there may be an indication that the user is excited in some way.</p>
    <p num="p-0006">As the individual lifestyles becomes increasingly busy there is less time to sit with friends and family to have a face to face conversation. In addition, people living far away from each other may not be able to have face to face conversations with others. As such, more and more people are using communication devices to hold conversations, as opposed to a face to face talk, to communicate with co-workers, friends, family and the like. However, non-verbal communication such as body movement, hand and facial gestures may be lacking from the conversation. These non-verbal communications are an important part of a conversation as they may indicate to a listener the emotional state of the speaker, they may emphasize what the speaker is saying and the like.</p>
    <p num="p-0007">Video calls may be used to enable individuals who are unable to have a conventional face to face conversation (e.g. a conversation where the individuals are physically located next to each other) to converse while looking at each other. However, in video calls usually only the face of the participants are shown on the display of the receiving device. Any facial expressions that do appear on the display of the receiving device may be misinterpreted or hard to see because of, for example, poor display resolution or a small display size, represented on the display at the wrong time due to network lag, and the like. Also due to the limited display size the body movements such as hand gestures may not be visible to the participants of the video call.</p>
    <p num="p-0008">It would be advantageous to supplement the video feed in a video call with visualizations that coincide with the content of the call in a manner that is clear to a participant of the call.</p>
    <heading>SUMMARY</heading> <p num="p-0009">In one embodiment, an apparatus is provided. The apparatus includes a display, an input unit and a processor connected to the display and input unit, the processor being configured to recognize an input and embed at least one effect, in response to the input, into a video feed transmitted from the apparatus.</p>
    <p num="p-0010">In another embodiment, a method is provided. The method includes recognizing an event with a first device during a video call or cast, embedding at least one effect, in response to the event, into a video feed transmitted from the first device and receiving the video feed in at least a second device so that the at least one effect is presented to a user of the at least second device in the received video feed.</p>
    <p num="p-0011">In one embodiment, a computer program product is provided. The computer program product includes a computer useable medium having computer readable code means embodied therein for causing a computer embed at least one effect into a video feed. The computer readable code means in the computer program product includes computer readable program code means for causing a computer to recognize an event during a video call or cast, computer readable program code means for causing a computer to embed at least one effect, in response to the event, into a video feed transmitted from the computer and computer readable program code means for causing at least a second computer to receive the video feed so that the at least one effect is presented to a user of the at least second computer in the received video feed.</p>
    <description-of-drawings> <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p num="p-0012">The foregoing aspects and other features of the disclosed embodiments are explained in the following description, taken in connection with the accompanying drawings, wherein:</p>
      <p num="p-0013"> <figref idrefs="DRAWINGS">FIG. 1</figref> illustrates a device in which aspects of the disclosed embodiments may be employed;</p>
      <p num="p-0014"> <figref idrefs="DRAWINGS">FIG. 2</figref> is a flow diagram in accordance with an embodiment;</p>
      <p num="p-0015"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a schematic illustrate of a device incorporating features of an embodiment;</p>
      <p num="p-0016"> <figref idrefs="DRAWINGS">FIG. 4</figref> is a table illustrating aspects of an embodiment;</p>
      <p num="p-0017"> <figref idrefs="DRAWINGS">FIG. 5</figref> is a flow diagram in accordance with an embodiment;</p>
      <p num="p-0018"> <figref idrefs="DRAWINGS">FIG. 6</figref> is a flow diagram in accordance with an embodiment;</p>
      <p num="p-0019"> <figref idrefs="DRAWINGS">FIG. 7</figref> is a table illustrating aspects of an embodiment;</p>
      <p num="p-0020"> <figref idrefs="DRAWINGS">FIG. 8</figref> is a table illustrating aspects of an embodiment;</p>
      <p num="p-0021"> <figref idrefs="DRAWINGS">FIG. 9</figref> is a flow diagram in accordance with an embodiment;</p>
      <p num="p-0022"> <figref idrefs="DRAWINGS">FIG. 10</figref> is a flow diagram in accordance with an embodiment;</p>
      <p num="p-0023"> <figref idrefs="DRAWINGS">FIGS. 11-13</figref> are screen shots illustrating aspects of the disclosed embodiments;</p>
      <p num="p-0024"> <figref idrefs="DRAWINGS">FIG. 14</figref> shows a device incorporating features of an embodiment;</p>
      <p num="p-0025"> <figref idrefs="DRAWINGS">FIG. 15</figref> shows another device incorporating features of an embodiment;</p>
      <p num="p-0026"> <figref idrefs="DRAWINGS">FIG. 16</figref> is a block diagram illustrating the general architecture of an exemplary mobile device in which aspects of an embodiment may be employed;</p>
      <p num="p-0027"> <figref idrefs="DRAWINGS">FIG. 17</figref> is a schematic illustration of a cellular telecommunications system, as an example, of an environment in which a communications device incorporating features of an exemplary embodiment may be applied; and</p>
      <p num="p-0028"> <figref idrefs="DRAWINGS">FIG. 18</figref> illustrates a block diagram of an example of an apparatus incorporating features that may be used to practice aspects of the disclosed embodiments.</p>
    </description-of-drawings> <heading>DETAILED DESCRIPTION OF THE EXEMPLARY EMBODIMENT(s)</heading> <p num="p-0029"> <figref idrefs="DRAWINGS">FIG. 1</figref> shows a communication device <b>100</b> in which aspects of the disclosed embodiments may be implemented. Although the embodiments disclosed will be described with reference to the embodiments shown in the drawings, it should be understood that the embodiments disclosed can be embodied in many alternate forms of embodiments. In addition, any suitable size, shape or type of elements or materials could be used.</p>
    <p num="p-0030">The communication device <b>100</b> may be any suitable communication device such as, for example a personal communicator, a tablet computer, a laptop or desktop computer, a television or television set top box or any other suitable device. In this example, the device <b>100</b> includes a keypad <b>130</b>, a display <b>120</b>, and a camera <b>140</b>. The camera <b>140</b> in <figref idrefs="DRAWINGS">FIG. 1</figref> is shown as being part of the device <b>100</b> but it should be understood that the camera may be a peripheral device connected to the device <b>100</b> via any suitable wired or wireless connection. The device may enable a user to communicate with other devices such as, for example, mobile communication devices, laptop computers, desktop computers and the like over any suitable communication network, such as network <b>150</b>. The network may be a cellular network, wide area network, local area network, the internet, mobile television network and the like.</p>
    <p num="p-0031">In accordance with the disclosed embodiments the device <b>100</b> may allow a user to initiate a video call, video conference or video cast to one or more other devices over the network <b>150</b> (<figref idrefs="DRAWINGS">FIG. 2</figref>, Block <b>200</b>). The device <b>100</b> may be configured recognize or acknowledge an event that indicates that a visual effect is to be added or embedded in the video feed of the call (<figref idrefs="DRAWINGS">FIG. 2</figref>, Block <b>210</b>) so that the effects are part of the video. Upon acknowledging the event the effect, such as a visual effect, may be added to the video feed (<figref idrefs="DRAWINGS">FIG. 2</figref>, Block <b>220</b>). The effect may be transferred to the receiving device where it is presented on the display of the receiving device over the video feed (<figref idrefs="DRAWINGS">FIG. 2</figref>, Block <b>230</b>).</p>
    <p num="p-0032">The effects that are added to the video feed and presented to the recipients of the video may correspond to an aspect of the video call, such as for example, the content of a call at any given time, the participant's attitude or state of mind at any time during the call and the like. The disclosed embodiments may supplement the video and audio feed of the call to accurately convey what the speaker is saying, for example, as it would be conveyed if the participants were having a face to face conversation. The disclosed embodiments may provide a substitute for non-verbal communications that are present but may not otherwise be visible during the video call. The presentation of the effects may also help one to understand the meaning of the conversation if the video and/or audio quality of the call are poor.</p>
    <p num="p-0033">The disclosed embodiments may allow a user of the device to have some form of entertainment during by causing, for example, comical effects to be displayed on the recipients devices. The disclosed embodiment may also allow users to personalize video calls/casts.</p>
    <p num="p-0034">Referring now to <figref idrefs="DRAWINGS">FIG. 3</figref>, a schematic illustration of a communications device <b>300</b> is shown. The communication device <b>300</b> may be substantially similar to that described above with respect to <figref idrefs="DRAWINGS">FIG. 1</figref>. The device <b>300</b> may include any suitable features such as, for example, a display <b>360</b>, a keyboard <b>330</b>, a camera <b>350</b>. The device <b>300</b> may also include other features such as any suitable motion detectors <b>320</b>, any suitable speech recognition units <b>310</b> and visual effect sets <b>340</b>. The display may be a touch enabled display. The keyboard may be any suitable keyboard including, but not limited to, a T9 or qwerty keyboard. The visual effects sets <b>340</b> may include any suitable visual effects that are added to the video feed during a video call/cast. The effect sets <b>340</b> may be individual effects or sets of effects that have, for example, a common theme (e.g. outdoors, celebrity, animal, faces and the like). The effect sets <b>340</b> may be user defined, obtained from other devices or downloaded from, for example the internet or some other source external to the device <b>300</b>. The effect sets <b>340</b> may include, but are not limited to, any combination of photographs, text, graphics, images, animations, cartoons, logos, videos, sounds and the like. The device <b>300</b> may include any suitable number of effect sets <b>340</b>.</p>
    <p num="p-0035">The user of device <b>300</b> may initiate a video call/cast to the user of device <b>370</b>. The user of device <b>300</b> may for any suitable reason want to add effects to the video broadcast from the device <b>300</b>. Reasons for adding effects to the video feed include, but are not limited to, entertainment, emphasis during the conversation, to convey an emotion, an advertisement, describe an event discussed during the call/cast and the like.</p>
    <p num="p-0036">In one embodiment the device <b>300</b> may be configured to recognize certain words or phrases via the speech recognition unit <b>310</b>. In this example, there may be a suitable settings menu in the device <b>310</b> for a user to associate a word or phrase with one or more effects. The device <b>300</b> may be configured to store any suitable information pertaining to the effects in any suitable manner. This information may be associated or mapped to other respective information in any suitable manner. For exemplary purposes only, a table is shown in <figref idrefs="DRAWINGS">FIG. 4</figref> illustrating a memory table <b>400</b> of the device <b>300</b>. The table may include any suitable information including, but not limited to, effect sets <b>420</b>, text associated with the effect set <b>410</b> and a voice clip <b>430</b> associated with a respective effect. In this example a user defined effect set <b>420</b> is shown in table <b>400</b>. The set may include any suitable number of effects. In this example, the device <b>300</b> may be configured so that the word “smile” and voice clip “Smile.mpg” is associated with a smile emoticon <b>440</b>. The voice clip may have any suitable file format and may be the user's voice or any other suitable voice pronouncing the word “smile”. In alternate embodiments the voice clip may include the pronunciation of any other word so that the smile appears when the other word is spoken. Similarly, the word “sad” and voice clip “Sad.wav” is associated with the sad emoticon <b>450</b>, the word “crash” and voice clip “Crash.avi” is associated with crash wordart <b>460</b> and the word “boom” and the sound clip “Boom.mpg” is associated with boom wordart <b>470</b>.</p>
    <p num="p-0037">During the video call/cast the user of device <b>300</b> may say the word “smile” (<figref idrefs="DRAWINGS">FIG. 5</figref>, Block <b>500</b>). The speech recognition unit <b>310</b> may recognize the word “smile” and cause the device <b>300</b> to insert the smile emoticon <b>440</b> into the video feed broadcast from the device <b>300</b> (<figref idrefs="DRAWINGS">FIG. 5</figref>, Blocks <b>510</b> and <b>520</b>). Although the effects are described in this example as being added into the video feed of the sending device it is noted that in the embodiments disclosed herein the effects may imposed over an incoming video feed in a receiving device. The smile emoticon <b>440</b> may be received in the other device(s) <b>370</b> along with the video feed and displayed on a display of the other device (<figref idrefs="DRAWINGS">FIG. 5</figref>, Block <b>530</b>). The effect, such as the smile emoticon <b>440</b> may be presented on the display of the device for any suitable period of time. For example, the display period for the effect may be settable by the sender of the effect or the recipient of the effect. In other embodiments the display period may be some predefined period of time set during the manufacture of the device. The addition of the effect may be automatic upon the recognition of a certain word or it may be input into the video feed manually. In other embodiments, the user may receive a prompt asking the user if the effect should be added to the video feed. In alternate embodiments, sounds may be inserted into the video feed. For example, then the word “crash” appears on the receiving device a crash sound may also be played. In other alternate embodiments the sound may be played without any visual effect.</p>
    <p num="p-0038">In another embodiment the device <b>300</b> may be configured to recognize non-verbal gestures (e.g. facial expressions, body movements and the like) of the user via the motion detectors <b>320</b>. In this example, the device may include any suitable software or hardware that would enable the device <b>300</b> to recognize the non-verbal gestures. For exemplary purposes only, a table is shown in <figref idrefs="DRAWINGS">FIG. 7</figref> illustrating a memory table <b>700</b> of the device <b>300</b>. The table may include, but is not limited to, an effect set <b>710</b> and an associated gesture <b>720</b>. In this example, the device <b>300</b> may be configured so that the non-verbal gesture of a smile is associated with a smile emoticon <b>730</b>. Similarly, the non-verbal gesture of a frown is associated with the sad emoticon <b>740</b>, the non-verbal gesture of a shoulder shrug is associated with the sad emoticon <b>750</b> and the non-verbal gesture of a winking is associated with a wink emoticon <b>760</b>.</p>
    <p num="p-0039">During the video call/cast the user of device <b>300</b> may, for example, frown (<figref idrefs="DRAWINGS">FIG. 6</figref>, Block <b>600</b>). The motion detectors <b>320</b> may recognize the frown and cause the device <b>300</b> to insert the frown emoticon <b>740</b> into the video feed broadcast from the device <b>300</b> (<figref idrefs="DRAWINGS">FIG. 6</figref>, Blocks <b>610</b> and <b>620</b>). The frown emoticon <b>740</b> may be received in the other device(s) <b>370</b> along with the video feed and displayed on a display of the other device (<figref idrefs="DRAWINGS">FIG. 6</figref>, Block <b>630</b>). The effect, such as the frown emoticon <b>740</b> may be presented on the display of the device for any suitable period of time. For example, the display period for the effect may be settable by the sender of the effect or the recipient of the effect. In other embodiments the display period may be some predefined period of time set during the manufacture of the device. The addition of the effect may be automatic upon the recognition of a certain word or it may be input into the video feed manually. In other embodiments, the user may receive a prompt asking the user if the effect should be added to the video feed. In other embodiments the device <b>300</b> may have motion sensors <b>380</b> for detecting the movement of the device so that the effects added to the video feed correspond to the motion of the phone. For example, is a user of device <b>300</b> shakes the device a crash effect may appear in the video feed. In another example, if the user of device <b>300</b> makes shapes by moving the device <b>300</b> the corresponding shapes may appear in the video feed. In alternate embodiments any suitable effects can be created and inserted into the video feed by moving the device <b>300</b> around.</p>
    <p num="p-0040">In one embodiment the effects may be added to the video feed by typing via the keyboard <b>330</b>. In alternate embodiments the text or any other suitable image or effect may be input using a pointer on a touch enabled screen of the device <b>300</b>. For exemplary purposes only, a table is shown in <figref idrefs="DRAWINGS">FIG. 8</figref> illustrating a memory table <b>800</b> of the device <b>300</b>. The table <b>800</b> may include, but is not limited to, an effect set <b>810</b> and an associated text representation of the gesture <b>820</b>. In this example, the device <b>300</b> may be configured so that the text “:)” is associated with a smile emoticon <b>830</b>. Similarly, the text“:(” may be associated with the sad emoticon <b>840</b> and the text“;)” may be associated with the wink effect <b>850</b>. In alternate embodiments, the effects may appear on the display of a recipient's device exactly as it appears when entered by the user. For example, if the user enters the text “:)” the text “:)” will appear on the recipients display rather than the emoticon smile emoticon. In other embodiments the user may enter symbols or any other suitable characters to be added to the video feed.</p>
    <p num="p-0041">During the video call/cast the user of device <b>300</b> may, for example, enter the text “;)” (<figref idrefs="DRAWINGS">FIG. 9</figref>, Block <b>900</b>). The device <b>300</b> may look at the table to determine the associated effect or it may add the text “:)” or any other image (e.g. drawings, photos, etc.) as it was inserted by the user into the video feed (<figref idrefs="DRAWINGS">FIG. 6</figref>, Blocks <b>910</b> and <b>920</b>). The text or corresponding effect <b>830</b> may be received in the other device(s) <b>370</b> along with the video feed and displayed on a display of the other device (<figref idrefs="DRAWINGS">FIG. 6</figref>, Block <b>630</b>). The text or corresponding effect may be presented on the display of the device for any suitable period of time. For example, the display period for the effect may be settable by the sender of the effect or the recipient of the effect. In other embodiments the display period may be some predefined period of time set during the manufacture of the device. The addition of the effect may be automatic upon the recognition of a certain word or it may be input into the video feed manually. In other embodiments, the user may receive a prompt asking the user if the effect should be added to the video feed.</p>
    <p num="p-0042">In another embodiment the user of device <b>300</b> may want to “draw” on the video image received in his device, such as during a conference call where more than one participant is displayed on the display of the device <b>300</b>. For example, when one of the participants is being mean during the call (e.g. the user of device <b>370</b>), the user of device <b>300</b> may draw horns on the head of the mean participant (<figref idrefs="DRAWINGS">FIG. 10</figref>, Block <b>1000</b>). The device <b>300</b> may recognize the input and send a signal back to device <b>370</b> to be inserted into the video feed broadcast from device <b>370</b> (<figref idrefs="DRAWINGS">FIG. 10</figref>, Blocks <b>1010</b>-<b>1030</b>). The image may be presented on the display of the other devices (<figref idrefs="DRAWINGS">FIG. 10</figref>, Block <b>1040</b>). In other embodiments the device <b>300</b> may send a signal to the devices of the other participants that may identify the feed to which the drawing is to be imposed (i.e. the video feed from device <b>370</b>) so that the other devices may impose the drawing over the feed from device <b>370</b> (<figref idrefs="DRAWINGS">FIG. 10</figref>, Blocks <b>1050</b> and <b>1060</b>). The image drawn over the image of the user of device <b>370</b> may follow the image as it is presented on the displays of the devices. For example as the user of device <b>370</b> moves his/her head around the horns will follow the head of the user of device <b>370</b> as it moves around on the display of the other devices.</p>
    <p num="p-0043">In other embodiments, there may be a virtual pet, face or avatar that is added to the video feed of the call/cast. The virtual pet or face may be any suitable image such as for example, the head or whole body of a dog, cat or person, a car and the like. As the user enters effects into the device <b>300</b>, a characteristic of the virtual pet or face changes according to the effect entered. For example, a virtual dog, such as the dog <b>1220</b> in <figref idrefs="DRAWINGS">FIG. 12B</figref>, is added to the video feed so that when a user of device <b>300</b> enters a happy effect the dog smiles and wags its tail. If the user enters an excited effect the dog may jump around and the like.</p>
    <p num="p-0044">Referring now to <figref idrefs="DRAWINGS">FIGS. 11-13</figref> exemplary screen shots are illustrated showing placement of the effects in the video feed. In the exemplary screen shots the effects may be added to the video feed in accordance with any of the disclosed embodiments described above. As can be seen in <figref idrefs="DRAWINGS">FIG. 11</figref>, a smiling face <b>1160</b>, a winking face <b>1165</b> and a laughing face <b>1170</b> are respectively added to the upper left of the screen shots <b>1100</b>-<b>1120</b>. A lightning bolt <b>1175</b>, text <b>1180</b> and hearts <b>1185</b> are added to the bottom left of the screen shots <b>1130</b>-<b>1150</b>. There may be any suitable settings menu in the device <b>300</b> for setting where the effects appear. For example the user may set the effects to appear in the upper left or right, lower left or right, center, along an edge of the image or the user may set the effects to appear in a random manner (e.g. anywhere on the display). In other embodiments, the location of the effects may be configured so that the effect follows a certain attribute of the video. For example, big red cartoon lips may be added to the video feed over the lips of a participant of a video call. The cartoon lips may follow the lips of the participant as the participant moves around in the video. In other embodiments the device may be configured to recognize faces so that the effects are not placed over a participant's face or conversely device may be configured to place the effects over a participant's face. For example, in <figref idrefs="DRAWINGS">FIG. 12A</figref> the device <b>300</b> may recognize the face of the participant shown in the Figure and place the effect <b>1210</b> in a portion of the video feed that does not cover the participant's face.</p>
    <p num="p-0045">Adding the effects tot he video feed may be context sensitive. For example, if the conversation is about how hot is it outside the effects added to the video may correspond to an effect set that pertains to heat (e.g. fire, the sun, etc.). If the conversation is about fashion the effects added to the video may be from an effect set pertaining to designer clothes and so on. In other embodiments, the effects added to the video feed may be randomly selected from one or more effect sets. For example, as can be seen in <figref idrefs="DRAWINGS">FIG. 12B</figref>, when the word “hot” is spoken the picture of the dog <b>1220</b> or any other random effect may be added to the video feed. In one embodiment intentional misunderstanding can be made with the effects where the effect has nothing to do with or is the opposite of (i.e. a misrepresentation of) the event triggering the addition of the effect into the video feed. For example, the user of device <b>300</b> may speak the word “hot” triggering the addition of a snowflake into the video feed. In another example, the user of device <b>300</b> may speak the word “car” triggering the addition of a cat into the video feed.</p>
    <p num="p-0046">In other embodiments, adding effects to the video feed may be used for advertising purposes. Companies may, for example contract with service providers so that as their company name is said an indication of the company's product is shown. For example, if a participant of the video call/cast mentions the word “computer” a logo <b>1310</b> for a computer company may appear. In another example, if a participant mentions a certain brand of car the logo or some other advertising for that car company may be added to the video feed.</p>
    <p num="p-0047">One embodiment of a device <b>300</b> in which the disclosed embodiments may be employed is illustrated in <figref idrefs="DRAWINGS">FIG. 14</figref>. The device may be any suitable device such as terminal or mobile communications device <b>1400</b>. The terminal <b>1400</b> may have a keypad <b>1410</b> and a display <b>1420</b>. The keypad <b>1410</b> may include any suitable user input devices such as, for example, a multi-function/scroll key <b>1430</b>, soft keys <b>1431</b>, <b>1432</b>, a call key <b>1433</b> and end call key <b>1434</b> and alphanumeric keys <b>1435</b>. The display <b>1420</b> may be any suitable display, such as for example, a touch screen display or graphical user interface. The display may be integral to the device <b>1400</b> or the display may be a peripheral display connected to the device <b>1400</b>. A pointing device, such as for example, a stylus, pen or simply the user's finger may be used with the display <b>1420</b>. In alternate embodiments any suitable pointing device may be used. In other alternate embodiments, the display may be a conventional display. The device <b>1400</b> may also include other suitable features such as, for example, a camera, loud speaker, motion detectors, speech recognition devices, connectivity ports or tactile feedback features. The mobile communications device may have a processor <b>1618</b> connected to the display for processing user inputs and displaying information on the display <b>1420</b>. A memory <b>1602</b> may be connected to the processor <b>1618</b> for storing any suitable information and/or applications associated with the mobile communications device <b>1400</b> such as phone book entries, calendar entries, web browser, etc.</p>
    <p num="p-0048">In one embodiment, the device, may be for example, a PDA style device <b>1400</b>′ illustrated in <figref idrefs="DRAWINGS">FIG. 15</figref>. The PDA <b>1400</b>′ may have a keypad <b>1410</b>′, a touch screen display <b>1420</b>′ and a pointing device <b>1450</b> for use on the touch screen display <b>1420</b>′. In still other alternate embodiments, the device may be a personal communicator, a tablet computer, a laptop or desktop computer, a television or television set top box or any other suitable device capable of containing the display <b>1420</b> and supported electronics such as the processor <b>1618</b> and memory <b>1602</b>.</p>
    <p num="p-0049"> <figref idrefs="DRAWINGS">FIG. 11</figref> illustrates in block diagram form one embodiment of a general architecture of the mobile device <b>1400</b>. The mobile communications device may have a processor <b>1618</b> connected to the display <b>1603</b> for processing user inputs and displaying information on the display <b>1603</b>. The processor <b>1618</b> controls the operation of the device and can have an integrated digital signal processor <b>1617</b> and an integrated RAM <b>1615</b>. The processor <b>1618</b> controls the communication with a cellular network via a transmitter/receiver circuit <b>1619</b> and an antenna <b>1620</b>. A microphone <b>1606</b> is coupled to the processor <b>1618</b> via voltage regulators <b>1621</b> that transform the user's speech into analog signals. The analog signals formed are A/D converted in an A/D converter (not shown) before the speech is encoded in the DSP <b>1617</b> that is included in the processor <b>1618</b>. The encoded speech signal is transferred to the processor <b>1618</b>, which e.g. supports, for example, the GSM terminal software. The digital signal-processing unit <b>1617</b> speech-decodes the signal, which is transferred from the processor <b>1618</b> to the speaker <b>1605</b> via a D/A converter (not shown).</p>
    <p num="p-0050">The voltage regulators <b>1621</b> form the interface for the speaker <b>1605</b>, the microphone <b>1606</b>, the LED drivers <b>1101</b> (for the LEDS backlighting the keypad <b>1607</b> and the display <b>1603</b>), the SIM card <b>1622</b>, battery <b>1624</b>, the bottom connector <b>1627</b>, the DC jack <b>1631</b> (for connecting to the charger <b>1633</b>) and the audio amplifier <b>1632</b> that drives the (hands-free) loudspeaker <b>1625</b>.</p>
    <p num="p-0051">A processor <b>1618</b> can also include memory <b>1602</b> for storing any suitable information and/or applications associated with the mobile communications device <b>1400</b> such as phone book entries, calendar entries, etc.</p>
    <p num="p-0052">The processor <b>1618</b> also forms the interface for peripheral units of the device, such as for example, a (Flash) ROM memory <b>1616</b>, the graphical display <b>1603</b>, the keypad <b>1607</b>, a ringing tone selection unit <b>1626</b>, an incoming call detection unit <b>1628</b>, the speech recognition unit <b>310</b> and a motion detectors <b>320</b>. These peripherals may be hardware or software implemented. In alternate embodiments, any suitable peripheral units for the device can be included.</p>
    <p num="p-0053">The software in the RAM <b>1615</b> and/or in the flash ROM <b>1616</b> contains instructions for the processor <b>1618</b> to perform a plurality of different applications and functions such as, for example, those described herein.</p>
    <p num="p-0054"> <figref idrefs="DRAWINGS">FIG. 17</figref> is a schematic illustration of a cellular telecommunications system, as an example, of an environment in which a communications device <b>1700</b> incorporating features of an embodiment may be applied. Communication device <b>1700</b> may be substantially similar to that described above with respect to terminal <b>300</b>. In the telecommunication system of <figref idrefs="DRAWINGS">FIG. 17</figref>, various telecommunications services such as cellular voice calls, www/wap browsing, cellular video calls, data calls, facsimile transmissions, music transmissions, still image transmission, video transmissions, electronic message transmissions and electronic commerce may be performed between the mobile terminal <b>1700</b> and other devices, such as another mobile terminal <b>1706</b>, a stationary telephone <b>1732</b>, or an internet server <b>1722</b>. It is to be noted that for different embodiments of the mobile terminal <b>1700</b> and in different situations, different ones of the telecommunications services referred to above may or may not be available. The aspects of the invention are not limited to any particular set of services in this respect.</p>
    <p num="p-0055">The mobile terminals <b>1700</b>, <b>1706</b> may be connected to a mobile telecommunications network <b>1710</b> through radio frequency (RF) links <b>1702</b>, <b>1708</b> via base stations <b>1704</b>, <b>1709</b>. The mobile telecommunications network <b>1710</b> may be in compliance with any commercially available mobile telecommunications standard such as GSM, UMTS, D-AMPS, CDMA2000, FOMA and TD-SCDMA.</p>
    <p num="p-0056">The mobile telecommunications network <b>1710</b> may be operatively connected to a wide area network <b>1720</b>, which may be the internet or a part thereof. An internet server <b>1722</b> has data storage <b>1724</b> and is connected to the wide area network <b>1720</b>, as is an internet client computer <b>1726</b>. The server <b>1722</b> may host a www/hap server capable of serving www/hap content to the mobile terminal <b>1700</b>.</p>
    <p num="p-0057">For example, a public switched telephone network (PSTN) <b>1730</b> may be connected to the mobile telecommunications network <b>1710</b> in a familiar manner. Various telephone terminals, including the stationary telephone <b>1732</b>, may be connected to the PSTN <b>1730</b>.</p>
    <p num="p-0058">The mobile terminal <b>1700</b> is also capable of communicating locally via a local link <b>1701</b> to one or more local devices <b>1703</b>. The local link <b>1701</b> may be any suitable type of link with a limited range, such as for example Bluetooth, a Universal Serial Bus (USB) link, a wireless Universal Serial Bus (WUSB) link, an IEEE 802.11 wireless local area network (WLAN) link, an RS-232 serial link, etc. The local devices <b>1703</b> can, for example, be various sensors that can communicate measurement values to the mobile terminal <b>1700</b> over the local link <b>1701</b>. The above examples are not intended to be limiting, and any suitable type of link may be utilized. The local devices <b>1703</b> may be antennas and supporting equipment forming a WLAN implementing Worldwide Interoperability for Microwave Access (WiMAX, IEEE 802.16), WiFi (IEEE 802.11x) or other communication protocols. The WLAN may be connected to the internet. The mobile terminal <b>1700</b> may thus have multi-radio capability for connecting wirelessly using mobile communications network <b>1710</b>, WLAN or both. Communication with the mobile telecommunications network <b>1710</b> may also be implemented using WiFi, WiMax, or any other suitable protocols, and such communication may utilize unlicensed portions of the radio spectrum (e.g. unlicensed mobile access (UMA)).</p>
    <p num="p-0059">The disclosed embodiments may also include software and computer programs incorporating the process steps and instructions described herein that are executed in different computers. <figref idrefs="DRAWINGS">FIG. 18</figref> is a block diagram of one embodiment of a typical apparatus <b>1800</b> incorporating features that may be used to practice aspects of the embodiments. As shown, a computer system <b>1802</b> may be linked to another computer system <b>1804</b>, such that the computers <b>1802</b> and <b>1804</b> are capable of sending information to each other and receiving information from each other. In one embodiment, computer system <b>1802</b> could include a server computer adapted to communicate with a network <b>1806</b>. Computer systems <b>1802</b> and <b>1804</b> can be linked together in any conventional manner including, for example, a modem, hard wire connection, or fiber optic link. Generally, information can be made available to both computer systems <b>1802</b> and <b>1804</b> using a communication protocol typically sent over a communication channel or through a dial-up connection on ISDN line. Computers <b>1802</b> and <b>1804</b> are generally adapted to utilize program storage devices embodying machine readable program source code which are adapted to cause the computers <b>1802</b> and <b>1804</b> to perform the method steps disclosed herein. The program storage devices incorporating aspects of the invention may be devised, made and used as a component of a machine utilizing optics, magnetic properties and/or electronics to perform the procedures and methods disclosed herein. In alternate embodiments, the program storage devices may include magnetic media such as a diskette or computer hard drive, which is readable and executable by a computer. In other alternate embodiments, the program storage devices could include optical disks, read-only-memory (“ROM”) floppy disks and semiconductor materials and chips.</p>
    <p num="p-0060">Computer systems <b>1802</b> and <b>1804</b> may also include a microprocessor for executing stored programs. Computer <b>1802</b> may include a data storage device <b>1808</b> on its program storage device for the storage of information and data. The computer program or software incorporating the processes and method steps incorporating aspects of the invention may be stored in one or more computers <b>1802</b> and <b>1804</b> on an otherwise conventional program storage device. In one embodiment, computers <b>1802</b> and <b>1804</b> may include a user interface <b>1810</b>, and a display interface <b>1812</b> from which aspects of the invention can be accessed. The user interface <b>1810</b> and the display interface <b>1812</b> can be adapted to allow the input of queries and commands to the system, as well as present the results of the commands and queries.</p>
    <p num="p-0061">The disclosed embodiments may allow a user of the device to have some form of entertainment during by causing, for example, comical effects to be displayed on the recipients devices. The disclosed embodiments may also allow users to personalize video calls/casts.</p>
    <p num="p-0062">In accordance with the disclosed embodiments the communication device may allow a user to initiate a video call, video conference or video cast to one or more other devices over a network. The device may be configured recognize or acknowledge an event that indicates that a visual effect is to be added to or imposed upon the video feed of the call. Upon acknowledging the event the effect, such as a visual effect, may be added to the video feed. The effect may be transferred to the receiving device where it is presented on the display of the receiving device over the video feed or the effect may be imposed over the video feed by the receiving device.</p>
    <p num="p-0063">The effects that are added to the video feed and presented to the recipients of the video may correspond to an aspect of the video call, such as for example, the content of a call at any given time (i.e. context sensitive), the participant's attitude or state of mind at any time during the call and the like. The disclosed embodiments may supplement the video and audio feed of the call to accurately convey what the speaker is saying, for example, as it would be conveyed if the participants were having a face to face conversation. The disclosed embodiments may provide a substitute for non-verbal communications that are present but may not otherwise be visible during the video call. The presentation of the effects may also help one to understand the meaning of the conversation if the video and/or audio quality of the call are poor. It is also noted that the embodiments described herein may be employed individually or in any combination with each other.</p>
    <p num="p-0064">It should be understood that the foregoing description is only illustrative of the embodiments. Various alternatives and modifications can be devised by those skilled in the art without departing from the embodiments. Accordingly, the present embodiments are intended to embrace all such alternatives, modifications and variances that fall within the scope of the appended claims.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7266289">US7266289</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 22, 2002</td><td class="patent-data-table-td patent-date-value">Sep 4, 2007</td><td class="patent-data-table-td ">Pioneer Corporation</td><td class="patent-data-table-td ">Device and method for recording video signal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20080111822">US20080111822</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 22, 2006</td><td class="patent-data-table-td patent-date-value">May 15, 2008</td><td class="patent-data-table-td ">Yahoo, Inc.!</td><td class="patent-data-table-td ">Method and system for presenting video</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348S473000">348/473</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348S578000">348/578</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348S460000">348/460</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348S014080">348/14.08</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348S468000">348/468</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348S461000">348/461</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04N0007140000">H04N7/14</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04N0007080000">H04N7/08</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L15/265">G10L15/265</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N7/147">H04N7/147</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=s9zDBwABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M1/72552">H04M1/72552</a></span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Apr 15, 2014</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIMS 3, 4, 7, 10 AND 23 ARE CANCELLED. CLAIMS 1, 8 AND 21 ARE DETERMINED TO BE PATENTABLE AS AMENDED. CLAIMS 2, 5-6, 9, 11-20, 22 AND 24, DEPENDENT ON AN AMENDED CLAIM, ARE DETERMINED TO BE PATENTABLE. NEW CLAIMS 25-30 ARE ADDED AND DETERMINED TO BE PATENTABLE.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 1, 2013</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20130807</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 22, 2007</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">NOKIA CORPORATION, FINLAND</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:REPONEN, ERIKA;KAUKO, JARMO;REEL/FRAME:019048/0911;SIGNING DATES FROM 20070306 TO 20070307</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:REPONEN, ERIKA;KAUKO, JARMO;SIGNING DATES FROM 20070306 TO 20070307;REEL/FRAME:019048/0911</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_4ff636b3d23669b7103f3b3a3a18b4cd.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U1Iw3wpgGsniZT-HqZbOHeklu8Icw\u0026id=s9zDBwABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U1Zuh8u2hC5_4Yr-naB3HfUlkqfcw\u0026id=s9zDBwABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U0AaL_zjH90AgNqwBvv_K6Y2a8b9g","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Visual_effects_for_video_calls.pdf?id=s9zDBwABERAJ\u0026output=pdf\u0026sig=ACfU3U3MO_nrfK_fvvOojONBaM7nMTnWHQ"},"sample_url":"http://www.google.com/patents/reader?id=s9zDBwABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>