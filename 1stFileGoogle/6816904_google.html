<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6816904 - Networked video multimedia storage server environment - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Networked video multimedia storage server environment"><meta name="DC.contributor" content="Lester Ludwig" scheme="inventor"><meta name="DC.contributor" content="William Blake Brown" scheme="inventor"><meta name="DC.contributor" content="Inn J. Yul" scheme="inventor"><meta name="DC.contributor" content="Anh T. Vuong" scheme="inventor"><meta name="DC.contributor" content="Richard W. Vanderlippe" scheme="inventor"><meta name="DC.contributor" content="Gerald Burnett" scheme="inventor"><meta name="DC.contributor" content="Chris Lauwers" scheme="inventor"><meta name="DC.contributor" content="Richard Lui" scheme="inventor"><meta name="DC.contributor" content="Daniel Applebaum" scheme="inventor"><meta name="DC.contributor" content="Collaboration Properties, Inc." scheme="assignee"><meta name="DC.date" content="2000-5-4" scheme="dateSubmitted"><meta name="DC.description" content="A video storage environment for a networked multimedia system comprising a plurality of workstations, each with audio and video reproduction and capture capabilities. The video storage environmental comprises one or more storage servers. In one embodiment, each video storage server comprises at least one storage cell. Each storage cell internally comprises at least one storage disk, at least one converter, and at least one storage disk, at least one converter, and at least one storage cell manager. The invention provides for various methods of resource allocation under various conditions and policies where there are pluralities of like entities. The invention also provides for transfers of video files among storage disks anywhere in the storage environment, and provides for the same video file to be used simultaneously by multiple users and types of applications. The approach can be extended to included multimedia files comprising any of graphics animations, dynamic annotations, text, and other media accompanying the stored video."><meta name="DC.date" content="2004-11-9" scheme="issued"><meta name="DC.relation" content="US:5262875" scheme="references"><meta name="DC.relation" content="US:5581479" scheme="references"><meta name="DC.relation" content="US:5594924" scheme="references"><meta name="DC.relation" content="US:5751336" scheme="references"><meta name="DC.relation" content="US:6212681" scheme="references"><meta name="DC.relation" content="US:6353699" scheme="references"><meta name="citation_patent_number" content="US:6816904"><meta name="citation_patent_application_number" content="US:09/565,192"><link rel="canonical" href="http://www.google.com/patents/US6816904"/><meta property="og:url" content="http://www.google.com/patents/US6816904"/><meta name="title" content="Patent US6816904 - Networked video multimedia storage server environment"/><meta name="description" content="A video storage environment for a networked multimedia system comprising a plurality of workstations, each with audio and video reproduction and capture capabilities. The video storage environmental comprises one or more storage servers. In one embodiment, each video storage server comprises at least one storage cell. Each storage cell internally comprises at least one storage disk, at least one converter, and at least one storage disk, at least one converter, and at least one storage cell manager. The invention provides for various methods of resource allocation under various conditions and policies where there are pluralities of like entities. The invention also provides for transfers of video files among storage disks anywhere in the storage environment, and provides for the same video file to be used simultaneously by multiple users and types of applications. The approach can be extended to included multimedia files comprising any of graphics animations, dynamic annotations, text, and other media accompanying the stored video."/><meta property="og:title" content="Patent US6816904 - Networked video multimedia storage server environment"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("x6HtU4nXOuehsASo54GwDw"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("USA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("x6HtU4nXOuehsASo54GwDw"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("USA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6816904?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6816904"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=ZJNpBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6816904&amp;usg=AFQjCNExrnBzxyj3NWayV1ucx4NVhc7CHQ" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6816904.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6816904.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6816904" style="display:none"><span itemprop="description">A video storage environment for a networked multimedia system comprising a plurality of workstations, each with audio and video reproduction and capture capabilities. The video storage environmental comprises one or more storage servers. In one embodiment, each video storage server comprises at least...</span><span itemprop="url">http://www.google.com/patents/US6816904?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6816904 - Networked video multimedia storage server environment</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6816904 - Networked video multimedia storage server environment" title="Patent US6816904 - Networked video multimedia storage server environment"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6816904 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/565,192</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Nov 9, 2004</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">May 4, 2000</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Nov 4, 1997</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US20050144284">US20050144284</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">09565192, </span><span class="patent-bibdata-value">565192, </span><span class="patent-bibdata-value">US 6816904 B1, </span><span class="patent-bibdata-value">US 6816904B1, </span><span class="patent-bibdata-value">US-B1-6816904, </span><span class="patent-bibdata-value">US6816904 B1, </span><span class="patent-bibdata-value">US6816904B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Lester+Ludwig%22">Lester Ludwig</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22William+Blake+Brown%22">William Blake Brown</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Inn+J.+Yul%22">Inn J. Yul</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Anh+T.+Vuong%22">Anh T. Vuong</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Richard+W.+Vanderlippe%22">Richard W. Vanderlippe</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Gerald+Burnett%22">Gerald Burnett</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Chris+Lauwers%22">Chris Lauwers</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Richard+Lui%22">Richard Lui</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Daniel+Applebaum%22">Daniel Applebaum</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Collaboration+Properties,+Inc.%22">Collaboration Properties, Inc.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6816904.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6816904.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6816904.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (6),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (129),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (16),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (12)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6816904&usg=AFQjCNEkouxw5ZxBhTYOllU1-YLUhu1Mrw">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6816904&usg=AFQjCNGWTDQ8rXFSgxgpJ9tTzLJ25iUc-g">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6816904B1%26KC%3DB1%26FT%3DD&usg=AFQjCNG_UBqzqJAE_chrq6Y3lWST0-28WA">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55344287" lang="EN" load-source="patent-office">Networked video multimedia storage server environment</invention-title></span><br><span class="patent-number">US 6816904 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA50745172" lang="EN" load-source="patent-office"> <div class="abstract">A video storage environment for a networked multimedia system comprising a plurality of workstations, each with audio and video reproduction and capture capabilities. The video storage environmental comprises one or more storage servers. In one embodiment, each video storage server comprises at least one storage cell. Each storage cell internally comprises at least one storage disk, at least one converter, and at least one storage disk, at least one converter, and at least one storage cell manager. The invention provides for various methods of resource allocation under various conditions and policies where there are pluralities of like entities. The invention also provides for transfers of video files among storage disks anywhere in the storage environment, and provides for the same video file to be used simultaneously by multiple users and types of applications. The approach can be extended to included multimedia files comprising any of graphics animations, dynamic annotations, text, and other media accompanying the stored video.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(47)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00018.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00018.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00019.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00019.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00020.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00020.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00021.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00021.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00022.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00022.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00023.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00023.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00024.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00024.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00025.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00025.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00026.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00026.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00027.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00027.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00028.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00028.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00029.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00029.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00030.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00030.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00031.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00031.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00032.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00032.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00033.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00033.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00034.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00034.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00035.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00035.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00036.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00036.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00037.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00037.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00038.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00038.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00039.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00039.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00040.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00040.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00041.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00041.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00042.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00042.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00043.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00043.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00044.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00044.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00045.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00045.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6816904B1/US06816904-20041109-D00046.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6816904B1/US06816904-20041109-D00046.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(22)</span></span></div><div class="patent-text"><div mxw-id="PCLM8759207" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is: </claim-statement> <div class="claim"> <div num="1" id="US-6816904-B1-CLM-00001" class="claim">
      <div class="claim-text">1. A networked multimedia system comprising:</div>
      <div class="claim-text">A) a plurality of workstations, each including </div>
      <div class="claim-text">i) video and audio reproduction capabilities, and </div>
      <div class="claim-text">ii) video and audio capture capabilities; </div>
      <div class="claim-text">B) at least one storage server </div>
      <div class="claim-text">i) the storage server including </div>
      <div class="claim-text">(a) at least one storage cell having </div>
      <div class="claim-text">at least one storage disk, </div>
      <div class="claim-text">a storage disk controller </div>
      <div class="claim-text">â€ƒassociated with each disk, and </div>
      <div class="claim-text">at least one converter; and </div>
      <div class="claim-text">(b) a storage cell manager (AVSM), and </div>
      <div class="claim-text">ii) configured to </div>
      <div class="claim-text">(a) store, </div>
      <div class="claim-text">for later retrieval, </div>
      <div class="claim-text">audio/video signals </div>
      <div class="claim-text">converted by the converter(s); and </div>
      <div class="claim-text">C) at least one signal path, </div>
      <div class="claim-text">i) interconnecting, </div>
      <div class="claim-text">(a) the workstations and the storage server, </div>
      <div class="claim-text">wherein the networked multimedia system is configured to </div>
      <div class="claim-text">ii) store the converted audio/video signals by </div>
      <div class="claim-text">(a) selecting any one of the group consisting of disks, disk controllers, converters, storage cells and storage cell managers (AVSM), according to the following approach: </div>
      <div class="claim-text">if a first converter </div>
      <div class="claim-text">â€ƒof a first cell </div>
      <div class="claim-text">â€ƒhas reached its capacity or bandwidth limit, </div>
      <div class="claim-text">â€ƒthen a second converter, of the first cell, is selected; </div>
      <div class="claim-text">if a first disk controller </div>
      <div class="claim-text">â€ƒof the first cell </div>
      <div class="claim-text">â€ƒhas reached its capacity or bandwidth limit </div>
      <div class="claim-text">â€ƒthen a second disk controller of the first cell is selected; </div>
      <div class="claim-text">if a first disk, </div>
      <div class="claim-text">â€ƒassociated with a first of the disk controllers </div>
      <div class="claim-text">â€ƒhas reached its capacity or bandwidth limit, </div>
      <div class="claim-text">â€ƒthen a second disk associated with the first disk controller is selected; </div>
      <div class="claim-text">if the first storage cell </div>
      <div class="claim-text">â€ƒassociated with a first storage cell manager (AVSM) </div>
      <div class="claim-text">â€ƒhas reached its capacity or bandwidth limit, </div>
      <div class="claim-text">â€ƒthen a second storage cell associated with the first storage cell manager (AVSM) is selected; and </div>
      <div class="claim-text">if the first storage cell manager (AVSM) has reached its capacity, </div>
      <div class="claim-text">â€ƒthen a second storage server, is selected, and </div>
      <div class="claim-text">(b) storing the converted signals using each of the selected ones of the group. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" id="US-6816904-B1-CLM-00002" class="claim">
      <div class="claim-text">2. The networked multimedia system of <claim-ref idref="US-6816904-B1-CLM-00001">claim 1</claim-ref>, wherein</div>
      <div class="claim-text">A) the system is configured to </div>
      <div class="claim-text">i) facilitate transfer </div>
      <div class="claim-text">(a) of converted audio/video signals </div>
      <div class="claim-text">(b) previously stored by a first storage cell </div>
      <div class="claim-text">(c) to another storage cell. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" id="US-6816904-B1-CLM-00003" class="claim">
      <div class="claim-text">3. The networked multimedia system of <claim-ref idref="US-6816904-B1-CLM-00001">claim 1</claim-ref> wherein</div>
      <div class="claim-text">A) the converter is at least one of </div>
      <div class="claim-text">i) a transcoder, an encoder and a combination encoder/transcoder. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" id="US-6816904-B1-CLM-00004" class="claim">
      <div class="claim-text">4. The networked multimedia system of <claim-ref idref="US-6816904-B1-CLM-00001">claim 1</claim-ref> wherein</div>
      <div class="claim-text">A) retrieved signals can </div>
      <div class="claim-text">i) result in audio/video reproduction </div>
      <div class="claim-text">ii) at one or more of the workstations. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" id="US-6816904-B1-CLM-00005" class="claim">
      <div class="claim-text">5. The networked multimedia system of <claim-ref idref="US-6816904-B1-CLM-00001">claim 1</claim-ref> wherein</div>
      <div class="claim-text">A) the converted signals are stored </div>
      <div class="claim-text">i) in at least one file, which can be accessed by </div>
      <div class="claim-text">(a) more than one workstation at the same time and/or </div>
      <div class="claim-text">(b) more than one application type. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" id="US-6816904-B1-CLM-00006" class="claim">
      <div class="claim-text">6. The networked multimedia system of <claim-ref idref="US-6816904-B1-CLM-00005">claim 5</claim-ref> wherein</div>
      <div class="claim-text">A) multiple copies of each such file can exist. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" id="US-6816904-B1-CLM-00007" class="claim">
      <div class="claim-text">7. The networked multimedia system of <claim-ref idref="US-6816904-B1-CLM-00005">claim 5</claim-ref> further comprising:</div>
      <div class="claim-text">A) a storage server defined by at least the following </div>
      <div class="claim-text">i) at least one of the storage cells, </div>
      <div class="claim-text">ii) at least one of the converters and </div>
      <div class="claim-text">iii) at least one controller configured to do at least one of the group of </div>
      <div class="claim-text">(a) determine whether signal conversion is to occur; </div>
      <div class="claim-text">(b) if the system comprises more than one converter, </div>
      <div class="claim-text">determine which of a plurality of converters will accomplish the conversion; </div>
      <div class="claim-text">(c) if the system comprises more than one storage cell, </div>
      <div class="claim-text">determine which of the storage cells will store the converted signals; </div>
      <div class="claim-text">(d) control the subsequent retrieval of the stored signals; and </div>
      <div class="claim-text">(e) determine which copy of the file is accessed for retrieval. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" id="US-6816904-B1-CLM-00008" class="claim">
      <div class="claim-text">8. The networked multimedia system of <claim-ref idref="US-6816904-B1-CLM-00007">claim 7</claim-ref>, wherein</div>
      <div class="claim-text">A) the storage server </div>
      <div class="claim-text">i) is physically distributed or decentralized across the system. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" id="US-6816904-B1-CLM-00009" class="claim">
      <div class="claim-text">9. The networked multimedia system according to <claim-ref idref="US-6816904-B1-CLM-00005">claim 5</claim-ref>, wherein the system is further configured to</div>
      <div class="claim-text">A) generate a sequence of graphics rendering events </div>
      <div class="claim-text">i) associated with computer generated images and/or overlay graphics at a workstation, and </div>
      <div class="claim-text">B) save the generated sequence. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" id="US-6816904-B1-CLM-00010" class="claim">
      <div class="claim-text">10. The networked multimedia system according to <claim-ref idref="US-6816904-B1-CLM-00009">claim 9</claim-ref>, wherein</div>
      <div class="claim-text">A) the generated sequence of graphics rendering events </div>
      <div class="claim-text">i) can be played back to render computer generated images and/or overlay graphics </div>
      <div class="claim-text">(a) at a workstation, </div>
      <div class="claim-text">(b) together with a workstation-user related audio and video </div>
      <div class="claim-text">(c) in a determinable temporal relationship, and </div>
      <div class="claim-text">B) the system can </div>
      <div class="claim-text">i) save the event sequence </div>
      <div class="claim-text">(a) related to the displayed computer generated and/or overlay graphics images and/or the audio and video </div>
      <div class="claim-text">ii) such that the event sequence can later be reproduced </div>
      <div class="claim-text">(a) at a workstation </div>
      <div class="claim-text">(b) in the determined temporal relationship. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" id="US-6816904-B1-CLM-00011" class="claim">
      <div class="claim-text">11. The networked multimedia system according to <claim-ref idref="US-6816904-B1-CLM-00010">claim 10</claim-ref>, wherein</div>
      <div class="claim-text">A) the at least one file includes </div>
      <div class="claim-text">i) the saved sequence of graphics rendering events. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" id="US-6816904-B1-CLM-00012" class="claim">
      <div class="claim-text">12. The networked multimedia system according to <claim-ref idref="US-6816904-B1-CLM-00003">claim 3</claim-ref>, further comprising:</div>
      <div class="claim-text">A) at least one decoder, configured to </div>
      <div class="claim-text">i) decode signals retrieved from storage. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" id="US-6816904-B1-CLM-00013" class="claim">
      <div class="claim-text">13. The networked multimedia system of claims <b>5</b>, wherein</div>
      <div class="claim-text">A) the file is a metafile including </div>
      <div class="claim-text">i) at least one of the group of an audio-video file, audio only file, video only file, a graphics event file, a window event file, an application startup event file, a bit map file, a postscript file, a graphics file, a synchronization file and a text file. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" id="US-6816904-B1-CLM-00014" class="claim">
      <div class="claim-text">14. The networked multimedia system of <claim-ref idref="US-6816904-B1-CLM-00013">claim 13</claim-ref>, wherein the metafile further includes</div>
      <div class="claim-text">i) at least one pointer referencing </div>
      <div class="claim-text">(a) a predetermined file or group of files </div>
      <div class="claim-text">on one or more of the storage cells </div>
      <div class="claim-text">ii) onto at least one storage cell; </div>
      <div class="claim-text">D) transferring</div>
      <div class="claim-text">i) previously stored audio/video signals </div>
      <div class="claim-text">(a) to another storage cell </div>
      <div class="claim-text">whether included in that or another storage cell; </div>
      <div class="claim-text">E) managing</div>
      <div class="claim-text">i) the storage arid </div>
      <div class="claim-text">ii) transfer, </div>
      <div class="claim-text">(a) of the audio/video converted signals, </div>
      <div class="claim-text">(b) among storage cells. </div>
    </div>
    </div> <div class="claim"> <div num="15" id="US-6816904-B1-CLM-00015" class="claim">
      <div class="claim-text">15. A method of using a networked multimedia system comprising:</div>
      <div class="claim-text">A) capturing audio and video at </div>
      <div class="claim-text">i) one or more workstations, each including </div>
      <div class="claim-text">(a) video and audio reproduction capabilities, and </div>
      <div class="claim-text">(b) video and audio capture capabilities; </div>
      <div class="claim-text">B) converting the captured audio and video </div>
      <div class="claim-text">i) into a form suitable for storage; </div>
      <div class="claim-text">C) storing </div>
      <div class="claim-text">i) for later retrieval, </div>
      <div class="claim-text">ii) the converted audio/video signals </div>
      <div class="claim-text">iii) by using </div>
      <div class="claim-text">(a) at least one storage cell having </div>
      <div class="claim-text">at least one storage disk, </div>
      <div class="claim-text">a storage disk controller </div>
      <div class="claim-text">â€ƒassociated with each disk, and </div>
      <div class="claim-text">at least one converter; and </div>
      <div class="claim-text">(b) a storage cell manager (AVSM); and </div>
      <div class="claim-text">D) storing the converted audio/video signals, </div>
      <div class="claim-text">i) by selecting any one of the group consisting of disks, disk controllers, converters, storage cells and storage cell managers (AVSM), according to the following approach: </div>
      <div class="claim-text">(a) if a first converter </div>
      <div class="claim-text">of a first cell </div>
      <div class="claim-text">has reached its capacity or bandwidth limit, </div>
      <div class="claim-text">then a second converter, of the first cell, is selected; </div>
      <div class="claim-text">(b) if a first disk controller </div>
      <div class="claim-text">of the first cell </div>
      <div class="claim-text">has reached its capacity or bandwidth limit </div>
      <div class="claim-text">then a second disk controller of the first cell is selected; </div>
      <div class="claim-text">(c) if a first disk, </div>
      <div class="claim-text">associated with a first of the disk controllers </div>
      <div class="claim-text">has reached its capacity or bandwidth limit, </div>
      <div class="claim-text">then a second disk associated with the first disk controller is selected; </div>
      <div class="claim-text">(d) if the first storage cell </div>
      <div class="claim-text">associated with a first storage cell manager (AVSM) </div>
      <div class="claim-text">has reached its capacity or bandwidth limit, </div>
      <div class="claim-text">then a second storage cell associated with the first storage cell manager (AVSM) is selected; and </div>
      <div class="claim-text">(e) if the first storage cell manager (AVSM) has reached its capacity, </div>
      <div class="claim-text">then a second storage server, is selected, and </div>
      <div class="claim-text">(b) between any two storage cells </div>
      <div class="claim-text">respectively under control of different storage cell managers (AVSM). </div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" id="US-6816904-B1-CLM-00016" class="claim">
      <div class="claim-text">16. The method of <claim-ref idref="US-6816904-B1-CLM-00015">claim 15</claim-ref>, further comprising the step of:</div>
      <div class="claim-text">A) reproducing </div>
      <div class="claim-text">i) audio and/or video </div>
      <div class="claim-text">ii) at one or more of the workstations </div>
      <div class="claim-text">iii) based on the retrieved signals. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="17" id="US-6816904-B1-CLM-00017" class="claim">
      <div class="claim-text">17. The method of <claim-ref idref="US-6816904-B1-CLM-00015">claim 15</claim-ref>, wherein</div>
      <div class="claim-text">A) converted signals are stored </div>
      <div class="claim-text">i) in at least one file, which can be accessed by </div>
      <div class="claim-text">(a) more than one workstation at the same time and/or </div>
      <div class="claim-text">(b) more than one application type. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" id="US-6816904-B1-CLM-00018" class="claim">
      <div class="claim-text">18. The method of <claim-ref idref="US-6816904-B1-CLM-00017">claim 17</claim-ref>, wherein</div>
      <div class="claim-text">A) multiple copies of each such file can exist. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" id="US-6816904-B1-CLM-00019" class="claim">
      <div class="claim-text">19. The method of <claim-ref idref="US-6816904-B1-CLM-00017">claim 17</claim-ref>, further comprising the steps of:</div>
      <div class="claim-text">A) generating a sequence of graphics rendering events </div>
      <div class="claim-text">i) associated with computer generated images and/or overlay graphics at a workstation, and </div>
      <div class="claim-text">B) saving the generated sequence. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" id="US-6816904-B1-CLM-00020" class="claim">
      <div class="claim-text">20. The method of <claim-ref idref="US-6816904-B1-CLM-00019">claim 19</claim-ref>, wherein</div>
      <div class="claim-text">A) the generated sequence of graphics rendering events </div>
      <div class="claim-text">i) can be played back to render computer generated images and/or overlay graphics </div>
      <div class="claim-text">(a) at a workstation, </div>
      <div class="claim-text">(b) together with a workstation user related audio and video </div>
      <div class="claim-text">(c) in a determinable temporal relationship the method comprising the step of: </div>
      <div class="claim-text">ii) saving th e event sequence </div>
      <div class="claim-text">(a) related to the displayed computer generated and/or overlay graphics images and/or the audio and video </div>
      <div class="claim-text">(b) such that the event sequence can later be reproduced </div>
      <div class="claim-text">at a workstation </div>
      <div class="claim-text">in the determined temporal relationship. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" id="US-6816904-B1-CLM-00021" class="claim">
      <div class="claim-text">21. The method of <claim-ref idref="US-6816904-B1-CLM-00020">claim 20</claim-ref>, wherein</div>
      <div class="claim-text">A) the at least one file includes </div>
      <div class="claim-text">i) the saved sequence of graphics rendering events. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="22" id="US-6816904-B1-CLM-00022" class="claim">
      <div class="claim-text">22. The method of <claim-ref idref="US-6816904-B1-CLM-00018">claim 18</claim-ref>, further comprising the step of:</div>
      <div class="claim-text">A) decoding signals retrieved from storage. </div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES54329440" lang="EN" load-source="patent-office" class="description">
    <heading>RELATED CASES</heading> <p>This application is a continuation of PCT Application No. PCT/US98/23596, filed Nov. 4, 1998, which claims priority from U.S. provisional Application No. 60/064,266, filed Nov. 4, 1997.</p>
    <heading>1. BACKGROUND OF THE INVENTION</heading> <p>1.1 Field of the Invention</p>
    <p>The present invention relates generally to a scalable networked multimedia system, and more particularly to a scalable audio-video server system and Application Program Interface (API) together with a range of associated software applications that together provide high-quality audio-video and multimedia processing capabilities.</p>
    <p>1.2 Background</p>
    <p>In recent years, considerable effort has been directed toward the development of hardware and software for network-based audio-video (A/V) and, more generally, networked multimedia systems. Such development has been driven by technology push from equipment manufacturers, as well as the commercial potential of entertainment applications such as video-on-demand and multi-player gaming; and business applications such as video messaging and multimedia conference collaboration.</p>
    <p>A critical factor impacting the usefulness and value of a network-based multimedia system is the manner in which computational resources are organized to define a video file storage/encoding/decoding/distributionsystem, hereafter referred to an Audio-Video Storage System (AVSS). Several key requirements exist with regard to defining a Audio Video Storage System applicable to business applications. With particular emphasis on deployment in a business environment supporting multimedia conference collaboration and characterized by on-premises (i.e., local) and wide-area networks, these requirements include the following:</p>
    <p>1) ubiquitous premises scalability;</p>
    <p>2) ubiquitous wide-area scalability;</p>
    <p>3) limited impact upon network loading;</p>
    <p>4) low implementation and operational costs;</p>
    <p>5) accommodation of multiple desktop platforms (modern, existing, and outdated);</p>
    <p>6) accommodation of multiple compression standards;</p>
    <p>7) support for a wide range of high-performance, high-quality video-enabled applications;</p>
    <p>8) cost-effective ability to upgrade across successive technology and standards generations; and</p>
    <p>9) API extensibility.</p>
    <p>A collaborative multimedia environment employing a video storage system necessarily includes a plurality of desktop workstations, encoding and decoding resources, video file storage resources, and a premises video distribution network. Two architectural design factors greatly influence the extent to which the aforementioned key requirements can be simultaneously met, namely, 1) the organization of the encoding/decoding resources and video file storage resources relative to the desktop workstations and video distribution network; and 2) the nature of the video distribution network itself.</p>
    <p>Encoding and decoding resources, as well as video file storage resources, can be allocated on a desktop-by-desktop basis, or a network (i.e., shared) basis. FIG. 1 illustrates an exemplary Erlang resource sharing utilization relationship <b>2</b> showing computational resource utilization efficiency <b>1</b> relative to the number of users sharing the resource <b>6</b> under fixed blocking conditions. As indicated in FIG. 1, network-based resource allocation <b>3</b>, <b>4</b> results in much higher resource utilization efficiency than desktop-dedicated resources, which are fully allocated to one user each (5). This is turn implies that video storage systems characterized by desktop-based resource allocation leverage technology investments for less effectively than systems that employ resource sharing, particularly in situations involving a significant number of workstation users <b>3</b> and/or relatively low usage rates (&lt;20% of workday) by users. Additionally, desktop-based resource allocation undesirably results in greater system upgrade costs.</p>
    <p>A video distribution network can be based upon analog technology, digital technology, or their combination. FIG. 2 is a graph showing the relative cost <b>2</b>.<b>1</b> of local-area analog and digital video signal distribution technology as a function of video quality <b>2</b>.<b>2</b>, and hence includes an analog cost-versus-performance/qualitycurve <b>2</b>.<b>3</b> and a digital cost-versus-performance/qualitycurve <b>2</b>.<b>4</b>. Across a first, lower-performance and lower-quality region <b>2</b>.<b>5</b> in FIG. 2 that could provide performance and quality suitable, for example, for technology experiments, digital signal distribution technology is significantly more expensive than its analog counterpart. The slope of the digital cost-versus-performance/quality curve <b>2</b>.<b>4</b> throughout this first region <b>2</b>.<b>5</b> is nearly constant, while that for analog cost-versus-performance/quality <b>2</b>.<b>3</b> gradually rises with increasing performance and image quality <b>2</b>.<b>2</b>. A second region <b>2</b>.<b>6</b> shown in FIG. 2 spans a practical region of operation <b>2</b>.<b>7</b> relative to business-performance and business-quality levels, which herein correspond to video delivered at 30 frames per second (fps) at a resolution ranging from approximately 320Ã—240 to 640Ã—480 pixels or other standard resolution. Across the second region <b>2</b>.<b>6</b>, the analog cost-versus-performance/quality curve <b>2</b>.<b>3</b> begins to increase rapidly as performance and quality <b>2</b>.<b>2</b> improves, eventually meeting and exceeding the digital cost-versus-performance/qualitycurve <b>2</b>.<b>4</b>. However. throughout most of the aforementioned practical region of operation <b>2</b>.<b>7</b>. analog signal distribution technology remains significantly less expensive than its digital counterpart. Thus, for most business environment performance and quality requirements, analog signal distribution technology is more cost-effective than digital signal distribution technology. Readily-available digital network technology lacks sufficient bandwidth for delivering business-quality, real-time or near-real-time video to a large number of users. Finally, a third region <b>2</b>.<b>8</b> in FIG. 2 spans high-end or special-situation performance and quality levels <b>2</b>.<b>2</b>. Within the third region <b>2</b>.<b>8</b>, the cost of digital signal distribution technology begins to rapidly escalate.</p>
    <p>FIG. 2 additionally indicates the manner in which the analog and digital cost-versus-performance/quality curves <b>2</b>.<b>3</b>, <b>2</b>.<b>4</b> can be expected to evolve over time. For each technology type, overall cost <b>2</b>.<b>1</b> will decrease relative to a given performance and quality level <b>2</b>.<b>2</b> as the technology evolves. The general shape of the curves <b>2</b>.<b>3</b>, <b>2</b>.<b>4</b> shown in FIG. 2, however, can be expected to remain essentially the same. Moreover, digital signal distribution technology is likely to evolve at a much more rapid pace than analog distribution technology in the near term, which implies higher costs <b>2</b>.<b>1</b> over a system's lifetime due to system upgrade frequency. Thus, video storage systems that rely upon all-digital video distribution technology are significantly less cost-effective than those employing analog distribution technology.</p>
    <p>Known premises-based networked video storage systems fail to come anywhere close to meeting the aforementioned key requirements. Much of the reason for this results from the design approaches taken relative to the aforementioned architectural considerations, particularly when the architectural cornerstones are driven by established technology marketing trends rather than designing to meet true business requirements. What is needed is a video storage system that utilizes resource sharing and the full evolvable range of networked signal distribution technology to meet key cost and application quality requirements described above.</p>
    <heading>2. SUMMARY OF THE INVENTION</heading> <p>The present invention is a networked multimedia system comprising a plurality of workstations and at least one storage server. At least one signal path interconnects the workstations and the storage server. Each workstation includes video and audio reproduction capabilities, as well as video and audio capture capabilities. Any given storage server comprises a set of storage cells that operate under the direction of a storage cell manager. A storage cell may include one or more encoding and/or transcoding converters configured to convert or transform audio and video signals originating at a workstation into a form suitable for digital storage. A storage cell may further include one or more decoding converters configured to convert digitally-stored signals into a form suitable for audio and video signal reproduction at a workstation. Each storage cell additionally includes at least one storage device and storage device controller capable of storing, for later retrieval, signals generated by one or more converters.</p>
    <p>The storage cell manager is responsive to signals received from the workstations, and oversees the operation of the storage cells to facilitate the storage of converted audio and video signals in at least one file that can be simultaneously accessed by one or more application programs executing on one or more workstations. In one embodiment, the storage cell manager directs the storage of converted signals by selecting a storage cell, and determining whether a converter contained therein has available bandwidth or capacity. If so, the storage cell manager selects a storage device controller within the selected storage cell, and determines whether the controller has available bandwidth or capacity. If so, the storage cell manager determines whether a storage device associated with the controller has available bandwidth or capacity, in which case the storage cell manager directs the converter and the selected storage device controller to convert and store the converted signals upon the selected storage device. In the event that bandwidth or capacity is unavailable for the converter, the storage device controller, or the storage device, the storage cell manager selects other such devices within the selected storage cell for consideration relative to bandwidth or capacity availability. If the selected storage cell has reached a bandwidth or capacity limit, the storage cell manager selects another storage cell for consideration. In the event that the storage cell manager or the storage server itself has reached a bandwidth or capacity limit, the storage cell manager requests another storage cell manager to direct the storage of the converted signals.</p>
    <p>The storage cell manager further selectively directs the copying or transfer of stored converted signals between 1) any plurality of storage devices within a given storage cell; and/or 2) any set of storage cells, where one or more such storage cells may be under the control of another storage cell manager. Such copying or transfer maximizes the likelihood that any given file containing converted signals will be simultaneously accessible to one or more application programs executing upon multiple workstations. The storage cell manager may also direct the transfer of stored converted signals to workstations or other types of servers coupled to the networked multimedia system.</p>
    <p>The storage cell manager also oversees the retrieval and decoding conversion of stored converted signals under the direction of one or more application programs executing on one or more workstations. Such retrieval and decoding conversion facilitates real-time and/or near real-time audio and video signal reproduction at such workstations.</p>
    <p>In common usage, a multimedia file comprises one or more types of files and/or references to files, where such files may include textual, graphical, image, audio, and/or video information and/or commands or event sequences for generating or rendering such information. The multimedia file also includes temporal correlation data specifying one or more manners in which its constituent files and/or file references are associated in time. In addition to the above operations, the storage cell manager in one embodiment oversees the storage of these multimedia files or portions of them. The storage cell manager further oversees the retrieval of multimedia file contents and the distribution of such contents to one or more workstations in conjunction with audio and video signal reproduction at such workstations, in a manner specified by the temporal correlation data.</p>
    <p>The system is configured to create a pointer referencing a predetermined file or group of files on one or more of the storage cells; to transmit at least the pointer to a receiving application program, and to render the contents of the file or group of files at a workstation by retrieving data from the file or group of files identified by the pointer.</p>
    <p>The system also provides a viewer application for setting up connections and sessions; preparing files for viewing, and providing the necessary viewer interface to enable a user to access and manipulate files on the workstation, as permitted.</p>
    <p>Further, the system has at least one stored-video application program on at least one of the storage sub-system and a workstation; and share at least one process primitive across at least two of the stored-video application programs. The stored-video application programs are one or more from the group consisting of: videoconference recording; video mail; video answering system; video documents; and video publishing. The system is further configured to share at least one data primitive across at least two of the plurality of stored-video application programs.</p>
    <p>The system can also invoke a process primitive and a data primitive as standardized attachments to a file. Such attachment is in a format that is accepted by third parties in accordance with a standard data interchange protocol.</p>
    <p>The system can also be configured to establish real time data sharing sessions between at least two workstations using a synchronized data sharing process as a shared conference window. It can use a video conference application to establish a video conference session among the interconnected workstations, and to use a video conference recording application to record at least a portion of the established video conference session on at least one storage sub-system.</p>
    <p>The system is further configured to provide at least one video mail application, which can do at least one of authoring a video mail message for transmission to storage subsystem and reading a video mail message previously stored on the storage subsystem. The system typically utilizes a storage cell to record at least one of an audio, a video message, and a multimedia message from an incoming caller whose call attempt is one of not answered by the recipient, refused by the recipient, and made solely to leave a message without connecting to the recipient. Video files stored on a storage cell can be included in an on-line electronic document.</p>
    <p>The system further can incorporate a shareboard or other window-sharing session, either static or synchronized with video or audio information to enable a multimedia implementation of at least one of video conference recording, video mail, video answering system, video documents, and video publishing.</p>
    <p>Moreover the system can incorporate a video editing program. The video editing program may be implemented either as an integral part of at least one of the applications or by incorporating a third-party video editor.</p>
    <p>The system can support video mail applications either using conventional third-party e-mail systems or e-mail systems enhanced to offer the ability to obtain and or file copy events from the e-mail system.</p>
    <p>The system can support video document applications either using conventional third-party document systems or document systems enhanced to offer the ability to obtain and or file copy events from the document system.</p>
    <p>In addition, the system can function as an internet proxy server for reducing the total number of copies of video files in the enterprise by file sharing, or reducing enterprise network load by transporting video files over a separate audio-video network, or transcoding among multiple audio-video formats, and or allowing any audio-video workstation in the enterprise to serve as a video publisher for the enterprise. Finally, the system, functioning as an internet proxy server, enables the implementation of at least one animated annotation on the internet.</p>
    <heading>3. BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p>For fuller understanding of the principles of the present invention, reference is made to the several figures of the drawing. In the drawing:</p>
    <p>FIG. 1 is an exemplary Erlang diagram showing computational resource utilization efficiency relative to the number of users sharing the resource under fixed blocking conditions.</p>
    <p>FIG. 2 is a graph showing the relative cost of analog and digital distribution technology as a function of performance and image quality.</p>
    <p>FIG. 3 is a block diagram of a Collaborative Multimedia Computing Environment (CMCE) employing an Audio/Video Server System (AVSS) constructed in accordance with the present invention.</p>
    <p>FIG. 4 is a block diagram of an alternate embodiment of a CMCE constructed in accordance with the present invention.</p>
    <p>FIG. 5 is a block diagram of an Audio/Video Storage Cell (AVSC) constructed in accordance with the present invention is shown.</p>
    <p>FIG. 6 is a block diagram of a first embodiment of a shared coding unit of the present invention.</p>
    <p>FIG. 7 is a block diagram of a second embodiment of the shared coding unit.</p>
    <p>FIG. 8 is a block diagram of a third embodiment of the shared coding unit.</p>
    <p>FIG. 9 is a block diagram of an Audio/Video Server Manager (AVSM) constructed in accordance with the present invention.</p>
    <p>FIG. 10 is a block diagram illustrating client-server session communication in one embodiment of the present invention.</p>
    <p>FIG. 11 is a block diagram showing an exemplary recording control Graphical User Interface (GUI).</p>
    <p>FIG. 12 is a block diagram showing an exemplary playback control GUI.</p>
    <p>FIG. 13 is a block diagram showing an exemplary browsing control GUI.</p>
    <p>FIG. 14 is a block diagram of an exemplary networked AVSS organization.</p>
    <p>FIG. 15 is a representation of a communication class hierarchy in the present invention.</p>
    <p>FIG. 16 is a representation of an AVSM class relationship in the present invention.</p>
    <p>FIG. 17 is a representation of an AVSC class relationship in the present invention.</p>
    <p>FIG. 18 is an exemplary request sequence diagram corresponding to AVSC channel acquisition.</p>
    <p>FIG. 19 is an exemplary request sequence diagram for an â€œOpenâ€ request.</p>
    <p>FIG. 20 is an exemplary request sequence diagram for a â€œCloseâ€ request.</p>
    <p>FIG. 21 is an exemplary request sequence diagram for a â€œDeleteâ€ request.</p>
    <p>FIG. 22 is an exemplary request sequence diagram for a â€œRecordâ€ request.</p>
    <p>FIG. 23 is a exemplary request sequence diagram corresponding to a request for a file transfer from a non-premises AVSS.</p>
    <p>FIG. 24 is an exemplary request sequence diagram corresponding to file replication operations.</p>
    <p>FIG. 25 is an exemplary request sequence diagram corresponding to particular AVSS administrative operations.</p>
    <p>FIG. 26 is a block diagram showing client application programs communicating with the AVSM and A/V network manager.</p>
    <p>FIG. 27 is an overview of the file hierarchy.</p>
    <p>FIG. 28 is an exemplar data primitive.</p>
    <p>FIG. 29 is an exemplar data primitive implemented as an attachment.</p>
    <p>FIG. 30 is an exemplar system for implementing the software applications of the present invention.</p>
    <p>FIG. 31 is a transaction flow diagram among the application elements which facilitate local-area video mail.</p>
    <p>FIG. 32 is a transaction flow diagram among the application elements which facilitate wide-area video mail.</p>
    <p>FIG. 33 is a representation of the life cycle of an exemplar video message file.</p>
    <p>FIG. 34 is a comparative diagram of the ownership and readability of video files associated with a video mail attachment, over the message lifetime.</p>
    <p>FIG. 35 is an overview of one embodiment of the video answering system of the present invention.</p>
    <p>FIG. 36 is a data flow diagram detailing the operation of one embodiment of the present invention.</p>
    <p>FIG. 37 an overview of the video conference recording application of the present invention.</p>
    <p>FIG. 38 an overview of several of the components of the present invention required to implement video conference recording.</p>
    <p>FIG. 39 is a signal state diagram of a two-party video conference.</p>
    <p>FIG. 40 is a signal state diagram of a video conference, including a third party or implementing conference recording.</p>
    <p>FIG. 41 is a signal state diagram at the initiation of conference recording.</p>
    <p>FIG. 42 is a signal state diagram of a first video conference representation.</p>
    <p>FIG. 43 is a signal state diagram of a second video conference representation.</p>
    <p>FIG. 44 is a signal state diagram of a third video conference representation.</p>
    <p>FIG. 45 is a data flow diagram of one software implementation of the video conference application of the present invention.</p>
    <p>FIG. 46 is a signal state diagram at the initiation of video conference playback.</p>
    <heading>4. DETAILED DESCRIPTION</heading> <heading>4.1 Architectural Framework</heading> <p>The present invention comprises at least one shared centralized audio-video (A/V) file storage and processing system within a collaborative or networked multimedia computing environment. Herein, this collaborative or networked multimedia computing environment comprises a plurality of user workstations plus multimedia-enabled servers that are linked together via one or more networks. The present invention further comprises workstation-based application programs plus control software executing on the servers that facilitate the exchange of A/V and/or multimedia information among workstations and servers in real-time, near-real-time, and/or non-real time. The A/V file storage and processing system provides a wide range of video storage and playback services to workstation application programs and/or other servers, including several of analog to digital encoding, digital to analog decoding, transcoding digital file storage, multimedia file recording, multimedia file playback analog streaming digital format transcoding, digital file streaming, digital file transfer, and file administration operations that facilitate the recording/encoding, storage, distribution, decoding/playback, copying, archival, and deletion of A/V and/or multimedia files.</p>
    <p>FIG. 3 is a block diagram of a Collaborative Multimedia Computing Environment (CMCE) <b>10</b> employing an Audio/Video Server System (AVSS) <b>100</b> constructed in accordance with the present invention. The CMCE <b>10</b> comprises a data network <b>20</b>; an A/V network <b>30</b>; a plurality of user workstations <b>40</b> and/or a set of A/V conference rooms <b>45</b>; an Audio/Video Server System (AVSS) <b>100</b>; and a set of supporting server systems that include an e-mail system <b>50</b>, an intranet server system <b>60</b>, and a firewall/internet gateway system <b>70</b>. The data network <b>20</b> couples the A/V network <b>30</b>, the workstations <b>40</b>, the A/V conference room(s) <b>45</b>, each supporting server system <b>50</b>, <b>60</b>, <b>70</b>, and the AVSS <b>100</b>. The data network <b>20</b> also maintains a wide area link that is coupled to a Wide Area Network (WAN) gateway <b>25</b>, which is coupled to a first WAN <b>29</b>. The A/V network <b>30</b> couples the workstations <b>40</b>, the A/V conference rooms <b>45</b>, and the AVSS <b>100</b>. The A/V network <b>30</b> maintains at least one trunk line coupling <b>16</b> to a remote and/or another local A/V network <b>30</b>. The A/V network <b>30</b> additionally maintains a trunk line coupling <b>16</b> to a coder/decoder (codec) gateway <b>38</b>, which is coupled to a second WAN <b>39</b>.</p>
    <heading>4.2 Signal Distribution Networks</heading> <p>The present invention supports both real-time (or near real-time) and delayed AN signal exchange or distribution on a local and/or remote basis. In the context of real-time A/V signal distribution via currently-available networking technologies, analog premises networking provides, for the foreseeable future, higher video quality and real-time performance at a lower cost than digital premises networking. In the embodiment shown in FIG. 3, the CMCE <b>10</b> relies upon the data network <b>20</b> to facilitate the exchange of digital information between CMCE elements, and the A/V network <b>30</b> to facilitate analog signal exchange. Thus, the analog signal premises distribution network shown in FIG. 3 provides a low-cost CMCE implementation capable of delivering high-quality A/V signals (i.e., NTSC television-quality video at 640Ã—480 pixels or similar standard television resolution, plus 7-15 kHz high-fidelity audio) in real-time. This embodiment additionally ensures that real-time A/V signal distribution has essentially no impact upon local data network loading. As described in detail below, alternate CMCE embodiments could rely upon a single physical network that utilizes any of a variety of suitable analog and/or digital multiplexing scheme: these will become increasingly important over time as suggested by the time evolution of the curves in FIG. <b>2</b>.</p>
    <p>In FIG. 3, the data network <b>20</b> provides for both local- and wide-area digital networking, while the A/V network <b>30</b> provides for both local- and wide-area analog networking. In terms of local analog networking, a single instance of the A/V network <b>30</b> typically directly serves one AVSS <b>100</b> plus an associated group of workstations <b>40</b> and/or A/V conference rooms <b>45</b> located within a single premises. Hence, an A/V network <b>30</b>, its corresponding AVSS <b>100</b> and workstations <b>40</b> and A/V conference rooms <b>45</b> are referred to herein as a premises group. Multiple proximate premises groups may be locally linked via trunk lines among corresponding data and A/V networks <b>20</b>, <b>30</b> to form a common campus. One example of this is a large office building where different areas or building floors are served by different premises groups. Another example involves situations where a corporate headquarters facility includes multiple generally-proximate buildings, one or more premises groups could be deployed within each building. Each of the premises groups within these buildings could then further be locally coupled to form a common campus, in a manner consistent with generally-accepted use of the term â€œcampusâ€ in corporate environments. The structure and functionality of the data and A/V networks <b>20</b>, <b>30</b>, and the manners in which they support premises, campus, and wide-area or remote networking, are described in detail hereafter.</p>
    <p>4.2.1 Data Network</p>
    <p>The data network <b>20</b> comprises a conventional network hub, plus data links <b>12</b> that implement an Ethernet, Asynchronous Transfer Mode (ATM), or other type of network, and which facilitates the exchange of data and control signals between the elements to which it is coupled either locally or via the wide area link. In the context of the present invention, the data network <b>20</b> is a Local Area Network (LAN) that spans an extent ranging from approximately a few tens of meters to perhaps one kilometer. The data links <b>12</b> can comprise unshielded twisted pair (UTP) wiring, compatible to standard telephone system wiring, or essentially any other type of network communication coupling (coax, optical, wireless radio, etc.).</p>
    <p>The data network <b>20</b> facilitates digital communication within at least one premises group, where such communication may include control signal transfer, digital file transfer, and/or digital streaming. Via a set of data links <b>12</b> locally coupled to conventional routers, data switching hubs, or a network backbone, one or more data networks <b>20</b> can readily serve multiple premises groups (i.e., a common campus) subject to geographic, network-loading, and signal quality constraints, in a manner readily understood by those skilled in the art. The data network's wide area link facilitates digital communication between premises and non-campus or remote CMCE elements. The wide area link is coupled to the gateway and WAN <b>25</b>, <b>29</b>, which are implemented in a conventional manner such as that described in U.S. Pat. No. 5,617,539.</p>
    <p>4.2.2 A/V Network</p>
    <p>The A/V network <b>30</b> distributes A/V signals under the direction of a server that is responsive to requests and messages received from-other CMCE elements. The A/V network <b>30</b> comprises an A/V switch <b>32</b>, an A/V network manager <b>34</b>, and a conference bridge <b>36</b>. The A/V switch <b>32</b> is coupled to the conference bridge <b>36</b>, the workstations <b>40</b>, and the AVSS <b>100</b> via analog links <b>14</b>. The A/V switch <b>32</b> maintains trunk line couplings <b>16</b> to one or more campus A/V networks, and also to the codec gateway <b>38</b>, which couples the A/V switch <b>32</b> to the second WAN <b>39</b>. The A/V network manager <b>34</b> is coupled to the data network <b>20</b>, the A/V switch <b>32</b>, and the conference bridge <b>36</b> via digital links <b>12</b>.</p>
    <p>The A/V switch <b>32</b> comprises standard commercially-available circuitry for selectively establishing analog signal couplings between a source port and one or more output ports. The A/V switch <b>32</b> may be implemented using CMOS analog switching elements that are linked by buses to form a cross-bar switch. Such CMOS analog switches are controlled by one or more microprocessors that receive commands through serial or data network ports.</p>
    <p>A trunk line <b>16</b> couples the A/V switch <b>32</b> to other campus-based A/V switches <b>32</b>. In addition, a trunk line <b>16</b> couples the A/V switch <b>32</b> to the codec gateway <b>38</b>. In one embodiment, the codec gateway <b>38</b> is conventional. Other embodiments may involve physically separate switching multiplexers and/or access multiplexers. The codec gateway <b>38</b> could be implemented, for example, using a Zydacron codec (Zydacron, Inc., Manchester, N.H.) or a Tandberg codec (Tandberg, Lysaker, Norway and Herndon, Va.). The codec gateway <b>38</b> is coupled to the second WAN <b>39</b>, thereby facilitating wide-area A/V networking. In general, the second WAN <b>39</b> comprises a conventional network capable of providing a guaranteed quality-of-service level and low latency data transfer, such as a T<b>1</b>, DS<b>3</b>, ISDN, public-switched, or other network. Those skilled in the art will understand that the WAN <b>39</b> utilized for wide-area A/V networking may be the same as that used for wide-area data networking (i.e., a single conventional ISDN, T-carrier. ATM, or frame relay telecommunications WAN), where the A/V and data signals are multiplexed in accordance with priority and quality-of-service considerations in a standard manner. That is, the present invention could be served by a single WAN rather than separate WANs <b>29</b>, <b>39</b>.</p>
    <p>The conference bridge <b>36</b> comprises conventional audio mixing and video mosaicing circuitry. The conference bridge <b>36</b> selectively provides A/V and/or multimedia conference participants with one or more conference video images, as well as conference audio streams. In one embodiment, the conference video images comprise mosaiced subsets of video images generated by conference participants, as well as a video mosaic of each video image associated with all conference participants. Similarly, the conference audio streams comprise subsets of audio signals generated by conference participants, plus an audio stream corresponding to all conference participants.</p>
    <p>The A/V network manager <b>34</b> comprises a server plus attendant software that coordinates or manages the operation of the A/V switch <b>32</b> and conference bridge <b>36</b> in response to requests received over the data network <b>20</b>. The A/V network manager <b>34</b> provides an Application Program Interface (API) through which client application programs may request A/V and/or multimedia switching and/or conferencing services. Thus, the A/V network manager <b>34</b> directs the A/V switch <b>32</b> in establishing A/V and/or multimedia sessions and/or conferencing sessions between workstations <b>40</b>, A/V conference rooms <b>45</b>, the AVSS <b>100</b>, and/or the trunk line <b>16</b>.</p>
    <p>In one embodiment, the A/V network manager comprises a collaborative A/V and/or multimedia conferencing system that is implemented in the manner described in U.S. Pat. No. 5,617,539, entitled â€œMultimedia Collaboration System with Separate Data Network and A/V Network Controlled by Information Transmitting on the Data Network.â€</p>
    <p>4.2.3 Multimedia Network</p>
    <p>Taken together, the data network <b>20</b> and the A/V network <b>30</b> comprise a Multimedia Local Area Network (MLAN). Wide-area networking is facilitated by the data network's wide-area link gateway and WAN <b>25</b>, <b>29</b>, plus the A/V network's trunk line coupling <b>16</b> to the codec gateway and WAN <b>38</b>, <b>39</b>. The MLAN WANs <b>29</b>, <b>39</b> of the present invention may be implemented in the manner described in U.S. Pat. No. 5,617,539. A collection of MLANs coupled by WAN <b>29</b>, <b>39</b> facilitate the exchange of A/V and/or multimedia information between premises, campus, and/or remote CMCE elements.</p>
    <p>4.2.4 Alternate Signal Distribution Architecture</p>
    <p>As previously indicated, the present invention could rely upon a single network for distributing both data and A/V signals. FIG. 4 is a block diagram of an alternate embodiment of a CMCE <b>11</b> constructed in accordance with the present invention. With regard to FIG. 3, like reference numbers have been used to denote like elements in FIG. <b>4</b>. In the alternate embodiment, the CMCE <b>11</b> comprises a data network <b>20</b>; an A/V conference manager <b>34</b> coupled to a conference bridge <b>36</b>; a set of workstations <b>40</b> and possibly one or more AV conference rooms <b>45</b>; a set of supporting server systems <b>50</b>, <b>60</b>, <b>70</b>; and an AVSS <b>100</b>. The data network <b>20</b> couples the A/V conference manager <b>34</b>, the conference bridge <b>36</b>, the workstations <b>40</b> and A/V conference room(s) <b>45</b>, each supporting server system <b>50</b>, <b>60</b>, <b>70</b>, and the AVSS <b>100</b>.</p>
    <p>In the single-network CMCE <b>11</b>, the data network <b>20</b> is typically implemented as an IP or ATM network. A/V file exchange between CMCE elements such as the AVSS <b>100</b> and workstations <b>40</b> or A/V conference rooms <b>45</b> would occur via streaming or file transfer, and each workstation <b>40</b> may include compression/decompression resources.</p>
    <heading>4.3 Workstations and A/V Conference Rooms</heading> <p>Users interact with CMCE elements, including the AVSS <b>100</b>, via application programs executing on workstations <b>40</b>. Each workstation <b>40</b> comprises a conventional desktop-based computer system having a processing unit, memory, disk drive, display device, keyboard, mouse, and speaker(s). Any particular workstation <b>40</b> can be implemented in accordance with essentially any hardware/software platform, such as a Windows-based (Microsoft Corporation, Redmond, Wash.) personal computer, an Apple-based (Apple Computer Corporation, Cupertino, Calif.) computer, a Unix-based computer, or other type of system. Each workstation <b>40</b> is also equipped with a video camera and a microphone, such as in the manner described in U.S. Pat. No. 5,617,539. The video camera and microphone generate a high-quality A/V signal that is directed to the A/V network <b>30</b> via an analog line <b>14</b>. In like manner, the display device receives a high-quality A/V signal from the A/V network <b>30</b> via the analog line <b>14</b>.</p>
    <p>In embodiments in which each workstation <b>40</b> is coupled to both the data network and the A/V network <b>30</b>, a variety of possibilities exist for the transfer of A/V and/or multimedia files to any given workstation <b>40</b>. In particular, the present invention supports â€œreal-timeâ€ analog A/V file transfer (i.e., analog streaming) to any workstation via the A/V network <b>30</b>, as well as both digital streaming and the transfer of entire digital files to any workstation <b>40</b> via the data network <b>20</b>. The present invention additionally supports real-time analog A/V file transfer simultaneously with either digital streaming or digital file transfer to any combination of workstations <b>40</b>. The particular types of file transfer employed at any given moment are determined by application programs executing on the workstations <b>40</b> as well as premises and/or non-premises application servers coupled to the CMCE <b>10</b>.</p>
    <p>In addition to exchanging A/V signals with the workstations <b>40</b>, the A/V network <b>30</b> can exchange A/V signals with presentation, conferencing, and/or computing resources located in one or more A/V conference rooms <b>45</b>, where such resources may include cameras, monitors, televisions, microphones, and speakers. This in turn means that a group meeting held in an A/V conference room <b>45</b> can exchange A/V and/or multimedia information with individual workstations <b>40</b> or other A/V conference rooms <b>45</b>, on a premises, campus, or remote basis.</p>
    <heading>4.4 Supporting Servers</heading> <p>The AVSS <b>100</b> provides novel A/V and/or multimedia functionality to various application programs, as described in detail below. In so doing, the AVSS <b>100</b> selectively leverages the capabilities of the supporting server systems <b>50</b>, <b>60</b>, <b>70</b>. The e-mail system <b>50</b> preferably comprises conventional e-mail server hardware and software capable of creating, storing, and distributing e-mail messages with attached files between users or target destinations. The manners in which the AVSS <b>100</b> interacts with the e-mail system <b>50</b> to provide video attachments and other multimedia e-mail capabilities are described in detail below.</p>
    <p>Many corporations or enterprises employ an intranet system, which comprises a private network upon which enterprise-related information is distributed and/or exchanged in accordance with conventional internet protocols. An intranet system leverages readily-available, low cost internet software tools to efficiently provide employees or enterprise members with access to enterprise-wide information. Such enterprise-wide information may include corporate communications documents or bulletins; training materials; product and/or project information; personnel directories; and member- or employee-specific data maintained within an enterprise database, where access to such data is granted only for authorized members. The intranet server system <b>60</b> comprises conventional hardware and software that provide information sharing or distribution services to employees or authorized members within a corporation or organization, where such information sharing occurs in accordance with a conventional internet protocol suite (i.e., TCP/IP). Details concerning the manners in which the AVSS <b>100</b> interacts with the intranet server system <b>60</b> to facilitate A/V and/or multimedia information sharing are provided below.</p>
    <p>The internet gateway/firewall system <b>70</b> comprises hardware and software that implement a conventional internet firewall security and File Transfer Protocol (FTP) gateway system, for exchanging messages and A/V or multimedia files between the AVSS <b>100</b> and the public internet <b>80</b>. Specific AVSS functionality in this regard is described in detail below.</p>
    <heading>4.5 AVSS Architecture</heading> <p>The AVSS <b>100</b> comprises a repository for A/V file storage and processing resources, to which application programs executing on premises, campus, and/or remote CMCE elements have shared access. In the present invention, application programs initiate or generate service requests directed to the AVSS <b>100</b> in response to user actions. Such application programs may be executing on premises, campus, or remote workstations <b>40</b>, as well as computers coupled to an enterprise intranet or the public internet. Service requests comprise an appeal for either a) A/V resource or service allocation; or b) AN resource or service state information. The AVSS <b>100</b> receives the service requests via the data network <b>20</b>, and establishes message-based service sessions with the A/V network <b>30</b>, workstations <b>40</b>, supporting servers <b>50</b>, <b>60</b>, <b>70</b>, and/or one or more campus-based or remote CMCE elements to provide A/V and/or multimedia services in accordance with such requests. Messages generated during a service session may comprise state information, control commands, and confirmations. The structure and functionality of the AVSS <b>100</b>, and the manners in which the AVSS <b>100</b> processes service requests and generates messages, are described in detail hereafter.</p>
    <p>As shown in FIG. 3, the AVSS <b>100</b> comprises an internal network <b>110</b>, at least one Audio/Video Storage Cell (AVSC) <b>120</b>, and an Audio/Video Server Manager (AVSM) <b>160</b>. The internal network <b>110</b> couples each AVSC <b>120</b>, the AVSM <b>160</b>, the data network <b>20</b>, the intranet server <b>60</b>, and the internet gateway/firewall system <b>70</b>. In addition, the AVSM <b>160</b> supports coupling to each AVSC <b>110</b> via a dedicated subnet <b>112</b>, as further described below. Moreover, an analog line <b>14</b> couples each AVSC <b>110</b> to the A/V network <b>30</b>.</p>
    <p>Each AVSC <b>120</b> serves as an A/V file repository, as well as a repository for shared A/V processing resources, including encoders, decoders, and possibly transcoders. The AVSM <b>160</b> coordinates the activities of the AVSCs <b>120</b>, and manages processes both internal and external to the AVSS <b>100</b> to carry out requests generated by either premises, campus, or remote CMCE elements. The structure and functionality of each AVSC <b>120</b> and the AVSM <b>160</b> are described in detail below.</p>
    <p>The internal network <b>110</b> comprises a conventional high-bandwidth network that facilitates high-speed transfer of A/V files between a) individual AVSCs <b>120</b>; b) AVSCs <b>120</b> and the data network <b>20</b>; and c) AVSCs <b>120</b> and the intranet server system <b>60</b> or the internet gateway/firewall system <b>70</b>. The internal network <b>110</b> additionally serves as the medium by which service requests and messages are exchanged between the AVSM <b>160</b> and the data network <b>20</b>. The bandwidth of the internal network <b>110</b> is sufficient to carry service requests, control messages, file transfers, and the file streaming capacity of the composite AVSS <b>100</b>. In one embodiment, the internal network <b>110</b> may be implemented as a portion of the data network <b>20</b>.</p>
    <p>The dedicated subnet <b>112</b> provides for the exchange of messages and control signals between the AVSM <b>160</b> and individual AVSCs <b>120</b>. In the event that the AVSS <b>100</b> utilizes few AVSCs <b>120</b>, such messages and control signals can be carried by the internal network <b>110</b>, eliminating the need for the subnet <b>112</b>. As the AVSS <b>100</b> is scaled, however, use of the subnet <b>112</b> is desirable to minimize internal network loading.</p>
    <p>4.5.1 AVSC Architecture</p>
    <p>As previously indicated, each AVSC <b>120</b> provides A/V file storage and AN processing resources that are shared by other CMCE elements. FIG. 5 is a block diagram of an AVSC <b>120</b> constructed in accordance with the present invention. The AVSC <b>120</b> comprises a processing unit <b>122</b>, a shared data storage unit <b>124</b>, at least one network interface <b>126</b>, an AVSM interface <b>128</b>, at least one shared coding unit <b>130</b>, at least one A/V interface unit <b>140</b>, and a memory <b>150</b> that includes an AVSC object memory <b>152</b>, an AVSC state memory <b>154</b>, and an operating system <b>156</b>. With the exception of the A/V interface unit <b>140</b>, each AVSC element is coupled to a common AVSC bus <b>159</b>. The A/V interface unit <b>140</b> serves as an interface between each shared coding unit <b>130</b> and the A/V switch <b>32</b>. Finally, the network interface <b>126</b> and the AVSM interface <b>128</b> couple the AVSC <b>120</b> to the AVSS internal network <b>110</b> and subnet <b>112</b>, respectively.</p>
    <p>The processing unit <b>122</b> comprises a conventional high-performance processor for executing program instructions stored within the memory <b>150</b>. The network interface <b>126</b> comprises conventional network interface circuitry for managing data exchanges between the AVSC <b>120</b> and the internal network <b>110</b>. In like manner, the AVSM interface <b>128</b> comprises conventional network interface circuitry for managing the exchange of messages and control signals between the AVSC <b>120</b> and the AVSM <b>160</b>. The operating system <b>156</b> preferably comprises conventional, real-time multitasking operating system software such as Windows NT (Microsoft Corporation, Redmond, Wash.) or real-time Unix.</p>
    <p>The AVSC object memory <b>152</b> stores a plurality of AVSC software objects that direct AVSC hardware allocation and resource locking; A/V file encoding, decoding, and transcoding operations; and file management operations such as file replication, transfer, and deletion, as described in detail below. The AVSC objects also maintain the contents of the AVSC state memory <b>154</b>, which includes the following information:</p>
    <p>1) encoder/decoder/transcoder resource capability and current status/utilization;</p>
    <p>2) current storage device capacity and utilization;</p>
    <p>3) a time-stamped and indexed request queue, for both incoming and outgoing requests;</p>
    <p>4) a time-stamped and indexed message queue, for both incoming and outgoing messages;</p>
    <p>5) a time-stamped and indexed file transfer event queue; and</p>
    <p>6) an AVSC event log, specifying standard time-stamped events, as well as occurrences of encoder, decoder, transcoder, storage device, and/or network faults.</p>
    <p>To minimize the amount of hardware required to meet any particular A/V file storage and/or multi-user A/V file access performance requirements, premises A/V file storage hardware should be shared or centralized rather than localized at each workstation <b>40</b>. Centralization of data storage hardware also maximizes system-wide utilization and administration efficiency due to statistical averaging, and can also improve fault-tolerance. The shared data storage unit <b>124</b> comprises at least one high-capacity disk drive for storing A/V files and data, as well as a corresponding disk drive controller. Within a CMCE <b>10</b> having a premises analog signal distribution network, A/V file storage resources are fully AVSC-based rather than workstation-based. In AVSS implementations having more than one AVSC <b>120</b>, multiple copies of any given A/V file may be stored across different AVSCs <b>120</b>, as described in detail below, to improve the system's response in accessing commonly accessed files. Further, when an AVSS <b>100</b> employs a single AVSC <b>120</b>, multiple copies of any given A/V file could be stored across multiple 1) shared data storage units <b>124</b> within the AVSC <b>120</b>; and/or 2) storage devices coupled to the AVSC <b>120</b>. From an external perspective, the AVSS <b>100</b> operating in this manner provides centralized A/V file storage, while A/V files are distributed internally amongst the shared data storage units <b>124</b> across the set of AVSCs <b>120</b>.</p>
    <p>Each shared coding unit <b>130</b> comprises at least one instance of encoding, decoding, and/or transcoding resources. For shared coding units with more than one encoder and/or decoder, a multichannel version of the A/V interface <b>140</b> will be required (or, alternatively multiple A/V interfaces <b>140</b> can be provided for a single coding unit <b>130</b>. In some embodiments, transcoding may be done in software by the Processing unit. In other embodiments, a dedicated hardware transcoder may be usedâ€”such an instance of <b>130</b> would not require an AV interface <b>140</b>. The present invention can support essentially any encoding format, including MPEG-based, RealMedia, and NetShow formats. In one embodiment, the AVSC <b>120</b> uses a given encoding format as a default format, for example, MPEG1 encoding that is tunable within a compression performance range of approximately 1.1 to 3 Mbps. The particular encoding format employed at any given time is an application-dependent and/or user-dependent parameter, as further described below.</p>
    <p>The creation and recording of an A/V file typically requires a single encoding session, which may be stopped or paused and restarted as necessary. Since an A/V file is likely to be played back multiple times, either in repeated or concurrent sessions, decoding is likely to be a much more frequently requested operation than encoding. Across the set of AVSCs <b>120</b> within the AVSS <b>100</b>, one or more shared coding units <b>130</b> may support different encoding, decoding, and/or transcoding session options. For example, the hardware within the shared coding unit <b>130</b> in any particular AVSC <b>120</b> could support, for example, any one of the following:</p>
    <p>1) one encoding session or one decoding session, mutually exclusive;</p>
    <p>2) one encoding session concurrent with one decoding session;</p>
    <p>3) multiple concurrent encoding and decoding sessions;</p>
    <p>4) multiple concurrent decoding sessions; or</p>
    <p>5) one or multiple transcoding sessions, in conjunction with any of the above.</p>
    <p>In addition, any AVSC <b>120</b> could support one of the above either mutually exclusive of or concurrent with file transfer to or from the shared data storage unit <b>124</b>.</p>
    <p>In general, at least one AVSC <b>120</b> is capable of concurrently supporting both A/V file encoding and decoding sessions, as well as multiple sessions simultaneously. Additionally, each AVSC <b>120</b> is highly scalable relative to the number of encoding, decoding, and transcoding resources required to meet evolving AVSS implementation needs. Furthermore, each AVSC <b>120</b> can support essentially any type of encoding, decoding, and/or transcoding resource without architectural modification, thereby readily accommodating A/V processing resource evolution over time. Thus, a wide variety of shared coding unit embodiments exist, examples of which are described hereafter.</p>
    <p>Referring now to FIG. 6, a block diagram of a first embodiment of the shared coding unit <b>130</b> is shown. In the first embodiment, the shared coding unit <b>130</b> comprises at least one encoder <b>132</b>, plus a set of decoders <b>134</b> capable of supporting multiple simultaneous decoding sessions. This requirement can be met through the use of a plurality of mutually-exclusive decoders <b>134</b>, or one or more concurrent-capable decoders <b>134</b>.</p>
    <p>Referring also now to FIG. 7, a block diagram of a second embodiment of the shared coding unit <b>130</b> is shown. In the second embodiment, the shared coding unit <b>130</b> comprises a set of decoders <b>134</b> capable of supporting multiple concurrent decoding sessions. As in the example embodiment, the set of decoders <b>134</b> can be implemented via multiple mutually-exclusive decoders <b>134</b>, or at least one concurrent-capable decoder <b>134</b>. In the second embodiment, the AVSC <b>120</b> in which the set of decoders <b>134</b> reside(s) is dedicated exclusively to fulfilling decoding requests, which in turn implies that at least one encoder <b>132</b> would be present within another AVSC <b>120</b>.</p>
    <p>Those skilled in the art will understand that A/V files may be encoded in accordance with a variety of formats. Thus, at times, transcoding may be required to convert an A/V file into a different format than that which was utilized to record the A/V file. Referring now to FIG. 8, a block diagram of a third embodiment of the shared coding unit <b>130</b> is shown, which comprises at least one encoder <b>132</b>, at least one decoder <b>134</b>, and a transcoder <b>136</b>. When an AVSC <b>120</b> includes one or more transcoders <b>136</b>, the present invention preferably provides for real-time transcoding, such that playback of A/V files requiring format conversion can occur without sequential full-file conversion delays.</p>
    <p>In a manner analogous to that for the shared data storage unit <b>124</b>, encoding, decoding, and transcoding resources are AVSC-based rather than workstation-based in the present invention. From an external perspective, the AVSS <b>100</b> provides centralized encoding, decoding, and transcoding resources, while internally the aforementioned resources are distributed amongst one or more AVSCs <b>120</b>. This pooled resource organization maximizes encoding, decoding, and transcoding resource utilization efficiency, while minimizing the number of such resources required to meet any given performance requirements.</p>
    <p>In an exemplary embodiment, an AVSC <b>120</b> is implemented as a personal computer server having a Pentium II (Intel Corporation, Santa Clara, Calif.) or generally-equivalent microprocessor, 128 Megabytes of Random Access Memory (RAM), an Optibase MovieMaker encoder and an Optibase VideoPlex decoder (Optibase Inc., San Jose, Calif.), a network interface card, and an UltraWide and Fast SCSI 18 Gigabyte or larger disk drive for storing A/V and related files.</p>
    <p>4.5.2 AVSM Architecture</p>
    <p>The AVSM <b>160</b> comprises hardware and software that manages or coordinates the processing of requests received from premises-based, campus-based, or remote CMCE elements. The processing of such requests may result in the creation, recording/encoding, storage, distribution, decoding/playback, archival, or deletion of A/V or multimedia files in the context of conference recording, telephone answering, e-mail, document generation, document publishing, or other applications as described in detail below. Those skilled in the art will understand that many architecturally varying embodiments are possible. The embodiment described below provides an enabling architectural example.</p>
    <p>The AVSM <b>160</b> comprises a network-based multitasking computer. FIG. 9 is a block diagram of an AVSM <b>160</b> constructed in accordance with the present invention. The AVSM <b>160</b> comprises a processing unit <b>162</b>, a network interface <b>164</b>, an AVSC interface unit <b>166</b>, a data storage unit <b>168</b>, and a memory <b>170</b> wherein an AVSM object memory <b>172</b>, an AVSS state memory <b>174</b>, an AVSS database memory <b>176</b>, and an operating system <b>178</b> reside. Each element within the AVSM <b>160</b> is coupled to a common AVSM bus <b>199</b>.</p>
    <p>The processing unit <b>162</b> comprises a conventional high-performance processor for executing stored program instructions, and the data storage unit <b>168</b> comprises at least one disk drive. The network interface <b>164</b> and the AVSC interface unit <b>166</b> comprise conventional network interface circuitry for managing communication with the internal network <b>110</b> and AVSCs <b>120</b>, respectively. The operating system <b>178</b> comprises a conventional multitasking operating system such as Windows NT.</p>
    <p>The AVSM object memory <b>172</b> stores a plurality of AVSM software objects that perform or manage the AVSS services and resource management operations described in detail below, which include a) the establishment of user request processing sessions; b) allocation of AVSC resources and associated resource locking; c) establishment of A/V network communication couplings; d) user application interfacing and message routing; e) inter- and intra-AVSS file transfer initiation; and f) administrative operations as described in detail below. AVSM objects also maintain the contents of the AVSS database, which is stored in the AVSS database memory <b>176</b> and/or upon the data storage unit <b>168</b>.</p>
    <p>The AVSS database stores a variety of information that defines a) the AVSS communications environment; b) the nature and capabilities of the shared AVSC resources; c) administrative parameters and usage data; and d) characteristics of the A/V files stored on the AVSCs <b>120</b>. Relative to the AVSS communications environment, the information the AVSS database stores includes the following:</p>
    <p>1) the number and types of file transfer channels, including digital file transfer channels arising from AVSC couplings to the internal network <b>110</b> and the data network <b>20</b>, plus analog file transfer channels corresponding to AVSC couplings to the A/V network <b>30</b>;</p>
    <p>2) a premises AVSS host name and port identifier;</p>
    <p>3) one or more non-premises AVSS host names and port identifiers, plus connection setup information associated with each non-premises AVSS <b>100</b>;</p>
    <p>4) A/V network configuration parameters;</p>
    <p>5) supporting server system configuration parameters;</p>
    <p>6) a user identification (ID) or name for each user within the premises group to which the AVSS <b>100</b> belongs, plus password information corresponding to each user ID; and</p>
    <p>7) communication preferences corresponding to each user ID, such as preferred encoding format and/or A/V file delivery format.</p>
    <p>Relative to shared AVSC resources, the AVSS database includes the following information:</p>
    <p>1) the number, types, and capacities of the storage resources within each AVSC <b>120</b>; and</p>
    <p>2) the number, types, and capabilities of the encoding, decoding, and transcoding resources within each AVSC <b>120</b>.</p>
    <p>The administrative parameters that reside within the AVSS database include a maximum allowable A/V file size, and a maximum allowable A/V file age. The maximum allowable A/V file age may be defined in relation to an A/V file's creation time and date, or relative to a most-recent access. The usage data includes an event statistics file that details event occurrence frequencies, such as a number of users that have accessed the AVSS <b>100</b> in a given time interval; a number of times an A/V file has been accessed; a most-recent access date and time for an A/V file; and an amount of time spent performing A/V file playback operations during a particular time interval.</p>
    <p>For each unique A/V file stored on the AVSCs <b>120</b>, the AVSS database stores a file parameter table that includes the following information:</p>
    <p>1) a filename;</p>
    <p>2) a file password;</p>
    <p>3) a user ID indicating file authorship;</p>
    <p>4) a file ownership or access privilege list, by user ID, which includes an access expiration date and time as well as recent access history (date and time) associated with the user ID;</p>
    <p>5) A/V encoding format;</p>
    <p>6) time and date of most-recent modification;</p>
    <p>7) file size;</p>
    <p>8) file playback duration;</p>
    <p>9) file age;</p>
    <p>10) location of each copy of the file across the AVSCs <b>120</b>;</p>
    <p>11) a list identifying and locating any AVSS-resident multimedia synchronization files associated with the A/V file; and</p>
    <p>12) a list specifying any non-AVSS target servers to which the file has been published, as described in detail below.</p>
    <p>For each premises group user ID, the AVSM objects could also maintain a user-specific file list in the AVSS database. In such an embodiment, the user-specific file list includes the name of each A/V file for which the user ID is specified in the file's access privilege list, and indicates whether the user ID is specified in the file's authorship data.</p>
    <p>AVSM objects additionally maintain the contents of the AVSS state memory <b>174</b> to reflect current resource, request, and message status across all currently-active request processing sessions. The contents of the AVSS state memory <b>174</b> include the following:</p>
    <p>1) a current user list that specifies user IDs corresponding to currently logged-in users;</p>
    <p>2) a list of currently-active request processing sessions;</p>
    <p>3) a time-stamped and indexed request queue identifying currently-pending requests, for both incoming and outgoing requests;</p>
    <p>4) currently-available capacity and utilization for each AVSS storage resource;</p>
    <p>5) current encoding/decoding/transcoding resource utilization; and</p>
    <p>6) an AVSM event log specifying session time-out occurrences; application errors; AVSC faults; network faults; changes in the current user list and currently-active session list; changes in storage resource utilization and available storage space; and changes in encoding, decoding, and/or transcoding resource utilization. The contents of the AVSM event log are used to update the event statistics file.</p>
    <p>In an exemplary embodiment, the AVSM <b>160</b> is implemented as a personal computer server having a Pentium II or generally-equivalent processor, a network interface card, 128 Megabytes of RAM, and a 10 Gigabyte or larger hard disk drive. Those skilled in the art will understand that the AVSM <b>160</b> could be implemented using a hardware/software platform that is essentially identical to that used for implementing an AVSC <b>120</b>.</p>
    <p>4.5.3 Scaling Hierarchy</p>
    <p>The present invention can be scaled to provide enhanced performance and/or additional capabilities. Two types of scaling are possible, namely, numerical and evolutionary scaling. Numerical scaling implies the incorporation of additional hardware elements, while evolutionary scaling implies replacement of existing hardware elements with higher-performance or more â€œtechnologically evolvedâ€ hardware. The architectures described herein flexibly accommodate numerical and/or evolutionary scaling across several interrelated hierarchical performance boundaries or levels, which include the following:</p>
    <p>4.5.3.1 Bus and processor throughputâ€”the capabilities of the bus and, processor within an AVSC <b>120</b> or AVSM <b>160</b> respectively define one type of AVSC or AVSM performance boundary. For example, bus throughput considerations limit the amount of bandwidth available for transferring information between elements within any given AVSC <b>120</b> or AVSM <b>160</b>. Through a hardware platform upgrade, which is an evolutionary scaling, this performance boundary can be surpassed or extended.</p>
    <p>4.5.3.2 Shared data storage unit throughput and capacityâ€”the data transfer rate and storage capacity of the shared data storage unit <b>124</b> within any particular AVSC <b>120</b> define another performance boundary. This boundary can be surpassed by the inclusion of additional data storage devices within the shared data storage unit <b>124</b>, and/or the use of higher-performance data storage devices. The performance gain achieved through numerical or evolutionary scaling in this case is dependent upon the bus throughput boundary described above.</p>
    <p>4.5.3.3 Shared coding unit throughput and capabilitiesâ€”the A/V file processing capabilities of the shared coding unit <b>130</b> within any given AVSC <b>120</b> also define a performance boundary. The incorporation of additional encoding, decoding, and/or transcoding resources within a shared coding unit, and/or the use of higher-performance A/V processing resources, results in the extension of this boundary. Those skilled in the art will understand that higher-performance resources in this case would likely perform essentially-simultaneous encoding, decoding, and/or transcoding operations on multiple execution threads. Those skilled in the art will additionally recognize that the performance gain achieved through numerical or evolutionary scaling here is dependent upon the aforementioned bus throughput boundary.</p>
    <p>4.5.3.4 Collective AVSC capabilities and throughputâ€”the overall A/V file processing performance and capabilities of the set of AVSCs <b>120</b> within the AVSS introduce another performance boundary, which can be surpassed through the incorporation of additional AVSCs <b>120</b> into the AVSS, and/or the use of higher-performance AVSCs <b>120</b>. Those skilled in the art will understand that increasing the performance of any given AVSC <b>120</b> involves surpassing one or more of the previously-described performance boundaries.</p>
    <p>4.5.3.5 Internal network bandwidthâ€”the data transfer capabilities or loading limitations of the AVSS internal network <b>110</b> introduce another performance boundary, which can be surpassed through evolutionary scaling.</p>
    <p>4.5.3.6 Collective networked AVSS performance and capabilitiesâ€”the performance and capabilities of an entire AVSS group coupled via a campus and/or wide-area network also introduces a performance boundary that can be readily surpassed through numerical and/or evolutional scaling in accordance with performance requirements relative to cost constraints.</p>
    <p>Those skilled in the art will recognize that just as a system can be scaled upward relative to performance, it can also be scaled downward, which might occur, for example, when cost constraints are of paramount importance. The architecture of the present invention, however, can provide high A/V and/or multimedia processing performance while leveraging the capabilities of readily-available, low-cost technology.</p>
    <heading>4.6 Functional Partitioning Between AVSM and AVSCs</heading> <p>The types of functions performed by the AVSM <b>160</b> are partitioned relative to those performed by the AVSCs <b>120</b> to ensure consistent operability across essentially any internal AVSS implementation, particularly in view of numerical and evolutionary scalability. As a result, the AVSM <b>160</b> can support a continually-evolving range of AVSC implementations without modification.</p>
    <p>The AVSM <b>160</b> provides centralized management of AVSS functionality. The AVSM <b>160</b> receives AVSS service requests and control messages from user applications. In response to service requests, the AVSM <b>160</b> establishes request processing sessions, as described in detail below. The AVSM generates a session handle to uniquely identify each such session. Exemplary AVSS service requests include the following:</p>
    <p>1) login {user ID, password};</p>
    <p>2) create and encode A/V file {session handle file name, file password, plus other parameters described below};</p>
    <p>3) fetch and decode A/V file {session handle, file name, file password};</p>
    <p>4) delete A/V file {session handle file name, file password};</p>
    <p>5) copy A/V file {source address, source session handle, target address, target session handle plus other parameters described below}; and</p>
    <p>6) move A/V file {source address, source session handle, target address, target session handle plus other parameters described below}.</p>
    <p>Other types of AVSS service requests include requests for sending or receiving streams; retrieving or distributing files; and performing administrative and diagnostic operations. Control messages received from user applications include acknowledgments; error codes; and interactive control requests such as start, stop, pause, reverse, and/or fast forward commands issued during the recording, playback, and/or editing of A/V files.</p>
    <p>In response to AVSS service requests, the AVSM <b>160</b> may directly perform one or more functions, and/or issue a set of high-level requests to its associated AVSCs <b>120</b> in accordance with AVSC capabilities and availability. In terms of directly-performed functions, the AVSM <b>160</b> establishes and manages request processing sessions with user applications; allocates A/V file names and tracks A/V file location and usage information; allocates AVSC resources to request processing sessions based upon AVSC capabilities and availability; allocates AVSC resources to file copy and move transactions; forwards file copy and move requests to campus or remote systems in the event that an A/V file does not reside upon one of the premises AVSCs <b>120</b>; and issues requests to the AN network <b>30</b> to establish AN communication and perform A/V network services (such as conferencing) for user applications.</p>
    <p>Any given AVSC <b>120</b> that receives a high-level request from the AVSM <b>160</b> carries out the request by performing a set of operations in accordance with its own methods and constraints. In this manner, the AVSM <b>160</b> may be designed to avoid â€œmicro-managingâ€ the details of AVSC <b>120</b> operation. Depending upon the nature of a high-level request, an AVSC <b>120</b> may allocate and lock an encoding, decoding, or transcoding resource; allocate file storage space; execute an encoding, decoding, or transcoding procedure; perform a file transfer or copy operation to or from a local or remote target destination or source; delete a file; or report request processing, message, storage device, and/or encoding, decoding, or transcoding resource status to the AVSM <b>160</b>.</p>
    <heading>4.7 AVSS Services</heading> <p>The AVSM <b>160</b> manages or provides a variety of AVSS services, including session management, file management, device management, and administrative services. Details of such services and the manners in which they are implemented are described hereafter.</p>
    <heading>4.7.1 Session Management Services</heading> <p>From the perspective of application programs executing on user workstations <b>40</b> or other computers coupled to the CMCE <b>10</b>, the premises AVSM <b>160</b> provides access to services associated with premises, campus, and/or wide-area AVSS resources. Any given application program typically generates one or more graphical windows plus menus and control, list, dialog, and/or other graphical boxes that form a portion of a Graphical User Interface (GUI), as described in detail below. The application program is responsive to selections that a user graphically indicates, in a manner well understood by those skilled in the art.</p>
    <p>Particular user selections indicate that the user requires AVSS-related services. In response to such selections, the application program issues service requests to the associated premises AVSM <b>160</b>. Upon receiving a service request, the AVSM <b>160</b> establishes a request processing session to provide or oversee the provision of the required service. During the session, the AVSM <b>160</b> may directly perform one or more operations, and/or the AVSM <b>160</b> may generate one or more requests directed to premises AVSCs <b>120</b>, the premises A/V network manager <b>34</b>, and/or campus or remote AVSMs <b>160</b>. Additionally, premises, campus, or remote CMCE elements involved in the session may generate requests directed to each other and/or the application program.</p>
    <p>FIG. 10 is a block diagram illustrating session communication protocols utilized in one embodiment of the present invention. Essentially any number of instances of a wide variety of application programs communicate with their corresponding premises AVSM <b>160</b> via an AVSM client protocol. A given premises AVSM <b>160</b> communicates with its associated AVSCs <b>120</b> via an AVSC client protocol, and the premises A/V network manager <b>34</b> via an A/V network client protocol. Cross-premises AVSM-to-AVSM communication is based upon an AVSM peer protocol, while analogous A/V network manager communication is based upon an A/V network peer protocol.</p>
    <p>Prior to gaining access to A/V processing resources or services, a workstation user must execute an AVSS access program, which issues a login request to the AVSM <b>160</b>. As described in detail below, the login request includes a user ID, which establishes the user's identity to the AVSM <b>160</b> for purposes of file ownership and access permission checking. The user ID also provides the AVSM <b>160</b> with the A/V network port information necessary for establishing any A/V network connections required. In one embodiment, the, login request additionally includes password information, which may be encrypted. When a user no longer requires access to A/V processing resources or services, the user may utilize the AVSS access program to issue a logout request to the AVSM <b>160</b>, which results in removal of the corresponding user ID from the current user list within the AVSS state memory <b>174</b>.</p>
    <p>Following a successful login, an application program executing on the user's workstation <b>40</b> may issue a request to the AVSM <b>160</b> for A/V processing resources and/or A/V file management services. Upon receipt of a request, the AVSM <b>160</b> establishes a session by performing any necessary file access authorization operations as described below; generating a session handle that uniquely identifies the session; adding the session handle to the currently-active session list in the AVSS state memory <b>174</b>; identifying and allocating appropriate AVSC resources; identifying and allocating appropriate A/V network resources; and issuing a session identification message to the requesting client, where the session identification message includes the session handle. After session establishment, the AVSM <b>160</b> manages the session by directly performing operations, and/or issuing or routing requests, control messages, and/or status messages to AVSCs <b>120</b>, the A/V network manager <b>34</b>, other AVSMs <b>160</b>, and/or the application program. The AVSM <b>160</b> terminates the session by deallocating resources, issuing a termination message or reply to the application program, and deleting the session handle from the currently-active session list. The AVSM <b>160</b> supports at least three types of sessions, namely, 1) viewer tool sessions; 2) maintenance tool sessions; and 3) administrative tool sessions. The characteristics of each of these session types, as well as the operations undertaken by the AVSM <b>160</b> in support thereof, are described in detail hereafter.</p>
    <p>4.7.1.1 Viewer Tool Sessions</p>
    <p>Herein, a viewer tool comprises an application program that controls A/V file recording as well as real-time analog or digital streaming A/V file delivery or viewing via one or more graphical windows, menus, and control, dialog, or other boxes that facilitate user input. The AVSM <b>160</b> establishes a viewer tool session when a viewer tool application executing on a workstation <b>40</b> or other computer issues a request specifying that either real-time (or near-real-time) analog or digital streaming delivery of an A/V or multimedia file is required (i.e., when a user needs to view an A/V file). Real-time analog or digital streaming delivery is required, for example, during A/V file recording, editing, and/or playback, and involves A/V signal delivery to a display device (such as a workstation monitor or television) under the direction of the viewer tool application program.</p>
    <p>To initiate a viewer tool session, the AVSM <b>160</b> determines whether the request received from the viewer tool application specifies an existing A/V or multimedia file. If so, the AVSM <b>160</b> performs file access authorization operations, as described in detail below, to verify whether the viewer tool user should be granted access to the file. If the file access authorization operations are unsuccessful, the AVSM <b>160</b> issues a reply to the requesting viewer tool application indicating proper access authorization does not exist, and the viewer tool session ends.</p>
    <p>Upon successful completion of the file access authorization operations, the AVSM <b>160</b> generates a session handle, and adds it to the currently-active session list. The AVSM <b>160</b> next identifies an AVSC <b>120</b> that includes A/V processing resources capable of performing the operations required by the viewer tool application. In one embodiment, the AVSC selection is performed in accordance with the resource allocation operations described in detail below. For the creation (i.e., recording) of a new A/V or multimedia file, the AVSM <b>160</b> selects an AVSC <b>120</b> upon which sufficient storage space and an appropriate type of encoder exist. For the playback or editing of an existing A/V or multimedia file, the AVSM <b>160</b> selects an AVSC <b>120</b> upon which a copy of the file resides and appropriate decoding or transcoding resources are present.</p>
    <p>After identifying an appropriate AVSC <b>120</b>, the AVSM <b>160</b> sends a request to the AVSC <b>120</b> to allocate the resources required for performing the operation(s) required by the viewer tool application program. In one embodiment, the AVSM <b>160</b> subsequently issues a request to the AV network manager <b>34</b> to establish any required A/V network connections. In an alternate embodiment, the viewer tool application program issues a request to the A/V network manager <b>34</b> to establish such connections. After AVSC selection/resource allocation and A/V network connection establishment, the AVSM <b>160</b> issues a session identification message containing the session handle to the viewer tool application, which indicates that the viewer tool session can proceed.</p>
    <p>During the viewer tool session, the AVSM <b>160</b> routes control messages received from the viewer tool application to the selected AVSC <b>120</b>, thereby facilitating AVSC performance of open-file, play, adjust-playback-rate, stop, seek, pause, rewind, fast-forward, record, save-file, or other operations as supported by the viewer tool application program The AVSM <b>160</b> may additionally issue status messages to the viewer tool application, which can include video frame numbers and/or running or elapsed time. In one embodiment, the AVSM <b>160</b> polls the AVSC <b>120</b> for status information, and forwards it to the viewer tool application program.</p>
    <p>A multimedia file comprises one or more types of files and/or references to files, where such files may include textual, graphical, image, audio, and/or video information as well as commands or event sequences for generating or rendering such information. The multimedia file may further include temporal correlation data specifying one or more manners in which its constituent files and/or file references are associated in time. In the context of the present invention, a multimedia file comprises one or more omnifiles, where an omnifile is defined as a metafile plus a corresponding pointer file. The metafile includes one or more of the following types of files:</p>
    <p>1) A/V files;</p>
    <p>2) audio-only files;</p>
    <p>3) video-only files;</p>
    <p>4) bitmap files;</p>
    <p>5) postscript files;</p>
    <p>6) graphics files;</p>
    <p>7) text files;</p>
    <p>8) application files; and</p>
    <p>9) synchronization files.</p>
    <p>The synchronization files associate events or commands occurring within the context of one or more application programs with particular time references, which may be A/V file frame numbers. Synchronization files can include application startup event files, text event files, window event files, graphics rendering event files, and shareboard event files, as further described below. In one embodiment, a synchronization file can include a priority level for each command or event specified therein. The various files that comprise the metafile may be stored across one or more workstations <b>40</b>, AVSCs <b>120</b>, servers, or other computers coupled to the CMCE <b>10</b>; The pointer file associated with a metafile contains pointers or references to each element within the metafile.</p>
    <p>In one embodiment, the synchronization files are stored on AVSCs <b>120</b>. In the event that the viewer tool session involves multimedia file recording, the AVSM <b>160</b> transfers application program synchronization information or commands received from the viewer tool application to the AVSC <b>120</b> such that the synchronization information can be saved in a synchronization file. For multimedia file playback and/or editing, the AVSM <b>160</b> requests the synchronization file from the appropriate AVSC <b>120</b>. To ensure temporal consistency during playback operations, the AVSM <b>160</b> polls the AVSC <b>120</b> allocated to serving the current viewer tool session for frame number or time-based A/V file data, and issues synchronization messages to the viewer tool application at appropriate times or intervals, in accordance with a currently-specified playback frame rate or speed. The synchronization messages contain event information, commands, and command priority levels specified within the synchronization file. Upon receipt of a synchronization message, the viewer tool application executes commands specified therein, or facilitates the execution of such commands via their transfer to other application programs, thereby regenerating application events and/or the presentation of textual, graphics, image, or other multimedia content information at appropriate times while A/V file playback occurs.</p>
    <p>The AVSM <b>160</b> could alternatively transfer the synchronization file to the viewer tool application, poll the AVSC <b>120</b> for frame-number or time-based A/V file data, and issue frame or time status messages to the viewer tool. The viewer tool application could then execute or issue the appropriate commands to facilitate multimedia synchronization upon receipt of frame or time status messages that correspond to frame numbers or times specified within the synchronization file. In other embodiments, the synchronization files themselves could be stored on the AVSM <b>160</b> rather than on AVSCs <b>120</b>, or the synchronization files could normally reside upon the user workstation <b>40</b>.</p>
    <p>In one embodiment, in the event that the viewer tool session involved A/V file or multimedia file editing and file edits or changes were saved while maintaining the filename, the AVSM <b>160</b> performs modification propagation operations as described in detail below to ensure that the file changes propagate to each copy of the A/V file stored on the AVSCs <b>120</b>.</p>
    <p>The operations described above pertain to within-premises viewer tool sessions. Non-premises viewer tool sessions occur in an analogous manner. In one embodiment, in the event that a viewer tool application requires real-time/near real-time analog or digital streaming A/V file delivery from a non-premises AVSS <b>100</b>, the viewer tool application issues a request to the appropriate non-premises AVSM <b>160</b>. In response, the non-premises AVSM <b>160</b> establishes a viewer tool session in the manner described above, such that analog A/V signals are exchanged via the premises workstation <b>40</b>, the premises codec gateway <b>38</b>, the second WAN <b>39</b>, and the non-premisescodec gateway <b>38</b> that serves the non-premises AVSS <b>100</b>.</p>
    <p>A viewer tool provides a workstation user with a Graphical User Interface (GUI) for controlling A/V file recording and playback operations. FIG. 11 is a block diagram showing an exemplary recording control GUI <b>500</b>. The recording control GUI <b>500</b> comprises a graphical window that includes a menu bar <b>502</b>, a title box <b>504</b>, a recording control panel <b>506</b>, and a time panel <b>520</b>. The menu bar <b>502</b> provides a workstation user with access to conventional types of submenus, including file, options, and help submenus. User-selectable operations from within the options submenu include login and change-password operations. User-selectable operations from within the file submenu include the following:</p>
    <p>1) Newâ€”opens a new A/V file for recording, in the manner described below;</p>
    <p>2) Openâ€”invokes a browser tool that facilitates opening and subsequent playback of an existing A/V file, as described below;</p>
    <p>2) Saveâ€”saves an A/V file with a given filename;</p>
    <p>3) Propertiesâ€”facilitates entry and editing of properties associated with the A/V file being recorded, such as encoding type and a natural language title; and</p>
    <p>4) Exitâ€”terminates the viewer tool session.</p>
    <p>The title box <b>504</b> identifies the current title of an A/V file under consideration. The recording control panel <b>506</b> provides a set of user-selectable buttons for controlling recording operations. These buttons include a start recording button <b>510</b>; a pause recording button <b>512</b>; a stop or end recording button <b>514</b>, and an erase or discard recorded information button <b>516</b>. The time panel <b>520</b> graphically indicates a current recording length or time via the provision of a slider bar <b>522</b> and a time box <b>524</b>.</p>
    <p>FIG. 12 is a block diagram showing an exemplary playback control GUI <b>600</b>. For ease of understanding, common elements shown in FIGS. 11 and 12 are denoted with common reference numbers. The playback control GUI <b>600</b> comprises a graphical window that includes a menu bar <b>502</b>, a title box <b>504</b>, a playback control panel <b>606</b>, and a time panel <b>520</b>. The menu bar <b>502</b> enables user to select from among the operations described above with reference to FIG. <b>11</b>. Similarly, the title box <b>504</b> indicates the current title of the A/V file under consideration. The playback control panel <b>606</b> includes a plurality of user-selectable buttons that facilitate playback control operations, including a rewind to beginning button <b>610</b>; a skip back button <b>612</b> that facilitates moving or jumping backward to an earlier position within an A/V file, where the earlier position corresponds to a predetermined or user-definable time increment, for example. 3 seconds; a playback start button <b>614</b>; a playback pause button <b>616</b>; a skip forward button <b>618</b> that facilitates moving or jumping backward to a later position within an A/V file, where the later position corresponds to a predetermined or user-definable time increment, for example, 3 seconds; and an erase file button <b>619</b>. In a manner analogous to that for the recording control GUI <b>500</b>, the time panel <b>520</b> graphically indicates a current playback length or time via the provision of a slider bar <b>522</b> and a time box <b>524</b>. Those skilled in the art will understand that the recording and playback control GUIs <b>500</b>, <b>600</b> provide workstation users with a graphical interface that is visually consistent with controls found on common devices such as Video Cassette Recorders (VCRs).</p>
    <p>In response to certain submenu selections, such as Open within the file submenu, the viewer tool transfers control to or invokes functionality within or associated with a browsing or administrative tool to provide a workstation user with a GUI for browsing and selecting A/V files and/or performing A/V file administration operations as described in detail below. FIG. 13 is a block diagram showing an exemplary browsing control GUI <b>700</b>. The browsing control GUI <b>700</b> comprises a dialog box that includes a file source listbox <b>702</b> and file source control buttons <b>704</b>; display format control buttons <b>706</b>; a file information listbox <b>710</b>; a file name box <b>722</b>; a file type listbox <b>724</b>; and operation control buttons <b>730</b>, <b>732</b>, <b>734</b>. The file source listbox <b>702</b> and associated file source control buttons <b>704</b> facilitate user-identification of a file source from which A/V files can be selected for consideration. The display format control buttons <b>706</b> facilitate user-selection of file information display formats, where available display formats include display-by-icon and display-by-attributes. For each A/V file for which the workstation user is identified as an owner, the file information listbox <b>710</b> displays file information in accordance with the selected file parameter display format. Such information can include a filename; a file location; a natural language title; a description; file authorship; and playback duration. The file name box <b>722</b> facilitates user-entry of a specific filename, and the file type listbox <b>724</b> facilitates browsing operations directed to files of a specific type. Finally, the operation control buttons include an open button <b>730</b>, the selection of which results in the opening of a specified or selected A/V file for playback operations; a cancel button <b>732</b>; and a help button <b>734</b>.</p>
    <p>4.7.2 Maintenance Tool Sessions</p>
    <p>The AVSM <b>160</b> establishes a maintenance tool session in response to a request that specifies that one or more user-based file maintenance operations are required. In the context of the present invention, a maintenance tool comprises an application program executing on a user workstation <b>40</b> or other computer, and which generates one or more graphical windows, menus, and/or boxes that facilitate particular A/V file maintenance operations in response to user input. In one embodiment, the user input may be in the form of text, graphical selections, or graphical drag-and-drop sequences.</p>
    <p>As described in detail below, the AVSM <b>160</b> supports many file management operations, a subset of that comprises file maintenance operations that are directly available to workstation users. In one embodiment, the file maintenance operations include file access list maintenance; file expiration date maintenance; file renaming; file transfer in the form of either copying or moving; file deletion; user-specific file listing; user-specific archival operations; and file attribute queries. Each of these operations is described in detail below in the context of AVSM file management operations. Those skilled in the art will recognize that the file maintenance operations could include additional or fewer types of operations in another embodiment.</p>
    <p>The AVSM <b>160</b> initiates a maintenance tool session by performing A/V file access authorization operations as described in detail below. If access authorization is unsuccessful, the AVSM <b>160</b> terminates the session. Otherwise, the AVSM <b>160</b> generates a session handle, adds it to the currently-active session list, and issues a session identification message to the maintenance tool application. The AVSM <b>160</b> determines whether access to one or more campus and/or remote AVSMs <b>160</b> is required to process the file maintenance request, and, if so, issues requests to allocate the required campus and/or remote resources. Once such resources are allocated, the campus and/or remote AVSMs <b>160</b> issue a reply to the premises AVSM <b>160</b> specifying AVSC network address information if required. The AVSM <b>160</b> subsequently determines what premises AVSCs <b>120</b> are required to process the file maintenance request, and issues requests to the premises AVSCs <b>120</b> to allocate the appropriate resources and begin performing the required operation.</p>
    <p>During the performance of file maintenance operations, the AVSM <b>160</b> issues status requests to premises and/or non-premises AVSCs <b>120</b>, which reply with status information. The AVSM <b>160</b> then sends status messages to the file maintenance tool application accordingly. Such status messages may indicate, for example, a percentage of the operation that has been completed or an estimated amount of time remaining until completion. Once the file maintenance operation is complete, the AVSM <b>160</b> issues a session termination reply to the maintenance tool.</p>
    <p>Some file maintenance operations, such as A/V file renaming or copying, may be requested within the context of a viewer tool session. That is, one or more aspects of maintenance tool functionality may be incorporated into a viewing tool. In such situations, the AVSM <b>160</b> performs or oversees the performance of the file maintenance operation within the context of the viewer tool session, in a manner essentially identical or analogous to that described for the file maintenance tool session. Moreover, in one embodiment, a maintenance tool may provide for the invocation of a particular viewer tool in response to user input, in which case the AVSM <b>160</b> establishes a viewer tool session as a result.</p>
    <p>4.7.3 Administrative Tool Sessions</p>
    <p>Herein, an administrative tool comprises an application program that generates, presents, and/or manages administrative information on a workstation-based or other display device in response to user input. The AVSM <b>160</b> establishes an administrative tool session in response to a request for user-accessible information stored in the AVSS database. Such information may include, for example, user account and password information, or particular data within the event statistics file.</p>
    <p>Upon receiving an administrative information request, the AVSM <b>160</b> generates a session handle, and adds it to the currently-active session list. The AVSM <b>160</b> then transfers data corresponding to the request to the administrative tool. Upon receiving the data, the administrative tool generates and/or presents administrative information to the user. If administrative data is user-modifiable, as could be the case for user passwords, the AVSM <b>160</b> changes the data within the AVSS database in response to a request received from the administrative tool. Some administrative data may be non-user-modifiable, such as current AVSS utilization; AVSS usage history for the user, which may include average session length, by session type; and/or current charges billed to the user.</p>
    <p>4.7.4 File Management Services</p>
    <p>As indicated above, the AVSM <b>160</b> provides or oversees the provision of a variety of file management services. A subset of the file management services comprise the user-accessible file maintenance operations, while other file management services are performed by the AVSM <b>160</b> on an internal-only basis. In one embodiment, the file management services include the following:</p>
    <p>4.7.4.1 Access Authorization Services</p>
    <p>AVSM software objects maintain the access privilege list within each A/V file's parameter table in the AVSS database. The access privilege list comprises a list of user IDs that specifies which users â€œownâ€ the A/V file, where ownership implies basic access rights to the A/V file. The extent of A/V file access rights may depend upon whether a user falls into a particular category, such as the A/V file's author or other category as detailed below. In one embodiment, an A/V file access request generated by a client application program includes a user ID, a filename, and a password. Upon receiving such a request, the AVSM <b>160</b> determines whether the user ID associated with the request is specified within the A/V file's access privilege list. If so, the AVSM <b>160</b> proceeds with processing the file access request. If the user ID is not specified within the access privilege list, the AVSM <b>160</b> compares the password specified within the A/V file access request with that in the file parameter table. If the passwords match, the AVSM <b>160</b> adds the user ID to the access privilege list, and proceeds with request processing operations. Otherwise, the AVSM <b>160</b> issues a reply containing an error code to the client application program to indicate that A/V file access has been denied.</p>
    <p>In one embodiment, the presence of a reserved user ID in an A/V file's access privilege indicates that the file may be accessed by any user, that is, the file is generally or publicly accessible. One embodiment of the present invention also provides for an administrative â€œsuper-userâ€ defined to have essentially unlimited access privileges for any A/V file. In response to a request for A/V file access received from an application program associated with the super-user, the AVSM <b>160</b> proceeds with request processing operations.</p>
    <p>4.7.4.2 Access List Maintenance</p>
    <p>In response to an access list maintenance request received from a client application program, the AVSM <b>160</b> selectively adds user IDs to or deletes user IDs from an A/V file's access permission list. The AVSM <b>160</b> performs access list maintenance operations only for users having a user ID specified in the access permission list, subject to the whether a user is the A/V file author, an owner, or a super-user. The AVSM <b>160</b> allows an A/V file author or a super-user to modify the access permission list as required. The AVSM <b>160</b> permits any given A/V file owner that is not the file's author to remove their user ID from the access permission list.</p>
    <p>In one embodiment, the AVSM <b>160</b> adds the user ID of a user who authors an A/V file to the A/V file's access permission list by default. The AVSM <b>160</b> may add other user IDs to the access permission list in response to client application program requests. For example, an A/V- or multimedia-enabled e-mail application program as described in detail below allows message authors to include an A/V or multimedia file as a MIME-based message attachment. The AVSM <b>160</b> adds the user ID corresponding to each message recipient to the A/V file's access permission list, as directed by the e-mail application program. The AVSM <b>160</b> may also or alternatively add a message recipient's user ID to the access permission list in response to an A/V file access request specifying a user ID, a filename, and a valid password.</p>
    <p>4.7.4.3 User-Specific File Listing</p>
    <p>An application program associated with or executing on behalf of a user may issue a file listing request to the AVSM <b>160</b>. In turn, the AVSM <b>160</b> performs a directory sort operation, and replies to the application program with a list identifying each A/V file for which the user is specified as an owner in the file's access permission list. In an alternate embodiment in which the AVSM <b>160</b> maintains a user-specific file list in the AVSS database as described above, the AVSM <b>160</b> could reply to the application program with data from the user-specific file list.</p>
    <p>The application program may subsequently display a set of filenames for which the user is specified as an owner. In one embodiment, the file listing request may indicate that data for publicly-available files is also required. If so, the AVSM <b>160</b> provides such data to the requesting application program.</p>
    <p>4.7.4.4 User-Specific File Archival</p>
    <p>An application program may additionally issue a file archival request to the AVSM <b>160</b>, where the file archival request identifies one or more A/V files owned by the user associated with the requesting application program for which archival operations are required. The AVSM <b>160</b> subsequently manages archival operations in which the identified A/V files or copies thereof are stored in a premises or non-premises user ID-based archive. The archival operations include one or more file transfer services as detailed below.</p>
    <p>4.7.4.5 Attribute Query</p>
    <p>In response to an attribute request received from an owner of an A/V file, the AVSM <b>160</b> provides attribute information for the A/V file, such as file author, size, encoding type, playback duration, current file age, time and date the owner last accessed the file, file expiration date (i.e., access expiration date) for the owner, and/or date of most-recent modification.</p>
    <p>4.7.4.6 File Transfer Services</p>
    <p>File transfer services supported by the AVSM <b>160</b> include a) A/V file copy and/or move (i.e., relocate) operations to and/or from a target destination or source; b) file import or export operations between a workstation <b>40</b> or other computer and an AVSC <b>120</b>; and c) digital file publishing operations between an AVSC <b>120</b> and a server external to the AVSS <b>100</b> in which the AVSC <b>120</b> resides. With regard to A/V file copy or move operations, the target destination or source may be premises-, campus-, or wide-area-based. Since any given AVSS <b>100</b> may be linked to another AVSS <b>100</b> via a LAN or a WAN, both intra- and inter-AVSS file transfers are possible in the context of the present invention. FIG. 14 is a block diagram of an exemplary networked AVSS organization. In the exemplary networked AVSS organization, a first AVSS <b>100</b>.<b>1</b> through kth AVSS <b>100</b>.<b>2</b> serve a common campus, and hence are coupled to a common data network <b>20</b>. A remote AVSS <b>100</b>.<b>3</b> is coupled to the common data network <b>20</b> via a WAN <b>29</b>. Those skilled in the art will recognize that the exemplary networked AVSS organization shown in FIG. 14 can be generalized to accommodate essentially any required network organization or geographic scaling.</p>
    <p>Each AVSS <b>100</b> in FIG. 14 includes a set of AVSCs <b>120</b>, where each AVSC <b>120</b> includes a number of disk drives or other devices for storing A/V files. The number of AVSCs <b>120</b> can vary from one AVSS <b>100</b> to another, as can the number and type of storage devices from one AVSC <b>120</b> to another. Relative to intra-AVSS file transfers, A/V files can be transferred from one storage device to another either a) within the same AVSC <b>120</b>, that is, â€œwithin cellâ€ (WC); or b) across different AVSCs <b>120</b> in the same AVSS <b>100</b>, or â€œcell to cellâ€ (CC). In terms of A/V file transfers directed from an AVSC storage device within one AVSS <b>100</b> to an AVSC storage device within another AVSS <b>100</b>, such transfers can occur either a) within the common campus, or local area (LA); or b) on a wide area (WA) basis, between a common campus AVSS <b>100</b> and the remote AVSS <b>100</b>.</p>
    <p>In response to a request that requires a cell-to-cell or within-cell A/V file transfer, the AVSM <b>160</b> selects and allocates a source and a destination premises AVSC <b>120</b>, and issues a request to the one of the selected AVSCs <b>120</b> to perform the required transfer. In one embodiment, the request may specify a copy-from, move-from, copy-to, or move-to operation. After the file transfer is complete, the AVSC <b>120</b> that fulfilled the request issues a reply to the AVSM <b>160</b>.</p>
    <p>If a file transfer request requires A/V file transfer between a premises and a non-premises AVSS <b>100</b>, the premises AVSM <b>160</b> issues a request to the non-premises AVSM <b>160</b> to obtain network address information for an associated non-premises AVSC <b>120</b> that can participate in fulfilling the request. Upon receiving such information from the non-premises AVSM <b>160</b>, the premises AVSM <b>160</b> selects and allocates an appropriate premises AVSC <b>120</b>, and issues a request to the premises AVSC <b>120</b> to perform the required file transfer operation. If the file transfer request specifies a copy-to or move-to operation, the A/V file will be transferred from the premises AVSC <b>120</b> to the non-premises AVSC <b>120</b> accordingly. If the file transfer request specifies a copy-from or move-from operation, the A/V file will be copied or moved from the non-premises AVSC <b>120</b> to the premises AVSC <b>120</b>, respectively. After the file transfer is complete, the premises AVSC <b>120</b> issues a reply to the premises AVSM <b>160</b>. The premises AVSM <b>160</b> then issues a notification to the non-premises AVSM <b>160</b> to indicate that the file transfer session is complete.</p>
    <p>FIG. 14 also shows a workstation <b>40</b> coupled to the common data network <b>20</b>. File import operations involve the transfer of a digital file from a workstation <b>40</b> or other computer to an AVSC <b>120</b>, while file export operations involve the transfer of a digital file from an AVSC <b>120</b> to a workstation <b>40</b> or other computer. In response to a file import request received from a client application program, the AVSM <b>160</b> selects and allocates an available AVSC <b>120</b> having sufficient storage space, and issues a request to the AVSC <b>120</b> to retrieve the file from the workstation <b>40</b> or other computer from which the import request originated. Similarly, in response to a file export request specifying an A/V file, the AVSM <b>160</b> selects and allocates an AVSC <b>120</b> upon which a copy of the file resides, and issues a request to the AVSC <b>120</b> to transfer the file to the workstation <b>40</b> or other computer from which the export request originated. Upon completion of file transfer operations corresponding to an import or/and export request, the AVSC <b>120</b> issues a reply to the AVSM <b>160</b>.</p>
    <p>File publishing operations involve the transfer of a digital file from an AVSC <b>120</b> to a server. The server may be essentially any computer system external to the AVSS <b>100</b> in which the AVSC <b>120</b> resides, for example, an intranet server system <b>60</b> or a server coupled to the internet <b>80</b>. FIG. 14 also shows an intranet server system <b>60</b> coupled to the common data network <b>20</b>. In response to a publish file request that specifies a filename, a target file format, and a network or IP address of a server, the AVSM <b>160</b> selects and allocates an AVSC <b>120</b> upon which a copy of the file resides. The AVSM <b>160</b> then issues a publish request to the AVSC <b>120</b>, which performs any necessary file format conversion, and transfers the file to the specified server. Upon completing the transfer, the AVSC <b>120</b> issues a reply to the AVSM <b>160</b>.</p>
    <p>In general, an AVSC <b>120</b> can directly perform both A/V file encoding, decoding, and/or transcoding operations, as well as file transfer operations. Any particular AVSC <b>120</b> will have a maximum number of simultaneously-active file transfers it can perform while still delivering high-quality encoding, decoding, or transcoding performance. Thus, file transfer needs must be balanced against encoding, decoding, and/or transcoding needs. Typically, encoding, decoding, and/or transcoding operations are given higher priority than file transfer operations. In one embodiment, an administrative application program facilitates defining a first-priority and a second-priority AVSC bus and disk bandwidth partition. The first-priority bandwidth partition is reserved for performing encoding/decoding/transcoding operations, while the second-priority partition is reserved for file transfer operations. As file transfer requests are allocated to an AVSC <b>120</b>, it services them immediately using bandwidth within the second-priority partition. If the AVSC <b>120</b> receives a file transfer request that it cannot immediately service, the AVSC <b>120</b> places the request into the request queue within its state memory <b>154</b>. Requests in this queue are served as more bandwidth becomes available within the second-priority partition. In one embodiment, the request queue is implemented as a First-In, First-Out (FIFO) queue in a conventional manner. The request queue could alternatively be implemented as a prioritized queue, and/or as a queue from which requests are serviced in accordance with current resource availability. Alternatively, requests that cannot be immediately serviced could be terminated and a reply sent back indicating resources are not currently available thereby allowing the AVSM <b>160</b> and/or client application to look for some other means to service the request.</p>
    <p>4.7.4.7 File Replication</p>
    <p>An A/V file should be present upon multiple AVSCs <b>120</b> to ensure that the file can be independently accessed by a reasonably large number of users at any given time. Thus, after A/V file creation (i.e., after a new A/V file has been created and saved), or after the transfer of a new A/V fife onto one of the premises AVSCs <b>120</b>, the AVSM <b>160</b> initiates file replication operations. During the file replication operations, copies of the A/V file are stored on a plurality of premises AVSCs <b>120</b>. In one embodiment, copies of associated multimedia synchronization files are also replicated.</p>
    <p>The particular file replication strategy employed at any given time depends upon several factors, including the number of premises AVSCs <b>120</b> present; the performance capabilities of each AVSC <b>120</b>; AVSS internal network bandwidth; and/or current user demand for AVSC resources. In an AVSS <b>100</b> employing a single AVSC <b>120</b>, file replication may not be performed, or limited file replication may occur between separate storage devices on the AVSC <b>120</b>. For an AVSS having a small number of AVSCs <b>120</b>â€”for example, three AVSCs <b>120</b> serving approximately <b>100</b> usersâ€”full replication may be performed. In this case, the AVSM <b>160</b> automatically oversees the copying of a new A/V file to each AVSC <b>120</b> within the AVSS <b>100</b>. The copying operations are performed in the manner described above for cell-to-cell file transfer operations. In general, the A/V file is not replicated to a non-premises AVSC <b>120</b> in the absence of an application-level request to do so.</p>
    <p>Depending upon the performance capabilities of each AVSC <b>120</b>, as well as the number of premises AVSCs present, full replication may significantly limit AVSC availability for performing other operations. Thus, for an AVSS <b>100</b> employing a medium to somewhat large number of premises AVSCs <b>120</b>â€”for example, five AVSCs <b>120</b> serving a few hundred to several hundred usersâ€”the AVSM <b>160</b> manages partial file replication operations. During the partial file replication operations, the AVSM <b>160</b> oversees A/V file copying to a subset of the premises AVSCs <b>120</b>, where the copying operations are performed in the manner described above for cell-to-cell file transfer. The A/V file is not generally replicated to a non-premises AVSC unless an application program issues a request requiring such replication. The selection of a particular AVSC <b>120</b> as a member of the aforementioned subset may be based upon the number of A/V files, available file storage space, or the number of decoders and transcoders residing upon the AVSC <b>120</b>.</p>
    <p>At some point, partial replication becomes untenable as the number of premises AVSCs <b>120</b> within the AVSS <b>100</b> becomes very large. In such situations, a subset of the AVSCs <b>120</b> are utilized as streaming A/V file servers, and the other AVSCs function as staging points for encoding, decoding, and/or transcoding operations. In one embodiment, the AVSM <b>160</b> manages file replication operations for the streaming A/V file servers in a manner analogous to that described above.</p>
    <p>4.7.4.8 File Renaming</p>
    <p>In response to a name change request, the AVSM <b>160</b> changes an A/V file's name as specified in the corresponding file parameter table, and subsequently oversees modification propagation operations as detailed below to change the name of each copy of the A/V file stored across the premises AVSCs <b>120</b>. In one embodiment, the AVSM <b>160</b> permits a name change only if requested by an application program corresponding to a super-user. The AVSM <b>160</b> may further permit a name change request for the A/V file's author in the event that the author is currently the sole A/V file owner.</p>
    <p>4.7.4.9 File Deletion</p>
    <p>In response to a file deletion request, the AVSM <b>160</b> directs the deletion of each copy of the A/V file from the premises AVSCs <b>120</b> by issuing a delete request to each AVSC <b>120</b> upon which a copy of the A/V file resides. In one embodiment, the AVSM <b>160</b> permits file deletion only if requested by an application program associated with a super-user. The AVSM <b>160</b> may further permit A/V file deletion by the A/V file's author in the event that the author is currently the sole A/V file owner. In one embodiment, the AVSM <b>160</b> additionally deletes or oversees the deletion of any multimedia synchronization files associated with the deleted A/V file. Additionally, the AVSM <b>160</b> may issue deletion requests to non-premises AVSM <b>160</b> and servers <b>60</b> and <b>80</b> to which it has transferred the A/V file.</p>
    <p>4.7.4.10 Modification Propagation</p>
    <p>As previously indicated, multiple copies of any given A/V file may reside upon the premises AVSCs <b>120</b>. The AVSM <b>160</b> may permit a given user, such as a super-user or an A/V file's author, to edit or change the file's name or contents after its creation. Each premises copy of the A/V file must be updated to reflect such modification. Thus, in one embodiment, after an A/V filename or contents change, the AVSM <b>160</b> issues a sequence of copy-to requests to the AVSC <b>120</b> upon which the modified file resides to ensure name and contents consistency between the A/V file copies stored on the AVSCs <b>120</b>. In an alternate embodiment, the AVSM <b>160</b> could issue copy-from requests. Additionally, the AVSM <b>160</b> may issue file modification requests to non-premises AVSM <b>160</b> and servers <b>60</b> and <b>80</b> to which it has transferred the A/V file.</p>
    <p>4.7.4.11 Expiration Operations</p>
    <p>An A/V file's access privilege list includes for each user ID an expiration date and/or time past which the user ID expires and is no longer valid for obtaining file access. For each A/V file parameter table in the AVSS database, the AVSM <b>160</b> determines whether any user IDs specified in the access privilege list have expired. If so, the AVSM <b>160</b> removes the expired user IDs from the access privilege list. Once no unexpired user IDs remain in the access privilege list, the AVSM <b>160</b> issues a file deletion request to each AVSC <b>120</b> upon which a copy of the A/V file resides. After each such AVSC <b>120</b> has deleted the A/V file, the AVSM <b>160</b> deletes the A/V file parameter table from the AVSS database. Alternatively, the AVSM <b>160</b> may log expired files and require deletion by an application program associated with a super-user.</p>
    <p>4.7.4.12 Database Consistency Operations</p>
    <p>The AVSM <b>160</b> periodically performs consistency checks to ensure that the file parameter tables within the AVSS database <b>136</b> accurately reflect the current premises AVSC file environment. In one embodiment, the AVSM <b>160</b> periodically queries each premises AVSC <b>120</b> to obtain data for each A/V file stored thereupon, where such data may include an A/V file's name, size, age, encoding format, storage location or address, or other information. In the event that an AVSC <b>120</b> returns a filename that is not specified in the AVSS database, the AVSM <b>160</b> generates a file parameter table corresponding to the filename, and adds it to the AVSS database. If no AVSC <b>120</b> returns a particular filename specified in the AVSS database, the AVSM <b>160</b> deletes the corresponding file parameter table from the AVSS database. If the file data returned by an AVSC <b>120</b> indicate a discrepancy in file size, age, encoding format, or other information, the AVSM <b>160</b> may update the AVSS database accordingly. The AVSM <b>160</b> may additionally log any discrepancies, as well as the type of action(s) undertaken in response to each discrepancy. Finally, if a particular AVSC <b>120</b> returns a filename specified in the AVSS database, but for which the file parameter table indicates that the file is not stored on this AVSC <b>120</b>, the AVSM <b>160</b> updates the file parameter table with the storage location of the A/V file upon the AVSC <b>120</b> in question.</p>
    <p>4.7.5 Device Management Services</p>
    <p>The AVSM <b>160</b> additionally performs device management services, which include AVSC scheduling and allocation services. AVSS resource scheduling and allocation is subject to several:</p>
    <p>interacting resource constraints, which include the following:</p>
    <p>1. AVSM multiple-thread resources (i.e., simultaneous use by multiple sessions):</p>
    <p>a. CPU capacity:</p>
    <p>i. encode session management</p>
    <p>ii. decode session management</p>
    <p>iii. file transfer session management</p>
    <p>b. Bus capacity</p>
    <p>c. Networking port capacity</p>
    <p>2. AVSC multiple-thread resources (i.e., simultaneous use by multiple sessions):</p>
    <p>a. CPU capacity:</p>
    <p>i. encode session management</p>
    <p>ii. decode session management</p>
    <p>iii. file transfer session management</p>
    <p>iv. stream session management</p>
    <p>v. audio-video stream multiplexing processes</p>
    <p>vi. audio-video stream demultiplexing processes</p>
    <p>vii. encode session processes</p>
    <p>viii. decode session processes</p>
    <p>ix. file transfer processes</p>
    <p>x. send stream processes</p>
    <p>xi. receive stream processes</p>
    <p>b. Bus capacity</p>
    <p>c. Disk controller/disk bus (e.g. SCSI) capacity</p>
    <p>d. Disk read/write/seek capacity</p>
    <p>e. Networking port capacity</p>
    <p>3. AVSC single-threaded resources (i.e., can be used by only one session at a time):</p>
    <p>a. encoder hardware and corresponding encode session(s)</p>
    <p>b. decoder hardware and corresponding decode session(s)</p>
    <p>Any type of session has a specific number of specific resources required to execute the session. Some embodiments may grant service requests without regard to details of the current utilization of the resources; however, this may result in underutilization of the resources or in unacceptable performance shortcomings or failure modes if the resource capacities are exceeded. One embodiment would base request acceptance on the needs of the request in view of the available unused capacity of the system. An example of how this can be accomplished is described hereafter.</p>
    <p>In some cases, resource requirements may be exact. In other cases, resource requirements may be in the form of upper bounds, estimated averages, or averages biased upwards to provide safety margins. These session-type resource usage estimates may or may not include additional â€œpaddingâ€ to allow for processing overhead (task switching, paging, etc.). In the absence of padding in the session-type resource usage estimates, overhead can be provide for by either</p>
    <p>reducing the maximum value of available resource to a value that safely accounts for worse-case incurred overhead at full operating capacity, or</p>
    <p>introducing a specific linear or nonlinear overhead function dependent on the number of various sessions active or projected within the AVSS <b>100</b>.</p>
    <p>Some of the resource usage estimates could be firm fixed values; for example:</p>
    <p>each â€œencodeâ€ session could be designed to require a fully dedicated encoder hardware subsystem;</p>
    <p>each â€œencodeâ€ session could be designed to allocate a fixed block of disk memory (said size dependent on the application) and attempts to record beyond this would be prevented; or</p>
    <p>every â€œdecodeâ€ session will have knowledge of the exact the file size</p>
    <p>It may be more likely, however, that resource usage estimates would not be firm; for example:</p>
    <p>all active encode sessions could share a slightly smaller pool of encoders; and</p>
    <p>all encode session requirements could be handled with averages and attempts to record beyond this would be allowed.</p>
    <p>To further illustrate an example of resource allocation process, the following table provides an example framework for resource requirements for several different types of sessions. Other embodiments of course may have different session types and resource requirements.</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" pgwide="1" class="description-table"> <tgroup align="left" colsep="0" rowsep="0" cols="2"> <colspec colname="offset" colwidth="126pt" align="left"> </colspec> <colspec colname="1" colwidth="168pt" align="center"> </colspec> <tbody valign="top"> <tr class="description-tr"> <td class="description-td"> </td>
                <td namest="offset" nameend="1" align="center" rowsep="1" class="description-td" colspan="2"> </td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">Type of Session</td>
              </tr> </tbody> </tgroup> <tgroup align="left" colsep="0" rowsep="0" cols="7"> <colspec colname="1" colwidth="126pt" align="left"> </colspec> <colspec colname="2" colwidth="28pt" align="left"> </colspec> <colspec colname="3" colwidth="28pt" align="left"> </colspec> <colspec colname="4" colwidth="28pt" align="left"> </colspec> <colspec colname="5" colwidth="28pt" align="left"> </colspec> <colspec colname="6" colwidth="28pt" align="left"> </colspec> <colspec colname="7" colwidth="28pt" align="left"> </colspec> <tbody valign="top"> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">Put</td>
                <td class="description-td">Get</td>
                <td class="description-td">Send</td>
                <td class="description-td">Receive</td>
              </tr> <tr class="description-tr"> <td class="description-td">Type of Resource</td>
                <td class="description-td">Encode</td>
                <td class="description-td">Decode</td>
                <td class="description-td">File</td>
                <td class="description-td">File</td>
                <td class="description-td">Stream</td>
                <td class="description-td">Stream</td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="7" align="center" rowsep="1" class="description-td" colspan="7"> </td>
              </tr> </tbody> </tgroup> <tgroup align="left" colsep="0" rowsep="0" cols="8"> <colspec colname="1" colwidth="35pt" align="left"> </colspec> <colspec colname="2" colwidth="91pt" align="left"> </colspec> <colspec colname="3" colwidth="28pt" align="left"> </colspec> <colspec colname="4" colwidth="28pt" align="left"> </colspec> <colspec colname="5" colwidth="28pt" align="left"> </colspec> <colspec colname="6" colwidth="28pt" align="left"> </colspec> <colspec colname="7" colwidth="28pt" align="left"> </colspec> <colspec colname="8" colwidth="28pt" align="left"> </colspec> <tbody valign="top"> <tr class="description-tr"> <td class="description-td">AVSM</td>
                <td class="description-td">Encode session management</td>
                <td class="description-td">M<sub>SM</sub> <sup>E</sup> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
              </tr> <tr class="description-tr"> <td class="description-td">CPU</td>
                <td class="description-td">Decode session management</td>
                <td class="description-td"> </td>
                <td class="description-td">M<sub>SM</sub> <sup>D</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td">capacity</td>
                <td class="description-td">file xfr session management</td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">M<sub>SM</sub> <sup>PF</sup> </td>
                <td class="description-td">M<sub>SM</sub> <sup>GF</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">stream session management</td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">M<sub>SM</sub> <sup>SS</sup> </td>
                <td class="description-td">M<sub>SM</sub> <sup>RS</sup> </td>
              </tr> </tbody> </tgroup> <tgroup align="left" colsep="0" rowsep="0" cols="7"> <colspec colname="1" colwidth="126pt" align="left"> </colspec> <colspec colname="2" colwidth="28pt" align="left"> </colspec> <colspec colname="3" colwidth="28pt" align="left"> </colspec> <colspec colname="4" colwidth="28pt" align="left"> </colspec> <colspec colname="5" colwidth="28pt" align="left"> </colspec> <colspec colname="6" colwidth="28pt" align="left"> </colspec> <colspec colname="7" colwidth="28pt" align="left"> </colspec> <tbody valign="top"> <tr class="description-tr"> <td class="description-td">AVSM Bus capacity</td>
                <td class="description-td">M<sub>B</sub> <sup>E</sup> </td>
                <td class="description-td">M<sub>B</sub> <sup>D</sup> </td>
                <td class="description-td">M<sub>B</sub> <sup>PF</sup> </td>
                <td class="description-td">M<sub>B</sub> <sup>GF</sup> </td>
                <td class="description-td">M<sub>B</sub> <sup>SS</sup> </td>
                <td class="description-td">M<sub>B</sub> <sup>RS</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td">AVSM Networking port capacity</td>
                <td class="description-td">M<sub>N</sub> <sup>E</sup> </td>
                <td class="description-td">M<sub>N</sub> <sup>D</sup> </td>
                <td class="description-td">M<sub>N</sub> <sup>PF</sup> </td>
                <td class="description-td">M<sub>N</sub> <sup>GF</sup> </td>
                <td class="description-td">M<sub>N</sub> <sup>SS</sup> </td>
                <td class="description-td">M<sub>N</sub> <sup>RS</sup> </td>
              </tr> </tbody> </tgroup> <tgroup align="left" colsep="0" rowsep="0" cols="8"> <colspec colname="1" colwidth="35pt" align="left"> </colspec> <colspec colname="2" colwidth="91pt" align="left"> </colspec> <colspec colname="3" colwidth="28pt" align="left"> </colspec> <colspec colname="4" colwidth="28pt" align="left"> </colspec> <colspec colname="5" colwidth="28pt" align="left"> </colspec> <colspec colname="6" colwidth="28pt" align="left"> </colspec> <colspec colname="7" colwidth="28pt" align="left"> </colspec> <colspec colname="8" colwidth="28pt" align="left"> </colspec> <tbody valign="top"> <tr class="description-tr"> <td class="description-td">AVSC</td>
                <td class="description-td">encode session management</td>
                <td class="description-td">C<sub>SM</sub> <sup>E</sup> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
              </tr> <tr class="description-tr"> <td class="description-td">CPU</td>
                <td class="description-td">decode session management</td>
                <td class="description-td"> </td>
                <td class="description-td">C<sub>SM</sub> <sup>D</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td">capacity</td>
                <td class="description-td">file xfr session management</td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">C<sub>SM</sub> <sup>PF</sup> </td>
                <td class="description-td">C<sub>SM</sub> <sup>GF</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">stream session management</td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">C<sub>SM</sub> <sup>SS</sup> </td>
                <td class="description-td">C<sub>SM</sub> <sup>RS</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">multiplexing processes</td>
                <td class="description-td">C<sub>MP</sub> <sup>E</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">demultiplexing processes</td>
                <td class="description-td"> </td>
                <td class="description-td">C<sub>MP</sub> <sup>D</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">encode session processes</td>
                <td class="description-td">C<sub>CP</sub> <sup>E</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">decode session processes</td>
                <td class="description-td"> </td>
                <td class="description-td">C<sub>CP</sub> <sup>D</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">file transfer processes</td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">C<sub>TP</sub> <sup>PF</sup> </td>
                <td class="description-td">C<sub>TP</sub> <sup>GF</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">send stream processes</td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">C<sub>SP</sub> <sup>SS</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">receive stream processes</td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td"> </td>
                <td class="description-td">C<sub>SP</sub> <sup>RS</sup> </td>
              </tr> </tbody> </tgroup> <tgroup align="left" colsep="0" rowsep="0" cols="7"> <colspec colname="1" colwidth="126pt" align="left"> </colspec> <colspec colname="2" colwidth="28pt" align="left"> </colspec> <colspec colname="3" colwidth="28pt" align="left"> </colspec> <colspec colname="4" colwidth="28pt" align="left"> </colspec> <colspec colname="5" colwidth="28pt" align="left"> </colspec> <colspec colname="6" colwidth="28pt" align="left"> </colspec> <colspec colname="7" colwidth="28pt" align="left"> </colspec> <tbody valign="top"> <tr class="description-tr"> <td class="description-td">AVSC Bus capacity</td>
                <td class="description-td">C<sub>B</sub> <sup>E</sup> </td>
                <td class="description-td">C<sub>B</sub> <sup>D</sup> </td>
                <td class="description-td">C<sub>B</sub> <sup>PF</sup> </td>
                <td class="description-td">C<sub>B</sub> <sup>GF</sup> </td>
                <td class="description-td">C<sub>B</sub> <sup>SS</sup> </td>
                <td class="description-td">C<sub>B</sub> <sup>RS</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td">AVSC Disk controller/disk bus capacity</td>
                <td class="description-td">C<sub>DC</sub> <sup>E</sup> </td>
                <td class="description-td">C<sub>DC</sub> <sup>D</sup> </td>
                <td class="description-td">C<sub>DC</sub> <sup>PF</sup> </td>
                <td class="description-td">C<sub>DC</sub> <sup>GF</sup> </td>
                <td class="description-td">C<sub>DC</sub> <sup>SS</sup> </td>
                <td class="description-td">C<sub>DC</sub> <sup>RS</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td">AVSC Disk read/write/seek capacity</td>
                <td class="description-td">C<sub>RW</sub> <sup>E</sup> </td>
                <td class="description-td">C<sub>RW</sub> <sup>D</sup> </td>
                <td class="description-td">C<sub>RW</sub> <sup>PF</sup> </td>
                <td class="description-td">C<sub>RW</sub> <sup>GF</sup> </td>
                <td class="description-td">C<sub>RW</sub> <sup>SS</sup> </td>
                <td class="description-td">C<sub>RW</sub> <sup>RS</sup> </td>
              </tr> <tr class="description-tr"> <td class="description-td">AVSC Networking port capacity</td>
                <td class="description-td">C<sub>N</sub> <sup>E</sup> </td>
                <td class="description-td">C<sub>N</sub> <sup>D</sup> </td>
                <td class="description-td">C<sub>N</sub> <sup>PF</sup> </td>
                <td class="description-td">C<sub>N</sub> <sup>GF</sup> </td>
                <td class="description-td">C<sub>N</sub> <sup>SS</sup> </td>
                <td class="description-td">C<sub>N</sub> <sup>RS</sup> </td>
              </tr> </tbody> </tgroup> <tgroup align="left" colsep="0" rowsep="0" cols="4"> <colspec colname="1" colwidth="35pt" align="left"> </colspec> <colspec colname="2" colwidth="91pt" align="left"> </colspec> <colspec colname="3" colwidth="28pt" align="left"> </colspec> <colspec colname="4" colwidth="140pt" align="left"> </colspec> <tbody valign="top"> <tr class="description-tr"> <td class="description-td">AVSC</td>
                <td class="description-td">encoder hardware</td>
                <td class="description-td">C<sub>EH</sub> <sup>E</sup> </td>
                <td class="description-td"> </td>
              </tr> <tr class="description-tr"> <td class="description-td">hardware</td>
                <td class="description-td">decoder hardware</td>
                <td class="description-td"> </td>
                <td class="description-td">C<sub>DH</sub> <sup>D</sup> </td>
              </tr> <tr class="description-tr"> <td namest="1" nameend="4" align="center" rowsep="1" class="description-td" colspan="4"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>Each type of resource, or group of resources, has a finite maximum performance value that can be provided by the AVSM <b>160</b> or AVSC <b>120</b> environments within the AVSS <b>100</b>.</p>
    <p>This finite maximum value will be termed a â€œcapacity bound.â€</p>
    <p>M<sub>SM</sub> <sup>MAX</sup>=AVSM CPU session management maximum capacity</p>
    <p>M<sub>B</sub> <sup>MAX</sup>=AVSM Bus maximum capacity</p>
    <p>M<sub>N</sub> <sup>MAX</sup>=AVSM Networking port maximum capacity</p>
    <p>C<sub>SM</sub> <sup>MAX</sup>=AVSC CPU session management maximum capacity</p>
    <p>C<sub>P</sub> <sup>MAX</sup>=AVSC CPU process maximum capacity</p>
    <p>C<sub>B</sub> <sup>MAX</sup>=AVSC Bus maximum capacity</p>
    <p>C<sub>DC</sub> <sup>MAX</sup>=AVSC Disk Controller/disk bus maximum capacity</p>
    <p>C<sub>RW</sub> <sup>MAX</sup>=AVSC Disk read/write/seek maximum capacity</p>
    <p>C<sub>N</sub> <sup>MAX</sup>=AVSC Networking port maximum capacity</p>
    <p>C<sub>EH</sub> <sup>MAX</sup>=AVSC Encoder hardware maximum capacity</p>
    <p>C<sub>DH</sub> <sup>MAX</sup>=AVSC Decoder hardware maximum capacity</p>
    <p>The amount of resource required by a potential combination of granted sessions would be determined, in many cases, by the sum of the total allocated resource estimates. Overhead can be included, for example, via the incorporation of a plurality of overhead constants that include the following:</p>
    <p>M<sub>SM</sub> <sup>OH</sup>=AVSM CPU session management overhead</p>
    <p>M<sub>B</sub> <sup>OH</sup>=AVSM Bus overhead</p>
    <p>M<sub>N</sub> <sup>OH</sup>=SM Networking port overhead</p>
    <p>C<sub>SM</sub> <sup>OH</sup>=AVSC CPU session management overhead</p>
    <p>C<sub>P</sub> <sup>OH</sup>=AVSC CPU process overhead</p>
    <p>C<sub>B</sub> <sup>OH</sup>=AVSC Bus overhead</p>
    <p>C<sub>DC</sub> <sup>OH</sup>=AVSC Disk Controller/disk bus overhead</p>
    <p>C<sub>RW</sub> <sup>OH</sup>=AVSC Disk read/write/seek overhead</p>
    <p>C<sub>N</sub> <sup>OH</sup>=AVSC Networking port overhead</p>
    <p>Overhead may also be treated as a linear function of the number of various session types. Incorporation of overhead results in the supportable operating states forming a lattice of points within a convex hull defined by the intersection of hyperplanes defined by the resource capacity bound inequalities. For example, with:</p>
    <p>
      <tables> <table frame="none" colsep="0" rowsep="0" class="description-table"> <tgroup align="left" colsep="0" rowsep="0" cols="3"> <colspec colname="offset" colwidth="35pt" align="left"> </colspec> <colspec colname="1" colwidth="42pt" align="left"> </colspec> <colspec colname="2" colwidth="140pt" align="left"> </colspec> <thead> <tr class="description-tr"> <td class="description-td"> </td>
                <td namest="offset" nameend="2" align="center" rowsep="1" class="description-td" colspan="3"> </td>
              </tr> </thead> <tbody valign="top"> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">n<sup>E </sup>=</td>
                <td class="description-td">number of encoding sessions</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">n<sup>D </sup>=</td>
                <td class="description-td">number of decoding sessions</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">n<sup>PF </sup>=</td>
                <td class="description-td">number of put file transfer sessions</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">n<sup>GF </sup>=</td>
                <td class="description-td">number of get file transfer sessions</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">n<sup>SS </sup>=</td>
                <td class="description-td">number send streaming sessions</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td class="description-td">n<sup>RS </sup>=</td>
                <td class="description-td">number of receive streaming sessions</td>
              </tr> <tr class="description-tr"> <td class="description-td"> </td>
                <td namest="offset" nameend="2" align="center" rowsep="1" class="description-td" colspan="3"> </td>
              </tr> </tbody> </tgroup> </table> </tables> </p>
    <p>then the supportable operating states in an implementation could be given by the following constraints:</p>
    <p>For each AVSM <b>160</b> (typically one per AVSS <b>100</b>),</p>
    <p>AVSM CPU capacity (session management)</p>
    <p>
      <maths> <formula-text>[<i>n</i> <sup>E</sup> <i>M</i> <sub>SM</sub> <sup>E</sup> <i>+n</i> <sup>D</sup> <i>M</i> <sub>SM</sub> <sup>D</sup> <i>+n</i> <sup>PF</sup> <i>M</i> <sub>SM</sub> <sup>PF</sup> <i>+n</i> <sup>GF</sup> <i>M</i> <sub>SM</sub> <sup>GF</sup> <i>+n</i> <sup>SS</sup> <i>M</i> <sub>SM</sub> <sup>SS</sup> <i>+n</i> <sup>RS</sup> <i>M</i> <sub>SM</sub> <sup>RS</sup> <i>]+M</i> <sub>SM</sub> <sup>OH</sup> <i>â‰¦M</i> <sub>SM</sub> <sup>MAX</sup> </formula-text> </maths> </p>
    <p>AVSM bus capacity:</p>
    <p>
      <maths> <formula-text>[<i>n</i> <sup>E</sup> <i>M</i> <sub>B</sub> <sup>E</sup> <i>+n</i> <sup>D</sup> <i>M</i> <sub>B</sub> <sup>D</sup> <i>+n</i> <sup>PF</sup> <i>M</i> <sub>B</sub> <sup>PF</sup> <i>+n</i> <sup>GF</sup> <i>M</i> <sub>B</sub> <sup>GF</sup> <i>+n</i> <sup>SS</sup> <i>M</i> <sub>B</sub> <sup>SS</sup> <i>+n</i> <sup>RS</sup> <i>M</i> <sub>B</sub> <sup>RS</sup> <i>]+M</i> <sub>B</sub> <sup>OH</sup> <i>â‰¦M</i> <sub>B</sub> <sup>MAX</sup> </formula-text> </maths> </p>
    <p>AVSM Networking port capacity:</p>
    <p>
      <maths> <formula-text>[<i>n</i> <sup>E</sup> <i>M</i> <sub>N</sub> <sup>E</sup> <i>+n</i> <sup>D</sup> <i>M</i> <sub>N</sub> <sup>D</sup> <i>+n</i> <sup>PF</sup> <i>M</i> <sub>N</sub> <sup>PF</sup> <i>+n</i> <sup>GF</sup> <i>M</i> <sub>N</sub> <sup>GF</sup> <i>+n</i> <sup>SS</sup> <i>M</i> <sub>N</sub> <sup>SS</sup> <i>+n</i> <sup>RS</sup> <i>M</i> <sub>N</sub> <sup>RS</sup> <i>]+M</i> <sub>N</sub> <sup>OH</sup> <i>â‰¦M</i> <sub>M</sub> <sup>MAX</sup> </formula-text> </maths> </p>
    <p>For each AVSC <b>120</b> (at least one, typically multiple, per AVSS <b>100</b>):</p>
    <p>AVSC CPU capacity (session management):</p>
    <p>
      <maths> <formula-text>[<i>n</i> <sup>E</sup> <i>C</i> <sub>SM</sub> <sup>E</sup> <i>+n</i> <sup>D</sup> <i>C</i> <sub>SM</sub> <sup>D</sup> <i>+n</i> <sup>PF</sup> <i>C</i> <sub>SM</sub> <sup>PF</sup> <i>+n</i> <sup>GF</sup> <i>C</i> <sub>SM</sub> <sup>GF</sup> <i>+n</i> <sup>SS</sup> <i>C</i> <sub>SM</sub> <sup>SS</sup> <i>+n</i> <sup>RS</sup> <i>C</i> <sub>SM</sub> <sup>RS</sup> <i>]+C</i> <sub>SM</sub> <sup>OH</sup> <i>â‰¦C</i> <sub>SM</sub> <sup>MAX</sup> </formula-text> </maths> </p>
    <p>AVSC CPU processes capacity:</p>
    <p>
      <maths> <formula-text>[<i>n</i> <sup>E</sup> <i>C</i> <sub>MP</sub> <sup>E</sup> <i>+n</i> <sup>D</sup> <i>C</i> <sub>MP</sub> <sup>D</sup> <i>+n</i> <sup>E</sup> <i>C</i> <sub>CP</sub> <sup>E</sup> <i>+n</i> <sup>D</sup> <i>C</i> <sub>CP</sub> <sup>D</sup> <i>+n</i> <sup>PF</sup> <i>C</i> <sub>TP</sub> <sup>PF</sup> <i>+n</i> <sup>GF</sup> <i>C</i> <sub>TP</sub> <sup>GF</sup> <i>+n</i> <sup>SS</sup> <i>C</i> <sub>SP</sub> <sup>SS</sup> <i>+n</i> <sup>RS</sup> <i>C</i> <sub>SP</sub> <sup>RS</sup>]</formula-text> </maths> </p>
    <p>â€ƒ+<i>C</i> <sup>OH</sup> <i>â‰¦C</i> <sub>CP</sub> <sup>MAX</sup> </p>
    <p>AVSC Bus capacity:</p>
    <p>
      <maths> <formula-text>[<i>n</i> <sup>E</sup> <i>C</i> <sub>B</sub> <sup>E</sup> <i>+n</i> <sup>D</sup> <i>C</i> <sub>B</sub> <sup>D</sup> <i>+n</i> <sup>PF</sup> <i>C</i> <sub>B</sub> <sup>PF</sup> <i>+n</i> <sup>GF</sup> <i>C</i> <sub>B</sub> <sup>GF</sup> <i>+n</i> <sup>SS</sup> <i>C</i> <sub>B</sub> <sup>SS</sup> <i>+n</i> <sup>RS</sup> <i>C</i> <sub>B</sub> <sup>RS</sup> <i>]+C</i> <sub>B</sub> <sup>OH</sup> <i>â‰¦C</i> <sub>B</sub> <sup>MAX</sup> </formula-text> </maths> </p>
    <p>AVSC Disk controller/disk bus capacity:</p>
    <p>
      <maths> <formula-text>[<i>n</i> <sup>E</sup> <i>C</i> <sub>DC</sub> <sup>E</sup> <i>+n</i> <sup>D</sup> <i>C</i> <sub>DC</sub> <sup>D</sup> <i>+n</i> <sup>PF</sup> <i>C</i> <sub>DC</sub> <sup>PF</sup> <i>+n</i> <sup>GF</sup> <i>C</i> <sub>DC</sub> <sup>GF</sup> <i>+n</i> <sup>SS</sup> <i>C</i> <sub>DC</sub> <sup>SS</sup> <i>+n</i> <sup>RS</sup> <i>C</i> <sub>DC</sub> <sup>RS</sup> <i>]+C</i> <sub>DC</sub> <sup>OH</sup> <i>â‰¦C</i> <sub>DC</sub> <sup>MAX</sup> </formula-text> </maths> </p>
    <p>AVSC Disk read/write/seek capacity:</p>
    <p>
      <maths> <formula-text>[<i>n</i> <sup>E</sup> <i>C</i> <sub>RW</sub> <sup>E</sup> <i>+n</i> <sup>D</sup> <i>C</i> <sub>RW</sub> <sup>D</sup> <i>+n</i> <sup>PF</sup> <i>C</i> <sub>RW</sub> <sup>PF</sup> <i>+n</i> <sup>GF</sup> <i>C</i> <sub>RW</sub> <sup>GF</sup> <i>+n</i> <sup>SS</sup> <i>C</i> <sub>RW</sub> <sup>SS</sup> <i>+n</i> <sup>RS</sup> <i>C</i> <sub>RW</sub> <sup>RS</sup> <i>]+C</i> <sub>RW</sub> <sup>OH</sup> <i>â‰¦C</i> <sub>RW</sub> <sup>MAX</sup> </formula-text> </maths> </p>
    <p>AVSC Networking port capacity:</p>
    <p>
      <maths> <formula-text>[<i>n</i> <sup>E</sup> <i>C</i> <sub>N</sub> <sup>E</sup> <i>+n</i> <sup>D</sup> <i>C</i> <sub>N</sub> <sup>D</sup> <i>+n</i> <sup>PF</sup> <i>C</i> <sub>N</sub> <sup>PF</sup> <i>+n</i> <sup>GF</sup> <i>C</i> <sub>N</sub> <sup>GF</sup> <i>+n</i> <sup>SS</sup> <i>C</i> <sub>N</sub> <sup>SS</sup> <i>+n</i> <sup>RS</sup> <i>C</i> <sub>N</sub> <sup>RS</sup> <i>]+C</i> <sub>N</sub> <sup>OH</sup> <i>â‰¦C</i> <sub>N</sub> <sup>MAX</sup> </formula-text> </maths> </p>
    <p>AVSC hardware:</p>
    <p>
      <maths> <formula-text>[<i>n</i> <sup>E</sup> <i>C</i> <sub>EH</sub> <sup>E</sup> <i>]â‰¦C</i> <sub>EH</sub> <sup>MAX</sup> </formula-text> </maths> </p>
    <p>
      <maths> <formula-text>[<i>n</i> <sup>D</sup> <i>C</i> <sub>DH</sub> <sup>D</sup> <i>]â‰¦C</i> <sub>DH</sub> <sup>MAX</sup> </formula-text> </maths> </p>
    <p>Here it is assumed that each encode session is represented with either a fixed or an average amount of disk capacity. Depending upon design choices, one embodiment may enforce a per-session file size limit, while another embodiment may permit file size overruns.</p>
    <p>Upon receiving one or more requests for services, the AVSM <b>160</b> would, for example, perform the following evaluations:</p>
    <p>serialize any multiple pending requests so the following evaluations are done fully separately in sequence for each pending request;</p>
    <p>check if accepting the new request would not exceed the AVSM's own capacity requirements; and</p>
    <p>check all AVSCs <b>120</b> within that AVSS <b>100</b> to see if at least one AVSC <b>120</b> could accept the new request without exceeding the AVSM's own capacity requirements (this could be done via either the AVSM's own image of the AVSC states or by polling at least one AVSC <b>120</b>).</p>
    <p>If more than one AVSC <b>120</b> can support the request:</p>
    <p>select one according to a conventional procedure such as first available, round robin, or operating lifetime;</p>
    <p>allocate the session to the AVSC <b>120</b>, ensuring directly or indirectly notification of the requesting application and the invocation of appropriate network connections; or</p>
    <p>otherwise, deny the request.</p>
    <p>Clearly there are many other possible implementations. The range of possible implementations could also include methods for ensuring that no single type of application or type of session monopolizes the AVSS <b>100</b> at the exclusion of other types of applications or sessions. In the previous illustrative example, this could be accomplished by adding further inequalities to the list above. For example, the total number of file transfer sessions can be limited by adding the constraint.</p>
    <p>
      <maths> <formula-text> <i>n</i>
          <sup>PF</sup> <i>+n</i>
          <sup>GF</sup> <i>â‰¦n</i>
          <sup>F:MAX</sup> </formula-text> </maths> </p>
    <p>whered n<sup>F:MAX </sup>is the maximum number of simultaneous file transfers allowed.</p>
    <p>The device management services additionally include AVSC validation operations, through which the AVSM <b>160</b> queries each AVSC <b>120</b> to determine the number, type, and capabilities of the AVSC's storage, encoding, decoding, and transcoding resources. The AVSM <b>160</b> uses the query results to update the AVSS database if needed, and further logs any discrepancies as well as the types of operations undertaken in response.</p>
    <p>4.7.6 Administrative Services</p>
    <p>The AVSM <b>160</b> also performs or manages the performance of administrative services that include system performance and utilization monitoring; system diagnostics; event capture operations; billing operations; password maintenance operations; and user maintenance operations.</p>
    <heading>4.8 Software Architecture and System Interface</heading> <p>4.8.1 Communication Class Hierarchy</p>
    <p>In the present invention, the manners in which AVSS-external and AVSS-internal clients may access AVSM, AVSC, and A/V network manager functionality are defined by a set of software communication class hierarchies. FIG. 15 is a block diagram showing a high-level AVSS communication class hierarchy <b>200</b> for one embodiment of the present invention. As shown in FIG. 15, the base class for this hierarchy is referred to as â€œComm,â€ which serves as a basis for AVSS-external and AVSS-internal client communication. The Comm class also serves as a basis for external and internal A/V network manager client communication, and is described in detail in U.S. Pat. No. 5,617,539.</p>
    <p>Several subclasses derived from the Comm class encapsulate particular aspects of AVSS functionality for AVSS-external and AVSS-internal client access. These subclasses include the following:</p>
    <p>1) AvnmCommâ€”the AvnmComm class provides access to functionality supported through its parent class, as well as client access to A/V network manager services.</p>
    <p>1) AvsCommâ€”the AvsComm class provides access to AVSS functionality that includes login authorization; A/V encoding, decoding, and transcoding; file transfer between AVSCs <b>120</b>; file importing and exporting; file and directory management; and AVSC configuration and status reporting.</p>
    <p>2) AvscCommâ€”the AvscComm class provides access to the functionality supported through its parent class, and in addition provides client and server access to additional AVSC-related services as detailed below.</p>
    <p>3) AvscAppCommâ€”the AvscAppComm class provides a client interface for accessing file import and export services.</p>
    <p>4) AvsmCommâ€”the AvsmComm class provides access to the functionality supported through its parent class, and in addition provides client and server access to the AVSM file and device management services described above.</p>
    <p>5) AvsmAppCommâ€”the AvsmAppComm class provides event handlers that support client application program access to AVSS services.</p>
    <p>6) AvsmAdminCommâ€”the AvsmAdminComm class provides AVSS-external and AVSS-internal clients access to AVSS administrative functionality.</p>
    <p>4.8.2 AVSM Class Relationship</p>
    <p>The classes of AVSM objects resident within the AVSM object memory <b>172</b> have a hierarchical class relationship. FIG. 16 is a block diagram of an AVSM class relationship <b>250</b> organized in accordance with one embodiment of the present invention. The AVSMManager class comprises comprises a container class for supporting session establishment, session management, and session termination operations. The AVSMManager class further includes parameters and methods for facilitating AVSM operation, i.e., implementing the AVSM <b>160</b>. The AVSMManager class also contains parameters specifying AVSS operational characteristics, such as AVSC session limits. plus port and connection information for premises AVSCs <b>120</b>, the A/V network manager <b>34</b>, and known non-premises AVSSs <b>100</b>. In one embodiment, the AVSMManager class includes methods that facilitate communication between lower-level classes within the AVSM class hierarchy <b>250</b>.</p>
    <p>An AVSMComm class provides a software infrastructure for implementing a server interface that manages client interactions, where clients include user application programs and/or a non-premises AVSS <b>100</b>. The AVSMComm class also provides a basis that allows the AVSM <b>160</b> to behave as a client relative to non-premisesAVSSs <b>100</b>. The AVSMComm class comprises methods for retrieving requests, building objects that are subclassed according to request type, and dispatching requests. The AVSMComm class further comprises methods for receiving replies, building objects subclassed according to reply type, and issuing replies. The AVSMComm class includes a reference to an AVSMRequestList class and an AVSMReplyList class.</p>
    <p>The AVSMRequestList class facilitates the implementation of a queue for requests received from AVSS-external clients, as well as a queue for requests directed to non-premises AVSSs <b>100</b>. The AVSMRequestList class comprises a reference to an AVSMRequest class, which comprises request parameters as well as methods for accessing data associated with a request. In one embodiment, the AVSMRequest class is subclassed according to request type. Thus, a request queue may include a first reference to an AVSMOpenRequest object that corresponds to an A/V file open request received from a client; and a second reference to an AVSMPlayRequest object that corresponds to an A/V file playback request received from a client.</p>
    <p>The AVSMReplyList class facilitates the implementation of a queue for replies directed from the AVSM <b>160</b> to external clients, as well as a queue for replies received from non-premises AVSSs <b>100</b>. The AVSMReplyList class comprises a reference to an AVSMReply class, where the AVSMReply class comprises reply parameters as well as a set of methods for accessing data associated with a reply. In a manner analogous to that for the AVSMRequest class, the AVSMReply class may be subclassed in accordance with reply type.</p>
    <p>An AVNMComm class provides a software infrastructure for implementing an A/V network client interface that includes parameters and methods for issuing requests to and receiving replies from the A/V network manager <b>34</b>. The AVNMComm class includes a reference to an AVNMRequestList class and an AVNMReplyList class. The AVNMRequestList class facilitates the implementation of a queue for requests directed to the A/V network manager <b>34</b>, and comprises a reference to anAVNMRequest class. The AVNMRequest class includes parameters corresponding to a request directed to the A/V network manager <b>34</b>, as well as methods for accessing data associated with the request. The AVNMRequest class may be subclassed in accordance with the type of request directed to the A/V network <b>30</b>. The AVNMReplyList class facilitates the implementation of a queue for replies received from the A/V network manager <b>34</b>, and includes a reference to an AVNMReply class. The AVNMReply class comprises parameters associated with replies received from the A/V network manager <b>34</b>, as well as methods for accessing any data associated therewith. The AVNMReply class may be subclassed in accordance with the types of a replies that may be received from the A/V network manager <b>34</b>.</p>
    <p>An AVSCComm class comprises a software infrastructure for implementing an AVSC client interface, and comprises parameters and methods for issuing requests to and receiving replies from AVSCs <b>120</b>. The AVSCComm class includes a reference to an AVSCRequestList class, which facilitates implementation of a queue for requests directed to AVSCs <b>120</b>. The AVSCRequestList class includes a reference to an AVSCRequest class, which includes parameters and methods for generating requests directed to an AVSC <b>120</b>, and which may be subclassed in accordance with the types of requests that may be directed to AVSCs <b>120</b>. The AVSCComm class also includes a reference to an AVSCReplyList class, which facilitates implementation of a queue for replies received from AVSCs <b>120</b>. The AVSCReplyList class includes a reference to an AVSCReply class, which comprises parameters and methods associated with receipt of AVSC replies. The AVSCReply class may be subclassed in accordance with AVSC reply types.</p>
    <p>A CellList class serves as a basis for a list that describes each premises AVSC <b>120</b>. The CellList includes a reference to a Cell class, which provides a basis for describing an AVSC <b>120</b>. The Cell class includes parameters that specify a network address, bandwidth capabilities, and a number of simultaneous requests that the AVSC <b>120</b> can process. The Cell class further includes references to a ListDisks class, a ListEncoders class, a ListDecoders class, and a ListTranscoders class.</p>
    <p>The ListDisks class facilitates creation of a list that references a Disk class, which supports an AVSC storage device description. The Disk class contains parameters that include such information as device transfer rate, seek time, and available storage space. The Disk class additionally contains methods for accessing and modifying such parameters.</p>
    <p>The ListEncoders class facilitates the implementation a list that references an Encoder class, which supports an AVSC encoder description. Encoder class parameters include such information as encoder type, bandwidth and I/O limitations, supported media formats, and input signal characteristics. The Encoder class additionally contains methods for accessing and modifying these parameters.</p>
    <p>The ListDecoders class facilitates the implementation of a list that references a Decoder class, which supports an AVSC decoder description. Decoder class parameters include such information as decoder type, bandwidth and I/O limitations, supported media formats, and output signal characteristics. The Decoder class additionally contains methods for accessing and modifying the Decoder class parameters.</p>
    <p>The ListTranscoders class facilitates the implementation of a list that references a Transcoder class, which supports an AVSC transcoder description. Transcoder class parameters include such information as transcoder type, bandwidth and I/O limitations, supported media formats, and input/output signal characteristics. The Transcoder class additionally contains methods for accessing and modifying the Transcoder class parameters.</p>
    <p>A LoginList class provides an implementation basis for a list describing currently logged-in users. The LoginList class references a Login class that supports a user description via parameters that include a user ID, a privilege level, a login date and time stamp, and a set of channel handles currently allocated to the user ID, where each channel handle corresponds to AVSC resources allocated to processing user requests as described in detail below.</p>
    <p>A SessionList class serves as basis for a list that describes currently active sessions. The SessionList class references a Session class, which provides a foundation for describing any particular session. The Session class is subclassed according to session type, where the subclasses include CopySession, PublishSession, ImportSession, ExportSession, AVSession, FileSession, and ListSession classes.</p>
    <p>The CopySession class provides a framework for describing and tracking a file copy operation between two AVSCs <b>120</b>. The CopySession class includes parameters such as IP or network addresses for the AVSCs <b>120</b> involved, a source and a destination filename, and channel handles corresponding to resources reserved on the AVSCs <b>120</b> for performing the file copy operation. In one embodiment, the CopySession class parameters also include a flag that specifies whether a copy operation or a transfer operation is required, where the transfer operation comprises file copying followed by deletion of the file from an origin AVSC <b>120</b>. The CopySession class further includes methods for accessing its parameters.</p>
    <p>The PublishSession class comprises parameters and methods that describe and monitor A/V and/or multimedia document publishing operations, through which AN and/or multimedia files may be copied to non-AVSS servers. PublishSession class parameters include IP or network addresses plus login data for a target or destination server, as well as a filename. PublishSession class methods includes data conversion and data transfer methods. The PublishSession class may be subclassed in accordance with particular media formats, where such subclasses include parameters such as destination server type and transcoding requirements. Exemplary subclasses include a NetShowPublish class and a RealMediaPublish class.</p>
    <p>The ImportSession class comprises parameters and methods that describe and monitor the reading of A/V file data into a target AVSC <b>120</b> from a source client application (or source workstation <b>40</b> or other computer upon which the client application is executing), and includes parameters specifying the target AVSC's IP or network address and an AVSC channel handle. The ExportSession class provides a basis for writing A/V file data from the AVSC <b>120</b> into a target client application program (or target workstation <b>40</b> or other computer upon which the client application program is running), and includes parameters specifying the source AVSC's IP or network address and an AVSC channel handle.</p>
    <p>The AVSession class comprises parameters and methods for initiating and managing A/V operations, which may include record, play, stream-in, stream-out, pause, resume, status, and stop operations.</p>
    <p>The FileSession class comprises parameters and methods that facilitate writing data to and reading data from an AVSC <b>120</b>. The ListSession class comprises parameters and methods that facilitate the generation of a list that specifies files and file attributes stored upon one or more AVSCs <b>120</b>.</p>
    <p>An AVMFileList class provides a basis for implementing the AVSM file parameter list within the AVSS database. The AVMFileList includes a reference to an AVMFile class, which includes parameters describing an A/V file, such as filename, file author or creator, a file title and description, file size, encoding type, and playback duration. The AVMFile class further includes a reference to an OwnerList class, which in turn references an Owner class. The Owner class provides a basis for describing an A/V file owner, and includes parameters such as user ID, a last access date and time, and an ownership expiration date. The OwnerList class includes methods for adding and deleting owners.</p>
    <p>Those skilled in the art will understand that the AVSM class hierarchy <b>250</b> can be readily modified or extended to provide or accommodate access to additional types of AVSM functionality.</p>
    <p>4.8.3 AVSC Class Relationship</p>
    <p>In a manner similar to that for the AVSM objects, the AVSC objects resident within the AVSC object memory <b>152</b> are hierarchically interrelated. FIG. 17 is a block diagram of an AVSC class relationship <b>300</b> organized in accordance with one embodiment of the present invention. In the-AVSC class relationship <b>300</b>, a MediaResource class serves as a container class for implementing multithreaded AVSC functionality, and includes methods for thread management. An AVSCComm class provides a software infrastructure for implementing an AVSC communication port. The AVSCComm class provides a basis for implementing a server that receives requests and issues replies, as well as a client that issues requests and receives replies. Typically, an AVSC <b>120</b> will act as a server the majority of the time; however, the AVSC <b>120</b> may act as a client during AVSC-to-AVSC file copy or transfer operations. The AVSCComm class comprises parameters and methods for retrieving requests, building objects that are subclassed according to request type, and dispatching requests. In one embodiment, request dispatch occurs via the creation of a thread of execution corresponding to a request. The AVSCComm class includes references to an AVSCRequestList class and an AVSCReplyList class.</p>
    <p>The AVSCRequestList class provides a basis for implementing a request queue, and references an AVSCRequest class. The AVSCRequest class facilitates request description, and May be subclassed in accordance with request type. The AVSCReplyList class provides a basis for implementing a reply queue, and references anAVSCReply class, which may be subclassed according to reply type.</p>
    <p>A MessageThreadList class facilitates implementation of a list of threads that are currently active within the AVSC <b>120</b>, and references a MediaThread class. The MediaThread class provides a framework for implementing a thread of execution that processes and fulfills a request. The MediaThread class includes thread parameters such as a thread ID and thread state information. The parameters may further include a filename, a password, and an IP or network address. The MediaThread class references a MediaMessage class. The MediaMessage class comprises parameters and methods that direct thread execution, and is subclassed according to thread type.</p>
    <p>In the context of the AVSC class relationship <b>300</b>, a â€œchannelâ€ comprises a group of resources allocated for performing a particular type of operation, such as an A/V file playback or record operation. A ChannelList class facilitates the implementation of a list that identifies or describes a current set of channels within the AVSC <b>120</b>. The ChannelList class references a MediaChannel class, which comprises a container class for representing a channel. The MediaChannel class selectively includes references to an Encoder class, a Decoder class, a Transcoder class, a Buffer class, a DiskAccess class, and a NetAccess class. The Encoder class provides a basis for describing and controlling an encoder within the AVSC <b>120</b>, and includes parameters specifying encoder type, supported quality levels, whether the encoder is busy or available, and methods that communicate with a set of software drivers that control encoder hardware. The Decoder class serves as a basis for describing and directing the operation of a decoder within the AVSC <b>120</b>, and includes parameters specifying decoder type, supported quality levels, availability, and methods that communicate with software drivers that control decoder operation. Similarly, the Transcoder class serves as a basis for describing and directing the operation of a transcoder within the AVSC <b>120</b>, and includes parameters specifying transcoder type, supported source and target coding formats, availability, and methods that communicate with software drivers that control transcoder operation. The Buffer class comprises parameters and methods that facilitate the implementation of memory and/or disk buffers, where such parameters may include buffer type, size, and fill level, and such methods may include program instructions for reading from and writing to the buffer. Finally, the NetAccess class serves as a basis for managing data exchange between the AVSC <b>120</b> and the AVSS internal network <b>110</b>. Each of the Encoder, Decoder, Buffer, DiskAccess, and NetAccess classes may be subclassed in accordance with the particular nature of the resources to which they correspond.</p>
    <p>The MediaChannel class also includes a reference to a MediaFile class, which serves as a wrapper for providing an interface to particular functionality and AVSC resources allocated for the channel. The MediaChannel additionally includes a reference to a MessageThreadList class, which facilitates implementation of a list that identifies threads currently active within the scope of the channel.</p>
    <p>An EncoderList class within the AVSC class relationship <b>300</b> serves as a basis for implementing a list that identifies the encoders within the AVSC <b>120</b>. The EncoderList class includes a reference to the aforementioned Encoder class. Similarly, a DecoderList class serves as a basis for implementing a list identifying the decoders within the AVSC <b>120</b>. The DecoderList class includes a reference to the Decoder class. A TranscoderList class facilitates implementation of a list identifying the transcoders within the AVSC <b>120</b>. The TranscoderList class also references the Transcoder class. A BufferList class facilitates implementation of a list that indicates buffer allocation within the AVSC <b>120</b>, and includes a reference to the Buffer class. A DiskAccessList class facilitates implementation of a list specifying allocation of storage devices within the AVSC <b>120</b>, and includes a reference to the Disk class. Finally, a NetAccessList class facilitates implementation of a list of allocated network access resources, and includes a reference to the NetAccess class.</p>
    <p>As with the AVSM class relationship <b>250</b>, those skilled in the art will understand that the AVSC class relationship <b>300</b> can be readily modified or extended to provide or accommodate access to additional types of AVSC functionality.</p>
    <heading>4.9 Client and Server Communication</heading> <p>In the present invention, an AVSSexternal or AVSS-internal client requiring a service accessed via an instance of a target object calls for the service by issuing a request. In response, the target object may perform one or more portions of the required service, act as a client and issue one or more requests to other objects, and/or issue one or more messages. Upon service completion, the target object issues a reply. Thus, from a hierarchical perspective, the issuance of a high-level request may give rise to the issuance of one or more lower-level requests and replies, followed by the issuance of a high-level reply. Each such request and reply in one embodiment of the present invention is exchanged in accordance with a conventional Internet Protocol (IP) suite, in a manner readily understood by those skilled in the art.</p>
    <heading>4.10 AVSM Request Categories</heading> <p>The requests understood by the AVSM <b>160</b> span several categories, which in one embodiment include the following:</p>
    <p>4.10.1 External Client to AVSS Requests</p>
    <p>Requests that may be used by AVSS-external clients requiring access to AVSS services include the following:</p>
    <p>a) Loginâ€”initiates a login session with the AVSS <b>100</b> and provides user information and associated A/V network service information to the AVSM <b>160</b>, where the user information includes a user ID and a password.</p>
    <p>b) Logoutâ€”terminates user interaction with the AVSS <b>100</b>.</p>
    <p>c) GetFileLinitsâ€”returns current administered and physical limits on the length of an A/V file to be encoded.</p>
    <p>d) AcquireAVChannelâ€”establishes a session by reserving particular AVSC resources for performing specified types of operations, such as encoding, decoding, transcoding, and/or streaming. Specifies a source AVSC resource and at least one destination AVSC resource to flexibly support multiple modes of operation, which include the following:</p>
    <p>1) Playback modeâ€”facilitates playback of an A/V file residing upon an AVSC storage device to a user workstation <b>40</b>, where such playback can occur at a specified speed or playback rate. Source resource=disk, destination resource=decoder.</p>
    <p>2) Preview modeâ€”facilitates recording setup and record preview operations involving a user workstation <b>40</b> (i.e., A/V signal â€œloop throughâ€ without storage of A/V signals to disk). Source resource=encoder, destination resource=decoder.</p>
    <p>3) Record modeâ€”facilitates A/V signal recording from a user workstation <b>40</b> to generate an A/V file upon an AVSC storage device. Source resource=encoder, destination resource=disk.</p>
    <p>4) Playback-while-recording modeâ€”facilitates A/V signal recording from a user workstation <b>40</b> with simultaneous A/V signal playback to the user workstation <b>40</b>. Source resource=encoder, destination resources disk and decoder.</p>
    <p>5) Broadcast-to-network modeâ€”facilitates broadcast of A/V signals from a user workstation <b>40</b> to a network (i.e., the AVSS internal network <b>110</b> or the data network <b>20</b>). Source resource=encoder, destination resource=network.</p>
    <p>6) Broadcast-and-playbackinodeâ€”facilitates A/V signal broadcast from a user workstation <b>40</b> to a network, with simultaneous A/V signal playback to the user workstation <b>40</b>. Source resource=encoder, destination resources=network and decoder.</p>
    <p>7) Broadcast-while-recording modeâ€”facilitates broadcast to a network while performing A/V signal recording from a user workstation <b>40</b> to an AVSC storage device. Source resource=encoder, destination resources=disk and network.</p>
    <p>8) Broadcast-with-record-and-playbackmodeâ€”facilitates broadcast to a network while recording A/V signals from a user workstation <b>40</b> to AVSC storage device and simultaneously playing A/V signals back to the user workstation <b>40</b>. Source resource=encoder, destination resources=network, disk, and decoder.</p>
    <p>9) Stream-in-to-disk modeâ€”facilitates receiving a data stream from a network to an AVSC storage device. Source resource=network, destination resource=disk.</p>
    <p>10) Stream-in-to-playback modeâ€”facilitates data streaming from a network to a user workstation <b>40</b> via AVSC decoder. Source resource=network, destination resource=decoder or transcoder.</p>
    <p>11) Stream-in-to-playback-with-save modeâ€”facilitates data streaming from a network to a user workstation <b>40</b>, and saves data being streamed in to an AVSC storage device. Source resource=network, destination resource=decoder or transcoder and disk.</p>
    <p>12) Stream-out-to-network modeâ€”facilitates data streaming from an AVSC storage device to a network at a specified streaming rate. Source resource=disk, destination resource=network.</p>
    <p>13) Stream-out-to-network-andplayback-mode facilitates data streaming from an AVSC storage device to a network at a specified streaming rate with simultaneous A/V signal playback to the user workstation <b>40</b>. Source resource=disk, destination resource=network and decoder</p>
    <p>Those skilled in the art will understand that a buffer resource upon an AVSC may be included as an intermediate destination resource for several types of operational modes, for example, those involving streaming. Those skilled in the art will additionally understand that many other modes of operation exist, for which a session may be established and resources may be reserved in a manner analogous to that described above. When appropriate (i.e., depending upon the specified mode of operation), the AcquireAVChannel request also specifies parameters such as a filename and password; A/V file encoding type; and/or a minimum frame count that indicates an amount of storage space that must be available upon a storage device. After successful resource reservation, a reply corresponding to AcquireAVChannel (i.e., an AcquireAVChannelReply) returns a session handle to a requesting client.</p>
    <p>e) AcquireDataChanmelâ€”establishes a session and reserves AVSC resources for particular types of file transfer operations, including copy-to, copy-from, file query, and file listing operations. This request includes parameters specifying a mode such as read-only or read-write, as well as parameters such as a filename, password, and a minimum amount of storage space that must be present upon an AVSC storage device. After AVSC resource reservation, a reply corresponding to AcquireDataChannel returns a session handle.</p>
    <p>f) AcquireAVFilelmportChannelâ€”establishes a session for importing an A/V file into the AVSS <b>100</b> and returns a session handle and the network or IP address of the AVSC <b>120</b> that will initially receive the A/V file.</p>
    <p>g) AcquireAVFileExportChannelâ€”establishes a session for exporting an A/V file from the AVSS <b>100</b> to the client and returns the network or IP address of the AVSC <b>120</b> upon which the file is stored, plus a session handle.</p>
    <p>h) AcquirePublishChannelâ€”establishes a session for publishing an A/V file from an AVSC <b>120</b> to one or more non-AVSS target servers. Parameters specified include a filename and password, as well as target server type. A reply associated with this request returns a session handle.</p>
    <p>i) PublishFileâ€”publishes a file to a non-AVSS target server. Specified parameters include a filename and password, a target server network or IP address, and target server login and password information.</p>
    <p>j) RequestPublishLocationsâ€”given a specified filename and password, returns known non-AVSS target server identification and network or IP address to which a file has previously been published, as indicated by the AVSS database.</p>
    <p>k) DeletePublishedFileâ€”given a specified filename, file password, and non-AVSS target server network or IP address or target server type, deletes file's publication references for target server or target server types from the AVSS database. If target server login, login password, and any delete authorization information is also specified, DeletePublishedFile additionally attempts to delete the published file from the target server itself.</p>
    <p>l) ReleaseAVChannelâ€”releases the resource or resources obtained through the AcquireAVChannel, AcquireAVFilelmportChannel, AcquireAVFileExportChannel, and AcquirePublishChannel requests, and deletes a corresponding session handle.</p>
    <p>m) AVSMStatusRequestâ€”returns status information to a client during file copy, import, export, and publishing operations when a valid session handle is specified, where such status information may include a number of bytes transferred or an estimated amount of time remaining until operation completion.</p>
    <p>n) GetAVFileAttributesâ€”returns attributes for an A/V file, including application program-defined attributes. Certain restrictions may exist with regard to which attributes may be returned. For example, only a super-user and the file author may be allowed to view a complete owner list. A user may be allowed to view file attributes if the user is specified as an owner for the file, and any attributes returned are specific to that user.</p>
    <p>Super-users and the file authors can access the following operations in one embodiment:</p>
    <p>o) SetAVFileAttributesâ€”sets the mutable attributes of an A/V file, including application program-defined attributes.</p>
    <p>p) AddOwnerâ€”causes the AVSM <b>160</b> to add a specified user ID to the access privilege list corresponding to an A/V file.</p>
    <p>q) RemoveOwnerâ€”causes the AVSM <b>160</b> to remove a specified user ID from the access privilege list corresponding to an A/V file.</p>
    <p>r) RequestAVFileâ€”requests an A/V file from a non-premises source AVSS <b>100</b>. If AV file is present within the domain of the AVSM <b>160</b> that receives this request, a file transfer procedure retrieves the A/V file from the non-premises source AVSS <b>100</b>.</p>
    <p>4.10.2 Client to AVSC via AVSM Requests</p>
    <p>Many of the requests the AVSM <b>160</b> issues to an AVSC <b>120</b> are variants of those sent to the AVSM <b>160</b> by client application programs. Requests that the AVSM <b>160</b> may receive from external clients and forward or pass to an AVSC <b>120</b> for servicing include the following:</p>
    <p>a) Openâ€”depending upon a mode specifier that indicates whether an A/V file is to be operated upon relative to A/V resource processes (MEDIA mode) or file transfer processes (DATA mode), opens or creates an A/V file and prepares it for encoding, decoding, and/or transcoding, streaming in, or streaming out (MEDIA mode), or data transfer (DATA mode). Upon completion of this request, a reply is issued that includes a session handle.</p>
    <p>b) Closeâ€”closes a file associated with a specified session handle.</p>
    <p>c) Readâ€”for a file opened in DATA mode, reads a specified number of bytes from the file and transfers them to the requesting client.</p>
    <p>d) Writeâ€”for a file opened in DATA mode, transfers and writes a specified number of bytes to the file.</p>
    <p>e) Seekâ€”for a file opened in DATA mode, positions the file at a specified byte. For a file opened in media mode, positions the file at a specified frame.</p>
    <p>f) Playâ€”causes the AVSC <b>120</b> to signal a decoder to begin processing a file and feeding the output to the A/V network <b>30</b>. Parameters include playback speed or rate as well as direction and processing length indicators that allow this request to implement fast-forward, rewind, or similar operations.</p>
    <p>g) Recordâ€”causes the AVSC <b>120</b> to signal an encoder to begin saving A/V input to the file. A record length limitation may be specified.</p>
    <p>h) Streamlnâ€”causes the AVSC <b>120</b> to stream data from the AVSS internal network <b>110</b> into a buffer, data storage, and/or decoding resource, in accordance with resources reserved for a particular channel.</p>
    <p>i) StreamOutâ€”causes the AVSC <b>120</b> to stream data from a buffer, data storage, and/or encoding resource to the AVSS internal network <b>110</b> at a specified data rate, in accordance with resources reserved for a particular channel.</p>
    <p>j) Stopâ€”causes suspension of a current encoding, decoding, transcoding, or streaming process.</p>
    <p>k) Statusâ€”returns current file mode and frame number if in MEDIA mode or byte position if in DATA mode.</p>
    <p>4.10.3 AVSM to AVSC Requests</p>
    <p>Requests the AVSM <b>160</b> issues to a target AVSC <b>120</b> in relation to A/V file access and file transfer include the following:</p>
    <p>a) AcquireChannelâ€”requests the AVSC <b>120</b> to create a channel reserving a specified encoder, decoder, and/or transcoder for use by an AVSS client. This request may additionally be used to reserve file transfer bandwidth for impending file transfer or streaming operations. Request completion results in the return of a channel handle that may be used to identify the allocated resources in subsequent operations.</p>
    <p>b) ReleaseChannelâ€”indicates to the AVSC <b>120</b> that the resources associated with a specified handle are no longer required and shall be released.</p>
    <p>c) SetMediaSetupâ€”sets or updates configuration information for the target AVSC <b>120</b>. This request is typically employed in the event that the AVSC's hardware configuration has been modified.</p>
    <p>d) GetMediaSetupâ€”returns configuration and usage information for the target AVSC <b>120</b>.</p>
    <p>e) GetDrivelnfoâ€”returns information about a specified storage device.</p>
    <p>f) GetMediaStatusâ€”returns current AVSC utilization and state information.</p>
    <p>g) Renameâ€”renames a specified file.</p>
    <p>h) Deleteâ€”deletes a specified file.</p>
    <p>i) CopyToâ€”initiates the transfer of a specified file to a specified destination AVSC <b>120</b>.</p>
    <p>j) CopyFromâ€”initiates the transfer of a specified file from a specified source AVSC <b>120</b>.</p>
    <p>4.10.4 AVSC to AVSM Requests</p>
    <p>An AYSC <b>120</b> may send informational requests to its managing AVSM <b>160</b> to indicate state changes associated with file transfer operations, A/V operation completion, and/or general hardware status. Such informational requests include the following:</p>
    <p>a) PlayEndâ€”indicates that a current play operation has completed, due to end-of-file or a play-length being reached.</p>
    <p>b) RecordEndâ€”indicates that a current record operation has completed.</p>
    <p>c) HardwareErrorâ€”indicates a hardware failure associated with a specified encoder, decoder, transcoder, or storage device.</p>
    <p>4.10.5 AVSM to AVSM Requests</p>
    <p>In addition to receiving external requests from client application programs, a premises AVSM <b>160</b> may receive requests from a non-premises AVSM <b>160</b>. Typically, such requests correspond to file transfer operations, and include the following:</p>
    <p>a) RequestAVFileSourceâ€”issued by the premises AVSM <b>160</b> to the non-premises AVSM <b>160</b> upon receipt of a â€œRequestAVFileâ€ from a client. The non-premises AVSM <b>160</b> returns i) the network or IP address of an AVSC <b>120</b> that can serve as a file transfer source; ii) attributes for a specified A/V file: and iii) a channel handle.</p>
    <p>b) NotifyAVFileSourceâ€”sent upon successful completion of an A/V file transfer between a premises and a non-premises AVSC <b>120</b>, and includes the user ID and source channel handle, such that the non-premises AVSM <b>160</b> can release source AVSC resources that had been reserved for the file transfer.</p>
    <p>4.10.6 AVSM Administration Requests</p>
    <p>Several requests, some of which may be accessible only to a super-user, may be used to perform AVSS configuration and administrative operations, and include the following:</p>
    <p>a) SetPrivilegedâ€”establishes a super-user session, through which the super-user can access many administrative functions. This request requires a password.</p>
    <p>b) GetAVSMLogInfoâ€”returns information stored in the AVSM event log.</p>
    <p>c) ClearAVSMLogInfoâ€”clears the AVSM event log.</p>
    <p>d) SetAVSMLogLevelâ€”specifies types of requests and events to log, as well as a level of detail for logging, and a maximum AVSM event log size.</p>
    <p>e) ListAVSCsâ€”returns a list of AVSCs <b>120</b> managed by the AVSM <b>160</b>.</p>
    <p>f) GetAVSCInfoâ€”returns configuration and utilization information for a particular AVSC <b>120</b>.</p>
    <p>g) GetAVSCLogInfoâ€”returns information stored in the AVSC event log, message queue.</p>
    <p>h) ClearAVSCLogInfoâ€”clears a specified AVSC's event log.</p>
    <p>i) SetAVSCLogLevelâ€”indicates to a specified AVSC <b>120</b> types of requests and events to log, as well as a level of detail for logging, and a maximum AVSC event log size.</p>
    <p>j) SetAVNMInfoâ€”updates A/V network manager information maintained within the AVSM <b>160</b>.</p>
    <p>k) AddAVSCâ€”adds configuration information corresponding to a newly-added premises AVSC <b>120</b> to the AVSS database. Configuration information for the new AVSC <b>120</b> is obtained by querying the AVSC <b>120</b>.</p>
    <p>l) RemoveAVSCâ€”removes information corresponding to an AVSC <b>120</b> no longer in the premises group from the AVSS database.</p>
    <p>m) ListAVSSsâ€”returns a network or IP address for each non-premises AVSS <b>100</b> about which the AVSM <b>160</b> is aware.</p>
    <p>n) GetAVSSInfoâ€”returns routing and/or connection setup information for a particular non-premises AVSS <b>100</b> known to the AVSM <b>160</b>.</p>
    <p>o) SetAVSSInfoâ€”sets routing and/or connection setup information for a specified non-premises AVSS <b>100</b>. In one embodiment, this information pertains to the host AVSM <b>160</b> for the specified AVSS <b>100</b>.</p>
    <p>p) AddAVSSâ€”adds a specified non-premises AVSS <b>100</b> to those known to the AVSM <b>160</b>. In one embodiment, routing and connection information for the newly-added AVSS <b>100</b> is specified through SetAVSSInfo.</p>
    <p>q) RemoveAVSSâ€”removes a non-premises AVSS <b>100</b> from those know to the AVSM <b>160</b>.</p>
    <p>r) SetAVFileLimitsâ€”sets a maximum A/V file length allowed during encoding operations.</p>
    <p>Those skilled in the art will recognize that an AVSS could support additional or fewer requests in an alternate embodiment.</p>
    <heading>4.11 AVSC Request Categories</heading> <p>The AVSC <b>120</b> provides support for a variety of requests, including those in the following categories:</p>
    <p>4.11.1 Allocation and Authorization Requests</p>
    <p>a) AuthorizationResourceâ€”ensures that a client requesting services provided by the AVSC <b>120</b> is either an AVSM <b>160</b> or a client providing a valid channel handle.</p>
    <p>b) AllocateChannelâ€”creates a channel to which specified resources are allocated, and returns a channel handle.</p>
    <p>c) ChangeChannelâ€”for a channel identified by a channel handle, adds or deletes specified resources to or from channel, respectively.</p>
    <p>d) ReleaseChannelâ€”for a channel identified by a channel handle, frees channel resources (i.e., channel resources are returned to â€œnot busyâ€ or â€œnot allocatedâ€ status), removes channel from channel list, and releases channel handle.</p>
    <p>4.11.2 File Management Requests</p>
    <p>a) CopyFromChannelâ€”establishes a copy session with a source AVSC <b>120</b> and performs file copy operations.</p>
    <p>b) CopyToChannelâ€”establishes a copy session with a target AVSC <b>120</b> and performs file copy operations.</p>
    <p>c) RenameChannelâ€”renames a specified file.</p>
    <p>d) DeleteChannelâ€”deletes a specified file.</p>
    <p>e) FindFilesFirstChannelâ€”performs file directory operations, returning first k files.</p>
    <p>f) FindFilesNextChannelâ€”performs file directory operations, returning next n files.</p>
    <p>g) GetFileInfoChannelâ€”opens a specified file and returns information about the media content of the file; this request may be employed, for example, to validate file import operations or for consistency checking.</p>
    <p>h) PublishDigitalFileChannelâ€”performs format conversion, login operations, and file copy operations to publish a specified file to another server in a specified format.</p>
    <p>4.11.3 Media Requests</p>
    <p>a) OpenChannelâ€”opens a specified A/V file on a specified channel.</p>
    <p>b) ReadChannelâ€”reads from an A/V file on a specified channel.</p>
    <p>c) WriteChannelâ€”writes to an A/V file on a specified channel.</p>
    <p>d) PlayChannelâ€”plays an A/V file on a specified channel at a specified frame rate.</p>
    <p>e) StreamInChannelâ€”streams an A/V file into a specified channel.</p>
    <p>f) StreamOutChannelâ€”streams an A/V file from a specified channel at a specified data rate.</p>
    <p>g) RecordChannelâ€”records an A/V file on a specified channel.</p>
    <p>h) PauseChannelâ€”pauses A/V file operations on a specified channel.</p>
    <p>i) StopChannelâ€”stops A/V file operations on a specified channel.</p>
    <p>j) ResumeChannelâ€”resumes A/V file operations on a specified channel.</p>
    <p>k) SeekChannelâ€”moves to a given position within an A/V file on a specified channel.</p>
    <p>l) StatusChannelâ€”returns information about current status of operations on a specified A/V file, including current frame or position within the file, on a specified channel.</p>
    <p>m) CloseChannelâ€”closes an A/V file on a specified channel.</p>
    <p>4.11.4 Administrative Requests</p>
    <p>a) InitializeResourceâ€”resets hardware, rebuilds lists, and reallocates objects.</p>
    <p>b) GetMediaSetupResourceâ€”returns information about encoders, decoders, transcoders, and storage devices within the AVSC <b>120</b>.</p>
    <p>c) SetMediaSetupResourceâ€”sets information about encoders, decoders, transcoders, and/or storage devices within the AVSC <b>120</b>.</p>
    <p>d) GetMediaStatusResourceâ€”returns current status of AVSC resources, including a number of channels open; a number of encoders, decoders, transcoders in use; storage device usage and available storage space; and buffer and internal network usage.</p>
    <p>e) HangupResourceâ€”performs failure recovery or system violation recovery operations.</p>
    <p>f) ShutdownResourceâ€”performs shutdown operations, after which AVSC <b>120</b> can be reinitialized.</p>
    <heading>4.12 Request Sequence Examples</heading> <p>The description that follows details the flow of requests and replies generated in response to exemplary AVSS-external requests corresponding to several operational categories, which are defined as follows:</p>
    <p>4.12.1 Session Establishment and Resource Reservation</p>
    <p>FIG. 18 is an exemplary request sequence diagram for the AcquireAVChannel request described above. In FIG. 18, and AVSS client (i.e., a client application program) issues an AcquireAVChannel request specifying a mode and mode-specific parameters to its premises AVSM <b>160</b>. The AVSM <b>160</b> determines a set of AVSC resources required to fulfill the channel acquisition request, and identifies an appropriate AVSC <b>120</b>. The AVSM <b>160</b> then issues an AcquireChannel request to the AVSC <b>120</b>, specifying a set of resources to be reserved for a channel. Upon reserving the resources, the AVSC <b>120</b> issues an AcquireChannelReply to the AVSM <b>160</b>, which includes a channel handle and a return code that indicates whether resource reservation was successful. The AVSM <b>160</b> subsequently issues an AcquireAVChannelReply that includes a session handle to the client. The client may subsequently use the session handle to request particular operations in accordance with the types of AVSC resources associated with the channel handle. For any given client, the AVSM <b>160</b> may map one or multiple session handles to a single channel and handle. The AVSM <b>160</b> is responsible for mapping session handles to the appropriate channel handle during subsequent operations. Those skilled in the art will understand that analogous operations occur for each particular type of channel acquisition request described above.</p>
    <p>4.12.2 A/V File Management</p>
    <p>Since the AVSM <b>160</b> is responsible for maintaining the AVSS database, it must perform some processing in response to client requests affecting A/V files. Since the A/V files are physically stored upon the AVSCs <b>120</b>, the AVSM <b>160</b> passes or forwards requests to the appropriate AVSC(s) <b>120</b> following such processing. In general, for AVSS clients requiring access to an A/V file, the AVSM <b>160</b> examines the AVSS database to determine whether a specified file exists, and upon whichAVSCs <b>120</b> the file resides. When an A/V file is created, the AVSM <b>160</b> adds a file parameter list to the AVSS database, and selects an available AVSC <b>120</b> to support an encoding session. The AVSM <b>160</b> issues a Record request to the selected AVSC <b>120</b>, updates its internal data, and upon completion of the recording operation issues a RecordReply to the client.</p>
    <p>FIG. 19 is an exemplary request sequence diagram for the â€œOpenâ€ request described above. In FIG. 19, an AVSS client (i.e., a client application program) issues an Open request specifying a session handle, a filename, and a mode (i.e., MEDIA, DATA read/write or read only) to its premises AVSM <b>160</b>. The AVSM <b>160</b> subsequently maps the session handle to the appropriate AVSC channel handle, and forwards the open request to the AVSC <b>120</b> corresponding to the channel handle. After opening the A/V file, the AVSC <b>120</b> issues an OpenReply to the AVSM <b>160</b>, which includes a channel handle and a return code that indicates whether the open operation was successful. The AVSC <b>160</b> in turn replies to the AVSS client with a session handle and a return code.</p>
    <p>FIG. 20 is an exemplary request sequence diagram for the above-mentioned â€œCloseâ€ request. In FIG. 20, an AVSS client issues a Close request that specifies a session handle and a filename to the AVSM <b>160</b>. The AVSM <b>160</b> maps the session handle to the appropriate channel handle, and issues or forwards the close request specifying the channel handle and the filename to the appropriate AVSC <b>120</b>. Upon completing the close operation, the AVSC <b>120</b> issues a close reply to the AVSM <b>160</b>, which includes the channel handle and a return code. The AVSM <b>160</b> maps the channel handle to the appropriate session handle, and forwards the close reply to the AVSS client.</p>
    <p>FIG. 21 is an exemplary request sequence diagram for an A/V file â€œDeleteâ€ request. In response to receipt of a delete request that specifies a session handle and a fully-qualified filename from an AVSS client, the AVSM <b>160</b> determines upon which AVSCs <b>120</b> copies of the A/V file reside. The AVSM <b>160</b> then maps the session handle to a first channel handle, and forwards the delete request to a first AVSC <b>120</b> upon which such a copy resides. Once the A/V file copy is deleted, the first AVSC <b>120</b> issues a DeleteReply to the AVSM <b>160</b>. The AVSM <b>160</b> then maps the session handle to a second channel handle, and forwards the AVSS client's delete request to a second AVSC <b>120</b> upon which a copy of the A/V file resides. The second AVSC <b>120</b> performs the required delete operation, and issues a DeleteReply to the AVSM <b>160</b>. The AVSM <b>160</b> continues this process by mapping the session handle to a next channel handle, and forwarding the AVSS client's delete request to a next AVSC <b>120</b> storing a copy of the A/V file after receiving a DeleteReply from an AVSC <b>120</b> that had just completed the delete operation, and so on, until copies of the A/V file no longer reside upon the AVSCs <b>120</b>. After the relevant AVSCs <b>120</b> have deleted the appropriate A/V file copies, the AVSM <b>160</b> updates the AVSS database <b>176</b> to reflect the deletion, and issues a DeleteReply to the AVSS client.</p>
    <p>4.12.3 Encoding, Decoding, or Transcoding Control</p>
    <p>FIG. 22 is an exemplary request sequence diagram for a â€œRecordâ€ request. Upon receiving a Record request from an AVSS client specifying a session handle and possibly a filename depending upon operational mode, the AVSM <b>160</b> maps the session handle to the appropriate channel handle. The AVSM <b>160</b> forwards the mapped record request to the AVSC <b>120</b>, which issues commands to the encoder associated with the channel handle. When the recording session is complete, the AVSC <b>120</b> issues a RecordReply that specifies the channel handle and a return code to the AVSM <b>160</b>. The AVSM <b>160</b> then maps the channel handle to a session handle, and forwards the RecordReply to the AVSS client, where the RecordReply includes the session handle and a return code.</p>
    <p>4.12.4 File Transfer</p>
    <p>Externally-generated file transfer requests may result in the generation of multiple intra-AVSS requests and replies, depending upon the location of the requested file. FIG. 23 is an exemplary request sequence diagram corresponding to a request for a file transfer from a non-premises AVSS <b>100</b>. To take advantage of the file transfer capabilities of the AVSS <b>100</b>, a client must have a source AVSM name or address as well as a filename. The client supplies these parameters to its premises AVSS <b>100</b> via a RequestAVFile request.</p>
    <p>The premises AVSM <b>160</b> determines whether the required file is locally available, that is, whether it is stored upon one of the premises AVSCs <b>120</b>. If not, the premises AVSM <b>160</b> issues a RequestAVFileSource request to the non-premises AVSS <b>100</b>. Via an AcquireChannel request, the non-premises AVSM <b>160</b> allocates resources in an AVSC <b>120</b> that can serve as a source for the file transfer. After the non-premises AVSC <b>120</b> resources have been allocated, it replies to the non-premises AVSM <b>160</b> with an AcquireChannelReply. The non-premises AVSM <b>160</b> then replies to the premises AVSM <b>160</b> by issuing a RequestAVFileSourceReply that specifies the location or network or IP address and channel handle of the allocated non-premises AVSC <b>120</b>.</p>
    <p>The premises AVSM <b>160</b> issues an AcquireChannel request to allocate a channel and corresponding resources on a premises AVSC <b>120</b> for performing the file transfer operation, and in response receives an AcquireChannelReply that specifies a channel handle. The premises AVSM <b>160</b> then issues a CopyFrom request to the premises AVSC <b>120</b>, where the CopyFrom request includes the source AVSC's location or address and channel handle. The premises AVSC <b>120</b> initiates the file transfer by issuing an Open request to the source AVSC <b>120</b>, where the Open request specifies that the file is to be opened in DATA mode. Upon receiving an OpenReply from the source AVSC <b>120</b>, the premises AVSC <b>120</b> issues a series of Read requests to the source AVSC <b>120</b>, which result in the file transfer. For each such Read request, the source AVSC <b>120</b> performs a read operation and issues a ReadReply to the premises AVSC <b>120</b>.</p>
    <p>After the file has been transferred, the premises AVSC <b>120</b> issues a Close request to the source AVSC <b>120</b>. The source AVSC <b>120</b> closes the file, and issues a CloseReply to the premises AVSC <b>120</b>, which in turn issues a CopyFromReply to the premises AVSM <b>160</b>. The premises AVSM <b>160</b> next issues a ReleaseChannel request to the premises AVSC <b>120</b> to release resources associated with the copy operation, after which the AVSC <b>120</b> issues a ReleaseChannelReply. The premises AVSM <b>160</b> subsequently updates the AVSS database <b>176</b> to reflect the presence of the new file. The premises AVSM <b>160</b> next sends a NotifyAVFileSource message to the non-premises or source AVSM <b>160</b> to indicate to the non-premises AVSM <b>160</b> that the transfer is complete. The non-premises AVSM <b>160</b> issues a ReleaseChannel request to the source AVSC <b>120</b>, which releases resources reserved for the copy operation and generates a ReleaseChannelReply directed to the non-premises AVSM <b>160</b>. The non-premises AVSM <b>160</b> then issues a NofityAVFileSourceReply to the premises AVSM <b>160</b>. Finally, the premises AVSM <b>160</b> sends a RequestAVFileReply to the AVSS client that required the file transfer operation.</p>
    <p>4.12.5 File Replication</p>
    <p>Once a new A/V file resides upon an AVSC <b>120</b>, the AVSM <b>160</b> may issue a series of requests to perform A/V file replication operations. In one embodiment, A/V file replication is required in the event that more than one owner is specified in the file's access privilege list. File replication would thus be performed, for example, after or as part of processing an â€œAddOwnerâ€ request received from a client application program. In another embodiment, A/V file replication is performed even if an A/V file has only one owner, such that a given A/V file resides upon at least two premises AVSCs <b>120</b>. This approach would enhance system reliability or fault tolerance. In such an embodiment, file replication would occur, for example, following a file transfer operation as described above with reference to FIG. <b>23</b>.</p>
    <p>FIG. 24 is an exemplary request sequence diagram corresponding to file replication operations. The AVSM <b>160</b> could perform file replication operations, for example, following issuance of a RequestAVFile reply to an AVSS client indicating completion of file transfer operations as previously described. To initiate file replication operations, the AVSM <b>160</b> issues a first AcquireChannel request to the AVSC <b>120</b> upon which the file resides, that is, the source AVSC <b>120</b>. After receiving an AcquireChannelReply, the AVSM <b>160</b> issues a second AcquireChannel request to a destination AVSC <b>120</b> to which the file will be copied. Once the AVSM <b>160</b> receives an AcquireChannelReply from the destination AVSC <b>120</b>, it issues a CopyFrom request to this AVSC <b>120</b>. The destination AVSC <b>120</b> then sends the source AVSC <b>120</b> an Open request. Upon receiving an OpenReply, the destination AVSC <b>120</b> send the source AVSC <b>120</b> a series of Read requests, which result in transfer of the file from the source to the destination AVSC <b>120</b>. After the source AVSC <b>120</b> has responded to a given Read request, it issues a ReadReply to the destination AVSC <b>120</b>. Upon responding to a last Read request and issuing a last ReadReply, the destination AVSC <b>120</b> sends a Close request to the source AVSC <b>120</b>, which closes the file and issues a CloseReply to the destination AVSC <b>120</b>. The destination AVSC <b>120</b> subsequently issues a CopyFromReply to the AVSM <b>160</b>. The AVSM <b>160</b> then issues a ReleaseChannel request to the destination AVSC <b>120</b>, and in turn receives a ReleaseChannelReply. According to the particular file replication strategy employed, the AVSM <b>160</b> may repeat the destination AVSC <b>120</b> selection, AcquireChannel, CopyFrom, and ReleaseChannel procedures described above for one or more other destination AVSCs <b>120</b>. The AVSM <b>160</b> could also select either AVSC <b>120</b> upon which a copy of the file currently resides as the source AVSC <b>120</b>. Once any given AVSC's role as a file source is completed, the AVSM <b>160</b> issues a ReleaseChannel request to that source AVSC <b>120</b>, which in turn releases resources reserved for performing the copying operations and generates a ReleaseChannelReply directed to the AVSM <b>160</b>. Those skilled in the art will understand that the file replication operations could be performed via CopyTo requests rather than CopyFrom requests.</p>
    <p>4.12.6 AVSS Administration</p>
    <p>The AVSM <b>160</b> maintains data corresponding to AVSC <b>120</b> resources and their capabilities. The AVSM may query an AVSC <b>120</b> when requested, or as part of an AVSS database update when a new AVSC <b>120</b> has been added to the premises group. FIG. 25 is an exemplary request sequence diagram corresponding to new AVSC addition and AVSC query requests. In response to an authorized client's issuance of an AddAVSC request specifying an AVSC <b>120</b> and a network or IP address, the AVSM <b>160</b> issues a GetMediaSerup request to the AVSC <b>120</b>. The AVSC <b>120</b> generates a GetMediaSetupReply, and provides data describing its resource types, capabilities, and characteristics to the AVSM <b>160</b>. The AVSM <b>160</b> then issues an AddAVSCReply to the requesting client. In response to a GetAVSCInfo request from the client, the AVSM <b>160</b> provides data describing the AVSC's resource types, capabilities, and characteristics to the client via a GetAVSCInfoReply.</p>
    <heading>4.13 Application Program Interface</heading> <p>Application programs executing on user workstations <b>40</b> or other computers act as clients relative to the AVSM <b>160</b>, and may further act as clients relative to the A/V network manager <b>34</b>. FIG. 26 is a block diagram showing client application programs communicating with the AVSM <b>160</b> and A/V network manager <b>34</b>. In one embodiment, a typical application program comprises a user interface plus a set of software objects based upon or derived from the AVSMAppComm class hierarchy <b>250</b> described above. The application program thus serves as a software wrapper that can act as an AVSM client to provide access to particular types of AVSS functionality.</p>
    <p>In response to particular user selections or actions, an application program issues requests to the AVSM <b>160</b>. Based upon replies received from the AVSM <b>160</b>, the application program selectively updates information presented to the user. Since a portion of the application program comprises objects corresponding to the AVSMAppComm, communication between the AVSM <b>160</b> and the application program occurs in a manner analogous to AVSM-to-AVSM communication. Additionally, since the AVSMAppComm can be readily extended or modified to reflect the evolution of AVSM functionality over time, new types of functionality can be readily accommodated with minimal application program modification.</p>
    <heading>4.14 Process and Data Primitives</heading> <p>The video storage server can support several types of applications. One of the ways that the several supported applications can share information, as well as cutting down on the coding required to embody the applications, is to share a set of common â€œprimitiveâ€ elements between the several applications.</p>
    <p>Primitives of the same class are combinable to achieve a specified result. One or more primitives may be combined to form new higher level primitives, which may in turn define yet another even higher level primitive, substantially ad infinitum. There are two broad classes of primitives: process primitives and data primitives.</p>
    <p>Process primitives are invoked to perform an action. In turn, they may invoke other actions, including invoking other primitives. Typically process primitives establish AVSS sessions of some type, (for instance at least one of playing a file, getting file lists), present the user with a graphical interface, establish network connections, as well as other operations such as rendering synchronized graphics. Examples of process primitives taught by the principles of the present invention are viewers, browsers, and administrative processes.</p>
    <p>Process primitives can be organized in a number of manners. A first manner is as a stand-alone process directly launched by a user. Alternatively, they may be launched from within another process, for instance a dynamic linked library or DLL. In this case, their thread of execution exists with the thread of execution of the launching process, and also terminates therewith. As a further alternative, process primitives may be launched by another process, as a separate process, whose thread of execution is not tied to the launching process. In this latter case, the termination of the launching process has no effect on the life of the process primitive.</p>
    <p>There are several broad mechanisms for executing process primitives. One process primitive execution methodology implements the direct invocation of the process primitive by the application user. Another process primitive execution methodology is utilized where a target application, including but not limited to third-party software applications, has previously been taught how to display or act upon the receipt of a data primitive in the form of an attachment, MIME type, file type and so forth. The receipt of the data primitive causes the target application, for instance the MIME compliant software application or the operating system, to launch a process primitive associated with the data primitive. Yet another methodology for executing process primitives, especially those process primitives directly accessed by third-party software, is simply to install the process primitive as a plug-in to that software.</p>
    <p>Data primitives refer to a number of specifically formatted file types in the context of their usage by process primitives or other related processes. Data primitives include, but are not necessarily limited to: audio-video files; audio-only files, video-only files; bitmap files; application files; postscript files; graphics files; text files; and synchronization files. Included in synchronization files are a number of time-stamped event file types including, but again not limited to: graphics event files; shareboard event files; window event files; application startup event files; and text event files. The time-stamps in the file signal when to take certain defined actions, thereby enabling the temporal synchronization of the information of one file with respect to another.</p>
    <p>A metafile is an abstraction representing a combination of one or more externally stored data primitives which together comprise all externally stored (â€œout-of-bandâ€) information comprising a particular AV or multimedia segment (such as a message). Associated with the metafile is a pointer file, which includes all referencing information for the various component files of the metafile (file names, permission keys, file system, etc.). One or more pointer files and their associated metafiles can be conceptually grouped to form a virtual aggregate file called an omnifile. The omnifile therefore must be in some fashion transferred successfully from one environment to another in order for authored information to be completely replayed in the new environment. This can be done in a number of ways as will be clear to one skilled in the art. One such embodiment is shown at FIG. <b>27</b>.</p>
    <p>In common practice, the term â€œmultimedia fileâ€ as been used to refer to an aggregate of all types of media files and external reference file pointers. Multimedia files, like metafiles and indeed like other primitives, are hierarchical: they are combinable into other files to perform a certain task, for instance as a multimedia file in any of a number of standard formats. Metafiles, as defined herein, are allowed to be distributed across several file systems and do not include pointer files. In contrast, multimedia files can include pointer files and all of their component media files are stored together, Omnifiles do include pointer files but again are a virtual object spanning potentially several vile systems and thus also differ from multimedia files.</p>
    <p>An application implementing the principles of the present invention specifically takes some of the file types of interest, and creates a MIME type associated with them. It then enables the teaching of some third party browsers including, but not limited to, NetscapeÂ® or Microsoft Internet ExplorerÂ®, or some e-mail packages including EudoraÂ® how to handle the MIME types. Accordingly, many of the third-party application types supported by the present invention are taught how to handle the multimedia files, omnifiles, metafiles, audio-video files, and pointer files of the present invention. Therefore, when a third party application receives one of these file types taught by the present invention, it knows how to invoke the proper process primitive.</p>
    <p>Referring now to FIG. 28, an exemplar data primitive <b>3904</b> is shown which requires a viewer or browser process to display or act upon it. The application has been taught, as described above, how to respond to the receipt of the data primitive, it â€œknowsâ€ when and how to launch process primitives <b>3912</b> and <b>3906</b>. In the viewer/browser exemplar herein presented, receipt of data primitive <b>3904</b> by an application causes the application to launch, at <b>3910</b> or <b>3914</b> either browser <b>3912</b> or viewer <b>3906</b>. Where browser <b>3912</b> is launched, it may invoke a plurality, not shown, of viewers <b>3906</b>. Viewer <b>3906</b> invokes, in turn a at least one data primitive <b>3908</b> for access to one or more files.</p>
    <p>Referring now to FIG. 29, data primitive <b>3904</b> is shown embodied as an attachment, for instance a MIME attachment, to a message <b>3902</b>. By way of illustration but not limitation, message <b>3902</b> could be an e-mail message, word processing document, text document, and so forth. Depending on the application, the attachment may be implemented as the previously discussed MIME attachment, or as a file type. When the application is instructed to act upon the attachment, it invokes the process primitive associated with the attachment type, as previously discussed.</p>
    <p>It will be understood that the immediately preceding discussion of the viewer/browser process primitive is presented for purposes of illustration and not limitation. The principles of the present invention comprehend an almost limitless number of process primitive configurations, of which the previous exemplar is but one. The teachings of the present invention specifically contemplate all such configurations.</p>
    <p>4.14.1 Viewer</p>
    <p>Video record and playback capabilities can separately be embedded in each application by stand alone means, but preferentially a standardized video record playback utility used across several applications is desirable. Thus a viewer is a good candidate for a process primitive and is explained here as one example of the use of process primitives as taught by the principles of the present invention.</p>
    <p>The viewer process primitive sets up the necessary connections and sessions, as previously described, prepares the files for viewing, and provides the necessary viewer interface to enable the user to access and manipulate the files, as permitted. Because the viewer is a standard video call, it can be treated like any other video call. This enables it to be merged with other video calls, conferences, and the like, in similar fashion to any other session enabled by the principles of the present invention.</p>
    <p>The implementation of this feature the present invention is disclosed in FIG. <b>29</b>. Having reference to that figure, the present invention contemplates the utilization of a shell document <b>3902</b> invoked for instance by an application in the MCG (not shown). An example of such an application would be the launching of a video mail session, video conference session, or substantially any of the other applications discussed herein. Embedded in shell document <b>3902</b> is a MIME attachment <b>3904</b>. MIME attachment <b>3904</b> invokes viewer <b>3906</b>, which enables the user to access the AVSS through the network as needed to record or playback video files <b>3908</b>. Permissions to record or playback specific video files may, or to the user accessing the applications. The shared connection, at <b>3910</b>, presents the novel advantage of enabling video recording or playback and combining it with other applications running concurrently in a separate session. In some applications there is provided a browser, <b>3912</b>, which enables the user to select between a number of applications <b>3906</b>. Alternatively, browser <b>3912</b> may be embedded in, or part of viewer <b>3906</b>.</p>
    <p>4.14.2 MIME Attachment</p>
    <p>Because the viewer is common across all the applications, and because the files it invokes are accessible by the several applications, a first application can use the viewer to record or playback a file, and a second application can utilize the viewer for video mail or substantially any other application. Accordingly, the video files are sharable at the file level, the attachment or MIME level, or at the connection level. The utilization of MIME (Multipurpose Internet Mail Extensions) attachments enables the invocation of the viewer and its associated networking connections within a standardized attachment which is accepted by third parties in accordance with standard MIME protocols. Thus the several applications taught in the present invention are interoperable by means of file sharing by the copying of MIME attachments, or by connection sharing.</p>
    <p>4.14.3 Synchronized Shareboard</p>
    <p>One example of such a concurrent session would be enable the concurrent implementation of share board graphics and a video conference session. The addition of multimedia synchronization capability to the previously discussed viewer and MIME implementation enables the several video applications previously discussed to include animated graphics files which are synchronized with the video. Share capabilities currently implemented by the principles of the present invention â€œgrabsâ€ a window and stores it as a bitmap file, enabling the user to draw on top of the bitmap file. This enables the document to be used by two or more users which includes overlay graphics, to be stored and synchronized for later synchronous playback.</p>
    <p>The synchronization is explained as follows: when the two or more users are conducting a video conference, a first window is opened, and a bitmap superimposed thereon. Any files read into the window or the bitmap may be time stamped. Any animations imposed on the bitmap are draw list events which may also be time stamped. Accordingly the principles of the present invention contemplate the capture of the several times stamps invoked during multimedia recording. This enables the later synchronization, during playback, of the several files to present a synchronous view of the entire session. Where a video conference includes N users, the recorded version of the synchronized session may be regarded as the N+1 user. At playback, the recorded version of the session is again treated like any other user.</p>
    <p>The recording process previously discussed is substantially the sequential recording of a sequence of events. If it is desirable that the previously recorded events be capable of reversal, it is necessary to render the graphics in such a way that they are reversible. Similarly, where the system enables â€œgo-tosâ€ it is necessary to render the graphics in such a manner that the rendered image is capable of retrieval at specified â€œgo-toâ€ points.</p>
    <p>Now we have the capability to grant any screen in the window and share it with any user or any storage device enabled.</p>
    <p>Shared applications (not understood). Recording of shared applications is accomplished by a similar means to bitmap capture.</p>
    <p>Where annotations that draw overlays are occurring on top of the video window, the workstations must have the capability for performing graphics overlay on top of video.</p>
    <p>4.14.4 Browser</p>
    <p>The browser aforementioned can also be adapted by those skilled in the art to act as a process primitive in a manner similar to that described thus far.</p>
    <heading>4.15 Applications Overview</heading> <p>The present invention teaches a number of novel software applications enabled by, and for use in conjunction with the video server system, or AVSS, previously discussed. As the present invention enables the implementation of its features in a scalable fashion, the programmatic elements of the present invention provide, among other benefits, the ability to scale the features and advantages presented herein over a wide range of hardware implementations. An overview of one such hardware implementation is shown at FIG. <b>30</b>.</p>
    <p>The present invention provides two broad classes of storage applications. Those which utilize only the basic audio and video recording, storage, browsing, and playback capabilities of the current AVSS will be referred to hereinafter as â€œvideoâ€ applications. This is typically a reduced-capability, lower cost implementation. A second class of applications utilize audio and video along with a synchronized data sharing capability, e.g., synchronized-shareboard/T.120recording, storage, browsing, and playback features. Such capabilities are essential for recording conferences, messages, or presentations where information cannot be conveniently conveyed by audio and video alone. This class of storage applications will be referred to hereinafter as â€œmultimediaâ€ applications. The additional functionality provided by this class of applications may require a somewhat higher implementation cost.</p>
    <p>The principles of the present invention contemplate the AVSS supporting several types of applications. One way in which the several applications can share information is to share certain common â€œprimitiveâ€ elements. This has the added advantage of cutting down on the coding required to embody those applications which utilize primitives. Accordingly, one or more of the applications taught by the present invention may utilize shared primitives to facilitate the interchange of information of between applications or applications elements. Alternatively, one or more of these applications may be implemented as a â€œstand-aloneâ€, or â€œone-offâ€ implementation which does not utilize shared primitives.</p>
    <heading>4.16 Video Applications</heading> <p>As previously mentioned, an AVSS can support several types of applications. By way of illustration, but not limitation, some of these applications include a video mail application which combines, in a novel manner, video clips, or files, with e-mail messages whereby one user can leave a detailed audio-video message on another user's e-mail system. Video mail is like e-mail, but with either a video attachment, or with a video file replacing normal e-mail text altogether. The richness and depth of communication afforded thereby enables significantly greater information interchange than is available by simple voice mail messaging. The Video Answering System taught by the present invention answers a user's e-mail when the user is away or otherwise occupied. This application presents the user with the option of greeting callers in a number of manners, and receiving their video messages in response. One alternative of this application contemplates its implementation using the Video Mail application previously discussed. Video Conference Recording enables a plurality of users to record in real time an established video conference. This application may be invoked at the user's command or may, alternatively be invoked automatically when certain conference parameters are met.</p>
    <p>In addition to these oriented to electronic meetings applications, the architecture of the AVSS system described herein renders, with facility, several additional capabilities. One such capability is the utilization of the present invention as a general-purpose audio-video storage device. This capability enables users to store and forward audio-video files, in a variety of formats, to users throughout the LAN or WAN. The connectivity of such networks to the Internet provides the additional capability of the system as an intranet or Internet gateway for audio, video, audio-video, or multimedia file transfer. Another application is Video Publishing which enables any of a number of system users to create, edit, store and disseminate complex, informationally rich, audio-video documents to a wide variety of recipients by means of LAN, WAN, or to a world-wide audience utilizing the Internet.</p>
    <p>4.16.1 Video Mail</p>
    <p>A first video application enabled by the principles of the present invention is video mail. In a first embodiment of this application a â€œvideo attachmentâ€ capability is provided to existing MIME-based mail systems. Alternatively, video mail may also be implemented by other attachment strategies, without attachments, or as a fully developed stand-alone application which does not rely on any underlying commercial e-mail package.</p>
    <p>In a first embodiment of the present invention, a video attachment utility utilizes the generic video storage and session establishment method hereinafter described. Moreover, this utility utilizes the generic MIME video attachment methodology hereinafter discussed. As opposed to the video mail methodology taught in U.S. Pat. No. 5,617,539 which starts a mode control GUI (or â€œMCGâ€) directly at the user's workstation, an alternative embodiment presented by the present invention sets a â€œMCGâ€ flag so that the AVSS starts a MCG on the user's workstation. Mail authoring invokes the request for a session with an encoder/decoder pair, while mail reading invokes a request for a session with a decoder only.</p>
    <p>The principles of the present invention contemplate the implementation thereof on a wide variety of hardware implementations. In the simplest case, a single AVSS serves both the message authoring user and the message recipient. In this implementation, there are no file transfers required out of the AVSS. Files may be transferred from the AVSS for other reasons, but there is no functional reason for the files to reside on another AVSS in this case. This could be considered a LAN implementation of the principles of the present invention.</p>
    <p>Where a given organization is sufficiently large or so geographically distributed that a single AVSS cannot handle the traffic for all the users thereof, multiple AVSSs may be implemented. This case has two sub-cases: the first sub-case is a WAN implementation, and the second is where the implementation at a given site, for instance a large campus, is so large it requires more than one AVSS. In the second sub-case, the multiple AVSSs are interconnected by trunking <b>16</b> between switches <b>32</b> and/or the local data LAN <b>20</b>. Where a file is required on a different AVSS than the recording AVSS in the second sub-case, it is simply transferred over the local interconnection environment <b>16</b> and <b>20</b>. In the first sub-case, require the transfer of relatively large files from one AVSS to another, this implementation may use of switched WAN services, frame relay, or one or more of their functional equivalents. Moreover, because there is may be so much bandwidth required for these transfers, it may be necessary to provide bandwidth management solutions. Examples of such network bandwidth management solutions include, but are specifically not limited to conducting file transfers at non-peak times, conducting file transfers in a preemptible manner, and other bandwidth conservation methodologies known or apparent to those having ordinary skill in the art. When file transfers are conducted in a preemptible manner, if a user initiates a higher priority action, for instance she initiates a video conference, the file transfer may be preempted; i.e. aborted, halted and restarted after the high priority application, etc.</p>
    <p>Referring now to FIG. 31, the transaction flow among application elements which facilitate a local or LAN implementation of video mail is shown, where a mail message incorporating a video message is both recorded and read from the same AVSS. Having continued reference to FIG. 31, when the user, at <b>1404</b>, initiates a new e-mail message, the source e-mail system <b>1402</b> queries, at <b>1406</b>, whether the user intends to attach a video attachment to the e-mail. If the user intends to submit a video attachment with the e-mail message, a video authoring request is initiated at <b>1412</b> to video mail application <b>1420</b>. Video mail application <b>1420</b> creates a unique video file name at <b>1422</b> and requests of the recording AVSS <b>1430</b>, at <b>1424</b>, an encode session. The audio-video file is then recorded on AVSS <b>1430</b>. AVSS <b>1430</b> returns the video attachment at <b>1432</b> to video mail application <b>1420</b>. Video mail application <b>1420</b> in turn sets the file name pointer at <b>1434</b> for e-mail program <b>1402</b>. Once the e-mail message and video attachment have been completed, the user sends the e-mail, for instance using SMTP protocol <b>1410</b>, by initiating the send procedure at <b>1408</b>. Once send procedure <b>1408</b> is initiated, e-mail program <b>1402</b> initiates a notification at <b>1414</b> to video mail application <b>1420</b> that the message has been sent. Video mail application <b>1420</b> then instructs the AVSS <b>1430</b> to release the encode session at <b>1426</b>.</p>
    <p>At this point, the e-mail message itself, not shown in this FIG., has been sent to the user in the normal manner, and the recorded A/V file resides in AVSS <b>1430</b>. When the e-mail program <b>1402</b> receives the e-mail message at <b>1492</b>, it sends, at <b>1460</b>, a receipt target and information request to video mail application <b>1420</b>. Responsive to the receipt target and information request <b>1460</b>, video mail application <b>1420</b> extracts a source AVSS address at <b>1472</b>. Video mail application <b>1420</b> then makes a file transfer request <b>1480</b> to AVSS <b>1430</b>.</p>
    <p>Responsive to request <b>1480</b>, AVSS <b>1430</b> confirms it already has the requested A/V file, and confirms, at <b>1448</b>, that the video file transfer is completed. Responsive to file transfer confirmation <b>1448</b>, video mail application <b>1420</b> at <b>1474</b> releases the mail message after the A/V file is received. Video mail application <b>1420</b> then releases the mail message to e-mail system <b>1402</b> at <b>1462</b>.</p>
    <p>At this point, e-mail system <b>1402</b> makes the mail available to the user at <b>1494</b>. Steps <b>1494</b> and <b>1460</b>, in operative combination, ensure that the user is not notified of a message with an associated video attachment before the video attachment arrives. When the user opens the video attachment at <b>1496</b>, e-mail system <b>1402</b> initiates an attachment open event and sets a file pointer at <b>1464</b> to video mail application <b>1420</b>. In response to set file pointer <b>1464</b>, video mail application <b>1420</b> opens a viewer and prepares to decode the AV file at <b>1476</b>. Video mail application <b>1420</b> in turn requests a decode session from AVSS <b>1430</b> at <b>1482</b>. Decoding of the A/V file by AVSS <b>1430</b> renders the file available to the user.</p>
    <p>At <b>1498</b>, once the user closes the message or the video attachment, a view message/attachment and file pointer close event is initiated at <b>1466</b>. This causes video mail application <b>1420</b> to release the view and release the file at <b>1478</b>. Responsive to release event <b>1478</b>, video mail application <b>1420</b> issues, at <b>1484</b>, a decode session release command to AVSS <b>1430</b>. When, at <b>1493</b>, the user deletes either the message or its video attachment, e-mail system <b>1402</b> initiates a message/attachment delete event <b>1468</b> to video mail application <b>1420</b>. Responsive to this event, video mail application <b>1420</b> deletes, at <b>1473</b>, one holder of the file, and at <b>1486</b> releases user ownership of the file to AVSS <b>1430</b>. Alternatively, of course, a video file may be set to expire after a pre-assigned lifetime.</p>
    <p>It should be noted that receipt target and information request <b>1460</b>, release mail message <b>1462</b>, view message/attachment and file pointer close event <b>1466</b>, and message/attachment delete event <b>1468</b> as taught by this invention are novel concepts and as such are not supported by any known widely available e-mail domain servers. Study of the principles herein disclosed will make obvious to one having ordinary skill in the art that additional dialogs and displays in the video mail application can provide the information supplied by the previously listed messages. Further, a variety of alternate implementations are of course possible and the teachings of the present invention render them clear to those skilled in the art. These alternatives include workarounds for the absence of any or all of <b>1460</b>, <b>1462</b>, <b>1466</b>, and <b>1468</b>; for example, the unavailability of <b>1468</b> could be handled by a file lifetime monitor which takes action to delete a file after a designated period of inactivity.</p>
    <p>Referring now to FIG. 32, the transaction flow among application elements which facilitate wide-area video mail is shown, where a mail message incorporating a video message is transmitted from a recording, or source. AVSS <b>1430</b> to a target AVSS <b>1450</b>.</p>
    <p>When the user, at <b>1404</b>, initiates a new email message, the source email system, <b>1402</b>, queries at <b>1406</b> whether the user intends to attach a video attachment to the email. If the user intends to submit a video attachment with the email message, a video authoring request is initiated at <b>1412</b> to the video mail application program <b>1420</b>. Video mail application program <b>1420</b> creates a unique video filename at <b>1422</b> and requests of the recording AVSS <b>1430</b>, at <b>1424</b>, an encode session. The video attachment is then recorded on the recording AVSS, <b>1430</b>. AVSS <b>1430</b> then returns video attachment <b>1432</b> to video mail application <b>1420</b>. Video mail application program <b>1420</b> in turn sets the video filename pointer at <b>1434</b> for source email program <b>1402</b>. Once the email program and video attachment have been completed, the user sends the email, for instance using SMTP protocol <b>1410</b>, by initiating the send procedure at <b>1408</b>. Once send procedure <b>1408</b> is initiated, source email package <b>1402</b> initiates a notification to the video mail application <b>1420</b>, at <b>1414</b>, that the email message has been sent. Video mail application <b>1420</b> then instructs the recording AVSS <b>1430</b> to release the encode session, at <b>1426</b>.</p>
    <p>At this point the email message itself, not shown in this figure, has been sent to a remote AVSS utilizing normal SMTP methodology, while the encoded A/V file resides in recording AVSS <b>1430</b>. When the email message is received, at <b>1492</b>, at target email domain <b>1490</b> a receipt target and information request <b>1460</b> is sent to target video mail application <b>1470</b>. Responsive to the receipt target and information request <b>1460</b> target video mail application <b>1470</b> extracts the source AVSS address at <b>1472</b>. Target video mail application <b>1470</b> then sends target AVSS <b>1450</b> a file transfer request <b>1480</b>. This in turn initiates the request <b>1481</b>, by target AVSS <b>1450</b>, to recording AVSS <b>1430</b> for the A/V file.</p>
    <p>The recording AVSS, <b>1430</b>, transfers the A/V file, at <b>1436</b>, to the target AVSS <b>1450</b>. Target AVSS <b>1450</b> confirms to the target video mail application <b>1470</b>, at <b>1448</b>, that the video file transfer is completed. Responsive to file transfer confirmation <b>1448</b>, target video mail application, <b>1470</b>, at <b>1474</b> releases the mail message after the A/V file is received. Target video mail application <b>1470</b> then releases the mail message to the target email domain <b>1490</b> at <b>1462</b>.</p>
    <p>At this point target email domain <b>1490</b> makes the mail available to the user at <b>1494</b>. Steps <b>1494</b> and <b>1460</b>, in operative combination, ensure that the user is not notified of a message with an associated video attachment before the video attachment arrives. When the user opens the video attachment at <b>1496</b>, target email domain <b>1490</b> initiates an attachment open event and sets a file pointer, at <b>1464</b>, to target video mail application <b>1470</b>. In response to the set file pointer <b>1464</b>, target video mail application <b>1470</b> opens a view and prepares to decode the file for the user at <b>1476</b>. Target video mail application <b>1470</b> in turn requests a decode session from the target AVSS <b>1450</b> at <b>1482</b>. Decoding of the A/V file by the target AVSS <b>1450</b> renders the file available to the user.</p>
    <p>At <b>1498</b>, once the user closes the message or the video attachment, a view message/attachment and file pointer close event is initiated at <b>1466</b>. This causes the target video mail application <b>1470</b> to release the view and release the file at <b>1478</b>. Responsive to release event <b>1478</b>, target video mail application <b>1470</b> issues, at <b>1484</b>, a decode session release command to target AVSS <b>1450</b>. When, at <b>1493</b>, the user deletes either the message or its video attachment, target email domain <b>1490</b> initiates a message/attachment delete event <b>1468</b> to target video mail application <b>1470</b>. Target video mail application <b>1470</b> deletes, at <b>1473</b>, one holder of the file, and at <b>1486</b> releases user ownership of the file to target AVSS <b>1450</b>. Alternatively of course, video files can expire after a pre-assigned lifetime.</p>
    <p>It should be noted that receipt target and information request <b>1460</b>, release mail message <b>1462</b>, view message/attachment and file pointer close event <b>1466</b>, and message/attachment delete event <b>1468</b> taught by this invention are novel concepts and as such are not supported by any known widely available e-mail domain servers. Study of the principles herein disclosed will make obvious to one having ordinary skill in the art that additional dialogs and displays in the video mail application can provide the information supplied by the previously listed messages. Further, a variety of alternate implementations are of course possible and the teachings of the present invention renders them clear to those skilled in the art. These alternatives include workarounds for the absence of any or all of <b>1460</b>, <b>1462</b>, <b>1466</b>, and <b>1468</b>; for example, the unavailability of <b>1468</b> could be handled by a file lifetime monitor which takes action to delete a file after a designated period of inactivity.</p>
    <p>The exemplar discussed above and illustrated in FIG. 32 demonstrates a wide-area implementation of the present invention utilizing two systems, <b>1400</b> and <b>1440</b>. Shown in the figure is a first methodology whereby two or more instances of a conventional electronic mail system, two or more AVSSs, and two or more instances of a simple video mail software application â€œmiddlewareâ€ can be used to create a multi-AVSS, WAN-capable video mail system. This relatively simple network implementation is presented herein for clarity. Study of the principles herein disclosed will make obvious to one having ordinary skill in the art that a number of target systems <b>1440</b> may be similarly implemented. All such implementations are specifically contemplated by the principles of the present invention.</p>
    <p>The novel concepts of file ownership and video message life cycle are shown in FIGS. 33 and 34. In several of the applications taught herein, including video mail, file ownership evolves through the different phases of a message's lifetime. At message authoring, for example, the file is owned by the message author, while at message review the file is owned by the message recipient. This notion of changing file ownership is presented in FIG. 34, which more clearly points out both ownership and readability of the video files associated with a video mail attachment over the message lifetime. Not shown in the figure are the effects of messaging forwarding, but this is simply a second authorship/recipientship transaction exactly like that shown in the figure.</p>
    <p>Having reference now to FIG. 33, the life cycle of an exemplar video message file is discussed as follows:</p>
    <p>When a video file is created, at <b>1510</b>, associated with a message by means of a reference pointer <b>1512</b>, then transmitted, a mailing list <b>1514</b> is obtained from the conventional email system, <b>1513</b>, and presented to the recording AVSS at <b>1516</b>. Recording AVSS <b>1516</b> determines the recipients' addresses from mailing list <b>1514</b>, which are then associated with the name of their serving AVSS by means of directory services. The directory services provide a mechanism whereby names, e-mail address or login identities of users may be looked up and associated with their video address in a LAN or WAN environment. The AVNM server in the present embodiment could provide such directory services, or these services may be provided by well known directory protocols such as LDAP.</p>
    <p>At <b>1518</b> the programmatic elements of the present invention make a determination as to whether or not the mailing list <b>1514</b> associated with the video mail message is empty. If, at <b>1520</b>, it is determined that the mailing list is not empty, a further determination is made for each recipient in the list if the current recipient of the message is served by the recording AVSS at <b>1522</b>. If, at <b>1524</b> the determination is made that the current recipient is served by the recording AVSS the file is retained on the AVSS and the next recipient is chosen from the mailing list at <b>1526</b>. Responsive to the selection of the next recipient at <b>1530</b>, step <b>1518</b> is again invoked until the mailing list is empty. Responsive to a determination of <b>1528</b> that the current recipient is not on the recording AVSS the file is transferred to a target AVSS <b>1580</b>. The completion of the copy at <b>1529</b> returns to step <b>1526</b> to select the next recipient from the mailing list. Responsive to the selection of the next recipient, at <b>1530</b>, step <b>1518</b> is again invoked until the mailing list is empty.</p>
    <p>It is a principal feature of the present invention that the relatively large video files associated with multimedia and video communications be distributed where required and maintained thereat, but only for so long as required. In this manner the principles of the present invention present a heretofore unattained level of systems economy with respect to mass storage, bandwidth, and other system delimiters. Accordingly, the principles of the present invention specifically contemplate the automated deletion of video files once certain deletion criteria have transpired. Examples of these criteria include, but are specifically not limited to: the reading of a giving file by all of its intended recipients, the passage of a certain period of time, a certain number of invocations of the message, and other message accounting parameters well known to those having ordinary skill in the art. Having this deletion criteria in mind, a determination is made at <b>1582</b> if one or more deletion criteria has been met. In the event that the deletion criteria has not been met the system loops back, at <b>1586</b>, and returns to wait for the deletion criteria to be met at <b>1582</b>. Responsive to a determination, at <b>1584</b>, that the deletion criteria has been met at <b>1588</b> the file is deleted from the target AVSS at <b>1588</b>.</p>
    <p>Returning now to the loop through the mailing list at step <b>1518</b>, if, at <b>1540</b> a determination is made that the mailing list is empty a query is made a <b>1542</b> as to whether any file recipients are served by the recording AVSS. Responsive to a determination at <b>1540</b> that no file recipients are served by the recording AVSS, the file is deleted from the recording AVSS at <b>1550</b> and this function of the present invention is completed at <b>1552</b>. Responsive to a determination by step <b>1542</b> at <b>1544</b> that file recipients are served by the recording AVSS, a determination is made at <b>1545</b> at to whether file deletion criteria has been met. If a determination is made at <b>1546</b> that the file deletion criteria has not been met the system loops back and returns to wait for the deletion criteria to be met at <b>1545</b>. Responsive to a determination by step <b>1545</b> at <b>1560</b> that the file deletion criteria has been met the file is deleted from the recording AVSS at step <b>1550</b> which action completes this function of the present invention at <b>1552</b>.</p>
    <p>From the foregoing, several novel aspects of the present invention are made manifest.</p>
    <p>Within each AVSS, copies of the file can be distributed to one or more additional disks with the same AVSC or multiple AVSCs so as to diminish blocking to file access. This improves both system responsiveness and system reliability. In smaller implementations of the present invention, this file distribution utilizes full replication of the video files. In larger implementations, a â€œhashingâ€ mapping is utilized, and is explained as follows: where a file may be in high usage, rather than leaving it on only one AVSS, for instance the AVSS on which the file was authored, thereby leaving it only on the one disk which may result in unwanted delays in access time, the principles of the present invention contemplate the distribution of high usage files to one or more storage devices within an AVSC, or indeed to a number of differentAVSCs within the system. This may be accomplished by means of any of a number of hashing schemes known to those having ordinary skill in the art. This of course improves the availability of the file, and thus the reliability of the system which relies on the file.</p>
    <p>The life cycle advantages presented by an exemplar embodiment of the present invention include, but are not necessarily limited to, the following:</p>
    <p>(A) Video files are maintained only on those AVSSs serving a specified recipient for the video file, unless stored at another location for purposes of reliability and data redundancy.</p>
    <p>(B) Video files are deleted automatically once all messages which include the video file have been deleted; and</p>
    <p>(C) Where a recipient is located on an AVSS other than the recording AVSS, the video file is automatically transferred to the target AVSS. The transfer function may be implemented as â€œcopyâ€ or â€œreliable moveâ€.</p>
    <p>These advantages ensure that relatively large video files are transferred only where needed, and retained only on those AVSSs which require them, and then only for as long as the files are required. Unneeded files are automatically deleted from any AVSS when no longer required by any recipient served by that AVSS.</p>
    <p>Referring now to FIG. 34, during the recording and review processes, <b>1602</b> and <b>1604</b>, a given file is both readable and owned by its author. Once the author sends the file, at <b>1606</b>, it is still owned by the author but is unreadable. This condition continues through SMPT and video file transfer <b>1608</b> and the receive phase, <b>1610</b>. Once the recipient is notified, at <b>1612</b>, of the arrival of the email message and video file, both file readability and file ownership pass to the recipient. This condition continues through the message read and delete phases, <b>1614</b> and <b>1616</b> respectively. Of course, once the file is deleted it is neither owned nor readable.</p>
    <p>The author could select to be a recipient or simply remain an owner. The file would therefore remain readable by the author during the send <b>1606</b>, transfer <b>1608</b>, received <b>1610</b> and notified <b>1612</b> steps. The author would participate in the read <b>1614</b> and delete <b>1616</b> phases.</p>
    <p>Certain AVSS capabilities are required depending on the conference client implemented at individual desktops or rooms. When the previously discussed video mail system is implemented inside an enterprise incorporating desktops or rooms outfitted with workstation conference clients as described in U.S. Pat. No. 5,617,539, the following AVSS features are assumed:</p>
    <p>1. Generic control protocol interface/API.</p>
    <p>2. A generic multi-platform audio-video recording/playback utility (similar to vfstool) referred to herein as the â€œmode control GUIâ€ or â€œMCGâ€.</p>
    <p>3. Multi-session recording/playback capability.</p>
    <p>4. Analog A/V I/O.</p>
    <p>5. An AVSC decoder is co-allocated when an encoder is allocated. This ensures at the decoder is always available during the record session for review.</p>
    <p>6. Variable capacity scaling to match the needs of supported applications.</p>
    <p>7. File transfer capability (pull model) among AVSSs driven by specific file transfer requests made by applications.</p>
    <p>8. Effective â€œloopbackâ€ at the AVSC of incoming video to outgoing video during record mode.</p>
    <p>9. A video editing capability. This capability may be internally implemented as part of an applications program, or in the alternative, may be capable of invoking third party video and/or audio editors.</p>
    <p>Where desktops or rooms are not outfitted with the workstation conference clients taught in U.S. Pat. No. 5,617,539, but rather are fitted with standard MPEG or other encoder/decoders, the following additional capabilities are required:</p>
    <p>10. The capability to accept and playback appropriate digital files from third parties as controlled by applications.</p>
    <p>11. The capability to transfer appropriate digital files to third parties as controlled by applications.</p>
    <p>Sending video email naturally implies the creation of a video file. The principles of the present invention contemplate numerous video file creation scenarios. Video files may be pre-recorded, edited, and stored using the AVSS as a video file storage repository. They may be created at the same time the user is sending the video email. Video files may be imported from other video sources as described elsewhere herein, and attached to video mail messages. Finally, a video mail attachment may completely obviate the need for text with the message. In this latter embodiment of the present invention, the video attachment becomes the message, to the exclusion of text. Accordingly, it will be appreciated that substantially any methodology used to create a video file may be implemented to create video attachments for video mail, and are all comprehended by the teachings of the present invention.</p>
    <p>When a user wishes to send a new audio-video attachment to a video mail message, this is accomplished by creating a new audio-video file by selecting FILE: NEW from the viewer main menu and then recording the video. Video recording may comprehend, in one or more embodiments, several levels of editing capability. The most rudimentary of these enables a user to delete a file and then re-record it. Only slightly more sophisticated is an embodiment which enables a user to first review his recording, and then delete it if needed</p>
    <p>A further improvement is an editing scheme which enables the user to insert/delete/import material into a file. Finally, the video file is saved as an audio-video file on the local file system. The present invention contemplates each of these editing strategies in each of the applications taught herein.</p>
    <p>While the present invention contemplates the incorporation therein of the above-described editing features, it also contemplates the utilization of a third-party video editor for any and all video editing. The third-party editor may be invoked from the present invention by means of primitive, plug-in, invocation button, scripts, or other invocation methodologies well known to those having ordinary skill in the art.</p>
    <p>Where desired, the user then uses her normal email package to create an e-mail text message. She then attaches the AV file pointers to the email and then transmits or sends the email. An alternative embodiment contemplates a more closely integrated mail package which includes an Attach Video menu item in the mail package compose dialogue. To slightly simplify the previously discussed process, the Attach Video menu item would launch the viewer with the âˆ’n switch to indicate the viewer should automatically create a new video file and open an encode session.</p>
    <p>When a user receives an email with an audio-video attachment, he opens the attachment using his email reader. This launches the viewer to open the audio-video attachment. If the video is password protected, the user is prompted the supply the password. If the password is incorrect, the file is not opened. Alternatively, the viewer will in turn, if necessary launch the conference mode of operation. In the case where the file is stored on the AVSS to which the viewer is connected, when conference successfully connects to the AVSS, the VCR controls for playback are enabled on the viewer. Utilizing the play, seek, and rewind buttons of those controls the user plays the video and sees and hears the output at the workstation.</p>
    <p>In the case where the file is not stored locally, the AVSS will initiate a file transfer to receive the MPEG video from the remote AVSS. The user is notified that the transfer is taking place. When the file transfer is complete, VCR type controls for playback will be enabled on the viewer. Alternatively, the user could choose to connect to the remote AVSS in a manner similar to connecting to the local AVSS.</p>
    <p>The preceding discussion has centered on a first embodiment of the present invention. Alternative embodiments contemplated by the teachings herein disclosed include: the attachment of A/V files by means of pointing to file locations as opposed to direct use of MIME systems; invocation of one or more of the previously discussed processes at the opening or closing of an attachment, or the opening or closing of a message; A/V network and server socket management schemes; video viewer and location and close management methodologies; and screen cleanup and connection management methodologies.</p>
    <p>4.16.2 Video Answering System</p>
    <p>The video answering application utilizes the AVSS to record an audio and video message from an incoming caller whose call attempt is either not answered or is refused by the recipient. The present invention also contemplates the incoming caller simply wishing to leave a message without connecting to the recipient. One implementation of this feature of the present invention supports the case were no A/V codec trunks are available at that the time when a call is placed to a user. The video answering system includes several major components: an answering module; a browser module; a playback module; and a module wherein file transfers are potentially handled among AVSSs.</p>
    <p>The principles of the present invention contemplate several methodologies whereby the video answering application is invoked. One way of invoking video answering is to provide an option to a caller at any time during the â€œringâ€ cycle to leave a message for recipient. Additionally, a caller may be presented the option for leaving the message in any of the following cases: after a specified duration of a ring cycle, i.e., a ringing time-out; when a call is refused; and when the caller receives a busy signal due to an excessive number of waiting calls.</p>
    <p>When this application is invoked, the caller may be given the option of leaving a message in a number of ways. Such notification methodologies include, but are specifically not limited to: simple text notification; simple audio tone; an audio-only greeting; a video-only greeting; an audio-video greeting, and a multimedia greeting. The decision to implement any of these notification methodologies depends on several factors including available storage capacity, bandwidth, desired system response parameters, desired system â€œlook and feel,â€ as well as other system constraints well known to those having ordinary skill in the art.</p>
    <p>Several implementations contemplated for WAN installations include alternatives for placement of the user greetings, which alternatives include user greetings located on the user's AVSS, or alternatively be distributed to each AVSS on the WAN to be stored locally thereat. These alternatives depend on many systems installation factors including the number of users, system usage, bandwidth, and so forth. With respect to message authoring, the message may be recorded locally and sent to a remote site as in video mail, previously discussed. Alternatively the system may be configured such that it establishes a direct link to the receiving AVSS and the message is recorded on the received side. This later implementation results in more assuredly prompt delivery of the message, but requires more immediate bandwidth.</p>
    <p>The caller may explicitly choose to not to leave a message by means of either a specific response to a dialog box, or simply by activating the hang-up button on the call viewer.</p>
    <p>One implementation of this feature the present invention provides a caller with the opportunity to leave a message automatically either as part of an existing call status, e.g.: a user busy pop-up, with the automatic playing of a pre-recorded video greeting, or other automatic message system invocation methodologies. Once the user accepts the option to leave the message, a record session request is passed to the AVSS. When the request is granted, an MCG is provided, and an AVNM connection is established. In the event that the recipient attempts to answer the call when the caller is recording the message, a non-destructive escape is provided which enables the caller to complete the message and then connect the call to the called party. A similar non-destructive escape is utilized when the called party attempts to call back when the message recording is a process. The principles of the present invention further contemplate a destructive escape if the caller wishes to abandon the message during the recording session. Finally, in the event that the AVSS has insufficient resources to grant the record request, a fall back mode presents the caller with the option to leave a non-video message. Such a non-video message is provided by the â€œLeave Wordâ€ function of the incorporated reference.</p>
    <p>In one embodiment of the present invention, recipients are notified when there are messages present in their respective queues. Since there could be more than one message in queue at any given time, the video answering system provides a browser to enable the recipient to review the messages waiting in her queue. The browser can display information about the video messages, including, but not limited to: caller's name, time and date of call, video file name, playback duration, description, text note from caller, and creation time of video. Finally, a playback mode is provided to enable the recipient to play the messages selected for viewing.</p>
    <p>File ownership in the video answering application is as follows: When a message is being authored, the file is owned by the message's author. When the message is being reviewed, file ownership passes to the recipient, as identified by the AVNM.</p>
    <p>Referring now to FIG. 35, an overview of one embodiment of the video answering system of the present invention is discussed. When a caller, for instance at workstation <b>1802</b>, calls a recipient, for instance the user at workstation <b>1804</b>, and the video answering system is invoked as previously discussed, a request is sent from workstation <b>1802</b> to AVNM <b>1702</b>. AVNM <b>1702</b> then forwards a request at <b>1710</b> to video answer module <b>1704</b>, which in turn submits a session request to AVSS <b>1708</b> at <b>1714</b>. Responsive to session request <b>1714</b>, AVSS <b>1708</b> provides a reply, <b>1716</b>, to the video answer module.</p>
    <p>Responsive to reply <b>1716</b>, the video answer module <b>1704</b> establishes, at <b>1712</b>, a connection control with AVNM <b>1702</b> and notifies video playback module <b>1706</b> of the establishment of a video answering session at <b>1718</b>. Responsive to this notification, video playback module <b>1706</b> submits its own session request to AVSS <b>1708</b> at <b>1720</b>. Responsive to this session request, AVSS <b>1708</b>, at <b>1722</b>, establishes a second connection control with AVNM <b>1702</b> at <b>1722</b>.</p>
    <p>A data flow diagram detailing the operation of one embodiment of the present invention is given at FIG. <b>36</b>. Having reference to that figure, a caller initiates a video call request at <b>3502</b>. When the video call request is not accepted at <b>3504</b>, the recipient's greeting is invoked at <b>3506</b>. As previously discussed, the video call request may not be accepted for a number of reasons, including but specifically not limited to: the caller exercising her option at any time during the â€œringâ€ cycle to leave a message for the recipient; after a specified duration of a ring cycle, i.e., after a ringing time-out; when a call is refused; and when the caller receives a busy signal due to an excessive number of waiting calls.</p>
    <p>If the video call is not accepted, the recipient's greeting is played at <b>3506</b> and the caller is presented, at <b>3508</b>, with the option of leaving a message for the recipient. This option may again be presented in a number of manners, by way of example but not limitation including: a simple text notification; a simple audio tone: an audio-only greeting; a video-only greeting; an audio-video greeting; and a multimedia greeting. If the user, at <b>3510</b>, exercises her option not to leave a video message, the video answering session terminates at <b>3512</b>. In the alternative, if the user exercises her option, at <b>3514</b>, to leave a video message, she may create the message at <b>3516</b> in any of the manners previously discussed for the creation of video messages.</p>
    <p>Responsive to the creation, by the caller, of the video message at step <b>3516</b>, at <b>3518</b> the message is sent to the appropriate AVSS. At some later point in time, the message is delivered to the recipient at <b>3520</b>. Subsequent to the recipient invoking a browser, reader, or other file inspection methodology for the purpose of reading the message, not shown in this view, the user is presented with the option, at <b>3522</b>, of deleting the message.</p>
    <p>In the event the recipient elects, at <b>3524</b>, to delete the message, it is deleted from the AVSS at <b>3526</b>. In the alternative, where the recipient elects, at <b>3528</b>, not to delete the message, it retained on the AVSS at <b>3530</b>.</p>
    <p>The principles of the present invention contemplate two broad strategies for implementing the video answering system taught herein. A first broad strategy implements a â€œlayeredâ€ methodology, integrating a number of applications, some of which may be preexisting, into the video answering system of the present invention. One such layered implementation contemplates the use of the previously discussed video mail application as a message handling vehicle. This embodiment inherently defines many of the major file ownership issues. Further, the previously discussed authoring, browsing, playback, and notification functions are all provided by the video mail application itself. Another version of a layered implementation contemplates the utilization of a dedicated instance of a conventional email system program to provide dedicated video answering system message notification and browsing.</p>
    <p>A second broad strategy contemplates the creation of a unique â€œstand-aloneâ€ application to perform the previously defined functions. This strategy, which does not utilize video mail or other program as a message handler, requires the implementation of user interfaces to present the caller with the option of leaving a message. Moreover, such an implementation requires non-mail software and new user interfaces to accomplish the tasks of message notification, message browsing, and message playback.</p>
    <p>4.16.2.1 Layered Implementation of Video Answering System</p>
    <p>This implementation contemplates the use of the previously discussed video mail application, or other discrete application as a message handler or other system element for the video answering system. According to this strategy, after a video connection has been established responsive to a video call not being accepted, video mail can be authored on either the transmit side, i.e., the caller's AVSS, or on the receive side, i.e., the recipient's AVSS. Which implementation constitutes the embodiment is an economical decision, based on a number of system-specific factors.</p>
    <p>Video mail created responsive to the video answering system can be transmitted to the recipient off-line, i.e. in a non-real-time mode. This implementation would likely have minimal impact on system bandwidth in most embodiments. One option for this implementation is to author on the send side, and utilize a priority queue to transmit the message during a period of low bandwidth utilization to the recipient's AVSS. This moves the message out of real time and transmits it at a lower bit rate than authoring on the receive side, which would necessitate full-bandwidth connectivity. The alternative to this latter embodiment is of course to author video mail messages responsive to video answering system invocation on the receive side or transmit the message at the same baud rate as in video conferencing. Either embodiment is a systems administrator issue: conservation of bandwidth against a degree of time delay in the message responses. Where messages are needed almost immediately, the expense of bandwidth may be necessitated.</p>
    <p>Where the video message system is invoked responsive to the fact that no communications trunks are available between sites, it may still be desirable to leave a message in any event, for later transmission when communications become available. This functionality may be enabled by a script provided by the system administrator or other automated invocation methodology known to those having ordinary skill in the art.</p>
    <p>Where the enterprise is not too large, all greetings files from all users could be stored at all sites. Greetings files from all users at all sites enables a rich environment for video answering, where, even when there are no communications trunks available, a caller is greeted with an audio-visual greeting from any recipient. Again, this is an issue of capability versus storage capacity. Accordingly, the system administrator can determine where greetings are stored in accordance with the principles of the present invention.</p>
    <p>Where an enterprise generally experiences low communications volumes, messages can be recorded on the receive side to good effect. This embodiment minimizes time delays in getting mail messages to the recipients. Even in this embodiment, where trunks become unavailable, or encounter periods of high use, the message can be recorded on the callers side under control of a script, and then sent as a mail message when bandwidth is available.</p>
    <p>Other modifications to this embodiment include giving the caller the option, on the GUI, of prioritizing her message. When sending a file, e.g. an MPEG file, use of video mail enables a caller to transmit the file as a response to an answering machine response.</p>
    <p>There are video mail systems that are entirely separate from video conferencing systems. For example, when a caller is recording a video mail message, she may be recording it in MPEG-1 video format. When a caller is talking to a recipient in real time, she may be sending video in, for example, H.320 format. The principles of the present invention contemplate the implementation of video answering by invoking video mail in such a way that it doesn't use H.320. Alternatively, where H.320 is recorded, the principles of the present invention specifically contemplate the use of this same â€œliveâ€ format for both recorded and real time video applications. This implementation further contemplates the utilization of bandwidth management techniques, recording schemes, and video compression algorithms to minimize the bandwidth impact of the relative large video messages engendered by the use of H.320 format video, as well as some other video formats.</p>
    <p>The use of video mail as the message handler for the video answering application presents an additional complication in that it can be recording on the caller's side or on the recipient's side. The latter option ties the application more closely to the act of calling the recipient, thereby ensuring prompt delivery. In one embodiment of the present invention, a call-back feature is implemented, which automatically initiates a call back to the sender to talk to them in real time, or a reply-back feature, to which automatically invokes video mail to enable the recipient of a video message to reply with a video message of her own.</p>
    <p>Where video mail is utilized, an attachment, for instance a MIME attachment constructed as herein after discussed, may be included in a caller's message to the recipient, the activation of which enables the recipient to return the call of the original caller. Such a MIME attachment could initiate a dialog box or button which, when activated, launches a reply call and/or message response. This provides a replacement for the â€œLeave Wordâ€ function taught in U.S. Pat. No. 5,617,539. Alternatively, the principles of the present invention contemplate having the â€œLeave Wordâ€ function implemented, using it to notify the recipient of the receipt of a message, and where it can be found, e.g., the recipient's mail queue. Alternatively, the â€œLeave Wordâ€ function could enable querying the message from the mail queue, or it could have an entirely separate account only for video mail.</p>
    <p>4.16.2.2 Stand-alone Implementation of Video Answering System</p>
    <p>In contrast to the previously discussed layered implementation of this application, an alternative embodiment of the present invention is characterized by being a stand-alone implementation of the video answering system of the present invention. This embodiment does not utilize any other programs, such as video mail, to provide the requisite message handling or other features of the video answering system. Many of the previously discussed engineering choices pertain to this embodiment, including but not limited to greetings used; location of stored greetings; how the greetings are transmitted; and whether messages are recorded remotely or locally. Because this embodiment does not utilize the functionality of other programs, these system engineering choices are not dependent on the message handling system of such an application to implement the answering machine function.</p>
    <p>Additionally, the GUI can be better tailored to its function as an answering machine when a stand-alone version is implemented. The stand-alone strategy obviates the need for the recipient to go to the mail system to check her mail file, or any other file specified for answering machine messages. This points out an inconvenience in those versions of video answering which implement video mail as a message handler: these implementations act more like mail systems and less like an answering machines. The stand-alone version has an increased potential for being more responsive to the user with fewer intermediate steps for the user to perform in order to access, review, and process her video answering system messages. A video mail-based system is set up as a stand-alone messaging environment, while an answering machine could be more easily tuned to the fact that the recipient had been called and been left a message. This makes the answering machine function much easier to use, and, in most implementations, has at least one less layer than mail system-based implementations. Moreover, the use of video mail-based systems may present limitations on greetings, file, and recording locations, etc.</p>
    <p>Another consideration is that, in some organizations, mail and answering machine functions may have very different urgencies. Accordingly, a call message may have a corporate priority above video mail messages. Where this is the case, answering system messages should be easier to get to, easier to review, and easier to respond to than mail messages in these organizations. In contrast, mail messages are more like memos: they are generally more thoughtfully produced documents. Moreover, mail messages not only have mail browsers associated therewith, but they typically also have one or more text fields. Accordingly, use of video mail as a message handler would likely present the recipient with one or more blank text fields when an answering machine message is displayed.</p>
    <p>As the use of a stand-alone answering system removes the extra layers inherent in the previously discussed video mail-based application, the recipient can see a listing of those persons who had called, like in the Leave Word function of the incorporated reference. This presents the user with the option of simply replying to the message by returning the call or by clicking on the entry to see what had been left in the answering machine messages: video; audio; or a combination thereof. In any case, response can be by message or by call. Accordingly, this embodiment contemplates the inclusion, in the message transmitted responsive to the video answering machine, of a broad array of video, audio, text, and multimedia attachments therein.</p>
    <p>4.16.3 Video Conference Recording</p>
    <p>The Video Conference Recording application allows users to record the audio-video portion of audio-video conferences.</p>
    <p>In conference recording for a call or conference of N participants, there are potentially N+1 possible viewpoints to record from: the conference as viewed by each individual and the conference as viewed as a sort of composite â€œglobal viewâ€, which comprehends the combined views, audio, video and multimedia, of all the conference participants. The most comprehensive form of a â€œglobal viewâ€ would be provided by recording each participant's video and audio separately in multiple concurrent record sessions, for later combination and playback.</p>
    <p>Although this extremely comprehensive embodiment would enable a reviewer to freely look back at each participant in full detail at any time, this approach is very re-source and disk-space intensive, as it requires a separate record for each participant. While the principles of the present invention specifically contemplate such an embodiment, for all but the most massively implemented hardware suites it will be appreciated that combining the several participants' views into a single global view file is generally preferred, as it is far less resource intensive. Accordingly, it will be appreciated that there is a strong overhead cost basis for recording only one â€œglobal viewâ€ video and audio signal.</p>
    <p>Referring now to FIG. 37, an overview of the video conference recording application of the present invention is discussed. When the user, for instance <b>1802</b>, desires to invoke the video conference recording feature taught herein, a request is sent to the AVNM <b>1810</b>. AVNM <b>1810</b> then forwards a request at <b>1812</b> to video conference recording application <b>1830</b> to initiate a conference recording session. Video conference recording application <b>1830</b> then submits a session request, <b>1814</b>, to AVSS <b>1816</b>. AVSS <b>1816</b> establishes connection control at <b>1818</b> with AVNM <b>1810</b>. Video conference recording application <b>1830</b> also allocates a conference bridge at <b>1820</b> to AVNM <b>1810</b>.</p>
    <p>A user invokes, at <b>1806</b>, a browser <b>1821</b> which in turn invokes a video playback application <b>1822</b>. Invocation of video playback application <b>1822</b> initiates a session request at <b>1824</b> to AVSS <b>1816</b>. Responsive to the session request, AVSS <b>1816</b> establishes connection control at <b>1820</b> with AVNM <b>1810</b>.</p>
    <p>Referring now to FIG. 38, an overview of several of the components of the present invention required to implement video conference recording is discussed. Having reference to that figure, a first workstation <b>1802</b> can comprise any of a number of the previously discussed workstation implementations including camera <b>2302</b>, microphone <b>2304</b>, monitor/video card <b>2306</b>, and speaker <b>2308</b>. These components may optionally be connected by means of cabling, or other connection means known to those having ordinary skill in the art, to additional hardware element add-on box <b>2310</b> which in turn is connected with MLAN switch <b>2206</b>. While the present invention contemplates the incorporation of a number of workstations to MLAN switch <b>2206</b>, only a second workstation, <b>1804</b>, is shown in this figure for purposes of illustrational clarity. Those having ordinary skill may art will recognize that a number of workstations may be so connected.</p>
    <p>MLAN switch <b>2206</b> is connected with AVNM <b>1810</b>, not shown in this view, at <b>2332</b>. A conference bridge <b>2208</b> and AVSS <b>1816</b> are severally connected to MLAN switch <b>2206</b>. Conference bridge <b>2208</b> further comprises a transceiver set <b>2336</b> in operative combination with MLAN switch <b>2206</b>. Connected to transceiver set <b>2336</b> are video switch <b>2340</b> and audio mixer <b>2342</b>. Further connected to video switch <b>2340</b> is a video mosaic generator <b>2346</b>.</p>
    <p>AVSS <b>1816</b> includes a further transceiver <b>2352</b>, an encoder <b>2354</b>, and a decoder <b>2356</b>. A storage device, <b>2408</b>, is coupled with encoder <b>2354</b> and decoder <b>2356</b>.</p>
    <p>When a point to point, or two-party, video conference is established, point-to-point connectivity is established between workstations <b>1802</b> and <b>1804</b> through MLAN switch <b>2206</b>, as shown in FIG. 39. A video signal <b>2902</b> is transmitted from camera <b>2302</b> of workstation <b>1802</b> to monitor <b>2320</b> of workstation <b>1804</b>. Moreover, an audio signal <b>2904</b> is transmitted from microphone <b>2304</b> to speaker <b>2322</b> of workstation <b>1804</b>. Similarly, camera <b>2316</b> of workstation <b>1804</b> transmits its video signal <b>2906</b> to monitor <b>2306</b> of workstation <b>1802</b>, while microphone <b>2318</b> transmits its audio signal <b>2908</b> to speaker <b>2308</b>. Having continued reference to that figure, the simple implementation of a conference between two users does not invoke either conference bridge <b>2208</b> or AVSS <b>1816</b>.</p>
    <p>The inclusion of a third party, or the implementation of the video conference recording request, invokes a conference bridge <b>2208</b> as shown in FIG. <b>40</b>. While this figure may be used to illustrate the principles of a multi-party conference call, only two workstations are illustrated herein for purposes of clarity. Having continued reference to the figure, the invocation of conference bridge <b>2208</b> is shown. In this case audio signals <b>3002</b> and <b>3004</b> are transmitted by means of MLAN switch <b>2206</b> through transceiver <b>23</b>.<b>36</b> to audio mixer <b>2342</b>, and thence to their respective recipients. In similar fashion video signals <b>2508</b> and <b>2502</b> are transmitted through MLAN switch <b>2206</b> through transceiver set <b>2336</b> and video switch <b>2340</b> to video mosaic generator <b>2346</b>. Mosaic generator <b>2346</b> transmits a signal including a mosaic video, <b>2510</b>, showing all users to video switch <b>2340</b>. Video switch <b>2340</b> then splits this video signal into signals <b>2506</b> and <b>2506</b>â€².</p>
    <p>When a user initiates a video conference recording request, the signal processing is substantially similar to that shown in FIG. 40, with the additions illustrated in FIG. <b>41</b>. Having reference to the latter figure, a third video signal <b>3104</b>, again comprising video mosaic <b>2510</b>, is split at video switch <b>2340</b> and transmitted through MLAN switch <b>2206</b> to AVSS <b>1816</b>. Similarly the audio signals <b>3002</b> and <b>3004</b> are combined by the audio mixer <b>2342</b> producing a summed audio signal <b>3102</b>. Summed audio signal <b>3102</b> is transmitted through MLAN switch <b>2206</b> to AVSS <b>1816</b>. The video mosaic <b>2510</b> and summed audio signal are received at transceiver <b>2352</b>, encoded at encoder <b>2354</b>, and stored on storage device <b>2408</b>. A subsequent request from a user for information stored on storage device <b>2408</b> is transmitted by means of decoder <b>2356</b> through MLAN switch <b>2206</b> and thence to the requesting users workstation.</p>
    <p>The present invention contemplates that a user may select between any of a number of video representations made available by mosaic generator <b>2346</b>. By way of illustration, but not limitation, three such alternatives are shown in FIGS. 42, <b>43</b>, and <b>44</b>. The previously discussed scenario is shown at FIG. 42 wherein user <b>1</b>, user <b>2</b>, and the AVSS all see a mosaic of both users. Referring now to FIG. 43, the view presented to each user, at their selection, is that of a close-up of the other user. Despite this selection, the AVSS continues to receive a mosaic of all users. Referring now to FIG. 44, user <b>1</b> in this view sees a mosaic of himself and user <b>2</b>, and user <b>2</b> sees a close-up of user <b>1</b>. As before, the AVSS continues to receive a mosaic of all users being recorded.</p>
    <p>The logic required to implement the previously discussed video conference recording application is presented in the data flow diagram of FIG. <b>45</b>. This logic may be implemented as software, hardware, firmware, or any combination of the foregoing. Having reference to that figure, a video conference recording request is initiated at <b>3402</b>. The principles of the present invention specifically contemplate a number of methodologies for initiating this request. One such methodology has been previously discussed, wherein a given user initiates a request for video conference recording. Such a request makes usable the recorded video conference to any user having permission thereto. An alternative to this embodiment contemplates the situation wherein, for legal or record purposes, it is desired that all video conferences be recorded. In this alternative, the administrator having administrative responsibility for the system taught and disclosed herein may mandate, at her option, that video conference recording be implemented for all video conferences conducted, or for certain specified video conferences, for instance between a specified set of users.</p>
    <p>Once the video conference recording request <b>3402</b> is initiated, video conference recording application <b>1830</b>, not shown in this view, is initiated at <b>3404</b>. When a conference bridge was not previously allocated, such a conference bridge is now allocated at <b>3406</b>. The initiation of the conference recording application initiates an AVSS session request at <b>3408</b> which in turn establishes connection control with AVNM <b>1810</b> at <b>3410</b>. This in turn enables AVNM connectivity with the users at <b>3412</b>.</p>
    <p>Conference bridge allocation <b>3406</b> enables the establishment of a network A/V connection from a MLAN switch <b>2206</b>, not shown in this figure, at <b>3414</b>. Video signals are transmitted to video switch <b>2340</b> at <b>3416</b>, which in turn transmits the signals to mosaic generator <b>2346</b> at <b>3420</b>. Mosaic generator <b>2346</b> in turn transmits the mosaic video back to video switch <b>2342</b>, at <b>3416</b>, from whence the mosaic video <b>2510</b> is transmitted to the user's and AVSS <b>1816</b> at <b>3418</b>.</p>
    <p>Referring again to step <b>3414</b>, audio signals are transmitted to audio mixer <b>2342</b> at <b>3422</b>. Audio mixer <b>2342</b> sends the summed audio signals at <b>3426</b> to AVSS <b>1816</b>. As shown at step <b>3424</b>, audio mixer <b>2342</b> further transmits to each user a copy of the summed audio signal, less that user's own audio signal.</p>
    <p>At this point mosaic video and summed audio are stored on storage device <b>2408</b> at step <b>3430</b>. When a user, having permissions, desires to access recorded video conference information, he may do so by means of browser <b>1821</b> invoked by MCG, not shown in this view, or other user interface at <b>3450</b>. Browser <b>1821</b> then accesses video playback application <b>1822</b> at <b>3452</b>. Step <b>3452</b> in turn initiates another AVSS session request at <b>3408</b>, enabling multiple concurrent recording and playback sessions by authorized users.</p>
    <p>Referring now to FIG. 46, the playback of recorded Video conference calls is explained. A previously recorded conference call is stored on storage device <b>2408</b> of AVSS <b>1816</b>. When playback of the conference call is invoked, it is transmitted at <b>4170</b> through decoder <b>2356</b>, which separates the recorded conference call into its composite video and audio signal components, <b>4180</b> and <b>4182</b> respectively. In the exemplar presented in FIG. 46, only two conference participants, at workstations <b>1802</b> and <b>1804</b>, are shown for purposes of illustrational clarity. Of course the principles of this embodiment of the present invention contemplate a larger plurality of user workstations.</p>
    <p>Video signal <b>4180</b> is transmitted through MLAN switch <b>2206</b> to transceiver <b>2336</b>, and thence through video switch <b>2340</b> to mosaic generator <b>2346</b>. Note that mosaic generator has inputs for each N conference participant plus one for video signal <b>4180</b>. In this manner, previously recorded video conferences are treated as additional conference participants during playback. The output from mosaic generator <b>2346</b> is a mosaic <b>4190</b> having N+1 frames: one for each participant, plus one for the recorded conference being played back. The output from mosaic generator <b>2346</b> is transmitted to video switch <b>2340</b>, where it is split into two video signals, <b>4188</b> and <b>4188</b>â€² and transmitted to workstations <b>1802</b> and <b>1804</b> as shown.</p>
    <p>A corresponding audio signal <b>4182</b> is simultaneously transmitted through MLAN switch <b>2206</b> and transceiver set <b>2336</b> to audio mixer <b>2342</b>. Audio mixer <b>2342</b> sums the audio inputs, <b>3004</b> and <b>3002</b>, from users at workstations <b>1802</b> and <b>1804</b> respectively with audio signal <b>4182</b> in the following manner: a first summed audio signal, <b>4184</b>, is transmitted to a first user at workstation <b>1802</b>, and is composed of the sum of audio signals from a second user at workstation <b>1804</b> and from the recorded video signal <b>4182</b>. A second summed signal <b>4186</b> is transmitted to a second user at workstation <b>1804</b>, and is composed of the sum of audio signals from the first user at workstation <b>1802</b> and from the recorded video signal <b>4182</b>. In this embodiment, note that in each case the workstation's own signal is not transmitted back to that workstation for purposes of improving audio clarity, but that the summed signal contains only audio from other â€œparticipantsâ€, which includes the signal from the recorded conference. The system may also maintain a summed audio signal for all participants.</p>
    <p>From the foregoing, the following features and advantages of a first embodiment the present invention are shown:</p>
    <p>1. The â€œglobal viewâ€ of the sum of all user's audio signals is used by the Video Conference application so as to capture all parts of the conversation.</p>
    <p>2. For multi-point conferences, the â€œglobal viewâ€ for both audio and video is readily obtained from the conference bridge hardware in the following manner:</p>
    <p>A. Global view video is naturally produced by the video mosaic box.</p>
    <p>B. Global view audio is provided by a special mix in the audio matrix mixer. This â€œglobalâ€ mix is exactly the one used in ganging distributed conference bridges. In other words, the â€œglobalâ€ mix is the audio signal recorded. A subset of the global mix is sent to each of the conference users.</p>
    <p>This subset includes the global mix less the user's own audio signal. This is necessary in order to preclude â€œechoingâ€ and other adverse audio effects.</p>
    <p>Each user is provided with a choice between global view video and a selected close-up view. This feature is enabled by the standard MCG.</p>
    <p>3. For point-to-point calls (i.e., a two-user conference), a signal capturing global view is generally not inherently available from within the hardware layer and is synthetically created as described.</p>
    <p>In multi-party conferences, a conference bridge is automatically allocated. Where a single global viewpoint is to be recorded, two-user calls mandate that a conference bridge be allocated by the video conference application. This conference bridge is most efficiently used if it comes from the same pool used for real-time multi-party conferences.</p>
    <p>In the case of protracted conference recordings it may be desirable to â€œtagâ€ points in time where certain discussion events occurred. One embodiment of the present invention contemplates the use of such tags. Tags are preferably given a unique identifier, for instance a searchable text label or letter, whereby a specific discussion event is uniquely identified.</p>
    <p>A user interface is provided to invoke and name a conference recording, and to invoke and operate the tag interface, where implemented. Additional user interfaces are implemented on the MCG for message notification; message browsing; and invocation of message playback.</p>
    <p>It should be further noted that the conference bridge is not sensitive as to the nature of the input source. Accordingly, by way of illustration but not limitation, input can be by means of security camera, VCR, or substantially any other video or audio source, and so forth. Moreover, a conference call can specifically include a previously recorded video file All these inputs can be recorded as they are discussed. From this, and as previously discussed, it follows that the system enables multiple simultaneous source sessions: e.g.: one or more recording sessions and one or more playback sessions, and they are all completely independent. Each is connected to the conference bridge, and the user interfaces are available to one or more users during the video meeting.</p>
    <p>The Video Conference Recording application previously discussed utilizes and implements the following capabilities:</p>
    <p>1. Generic control protocol interface/API.</p>
    <p>2. A generic multi-platform audio-video recording/playback utility referred to herein as the â€œmode control GUIâ€ or â€œMCGâ€.</p>
    <p>3. Multi-session recording/playback capability.</p>
    <p>4. Analog audio-video input and output.</p>
    <p>5. A decoder co-allocated when an encoder is allocated. This ensures that the decoder is always available during the record session for review.</p>
    <p>6. Variable capacity scaling to match the needs of supported applications, and hardware configurations.</p>
    <p>Moreover, the following capabilities may be required or advantageous, depending on how wide-area calls are handled, or where architecturally convenient.</p>
    <p>7. File transfer capability (pull model) among AVSSs driven by specific file transfer requests made by applications.</p>
    <p>8. Effective â€œloopbackâ€ at the AVSC of incoming video to outgoing video during record mode.</p>
    <p>9. A video editing capabilities. This capability may be internally implemented as part of an applications program, or in the alternative, may be capable of invoking third party video and/or audio editors.</p>
    <p>4.16.4 Video Documents</p>
    <p>The Video Documents application allows stored video files to be included in an on-line electronic document such as Wordâ„¢ or Framemakerâ„¢. This can be used for many enterprise-wide applications including, but not necessarily limited to: training, corporate memory, procedures, references, marketing, and so forth. There are also many potential overlaps with the Intranet and Internet, as discussed below.</p>
    <p>One embodiment of this feature of the present invention is shown in FIG. <b>19</b>. Having reference to that figure, substantially any archival video are combinable with text or documents. Moreover, the principles of the present invention comprehend video overlay graphics, which overlay the video, as well as a compendium of a plurality of video files which further include additional text or graphics, standalone videos, and MIME attached video files. Accordingly, the video document may bring up a video window alongside the document as viewed by the user, or overlaying the document, which enables the user to play the video while viewing the document. Video can come from any previously discussed source. Videos in a video document are sharable during a conference call or by video mail or messaging.</p>
    <p>An embodiment of the present invention contemplates attaching the viewer to a message as a MIME or other functional attachment. A given call may be expanded to include other participants and other mechanisms to merge the document into a running conference. In other words, by the establishment of another concurrent session. This is more than simple video as it enables synchronized graphics either overlaying the video or accompanying the video.</p>
    <p>Of course, any such implementation which necessitates the interface with third-party software raises some interoperability issues. Some of these issues include:</p>
    <p>The necessity, in some implementations, to make available an additional document type for the supported package using MIME extensions.</p>
    <p>The workstation Conference client video window and the MCG may be used for video delivery. However, in some applications of the principles of the present invention, it may make more aesthetic sense to include the video window and some associated controls, e.g., the playback control, in the document itself. One means of implementing this feature would be as a graphics type within an anchored frame.</p>
    <p>The necessity, in some implementations, to capture both â€œvideo typeâ€ delete events and â€œentire documentâ€ delete events so as to not fill the file system with unused video files.</p>
    <p>The necessity, again, in some implementations, for an automatic file transfer or user-warning mechanism to ensure that video files are transferred when the document is transferred to a domain not served by the authoring AVSS.</p>
    <p>Each of these issues is, of course, highly application specific. The implementation of specific solutions thereto is within the purview of those having ordinary skill in the art, in light of the teachings herein enumerated.</p>
    <p>The system can support video document applications either using conventional third-party document systems or document systems enhanced to offer the ability to obtain and or file copy events from the document system.</p>
    <p>The present invention can use word processing GUIs and the MCG. Accordingly, additional user interfaces are not required.</p>
    <p>The Video Documents application requires the following capabilities:</p>
    <p>1. Generic control protocol interface/API.</p>
    <p>2. A generic multi-platform audio-video recording/playback utility (similar to vfstool) referred to herein as the â€œmode control GUIâ€ or â€œMCGâ€.</p>
    <p>3. Multi-session recording/playback capability.</p>
    <p>4. Analog A/V I/O.</p>
    <p>5. An AVSC decoder is co-allocated when an encoder is allocated. This ensures that the decoder is always available during the record session for review.</p>
    <p>6. Variable capacity scaling to match the needs of supported applications.</p>
    <p>7. File transfer capability (pull model) among AVSSs driven by specific file transfer requests made by applications.</p>
    <p>8. Effective â€œloopbackâ€ at the AVSC of incoming video to outgoing video during record</p>
    <p>Additionally, the following feature is required to include other video file sources:</p>
    <p>9. The capability to accept and playback appropriate digital files from third parties as controlled by applications.</p>
    <p>Finally, in a first embodiment of the present invention, in order to support Internet access, the following feature is required:</p>
    <p>10. The capability to transfer appropriate digital files to third parties as controlled by applications.</p>
    <p>12. A video editing capabilities. This capability may be internally implemented as part of an applications program, or in the alternative, may be capable of invoking third party video and/or audio editors.</p>
    <p>4.16.5 General Purpose Video Storage</p>
    <p>In many cases there are needs for less sophisticated â€œrawâ€ video storage capabilities reachable from workstations over the video distribution network. These â€œrawâ€ needs amount to the functionality of a network-accessible VCR or writable laserdisk with its information organized according to file system conventions. Sources for the stored video content may be, for example, clips from broadcast news programs, copies of camcorder footage, captured segments from videotapes, video files transferred from the CDs, DVD, or the Internet. Although user workstations are likely involved with the acquisition, capture, or transfer of such video information (via auxiliary audio/video input jacks at the desktop workstation, file transfer actions, etc.), in general the genesis of this class of video is in contrast video captured from video calls, video conferences, message authoring, or video document authoring. Further, the storage of this class of â€œrawâ€ video information would not support supplementing the video with any annotations, text, shared graphics, etc.</p>
    <p>Current trends in computing lead users to treat such â€œrawâ€ video clips as any other file in their networked computing environment, freely copying individual copies of large video files and storing them on local disk or in standard data-network file servers. Such an approach has several disadvantages:</p>
    <p>such video only available at workstations with decoding capabilities for the needed video protocol and file format</p>
    <p>data networks and standard data file systems get heavily loaded given even a modest amount of video usage</p>
    <p>large numbers of private copies of large video files load local disks and file servers even further by huge multiples.</p>
    <p>It is obvious to one skilled in the art that the current invention easily addresses these shortcomings by offering:</p>
    <p>centralized shared encoding, decoding, and transcoding</p>
    <p>moving video over appropriate video-engineered networks rather than data networks</p>
    <p>sharing of a small number (one to a few) of these large-size files across the entire community of users who need access to the file</p>
    <p>still permitting file-transfer and digital streaming where required or desired.</p>
    <p>To do this, the video file formats used must match those supported by the AVSS.</p>
    <p>By appropriately designing the invention's handling of this class of video information, the video information can be freely exchanged with other types of applications. For example, subject to file permissions, video information can be exchanged between â€œrawâ€ video clips of the general purpose video storage application and other AVSS applications:</p>
    <p>a â€œrawâ€ video clip can be incorporated into a videomail message, video document, video publishing applications, video web pages, etc.</p>
    <p>a â€œrawâ€ video clip can be viewed within a video call or conference</p>
    <p>any video captured within a recorded video call, recorded video conference, video message, or other networked video applications could be captured as a â€œrawâ€ video clip.</p>
    <p>In some situations such exchange of video information between applications would involve an entire unaltered video file. The actual methods for the exchange for this case can be accomplished in at least one of two ways:</p>
    <p>creating a new application-owner of the existing file</p>
    <p>playing back the file as rendered video which is then re-recorded, possibly during viewing, by another networked video application</p>
    <p>Other methods are also possible for one skilled in the art.</p>
    <p>In other situations, only segments of an original video file are to be transferred between applications, or other edits of the original video file may be required. For these cases, the actual methods for the exchange could include at least one of the two following ways:</p>
    <p>use of a video file editor on the existing file to create a new (edited) file, and assigning the appropriate application-owner to this new file</p>
    <p>playing back the file as rendered video and, during viewing, re-record selected segments by means of another networked video application.</p>
    <p>Other methods are also possible for one skilled in the art.</p>
    <p>The General Purpose Video Storage application requires, in most implementations, the following capabilities:</p>
    <p>1. Generic control protocol interface/API.</p>
    <p>2. A generic multi-platform audio-video recording/playback utility (similar to vfstool) referred to herein as the â€œmode control GUIâ€ or â€œMCGâ€.</p>
    <p>3. Multi-session recording/playback capability.</p>
    <p>4. Analog A/V I/O.</p>
    <p>5. An AVSC decoder is co-allocated when an encoder is allocated. This ensures that the decoder is always available during the record session for review.</p>
    <p>6. Variable capacity scaling to match the needs of supported applications.</p>
    <p>7. File transfer capability (pull model) among AVSSs driven by specific file transfer requests made by applications.</p>
    <p>8. Effective â€œloopbackâ€ at the AVSC of incoming video to outgoing video during record.</p>
    <p>9. The capability to accept and playback appropriate digital files from third parties as controlled by applications.</p>
    <p>10. The capability to transfer appropriate digital files to third parties as controlled by applications.</p>
    <p>12. A video editing capabilities. This capability may be internally implemented as part of an applications program, or in the alternative, may be capable of invoking third party video and/or audio editors.</p>
    <p>In some limited domain cases, only features <b>1</b>-<b>8</b> may be required.</p>
    <p>4.16.6 Intranet Video Storage Utility</p>
    <p>Intranet refers to the use of Internet utilities, such as web pages, web browsers and web sites, as a means for distributing enterprise-internal information. In that web page authoring tools commonly accept MIME attachments, clearly the AVSS video MIME attachment service primitive can be used in a web page within an enterprise outfitted with the invention. As shown in FIGS. 3 and 4, the AVSS can readily be connected with the Internet to facilitate the use of its capabilities with web pages and other Internet utilities.</p>
    <p>Use of the AVSS video MIME attachment allows not only the attachment of video clips to arbitrary intranet web pages but also any other features that could be included in the MIME attachments, such as synchronized shareboard sessions, discussed later). Further, as any AVSS video file or AVSS MIME attachment can, subject to permissions, be transferred easily from AVSS-application to AVSS-application, the intranet can serve as another useful enterprise-internal video publishing method.</p>
    <p>It should be noted that other types of AVSS-based video applications could be written to serve as web page building blocks by those skilled in the art. Thus, the AVSS advantages described above can be obtained by those skilled in the art via means other than the AVSS MIME attachment.</p>
    <p>The Intranet Video Storage application enables raw audio-video files as well as video documents be stored, searched, and accessed within the enterprise, in accordance with Intranet browser interfaces and other conventions.</p>
    <p>The General Purpose Video Storage application requires, in most implementations thereof, the following capabilities:</p>
    <p>1. Generic control protocol interface/API.</p>
    <p>2. A generic multi-platform audio-video recording/playback utility (similar to vfstool) referred to herein as the â€œmode control GUIâ€ or â€œMCGâ€.</p>
    <p>3. Multi-session recording/playback capability.</p>
    <p>4. Analog A/V I/O.</p>
    <p>5. An AVSC decoder is co-allocated when an encoder is allocated. This ensures that the decoder is always available during the record session for review.</p>
    <p>6. Variable capacity scaling to match the needs of supported applications.</p>
    <p>7. File transfer capability (pull model) among AVSSs driven by specific file transfer requests made by applications.</p>
    <p>8. Effective â€œloopbackâ€ at the AVSC of incoming video to outgoing video during record</p>
    <p>9. The capability to accept and playback appropriate digital files from third parties as controlled by applications.</p>
    <p>12. A video editing capabilities. This capability may be internally implemented as part of an applications program, or in the alternative, may be capable of invoking third party video and/or audio editors.</p>
    <p>In some limited domain cases, only features <b>1</b>-<b>8</b> may be required.</p>
    <p>4.16.7 Internet Gateway</p>
    <p>Current trends in desktop computing technology, networking, and Internet usage has just started the emergence of video files and streaming over the Internet. These few early video exchanges are essentially all low resolution and usually low frame-rate, hence the resulting impact on limited Internet bandwidth, limited corporate network bandwidth, and limited enterprise file system space has been noticeable but manageable. As video rapidly increases in importance to business, limited Internet bandwidth, limited corporate network bandwidth, and limited enterprise file system space will be immediately taxed. Also to be noted is that increasing business use of video will demand increasing resolution and frame-rate, ballooning the bandwidth and file size requirement for each second of video information. Finally, current trends continue to encourage a plethora of video standards and protocols; to support all of them and their respective evolutions at each desktop workstation demands costly computing power, applications software, dedicated hardware, and administration; in many enterprises, this is completely intractable.</p>
    <p>As shown in FIGS. 3 and 4, the AVSS can readily be connected with the Internet. With this connectivity, the AVSS can be used as a gateway for incoming Internet video streams (which it can read into and thus convert into a file) as well as a proxy-server for incoming video file transfers from the Internet. Once these resulting video files are in the AVSS, perhaps initially captured as â€œrawâ€ video (as described earlier), they then can be viewed at user workstations or used in other AVSS applications. The AVSS can provide any transcoding operations that may be needed. This approach permits, at no appreciable additional cost, an enterprise outfitted with the invention to pull in large amounts of video information from the Internet, store it economically, and make it available to any user with a workstation fitted with the invention's low-cost audio-video networking hardware and software.</p>
    <p>The AVSS can also be used as a repository for video files accessed outside the enterprise via the Internet. The simplest implementation of this nature would simply make â€œrawâ€ video files available on the network. This could be extended to include annotation animations, generated for example by shareboard or a similar data-sharing application; such annotation animations could in fact be available stand-alone or temporally synchronized to audio, video, or audio-video material. Software viewers and players (with software-only video decoding, for example) could be created as application software for use with, or incorporation into, Internet browsers. The software viewers and players can be implemented and deployed in various ways in a manner readily understood by those skilled in the art, for example:</p>
    <p>an applet downloaded from the server over the Internet</p>
    <p>a full application downloaded from the server over the Internet, said application such that it can be installed on the Internet-user's personal computer</p>
    <p>a stand-alone application product bought and sold in the marketplace</p>
    <p>It is noted that all of the above functionality could further be extended in scope to include the capability of handling the full AVSS MIME attachments.</p>
    <p>The Internet Gateway application implements at least one of the following functions: the reception of incoming audio-video information from the internet; and the making of internal audio-video information digitally accessible to third parties on the Internet.</p>
    <p>Implementation of a first embodiment of this part of the present invention requires the following AVSS features:</p>
    <p>8. Effective â€œloopbackâ€ at the AVSC of incoming video to outgoing video during record</p>
    <p>9. The capability to accept and playback appropriate digital files from third parties as controlled by applications.</p>
    <p>12. A video editing capabilities. This capability may be internally implemented as part of an applications program, or in the alternative, may be capable of invoking third party video and/or audio editors.</p>
    <p>Where a remote user is outside the enterprise, the principles of the present contemplate the utilization of third-party browsers including, but specifically not limited to Netscape, Microsoft Internet Explorer, and/or FTP.</p>
    <p>Embodiments of the Internet Gateway application would typically require the following capabilities:</p>
    <p>1. Generic control protocol interface/API.</p>
    <p>2. A generic multi-platform audio-video recording/playback utility (similar to vfstool) referred to herein as the â€œmode control GUIâ€ or â€œMCGâ€.</p>
    <p>3. Multi-session recording/playback capability.</p>
    <p>4. Analog A/V I/O.</p>
    <p>5. An AVSC decoder is co-allocated when an encoder is allocated. This ensures that the decoder is always available during the record session for review.</p>
    <p>6. Variable capacity scaling to match the needs of supported applications.</p>
    <p>7. File transfer capability (pull model) among AVSSs driven by specific file transfer requests made by applications.</p>
    <p>8. Effective â€œloopbackâ€ at the AVSC of incoming video to outgoing video during record</p>
    <p>Additionally, at least one, and in some implementations both, of the following capabilities are required by some embodiments:</p>
    <p>9. The capability to accept and playback appropriate digital files from third parties as controlled by applications.</p>
    <p>10. The capability to transfer appropriate digital files to third parties as controlled by applications.</p>
    <p>12. A video editing capabilities. This capability may be internally implemented as part of an applications program, or in the alternative, may be capable of invoking third party video and/or audio editors.</p>
    <p>4.16.8 Video Publishing</p>
    <p>Video publishing is accomplished by a user posting video on the AVSS for viewing within the desktop collaboration system or throughout the standard data-based Internet/Intranet.</p>
    <p>Desktops (analog or digital) can access stored video documents, either over the LAN, the WAN, or the Internet. They can also be made available to CMW users, as well as outside workstations which have reduced graphics capability. Video publishing enables the publishing of the documents to the outside world, as well as the importing, from the outside world, of additional material, again including text, graphics, audio, video or multimedia files. This latter feature comprehends the import and export of a number of disparate video and other formats including, but specifically not limited to, MPEG1-7 Motion JPEG, Video for Windows, Quicktime, DVI, recorded H.320, wavelets, and wavelet-compressed fractals.</p>
    <p>The implementation of video publishing on the AVSS comprehends a transcoding methodology embodied as hardware, software or firmware to enable the transcoding among the several formats required, thereby making the video files substantially format transparent to the receiver thereof. Any user with a workstation and permissions from the system administrator can accordingly publish documents to any user on the net or any other person outside the net who has access to the files. This enables high quality video publishing from analog equipment without the expense of implementing digital equipment</p>
    <p>At some point, it becomes cost-prohibitive to maintain all the video files ever created by an enterprise on rapid online storage, such as magnetic or magneto-optical hard drives. At some point long term or mass storage must be implemented for outdated or seldom-accessed video files. The present invention contemplates the inclusion therein of an automated routine which determines the relative time importance of files and either moves them to and from one or more types of long term storage device, including mass storage, magnetic tape, or other long term storage technology well known to those having ordinary skill in the art, or which deletes the files from the system. This algorithm either presents the user with the option of archiving her video files, or does so automatically. This may be done by user script from the System Administrator, or other archival storage technology well known to those having ordinary skill in the art.</p>
    <heading>4.17 Multimedia Applications</heading> <p>The present invention enables the establishment of real time data sharing sessions between two workstations using shareboard or other synchronized data sharing technologies. The implementation of shareboard enables a given application to be invokable on each work ion involved in the video conference, video mail or other application taught herein. Any user having permission to access the file may then import onto his screen, as another window, a copy of the application onto that screen. Independent scrolling of each screen is enabled. On top of this application in the window the users may then independently point, draw, type text, and perform other graphics functions by means of graphics overlay files. The user can further grab other file programs and so forth and draw them into the conference window. Further, the users may independently telepoint the items on one or more of these applications. Each of the shareboard features are further explain in the incorporated reference.</p>
    <p>The present invention enables the shareboard features previously discussed to be extended to the use of stored applications. This enables a user to simultaneously narrated and telepoint, for instance point into the subject of interest and using language like â€œthis one here and that one over thereâ€. Accordingly, it will be appreciated that what is required is minimally the user's voice being synchronized with the actions of the pointer. The present invention not only enables this feature in real time, but further enables that this synchronized speech and graphics capability is storable and retrievable such that when the stored file is invoked the data, speech, and graphics synchronlzation remains accurate. This enables the file to be recorded, stored, and sent to another user as video mail, as an input in response to an answering system greeting, as a document, or substantially in any other file format taught or made obvious by the teachings of the present inventions. It should be noted that this storage includes underline bitmaps of any application so invoked.</p>
    <p>The shareboard reference, fully discussed n the incorporated reference, is used in a first embodiment of the present invention. This provides for a uniform user interface. As shareboard is an intricate feature of the video conference methodologies taught herein, the extension of its features to the other applications taught herein enables like functionality without the implementation of additional recording tools or user interfaces. In a very real sense the utilization of shareboard not only means that a user must learn only one interface, usable in real time or otherwise, but it also helps with the users ideation that he is enabled, by the principles of the present invention, to share data across the several applications forming the application suite herein. The combination of multimedia synchronization, previously discussed, combined with shareboard functionality solves a number of compatibility, data reuse, and user interface problems. Moreover, the user shareboard in operative combination with the multimedia synchronization previously discussed enables the implementation of the principles of the present invention utilizing very specific data storage formats as well as the sharing of bitmaps.</p>
    <p>A slightly more general case in the use of shareboard is application sharing. When an application is invoked and a plurality of conference users can simultaneously utilize it, for instance each entering data in a single shared spreadsheet program. The present invention is no longer operating with only a snapshot of a bitmap image, but rather is actually working in the process of the application and the several users are simultaneously changing files together.</p>
    <p>4.17.1 Multimedia Mail</p>
    <p>The Multimedia Mail application adds synchronized datashare, e.g., synchronized-Shareboard/T.120 recording, storage, browsing, and playback capabilities to the Video Mail application described in the previous section.</p>
    <p>This inclusion of this feature provides significant value in that mail messages can now include the same media options as real-time desktop video conferencing. Thus a user can telepoint to one number in a spreadsheet, and then circle another number in the spreadsheet, as she first smiles and then says â€œHow on earth could this number be half of this number?â€ and the entire cross-media message gets captured and conveyed as it would have been in a live desktop video conference, including all the subtle communications included by gesture, tone of voice, body language, and so forth.</p>
    <p>The full content of other multimedia storage applications with synchronized datashare, e.g., synchronized-Shareboard T.120 recording and playback capabilities. Thus a user can forward a multimedia answering application message, multimedia conference recording, and so forth with other authors or authoring venues in multimedia mail or other multimedia application taught herein. This application requires a new AVSS feature:</p>
    <p>11. Synchronized datashare, e.g., synchronized-shareboard/T.120 recording, storage, browsing, and playback. This event list information is best stored in a separate file from the MPEG A/V file; the pair of this event list and the MPEG A/V file could be called a â€œmulti-media meta-file.â€</p>
    <p>4.17.2 Multimedia Answering System</p>
    <p>The Multimedia Answering application adds synchronized datashare, e.g., synchronized-Shareboard/T.120 recording, storage, browsing, and playback capabilities to the Video Answering application described in the previous section.</p>
    <p>Utilization of this feature provides significant value-in that a user can leave a more complete message in comparatively very little time should the call recipient themselves or the necessary connectivity not be available at the time a call attempt is made. This is because all relevant graphic overlay can be included in either the recipient's greeting or the caller's message.</p>
    <p>Again, this application, like all other multimedia applications discussed herein utilizes the previously discussed feature <b>11</b>.</p>
    <p>4.17.3 Multimedia Conference Recording</p>
    <p>The Multimedia Conference Recording application adds synchronized datashare, e.g., synchronized-Shareboard/T.120 recording, storage, browsing, and playback capabilities feature <b>11</b> to the Video Conference Recording application described in the previous section. This ensures that all transactions spanning the full spectrum of media used in desktop teleconferences of two or more participants can be captured for later review.</p>
    <p>4.17.4 Multimedia Documents</p>
    <p>The Multimedia Documents application adds synchronized datashare, e.g., synchronized-Shareboard/T.120 recording, storage, browsing, and playback capabilities of feature <b>11</b> to the video documents application described in the previous section. This ensures that procedural descriptions leveraging video can also include synchronized telepointing and annotation. This is extraordinarily valuable in on-line training and reference material applications.</p>
    <p>4.17.5 General Purpose Multimedia Storage</p>
    <p>The General Purpose Multimedia Storage application adds synchronized datashare, e.g., synchronized-Shareboard/T.120 recording, storage, browsing, and playback capabilities feature <b>11</b> to the General Purpose Video Storage application described in the previous section. The advantages presented by such an implementation mirror those discussed above in the section entitled â€œMultimedia Documentsâ€.</p>
    <p>4.17.6 Intranet Multimedia Storage Utility</p>
    <p>The Intranet Multimedia Storage application adds synchronized datashare, e.g., synchronized-Shareboard/T.120 recording, storage, browsing, and playback capabilities of feature <b>11</b> to the Intranet Video Storage application described in the previous section. Again, the advantages presented thereby are similar to those discussed above in the section entitled â€œMultimedia Documentsâ€.</p>
    <p>The principles of the present invention have been discussed herein with reference to certain embodiments thereof. Study of the principles disclosed herein will render obvious to those having ordinary skill in the art certain modifications thereto. The principles of the present invention specifically contemplate all such modifications. The present invention may be practiced without any element disclosed herein.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5262875">US5262875</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 30, 1992</td><td class="patent-data-table-td patent-date-value">Nov 16, 1993</td><td class="patent-data-table-td ">Instant Video Technologies, Inc.</td><td class="patent-data-table-td ">Audio/video file server including decompression/playback means</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5581479">US5581479</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 15, 1993</td><td class="patent-data-table-td patent-date-value">Dec 3, 1996</td><td class="patent-data-table-td ">Image Telecommunications Corp.</td><td class="patent-data-table-td ">Information service control point, which uses different types of storage devices, which retrieves information as blocks of data, and which uses a trunk processor for transmitting information</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5594924">US5594924</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 18, 1995</td><td class="patent-data-table-td patent-date-value">Jan 14, 1997</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Multiple user multimedia data server with switch to load time interval interleaved data to plurality of time interval assigned buffers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5751336">US5751336</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 12, 1995</td><td class="patent-data-table-td patent-date-value">May 12, 1998</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Permutation based pyramid block transmission scheme for broadcasting in video-on-demand storage systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6212681">US6212681</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 29, 1996</td><td class="patent-data-table-td patent-date-value">Apr 3, 2001</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Information processing apparatus and method therefor in a data transfer network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6353699">US6353699</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 24, 1995</td><td class="patent-data-table-td patent-date-value">Mar 5, 2002</td><td class="patent-data-table-td ">Barry H. Schwab</td><td class="patent-data-table-td ">Method and apparatus for compiling audio/video information from remote sites into a final video program</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7003569">US7003569</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 20, 2001</td><td class="patent-data-table-td patent-date-value">Feb 21, 2006</td><td class="patent-data-table-td ">Cypress Semiconductor Corp.</td><td class="patent-data-table-td ">Follow-up notification of availability of requested application service and bandwidth between client(s) and server(s) over any network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7007098">US7007098</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 17, 2000</td><td class="patent-data-table-td patent-date-value">Feb 28, 2006</td><td class="patent-data-table-td ">Nortel Networks Limited</td><td class="patent-data-table-td ">Methods of controlling video signals in a video conference</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7058903">US7058903</a></td><td class="patent-data-table-td patent-date-value">Oct 24, 2000</td><td class="patent-data-table-td patent-date-value">Jun 6, 2006</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Image database jog/shuttle search</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7080219">US7080219</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 30, 2005</td><td class="patent-data-table-td patent-date-value">Jul 18, 2006</td><td class="patent-data-table-td ">Hitachi, Ltd.</td><td class="patent-data-table-td ">Storage controlling device and control method for a storage controlling device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7102691">US7102691</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 8, 2001</td><td class="patent-data-table-td patent-date-value">Sep 5, 2006</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Method and apparatus for remote use of personal computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7103912">US7103912</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 29, 2001</td><td class="patent-data-table-td patent-date-value">Sep 5, 2006</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">User authorization management system using a meta-password and method for same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7133408">US7133408</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 13, 2000</td><td class="patent-data-table-td patent-date-value">Nov 7, 2006</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Shared decoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7162528">US7162528</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 1, 2002</td><td class="patent-data-table-td patent-date-value">Jan 9, 2007</td><td class="patent-data-table-td ">The United States Of America As Represented By The Secretary Of The Navy</td><td class="patent-data-table-td ">Collaborative environment implemented on a distributed computer network and software therefor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7178152">US7178152</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 3, 2001</td><td class="patent-data-table-td patent-date-value">Feb 13, 2007</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Application programming interface for communication between audio/video file system and audio video controller</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7191220">US7191220</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 22, 2003</td><td class="patent-data-table-td patent-date-value">Mar 13, 2007</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Information processing device and method, and program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7249156">US7249156</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 6, 2001</td><td class="patent-data-table-td patent-date-value">Jul 24, 2007</td><td class="patent-data-table-td ">Lg Electronics Inc.</td><td class="patent-data-table-td ">Method of providing a file transfer service through a mobile communication network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7260090">US7260090</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 26, 2002</td><td class="patent-data-table-td patent-date-value">Aug 21, 2007</td><td class="patent-data-table-td ">Ontash &amp; Ermac, Inc.</td><td class="patent-data-table-td ">Analog gateway</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7262788">US7262788</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 27, 2004</td><td class="patent-data-table-td patent-date-value">Aug 28, 2007</td><td class="patent-data-table-td ">Fujifilm Corporation</td><td class="patent-data-table-td ">Conference support system, information displaying apparatus, machine readable medium storing thereon a plurality of machine readable instructions, and control method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7293066">US7293066</a></td><td class="patent-data-table-td patent-date-value">Jan 21, 2004</td><td class="patent-data-table-td patent-date-value">Nov 6, 2007</td><td class="patent-data-table-td ">Cisco Technology, Inc.</td><td class="patent-data-table-td ">Methods and apparatus supporting access to stored data</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7324069">US7324069</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 23, 2004</td><td class="patent-data-table-td patent-date-value">Jan 29, 2008</td><td class="patent-data-table-td ">Pixar</td><td class="patent-data-table-td ">Animation review methods and apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7346692">US7346692</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 9, 2002</td><td class="patent-data-table-td patent-date-value">Mar 18, 2008</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Information processing apparatus, information processing method, and program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7353199">US7353199</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 24, 1999</td><td class="patent-data-table-td patent-date-value">Apr 1, 2008</td><td class="patent-data-table-td ">Perfect Web Technologies, Inc.</td><td class="patent-data-table-td ">Method of moderating external access to an electronic document authoring development and distribution system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7356771">US7356771</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 26, 2002</td><td class="patent-data-table-td patent-date-value">Apr 8, 2008</td><td class="patent-data-table-td ">Openpages</td><td class="patent-data-table-td ">Adaptive content platform and method of using same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7444374">US7444374</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 27, 2000</td><td class="patent-data-table-td patent-date-value">Oct 28, 2008</td><td class="patent-data-table-td ">Michelle Baker</td><td class="patent-data-table-td ">Electronic mail software with modular integrated authoring/reading software components including methods and apparatus for controlling the interactivity between mail authors and recipients</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7464137">US7464137</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 8, 2006</td><td class="patent-data-table-td patent-date-value">Dec 9, 2008</td><td class="patent-data-table-td ">Cisco Technology, Inc.</td><td class="patent-data-table-td ">On-line conference recording system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7473040">US7473040</a></td><td class="patent-data-table-td patent-date-value">Oct 14, 2005</td><td class="patent-data-table-td patent-date-value">Jan 6, 2009</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">High definition camera pan tilt mechanism</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7504968">US7504968</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 13, 2004</td><td class="patent-data-table-td patent-date-value">Mar 17, 2009</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">Media data decoding device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7525584">US7525584</a></td><td class="patent-data-table-td patent-date-value">Jan 5, 2004</td><td class="patent-data-table-td patent-date-value">Apr 28, 2009</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Fast edge directed demosaicing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7545435">US7545435</a></td><td class="patent-data-table-td patent-date-value">Apr 15, 2005</td><td class="patent-data-table-td patent-date-value">Jun 9, 2009</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Automatic backlight compensation and exposure control</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7545758">US7545758</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 11, 2002</td><td class="patent-data-table-td patent-date-value">Jun 9, 2009</td><td class="patent-data-table-td ">Siemens Communications, Inc.</td><td class="patent-data-table-td ">System and method for collaboration summarization playback</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7548951">US7548951</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 25, 2005</td><td class="patent-data-table-td patent-date-value">Jun 16, 2009</td><td class="patent-data-table-td ">Pioneer Corporation</td><td class="patent-data-table-td ">Minute file creation method, minute file management method, conference server, and network conference system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7562141">US7562141</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 24, 2003</td><td class="patent-data-table-td patent-date-value">Jul 14, 2009</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Using an information image to perform a predetermined action</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7572073">US7572073</a></td><td class="patent-data-table-td patent-date-value">Oct 17, 2005</td><td class="patent-data-table-td patent-date-value">Aug 11, 2009</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Camera support mechanism</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7593539">US7593539</a></td><td class="patent-data-table-td patent-date-value">Apr 17, 2006</td><td class="patent-data-table-td patent-date-value">Sep 22, 2009</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Microphone and speaker arrangement in speakerphone</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7602141">US7602141</a></td><td class="patent-data-table-td patent-date-value">Oct 17, 2005</td><td class="patent-data-table-td patent-date-value">Oct 13, 2009</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Battery operated speakerphone and charging stand</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7634575">US7634575</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 9, 2003</td><td class="patent-data-table-td patent-date-value">Dec 15, 2009</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">Method and system for clustering data streams for a virtual environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7650621">US7650621</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 9, 2001</td><td class="patent-data-table-td patent-date-value">Jan 19, 2010</td><td class="patent-data-table-td ">United Video Properties, Inc.</td><td class="patent-data-table-td ">Systems and methods for providing storage of data on servers in an on-demand media delivery system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7657601">US7657601</a></td><td class="patent-data-table-td patent-date-value">Dec 30, 2005</td><td class="patent-data-table-td patent-date-value">Feb 2, 2010</td><td class="patent-data-table-td ">At&amp;T Intellectual Property, I,L.P.</td><td class="patent-data-table-td ">Methods and computer programs for formulating messaging platform capacity projections</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7667728">US7667728</a></td><td class="patent-data-table-td patent-date-value">Oct 17, 2005</td><td class="patent-data-table-td patent-date-value">Feb 23, 2010</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Video and audio conferencing system with spatial audio</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7667762">US7667762</a></td><td class="patent-data-table-td patent-date-value">Aug 1, 2006</td><td class="patent-data-table-td patent-date-value">Feb 23, 2010</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Dual sensor video camera</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7673315">US7673315</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 29, 2000</td><td class="patent-data-table-td patent-date-value">Mar 2, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">System and method for providing program criteria representing audio and/or visual programming</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7688345">US7688345</a></td><td class="patent-data-table-td patent-date-value">Oct 17, 2005</td><td class="patent-data-table-td patent-date-value">Mar 30, 2010</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Audio output in video conferencing and speakerphone based on call type</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7692683">US7692683</a></td><td class="patent-data-table-td patent-date-value">Oct 17, 2005</td><td class="patent-data-table-td patent-date-value">Apr 6, 2010</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Video conferencing system transcoder</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7717629">US7717629</a></td><td class="patent-data-table-td patent-date-value">Apr 14, 2006</td><td class="patent-data-table-td patent-date-value">May 18, 2010</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Coordinated camera pan tilt mechanism</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7720232">US7720232</a></td><td class="patent-data-table-td patent-date-value">Oct 14, 2005</td><td class="patent-data-table-td patent-date-value">May 18, 2010</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Speakerphone</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7720236">US7720236</a></td><td class="patent-data-table-td patent-date-value">Apr 14, 2006</td><td class="patent-data-table-td patent-date-value">May 18, 2010</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Updating modeling information based on offline calibration experiments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7760887">US7760887</a></td><td class="patent-data-table-td patent-date-value">Apr 17, 2006</td><td class="patent-data-table-td patent-date-value">Jul 20, 2010</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Updating modeling information based on online data gathering</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7783985">US7783985</a></td><td class="patent-data-table-td patent-date-value">Jan 4, 2006</td><td class="patent-data-table-td patent-date-value">Aug 24, 2010</td><td class="patent-data-table-td ">Citrix Systems, Inc.</td><td class="patent-data-table-td ">Systems and methods for transferring data between computing devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7805613">US7805613</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 30, 2003</td><td class="patent-data-table-td patent-date-value">Sep 28, 2010</td><td class="patent-data-table-td ">Time Warner Cable, Inc.</td><td class="patent-data-table-td ">Technique for recording entertainment programming content with limited memory capacity</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7826624">US7826624</a></td><td class="patent-data-table-td patent-date-value">Apr 18, 2005</td><td class="patent-data-table-td patent-date-value">Nov 2, 2010</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Speakerphone self calibration and beam forming</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7843508">US7843508</a></td><td class="patent-data-table-td patent-date-value">Aug 29, 2007</td><td class="patent-data-table-td patent-date-value">Nov 30, 2010</td><td class="patent-data-table-td ">Mediostream, Inc.</td><td class="patent-data-table-td ">Method and system for direct recording of video information onto a disk medium</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7864221">US7864221</a></td><td class="patent-data-table-td patent-date-value">Apr 17, 2006</td><td class="patent-data-table-td patent-date-value">Jan 4, 2011</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">White balance for video applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7864714">US7864714</a></td><td class="patent-data-table-td patent-date-value">May 2, 2005</td><td class="patent-data-table-td patent-date-value">Jan 4, 2011</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Capability management for automatic dialing of video and audio point to point/multipoint or cascaded multipoint calls</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7865596">US7865596</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 2, 2001</td><td class="patent-data-table-td patent-date-value">Jan 4, 2011</td><td class="patent-data-table-td ">Oracle America, Inc.</td><td class="patent-data-table-td ">Switching system for managing storage in digital networks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7886070">US7886070</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 15, 2008</td><td class="patent-data-table-td patent-date-value">Feb 8, 2011</td><td class="patent-data-table-td ">International Business Corporation</td><td class="patent-data-table-td ">Source updating for streaming based servers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7886330">US7886330</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 5, 2001</td><td class="patent-data-table-td patent-date-value">Feb 8, 2011</td><td class="patent-data-table-td ">Jlb Ventures Llc</td><td class="patent-data-table-td ">Enhanced home entertainment system with removable long-term storage</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7895328">US7895328</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 14, 2003</td><td class="patent-data-table-td patent-date-value">Feb 22, 2011</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">System and method for context-based serialization of messages in a parallel execution environment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7903137">US7903137</a></td><td class="patent-data-table-td patent-date-value">Apr 17, 2006</td><td class="patent-data-table-td patent-date-value">Mar 8, 2011</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Videoconferencing echo cancellers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7907164">US7907164</a></td><td class="patent-data-table-td patent-date-value">Apr 17, 2006</td><td class="patent-data-table-td patent-date-value">Mar 15, 2011</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Integrated videoconferencing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7907745">US7907745</a></td><td class="patent-data-table-td patent-date-value">Sep 17, 2009</td><td class="patent-data-table-td patent-date-value">Mar 15, 2011</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Speakerphone including a plurality of microphones mounted by microphone supports</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7926066">US7926066</a></td><td class="patent-data-table-td patent-date-value">Apr 5, 2005</td><td class="patent-data-table-td patent-date-value">Apr 12, 2011</td><td class="patent-data-table-td ">Openpages, Inc.</td><td class="patent-data-table-td ">Adaptive content platform and application integration with the platform</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7961232">US7961232</a></td><td class="patent-data-table-td patent-date-value">Jan 21, 2009</td><td class="patent-data-table-td patent-date-value">Jun 14, 2011</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Calculating interpolation errors for interpolation edge detection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7970150">US7970150</a></td><td class="patent-data-table-td patent-date-value">Apr 11, 2006</td><td class="patent-data-table-td patent-date-value">Jun 28, 2011</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Tracking talkers using virtual broadside scan and directed beams</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7970151">US7970151</a></td><td class="patent-data-table-td patent-date-value">Apr 11, 2006</td><td class="patent-data-table-td patent-date-value">Jun 28, 2011</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Hybrid beamforming</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7971144">US7971144</a></td><td class="patent-data-table-td patent-date-value">Oct 31, 2007</td><td class="patent-data-table-td patent-date-value">Jun 28, 2011</td><td class="patent-data-table-td ">Openpages</td><td class="patent-data-table-td ">Adaptive content platform and method of using same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7986335">US7986335</a></td><td class="patent-data-table-td patent-date-value">Apr 17, 2006</td><td class="patent-data-table-td patent-date-value">Jul 26, 2011</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Set top box videoconferencing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7990410">US7990410</a></td><td class="patent-data-table-td patent-date-value">Apr 17, 2006</td><td class="patent-data-table-td patent-date-value">Aug 2, 2011</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Status and control icons on a continuous presence display in a videoconferencing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7991167">US7991167</a></td><td class="patent-data-table-td patent-date-value">Apr 13, 2006</td><td class="patent-data-table-td patent-date-value">Aug 2, 2011</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Forming beams with nulls directed at noise sources</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8015495">US8015495</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 2003</td><td class="patent-data-table-td patent-date-value">Sep 6, 2011</td><td class="patent-data-table-td ">Groupserve It Trust Llc</td><td class="patent-data-table-td ">Centrifugal communication and collaboration method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8019809">US8019809</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 16, 2005</td><td class="patent-data-table-td patent-date-value">Sep 13, 2011</td><td class="patent-data-table-td ">Cox Communications, Inc.</td><td class="patent-data-table-td ">Tightly-coupled disk-to-CPU storage server</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8054336">US8054336</a></td><td class="patent-data-table-td patent-date-value">May 2, 2005</td><td class="patent-data-table-td patent-date-value">Nov 8, 2011</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">High definition pan tilt zoom camera with embedded microphones and thin cable for data and power</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8068541">US8068541</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 1, 2006</td><td class="patent-data-table-td patent-date-value">Nov 29, 2011</td><td class="patent-data-table-td ">Jan Harding Thomsen</td><td class="patent-data-table-td ">Systems and methods for transcoding bit streams</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8112488">US8112488</a></td><td class="patent-data-table-td patent-date-value">Dec 16, 2009</td><td class="patent-data-table-td patent-date-value">Feb 7, 2012</td><td class="patent-data-table-td ">At&amp;T Intellectual Property I, L.P.</td><td class="patent-data-table-td ">Methods and computer programs for formulating messaging platform capacity projections</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8116500">US8116500</a></td><td class="patent-data-table-td patent-date-value">Apr 17, 2006</td><td class="patent-data-table-td patent-date-value">Feb 14, 2012</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Microphone orientation and size in a speakerphone</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8120638">US8120638</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 2007</td><td class="patent-data-table-td patent-date-value">Feb 21, 2012</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Speech to text conversion in a videoconference</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8125508">US8125508</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 2007</td><td class="patent-data-table-td patent-date-value">Feb 28, 2012</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Sharing participant information in a videoconference</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8125509">US8125509</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 2007</td><td class="patent-data-table-td patent-date-value">Feb 28, 2012</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Facial recognition for a videoconference</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8139100">US8139100</a></td><td class="patent-data-table-td patent-date-value">Jul 11, 2008</td><td class="patent-data-table-td patent-date-value">Mar 20, 2012</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Virtual multiway scaler compensation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8149739">US8149739</a></td><td class="patent-data-table-td patent-date-value">Apr 14, 2006</td><td class="patent-data-table-td patent-date-value">Apr 3, 2012</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Background call validation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8190707">US8190707</a></td><td class="patent-data-table-td patent-date-value">Nov 16, 2007</td><td class="patent-data-table-td patent-date-value">May 29, 2012</td><td class="patent-data-table-td ">Citrix Systems, Inc.</td><td class="patent-data-table-td ">System and method for transferring data among computing environments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8191092">US8191092</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 19, 2002</td><td class="patent-data-table-td patent-date-value">May 29, 2012</td><td class="patent-data-table-td ">Jlb Ventures Llc</td><td class="patent-data-table-td ">Method and system for replacing/obscuring titles and descriptions of recorded content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8230475">US8230475</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 16, 2007</td><td class="patent-data-table-td patent-date-value">Jul 24, 2012</td><td class="patent-data-table-td ">At&amp;T Intellectual Property I, L.P.</td><td class="patent-data-table-td ">Methods and computer program products for subcontent tagging and playback</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8237765">US8237765</a></td><td class="patent-data-table-td patent-date-value">Jun 19, 2008</td><td class="patent-data-table-td patent-date-value">Aug 7, 2012</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Video conferencing device which performs multi-way conferencing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8237770">US8237770</a></td><td class="patent-data-table-td patent-date-value">Apr 18, 2006</td><td class="patent-data-table-td patent-date-value">Aug 7, 2012</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Audio based on speaker position and/or conference location</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8250195">US8250195</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 10, 2008</td><td class="patent-data-table-td patent-date-value">Aug 21, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Leveraging synchronous communication protocols to enable asynchronous application and line-of-business behaviors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8255570">US8255570</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 12, 2007</td><td class="patent-data-table-td patent-date-value">Aug 28, 2012</td><td class="patent-data-table-td ">Citrix Systems, Inc.</td><td class="patent-data-table-td ">Systems and methods of compression history expiration and synchronization</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8305421">US8305421</a></td><td class="patent-data-table-td patent-date-value">Jun 29, 2009</td><td class="patent-data-table-td patent-date-value">Nov 6, 2012</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Automatic determination of a configuration for a conference</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8307273">US8307273</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 30, 2002</td><td class="patent-data-table-td patent-date-value">Nov 6, 2012</td><td class="patent-data-table-td ">The Board Of Trustees Of The Leland Stanford Junior University</td><td class="patent-data-table-td ">Methods and apparatus for interactive network sharing of digital video content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8311129">US8311129</a></td><td class="patent-data-table-td patent-date-value">Dec 15, 2006</td><td class="patent-data-table-td patent-date-value">Nov 13, 2012</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Temporal video filtering</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8317622">US8317622</a></td><td class="patent-data-table-td patent-date-value">Sep 7, 2009</td><td class="patent-data-table-td patent-date-value">Nov 27, 2012</td><td class="patent-data-table-td ">Wms Gaming, Inc.</td><td class="patent-data-table-td ">Wagering game establishment data import/export architecture</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8319814">US8319814</a></td><td class="patent-data-table-td patent-date-value">Jun 19, 2008</td><td class="patent-data-table-td patent-date-value">Nov 27, 2012</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Video conferencing system which allows endpoints to perform continuous presence layout selection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8320448">US8320448</a></td><td class="patent-data-table-td patent-date-value">Nov 28, 2008</td><td class="patent-data-table-td patent-date-value">Nov 27, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Encoder with multiple re-entry and exit points</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8350891">US8350891</a></td><td class="patent-data-table-td patent-date-value">Nov 16, 2009</td><td class="patent-data-table-td patent-date-value">Jan 8, 2013</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Determining a videoconference layout based on numbers of participants</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8359014">US8359014</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 8, 2010</td><td class="patent-data-table-td patent-date-value">Jan 22, 2013</td><td class="patent-data-table-td ">Julia N M N Olincy</td><td class="patent-data-table-td ">I am driving/busy automatic response system for mobile phones</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8386238">US8386238</a></td><td class="patent-data-table-td patent-date-value">Nov 5, 2008</td><td class="patent-data-table-td patent-date-value">Feb 26, 2013</td><td class="patent-data-table-td ">Citrix Systems, Inc.</td><td class="patent-data-table-td ">Systems and methods for evaluating a sequence of characters</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8456510">US8456510</a></td><td class="patent-data-table-td patent-date-value">Feb 25, 2010</td><td class="patent-data-table-td patent-date-value">Jun 4, 2013</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Virtual distributed multipoint control unit</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8477173">US8477173</a></td><td class="patent-data-table-td patent-date-value">Dec 16, 2005</td><td class="patent-data-table-td patent-date-value">Jul 2, 2013</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">High definition videoconferencing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8487976">US8487976</a></td><td class="patent-data-table-td patent-date-value">Jan 19, 2007</td><td class="patent-data-table-td patent-date-value">Jul 16, 2013</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Participant authentication for a videoconference</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8495658">US8495658</a></td><td class="patent-data-table-td patent-date-value">Mar 30, 2011</td><td class="patent-data-table-td patent-date-value">Jul 23, 2013</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Adaptive content platform and application integration with the platform</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8514265">US8514265</a></td><td class="patent-data-table-td patent-date-value">Oct 2, 2008</td><td class="patent-data-table-td patent-date-value">Aug 20, 2013</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Systems and methods for selecting videoconferencing endpoints for display in a composite video image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8578076">US8578076</a></td><td class="patent-data-table-td patent-date-value">May 3, 2010</td><td class="patent-data-table-td patent-date-value">Nov 5, 2013</td><td class="patent-data-table-td ">Citrix Systems, Inc.</td><td class="patent-data-table-td ">Systems and methods for establishing a cloud bridge between virtual storage resources</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8581959">US8581959</a></td><td class="patent-data-table-td patent-date-value">Sep 6, 2012</td><td class="patent-data-table-td patent-date-value">Nov 12, 2013</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Video conferencing system which allows endpoints to perform continuous presence layout selection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8583751">US8583751</a></td><td class="patent-data-table-td patent-date-value">Sep 14, 2012</td><td class="patent-data-table-td patent-date-value">Nov 12, 2013</td><td class="patent-data-table-td ">Facebook, Inc.</td><td class="patent-data-table-td ">Providing an indication that a user of a communications system is composing a message</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8589957">US8589957</a></td><td class="patent-data-table-td patent-date-value">Jun 27, 2011</td><td class="patent-data-table-td patent-date-value">Nov 19, 2013</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Adaptive platform</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8612546">US8612546</a></td><td class="patent-data-table-td patent-date-value">Apr 30, 2012</td><td class="patent-data-table-td patent-date-value">Dec 17, 2013</td><td class="patent-data-table-td ">Citrix Systems, Inc.</td><td class="patent-data-table-td ">System and method for transferring data among computing environments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8619188">US8619188</a></td><td class="patent-data-table-td patent-date-value">Oct 19, 2010</td><td class="patent-data-table-td patent-date-value">Dec 31, 2013</td><td class="patent-data-table-td ">Mediostream, Inc.</td><td class="patent-data-table-td ">Method and system for direct recording of video information onto a disk medium</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8626822">US8626822</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 28, 2008</td><td class="patent-data-table-td patent-date-value">Jan 7, 2014</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">Method for implementing network resource access functions into software applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8633962">US8633962</a></td><td class="patent-data-table-td patent-date-value">Jun 19, 2008</td><td class="patent-data-table-td patent-date-value">Jan 21, 2014</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Video decoder which processes multiple video streams</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8635363">US8635363</a></td><td class="patent-data-table-td patent-date-value">Jun 25, 2010</td><td class="patent-data-table-td patent-date-value">Jan 21, 2014</td><td class="patent-data-table-td ">Citrix Systems, Inc.</td><td class="patent-data-table-td ">System, method and computer program product to maximize server throughput while avoiding server overload by controlling the rate of establishing server-side network connections</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8643695">US8643695</a></td><td class="patent-data-table-td patent-date-value">Feb 25, 2010</td><td class="patent-data-table-td patent-date-value">Feb 4, 2014</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Videoconferencing endpoint extension</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8713114">US8713114</a></td><td class="patent-data-table-td patent-date-value">Jun 6, 2011</td><td class="patent-data-table-td patent-date-value">Apr 29, 2014</td><td class="patent-data-table-td ">Facebook, Inc.</td><td class="patent-data-table-td ">Direct file transfer between subscribers of a communications systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8717404">US8717404</a></td><td class="patent-data-table-td patent-date-value">Apr 26, 2011</td><td class="patent-data-table-td patent-date-value">May 6, 2014</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Recording a videoconference based on recording configurations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8775557">US8775557</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 26, 2011</td><td class="patent-data-table-td patent-date-value">Jul 8, 2014</td><td class="patent-data-table-td ">Facebook, Inc.</td><td class="patent-data-table-td ">Transferring files</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8780166">US8780166</a></td><td class="patent-data-table-td patent-date-value">Apr 26, 2011</td><td class="patent-data-table-td patent-date-value">Jul 15, 2014</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Collaborative recording of a videoconference using a recording server</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8786665">US8786665</a></td><td class="patent-data-table-td patent-date-value">Apr 26, 2011</td><td class="patent-data-table-td patent-date-value">Jul 22, 2014</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Streaming a videoconference from a server including boundary information for client layout adjustment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8786666">US8786666</a></td><td class="patent-data-table-td patent-date-value">Apr 26, 2011</td><td class="patent-data-table-td patent-date-value">Jul 22, 2014</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Providing separate video and presentation streams to a recording server</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8786667">US8786667</a></td><td class="patent-data-table-td patent-date-value">Apr 26, 2011</td><td class="patent-data-table-td patent-date-value">Jul 22, 2014</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Distributed recording of a videoconference in multiple formats</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8786668">US8786668</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 2012</td><td class="patent-data-table-td patent-date-value">Jul 22, 2014</td><td class="patent-data-table-td ">Lifesize Communications, Inc.</td><td class="patent-data-table-td ">Sharing participant information in a videoconference</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20030007782">US20030007782</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 19, 2002</td><td class="patent-data-table-td patent-date-value">Jan 9, 2003</td><td class="patent-data-table-td ">Yakov Kamen</td><td class="patent-data-table-td ">Method and system for replacing/obscuring titles and descriptions of recorded content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20070157263">US20070157263</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 19, 2006</td><td class="patent-data-table-td patent-date-value">Jul 5, 2007</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Content management system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20080229137">US20080229137</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 12, 2007</td><td class="patent-data-table-td patent-date-value">Sep 18, 2008</td><td class="patent-data-table-td ">Allen Samuels</td><td class="patent-data-table-td ">Systems and methods of compression history expiration and synchronization</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100070878">US20100070878</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 2008</td><td class="patent-data-table-td patent-date-value">Mar 18, 2010</td><td class="patent-data-table-td ">At&amp;T Intellectual Property I, L.P.</td><td class="patent-data-table-td ">Providing sketch annotations with multimedia programs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100262489">US20100262489</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 9, 2009</td><td class="patent-data-table-td patent-date-value">Oct 14, 2010</td><td class="patent-data-table-td ">Robert Salinas</td><td class="patent-data-table-td ">Mobile enabled advertising and marketing methods for computer games, simulations, demonstrations, and the like</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100293470">US20100293470</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 12, 2009</td><td class="patent-data-table-td patent-date-value">Nov 18, 2010</td><td class="patent-data-table-td ">Microsoft Corporatioin</td><td class="patent-data-table-td ">Hierarchically-Organized Control Galleries</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110151852">US20110151852</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 8, 2010</td><td class="patent-data-table-td patent-date-value">Jun 23, 2011</td><td class="patent-data-table-td ">Julia Olincy</td><td class="patent-data-table-td ">I am driving/busy automatic response system for mobile phones</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110161314">US20110161314</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 21, 2010</td><td class="patent-data-table-td patent-date-value">Jun 30, 2011</td><td class="patent-data-table-td ">Rathod Yogesh Chunilal</td><td class="patent-data-table-td ">Method and system for managing resources for providers</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110261149">US20110261149</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 26, 2011</td><td class="patent-data-table-td patent-date-value">Oct 27, 2011</td><td class="patent-data-table-td ">Raphael Anuar</td><td class="patent-data-table-td ">Initiating Recording of a Videoconference via a Single User Interaction</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110314117">US20110314117</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 26, 2011</td><td class="patent-data-table-td patent-date-value">Dec 22, 2011</td><td class="patent-data-table-td ">Aol Inc.</td><td class="patent-data-table-td ">Transferring files</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120096084">US20120096084</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 13, 2010</td><td class="patent-data-table-td patent-date-value">Apr 19, 2012</td><td class="patent-data-table-td ">International Business Machines Incorporated</td><td class="patent-data-table-td ">Shared media experience distribution and playback</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120216223">US20120216223</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 30, 2012</td><td class="patent-data-table-td patent-date-value">Aug 23, 2012</td><td class="patent-data-table-td ">Jlb Ventures Llc</td><td class="patent-data-table-td ">Method and system for replacing/obscuring titles and descriptions of recorded content</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2133783A1?cl=en">EP2133783A1</a></td><td class="patent-data-table-td patent-date-value">Jun 9, 2008</td><td class="patent-data-table-td patent-date-value">Dec 16, 2009</td><td class="patent-data-table-td ">Deutsche Thomson OHG</td><td class="patent-data-table-td ">Data storage server, stored set of instructions, and method of controlling data access</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2007019366A2?cl=en">WO2007019366A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 3, 2006</td><td class="patent-data-table-td patent-date-value">Feb 15, 2007</td><td class="patent-data-table-td ">Canon Dev Americas Inc</td><td class="patent-data-table-td ">Enabling non real-time communication enabled devices to participate in real time communication scenarios</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2007097563A1?cl=en">WO2007097563A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 21, 2007</td><td class="patent-data-table-td patent-date-value">Aug 30, 2007</td><td class="patent-data-table-td ">Samsung Electronics Co Ltd</td><td class="patent-data-table-td ">Method and apparatus for managing audio/video (av) network</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2010028322A1?cl=en">WO2010028322A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 7, 2009</td><td class="patent-data-table-td patent-date-value">Mar 11, 2010</td><td class="patent-data-table-td ">Wms Gaming, Inc.</td><td class="patent-data-table-td ">Wagering game establishment data import/export architecture</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc709/defs709.htm&usg=AFQjCNFBXWYqUOVuuxerz7B8cqt9daJk7Q#C709S226000">709/226</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc725/defs725.htm&usg=AFQjCNEqTdrBzYZIjJoruRv4pdIQa0i0Wg#C725S145000">725/145</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc709/defs709.htm&usg=AFQjCNFBXWYqUOVuuxerz7B8cqt9daJk7Q#C709S204000">709/204</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc348/defs348.htm&usg=AFQjCNHmQdDBdWG-OR5Ys7dyx9RrqLjUZg#C348SE07081">348/E07.081</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0015160000">G06F15/16</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0017300000">G06F17/30</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06F0015173000">G06F15/173</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04M0003560000">H04M3/56</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04N0007140000">H04N7/14</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=H04L0012580000">H04L12/58</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04L12/58">H04L12/58</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04M3/567">H04M3/567</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=ZJNpBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=H04N7/147">H04N7/147</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">H04L12/58</span>, <span class="nested-value">H04M3/56M</span>, <span class="nested-value">H04N7/14A3</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Apr 18, 2012</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 10, 2010</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100616</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:INTELLECTUAL VENTURES FUND 61 LLC;REEL/FRAME:025339/0981</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">PRAGMATUS AV LLC, VIRGINIA</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 12, 2010</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">AVISTAR COMMUNICATIONS CORPORATION,CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE BY SECURED PARTY;ASSIGNOR:BALDWIN ENTERPRISES, INC.;US-ASSIGNMENT DATABASE UPDATED:20100216;REEL/FRAME:23928/118</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090115</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">INTELLECTUAL VENTURES FUND 61 LLC,NEVADA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:AVISTAR COMMUNICATIONS CORPORATION;US-ASSIGNMENT DATABASEUPDATED:20100216;REEL/FRAME:23928/222</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20091217</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:AVISTAR COMMUNICATIONS CORPORATION;REEL/FRAME:23928/222</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE BY SECURED PARTY;ASSIGNOR:BALDWIN ENTERPRISES, INC.;REEL/FRAME:23928/118</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:AVISTAR COMMUNICATIONS CORPORATION;REEL/FRAME:023928/0222</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE BY SECURED PARTY;ASSIGNOR:BALDWIN ENTERPRISES, INC.;REEL/FRAME:023928/0118</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">INTELLECTUAL VENTURES FUND 61 LLC, NEVADA</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">AVISTAR COMMUNICATIONS CORPORATION, CALIFORNIA</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 5, 2010</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COLLABORATION PROPERTIES, INC., CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LUDWIG, LESTER F.;BROWN, WILLIAM BLAKE;INN, YUL J.;AND OTHERS;REEL/FRAME:023731/0392;SIGNING DATES FROM 20091209 TO 20100102</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 30, 2009</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">AVISTAR COMMUNICATIONS CORPORATION, CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE OF SECURITY INTEREST IN INTELLECTUAL PROPERTY;ASSIGNOR:BALDWIN ENTERPRISES, INC., AS COLLATERAL AGENT;REEL/FRAME:023708/0861</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20091229</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">RELEASE OF SECURITY INTEREST IN INTELLECTUAL PROPERTY;ASSIGNOR:BALDWIN ENTERPRISES, INC., AS COLLATERAL AGENT;REEL/FRAME:23708/861</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">AVISTAR COMMUNICATIONS CORPORATION,CALIFORNIA</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 10, 2009</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">THE PATENTABILITY OF CLAIMS 1-14 IS CONFIRMED. CLAIM 15 IS DETERMINED TO BE PATENTABLE AS AMENDED. CLAIMS 16-22, DEPENDENT ON AN AMENDED CLAIM, ARE DETERMINED TO BE PATENTABLE.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 29, 2008</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080229</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 25, 2008</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 7, 2008</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">BALDWIN ENTERPRISES, INC., AS COLLATERAL AGENT, UT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:AVISTAR COMMUNICATIONS CORPORATION;REEL/FRAME:020325/0091</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20080104</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:AVISTAR COMMUNICATIONS CORPORATION;REEL/FRAME:20325/91</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">BALDWIN ENTERPRISES, INC., AS COLLATERAL AGENT,UTA</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 2, 2007</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">AVISTAR COMMUNICATIONS CORPORATION, CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">MERGER;ASSIGNOR:COLLABORATION PROPERTIES, INC.;REEL/FRAME:019910/0032</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20071001</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">MERGER;ASSIGNOR:COLLABORATION PROPERTIES, INC.;REEL/FRAME:19910/32</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">AVISTAR COMMUNICATIONS CORPORATION,CALIFORNIA</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 23, 2004</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COLLABORATION PROPERTIES, INC., CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CORRECTIVE ASSIGNMENT TO CORRECT THE ADDRESS OF THE ASSIGNEE PREVIOUSLY RECORDED ON  REEL 012619 FRAME 0405;ASSIGNORS:LUDWIG, LESTER F.;BROWN, WILLIAM BLAKE;INN, YUL J.;AND OTHERS;REEL/FRAME:015019/0688;SIGNING DATES FROM 20011115 TO 20020109</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COLLABORATION PROPERTIES, INC. 555 DOLPHIN DRIVE,</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CORRECTIVE ASSIGNMENT TO CORRECT THE ADDRESS OF THE ASSIGNEE PREVIOUSLY RECORDED ON REEL 012619 FRAME 0405. ASSIGNOR(S) HEREBY CONFIRMS THE CORRECTION OF ASSIGNEE S ADDRESS FROM REDWOOD CITY TO REDWOOD SHORES.;ASSIGNORS:LUDWIG, LESTER F. /AR;REEL/FRAME:015019/0688;SIGNING DATES FROM 20011115 TO 20020109</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 13, 2002</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COLLABORATION PROPERTIES, INC., CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LUDWIG, LESTER F.;BROWN, WILLIAM BLAKE;INN, YUL J.;AND OTHERS;REEL/FRAME:012619/0405;SIGNING DATES FROM 20011115 TO 20020109</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">COLLABORATION PROPERTIES, INC. 555 DOLPHIN DRIVE,</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LUDWIG, LESTER F. /AR;REEL/FRAME:012619/0405;SIGNING DATES FROM 20011115 TO 20020109</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U11tdy-F-N8DduNS1B07INwBNm2SQ\u0026id=ZJNpBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U15gkitnK4btmDZfR3ZpDO6WZlCkA\u0026id=ZJNpBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U3KzzVkVKeKFbC-JPJWwdgkRDUPqw","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Networked_video_multimedia_storage_serve.pdf?id=ZJNpBAABERAJ\u0026output=pdf\u0026sig=ACfU3U0BRc3UZPwkMIhq--7MZ3GhbVj2ew"},"sample_url":"http://www.google.com/patents/reader?id=ZJNpBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>