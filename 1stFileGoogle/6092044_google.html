<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6092044 - Pronunciation generation in speech recognition - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Pronunciation generation in speech recognition"><meta name="DC.contributor" content="James K. Baker" scheme="inventor"><meta name="DC.contributor" content="Gregory J. Gadbois" scheme="inventor"><meta name="DC.contributor" content="Charles E. Ingold" scheme="inventor"><meta name="DC.contributor" content="Joel W. Parke" scheme="inventor"><meta name="DC.contributor" content="Stijn Van Even" scheme="inventor"><meta name="DC.contributor" content="Dragon Systems, Inc." scheme="assignee"><meta name="DC.date" content="1997-3-28" scheme="dateSubmitted"><meta name="DC.description" content="A method of adding a word to a speech recognition vocabulary includes creating a collection of possible phonetic pronunciations from a spelling of the word and using speech recognition to find a pronunciation from the collection that best matches an utterance of the word. The collection is created by comparing the spelling to a rules list of letter strings with associated phonemes. The list is searched for a letter string from the spelling of length greater than one letter. The collection is limited to phonetic pronunciations containing phonemes associated with the letter string of length greater than one. In another method, a net of possible phonetic pronunciations of the word is created from the spelling and speech recognition is used to find the pronunciation from the net that best matches the utterance of the word. The invention also features methods of assigning a pre-filtering class to a word."><meta name="DC.date" content="2000-7-18" scheme="issued"><meta name="DC.relation" content="EP:0562138:A1" scheme="references"><meta name="DC.relation" content="US:4481593" scheme="references"><meta name="DC.relation" content="US:4489435" scheme="references"><meta name="DC.relation" content="US:4718094" scheme="references"><meta name="DC.relation" content="US:4783803" scheme="references"><meta name="DC.relation" content="US:4805218" scheme="references"><meta name="DC.relation" content="US:4805219" scheme="references"><meta name="DC.relation" content="US:4829576" scheme="references"><meta name="DC.relation" content="US:4833712" scheme="references"><meta name="DC.relation" content="US:5027406" scheme="references"><meta name="DC.relation" content="US:5208897" scheme="references"><meta name="DC.relation" content="US:5222188" scheme="references"><meta name="DC.relation" content="US:5293451" scheme="references"><meta name="DC.relation" content="US:5329609" scheme="references"><meta name="DC.relation" content="US:5428707" scheme="references"><meta name="DC.relation" content="US:5440663" scheme="references"><meta name="DC.relation" content="US:5497447" scheme="references"><meta name="DC.relation" content="US:5500920" scheme="references"><meta name="DC.relation" content="US:5623578" scheme="references"><meta name="DC.relation" content="US:5652828" scheme="references"><meta name="DC.relation" content="US:5748840" scheme="references"><meta name="DC.relation" content="US:5751906" scheme="references"><meta name="DC.relation" content="US:5765132" scheme="references"><meta name="DC.relation" content="US:5794189" scheme="references"><meta name="DC.relation" content="US:5815639" scheme="references"><meta name="DC.relation" content="US:5850627" scheme="references"><meta name="citation_reference" content="Asadi, Ayman, &quot;Automatic Modeling for Adding New Words to a Large Vocabulary . . . &quot;, ICASSP 91, vol. 1, pp. 305-308, 1991."><meta name="citation_reference" content="Asadi, Ayman, Automatic Modeling for Adding New Words to a Large Vocabulary . . . , ICASSP 91, vol. 1, pp. 305 308, 1991."><meta name="citation_reference" content="Asadi, et al.; &quot;Automatic Modeling for Adding New Words to a Large-Vocabulary Continuous Speech Recognition System&quot;; ICASSP 91 vol. 1; International Conference; pp. 305-308."><meta name="citation_reference" content="Asadi, et al.; Automatic Modeling for Adding New Words to a Large Vocabulary Continuous Speech Recognition System ; ICASSP 91 vol. 1; International Conference; pp. 305 308."><meta name="citation_reference" content="Bahl, et al.; &quot;A Maximum Likelihood Approach to Continuous Speech Recognition&quot;; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PAMI-5; No. 2, Mar. 1983."><meta name="citation_reference" content="Bahl, et al.; A Maximum Likelihood Approach to Continuous Speech Recognition ; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PAMI 5; No. 2, Mar. 1983."><meta name="citation_reference" content="Bahl, L.R., &quot;Adaptation of Large Vocabulary Recognition System&quot; ICASSP-92, vol. 1, pp. I477-480 Mar. 1992."><meta name="citation_reference" content="Bahl, L.R., &quot;Automatic High-Resolution Labeling of Speech Waveforms&quot;, IBM Technical Disclosure Bulletin, vol. 23, No. 7B, pp. 3466-3467, Dec. 1980."><meta name="citation_reference" content="Bahl, L.R., &quot;Automatic Phonetic Baseform Determination&quot;, ICASSP 91, vol. 1, pp. 173-176, May 1991."><meta name="citation_reference" content="Bahl, L.R., &quot;Automatic Selection of Speech Prototypes &quot; IBM Technical Disclosure Bulletin vol. 24, No. 4, pp. 2042-2043, Sep. 1981."><meta name="citation_reference" content="Bahl, L.R., &quot;Interpolation of Estimators Derived From Sparse Data&quot;, IBM Technical Disclosure Bulletin vol. 24, No. 4, pp. 2038-2041, Sep. 1981."><meta name="citation_reference" content="Bahl, L.R., Adaptation of Large Vocabulary Recognition System ICASSP 92, vol. 1, pp. I477 480 Mar. 1992."><meta name="citation_reference" content="Bahl, L.R., Automatic High Resolution Labeling of Speech Waveforms , IBM Technical Disclosure Bulletin, vol. 23, No. 7B, pp. 3466 3467, Dec. 1980."><meta name="citation_reference" content="Bahl, L.R., Automatic Phonetic Baseform Determination , ICASSP 91, vol. 1, pp. 173 176, May 1991."><meta name="citation_reference" content="Bahl, L.R., Automatic Selection of Speech Prototypes IBM Technical Disclosure Bulletin vol. 24, No. 4, pp. 2042 2043, Sep. 1981."><meta name="citation_reference" content="Bahl, L.R., Interpolation of Estimators Derived From Sparse Data , IBM Technical Disclosure Bulletin vol. 24, No. 4, pp. 2038 2041, Sep. 1981."><meta name="citation_reference" content="Bahl, Lalit, &quot;A Maximum LikeLihood Approach to Continuous Speech Recognition&quot;, IEEE Transactions on Patern Analysis and Machine Intelligence, vol. PAMI-5, No. 2, pp. 179-190, Mar. 1983."><meta name="citation_reference" content="Bahl, Lalit, A Maximum LikeLihood Approach to Continuous Speech Recognition , IEEE Transactions on Patern Analysis and Machine Intelligence, vol. PAMI 5, No. 2, pp. 179 190, Mar. 1983."><meta name="citation_reference" content="Das, S.K., &quot;System for Temporal Registration of Quasi-Phonemic Utterance Representations&quot;, IBM Technical Disclosure Bulletin, Bol. 23, No. 7A, pp. 3047-3050, Dec. 1980."><meta name="citation_reference" content="Das, S.K., System for Temporal Registration of Quasi Phonemic Utterance Representations , IBM Technical Disclosure Bulletin, Bol. 23, No. 7A, pp. 3047 3050, Dec. 1980."><meta name="citation_reference" content="European Search Report dated Apr. 7, 1999."><meta name="citation_reference" content="Haeb Unbach, R., Automatic Transcription of Unknown Words in a Speech Recognition System , The 1995 International Conference on Acoustice, Speech, and Signal Processing, vol. 1, pp. 840 843, May 1995."><meta name="citation_reference" content="Haeb-Unbach, R., &quot;Automatic Transcription of Unknown Words in a Speech Recognition System&quot;, The 1995 International Conference on Acoustice, Speech, and Signal Processing, vol. 1, pp. 840-843, May 1995."><meta name="citation_reference" content="Hunnicutt, Sheri, &quot;Reversible Letter-to-Sound Sound-to-Letter Generation . . . &quot;, Eurospeech &#39;93, vol. 2, pp. 763-766."><meta name="citation_reference" content="Hunnicutt, Sheri, Reversible Letter to Sound Sound to Letter Generation . . . , Eurospeech 93, vol. 2, pp. 763 766."><meta name="citation_reference" content="Imai, Toru, &quot;ANew Method for Automatic Generation of Speaker-Dependent Phonological Rules&quot;, The 1995 International Conference on Acoustice, Speech, and Signal Processing, vol. 1, pp. 864-867, May 1995."><meta name="citation_reference" content="Imai, Toru, ANew Method for Automatic Generation of Speaker Dependent Phonological Rules , The 1995 International Conference on Acoustice, Speech, and Signal Processing, vol. 1, pp. 864 867, May 1995."><meta name="citation_reference" content="Kita, Kenji et al., &quot;Processing Unknown Words in Continuous Speech Recognition,&quot; IEICE Trans., vol. E74, No. 7 (Jul. 1991), pp. 1811-1815."><meta name="citation_reference" content="Kita, Kenji et al., Processing Unknown Words in Continuous Speech Recognition, IEICE Trans., vol. E74, No. 7 (Jul. 1991), pp. 1811 1815."><meta name="citation_reference" content="Merialdo B., &quot;Multilevel decoding for Very-Large-Size-Dictionary speech recognition&quot;, IBM J. Res. Develop., vol. 32, No. 2, Mar. 1988."><meta name="citation_reference" content="Merialdo B., Multilevel decoding for Very Large Size Dictionary speech recognition , IBM J. Res. Develop., vol. 32, No. 2, Mar. 1988."><meta name="citation_reference" content="Wothke, K., &quot;Morphologically based automatic phonetic transcription&quot;, IBM Systems Journal, vol. 32, No. 3, 1993."><meta name="citation_reference" content="Wothke, K., Morphologically based automatic phonetic transcription , IBM Systems Journal, vol. 32, No. 3, 1993."><meta name="citation_patent_number" content="US:6092044"><meta name="citation_patent_application_number" content="US:08/825,141"><link rel="canonical" href="http://www.google.com/patents/US6092044"/><meta property="og:url" content="http://www.google.com/patents/US6092044"/><meta name="title" content="Patent US6092044 - Pronunciation generation in speech recognition"/><meta name="description" content="A method of adding a word to a speech recognition vocabulary includes creating a collection of possible phonetic pronunciations from a spelling of the word and using speech recognition to find a pronunciation from the collection that best matches an utterance of the word. The collection is created by comparing the spelling to a rules list of letter strings with associated phonemes. The list is searched for a letter string from the spelling of length greater than one letter. The collection is limited to phonetic pronunciations containing phonemes associated with the letter string of length greater than one. In another method, a net of possible phonetic pronunciations of the word is created from the spelling and speech recognition is used to find the pronunciation from the net that best matches the utterance of the word. The invention also features methods of assigning a pre-filtering class to a word."/><meta property="og:title" content="Patent US6092044 - Pronunciation generation in speech recognition"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("qKDtU-8jxsWwBO7QgIAD"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("USA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("qKDtU-8jxsWwBO7QgIAD"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("USA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6092044?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6092044"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=V3NQBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6092044&amp;usg=AFQjCNEIETNJBW-BET3DI4NYsitCO2jG0g" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6092044.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6092044.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6092044" style="display:none"><span itemprop="description">A method of adding a word to a speech recognition vocabulary includes creating a collection of possible phonetic pronunciations from a spelling of the word and using speech recognition to find a pronunciation from the collection that best matches an utterance of the word. The collection is created by...</span><span itemprop="url">http://www.google.com/patents/US6092044?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6092044 - Pronunciation generation in speech recognition</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6092044 - Pronunciation generation in speech recognition" title="Patent US6092044 - Pronunciation generation in speech recognition"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6092044 A</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 08/825,141</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Jul 18, 2000</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Mar 28, 1997</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Mar 28, 1997</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/EP0874353A2">EP0874353A2</a>, </span><span class="patent-bibdata-value"><a href="/patents/EP0874353A3">EP0874353A3</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">08825141, </span><span class="patent-bibdata-value">825141, </span><span class="patent-bibdata-value">US 6092044 A, </span><span class="patent-bibdata-value">US 6092044A, </span><span class="patent-bibdata-value">US-A-6092044, </span><span class="patent-bibdata-value">US6092044 A, </span><span class="patent-bibdata-value">US6092044A</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22James+K.+Baker%22">James K. Baker</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Gregory+J.+Gadbois%22">Gregory J. Gadbois</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Charles+E.+Ingold%22">Charles E. Ingold</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Joel+W.+Parke%22">Joel W. Parke</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Stijn+Van+Even%22">Stijn Van Even</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Dragon+Systems,+Inc.%22">Dragon Systems, Inc.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6092044.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6092044.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6092044.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (26),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (33),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (32),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (5),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (13)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=V3NQBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6092044&usg=AFQjCNETZzxI5SBV2S6GfOQsmBTvjbS7SA">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=V3NQBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6092044&usg=AFQjCNHsI5bGt86UjT-LVGYvG9_UUxeM0A">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=V3NQBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6092044A%26KC%3DA%26FT%3DD&usg=AFQjCNH566XYiLSmj8JSPOVVDcend5Kbfg">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT54629539" lang="EN" load-source="patent-office">Pronunciation generation in speech recognition</invention-title></span><br><span class="patent-number">US 6092044 A</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA38098305" lang="EN" load-source="patent-office"> <div class="abstract">A method of adding a word to a speech recognition vocabulary includes creating a collection of possible phonetic pronunciations from a spelling of the word and using speech recognition to find a pronunciation from the collection that best matches an utterance of the word. The collection is created by comparing the spelling to a rules list of letter strings with associated phonemes. The list is searched for a letter string from the spelling of length greater than one letter. The collection is limited to phonetic pronunciations containing phonemes associated with the letter string of length greater than one. In another method, a net of possible phonetic pronunciations of the word is created from the spelling and speech recognition is used to find the pronunciation from the net that best matches the utterance of the word. The invention also features methods of assigning a pre-filtering class to a word.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(16)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-2.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-2.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-3.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-3.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-4.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-4.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-5.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-5.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-6.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-6.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-7.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-7.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-8.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-8.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-9.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-9.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-10.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-10.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-11.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-11.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-12.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-12.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-13.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-13.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-14.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-14.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-15.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-15.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-16.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-16.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/pages/US6092044-17.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/pages/US6092044-17.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(14)</span></span></div><div class="patent-text"><div mxw-id="PCLM5596439" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is:</claim-statement> <div class="claim"> <div num="1" class="claim">
      <div class="claim-text">1. A method of adding a word to a speech recognition vocabulary, comprising:<div class="claim-text">receiving a spelling of the word,</div> <div class="claim-text">receiving an utterance of the word,</div> <div class="claim-text">creating a collection of possible phonetic pronunciations of the word by:<div class="claim-text">comparing the spelling to a rules list of letter strings with associated phonemes, wherein the comparing includes searching the letter strings of the rules list for a letter string from the spelling of length greater than one letter, and</div> <div class="claim-text">limiting the collection of possible phonetic pronunciations to phonetic pronunciations containing phonemes associated with the letter string of length greater than one,</div> </div> <div class="claim-text">using speech recognition to find a best-matching pronunciation from the collection that best matches the utterance of the word, and</div> <div class="claim-text">adding the word to the speech recognition vocabulary using the spelling and the best-matching pronunciation.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" class="claim">
      <div class="claim-text">2. The method of claim 1 wherein said step of comparing includes searching the letter strings of the rules list for a longest length letter string from the spelling.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" class="claim">
      <div class="claim-text">3. The method of claim 2 wherein said step of limiting includes limiting the collection of possible phonetic pronunciations to phonetic pronunciations containing phonemes associated with the letter string of longest length.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" class="claim">
      <div class="claim-text">4. The method of claim 1 wherein the step of comparing includes starting the comparing with letter strings from the spelling beginning with a first letter of the spelling.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" class="claim">
      <div class="claim-text">5. The method of claim 4 wherein the step of comparing includes searching the letter strings of the rules list for an initial longest length letter string from the spelling beginning with the first letter.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" class="claim">
      <div class="claim-text">6. The method of claim 5 wherein the step of comparing includes searching the letter strings of the rules list for a subsequent longest length letter string from the spelling beginning with a letter following the last letter in the initial longest length letter string.</div>
    </div>
    </div> <div class="claim"> <div num="7" class="claim">
      <div class="claim-text">7. A method of adding a word to a speech recognition vocabulary, comprising:<div class="claim-text">receiving a spelling of the word,</div> <div class="claim-text">receiving an utterance of the word,</div> <div class="claim-text">creating a net of possible phonetic pronunciations of the word by comparing the spelling to a rules list of letter strings with associated phonemes,</div> <div class="claim-text">using speech recognition to find a best-matching pronunciation from the net that best matches the utterance of the word, and</div> <div class="claim-text">adding the word to the speech recognition vocabulary using the spelling and the best-matching pronunciation.</div> </div>
    </div>
    </div> <div class="claim"> <div num="8" class="claim">
      <div class="claim-text">8. A method of assigning a pre-filtering class to a word when adding a word to a speech recognition dictionary, the method comprising:<div class="claim-text">matching first two letters of the word to classified words starting with the same two letters to form a sub-list of classified words,</div> <div class="claim-text">matching phonemes of the word to phonemes of the classified words in the sub-list to form a class list, and</div> <div class="claim-text">placing the word in the class list.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" class="claim">
      <div class="claim-text">9. The method of claim 8 wherein the step of matching phonemes of the word to phonemes of the classified words includes matching the first four phonemes of the word to the first four phonemes of the classified words.</div>
    </div>
    </div> <div class="claim"> <div num="10" class="claim">
      <div class="claim-text">10. A method of assigning a pre-filtering class to a word when adding a word to a speech recognition dictionary, the comprising:<div class="claim-text">performing a direct look-up of a first phoneme of the word in a database of classified words organized alphabetically by their phonemes,</div> <div class="claim-text">matching the first phoneme of the word to a first word in the database having the same first phoneme,</div> <div class="claim-text">selecting the first word in the database having the same first phoneme and following words in the database to form a sub-list,</div> <div class="claim-text">matching the first phoneme of the word to the first phoneme of the classified words in the sub-list to form a class list, and placing the word in the class list.</div> </div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" class="claim">
      <div class="claim-text">11. The method of claim 10 wherein the step of matching the first phoneme of the word to a first word in the database having the same first phoneme includes matching the first four phonemes of the word to a first word in the database having the same first four phonemes.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" class="claim">
      <div class="claim-text">12. The method of claim 11 wherein the step of selecting the first word in the database having the same first phoneme includes selecting the first word in the database having the same first four phonemes.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" class="claim">
      <div class="claim-text">13. The method of claim 12 wherein the step of matching the first phoneme of the word to the first phoneme of the classified words in the sub-list includes matching the first four phonemes of the word to the first four phonemes of the classified words in the sub-list.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" class="claim">
      <div class="claim-text">14. The method of claim 10 wherein the step of selecting includes selecting the first word in the database having the same first phoneme and 199 following words in the database to form the sub-list.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES67503422" lang="EN" load-source="patent-office" class="description">
    <heading>BACKGROUND</heading> <p>The invention relates to generating pronunciations for words added to a dictation vocabulary used in a speech recognition system.</p>
    <p>A speech recognition system analyzes a person's speech to determine what the person said. Most speech recognition systems are frame-based. In a frame-based system, a processor divides a signal descriptive of the speech to be recognized into a series of digital frames, each of which corresponds to a small time increment of the speech. The processor then compares the digital frames to a set of speech models. Each speech model may represent a word from a vocabulary of words, and may represent how that word is spoken by a variety of speakers. A speech model also may represent a sound, or phoneme, that corresponds to a portion of a word. Collectively, the constituent phonemes for a word represent the phonetic spelling of the word.</p>
    <p>The processor determines what the speaker said by finding the speech models that best match the digital frames that represent the person's speech. The words or phrases corresponding to the best matching speech models are referred to as recognition candidates. The processor may produce a single recognition candidate for each utterance, or may produce a list of recognition candidates. Speech recognition is discussed in U.S. Pat. No. 4,805,218, entitled "METHOD FOR SPEECH ANALYSIS AND SPEECH RECOGNITION," which is incorporated by reference.</p>
    <p>A speech recognition system may be a "discrete" system--i.e., one which recognizes discrete words or phrases but which requires the speaker to pause briefly between each discrete word or phrase. Alternatively, a speech recognition system may be "continuous" meaning that the recognition software can recognize spoken words or phrases regardless of whether the speaker pauses between them. Continuous speech recognition systems typically have a higher incidence of recognition errors in comparison to discrete recognition systems due to complexities of recognizing continuous speech. A more detailed description of continuous speech recognition is provided in U.S. Pat. No. 5,202,952, entitled "LARGE-VOCABULARY CONTINUOUS SPEECH PREFILTERING AND PROCESSING SYSTEM," which is incorporated by reference.</p>
    <heading>SUMMARY</heading> <p>In one aspect, the invention features adding a word to a speech recognition vocabulary. A spelling and an utterance of the word are received a collection of possible phonetic pronunciations of the word are created. Speech recognition techniques are then used to find a pronunciation from the collection that best matches the utterance of the word. The word is then added to the speech recognition vocabulary using the spelling and the best-matching pronunciation.</p>
    <p>The collection of possible phonetic pronunciations of the word is created by comparing the spelling of the word to a rules list of letter strings with associated phonemes. The letter strings of the rules list are searched for a letter string from the spelling of length greater than one letter. The collection of possible phonetic pronunciations is limited to phonetic pronunciations containing phonemes associated with the letter string of length greater than one.</p>
    <p>Implementations may further include the following features. The letter strings of the rules list may be searched for a longest length letter string from the spelling. The collection of possible phonetic pronunciations may be limited to phonetic pronunciations containing phonemes associated with the letter string of longest length.</p>
    <p>The comparing may start with letter strings from the spelling beginning with a first letter of the spelling. The letter strings of the rules list may be searched for an initial longest length letter string from the spelling beginning with the first letter, and then may be searched for a subsequent longest length letter string from the spelling beginning with a letter following the last letter in the initial longest length letter string.</p>
    <p>In another aspect, the invention features adding a word to a speech recognition vocabulary by receiving a spelling and an utterance of the word, and creating a net of possible phonetic pronunciations of the word by comparing the spelling to a rules list of letter strings with associated phonemes. Speech recognition is used to find a pronunciation from the net that best matches the utterance of the word. The word then is added to the speech recognition vocabulary using the spelling and the best-matching pronunciation.</p>
    <p>In another aspect, the invention features assigning a pre-filtering class to a word by matching the first two letters of the word to classified words starting with the same two letters to form a sub-list of classified words. Phonemes of the word are matched to phonemes of the classified words in the sub-list to form a class list. The word then is placed in the class list.</p>
    <p>Implementations may further include the following features. Phonemes of the word may be matched to phonemes of the classified words by matching the first four phonemes of the word to the first four phonemes of the classified words.</p>
    <p>In another aspect, the invention features assigning a pre-filtering class to a word by performing a direct look-up of a first phoneme of the word in a database of classified words organized alphabetically by their phonemes, matching the first phoneme of the word to a first word in the database having the same first phoneme, selecting the first word in the database having the same first phoneme and a large number (e.g., 199) following words in the database to form a sub-list, matching the first phoneme of the word to the first phoneme of the classified words in the sub-list to form a class list, and placing the word in the class list.</p>
    <p>Implementations may further include the following features. The first phoneme of the word may be matched to a first word in the database having the same first phoneme by matching the first four phonemes of the word to a first word in the database having the same first four phonemes. The first word in the database having the same first four phonemes may be selected. The first phoneme of the word may be matched to the first phoneme of the classified words in the sub-list by matching the first four phonemes of the word to the first four phonemes of the classified words in the sub-list.</p>
    <p>Other features and advantages will become apparent from the following description, including the drawings, and from the claims.</p>
    <heading>BRIEF DESCRIPTION OF THE DRAWINGS</heading> <p>FIG. 1 is a block diagram of a speech recognition system.</p>
    <p>FIG. 2 is a block diagram of speech recognition software of the system of FIG. 1.</p>
    <p>FIG. 3 is a flow chart of a signal processing procedure performed by the software of FIG. 2.</p>
    <p>FIGS. 4A and 4B are state diagrams of a constraint grammar.</p>
    <p>FIG. 5 is a graph of a lexical tree.</p>
    <p>FIG. 6 is a graph of a portion of the lexical tree of FIG. 5.</p>
    <p>FIG. 7 is a flow chart of a pre-filtering procedure performed by the software of FIG. 2.</p>
    <p>FIGS. 8A, 8B and 8C are state graphs representing nodes of the lexical tree of FIG. 5.</p>
    <p>FIGS. 9 and 10 are charts of scores corresponding to the states of the state graphs of FIGS. 8A, 82 and 8C.</p>
    <p>FIG. 11 is a flow chart of a procedure for processing nodes of a lexical tree.</p>
    <p>FIG. 12 is a flow chart of a speech recognition procedure.</p>
    <p>FIG. 13 is a flow chart of a procedure for adding a word to a dictation vocabulary.</p>
    <p>FIG. 14 is a flow chart of a procedure for using a rules list to create a constraint grammar from a spelled word.</p>
    <p>FIG. 15 is a flow chart of another procedure for adding a word to a dictation vocabulary.</p>
    <p>FIG. 16 is an illustration of a net created from a spelled word.</p>
    <p>FIG. 17 is an illustration of a user interface for use when adding a word to a dictation vocabulary.</p>
    <p>FIG. 18 is a flow chart of a procedure for adding a word to a pre-filter class.</p>
    <p>FIG. 19 is a flow chart of another procedure for adding a word to a pre-filter class.</p>
    <p>FIG. 20 is a table relating phoneme symbols to human pronunciations.</p>
    <heading>DESCRIPTION</heading> <p>FIG. 1 is a block diagram of a speech recognition system 100. The system includes input/output (I/O) devices (e.g., microphone 105, mouse 110, keyboard 115, and display 120) and a general purpose computer 125 having a processor 130, an I/O unit 135 and a sound card 140. A memory 145 stores data and programs such as an operating system 150, an application program 155 (e.g., a word processing program), and speech recognition software 160.</p>
    <p>The microphone 105 receives the user's speech and conveys the speech, in the form of an analog signal, to the sound card 140, which in turn passes the signal through an analog-to-digital (A/D) converter to transform the analog signal into a set of digital samples. Under control of the operating system 150 and the speech recognition software 160, the processor 130 identifies utterances in the user's continuous speech. Utterances are separated from one another by a pause having a sufficiently-large, predetermined duration (e.g., 160-250 milliseconds). Each utterance may include one or more words of the user's speech.</p>
    <p>FIG. 2 illustrates components of the speech recognition software 160. For ease of discussion, the following description indicates that the components carry out operations to achieve specified results. However, it should be understood that each component actually causes the processor 130 to operate in the specified manner.</p>
    <p>Initially, a front end processing module 200 converts the digital samples 205 from the sound card 140 into frames of parameters 210 that represent the frequency content of an utterance. Each frame includes 24 parameters and represents a short portion (e.g., 10 milliseconds) of the utterance.</p>
    <p>As shown in FIG. 3, the front end processing module 200 produces a frame from digital samples according to a procedure 300. The module first produces a frequency domain representation X(f) of the portion of the utterance by performing a Fast Fourier Transform (FFT) on the digital samples (step 305). Next, the module determines log(X(f))<sup>2</sup> (step 310). The module then performs frequency warping (step 315) and a filter bank analysis (step 320) to achieve speaker normalization. See S. Wegmann et al., "Speaker Normalization on Conversational Speech," Proc. 1996 ICASSP, pp. I.339-I.341, which is incorporated by reference.</p>
    <p>From the normalized results, the module performs cepstral analysis to produce twelve cepstral parameters (step 325). The module generates the cepstral parameters by performing an inverse cosine transformation on the logarithms of the frequency parameters. Cepstral parameters and cepstral differences have been found to emphasize information important to speech recognition more effectively than do the frequency parameters. After performing channel normalization of the cepstral parameters (step 330), the module produces twelve cepstral differences (i.e., the differences between cepstral parameters in successive frames) (step 335) and twelve cepstral second differences (i.e., the differences between cepstral differences in successive frames) (step 340). Finally, the module performs an IMELDA linear combination transformation to select the twenty four most useful parameters from the twelve cepstral parameters, the twelve cepstral differences, and the twelve cepstral second differences (step 345).</p>
    <p>Referring again to FIG. 2, a recognizer 215 receives and processes the frames of an utterance to identify text corresponding to the utterance. The recognizer entertains several hypotheses about the text and associates a score with each hypothesis. The score reflects the probability that a hypothesis corresponds to the user's speech. For ease of processing, scores are maintained as negative logarithmic values. Accordingly, a lower score indicates a better match (a high probability) while a higher score indicates a less likely match (a lower probability), with the likelihood of the match decreasing as the score increases. After processing the utterance, the recognizer provides the best-scoring hypotheses to the control/interface module 220 as a list of recognition candidates, where each recognition candidate corresponds to a hypothesis and has an associated score. Some recognition candidates may correspond to text while other recognition candidates correspond to commands. Commands may include words, phrases or sentences.</p>
    <p>The recognizer 215 processes the frames 210 of an utterance in view of one or more constraint grammars 225. A constraint grammar, which also may be referred to as a template or restriction rule, may be a limitation on the words that may correspond to an utterance, a limitation on the order or grammatical form of the words, or both. For example, a constraint grammar for menu-manipulation commands may include only entries from the menu (e.g., "file", "edit") or command words for navigating through the menu (e.g., "up", "down", "top", "bottom"). Different constraint grammars may be active at different times. For example, a constraint grammar may be associated with a particular application program 155 and may be activated when the user opens the application program and deactivated when the user closes the application program. The recognizer 215 discards any hypothesis that does not comply with an active constraint grammar. In addition, the recognizer 215 may adjust the score of a hypothesis associated with a particular constraint grammar based on characteristics of the constraint grammar.</p>
    <p>As shown in FIG. 4A, which illustrates the constraint grammar for a "select" command used to select previously recognized text, a constraint grammar may be illustrated as a state diagram 400. The "select" command includes the word "select" followed by one more previously-recognized words, with the words being in the order recognized. The first state 405 of the constraint grammar indicates that the first word of the select command must be "select". After the word "select", the constraint grammar permits a transition along a path 410 to a second state 415 that requires the next word in the command to be a previously-recognized word. A path 420, which returns to the second state 415, indicates that the command may include additional previously-recognized words. A path 425, which exits the second state 415 and completes the command, indicates that the command may include only previously-recognized words. FIG. 4B illustrates the state diagram 450 of the constraint grammar for the select command when a previously-recognized utterance is "four score and seven". This state diagram could be expanded to include words from additional utterances.</p>
    <p>The constraint grammar also may be expressed in Backus-Naur Form (BNF) or Extended BNF (EBNF). In EBNF, the grammar for the "select" command is:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________ &lt;recognition result&gt; ::= &lt;"select"&gt;&lt;word&gt;,where &lt;word&gt; ::= &lt;[PRW<sup>1</sup> [PRW<sup>2</sup> [PWR3.. PRW<sup>n</sup> ]]] .linevertsplit.       [PRW<sup>2</sup> [PWR3.. PRW<sup>n</sup> ]] .linevert split. .. PRW<sup>n</sup>and PRW ::= &lt;previously-recognized word&gt;.______________________________________</pre>
    
    <p>As illustrated in FIGS. 4A and 4B, this notation indicates that "select" may be followed by any ordered sequence of previously-recognized words. Constraint grammars are discussed further in U.S. patent application Ser. No. 08/559,207, filed Nov. 13, 1995 and entitled "CONTINUOUS RECOGNITION OF SPEECH AND COMMANDS", which is incorporated by reference.</p>
    <p>One constraint grammar 225 that may be used by the speech recognition software 160 is a large vocabulary dictation grammar. The large vocabulary dictation grammar identifies words included in the active vocabulary 230, which is the vocabulary of words known to the software. The large vocabulary dictation grammar also indicates the frequency with which words occur. The large vocabulary dictation grammar may include a language model. The language model may be a unigram model that indicates the frequency with which a word occurs independently of context, or a bigram model that indicates the frequency with which a word occurs in the context of a preceding word. For example, a bigram model may indicate that a noun or adjective is more likely to follow the word "the" than is a verb or preposition. Other constraint grammars 225 include an in-line dictation macros grammar for dictation commands, such as "/CAP" to capitalize a word and "/New-Paragraph" to start a new paragraph; a select X Y Z grammar used in selecting text, an error correction commands grammar; a dictation editing grammar, an application command and control grammar that may be used to control a particular application program 155; a global command and control grammar that may be used to control the operating system 150 and the speech recognition software 160; a menu and dialog tracking grammar that may be used to manipulate menus; and a keyboard control grammar that permits the use of speech in place of input devices, such as the keyboard 115 or the mouse 110.</p>
    <p>The active vocabulary 230 uses a pronunciation model in which each word is represented by a series of phonemes that comprise the phonetic spelling of the word. In particular, each phoneme is represented as a triphone that includes three nodes. A triphone is a context-dependent phoneme. For example, the triphone "abc" represents the phoneme "b" in the context of the phonemes "a" and "c", with the phoneme "b" being preceded by the phoneme "a" and followed by the phoneme "c".</p>
    <p>One or more vocabulary files may be associated with each user. The vocabulary files contain all of the words, pronunciations and language model information for the user. Dictation and command grammars may be split between vocabulary files to optimize language model information and memory use, and to keep each single vocabulary file under 64K words. There also is a set of system vocabularies.</p>
    <p>Each dictation topic (e.g., "medical" or "legal") has its own vocabulary file. This allows the active dictation vocabulary to grow almost as large as 64 thousand words, and allows each dictation topic to have its own language model A dictation topic will consists of a set of words which represent the active vocabulary. There are around 30,000 words in each topic. This represents the words which are considered for normal recognition.</p>
    <p>Separate acoustic models 235 are provided for each user of the system. Initially speaker-independent acoustic models of male or female speech are adapted to a particular user's speech using an enrollment program. The acoustic models may be further adapted as the system is used. The acoustic models are maintained in a file separate from the active vocabulary 230.</p>
    <p>The acoustic models 235 represent each triphone node as a mixture of Gaussian probability density functions ("PDFS"). For example, node "i" of a triphone "abc" may be represented as ab<sup>i</sup> c: ##EQU1## where each w<sub>k</sub> is a mixture weight, ##EQU2## μ<sub>k</sub> is a mean vector for the probability density function ("PDF" ) N<sub>k</sub>, and c<sub>k</sub> is the covariance matrix for the PDF N<sub>k</sub>. Like the frames in the sequence of frames, the vectors μ<sub>k</sub> each include 24 parameters. The matrices c<sub>k</sub> are twenty four by twenty four matrices. Each triphone node may be represented as a mixture of up to sixteen different PDFS.</p>
    <p>A particular PDF may be used in the representation of multiple triphone nodes. Accordingly, the acoustic models represent each triphone node as a collection of mixture weights w<sub>k</sub> associated with up to sixteen different PDFs N<sub>k</sub> and separately represent each PDF N<sub>K</sub> using a mean vector μ<sub>k</sub> and a covariance matrix c<sub>k</sub>.</p>
    <p>The recognizer 215 operates in parallel with a pre-filtering procedure 240. Upon initiating processing of an utterance, the recognizer requests from the pre-filtering procedure a list of words that may have been spoken as the first word of the utterance (i.e., words that may correspond to the first and subsequent frames of the utterance). The pre-filtering procedure performs a coarse comparison of the sequence of frames with the active vocabulary 230 to identify a subset of the vocabulary for which a more extensive comparison using the recognizer is justified.</p>
    <p>Referring to FIGS. 5 and 6, the pre-filtering procedure 240 uses a lexical tree 500 that is initialized before processing begins. The lexical tree represents the active vocabulary 230 based on the phonetic relationships between words in the vocabulary. The lexical tree includes a root node 505 that represents new words entering the lexical tree. From the root node 505, the tree expands to a group 510 of nodes that correspond to phonemes with which words start. A silence node 512 that represents silence also may be reached from the root node 505.</p>
    <p>Each node in the group 510 represents a phoneme that appears at the beginning of one or more words. For example, in the portion 600 of the lexical tree 500 illustrated in FIG. 6, a node 610 corresponds to all words in the vocabulary that start with the phoneme "H". Together, the nodes in the group 510 include representations of the starting phoneme of every word in the vocabulary.</p>
    <p>The lexical tree continues to expand until it reaches leaf nodes 515 that represent the actual words of the vocabulary. For example, as indicated by the square marker, leaf node 615 of FIG. 6 corresponds to the word "healing". An internal node of the tree also may represent a word of the vocabulary. For example, the node 520 might represent a particular vocabulary word in addition to representing the first two phonemes of other vocabulary words. Similarly, the leaf node 620 of FIG. 6 corresponds to the words "heal" and "heel" while also corresponding to the first three phonemes of the words "heals", "heels" and "healing". Node 620 also illustrates that, since multiple words may have the same phonetic spelling, a leaf node may correspond to more than one word. As illustrated in FIG. 6, leaf nodes may appear at different levels within the lexical tree. Leaf nodes also may correspond to commands. For example, a leaf node may correspond to the word "select" and to the command "SELECT". As noted above, commands may be associated with particular constraint grammars 225.</p>
    <p>Operation of the pre-filtering procedure 240 is illustrated in FIG. 7. The pre-filtering procedure begins by retrieving the next frame of parameters for an utterance (step 700). Immediately after initialization, the next frame will be the first frame for the utterance. Thereafter, the next frame will be the frame following the last frame that was processed by the pre-filtering procedure when the pre-filtering procedure was last called. The pre-filtering procedure does not reinitialize the lexical tree between requests for list of words. Accordingly, the state of the lexical tree when a list of words is requested corresponds to the state of the lexical tree after a previous list of words was returned.</p>
    <p>After retrieving a frame of data, the pre-filtering procedure finds an active node in the tree with no unprocessed active successors (step 705). Successors of a node also may be referred to as subnodes of the node. When the lexical tree is initialized, the silence node 512 is the only active node.</p>
    <p>Next, the pre-filtering procedure processes the current node (step 710) according to a node-processing procedure 1100 that is discussed below with reference to FIG. 11. The node-processing procedure determines whether the node should spawn additional active nodes and whether the node should be rendered inactive. If the node is a leaf node, the node-processing procedure also determines whether the word corresponding to the node should be added to a word list for a time associated with the node.</p>
    <p>After processing the node (step 710), the pre-filtering procedure determines whether the node is the highest node in the tree (i.e., the root node) (step 715). If the node is not the highest node, then the pre-filtering procedure goes to the next node having no unprocessed active subnodes (step 720) and processes that node (step 710). When searching for the next node to process, the pre-filtering procedure considers inactive nodes having active subnodes or active siblings.</p>
    <p>If the processed node is the highest active node (step 715), then the pre-filtering procedure processes the silence node 512 (step 725). In general, the silence node is processed by comparing a frame to a model for silence and adding the resulting score to the minimum of the current score for the silence node and the score for the root node 505.</p>
    <p>Next, the pre-filtering procedure reseeds the lexical tree (step 730). The pre-filtering procedure reseeds the tree whenever the silence node 512 is active or a word was produced by a leaf node of the lexical tree, regardless of whether the word was added to the list of words. The pre-filtering procedure reseeds the tree by replacing the score for the root node 505 with the minimum of the score for the silence node 512 and the scores for any words produced by leaf nodes of the lexical tree for the current frame. If the silence node is inactive and no leaf node has produced a word, then the pre-filtering procedure replaces the score for the root node 505 with a bad score (i.e., a score having a value larger than a pruning threshold).</p>
    <p>Next, the pre-filtering procedure determines whether more words may be added to the word list for the requested time (step 735). If there are no active nodes in the lexical tree corresponding to speech that started at, before, or slightly after the start time for which the list was requested, and if the last frame to be processed corresponds to a time that is slightly after the start time for which the list was requested, then no more words may be added to the word list. A word produced by the lexical tree is added to the list of words corresponding to the start time of the word and to lists of words corresponding to times that precede and follow the start time of the word. It is for this reason that the pre-filtering procedure waits until there are no active nodes in the tree corresponding to speech that started slightly after the start time for the list of words. If more words may be added, then the pre-filtering procedure retrieves the next frame of parameters (step 700) and repeats the steps discussed above.</p>
    <p>If words cannot be added to the word list (step 735), then the pre-filtering procedure returns the word list (step 740) to the recognizer 215. If the word list includes more than a predefined number of words, then the pre-filtering procedure removes words from the list prior to returning the list. The pre-filtering procedure removes the words that are least likely to correspond to the user's speech and removes enough words to reduce the number of words on the list to the predefined number. The procedure also deletes any lists of words for times prior to the requested start time.</p>
    <p>Each node of the lexical tree 500 (FIG. 5) represents a sequence of states for a particular phoneme. For example, FIG. 8A illustrates a node 800 that includes a first state 805, a second state 810, and a third state 815. A comparison with a frame of parameters may cause the score in a particular state to remain in the state (through a path 820). A score remains in the state when the score, after being adjusted based on a comparison with a model for the state, is better than a score passed from a preceding state or node, or when no score is passed from a preceding state or node. The comparison also may cause the score to be passed to a subsequent state through a path 825. A score is passed to a subsequent state when the score, after being adjusted based on a comparison with a model for the subsequent state, is better than the score in the subsequent state, or when no score is associated with the subsequent state. The score for the third state 815 may be passed to one or more subsequent nodes through a path 830.</p>
    <p>Referring to FIG. 8B, the node 512 that corresponds to silence is represented by a single state 840. Each comparison with a frame of parameters may cause a score in the node to remain in the state 840 (through the path 845) and also may cause the score to be passed to the root node 505 through a path 850.</p>
    <p>Referring to FIG. 8C, the root node 505 is represented by a single state 860. Comparison with a frame causes the score in the node to be passed to one or more subsequent nodes (including the silence node 512) through a path 865.</p>
    <p>Each state of a node may be represented by four values: a score, a starting time, a leaving penalty, and a staying penalty. The score represents the likelihood that a series of frames has placed the lexical tree in the state (i.e., the probability that the series of frames corresponds to the word or portion of a word to which the state corresponds). The scores are maintained as negative logarithmic values.</p>
    <p>The starting time identifies the hypothesized time at which the user began to speak the word or words represented by the state. In particular, the starting time identifies the time at which the score associated with the state entered the lexical tree (i.e., the time at which the score was passed from the state 840 along the path 850).</p>
    <p>The leaving and staying penalties are fixed values associated with the state. The staying penalty is added to any score that stays in the state. The staying penalty is related inversely to the length of the sound represented by the state and to the length of the phoneme represented by the node to which the state belongs. For example, the staying penalty could be proportional to -log(1-1/d<sub>avg</sub>), where d<sub>avg</sub> is the average duration, in frames, of the sound represented by the state. Thus, the staying penalty has a relatively large value when the sound corresponding to the state occurs for only a small amount of time and a relatively small value when the sound corresponding to the state occurs for a large amount of time.</p>
    <p>The leaving penalty is added to any score that exits the state, and includes a duration component and a language model component. The duration component is related directly to the length of the sound represented by the state and to the length of the phoneme represented by the node to which the state belongs. For example, the duration component of the leaving penalty could be proportional to -log(1/d<sub>avg</sub>). Thus, the duration component of the leaving penalty has a relatively large value when the sound corresponding to the state occurs for a large amount of time and a relatively small value when the sound corresponding to the state occurs for a small amount of time.</p>
    <p>The language model components of the leaving penalties for all states in a particular node together represent a language model score for the phoneme associated with that node. The language model score represents the likelihood that a word including the phoneme will occur in speech. The language model score included in the leaving penalties for a node is the increase in the best language model score for the branch of the lexical tree that begins with the node relative to the branch of the lexical tree that begins with the node's parent.</p>
    <p>The following discussion assumes that there are no leaving or staying penalties associated with the state 840 or the state 860. The same result could be achieved by setting the leaving and staying penalties for states 840 and 860 equal to zero. The following discussion also assumes that the first frame is the first frame that may correspond to speech instead of silence.</p>
    <p>FIG. 9 provides a simplified example of how scores propagate through the lexical tree. Before the first frame is retrieved (row 900), state 840 (which corresponds to silence) has a score of 0 and no other nodes are active. The score of 0 means that there is a one hundred percent probability that the system is starting from silence.</p>
    <p>After the first frame is retrieved (row 905), the score for the state 840 (S<sub>A1</sub>) is set equal to the acoustic score (A<sub>A1</sub>) resulting from an acoustic match of the first frame with an acoustic model corresponding to the state 840 (i.e., the acoustic model for silence). Thus, the score for the state 840 (S<sub>A1</sub>) is set equal to the likelihood that the first frame corresponds to silence.</p>
    <p>Retrieval of the first frame also causes the state 805 to become an active state. Assuming that the node 800 corresponds to a phoneme that starts a word, the score for the state 805 (S<sub>B1</sub>) is set equal to the acoustic score (A<sub>B1</sub>) resulting from an acoustic match of the first frame with the acoustic model corresponding to the state 805. Thus, the score for the state 805 (S<sub>B1</sub>) is set equal to the likelihood that the first frame corresponds to the state 805. The starting time for the state 805 is set equal the time associated with the first frame. This value for the starting time indicates that the score at state 805 represents a word that started at a time corresponding to the first frame. The starting time moves with the score as the score propagates through the lexical tree.</p>
    <p>After the second frame is retrieved (row 910), the score for the state 840 (S<sub>A2</sub>) is set equal to the sum of the previous score for the state (S<sub>A1</sub>) and the acoustic score (A<sub>A2</sub>) resulting from an acoustic match of the second frame with the acoustic model for silence:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">S<sub>A2</sub> =S<sub>A1</sub> +A<sub>A2</sub> =A<sub>A1</sub> +A<sub>A2</sub>.</pre>
    
    <p>As noted above, each of the scores corresponds to a negative logarithmic probability. Accordingly, adding scores together corresponds to multiplying the probabilities. Thus, the score for the state 840 (S<sub>A2</sub>) equals the likelihood that both of the first and second frames correspond to silence. This process is repeated for subsequent frames (e.g., lines 915 and 920) so that the score for the state 840 at a frame "n" (S<sub>An</sub>) equals: ##EQU3## This expression assumes that the silence node 512 is not reseeded from the root node 505. If reseeding occurs at a frame n, then the value of S<sub>An-1</sub> would be replaced by the score in the root node 505 for the frame n-1.</p>
    <p>After the second frame is retrieved, the score for the state 805 (S<sub>B2</sub>) is set equal to:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">S<sub>B2</sub> =min (S<sub>B1</sub> +stay<sub>B</sub>, S<sub>A1</sub>)+A<sub>B2</sub>,</pre>
    
    <p>where A<sub>B2</sub> is the acoustic score resulting from an acoustic match of the second frame with the acoustic model corresponding to state 805 and stay<sub>B</sub> is the staying penalty for state 805. The score for state 805 (S<sub>B2</sub>) corresponds to the more likely of two alternatives: (1) the first frame was silence and the second frame was the sound represented by the state 805 or (2) both of the first and second frames were the sound represented by the state 805. The first alternative corresponds to a transition from state 840 to state 805 along the path 850. The second alternative corresponds to a transition from state 805 back to state 805 along path 820. When the first alternative is the more likely, the starting time corresponding to the first frame that was stored previously for the state 805 is replaced by a value corresponding to the second frame. This value indicates that the score at state 805 represents a word that started with the second frame.</p>
    <p>After the second frame is retrieved, the state 810 becomes an active state. The score for the state 810 (S<sub>C2</sub>) is set equal to:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">S<sub>C2</sub> =S<sub>B1</sub> +leave<sub>B</sub> +A<sub>C2</sub>,</pre>
    
    <p>where A<sub>C2</sub> is the acoustic score resulting from an acoustic match of the second frame with the acoustic model corresponding to state 810 and leave<sub>B</sub> is the leaving penalty for the state 805. Similarly, leaves and leave<sub>D</sub> are leaving penalties for, respectively, states 810 and 815. The sum of language model components of leave<sub>B</sub>, leave<sub>C</sub> and leave<sub>D</sub> represents the language model score for the phoneme represented by the node 800.</p>
    <p>The methodology for determining state scores for states other than the silence state can be expressed more generally as:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">S<sub>i</sub>,j =min (S<sub>i</sub>,j-1 +stay<sub>i</sub>, S<sub>i-1</sub>,j-1 +leave<sub>j-1</sub>)+A<sub>i</sub>,j.</pre>
    
    <p>for i greater than zero (where i equals zero corresponds to silence), and with the boundary condition that the score for an inactive state equals infinity or some sufficiently large value. The starting time for the state may be represented as:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">t<sub>i</sub>,j =t<sub>i</sub>,j-i for S<sub>i</sub>,j-1 +stay<sub>i</sub> ≦Si-1,j-1+leave<sub>j-1</sub>,</pre>
    
    <p>or</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">t<sub>i</sub>,j =t<sub>i-1</sub>,j-1 for S<sub>i</sub>,j-1 +stay<sub>i</sub> &gt;Si-1,j-1+leave<sub>j-1</sub>,</pre>
    
    <p>for i and j greater than zero and with the boundary condition that the time value for a newly active state represents the frame at which the state became active. As previously noted, state scores for the silence state may be determined as: ##EQU4## with the boundary condition that S<sub>0</sub>,0 equals zero. An even more general form, in which the scores are expressed as functions of the various parameters, is illustrated in FIG. 10.</p>
    <p>Referring to FIG. 11, a node may be processed according to a node-processing procedure 1100. Initially, the node-processing procedure updates the scores and time values for each state of the node (step 1105). The node-processing procedure updates the scores and time values by generating acoustic scores and using the equations discussed above.</p>
    <p>When the last state of the node was active prior to updating the scores for the node, the node-processing procedure uses the score for the last state to generate scores for any inactive subnodes of the node. If the generated score for a subnode does not exceed a pruning threshold, then the node-processing procedure activates that subnode and provides the subnode with the generated score.</p>
    <p>Next, the node-processing procedure determines whether the score of any state of the node exceeds the pruning threshold (step 1110). When a score exceeds the pruning threshold, the likelihood that the word represented by the score was spoken is deemed to be too small to merit further consideration. For this reason, the procedure prunes the lexical tree by deactivating any state having a score that exceeds the pruning threshold (step 1115). If every state of the node is deactivated, then the node-processing procedure also deactivates the node. The node-processing procedure may deactivate a node or state by deleting a record associated with the node or state, or by indicating in the record that the node or state is inactive. Similarly, the node-processing procedure may activate a node or state by creating a record and associating the record with the node or state, or by indicating in an existing record that the node or state is active. The procedure may use a dynamic pruning threshold that accounts for variations in the average or best score in the lexical tree at any given time.</p>
    <p>Next, the node-processing procedure determines whether a word is to be added to a list of words (step 1120). A word is added to the list of words when the node being processed corresponds to the last phoneme of a word, a score has been propagated out of the last state of the node, and the score is less than a list threshold. Before comparing the score to the list threshold, the node-processing procedure adds a language model score to the score. The language model score corresponds to the difference between the language model score for the word and the incremental language model score that is already included in the score. In general, the list threshold has a lower value than the pruning threshold. If the node being processed corresponds to the last phoneme of multiple words having the same phonetic spelling, then all of the words to which the node corresponds are added to the list of words.</p>
    <p>If the noted conditions are met, the node-processing procedure adds the word or words to the list (step 1125). A word is stored in the list of words along with the score propagated out of the last state. If the word is on the list already, then the node-processing procedure stores with the list the better of the score already stored with the list or the score propagated out of the last state. The scores for words in a list of words are returned along with the list of words. The recognizer 215 uses these scores in making the detailed match.</p>
    <p>The node-processing procedure also adds the word to lists of words for times that precede or follow the starting time to account for possible inaccuracies in the starting time of the word that may result from selecting the better of a score that remains in a state or a score propagated from a prior state. Spreading the word across multiple lists ensures that these inaccuracies will not impinge on the accuracy of the speech recognition system. The node-processing procedure spreads the word across multiple lists based on the length of the word.</p>
    <p>After adding a word to the list of words (step 1125), the node-processing procedure saves the score associated with the word as a reseeding score for use in reseeding the tree (step 1130). Production of a word by the lexical tree means that the current frame may correspond to the last frame of the word (with the probability of such a correspondence being reflected by the score associated with the word). This means that the next frame may correspond to the beginning of a word or to silence resulting from a pause between words. The pre-filtering procedure reseeds the tree (step 730 of FIG. 7) to account for this possibility.</p>
    <p>For a given frame, multiple nodes may produce words. However, the tree only needs to be reseeded once. To account for this, the node-processing procedure only saves the score associated with a word (S<sub>w</sub>) as the reseeding score (S<sub>RS</sub>) if the word is the first word to be generated by the tree for the current frame of if the word score is less than the score for all other words generated by previously-processed nodes for the current frame (S<sub>RS</sub> '):</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">S<sub>RS</sub> =min(S<sub>w</sub>, S<sub>RS</sub> ')</pre>
    
    <p>Saving only the lowest score (i.e., the score indicating the highest probability that the current frame was the last frame of a word) ensures that the tree will be reseeded using the highest probability that the next frame is the first frame of a new word.</p>
    <p>To reseed the tree (step 730 of FIG. 7), the pre-filtering procedure activates the root node 505 and associates the minimum of the reseeding score (SRS) and the score for the silence node 512 with the root node. During processing of the next frame, the active root node 505 may be used to activate nodes in the group 510 or to activate the silence node 512.</p>
    <p>Processing of the node is complete after the node-processing procedure saves a score for use in reseeding the tree (step 1130), or if no word is to be added to the list of words (step 1120). The lexical tree pre-filtering procedure is discussed in detail in U.S. patent application No. 08/701,393, filed on Aug. 22, 1996 and entitled "LEXICAL TREE PRE-FILTERING IN SPEECH RECOGNITION", which is incorporated by reference.</p>
    <p>After the pre-filtering procedure responds with the requested list of words, the recognizer initiates a hypothesis for each word from the list and compares acoustic models for the word to the frames of parameters representing the utterance. The recognizer uses the results of these comparisons to generate scores for the hypotheses. Hypotheses having excessive scores are eliminated from further consideration. As noted above, hypotheses that comply with no active constraint grammar also are eliminated.</p>
    <p>When the recognizer determines that a word of a hypothesis has ended, the recognizer requests from the pre-filtering procedure a list of words that may have been spoken just after the ending-time of the word. The recognizer then generates a new hypotheses for each word on the list, where the new hypothesis includes the words of the old hypothesis plus the new word.</p>
    <p>In generating the score for a hypothesis, the recognizer uses acoustic scores for words of the hypothesis, a language model score that indicates the likelihood that words of the hypothesis are used together, and scores provided for each word of the hypothesis by the pre-filtering procedure. The scores provided by the pre-filtering procedure include components corresponding to a crude acoustic comparison and a language model score indicative of the likelihood that a word is used, independently of context. The recognizer may eliminate anvle hypothesis that is associated with a constraint grammar (e.g., a command hypothesis), but does not comply with the constraint grammar.</p>
    <p>Referring to FIG. 12, the recognizer 215 operates according to a procedure 1200. First, prior to processing, the recognizer 215 initializes the lexical tree 500 as described above (step 1205). The recognizer 215 then retrieves a frame of parameters (step 1210) and determines whether there are hypotheses to be considered for the frame (step 1215). The first frame always corresponds to silence so that there are no hypotheses to be considered for the first frame.</p>
    <p>If hypotheses need to be considered for the frame (step 1215), then the recognizer 215 goes to the first hypothesis (step 1220). The recognizer then compares the frame to acoustic models 235 for the last word of the hypothesis (step 1225) and, based on the comparison, updates a score associated with the hypothesis (step 1230).</p>
    <p>After updating the score (step 1230), the recognizer determines whether the user was likely to have spoken the word or words corresponding to the hypothesis (step 1235). The recognizer makes this determination by comparing the current score for the hypothesis to a threshold value. If the score exceeds the threshold value, then the recognizer 215 determines that the hypothesis is too unlikely to merit further consideration and deletes the hypothesis (step 1240).</p>
    <p>If the recognizer determines that the word or words corresponding to the hypothesis were likely to have been spoken by the user, then the recognizer determines whether the last word of the hypothesis is ending (step 1245). The recognizer determines that a word is ending when the frame corresponds to the last component of the model for the word. If the recognizer determines that a word is ending (step 1245), the recognizer sets a flag that indicates that the next frame may correspond to the beginning of a word (step 1250).</p>
    <p>If there are additional hypotheses to be considered for the frame (step 1255), then the recognizer selects the next hypothesis (step 1260) and repeats the comparison (step 1225) and other steps. If there are no more hypotheses to be considered for the frame (step 1255), then the recognizer determines whether there are more frames to be considered for the utterance (step 1265). The recognizer determines that there are more frames to be considered when two conditions are met. First, more frames must be available. Second, the best scoring node for the current frame or for one or more of a predetermined number of immediately preceding frames must have been a node other than the silence node (i.e., the utterance has ended when the silence node is the best scoring node for the current frame and for a predetermined number of consecutive preceding frames).</p>
    <p>If there are more frames to be considered (step 1265) and the flag indicating that a word has ended is set (step 1270), or if there were no hypotheses to be considered for the frame (step 1215), then the recognizer requests from the pre-filtering procedure 240 a list of words that may start with the next frame (step 1275).</p>
    <p>Upon receiving the list of words from the pre-filtering procedure, the recognizer uses the list of words to create hypotheses or to expand any hypothesis for which a word has ended (step 1280). Each word in the list of words has an associated score. Prior to adding a list word to a hypothesis, the recognizer modifies the list score (S<sub>L</sub>) for the word to produce a modified list score (SM<sub>L</sub>) as:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="equation">S<sub>ML</sub> =S<sub>L</sub> +L<sub>C</sub> L<sub>L</sub>,</pre>
    
    <p>where L<sub>C</sub> is a language model score that represents the frequency with which the pair of words that includes the list word and the immediately preceding word in the hypothesis are used together in speech, and L<sub>L</sub> is a language model score included in the list score and corresponds to the frequency with which the list word is used in speech, without reference to context. The recognizer then adds the modified list score to the score for the hypothesis and compares the result to a threshold value. If the result is less than the threshold value, then the recognizer maintains the hypothesis. Otherwise, the recognizer determines that the hypothesis does not merit further consideration and abandons the hypothesis. As an additional part of creating or expanding the hypotheses, the recognizer compares the hypotheses to the active constraint grammars 225 and abandons any hypothesis that corresponds to no active constraint grammar. The recognizer then retrieves the next frame (step 1210) and repeats the procedure.</p>
    <p>If there are no more speech frames to process, then the recognizer 215 provides the most likely hypotheses to the control/interface module 220 as recognition candidates (step 1285).</p>
    <p>The control/interface module 215 controls operation of the speech recognition software and provides an interface to other software or to the user. The control/interface module receives the list of recognition candidates for each utterance from the recognizer. Recognition candidates may correspond to dictated text, speech recognition commands, or external commands. When the best-scoring recognition candidate corresponds to dictated text, the control/interface module provides the text to an active application, such as a word processor. The control/interface module also may display the best-scoring recognition candidate to the user through a graphical user interface. The control/interface module controls operation of the speech recognition software in response to speech recognition commands (e.g., "wake up", "make that"), and forwards external commands to the appropriate software.</p>
    <p>The control/interface module also controls the active vocabulary, acoustic models, and constraint grammars that are used by the recognizer. For example, when the speech recognition software is being used in conjunction with a particular application (e.g., Microsoft Word), the control/interface module updates the active vocabulary to include command words associated with that application and activates constraint grammars associated with the application.</p>
    <p>Other functions provided by the control/interface module include an enrollment program, a vocabulary customizer, and a vocabulary manager. The enrollment program collects acoustic information from a user and trains or adapts a user's models based on that information. The vocabulary customizer optimizes the language model of a specific topic by scanning user supplied text. The vocabulary manager is a developer tool which is used to browse and manipulate vocabularies, grammars and macros. Each function of the control/interface module 220 may be implemented as an executable program that is separate from the main speech recognition software.</p>
    <p>A complete dictation vocabulary consists of the active vocabulary plus a backup vocabulary 245. The backup vocabulary may include files that contain user-specific backup vocabulary words and system-wide backup vocabulary words. User-specific backup vocabulary words include words which a user has created while using the speech recognition software (up to a limit of 64,000 words). These words are stored in vocabulary files for the user and for the dictation, and are available as part of the backup dictionary for the dictation topic regardless of user, and to the user regardless of which dictation topic is being used. For example, if a user is using a medical topic and adds the word "ganglion" to the dictation vocabulary, any other user of the medical topic will have immediate access to the word "ganglion". In addition, the word will be written into the user-specific backup vocabulary. Then, if the user says "ganglion" while using a legal topic, the word "ganglion" will be available during correction from the backup dictionary.</p>
    <p>In addition to the user-specific backup dictionaries noted above, there is a system-wide backup dictionary. The system-wide backup dictionary contains all the words known to the system, including words which may currently be in an active vocabulary.</p>
    <p>During error correction, word searches of the backup vocabularies start with the user-specific backup dictionary and then check the system-wide backup dictionary. The backup dictionaries also are searched when there are new words in text that a user has typed.</p>
    <p>Two methods of adding new, non-user-specific words to the dictation vocabulary will now be described. Referring to FIG. 13, in the first method 1300, to add a new word, the user types in the word (step 1305) and utters the word (step 1310). Using a rules list that contains possible phonemes associated with letters and strings of letters, and their associated probabilities (frequencies of occurrence in the vocabulary), a constraint grammar containing a word list of possible phonetic spellings is created from the spelled word (step 1315). An example of a rules list is provided in Appendix A. A table relating the phoneme symbols in Appendix A to human pronunciations is included as FIG. 20., Recognizer 215 is then used to choose the best phonetic spelling from the constraint grammar by recognizing the spoken word against the constraint grammar (steps 1320 and 1325). The spelled word with the phonetic spelling of the best result is then added to the dictionary (step 1330).</p>
    <p>FIG. 14 shows a method for creating the constraint grammar (step 1315). After the user spells and utters the new word, a letter counter, n, is initialized to zero so as to point to the first letter of the spelled word (step 1405). The longest string of spelled letters starting with the letter in the nth place is then found in the rules list (step 1410).</p>
    <p>For example, if the new word is "weight", "w" is in the zero place and the longest string of letters in the rules starting with "w" is "w" (i.e., "we", "wei", etc. are not in the rules list.) The possible phonemes for "w", there being four in the rules list, with their associated probabilities, expressed as negative logarithm probabilities, are then stored in a phoneme list (step 1415).</p>
    <p>The phoneme list for n=0 and letter string "w" can be expressed as:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________  phoneme         probability______________________________________  d@bLyH 100  hw     670  v      780  w      1______________________________________</pre>
    
    <p>Letter counter n is then incremented with its new value equal to its old value plus the length of the letter string in step 1410 (step 1420). The value of letter counter n is checked against the length of the spelled word to see if the end of the word has been reached (step 1425). If the end of the word has not been reached, the longest string of spelled letters in the rules list starting with the letter in the nth place is found (step 1410). In the example, n is incremented to one, and the longest string of spelled letters in the rules list starting with "e", the letter in the first place, is found. The longest letter string is "eigh". The possible phonemes for "eigh", there being two in the rules list, are then stored in the phoneme list with their associated probabilities.</p>
    <p>The phoneme list for n=1 and letter string "eigh" can be expressed as:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________  phoneme         probability______________________________________  $      9  B      239______________________________________</pre>
    
    <p>Letter counter n is then incremented by the length of the letter string "eigh", i.e., n=1+4. The longest string of spelled letters starting with "t" is t (there being no spelled letters after t). The possible phonemes for "t", there being three in the rules list, are then stored in the phoneme list with their associated probabilities.</p>
    <p>The phoneme list for n=5 and letter string "t" can be expressed as:</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________  phoneme         probability______________________________________  d      897  t      1  t/     100______________________________________</pre>
    
    <p>When the end of the word has been reached, the constraint grammar is created by forming the complete phonetic spellings of the spelled word from the phonetic fragments in the phoneme list and adding the associated probabilities (step 1430).</p>
    <p>
      </p> <pre xml:space="preserve" listing-type="tabular">______________________________________  word    probability______________________________________  w$t     11  w$t/    110  d@bLyH$t          110  d@bLyH$t/          209  w8t     241  d@bLyH8t          340  w8t/    340  d@bLyH8t/          439  etc.______________________________________</pre>
    
    <p>The constraint grammar is restricted to the 200 phonetic spellings having the highest probabilities (the lowest -log values). Recognizer 215 is then used to recognize the uttered word against the constraint grammar and to select the best phonetic spelling of the spoken word.</p>
    <p>Referring to FIG. 15, in the second method 1500 of adding new words to the vocabulary, after the user spells the word (step 1505) and utters the word (step 1510), a rules list is used to create a net containing all possible phonetic spellings of the spelled word (step 1515). See L. Bahl et al., "A Maximum Likelihood Approach to Continuous Speech Recognition," IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-5, No.2, March 1983, pp. 179-190, which is incorporated by reference, for a general description of net formation. An example of a rules list for use with this method is provided in Appendix B.</p>
    <p>FIG. 16 shows an example of a net 1650 created for the word "weight". The rules list provides a list of phonemes 1655 associated with each letter node 1660 of net 1650. Recognizer 215 uses continuous recognition of the uttered word to prune out the paths of net 1650 that do not contain phonemes corresponding to the spoken word to choose the best phonetic spelling (steps 1520 and 1525). The spelled word with the phonetic spelling of the best result is then added to the dictionary (step 1530). In the example of FIG. 16, the paths leading to "gh", "i" and "igh" are pruned out by the recognizer, and "w$t" is selected by the recognizer. If the acoustic match of the appropriate section of the spoken word to the phonemes 1655 does not result in a strong distinction between possible phonemes 1655, the recognizer will select the phoneme having the higher associated probability.</p>
    <p>The phonemes in the rules list of Appendix B can also be used to perform phoneme recognition. Concurrently with using the net, or as a back-off procedure if no match is found using the net, continuous recognition of the uttered word against all of the phonemes in the rules list can be performed. This is particularly effective in cases such as "FIG. " pronounced "figure", where the word is abbreviated and the period symbol is not pronounced.</p>
    <p>Referring to FIG. 17, to add a new word, the user interfaces with a control window 1750. The word is typed into box 1752, and the users utters the word after turning a microphone on using microphone button 1754. The pronunciation of the word determined by either of the methods described above then appears in the pronunciation box 1756. The user can edit the pronunciation in box 1756 or directly type the pronunciation into box 1756.</p>
    <p>Once the phonetic spelling has been generated it needs to be added to the dictionary. The add button 1758 adds the word to the vocabulary and places the word in a pre-filter class. The new word is assigned a pre-filter class by comparing the new word to words already classified. Referring to FIG. 18, the first two letters of the new word are selected (step 1805) and matched to classified words starting with the same two letters (step 1810) to form a sub-list of classified words (step 1815). For example, if the new word is "hyperbole", a sub-list of classified words is formed that start with "hy". The first four phonemes of the new word are then matched against the first four phonemes of the words in the sub-list (step 1820) to form a class list (step 1825). In the example, the first four phonemes are "h ai p r". The class list contains all classified words in the sub-list that begin with these four phonemes. Words such as "hypertension" are included in the class list. The new word is placed in the class list (step 1830). If there is no match for the first four phonemes, an attempt is made to match the first three, two, or one phonemes, in that order. If no match is found, the search is done on the entire classified word list as a back-off.</p>
    <p>Referring to FIG. 19, in a preferred, faster procedure for assigning a pre-filter class to a new word, a direct look-up is performed of the pronunciation of the new word against a database containing a list of all classified words placed in alphabetic order by their first four phonemes (step 1905). The direct look-up matches the first four phonemes of the new word to the first classified word in the list having the same first four phonemes, or a sub-set of the first four phonemes if there is no match. Starting with the first classified word that matches, that word, and the next 199 words in the list of classified words form the sub-list (step 1910). The first four phonemes of the new word are then matched against the first four phonemes of the words in the sub-list (step 1915) to form a class list (step 1920). The new word is placed in the class list (step 1925).</p>
    <p>The initials box 1760 is used to indicate that the word being added is pronounced as a set of initials. Activating the text-to-speech button 1762 plays back the phonemes in pronunciation box 1756. A next word button 1764 is used if the user is adding words from input files; activating button 1764 displays the next word in the file in box 1752. Similarly, activating previous word button 1766 displays the previous word in the input file in box 1752. Activating a phoneme table button 68 opens a table containing valid phonemes. Pronunciation box 1756 can be edited using the phoneme table. Activating word history button 1770 displays the words in the current vocabulary and allows the user to delete a word, display all pronunciations for a word, add a pronunciation for a word, and delete a pronunciation for a word. Frequency buttons 1772 are used to specify the relative frequency with which the pronunciation in box 1756 is used.</p>
    <p>Appendix C is the source code in C++, including header files, for the method of adding a word to the dictation vocabulary described in FIGS. 13 and 14. Appendix D is the source code in C++, including header files, for the method of adding a word to the dictation vocabulary described in FIGS. 16 and 17. The code in appendices C and D can be applied to languages other than English.</p>
    <p>Other embodiments are within the scope of the following claims. For example, the techniques described here are not limited to any particular hardware or software configuration; they may find applicability in any computing or processing environment that may be used for speech recognition. The techniques may be implemented in hardware or software, or a combination of the two. Preferably, the techniques are implemented in computer programs executing on programmable computers that each include a processor, a storage medium readable by the processor (including volatile and non-volatile memory and/or storage elements), at least one input device, and one or more output devices. Program code is applied to data entered using the input device to perform the functions described and to generate output information. The output information is applied to one or more output devices.</p>
    <p>Each program is preferably implemented in a high level procedural or object oriented programming language to communicate with a computer system. However, the programs can be implemented in assembly or machine language, if desired. In any case, the language may be a compiled or interpreted language.</p>
    <p>Each such computer program is preferably stored on a storage medium or device (e.g., CD-ROM, hard disk or magnetic diskette) that is readable by a general or special purpose programmable computer for configuring and operating the computer when the storage medium or device is read by the computer to perform the procedures described in this document. The system may also be considered to be implemented as a computer-readable storage medium, configured with a computer program, where the storage medium so configured causes a computer to operate in a specific and predefined manner.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4481593">US4481593</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 5, 1981</td><td class="patent-data-table-td patent-date-value">Nov 6, 1984</td><td class="patent-data-table-td ">Exxon Corporation</td><td class="patent-data-table-td ">Continuous speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4489435">US4489435</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 5, 1981</td><td class="patent-data-table-td patent-date-value">Dec 18, 1984</td><td class="patent-data-table-td ">Exxon Corporation</td><td class="patent-data-table-td ">Method and apparatus for continuous word string recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4718094">US4718094</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 27, 1986</td><td class="patent-data-table-td patent-date-value">Jan 5, 1988</td><td class="patent-data-table-td ">International Business Machines Corp.</td><td class="patent-data-table-td ">Speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4783803">US4783803</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 12, 1985</td><td class="patent-data-table-td patent-date-value">Nov 8, 1988</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Speech recognition apparatus and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4805218">US4805218</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 3, 1987</td><td class="patent-data-table-td patent-date-value">Feb 14, 1989</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Method for speech analysis and speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4805219">US4805219</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 3, 1987</td><td class="patent-data-table-td patent-date-value">Feb 14, 1989</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Method for speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4829576">US4829576</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 21, 1986</td><td class="patent-data-table-td patent-date-value">May 9, 1989</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Voice recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4833712">US4833712</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 29, 1985</td><td class="patent-data-table-td patent-date-value">May 23, 1989</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Automatic generation of simple Markov model stunted baseforms for words in a vocabulary</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5027406">US5027406</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 6, 1988</td><td class="patent-data-table-td patent-date-value">Jun 25, 1991</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Method for interactive speech recognition and training</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5208897">US5208897</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 28, 1990</td><td class="patent-data-table-td patent-date-value">May 4, 1993</td><td class="patent-data-table-td ">Emerson &amp; Stern Associates, Inc.</td><td class="patent-data-table-td ">Method and apparatus for speech recognition based on subsyllable spellings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5222188">US5222188</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 21, 1990</td><td class="patent-data-table-td patent-date-value">Jun 22, 1993</td><td class="patent-data-table-td ">Emerson &amp; Stern Associates, Inc.</td><td class="patent-data-table-td ">Method and apparatus for speech recognition based on subsyllable spellings</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5293451">US5293451</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 23, 1990</td><td class="patent-data-table-td patent-date-value">Mar 8, 1994</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and apparatus for generating models of spoken words based on a small number of utterances</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5329609">US5329609</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 30, 1991</td><td class="patent-data-table-td patent-date-value">Jul 12, 1994</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Recognition apparatus with function of displaying plural recognition candidates</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5428707">US5428707</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 13, 1992</td><td class="patent-data-table-td patent-date-value">Jun 27, 1995</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Apparatus and methods for training speech recognition systems and their users and otherwise improving speech recognition performance</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5440663">US5440663</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 4, 1993</td><td class="patent-data-table-td patent-date-value">Aug 8, 1995</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Computer system for speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5497447">US5497447</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 8, 1993</td><td class="patent-data-table-td patent-date-value">Mar 5, 1996</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Speech coding apparatus having acoustic prototype vectors generated by tying to elementary models and clustering around reference vectors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5500920">US5500920</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 1994</td><td class="patent-data-table-td patent-date-value">Mar 19, 1996</td><td class="patent-data-table-td ">Xerox Corporation</td><td class="patent-data-table-td ">Semantic co-occurrence filtering for speech recognition and signal transcription applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5623578">US5623578</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 28, 1993</td><td class="patent-data-table-td patent-date-value">Apr 22, 1997</td><td class="patent-data-table-td ">Lucent Technologies Inc.</td><td class="patent-data-table-td ">Speech recognition system allows new vocabulary words to be added without requiring spoken samples of the words</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5652828">US5652828</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 1, 1996</td><td class="patent-data-table-td patent-date-value">Jul 29, 1997</td><td class="patent-data-table-td ">Nynex Science &amp; Technology, Inc.</td><td class="patent-data-table-td ">Automated voice synthesis employing enhanced prosodic treatment of text, spelling of text and rate of annunciation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5748840">US5748840</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 9, 1995</td><td class="patent-data-table-td patent-date-value">May 5, 1998</td><td class="patent-data-table-td ">Audio Navigation Systems, Inc.</td><td class="patent-data-table-td ">Methods and apparatus for improving the reliability of recognizing words in a large database when the words are spelled or spoken</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5751906">US5751906</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 29, 1997</td><td class="patent-data-table-td patent-date-value">May 12, 1998</td><td class="patent-data-table-td ">Nynex Science &amp; Technology</td><td class="patent-data-table-td ">Method for synthesizing speech from text and for spelling all or portions of the text by analogy</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5765132">US5765132</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 26, 1995</td><td class="patent-data-table-td patent-date-value">Jun 9, 1998</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Building speech models for new words in a multi-word utterance</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5794189">US5794189</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 13, 1995</td><td class="patent-data-table-td patent-date-value">Aug 11, 1998</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Continuous speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5815639">US5815639</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 20, 1993</td><td class="patent-data-table-td patent-date-value">Sep 29, 1998</td><td class="patent-data-table-td ">Engate Incorporated</td><td class="patent-data-table-td ">Computer-aided transcription system using pronounceable substitute text with a common cross-reference library</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5850627">US5850627</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 26, 1997</td><td class="patent-data-table-td patent-date-value">Dec 15, 1998</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Apparatuses and methods for training and operating speech recognition systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0562138A1?cl=en">EP0562138A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 25, 1992</td><td class="patent-data-table-td patent-date-value">Sep 29, 1993</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and apparatus for the automatic generation of Markov models of new words to be added to a speech recognition vocabulary</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Asadi, Ayman, "<a href='http://scholar.google.com/scholar?q="Automatic+Modeling+for+Adding+New+Words+to+a+Large+Vocabulary+.+.+.+"'>Automatic Modeling for Adding New Words to a Large Vocabulary . . . </a>", ICASSP 91, vol. 1, pp. 305-308, 1991.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Asadi, Ayman, Automatic Modeling for Adding New Words to a Large Vocabulary . . . , ICASSP 91, vol. 1, pp. 305 308, 1991.</td></tr><tr><td class="patent-data-table-td ">3</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Asadi, et al.; "<a href='http://scholar.google.com/scholar?q="Automatic+Modeling+for+Adding+New+Words+to+a+Large-Vocabulary+Continuous+Speech+Recognition+System"'>Automatic Modeling for Adding New Words to a Large-Vocabulary Continuous Speech Recognition System</a>"; ICASSP 91 vol. 1; International Conference; pp. 305-308.</td></tr><tr><td class="patent-data-table-td ">4</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Asadi, et al.; Automatic Modeling for Adding New Words to a Large Vocabulary Continuous Speech Recognition System ; ICASSP 91 vol. 1; International Conference; pp. 305 308.</td></tr><tr><td class="patent-data-table-td ">5</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bahl, et al.; "<a href='http://scholar.google.com/scholar?q="A+Maximum+Likelihood+Approach+to+Continuous+Speech+Recognition"'>A Maximum Likelihood Approach to Continuous Speech Recognition</a>"; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PAMI-5; No. 2, Mar. 1983.</td></tr><tr><td class="patent-data-table-td ">6</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Bahl, et al.; A Maximum Likelihood Approach to Continuous Speech Recognition ; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. PAMI 5; No. 2, Mar. 1983.</td></tr><tr><td class="patent-data-table-td ">7</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bahl, L.R., "<a href='http://scholar.google.com/scholar?q="Adaptation+of+Large+Vocabulary+Recognition+System"'>Adaptation of Large Vocabulary Recognition System</a>" ICASSP-92, vol. 1, pp. I477-480 Mar. 1992.</td></tr><tr><td class="patent-data-table-td ">8</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bahl, L.R., "<a href='http://scholar.google.com/scholar?q="Automatic+High-Resolution+Labeling+of+Speech+Waveforms"'>Automatic High-Resolution Labeling of Speech Waveforms</a>", IBM Technical Disclosure Bulletin, vol. 23, No. 7B, pp. 3466-3467, Dec. 1980.</td></tr><tr><td class="patent-data-table-td ">9</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bahl, L.R., "<a href='http://scholar.google.com/scholar?q="Automatic+Phonetic+Baseform+Determination"'>Automatic Phonetic Baseform Determination</a>", ICASSP 91, vol. 1, pp. 173-176, May 1991.</td></tr><tr><td class="patent-data-table-td ">10</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bahl, L.R., "<a href='http://scholar.google.com/scholar?q="Automatic+Selection+of+Speech+Prototypes+"'>Automatic Selection of Speech Prototypes </a>" IBM Technical Disclosure Bulletin vol. 24, No. 4, pp. 2042-2043, Sep. 1981.</td></tr><tr><td class="patent-data-table-td ">11</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bahl, L.R., "<a href='http://scholar.google.com/scholar?q="Interpolation+of+Estimators+Derived+From+Sparse+Data"'>Interpolation of Estimators Derived From Sparse Data</a>", IBM Technical Disclosure Bulletin vol. 24, No. 4, pp. 2038-2041, Sep. 1981.</td></tr><tr><td class="patent-data-table-td ">12</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Bahl, L.R., Adaptation of Large Vocabulary Recognition System ICASSP 92, vol. 1, pp. I477 480 Mar. 1992.</td></tr><tr><td class="patent-data-table-td ">13</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Bahl, L.R., Automatic High Resolution Labeling of Speech Waveforms , IBM Technical Disclosure Bulletin, vol. 23, No. 7B, pp. 3466 3467, Dec. 1980.</td></tr><tr><td class="patent-data-table-td ">14</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Bahl, L.R., Automatic Phonetic Baseform Determination , ICASSP 91, vol. 1, pp. 173 176, May 1991.</td></tr><tr><td class="patent-data-table-td ">15</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Bahl, L.R., Automatic Selection of Speech Prototypes IBM Technical Disclosure Bulletin vol. 24, No. 4, pp. 2042 2043, Sep. 1981.</td></tr><tr><td class="patent-data-table-td ">16</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Bahl, L.R., Interpolation of Estimators Derived From Sparse Data , IBM Technical Disclosure Bulletin vol. 24, No. 4, pp. 2038 2041, Sep. 1981.</td></tr><tr><td class="patent-data-table-td ">17</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Bahl, Lalit, "<a href='http://scholar.google.com/scholar?q="A+Maximum+LikeLihood+Approach+to+Continuous+Speech+Recognition"'>A Maximum LikeLihood Approach to Continuous Speech Recognition</a>", IEEE Transactions on Patern Analysis and Machine Intelligence, vol. PAMI-5, No. 2, pp. 179-190, Mar. 1983.</td></tr><tr><td class="patent-data-table-td ">18</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Bahl, Lalit, A Maximum LikeLihood Approach to Continuous Speech Recognition , IEEE Transactions on Patern Analysis and Machine Intelligence, vol. PAMI 5, No. 2, pp. 179 190, Mar. 1983.</td></tr><tr><td class="patent-data-table-td ">19</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Das, S.K., "<a href='http://scholar.google.com/scholar?q="System+for+Temporal+Registration+of+Quasi-Phonemic+Utterance+Representations"'>System for Temporal Registration of Quasi-Phonemic Utterance Representations</a>", IBM Technical Disclosure Bulletin, Bol. 23, No. 7A, pp. 3047-3050, Dec. 1980.</td></tr><tr><td class="patent-data-table-td ">20</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Das, S.K., System for Temporal Registration of Quasi Phonemic Utterance Representations , IBM Technical Disclosure Bulletin, Bol. 23, No. 7A, pp. 3047 3050, Dec. 1980.</td></tr><tr><td class="patent-data-table-td ">21</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">European Search Report dated Apr. 7, 1999.</td></tr><tr><td class="patent-data-table-td ">22</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Haeb Unbach, R., Automatic Transcription of Unknown Words in a Speech Recognition System , The 1995 International Conference on Acoustice, Speech, and Signal Processing, vol. 1, pp. 840 843, May 1995.</td></tr><tr><td class="patent-data-table-td ">23</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Haeb-Unbach, R., "<a href='http://scholar.google.com/scholar?q="Automatic+Transcription+of+Unknown+Words+in+a+Speech+Recognition+System"'>Automatic Transcription of Unknown Words in a Speech Recognition System</a>", The 1995 International Conference on Acoustice, Speech, and Signal Processing, vol. 1, pp. 840-843, May 1995.</td></tr><tr><td class="patent-data-table-td ">24</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Hunnicutt, Sheri, "<a href='http://scholar.google.com/scholar?q="Reversible+Letter-to-Sound+Sound-to-Letter+Generation+.+.+.+"'>Reversible Letter-to-Sound Sound-to-Letter Generation . . . </a>", Eurospeech '93, vol. 2, pp. 763-766.</td></tr><tr><td class="patent-data-table-td ">25</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Hunnicutt, Sheri, Reversible Letter to Sound Sound to Letter Generation . . . , Eurospeech 93, vol. 2, pp. 763 766.</td></tr><tr><td class="patent-data-table-td ">26</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Imai, Toru, "<a href='http://scholar.google.com/scholar?q="ANew+Method+for+Automatic+Generation+of+Speaker-Dependent+Phonological+Rules"'>ANew Method for Automatic Generation of Speaker-Dependent Phonological Rules</a>", The 1995 International Conference on Acoustice, Speech, and Signal Processing, vol. 1, pp. 864-867, May 1995.</td></tr><tr><td class="patent-data-table-td ">27</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Imai, Toru, ANew Method for Automatic Generation of Speaker Dependent Phonological Rules , The 1995 International Conference on Acoustice, Speech, and Signal Processing, vol. 1, pp. 864 867, May 1995.</td></tr><tr><td class="patent-data-table-td ">28</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Kita, Kenji et al., "<a href='http://scholar.google.com/scholar?q="Processing+Unknown+Words+in+Continuous+Speech+Recognition%2C"'>Processing Unknown Words in Continuous Speech Recognition,</a>" IEICE Trans., vol. E74, No. 7 (Jul. 1991), pp. 1811-1815.</td></tr><tr><td class="patent-data-table-td ">29</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Kita, Kenji et al., Processing Unknown Words in Continuous Speech Recognition, IEICE Trans., vol. E74, No. 7 (Jul. 1991), pp. 1811 1815.</td></tr><tr><td class="patent-data-table-td ">30</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Merialdo B., "<a href='http://scholar.google.com/scholar?q="Multilevel+decoding+for+Very-Large-Size-Dictionary+speech+recognition"'>Multilevel decoding for Very-Large-Size-Dictionary speech recognition</a>", IBM J. Res. Develop., vol. 32, No. 2, Mar. 1988.</td></tr><tr><td class="patent-data-table-td ">31</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Merialdo B., Multilevel decoding for Very Large Size Dictionary speech recognition , IBM J. Res. Develop., vol. 32, No. 2, Mar. 1988.</td></tr><tr><td class="patent-data-table-td ">32</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Wothke, K., "<a href='http://scholar.google.com/scholar?q="Morphologically+based+automatic+phonetic+transcription"'>Morphologically based automatic phonetic transcription</a>", IBM Systems Journal, vol. 32, No. 3, 1993.</td></tr><tr><td class="patent-data-table-td ">33</td><td class="patent-data-table-td "><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td ">Wothke, K., Morphologically based automatic phonetic transcription , IBM Systems Journal, vol. 32, No. 3, 1993.</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6304848">US6304848</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 13, 1998</td><td class="patent-data-table-td patent-date-value">Oct 16, 2001</td><td class="patent-data-table-td ">Medical Manager Corp.</td><td class="patent-data-table-td ">Medical record forming and storing apparatus and medical record and method related to same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6314165">US6314165</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 30, 1998</td><td class="patent-data-table-td patent-date-value">Nov 6, 2001</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Automated hotel attendant using speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6363342">US6363342</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 18, 1998</td><td class="patent-data-table-td patent-date-value">Mar 26, 2002</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">System for developing word-pronunciation pairs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6411932">US6411932</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 8, 1999</td><td class="patent-data-table-td patent-date-value">Jun 25, 2002</td><td class="patent-data-table-td ">Texas Instruments Incorporated</td><td class="patent-data-table-td ">Rule-based learning of word pronunciations from training corpora</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6587830">US6587830</a></td><td class="patent-data-table-td patent-date-value">Sep 21, 2001</td><td class="patent-data-table-td patent-date-value">Jul 1, 2003</td><td class="patent-data-table-td ">Medical Manager Health Systems, Inc.</td><td class="patent-data-table-td ">Medical record forming and storing apparatus and medical record and method related to same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6678658">US6678658</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 7, 2000</td><td class="patent-data-table-td patent-date-value">Jan 13, 2004</td><td class="patent-data-table-td ">The Regents Of The University Of California</td><td class="patent-data-table-td ">Speech processing using conditional observable maximum likelihood continuity mapping</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7099828">US7099828</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 7, 2001</td><td class="patent-data-table-td patent-date-value">Aug 29, 2006</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Method and apparatus for word pronunciation composition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7120582">US7120582</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 7, 1999</td><td class="patent-data-table-td patent-date-value">Oct 10, 2006</td><td class="patent-data-table-td ">Dragon Systems, Inc.</td><td class="patent-data-table-td ">Expanding an effective vocabulary of a speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7127397">US7127397</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 31, 2001</td><td class="patent-data-table-td patent-date-value">Oct 24, 2006</td><td class="patent-data-table-td ">Qwest Communications International Inc.</td><td class="patent-data-table-td ">Method of training a computer system via human voice input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7263694">US7263694</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 26, 2001</td><td class="patent-data-table-td patent-date-value">Aug 28, 2007</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Directed non-cyclic graph walking system for data processing and analysis in software application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7272560">US7272560</a></td><td class="patent-data-table-td patent-date-value">Mar 22, 2004</td><td class="patent-data-table-td patent-date-value">Sep 18, 2007</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Methodology for performing a refinement procedure to implement a speech recognition dictionary</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7295979">US7295979</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 22, 2001</td><td class="patent-data-table-td patent-date-value">Nov 13, 2007</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Language context dependent data labeling</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7346495">US7346495</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 30, 2000</td><td class="patent-data-table-td patent-date-value">Mar 18, 2008</td><td class="patent-data-table-td ">Intel Corporation</td><td class="patent-data-table-td ">Method and system for building a domain specific statistical language model from rule based grammar specifications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7349846">US7349846</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 24, 2004</td><td class="patent-data-table-td patent-date-value">Mar 25, 2008</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Information processing apparatus, method, program, and storage medium for inputting a pronunciation symbol</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7392182">US7392182</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 18, 2002</td><td class="patent-data-table-td patent-date-value">Jun 24, 2008</td><td class="patent-data-table-td ">Harman International Industries, Inc.</td><td class="patent-data-table-td ">Speech recognition system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7392189">US7392189</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 21, 2003</td><td class="patent-data-table-td patent-date-value">Jun 24, 2008</td><td class="patent-data-table-td ">Harman Becker Automotive Systems Gmbh</td><td class="patent-data-table-td ">System for speech recognition with multi-part recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7451081">US7451081</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 13, 2007</td><td class="patent-data-table-td patent-date-value">Nov 11, 2008</td><td class="patent-data-table-td ">At&amp;T Corp.</td><td class="patent-data-table-td ">System and method of performing speech recognition based on a user identifier</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7953594">US7953594</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 31, 2006</td><td class="patent-data-table-td patent-date-value">May 31, 2011</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Speech recognition method and apparatus using lexicon group tree</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8108205">US8108205</a></td><td class="patent-data-table-td patent-date-value">Dec 1, 2006</td><td class="patent-data-table-td patent-date-value">Jan 31, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Leveraging back-off grammars for authoring context-free grammars</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8145481">US8145481</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 9, 2008</td><td class="patent-data-table-td patent-date-value">Mar 27, 2012</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">System and method of performing user-specific automatic speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8280733">US8280733</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 17, 2010</td><td class="patent-data-table-td patent-date-value">Oct 2, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Automatic speech recognition learning using categorization and selective incorporation of user-initiated corrections</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8423354">US8423354</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 5, 2010</td><td class="patent-data-table-td patent-date-value">Apr 16, 2013</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Speech recognition dictionary creating support device, computer readable medium storing processing program, and processing method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8527270">US8527270</a></td><td class="patent-data-table-td patent-date-value">Jul 30, 2010</td><td class="patent-data-table-td patent-date-value">Sep 3, 2013</td><td class="patent-data-table-td ">Sri International</td><td class="patent-data-table-td ">Method and apparatus for conducting an interactive dialogue</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090106028">US20090106028</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 18, 2007</td><td class="patent-data-table-td patent-date-value">Apr 23, 2009</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Automated tuning of speech recognition parameters</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110015927">US20110015927</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 17, 2010</td><td class="patent-data-table-td patent-date-value">Jan 20, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">System and method for efficient laser processing of a moving web-based material</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110119052">US20110119052</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 5, 2010</td><td class="patent-data-table-td patent-date-value">May 19, 2011</td><td class="patent-data-table-td ">Fujitsu Limited</td><td class="patent-data-table-td ">Speech recognition dictionary creating support device, computer readable medium storing processing program, and processing method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120029904">US20120029904</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 30, 2010</td><td class="patent-data-table-td patent-date-value">Feb 2, 2012</td><td class="patent-data-table-td ">Kristin Precoda</td><td class="patent-data-table-td ">Method and apparatus for adding new vocabulary to interactive translation and dialogue systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120185237">US20120185237</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 26, 2012</td><td class="patent-data-table-td patent-date-value">Jul 19, 2012</td><td class="patent-data-table-td ">At&amp;T Intellectual Property Ii, L.P.</td><td class="patent-data-table-td ">System and method of performing user-specific automatic speech recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120245919">US20120245919</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 23, 2009</td><td class="patent-data-table-td patent-date-value">Sep 27, 2012</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Probabilistic Representation of Acoustic Segments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP0984428A2?cl=en">EP0984428A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 23, 1999</td><td class="patent-data-table-td patent-date-value">Mar 8, 2000</td><td class="patent-data-table-td ">Matsushita Electric Industrial Co., Ltd.</td><td class="patent-data-table-td ">Method and system for automatically determining phonetic transciptions associated with spelled words</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2005052912A2?cl=en">WO2005052912A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 12, 2004</td><td class="patent-data-table-td patent-date-value">Jun 9, 2005</td><td class="patent-data-table-td ">Matsushita Electric Ind Co Ltd</td><td class="patent-data-table-td ">Apparatus and method for voice-tagging lexicon</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2011037562A1?cl=en">WO2011037562A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 23, 2009</td><td class="patent-data-table-td patent-date-value">Mar 31, 2011</td><td class="patent-data-table-td ">Nuance Communications, Inc.</td><td class="patent-data-table-td ">Probabilistic representation of acoustic segments</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=V3NQBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704S254000">704/254</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=V3NQBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc704/defs704.htm&usg=AFQjCNFXKBc0Q4Cbl6DXC_PCjlN_Rft8Xg#C704SE15008">704/E15.008</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=V3NQBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G10L0015060000">G10L15/06</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=V3NQBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G10L15/063">G10L15/063</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G10L15/063</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Jan 18, 2012</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">12</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 1, 2009</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIMS 1-14 ARE CANCELLED. NEW CLAIMS 15-21 ARE ADDED AND DETERMINED TO BE PATENTABLE.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 28, 2008</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 18, 2008</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Mar 27, 2007</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20061121</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 24, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG. STAMFORD BRANCH, CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:018160/0909</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20060331</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:018160/0909</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG. STAMFORD BRANCH,CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:18160/909</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Apr 7, 2006</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG, STAMFORD BRANCH, CONNECTICUT</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">SECURITY AGREEMENT;ASSIGNOR:NUANCE COMMUNICATIONS, INC.;REEL/FRAME:017435/0199</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20060331</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">USB AG, STAMFORD BRANCH,CONNECTICUT</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 6, 2005</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">NUANCE COMMUNICATIONS, INC., MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CHANGE OF NAME;ASSIGNOR:SCANSOFT, INC.;REEL/FRAME:016851/0772</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20051017</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 11, 2004</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">May 11, 2004</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 4, 2004</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Oct 8, 2002</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">L &amp; H HOLDINGS USA, INC., MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">MERGER;ASSIGNOR:DRAGON SYSTEMS, INC.;REEL/FRAME:013362/0732</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20000607</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">SCANSOFT, INC., MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:L &amp; H HOLDINGS USA, INC.;REEL/FRAME:013362/0739</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20011212</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 17, 1997</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">DRAGON SYSTEMS, INC., MASSACHUSETTS</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BAKER, JAMES K.;GADBOIS, GREGORY J.;INGOLD, CHARLES E.;AND OTHERS;REEL/FRAME:008805/0912;SIGNING DATES FROM 19971016 TO 19971104</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U2PK-NlWX9SG6lvq_K1uj2gYhBTYg\u0026id=V3NQBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U2cRPmHKUbDKIUI62sRX8y1qZiVZA\u0026id=V3NQBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U3g1wBxnTzOWpgZClheSWRp6O8Fag","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Pronunciation_generation_in_speech_recog.pdf?id=V3NQBAABERAJ\u0026output=pdf\u0026sig=ACfU3U0CoVMp6_zX6sZgeGP_yIXUtp-yJA"},"sample_url":"http://www.google.com/patents/reader?id=V3NQBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>