<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US6754400 - System and method for creation, processing and visualization of omni ... - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_4ff636b3d23669b7103f3b3a3a18b4cd/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_4ff636b3d23669b7103f3b3a3a18b4cd__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="System and method for creation, processing and visualization of omni-directional images"><meta name="DC.contributor" content="Rusu Mihai Florin" scheme="inventor"><meta name="DC.contributor" content="Sever Serban" scheme="inventor"><meta name="DC.contributor" content="Richard Wilson, Jr." scheme="assignee"><meta name="DC.date" content="2001-2-6" scheme="dateSubmitted"><meta name="DC.description" content="A system and method for digitally rendering omni-directional images includes capturing images of a region 360 degrees about an origin point in a single plane, and assembling the images in a digital format to create a complete spherical image surrounding the origin point. The spherical image is projected onto faces of a cube surrounding the spherical image. Images which were projected on the faces of the cube may then be rendered to provide an omni-directional image."><meta name="DC.date" content="2004-6-22" scheme="issued"><meta name="DC.relation" content="US:4515450" scheme="references"><meta name="DC.relation" content="US:5185667" scheme="references"><meta name="DC.relation" content="US:5582518" scheme="references"><meta name="DC.relation" content="US:5687249" scheme="references"><meta name="DC.relation" content="US:5877801" scheme="references"><meta name="DC.relation" content="US:6320584" scheme="references"><meta name="DC.relation" content="US:6385349" scheme="references"><meta name="DC.relation" content="US:6434265" scheme="references"><meta name="citation_reference" content="Ken Turkowski, &quot;Making Environment Maps from Fisheye Photographs&quot;, http://www.worldserver.com/turk/quicktimevr/fisheye.html; Jul. 4, 1999."><meta name="citation_reference" content="Ned Greene, &quot;Environment Mapping and Other Applications of World Projections&quot;, Nov. 1986 IEEE."><meta name="citation_patent_number" content="US:6754400"><meta name="citation_patent_application_number" content="US:09/777,912"><link rel="canonical" href="http://www.google.com/patents/US6754400"/><meta property="og:url" content="http://www.google.com/patents/US6754400"/><meta name="title" content="Patent US6754400 - System and method for creation, processing and visualization of omni-directional images"/><meta name="description" content="A system and method for digitally rendering omni-directional images includes capturing images of a region 360 degrees about an origin point in a single plane, and assembling the images in a digital format to create a complete spherical image surrounding the origin point. The spherical image is projected onto faces of a cube surrounding the spherical image. Images which were projected on the faces of the cube may then be rendered to provide an omni-directional image."/><meta property="og:title" content="Patent US6754400 - System and method for creation, processing and visualization of omni-directional images"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("aafpU5KUJ4aAogT024KwDg"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407464522.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("AUS"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("aafpU5KUJ4aAogT024KwDg"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407464522.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("AUS"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us6754400?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US6754400"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=kw1nBAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS6754400&amp;usg=AFQjCNG1CkLDKaP2m2oXMWyVv1WZ-Mx9Bw" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US6754400.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US6754400.pdf"></a><a class="appbar-application-grant-link" data-label="Application" href="/patents/US20020141659"></a><a class="appbar-application-grant-link" data-selected="true" data-label="Grant" href="/patents/US6754400"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US6754400" style="display:none"><span itemprop="description">A system and method for digitally rendering omni-directional images includes capturing images of a region 360 degrees about an origin point in a single plane, and assembling the images in a digital format to create a complete spherical image surrounding the origin point. The spherical image is projected...</span><span itemprop="url">http://www.google.com/patents/US6754400?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US6754400 - System and method for creation, processing and visualization of omni-directional images</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US6754400 - System and method for creation, processing and visualization of omni-directional images" title="Patent US6754400 - System and method for creation, processing and visualization of omni-directional images"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US6754400 B2</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/777,912</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Jun 22, 2004</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Feb 6, 2001</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Feb 6, 2001</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Also published as</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US20020141659">US20020141659</a></span></span></td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">09777912, </span><span class="patent-bibdata-value">777912, </span><span class="patent-bibdata-value">US 6754400 B2, </span><span class="patent-bibdata-value">US 6754400B2, </span><span class="patent-bibdata-value">US-B2-6754400, </span><span class="patent-bibdata-value">US6754400 B2, </span><span class="patent-bibdata-value">US6754400B2</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Rusu+Mihai+Florin%22">Rusu Mihai Florin</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Sever+Serban%22">Sever Serban</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Richard+Wilson,+Jr.%22">Richard Wilson, Jr.</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US6754400.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6754400.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US6754400.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (8),</span> <span class="patent-bibdata-value"><a href="#npl-citations">Non-Patent Citations</a> (2),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (20),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (14),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (8)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/6754400&usg=AFQjCNF2VgDpnHcxRs5BiOn9RpB9lEpVmg">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D6754400&usg=AFQjCNHgAoDI1wFfIzerRWp29VRfxo7mfg">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D6754400B2%26KC%3DB2%26FT%3DD&usg=AFQjCNF7QPzVXkI4FPFo-iDtbSEKAyYinA">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55279499" lang="EN" load-source="patent-office">System and method for creation, processing and visualization of omni-directional images</invention-title></span><br><span class="patent-number">US 6754400 B2</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA50683326" lang="EN" load-source="patent-office"> <div class="abstract">A system and method for digitally rendering omni-directional images includes capturing images of a region 360 degrees about an origin point in a single plane, and assembling the images in a digital format to create a complete spherical image surrounding the origin point. The spherical image is projected onto faces of a cube surrounding the spherical image. Images which were projected on the faces of the cube may then be rendered to provide an omni-directional image.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(10)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6754400B2/US06754400-20040622-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6754400B2/US06754400-20040622-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6754400B2/US06754400-20040622-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6754400B2/US06754400-20040622-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6754400B2/US06754400-20040622-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6754400B2/US06754400-20040622-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6754400B2/US06754400-20040622-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6754400B2/US06754400-20040622-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6754400B2/US06754400-20040622-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US6754400B2/US06754400-20040622-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(40)</span></span></div><div class="patent-text"><div mxw-id="PCLM8691129" lang="EN" load-source="patent-office" class="claims">
    <claim-statement>What is claimed is: </claim-statement> <div class="claim"> <div num="1" id="US-6754400-B2-CLM-00001" class="claim">
      <div class="claim-text">1. A method for digitally rendering omni-directional images comprising the steps of:</div>
      <div class="claim-text">capturing images surrounding an origin point in a at least two hemispheres surrounding the origin point; </div>
      <div class="claim-text">assembling the images in a digital format to create a complete spherical image surrounding the origin point; </div>
      <div class="claim-text">projecting the spherical image onto faces of a to cube surrounding the spherical image; and </div>
      <div class="claim-text">storing images projected on the faces of the cube to provide an omni-directional image. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="2" id="US-6754400-B2-CLM-00002" class="claim">
      <div class="claim-text">2. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00001">claim 1</claim-ref>, wherein the step of assembling the images in a digital format includes the step of transferring two planar fish-eye images to hemispherical images while removing distortions in the planar fish-eye images.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="3" id="US-6754400-B2-CLM-00003" class="claim">
      <div class="claim-text">3. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00001">claim 1</claim-ref>, wherein the step of assembling the images in the digital format includes the step of removing a demarcation line between the images in the digital format by averaging color and brightness characteristics between the images in the digital format.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="4" id="US-6754400-B2-CLM-00004" class="claim">
      <div class="claim-text">4. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00001">claim 1</claim-ref>, wherein the step of assembling the images in the digital format includes the step of removing a demarcation line between the images in the digital format by employing an error function between pixels of the images.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="5" id="US-6754400-B2-CLM-00005" class="claim">
      <div class="claim-text">5. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00001">claim 1</claim-ref>, wherein the images are taken with a fish-eye lens and further comprising the step of removing a demarcation line by removing pixels from the images in the digital format.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="6" id="US-6754400-B2-CLM-00006" class="claim">
      <div class="claim-text">6. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00001">claim 1</claim-ref>, wherein the step of projecting the spherical image onto faces of a cube surrounding the spherical image includes the step of providing a uniform resolution on a whole surface the cube.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="7" id="US-6754400-B2-CLM-00007" class="claim">
      <div class="claim-text">7. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00001">claim 1</claim-ref>, wherein the step of projecting the spherical image onto faces of a cube surrounding the spherical image includes the step of changing a resolution of the image projected on the cube.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="8" id="US-6754400-B2-CLM-00008" class="claim">
      <div class="claim-text">8. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00001">claim 1</claim-ref>, further comprising the step of displaying a portion of the image projected on the cube.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="9" id="US-6754400-B2-CLM-00009" class="claim">
      <div class="claim-text">9. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00008">claim 8</claim-ref>, wherein the step of displaying includes the step of resealing the image projected on the cube to provide a visualized image that creates a sensation of rectilinear movement in the visualized image.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="10" id="US-6754400-B2-CLM-00010" class="claim">
      <div class="claim-text">10. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00001">claim 1</claim-ref>, further comprising the step of dynamically exploring at least portions of a complete image projected on the faces of the cube.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="11" id="US-6754400-B2-CLM-00011" class="claim">
      <div class="claim-text">11. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00010">claim 10</claim-ref>, wherein the step of dynamically exploring includes providing rotations and translations in all degrees of freedom to permit exploring of the cube in a three-dimensional space.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="12" id="US-6754400-B2-CLM-00012" class="claim">
      <div class="claim-text">12. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00001">claim 1</claim-ref>, further comprising the step of transferring from one omni-directional image to another.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="13" id="US-6754400-B2-CLM-00013" class="claim">
      <div class="claim-text">13. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00001">claim 1</claim-ref>, wherein the images are captured by a fish-eye lens and further comprising the step of calibrating the fish-eye lens to determine radial distortion coefficients for determining specific distortions of the lens.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="14" id="US-6754400-B2-CLM-00014" class="claim">
      <div class="claim-text">14. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00001">claim 1</claim-ref>, further comprising the step of providing a user interface to permit manipulation of a displayed portion of the omni-directional image.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="15" id="US-6754400-B2-CLM-00015" class="claim">
      <div class="claim-text">15. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00001">claim 1</claim-ref>, further comprising the step of determining a center of the image to eliminate geometric distortions.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="16" id="US-6754400-B2-CLM-00016" class="claim">
      <div class="claim-text">16. A program storage device readable by machine, tangibly embodying a program of instructions executable by the machine to perform method steps as recited in <claim-ref idref="US-6754400-B2-CLM-00001">claim 1</claim-ref>.</div>
    </div>
    </div> <div class="claim"> <div num="17" id="US-6754400-B2-CLM-00017" class="claim">
      <div class="claim-text">17. A method for digitally rendering omni-directional images comprising the steps of:</div>
      <div class="claim-text">capturing two fish-eye images, the fish-eye images being taken 180 degrees apart with respect to a plane from an origin point; </div>
      <div class="claim-text">converting the two fish-eye images to a digital format; </div>
      <div class="claim-text">assembling the two fish-eye images in the digital format to create a spherical image surrounding the origin point; </div>
      <div class="claim-text">projecting the spherical image onto faces of a cube surrounding the spherical image; </div>
      <div class="claim-text">storing images projected on the faces of the cube; and </div>
      <div class="claim-text">transferring a portion of the images projected on the faces of the cube to provide an omni-directional image to for visualization. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="18" id="US-6754400-B2-CLM-00018" class="claim">
      <div class="claim-text">18. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00017">claim 17</claim-ref>, wherein the step of converting the two fish-eye images to a digital format includes the step of transferring planar fish-eye images to a hemispherical image to remove distortions.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="19" id="US-6754400-B2-CLM-00019" class="claim">
      <div class="claim-text">19. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00017">claim 17</claim-ref>, wherein the step of assembling the two fish-eye images in the digital format to create a spherical image surrounding the origin point includes the step of removing a demarcation line between the two fish-eye images in the digital format by averaging color and brightness characteristics between the two fish-eye images in the digital format.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="20" id="US-6754400-B2-CLM-00020" class="claim">
      <div class="claim-text">20. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00019">claim 19</claim-ref>, wherein the step of removing a demarcation line includes removing halo regions from the two fish-eye images in the digital format.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="21" id="US-6754400-B2-CLM-00021" class="claim">
      <div class="claim-text">21. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00017">claim 17</claim-ref>, wherein the step of projecting the spherical image onto faces of a cube surrounding the spherical image includes the step of providing a uniform resolution on a whole surface the cube.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="22" id="US-6754400-B2-CLM-00022" class="claim">
      <div class="claim-text">22. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00017">claim 17</claim-ref>, further comprising the step of changing a resolution of the omni-directional image projected on the cube.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="23" id="US-6754400-B2-CLM-00023" class="claim">
      <div class="claim-text">23. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00017">claim 17</claim-ref>, further comprising the step of resealing the omni-directional image projected on the cube to provide a visualized image that creates a sensation of rectilinear movement in the visualized image.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="24" id="US-6754400-B2-CLM-00024" class="claim">
      <div class="claim-text">24. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00017">claim 17</claim-ref>, further comprising the step of dynamically exploring at least portions of the omni-directional image on the faces of the cube.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="25" id="US-6754400-B2-CLM-00025" class="claim">
      <div class="claim-text">25. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00024">claim 24</claim-ref>, wherein the step of dynamically exploring includes providing rotations and translations in all degrees of freedom to permit exploring of the cube in a three-dimensional space.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="26" id="US-6754400-B2-CLM-00026" class="claim">
      <div class="claim-text">26. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00017">claim 17</claim-ref>, wherein the step of transferring a portion of the images includes the step of transferring from one omni-directional image to another.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="27" id="US-6754400-B2-CLM-00027" class="claim">
      <div class="claim-text">27. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00017">claim 17</claim-ref>, further comprising the step of calibrating the fish-eye lens to determine radial distortion coefficients for determining specific distortions of the lens.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="28" id="US-6754400-B2-CLM-00028" class="claim">
      <div class="claim-text">28. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00017">claim 17</claim-ref>, wherein the step of assembling the two fish-eye images includes the step of removing a demarcation line between the images in the digital format by employing an error function between pixels of the images.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="29" id="US-6754400-B2-CLM-00029" class="claim">
      <div class="claim-text">29. The method as recited in <claim-ref idref="US-6754400-B2-CLM-00017">claim 17</claim-ref>, further comprising the step of determining a center of the image to eliminate geometric distortions.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="30" id="US-6754400-B2-CLM-00030" class="claim">
      <div class="claim-text">30. A program storage device readable by machine, tangibly embodying a program of instructions executable by the machine to perform method steps as recited in <claim-ref idref="US-6754400-B2-CLM-00017">claim 17</claim-ref>.</div>
    </div>
    </div> <div class="claim"> <div num="31" id="US-6754400-B2-CLM-00031" class="claim">
      <div class="claim-text">31. A system for digitally rendering omni-directional images, comprising:</div>
      <div class="claim-text">a computer device which receives digital images of a spherical region about an origin point; </div>
      <div class="claim-text">the computer device including a program storage device readable by the computer device, tangibly embodying a program of instructions executable by the computer device to: </div>
      <div class="claim-text">assemble the images to create a complete spherical image surrounding the origin point; and </div>
      <div class="claim-text">project the spherical image onto faces of a cube which surrounds the spherical image; and </div>
      <div class="claim-text">a display coupled to the computer device for displaying an omni-directional image mapped on the faces of the cube such that an entire surface of the cube is capable of being explored. </div>
    </div>
    </div> <div class="claim-dependent"> <div num="32" id="US-6754400-B2-CLM-00032" class="claim">
      <div class="claim-text">32. The system as recited in <claim-ref idref="US-6754400-B2-CLM-00031">claim 31</claim-ref>, further comprising a user interface to permit manipulation of a displayed portion of the omni-directional image.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="33" id="US-6754400-B2-CLM-00033" class="claim">
      <div class="claim-text">33. The system as recited in <claim-ref idref="US-6754400-B2-CLM-00032">claim 32</claim-ref>, wherein the user interface permits resealing of the image projected on the cube to provide a visualized image that creates a sensation of rectilinear movement in the visualized image.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="34" id="US-6754400-B2-CLM-00034" class="claim">
      <div class="claim-text">34. The system as recited in <claim-ref idref="US-6754400-B2-CLM-00032">claim 32</claim-ref>, wherein the user interface permits dynamical exploration of at least portions of a complete image projected on the faces of the cube.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="35" id="US-6754400-B2-CLM-00035" class="claim">
      <div class="claim-text">35. The system as recited in <claim-ref idref="US-6754400-B2-CLM-00031">claim 31</claim-ref>, wherein the manipulation of a displayed portion of the omni-directional image includes rotation and rectilinear motion in all directions.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="36" id="US-6754400-B2-CLM-00036" class="claim">
      <div class="claim-text">36. The system as recited in <claim-ref idref="US-6754400-B2-CLM-00031">claim 31</claim-ref>, further comprising a camera including a fish-eye lens which captures two images encompassing a sphere surrounding the origin point.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="37" id="US-6754400-B2-CLM-00037" class="claim">
      <div class="claim-text">37. The system as recited in <claim-ref idref="US-6754400-B2-CLM-00031">claim 31</claim-ref>, wherein the program storage device stores color and brightness for points in the images.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="38" id="US-6754400-B2-CLM-00038" class="claim">
      <div class="claim-text">38. The system as recited in <claim-ref idref="US-6754400-B2-CLM-00031">claim 31</claim-ref>, wherein the faces of the cube include a uniform resolution on a whole surface the cube.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="39" id="US-6754400-B2-CLM-00039" class="claim">
      <div class="claim-text">39. The system as recited in <claim-ref idref="US-6754400-B2-CLM-00031">claim 31</claim-ref>, wherein the system permits rotations and translations of the displayed image in all degrees of freedom to permit exploring of the cube in a three-dimensional space.</div>
    </div>
    </div> <div class="claim-dependent"> <div num="40" id="US-6754400-B2-CLM-00040" class="claim">
      <div class="claim-text">40. The system as recited in <claim-ref idref="US-6754400-B2-CLM-00031">claim 31</claim-ref>, wherein the system permits a transfer from one omni-directional image to another.</div>
    </div>
  </div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES54198852" lang="EN" load-source="patent-office" class="description">
    <heading>BACKGROUND</heading> <p>1. Technical Field</p>
    <p>This disclosure relates to image creation processing and visualization and more particularly, to a system and method for providing digital images with a three dimensional perspective while eliminating distortions and reducing computation time.</p>
    <p>2. Description of the Related Art</p>
    <p>Imaging systems which record real-world images and convert then to digital format are known in the art. These imaging systems employ a camera, such as a digital camera, to digitally render an image in a format compatible for use with computers. These digitally rendered images may then be stored, transferred (e.g., over the Internet) or otherwise manipulated by known techniques.</p>
    <p>Systems known in art also provide for perspective imaging of three dimensional images in two dimensions, (e.g., rendered on a computer display). These systems, however, are often limited and may suffer from at least some of the following disadvantages:</p>
    <p>1. The systems are dedicated exclusively to orientating robots in a limited space;</p>
    <p>2. The visualization by a user consists only of views with rotation and rectilinear movement in a horizontal plane;</p>
    <p>3. Spherical images presented by the systems include distortions on the demarcation line between two of more images which are combined.</p>
    <p>Therefore, a need exists for a system and method, which solves the problems and difficulties of the prior art and increases the capabilities of three dimensional visualization of digitally rendered images.</p>
    <heading>SUMMARY OF THE INVENTION</heading> <p>The creation, processing and visualization of omni-directional images system of the present invention, gathers two plane complementary images, preferably captured with a digital camera endowed with a fish-eye lens having a 180 degree FOV open angle. The system reforms the hemispherical images associated with the planar images taken by the fisheye lens, using specific laws of projection, corrects the radial and geometrical distortions as well as the color and light distortions and assembles the images into a complete spherical image, which is projected on a cube. Viewing of these images creates an impression of turning around in all the directions and of rectilinear movement in virtual space. The impression created is very similar to the human eye perception of the tri-dimensional space, and it permits continuously or a successive visualization of some three-dimensional images including specific details that a user may want to examine closer.</p>
    <p>These and other objects, features and advantages of the present invention will become apparent from the following detailed description of illustrative embodiments thereof, which is to be read in connection with the accompanying drawings.</p>
    <heading>BRIEF DESCRIPTION OF DRAWINGS</heading> <p>This disclosure will present in detail the following description of preferred embodiments with reference to the following figures wherein:</p>
    <p>FIG. 1 is a block/flow diagram for a system/method for creating omni-directional images in accordance with the present invention;</p>
    <p>FIG. 2 is a structure of a planar fish-eye image with distortions taken with a wide opened angle of 180-degree field of view (FOV), two of these fish-eye images taken with 180 degree FOV in a plane include all the information for a spherical representation in accordance with the invention;</p>
    <p>FIG. 3 shows a projection characteristic for the fish-eye lens image of FIG. 2 for providing an ideal equi-distance projection model in accordance with the invention;</p>
    <p>FIG. 4 shows a reference system for a complete spherical image and a cube that the spherical image will be projected on in accordance with the present invention;</p>
    <p>FIG. 5 shows a projection of the spherical image of FIG. 4 on the cube's surfaces along with attributes associated with on the spherical image in accordance with the present invention;</p>
    <p>FIG. 6 is a block/flow diagram of a system/method for dynamically visualizing and exploring omni-directional images in accordance with the present invention;</p>
    <p>FIG. 7 is a perspective view showing the generation of window showing a portion of the cube image needed for dynamic visualization and exploration of the cube image in accordance with the present invention;</p>
    <p>FIG. 8 is an illustrative a perspective view showing the mapping of points onto a cube face in accordance with the present invention; and</p>
    <p>FIG. 9 is block/flow diagram showing the removal of distortions in fisheye images in accordance with the present invention.</p>
    <heading>DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS</heading> <p>The present invention provides a full unitary system capable of creating a complete spherical image using plane images to correct radial, geometrical, color and light distortions associated with formation of the spherical image. The omni-directional spherical image provided by the present invention provides the user with additional degrees of freedom, permitting the viewer to visualize rotation in a horizontal, vertical or any other axes plane, and to realize a rectilinear movement. The present invention permits visualization from all possible directions of a spherical image as well as eliminates distortions of hemispherical images. In addition, the system of the present invention permits the successive visualization of a logical chain of images. For example, in one application of the present invention a user may seamlessly walk between rooms in a virtual environment.</p>
    <p>In one embodiment of the present invention two plane images of an environment are taken a 180 degrees field of view (FOV) to provide a full 360 degree image of the environment. The two plane images are preferably taken in a horizontal plane, both of them being created by a wide angle lens, such as a “fish-eye” lens (the fish-eye lens has a 180 degree FOV wide open angle) and a camera, such as a digital camera, although any camera may be employed and the images digitally rendered at a later time. The system is also able to remake the hemispherical images associated with the plane images by specific projection and correction laws to assemble the images in a complete spherical image and permitting the visualization of different parts of the complete image, creating the sensation to a viewer that the image permits viewing by rotation and rectilinear movement in all directions. Advantageously, the sensation is similar to human eye perception.</p>
    <p>The present invention provides at least the following advantages. The present invention in its software form is compact, unitary and easy to transfer onto and between computers. The present invention creates a complete spherical image based on two plane images, and aides in the calibration of a digital camera working with a fish-eye lens in determining the radial correction parameters. The present invention is capable of filtering out geometrical distortions, light distortions and/or color distortions between images. The present invention is able to take images created at a given location and transmit the realized images over a distance via a network, such as the Internet, a local area network or any other network system. A complete visualization from all directions of different parts of the spherical image is provided, reproducing all the freedom degrees in a three-dimensional space. The present invention also provides optimization between calculation time and the image quality. The present invention permits the successive visualization of some omni-directional images having a frequency that is compatible to the human eye retina remanence (e.g., at least 3 images/second, and preferably greater than or equal to about 30 image/second) so that the result will be a continuous visualization.</p>
    <p>It should be understood that the elements shown in the FIGS. may be implemented in various forms of hardware, software or combinations thereof. Preferably, these elements are implemented in software on one or more appropriately programmed general purpose digital computers having a processor and memory and input/output interfaces. Referring now to the drawings in which like numerals represent the same or similar elements and initially to FIG. 1, a block/flow represents a system/method for creation, processing and visualization of omni-directional images in accordance with the present invention. In block <b>10</b>, a camera <b>100</b>, preferably a digital camera, is provided with a wide angle lens <b>102</b> for capturing a hemispherical image or portion thereof of a surrounding environment. In one particularly useful embodiment, the environment includes a room in a building or house, although other environments are also contemplated. Wide angle lens <b>102</b> may include a fish-eye lens, preferably a 180 degree field of view opened angle lens. Lens <b>102</b> may include a focal distance of, for example, 8 mm. Camera <b>100</b> is fixed on a tripod <b>104</b> or other support structure which permits a rotation of the camera in a horizontal plane (other rotational planes may also be employed). A scaled measurement device <b>106</b> or VERNIER may be employed to measure rotational angles and/or to restrict rotational motion. It is preferable that mount <b>104</b> is able to rotate camera <b>100</b> about the focal point of lens <b>102</b>. Camera <b>100</b> is rotated in the selected plane of rotation. In a preferred embodiment, the capture of the two images is performed by capturing a first image and then rotating the camera in the selected (e.g., horizontal) plane by 180 degrees and capturing a second image. More images may be taken as well, however, two images will be illustratively described throughout this disclose, for simplicity.</p>
    <p>In block <b>12</b>, image information captured by camera <b>100</b> is stored numerically in two memory matrices of system <b>200</b>, e.g., one matrix for each 180 degree image. System <b>200</b> preferably includes a computer system with memory and input/output interfaces. Camera <b>100</b> may be remotely located relative to system <b>200</b>, and images may be transmitted to system <b>200</b> over a network, such as the Internet, satellite, wireless link, etc. The maximum dimension of the matrices corresponds to the best sensitivity/resolution desired or attainable by the fisheye images. In one example, the resolution is about 1400×700 dpi. In memory matrices, colors of pixels or points are stored as defined by the polar coordinates of the pixels of the image. The polar coordinates include the radius r and the angle θ (see e.g., FIG. <b>3</b>). Every memory location stores, the color of the specified point expressed as a combination of the red, blue and green (RGB) colors. The representative of the information is preferably hexadecimal and each color has, for example, 256 color levels (0-absent, 255-total), each color level will have two memory cells reserved so that every memory word will be six memory cells long (two memory location for each color (RGB)). If brightness α of the point is considered, then more memory cells are needed (e.g., two additional memory cells and thus eight memory cells are employed). The image points are mapped to the matrices however distortions in the captured images need to be accounted for.</p>
    <p>In FIG. 2, a plane fish-eye image is illustratively shown to indicate distortion in a three-dimensional space. Walls <b>302</b>, ceiling <b>303</b> and floor <b>305</b> of a room include curved surfaces due to the wide angle (fish-eye lens) distortion. FIG. 2 is a planar projection <b>404</b> of a hemisphere captured by a camera with a fish-eye lens. Light in a hemisphere is captured by fish-eye lens <b>102</b> to permit mapping of the image to a planar projection <b>404</b>. Points on the planar projection may be defined in polar coordinates r<sub>L </sub>and the angle θ<sub>L </sub>(See FIG. <b>3</b>). The subscript L denotes dimensions in a circle of the same latitude.</p>
    <p>Two planar projection images are obtained as described with reference to FIG. <b>1</b>. The two images are offset by a 180-degree angle in a same rotation plane. These two images provide all the information of a three-dimensional space surrounding a camera but with geometrical distortions due to the lens. Even if the fisheye lens has a 180 degree wide open angle, its real angle is about 183 degrees and supplementary information will be represented as a halo that will be cut off from the image at the beginning of the process.</p>
    <p>There are three types of distortions specific to the fisheye images:</p>
    <p>1. Radial distortions</p>
    <p>2. Geometrical distortions</p>
    <p>3. Brightness and color distortions.</p>
    <p>Radial distortion are addressed below with reference to FIG. 1 block <b>13</b> and <b>14</b>. The geometric distortions are reduced or eliminated in accordance with the present invention in block <b>10</b> of FIG. <b>1</b>. FIG. 9 shows a block diagram for method steps for eliminating these distortions in accordance with the present invention.</p>
    <p>In block <b>801</b>, the radial distortions are removed preferably using the real equidistant pattern, as described herein below.</p>
    <p>The geometrical distortions are caused by the difficulty of positioning the camera and the fisheye lens in a perfectly horizontal plane. In real situations, the fisheye image may be rotated slightly in relation to the horizontal plane. The geometrical distortions are also caused the difficulty of rotating the camera with exactly 180° when to get two fisheye images to make a complete image. A supplementary rotation in relation to the center axis of rotation may be needed. The geometrical distortions are caused by the distances in every fisheye image of areas placed at the circumference of the image which is also encountered in the repetitive image (overlap) which, should be removed when the entire omni-directional image is formed.</p>
    <p>The effect of these inherent errors in positioning the camera for each picture, include the following. The errors in the positioning on the horizontal plane (the camera may be tilted forward (backward) or sideways) cause the center of the (circular) fisheye image to be shifted away from the center of the window of the image to be taken. Consequently, the respective correction is needed by establishing of the center of the image (the inventors have noticed that errors of few pixels at the image level cause important deviations in assembling the fisheye images). The errors in the side-to-side tilt of the camera causes the two fisheye images not to assemble correctly and this would be very visible on the border lines of the two images. In addition, the failure to remove the repetitive areas in the two fisheye images leads to the presence of some repeated regions in the omni-directional image, as well as to the unpleasant presence of a (blue) shade.</p>
    <p>Correcting the errors for these geometrical distortions may include the following measures:</p>
    <p>1. fixing the center of the fisheye image;</p>
    <p>2. automatically establishing the radial cutting degree of every fisheye image, as well as the rotation of one image to another for as precise a fitting as possible.</p>
    <p>Geometrical distortions are accounted for in block <b>802</b>, by determining the center of the fisheye image. Each fisheye image is enclosed in a black window (e.g., region <b>308</b> in FIG. <b>2</b>). The center of the fisheye image may be different from the center of the window (the desired image to be taken). Since the entire mathematical equidistant pattern (for radial distortions) functions only with respect to the center of the image, the center needs to be determined accurately or fixed with a precision of, for example, one to two pixels. This action is made more difficult in that acquiring any fisheye image the side-light causes changes in the outline of the image in different areas of the image. Also, the lower part of the image (e.g., the part of the image closest to the camera mount or tripod) includes a dark color (close to black). These inconveniences are overcome with the help of the following actions in accordance with the present invention. These actions may include the following.</p>
    <p>In block <b>805</b>, filtering the fisheye image until, e.g., only two colors exist in the window, preferably, black and white. This underlines or highlights the outline of the fisheye image and removes the side-light effects. In block <b>807</b>, the fisheye image may be covered with a larger number of rays (more light). The rays are considered from a center point inside which may represent the center of the image. In block <b>809</b>, the center of the fisheye image is maintained as the point for which a number of equal rays is the largest (preferably with a precision of one or two pixels). In block <b>811</b>, to increase the precision in determining the center of the fisheye image, the rays from the area of the two poles are avoided (north—superior or top of the image and south—inferior or bottom of the image), which, statistically, are the most disturbed. By performing the center of the image determination and the other steps. An image with reduced geometric distortion is provided.</p>
    <p>In block <b>814</b>, brightness and color distortions are resolved. This may include pixel averaging or other methods as described herein.</p>
    <p>Referring to FIGS. 1 and 3, in block <b>13</b>, each of the two images are converted into a hemispherical image <b>410</b>, removing the geometrical distortions created by the lens, as described above with reference to FIG. <b>9</b>. For radial distortions, the projection laws specific to the ideal model of equi-distance for the fisheye lens may include:</p>
    <p>
      <maths> <formula-text> <i>x</i> <sub>L</sub> <i>=cφ</i> <sub>S </sub>cos θ<sub>S </sub> </formula-text> </maths> </p>
    <p>
      <maths> <formula-text> <i>y</i> <sub>L</sub> <i>=cφ</i> <sub>S </sub>sin θ<sub>S</sub>  (1) </formula-text> </maths> </p>
    <p>where θ<sub>s </sub>and φ<sub>s </sub>are the spherical coordinates in the three-dimensional space, c is a scale factor depending on the focal distance of the objective and on the proper scaling of the image representation. Notice that:</p>
    <p>
      <maths> <formula-text>θ<sub>L</sub>=θ</formula-text> </maths> </p>
    <p>
      <maths> <formula-text>{square root over (<i>x</i> <sub>L</sub> <sup>2</sup> <i>+y</i> <sub>L</sub> <sup>2</sup>)}=<i>cφ</i> <sub>S</sub> <i>=r</i> <sub>L</sub>  (2) </formula-text> </maths> </p>
    <p>where r<sub>L </sub>is the polar distance in a planar fish-eye image and θ<sub>L </sub>is the angle of the fish-eye image. In other words, a circle on the fisheye image transforms to a circle of same latitude on the image hemisphere being projected.</p>
    <p>In block <b>14</b>, radial distortion is specific to this kind of lens. The characteristic difference from the linear dependence of equation (2) for the real model of the fisheye lens shows that:</p>
    <p>
      <maths> <formula-text> <i>r</i> <sub>L</sub> <i>=c</i> <sub>1</sub>φ<sub>S</sub> <i>+c</i> <sub>2</sub>φ<sub>S</sub> <sup>3</sup> <i>+c</i> <sub>3</sub>φ<sub>S</sub> <sup>5</sup>+ . . .   (3) </formula-text> </maths> </p>
    <p>where c<sub>1</sub>, c<sub>2</sub>, C<sub>3 </sub>are constants which directly depend on the physical realization or construction of the fish-eye lens. (other distortion equations are also contemplated). These parameters (c<sub>1</sub>, c<sub>2</sub>, c<sub>3</sub>) account for the distortion of the lens. The lens can therefore be calibrated to determine these parameters or the distortion coefficients provided by the lens manufacturer. A model which includes up to the third term in the series of equation (3) is considered precise enough for the whole scale of fish-eye lens for purposes of this disclosure.</p>
    <p>Parameters or coefficients (e.g., c<sub>1</sub>, c<sub>2</sub>, c<sub>3</sub>) may be determined for the hemispherical model, using a procedure which provides global minimization of a performance index to reduce or eliminate radial distortion effects. Calibration of the lens may be performed one time, and therefore block <b>14</b> is optional if the lens distortion is known or has been previously calibrated. In one embodiment, at least three images, preferably four images (denoted by I<sub>i</sub>) are taken with the fisheye lens at equal angles in a horizontal plane (e.g., 4 images at 90 degrees), so that there will be several overlapping zones (redundancy zones with about a 90 degree overlap) situated between the neighboring images (denoted by A<sub>ij</sub>). The overlap provides redundant information so that the distortion parameters can be determined. The performance indices will be selected by practical type and based on the errors coming from the redundancy form, that means: <maths> <math> <mtable> <mtr> <mtd> <mrow> <mrow> <msub> <mi>J</mi> <mi>ij</mi> </msub> <mo>=</mo> <mrow> <mrow> <mrow> <mfrac> <mn>1</mn> <msub> <mi>A</mi> <mi>ij</mi> </msub> </mfrac> <mo></mo> <mrow> <munder> <mo>∑</mo> <mrow> <msub> <mi>x</mi> <mi>k</mi> </msub> <mo>∈</mo> <msub> <mi>A</mi> <mi>ij</mi> </msub> </mrow> </munder> <mo></mo> <msubsup> <mi>ɛ</mi> <mi>k</mi> <mn>2</mn> </msubsup> </mrow> </mrow> <mo>⇒</mo> <mi>J</mi> </mrow> <mo>=</mo> <mrow> <munder> <mo>∑</mo> <mrow> <mrow> <mo>∀</mo> <mrow> <mo>(</mo> <mrow> <mi>i</mi> <mo>,</mo> <mi>j</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>:</mo> <mrow> <msub> <mi>A</mi> <mi>ij</mi> </msub> <mo>≠</mo> <mn>0</mn> </mrow> </mrow> </munder> <mo></mo> <msub> <mi>J</mi> <mi>ij</mi> </msub> </mrow> </mrow> </mrow> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mrow> <msub> <mi>ɛ</mi> <mi>k</mi> </msub> <mo>=</mo> <mrow> <mrow> <mo>[</mo> <mrow> <mrow> <msub> <mi>I</mi> <mi>i</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <msub> <mi>x</mi> <mi>k</mi> </msub> <mo>)</mo> </mrow> </mrow> <mo>+</mo> <msub> <mi>α</mi> <mi>i</mi> </msub> </mrow> <mo>]</mo> </mrow> <mo>-</mo> <mrow> <mo>{</mo> <mrow> <mrow> <msub> <mi>I</mi> <mi>j</mi> </msub> <mo></mo> <mrow> <mo>[</mo> <mrow> <mi>T</mi> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mi>k</mi> </msub> <mo>;</mo> <mi>p</mi> </mrow> <mo>)</mo> </mrow> </mrow> <mo>]</mo> </mrow> </mrow> <mo>+</mo> <msub> <mi>α</mi> <mi>j</mi> </msub> </mrow> <mo>}</mo> </mrow> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>4</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-M00001.png"> <img id="EMI-M00001" file="US06754400-20040622-M00001.TIF" img-content="math" img-format="tif" alt="Figure US06754400-20040622-M00001" src="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-M00001.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00001" attachment-type="nb" file="US06754400-20040622-M00001.NB"> </attachment> </attachments> </maths> </p>
    <p>where J<sub>ij </sub>determines a square medium error between the images i and j at the overlapping level, p is the vector of the parameters (c<sub>1</sub>, c<sub>2</sub>, and c<sub>3</sub>, the coefficients belonging to the real model of the fisheye objective (see equation (3)) and J is the entire index of performance to be minimized to obtain an image closest to the ideal model for the fish-eye lens. The relation of equation (4) includes a function T which denotes a transformation of connection between the i and j images. This may be determined by:</p>
    <p>
      <maths> <formula-text> <i>T</i>(<i>x</i> <sub>k</sub>)=<i>T</i> <sub>3</sub>(<i>T</i> <sub>2</sub>(<i>T</i> <sub>1</sub>(<i>x</i> <sub>k</sub>)))  (5) </formula-text> </maths> </p>
    <p>where T<sub>1 </sub>(X<sub>k</sub>) corresponds to the transformation of each point x<sub>k </sub>of the plane image j at the corresponding point of the associated hemispherical image; in this direction, the models of equations (1), (2), and (3) will be exploited in an inverse order. Inverse functions Φ and arc tangent are employed as shown in equation (6).</p>
    <p>
      <maths> <formula-text>φ<sub>S</sub>=Φ(<i>r;c</i> <sub>1</sub> <i>,c</i> <sub>2</sub> <i>,c</i> <sub>3</sub>)  (6) </formula-text> </maths> <maths> <math> <mrow> <mrow> <mi>tan</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <msub> <mi>θ</mi> <mi>S</mi> </msub> </mrow> <mo>=</mo> <mrow> <mrow> <mfrac> <msub> <mi>y</mi> <mi>S</mi> </msub> <msub> <mi>x</mi> <mi>S</mi> </msub> </mfrac> <mo>⇒</mo> <msub> <mi>θ</mi> <mi>S</mi> </msub> </mrow> <mo>=</mo> <mrow> <mi>arctan</mi> <mo></mo> <mrow> <mo>(</mo> <mfrac> <msub> <mi>y</mi> <mi>S</mi> </msub> <msub> <mi>x</mi> <mi>S</mi> </msub> </mfrac> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-M00002.png"> <img id="EMI-M00002" file="US06754400-20040622-M00002.TIF" img-content="math" img-format="tif" alt="Figure US06754400-20040622-M00002" src="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-M00002.png" class="patent-full-image"> </a> </div>
        <attachments> <attachment idref="MATHEMATICA-00002" attachment-type="nb" file="US06754400-20040622-M00002.NB"> </attachment> </attachments> </maths> </p>
    <p>A Newton-Raphson procedure or other numerical procedure may be needed to solve equation (6). The coordinates of the associated point on the sphere are</p>
    <p>
      <maths> <formula-text>x<sub>S</sub>=R sin φ<sub>S</sub>cos θ<sub>S </sub> </formula-text> </maths> </p>
    <p>
      <maths> <formula-text>y<sub>S</sub>=R sin φ<sub>S</sub>sin θ<sub>S </sub> </formula-text> </maths> </p>
    <p>
      <maths> <formula-text>z<sub>S</sub>=R cos φ<sub>S</sub>  (7) </formula-text> </maths> </p>
    <p>T<sub>2 </sub>in equation (5) corresponds to rotation matrices (in a horizontal plane) between the image i and the image j;</p>
    <p>T<sub>3 </sub>in equation (5) corresponds to the hemispherical image j in the planar fish-eye projection image, according to the models of equations (1) and (3).</p>
    <p>To establish the indices of performance error the color and light attributes of those points that are common for i and j images are used to determine overlap between the images.</p>
    <p>In block <b>15</b>, a complete spherical image is created by assembling two hemispheres and eliminating the redundant information on the limits of the fish-eye images. Care should be taken to realize correctly, the coincidence of the equal latitude lines of the hemispherical images. The convention described below may be used: <maths> <math> <mrow> <msub> <mi>ϕ</mi> <mi>S</mi> </msub> <mo>∈</mo> <mrow> <mrow> <mo>[</mo> <mrow> <mn>0</mn> <mo>,</mo> <mfrac> <mi>π</mi> <mn>2</mn> </mfrac> </mrow> <mo>]</mo> </mrow> <mo>⇒</mo> <mrow> <mi>points</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>of</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>the</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>first</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>image</mi> </mrow> </mrow> </mrow> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-M00003.png"> <img id="EMI-M00003" file="US06754400-20040622-M00003.TIF" img-content="math" img-format="tif" alt="Figure US06754400-20040622-M00003" src="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-M00003.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00003" attachment-type="nb" file="US06754400-20040622-M00003.NB"> </attachment> </attachments> </maths> <maths> <math> <mtable> <mtr> <mtd> <mrow> <msub> <mi>ϕ</mi> <mi>S</mi> </msub> <mo>∈</mo> <mrow> <mrow> <mrow> <mo>(</mo> <mrow> <mfrac> <mi>π</mi> <mn>2</mn> </mfrac> <mo>,</mo> <mi>π</mi> </mrow> </mrow> <mo>]</mo> </mrow> <mo>⇒</mo> <mrow> <mi>points</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>of</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>the</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>second</mi> <mo></mo> <mstyle> <mtext> </mtext> </mstyle> <mo></mo> <mi>image</mi> </mrow> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>8</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-M00004.png"> <img id="EMI-M00004" file="US06754400-20040622-M00004.TIF" img-content="math" img-format="tif" alt="Figure US06754400-20040622-M00004" src="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-M00004.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00004" attachment-type="nb" file="US06754400-20040622-M00004.NB"> </attachment> </attachments> </maths> </p>
    <p>The corrections that are performed, in block <b>15</b>, include the removal the peripherical pieces of the plane fisheye images that are strongly distorted (the halo). This may be realized by cutting out of the images the external pieces having a coefficient of about, e.g., 98% from the image radius. The light between the two hemispherical images is corrected to ensure consistency in viewing. This may be performed by taking a mean value between parameters of the images (e.g., brightness, color, etc.) (see block <b>814</b> in FIG. <b>9</b>). Corrections on the demarcation line between the two hemispherical images is performed. For this purpose, the color, brightness, contrast, etc. attributes of the pixels belonging to the limits (frontiers) of the neighboring images are mediated (average or mean value is employed). One hemispherical image may be employed as a reference image while the other hemispherical image is adjusted to reduce or eliminate the demarcation line. In a preferred embodiment, the mean value may be linearly interpolated over a band or group of pixels so that the color, brightness, etc. attributes of the pixels in a whole band are merged to hide or erase the demarcation line.</p>
    <p>Alignment of the two hemispherical images may be performed by an alignment method. The alignment method includes the automatic determination of the radial cutting degree (the amount of the image to be cut off) of each fisheye image, as well as of the rotation of the image. In the framework of a computer program, a sub-program was created by the inventors which permits the successive cutting of a single row of pixels at a time from the image, followed by a rotation with respect to the z axis to move the images closer.</p>
    <p>Lines, brightness, color or other criteria in the image give clues as to the orientation of the image. By determining the errors between pixels between the two images, (error functions are known in the art), an iterative process is performed to shift the rotation (after cutting a previous row of pixels) until a minimum is achieved for a threshold criteria based on the error between the pixels of the corresponding images situated on the circumference of the two fisheye images. The minimum achieved indicated the demarcation line at which the images are to be combined in an omni-directional image. In block <b>16</b>, a complete spherical image is stored.</p>
    <p>In block <b>17</b>, a mapping of the spherical image on a cube's faces is performed. To simplify the relation, the present disclosure will consider a cube <b>502</b> directed to a spherical image <b>504</b> as illustratively presented in FIG. 4. A corresponding projection of image <b>504</b> is illustratively represented according to FIG. <b>5</b>. In FIG. 5, each point P<sub>S </sub>on the sphere image <b>504</b> is associated with and projected to a point P<sub>c </sub>on cube <b>502</b> that will gather all the attributes of color, contrast and light of the point on the sphere <b>504</b> (the corresponding point in the initial fish-eye images). The correspondence is denoted by:</p>
    <p>
      <maths> <formula-text>(x<sub>S</sub>, y<sub>S</sub>, z<sub>S</sub>)→(x<sub>C</sub>, y<sub>C</sub>, z<sub>C</sub>)  (9) </formula-text> </maths> </p>
    <p>The image projection to a cube provides many advantages over the prior art as will described in greater detail below.</p>
    <p>Returning to FIG. 1, block <b>18</b> includes a user interface, which may include a keypad, mouse, touch screen device, a speech recognition system or any other input device. In one embodiment, the user interface is employed with blocks <b>12</b>-<b>17</b> to control the mapping process. For example, a user may choose a pair of fisheye images that will be processed, fix the resolution for the cubical mapping images, filter the images (e.g., attaching the neighboring points of the images by the mean of color attributes, correct the light between the two hemispherical images, make corrections on the demarcation line between the two hemispherical images (e.g., blur), or perform other user controlled functions. Automation of any or all of these processes is also contemplated. Block <b>19</b> includes a display device which is employed in the creation, processing and visualization of images and is employed by a user to perform interfacing tasks, etc. in accordance with the description with reference to FIG. <b>1</b>.</p>
    <p>To obtain images of a given resolution and of a uniform density on the faces of the cube, in block <b>17</b>, the exploitation of the previously described algorithm will be made on the inverse correspondence. That means exploitation of each face of the cube <b>504</b> with a given resolution (the resolution can be modified as desired by a user) on the two directions of the associated plane for that face of the cube is performed, where the current point is P<sub>C </sub>(x<sub>C</sub>, x<sub>C</sub>, z<sub>C</sub>) having known coordinates. P<sub>C </sub>(x<sub>C</sub>, x<sub>C</sub>, z<sub>C</sub>) will receive the color attributes of the correspondent point of the plane fish-eye image, previously taken. The point on the sphere corresponding to P<sub>C </sub>is P<sub>S </sub>(x<sub>S</sub>, y<sub>S</sub>, z<sub>S</sub>) and P<sub>S </sub>has polar coordinates:</p>
    <p> <maths> <formula-text> <i>R</i> <sub>S</sub> <i>={square root over (x<sub>C</sub> <sup>2</sup> <i>+y</i> <sub>C</sub> <sup>2</sup> <i>+z</i> <sub>C</sub> <sup>2</sup>)}</i> </formula-text> </maths> <maths> <math> <mrow> <mrow> <msub> <mi>ϕ</mi> <mi>S</mi> </msub> <mo>=</mo> <mrow> <mrow> <mi>arctan</mi> <mo></mo> <mfrac> <msub> <mi>z</mi> <mi>C</mi> </msub> <msqrt> <mrow> <msubsup> <mi>x</mi> <mi>C</mi> <mn>2</mn> </msubsup> <mo>+</mo> <msubsup> <mi>y</mi> <mi>C</mi> <mn>2</mn> </msubsup> </mrow> </msqrt> </mfrac> </mrow> <mo>⇒</mo> <mrow> <msub> <mi>P</mi> <mi>S</mi> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mi>S</mi> </msub> <mo>,</mo> <msub> <mi>y</mi> <mi>S</mi> </msub> <mo>,</mo> <msub> <mi>z</mi> <mi>S</mi> </msub> </mrow> <mo>)</mo> </mrow> </mrow> </mrow> </mrow> <mo>;</mo> </mrow> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-M00005.png"> <img id="EMI-M00005" file="US06754400-20040622-M00005.TIF" img-content="math" img-format="tif" alt="Figure US06754400-20040622-M00005" src="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-M00005.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00005" attachment-type="nb" file="US06754400-20040622-M00005.NB"> </attachment> </attachments> </maths> θ<sub>S</sub>=arc tan <i>g</i>(<i>x</i> <sub>C</sub> <i>+jy</i> <sub>C</sub>); <i>j={square root over (−<b>1</b>)}</i>  (<b>10</b>)</p>
    <p>and these coordinates may be passed into Cartesian Coordinates with equation (5). The points of the fish-eye images that will be attached to P<sub>S </sub>are P<sub>L </sub>(r<sub>L</sub>, θ<sub>L</sub>), having the polar coordinates:</p>
    <p>
      <maths> <formula-text> <i>r</i> <sub>L</sub> <i>=c</i> <sub>1</sub>φ<sub>S</sub> <i>+c</i> <sub>2</sub>φ<sub>S</sub> <sup>3</sup> <i>+c</i> <sub>3</sub>φ<sub>S</sub> <sup>5 </sup> </formula-text> </maths> </p>
    <p>
      <maths> <formula-text>θ<sub>L</sub>=θ<sub>S</sub>  (11) </formula-text> </maths> </p>
    <p>Once P<sub>L</sub>'s attributes are identified, P<sub>L</sub>'s attributes are attached to P<sub>C</sub>. Using this procedure, the whole mapping on the cube operation is performed quickly and efficiently. Advantageously, the resolution of the mapping on the cube can be easily selected and changed. A uniform fisheye image projected to a cube may include areas of non-uniformity.</p>
    <p>Referring to FIG. 8, in accordance with the present invention, the resolution on cube faces <b>702</b> is determined or selected by a user. Each face of the cube is divided with a step size chosen for each direction (e.g., x and y). The step size or spacings (shown illustratively as a grid <b>701</b> in FIG. 8) will determine the resolution of that face of the cube. As shown in FIG. 8, a point P<sub>C </sub>on the cube may be selected based on the desired spacings (resolution) of grid lines of the cube face <b>702</b>. A point P<sub>S </sub>on the sphere <b>504</b> is determined in correspondence with P<sub>C</sub>. Using a reference point O (e.g., origin), P<sub>S </sub>is identified for a point P<sub>L </sub>from the fisheye lens image <b>703</b>. Once P<sub>L </sub>is identified attributes of the point are known, e.g., color, contrast, luminosity, brightness, etc. The fisheye image point is then attached to P<sub>C</sub>.</p>
    <p>In some situations, P<sub>L </sub>may not coincide with an image point of the fisheye image. In this situation the average or median value of attributes (e.g., color, contrast, luminosity, brightness, etc.) for surrounding or neighboring points may be employed and mapped to the cube face <b>701</b>.</p>
    <p>Referring to FIG. 6, a block/flow diagram of a visualization system <b>202</b> for cubical mapped images is illustratively shown in accordance with the invention. System <b>202</b> includes a program storage device and processor, such as a computer for providing visualization processing for a user. System <b>202</b> may be a remote personal computer system connected to a network <b>30</b>, such as the Internet. Block <b>21</b> has access to the images lying on the faces of the cube as determined in block <b>17</b> of FIG. <b>1</b>. Images of block <b>17</b> may be provided through the Internet, or other network, be stored within system <b>202</b> for visual rendering on display <b>19</b>. From the images of block <b>17</b> of FIG. 1, rectangular pieces are rendered, in block <b>21</b>, and submitted to visualization on a display or screen <b>19</b>. Data may be presented for a rectangular piece <b>602</b>, and illustratively shown in FIG. <b>7</b>. In block <b>22</b>, the vectors {right arrow over (P)}<sub>0</sub>, {right arrow over (P)}<sub>1</sub>, {right arrow over (P)}<sub>2 </sub>are generated and will limit the rectangular area to be visualized (e.g., piece <b>602</b>). The complete visualized surface is a current vector</p>
    <p>
      <maths> <formula-text>{right arrow over (<i>Q</i>)}(<i>u,v</i>)=<i>{right arrow over (P)}</i> <sub>o</sub> <i>+u</i>(<i>{right arrow over (P)}</i> <sub>1</sub> <i>−{right arrow over (P)}</i> <sub>o</sub>)+<i>v</i>(<i>{right arrow over (P)}</i> <sub>2</sub> <i>−{right arrow over (P)}</i> <sub>o</sub>); <i>uε[</i>0,1<i>], vε[</i>0,1]  (12) </formula-text> </maths> </p>
    <p>The current vector will cross the faces of the cube giving the user a perspective view. In accordance with the present invention, the user is able to view any area in the spherical region surrounding the origin O. The viewer can control the angles of rotation as indicated by arrows “A”, “B” and “C” by employing interface <b>18</b>. Advantageously, the user can look upward, downward and side to side, 360 degrees in any direction. In this way, ceiling, floors, walls, etc. may be viewed in a room or environment.</p>
    <p>Each resulting point in rectangular piece <b>602</b> will include the color and light attributes presented by the equation (9) (determined in FIG. 1) and stored in the block <b>23</b>. Rotation in a horizontal plane and/or in a vertical plane, the {right arrow over (P)}<sub>0</sub>, {right arrow over (P)}<sub>1</sub>, {right arrow over (P)}<sub>2 </sub>vectors permit visualization of all or part of pieces of the cube's faces. This creates a sensation of natural motion to the user turning to look around a room or other environment. For example, turning 2 “P” vectors around a third “P” vector, the effect of turning around on an axis is experienced. The user can also move into other rooms or areas by employing similar methods. For example, changing the open angles between the three vectors the effect of rectilinear movement is realized to the effect of getting nearer or moving away from the visualized image, such as a doorway. As a new room is entered the surrounding cube is updated for the new room to supply new information about the surrounding area.</p>
    <p>To increase the visualization speed and to obtain a better image with a higher resolution, a visualization subsystem <b>25</b> may be included which directly employs specialized subroutines in triangle rastering. These subroutines may include specially developed routines or commercially available subroutines, such as for example, an open graphics library (OPENGL) program. The open graphics library program can be employed with cubical formulas only. Spherical systems of the prior art cannot employ open graphics library programs since only spherical systems are employed. The open graphic library program advantageously employs the same baleation functions (directions of viewing, e.g., right to left and up and down) as the triangles which produce the visualization window.</p>
    <p>Mapping or rendering, e.g., on the faces of a cube, is compatible to such a triangle rastering visualization. As accelerated video cards have their own facilities of triangle rastering, this visualization subsystem <b>25</b> may be generated to specify the cube having the complete image on its faces and to specify the texture of the cube's faces, thus specifying the resolution too. The visualization place needs to be determined (by the way of its coordinates), e.g., the center of the cube. The part of the cube to be visualized is specified with the help of the visualization window situated inside the cube. The visualization window provides a perspective view of the faces of the cube. The visualization window is specified by points, e.g., P<sub>1 </sub>(x<sub>1</sub>, y<sub>1</sub>, z<sub>1</sub>), P<sub>2 </sub>(x<sub>2</sub>, y<sub>2</sub>, z<sub>2</sub>), P<sub>3 </sub>(x<sub>3</sub>, y<sub>3</sub>, z<sub>3</sub>), all of them being situated on the cube's faces. A fourth point considered here as an opposite to P<sub>1 </sub>and has the coordinates: <maths> <math> <mtable> <mtr> <mtd> <mrow> <mrow> <msub> <mi>P</mi> <mn>4</mn> </msub> <mo></mo> <mrow> <mo>(</mo> <mrow> <msub> <mi>x</mi> <mn>4</mn> </msub> <mo>,</mo> <msub> <mi>y</mi> <mn>4</mn> </msub> <mo>,</mo> <msub> <mi>z</mi> <mn>4</mn> </msub> </mrow> <mo>)</mo> </mrow> </mrow> <mo>:</mo> <mrow> <mo>{</mo> <mtable> <mtr> <mtd> <mrow> <msub> <mi>x</mi> <mn>4</mn> </msub> <mo>=</mo> <mrow> <mrow> <mo>-</mo> <msub> <mi>x</mi> <mn>1</mn> </msub> </mrow> <mo>+</mo> <msub> <mi>x</mi> <mn>2</mn> </msub> <mo>+</mo> <msub> <mi>x</mi> <mn>3</mn> </msub> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>y</mi> <mn>4</mn> </msub> <mo>=</mo> <mrow> <mrow> <mo>-</mo> <msub> <mi>y</mi> <mn>1</mn> </msub> </mrow> <mo>+</mo> <msub> <mi>y</mi> <mn>2</mn> </msub> <mo>+</mo> <msub> <mi>y</mi> <mn>3</mn> </msub> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>z</mi> <mn>4</mn> </msub> <mo>=</mo> <mrow> <mrow> <mo>-</mo> <msub> <mi>z</mi> <mn>1</mn> </msub> </mrow> <mo>+</mo> <msub> <mi>z</mi> <mn>2</mn> </msub> <mo>+</mo> <msub> <mi>z</mi> <mn>3</mn> </msub> </mrow> </mrow> </mtd> </mtr> </mtable> </mrow> </mrow> </mtd> <mtd> <mrow> <mo>(</mo> <mn>13</mn> <mo>)</mo> </mrow> </mtd> </mtr> </mtable> </math> <div class="patent-image small-patent-image"> <a href="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-M00006.png"> <img id="EMI-M00006" file="US06754400-20040622-M00006.TIF" img-content="math" img-format="tif" alt="Figure US06754400-20040622-M00006" src="//patentimages.storage.googleapis.com/US6754400B2/US06754400-20040622-M00006.png" class="patent-full-image"> </a> </div> <attachments> <attachment idref="MATHEMATICA-00006" attachment-type="nb" file="US06754400-20040622-M00006.NB"> </attachment> </attachments> </maths> </p>
    <p>The points are taken close enough to the point of visualization so that visualization window remains inside the cube for all positions of the window.</p>
    <p>In case a successive visualization of several images is desired (being in a logical sequence or chain of images) to be mapped on a cube. A “jumper point” is attached to every image so that as the “jumper point” is reached (activated), the current image is closed and a new image is opened to be visualized according to the above mentioned procedure. Jumper points may be employed in doorways or other areas between rooms, on objects within the rooms, for example, an object may be imaged which when approached (e.g., moving closer to it in virtual space) or clicked on will provide new images to open (e.g., a close up, etc.).</p>
    <p>The creation processing and visualization omni-directional images system may be employed in a plurality of applications. For example, in a particular useful embodiment, the present invention may include images for a home or building to be sold by a real estate agent. The real estate agent stores wide angle photographs which can be downloaded and viewed by potential buyers over the Internet. A whole presentation of the indoor and the outside of some chalets, apartments, hotels, offices or houses for sale may be provided. This is possible be capturing the fish-eye images of all spaces that will be visualized and storing the images in a desired sequence. A virtual tour of the property is therefore realized where a person can walk in the front door and tour the building by moving through the building room by room with the capability of viewing walls ceiling and floors.</p>
    <p>A real estate that has such a system will be able to offer to its clients full information on the objects for sale and to give them opportunity of visualizing the desired building exactly as if they would have visited it, with all the details. All the details includes woodwork, furniture view from windows and any other details present on the premises. Jump points may be placed on objects within the home to permit other images to be seen. For example, a picture or window may include a jump point which changes the resolution or opens a new image.</p>
    <p>The present invention may be employed to make presentations of indoor or outdoor historical or architectural monuments. For example, tourist agencies or historic scenes may be preserved or presented to a user by software or Internet for personal amusement or as an educational tool. The present invention may be employed to make a full presentation of galleries or museums. The virtual visitor will have the feeling he is really moving through this works of art and will have the opportunity to make inquiries or learn detail about the objects or pictures on display. The present invention may be employed to capture three dimensional images of events or gatherings, e.g., a family gathering, sporting event, etc., to create virtual albums including complete images which can be dynamically visualized every time so that the user will really participate in the action in the visualized place. Some natural or artificial phenomena having a high level of danger or taking place in difficult access areas (e.g., outer space) may be mapped and visualized by the present invention.</p>
    <p>Another aspect of the present invention employs real-time imaging. The present invention may employ a camera set up which is continuously updating images. In this way, the system is able to supervise continuously any area that needs special attention (e.g., commercial space, banks, museums, treasures). For this purpose, two fish-eye lenses connected located 180 degrees apart are needed, and the system of FIG. 1 will be used to continuously update images and convert the images for visualization (by the system of FIG. <b>7</b>). In a preferred embodiment, the system can capture frames at a rate of about 30 frames per second. The frames can be processed and visualized but the user will have to accept a poorer quality of the images unless higher processing rates are achieved.</p>
    <p>Other applications of the present invention may include supervising and recording the access of persons in private houses or public institutions, miniaturizing the fish-eye lens to provide a system which can be employed in the medical field to visualize internal organs to improve diagnosis. The system of the present invention may be inserted into other systems able to create virtual realities on educational purposes or simply for fun or it may be used as a training tool for training of pilots, astronauts and navy personnel, etc. Many other uses and applications may be realized by the present invention.</p>
    <p>Having described preferred embodiments for a novel system and method for creation, processing and visualization of omni-directional images (which are intended to be illustrative and not limiting), it is noted that modifications and variations can be made by persons skilled in the art in light of the above teachings. It is therefore to be understood that changes may be made in the particular embodiments of the invention disclosed which are within the scope and spirit of the invention as outlined by the appended claims. Having thus described the invention with the details and particularity required by the patent laws, what is claimed and desired protected by Letters Patent is set forth in the appended claims.</p>
    </div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4515450">US4515450</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 29, 1982</td><td class="patent-data-table-td patent-date-value">May 7, 1985</td><td class="patent-data-table-td ">Mcdonnell Douglas Corporation</td><td class="patent-data-table-td ">Transparency for use with a flying spot scanner to generate images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5185667">US5185667</a></td><td class="patent-data-table-td patent-date-value">May 13, 1991</td><td class="patent-data-table-td patent-date-value">Feb 9, 1993</td><td class="patent-data-table-td ">Telerobotics International, Inc.</td><td class="patent-data-table-td ">Omniview motionless camera orientation system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5582518">US5582518</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 21, 1995</td><td class="patent-data-table-td patent-date-value">Dec 10, 1996</td><td class="patent-data-table-td ">Thomson-Csf</td><td class="patent-data-table-td ">System for restoring the visual environment of a pilot in a simulator</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5687249">US5687249</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 1, 1994</td><td class="patent-data-table-td patent-date-value">Nov 11, 1997</td><td class="patent-data-table-td ">Nippon Telephone And Telegraph</td><td class="patent-data-table-td ">Method and apparatus for extracting features of moving objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5877801">US5877801</a></td><td class="patent-data-table-td patent-date-value">Jun 5, 1997</td><td class="patent-data-table-td patent-date-value">Mar 2, 1999</td><td class="patent-data-table-td ">Interactive Pictures Corporation</td><td class="patent-data-table-td ">System for omnidirectional image viewing at a remote location without the transmission of control signals to select viewing parameters</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6320584">US6320584</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 13, 1999</td><td class="patent-data-table-td patent-date-value">Nov 20, 2001</td><td class="patent-data-table-td ">Imove Inc.</td><td class="patent-data-table-td ">Method and apparatus for simulating movement in multidimensional space with polygonal projections from subhemispherical imagery</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6385349">US6385349</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 3, 2000</td><td class="patent-data-table-td patent-date-value">May 7, 2002</td><td class="patent-data-table-td ">Mgi Software Corporation</td><td class="patent-data-table-td ">Method and system for compositing images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6434265">US6434265</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 25, 1998</td><td class="patent-data-table-td patent-date-value">Aug 13, 2002</td><td class="patent-data-table-td ">Apple Computers, Inc.</td><td class="patent-data-table-td ">Aligning rectilinear images in 3D through projective registration and calibration</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="npl-citations"></a><div class="patent-section-header"><span class="patent-section-title">Non-Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th colspan="3"class="patent-data-table-th">Reference</th></tr></thead><tr><td class="patent-data-table-td ">1</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ken Turkowski, "<a href='http://scholar.google.com/scholar?q="Making+Environment+Maps+from+Fisheye+Photographs"'>Making Environment Maps from Fisheye Photographs</a>", http://www.worldserver.com/turk/quicktimevr/fisheye.html; Jul. 4, 1999.</td></tr><tr><td class="patent-data-table-td ">2</td><td class="patent-data-table-td "></td><td class="patent-data-table-td ">Ned Greene, "<a href='http://scholar.google.com/scholar?q="Environment+Mapping+and+Other+Applications+of+World+Projections"'>Environment Mapping and Other Applications of World Projections</a>", Nov. 1986 IEEE.</td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6985183">US6985183</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 19, 2001</td><td class="patent-data-table-td patent-date-value">Jan 10, 2006</td><td class="patent-data-table-td ">Appro Technology Inc.</td><td class="patent-data-table-td ">Method for exploring viewpoint and focal length of camera</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7219997">US7219997</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 27, 2003</td><td class="patent-data-table-td patent-date-value">May 22, 2007</td><td class="patent-data-table-td ">Riken</td><td class="patent-data-table-td ">Method and apparatus for three dimensionally displaying eyeground and measuring coordinates thereof</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7292722">US7292722</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 12, 2006</td><td class="patent-data-table-td patent-date-value">Nov 6, 2007</td><td class="patent-data-table-td ">Ntt Docomo, Inc.</td><td class="patent-data-table-td ">Representation and coding of panoramic and omnidirectional images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7308131">US7308131</a></td><td class="patent-data-table-td patent-date-value">Dec 3, 2002</td><td class="patent-data-table-td patent-date-value">Dec 11, 2007</td><td class="patent-data-table-td ">Ntt Docomo, Inc.</td><td class="patent-data-table-td ">Representation and coding of panoramic and omnidirectional images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7412091">US7412091</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 7, 2001</td><td class="patent-data-table-td patent-date-value">Aug 12, 2008</td><td class="patent-data-table-td ">Ilookabout Inc.</td><td class="patent-data-table-td ">System and method for registration of cubic fisheye hemispherical images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7444035">US7444035</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 8, 2007</td><td class="patent-data-table-td patent-date-value">Oct 28, 2008</td><td class="patent-data-table-td ">Arcsoft, Inc.</td><td class="patent-data-table-td ">Better picture for inexpensive cameras</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7831471">US7831471</a></td><td class="patent-data-table-td patent-date-value">Jun 3, 2005</td><td class="patent-data-table-td patent-date-value">Nov 9, 2010</td><td class="patent-data-table-td ">Total Intellectual Property Protection Services, LLC</td><td class="patent-data-table-td ">Virtual digital imaging and method of using the same in real estate</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7834883">US7834883</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 3, 2005</td><td class="patent-data-table-td patent-date-value">Nov 16, 2010</td><td class="patent-data-table-td ">Total Intellectual Property Protection Services, LLC</td><td class="patent-data-table-td ">Virtual digital imaging and method of using the same in real estate</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7865013">US7865013</a></td><td class="patent-data-table-td patent-date-value">Jul 10, 2008</td><td class="patent-data-table-td patent-date-value">Jan 4, 2011</td><td class="patent-data-table-td ">Ilookabout Inc.</td><td class="patent-data-table-td ">System and method for registration of cubic fisheye hemispherical images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8009903">US8009903</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 27, 2007</td><td class="patent-data-table-td patent-date-value">Aug 30, 2011</td><td class="patent-data-table-td ">Panasonic Corporation</td><td class="patent-data-table-td ">Image processor, image processing method, storage medium, and integrated circuit that can adjust a degree of depth feeling of a displayed high-quality image</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8241125">US8241125</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 17, 2007</td><td class="patent-data-table-td patent-date-value">Aug 14, 2012</td><td class="patent-data-table-td ">Sony Computer Entertainment Europe Limited</td><td class="patent-data-table-td ">Apparatus and method of interaction with a data processor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8379976">US8379976</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 6, 2010</td><td class="patent-data-table-td patent-date-value">Feb 19, 2013</td><td class="patent-data-table-td ">Dai Nippon Printing Co., Ltd.</td><td class="patent-data-table-td ">Image processing apparatus and method and a computer-readable recording medium on which an image processing program is stored</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8463071">US8463071</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 13, 2012</td><td class="patent-data-table-td patent-date-value">Jun 11, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Navigating images using image based geometric alignment and object based controls</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8515159">US8515159</a></td><td class="patent-data-table-td patent-date-value">Mar 14, 2012</td><td class="patent-data-table-td patent-date-value">Aug 20, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Navigating images using image based geometric alignment and object based controls</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8635557">US8635557</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 10, 2008</td><td class="patent-data-table-td patent-date-value">Jan 21, 2014</td><td class="patent-data-table-td ">205 Ridgmont Solutions, L.L.C.</td><td class="patent-data-table-td ">System to navigate within images spatially referenced to a computed space</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8744214">US8744214</a></td><td class="patent-data-table-td patent-date-value">May 21, 2013</td><td class="patent-data-table-td patent-date-value">Jun 3, 2014</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Navigating images using image based geometric alignment and object based controls</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090076719">US20090076719</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 10, 2008</td><td class="patent-data-table-td patent-date-value">Mar 19, 2009</td><td class="patent-data-table-td ">Pixearth Corporation</td><td class="patent-data-table-td ">System to navigate within images spatially referenced to a computed space</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100254602">US20100254602</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 6, 2010</td><td class="patent-data-table-td patent-date-value">Oct 7, 2010</td><td class="patent-data-table-td ">Dai Nippon Printing Co., Ltd.</td><td class="patent-data-table-td ">Image processing apparatus and method and a computer-readable recording medium on which an image processing program is stored</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110262044">US20110262044</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 2, 2010</td><td class="patent-data-table-td patent-date-value">Oct 27, 2011</td><td class="patent-data-table-td ">Elitegroup Computer Systems Co., Ltd.</td><td class="patent-data-table-td ">Energy saving method for electronic device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120169770">US20120169770</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 13, 2012</td><td class="patent-data-table-td patent-date-value">Jul 5, 2012</td><td class="patent-data-table-td ">University Of Washington</td><td class="patent-data-table-td ">Navigating Images Using Image Based Geometric Alignment And Object Based Controls</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S285000">382/285</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc345/defs345.htm&usg=AFQjCNF0b52M2HqQQp5rThx3mQ75nwjbGg#C345S424000">345/424</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S167000">382/167</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S296000">382/296</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S284000">382/284</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S299000">382/299</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S274000">382/274</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S154000">382/154</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S295000">382/295</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc382/defs382.htm&usg=AFQjCNEqDOPTZQm3eGeSecvjb7gyEoTppw#C382S275000">382/275</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc345/defs345.htm&usg=AFQjCNF0b52M2HqQQp5rThx3mQ75nwjbGg#C345S419000">345/419</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G06T0015200000">G06T15/20</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=kw1nBAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06T15/205">G06T15/205</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G06T15/20B</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Sep 24, 2013</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIMS 2, 8, 9, 12, 18, 23 AND 33 ARE CANCELLED.CLAIMS 1, 4-6, 14, 16, 17, 30, 31 AND 38 ARE DETERMINED TO BE PATENTABLE AS AMENDED.CLAIMS 3, 7, 10, 11, 13, 15, 19-22, 24-29, 32, 34-37, 39 AND 40, DEPENDENT ON AN AMENDED CLAIM, ARE DETERMINED TO BE PATENTABLE.NEW CLAIMS 41-75 ARE ADDED AND DETERMINED TO BE PATENTABLE.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 19, 2012</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jun 19, 2012</td><td class="patent-data-table-td ">SULP</td><td class="patent-data-table-td ">Surcharge for late payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">7</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 6, 2012</td><td class="patent-data-table-td ">REMI</td><td class="patent-data-table-td ">Maintenance fee reminder mailed</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 10, 2012</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20111121</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 7, 2009</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">TOUR TECHNOLOGY SYSTEMS, LLC, NEW YORK</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:WILSON, RICHARD, JR.;REEL/FRAME:022645/0752</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090507</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Dec 18, 2007</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Feb 6, 2001</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">WILSON, RICHARD, JR., NEW YORK</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:FLORIN, RUSU MIHAI;SERBAN, SEVER;REEL/FRAME:011548/0229</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20010131</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">WILSON, RICHARD, JR. 221-29 105TH AVENUEQUEENS VIL</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:FLORIN, RUSU MIHAI /AR;REEL/FRAME:011548/0229</span></div></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_4ff636b3d23669b7103f3b3a3a18b4cd.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U0S9WpXak5vM9rdZ7O5x-hAuj-YmQ\u0026id=kw1nBAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U3UIy7wxramGPjvMfZ3f_oF95acAA\u0026id=kw1nBAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U0wde7cgtTMjVJYfB1AHhXxIZz1Ww","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/System_and_method_for_creation_processin.pdf?id=kw1nBAABERAJ\u0026output=pdf\u0026sig=ACfU3U1q6pfTWQ_WUOSxUy-RjvFlBKyKLw"},"sample_url":"http://www.google.com/patents/reader?id=kw1nBAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>