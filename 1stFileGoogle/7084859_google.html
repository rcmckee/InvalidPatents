<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html><head><title>Patent US7084859 - Programmable tactile touch screen displays and man-machine interfaces for ... - Google Patents</title><script>(function(){(function(){function e(a){this.t={};this.tick=function(a,c,b){var d=void 0!=b?b:(new Date).getTime();this.t[a]=[d,c];if(void 0==b)try{window.console.timeStamp("CSI/"+a)}catch(e){}};this.tick("start",null,a)}var a;window.performance&&(a=window.performance.timing);var f=a?new e(a.responseStart):new e;window.jstiming={Timer:e,load:f};if(a){var c=a.navigationStart,d=a.responseStart;0<c&&d>=c&&(window.jstiming.srt=d-c)}if(a){var b=window.jstiming.load;0<c&&d>=c&&(b.tick("_wtsrt",void 0,c),b.tick("wtsrt_",
"_wtsrt",d),b.tick("tbsd_","wtsrt_"))}try{a=null,window.chrome&&window.chrome.csi&&(a=Math.floor(window.chrome.csi().pageT),b&&0<c&&(b.tick("_tbnd",void 0,window.chrome.csi().startE),b.tick("tbnd_","_tbnd",c))),null==a&&window.gtbExternal&&(a=window.gtbExternal.pageT()),null==a&&window.external&&(a=window.external.pageT,b&&0<c&&(b.tick("_tbnd",void 0,window.external.startE),b.tick("tbnd_","_tbnd",c))),a&&(window.jstiming.pt=a)}catch(g){}})();})();
</script><link rel="stylesheet" href="/patents/css/_8a2b04e7bf975d5171d8e4c0b6365c7a/kl_intl_patents_bundle.css" type="text/css" /><script src="/books/javascript/atb_8a2b04e7bf975d5171d8e4c0b6365c7a__en.js"></script><script>function googleTranslateElementInit() {new google.translate.TranslateElement({pageLanguage: "en",gaTrack: true,gaId: "UA-27188110-1",multilanguagePage: true});}</script><script src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script><meta name="DC.type" content="Patent"><meta name="DC.title" content="Programmable tactile touch screen displays and man-machine interfaces for improved vehicle instrumentation and telematics"><meta name="DC.contributor" content="Timothy R. Pryor" scheme="inventor"><meta name="DC.contributor" content="Pryor Timothy R" scheme="assignee"><meta name="DC.date" content="2001-2-22" scheme="dateSubmitted"><meta name="DC.description" content="Disclosed are new methods and apparatus particularly suited for applications in a vehicle, to provide a wide range of information, and the safe input of data to a computer controlling the vehicle subsystems or “Telematic” communication using for example GM&#39;s “ONSTAR” or cellular based data sources. Preferred embodiments utilize new programmable forms of tactile touch screens and displays employing tactile physical selection or adjustment means which utilize direct optical data input. A revolutionary form of dashboard or instrument panel results which is stylistically attractive, lower in cost, customizable by the user, programmable in both the tactile and visual sense, and with the potential of enhancing interior safety and vehicle operation. Non-automotive applications of the invention are also disclosed, for example means for general computer input using touch screens and home automation systems."><meta name="DC.date" content="2006-8-1" scheme="issued"><meta name="DC.relation" content="US:20020130839:A1" scheme="references"><meta name="DC.relation" content="US:3825730" scheme="references"><meta name="DC.relation" content="US:4561017" scheme="references"><meta name="DC.relation" content="US:4631525" scheme="references"><meta name="DC.relation" content="US:4787040" scheme="references"><meta name="DC.relation" content="US:5418760" scheme="references"><meta name="DC.relation" content="US:5483261" scheme="references"><meta name="DC.relation" content="US:5530456" scheme="references"><meta name="DC.relation" content="US:5572239" scheme="references"><meta name="DC.relation" content="US:5709219" scheme="references"><meta name="DC.relation" content="US:5712661" scheme="references"><meta name="DC.relation" content="US:5726685" scheme="references"><meta name="DC.relation" content="US:5805145" scheme="references"><meta name="DC.relation" content="US:5805146" scheme="references"><meta name="DC.relation" content="US:5841428" scheme="references"><meta name="DC.relation" content="US:5859631" scheme="references"><meta name="DC.relation" content="US:5867149" scheme="references"><meta name="DC.relation" content="US:5871251" scheme="references"><meta name="DC.relation" content="US:5923319" scheme="references"><meta name="DC.relation" content="US:5936613" scheme="references"><meta name="DC.relation" content="US:5949345" scheme="references"><meta name="DC.relation" content="US:5982355" scheme="references"><meta name="DC.relation" content="US:5995104" scheme="references"><meta name="DC.relation" content="US:6057540" scheme="references"><meta name="DC.relation" content="US:6066075" scheme="references"><meta name="DC.relation" content="US:6219035" scheme="references"><meta name="DC.relation" content="US:6256020" scheme="references"><meta name="DC.relation" content="US:6278441" scheme="references"><meta name="DC.relation" content="US:6414672" scheme="references"><meta name="DC.relation" content="US:6421042" scheme="references"><meta name="DC.relation" content="US:6421046" scheme="references"><meta name="DC.relation" content="US:6441806" scheme="references"><meta name="DC.relation" content="US:6563492" scheme="references"><meta name="citation_patent_number" content="US:7084859"><meta name="citation_patent_application_number" content="US:09/789,538"><link rel="canonical" href="http://www.google.com/patents/US7084859"/><meta property="og:url" content="http://www.google.com/patents/US7084859"/><meta name="title" content="Patent US7084859 - Programmable tactile touch screen displays and man-machine interfaces for improved vehicle instrumentation and telematics"/><meta name="description" content="Disclosed are new methods and apparatus particularly suited for applications in a vehicle, to provide a wide range of information, and the safe input of data to a computer controlling the vehicle subsystems or “Telematic” communication using for example GM&#39;s “ONSTAR” or cellular based data sources. Preferred embodiments utilize new programmable forms of tactile touch screens and displays employing tactile physical selection or adjustment means which utilize direct optical data input. A revolutionary form of dashboard or instrument panel results which is stylistically attractive, lower in cost, customizable by the user, programmable in both the tactile and visual sense, and with the potential of enhancing interior safety and vehicle operation. Non-automotive applications of the invention are also disclosed, for example means for general computer input using touch screens and home automation systems."/><meta property="og:title" content="Patent US7084859 - Programmable tactile touch screen displays and man-machine interfaces for improved vehicle instrumentation and telematics"/><meta property="og:type" content="book"/><meta property="og:site_name" content="Google Books"/><meta property="og:image" content="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><link rel="image_src" href="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"/><script>(function(){try{var aa=function(a,b,c,d){d=d||{};d._sn=["cfg",b,c].join(".");window.gbar.logger.ml(a,d)};var g=window.gbar=window.gbar||{},l=window.gbar.i=window.gbar.i||{},m={},n;function _tvn(a,b){var c=parseInt(a,10);return isNaN(c)?b:c}function _tvf(a,b){var c=parseFloat(a);return isNaN(c)?b:c}function _tvv(a){return!!a}function p(a,b,c){(c||g)[a]=b}g.bv={n:_tvn("2",0),r:"",f:".67.",e:"0",m:_tvn("0",1)};
function q(a,b,c){var d="on"+b;if(a.addEventListener)a.addEventListener(b,c,!1);else if(a.attachEvent)a.attachEvent(d,c);else{var f=a[d];a[d]=function(){var a=f.apply(this,arguments),b=c.apply(this,arguments);return void 0==a?b:void 0==b?a:b&&a}}}var s=function(a){return function(){return g.bv.m==a}},ba=s(1),ca=s(2);p("sb",ba);p("kn",ca);l.a=_tvv;l.b=_tvf;l.c=_tvn;l.i=aa;var da=window.gbar.i.i;var t,u,v,w;function ea(a){v=a}function fa(a){var b;if(b=v&&window.encodeURIComponent)b=a.href,b=!b.match(/^http[s]?:\/\/accounts\.google\.[^/]*\/ClearSID/i)&&!b.match(/^http[s]?:\/\/[^/]*\/accounts\/ClearSID/i);if(b=b&&encodeURIComponent(v()))a.href=a.href.replace(/([?&]continue=)[^&]*/,"$1"+b)}function ga(a){window.gApplication&&(a.href=window.gApplication.getTabUrl(a.href))}
function ha(a){var b=document.forms[0].q,c=window.encodeURIComponent&&b&&b.value,b=b&&b.placeholder;c&&c!=b&&(a.href=a.href.replace(/([?&])q=[^&]*|$/,function(a,b){return(b||"&")+"q="+encodeURIComponent(c)}))}n=l.a("")?ga:ha;
function x(a,b,c,d,f,e){var h=document.getElementById(a);if(h){var k=h.style;k.left=d?"auto":b+"px";k.right=d?b+"px":"auto";k.top=c+"px";k.visibility=u?"hidden":"visible";f&&e?(k.width=f+"px",k.height=e+"px"):(x(t,b,c,d,h.offsetWidth,h.offsetHeight),u=u?"":a)}}
var y=[],ia=function(a,b){y.push(b)},ja=function(a){a=a||window.event;var b=a.target||a.srcElement;a.cancelBubble=!0;null==t&&(a=document.createElement(Array.every||window.createPopup?"iframe":"div"),a.frameBorder="0",t=a.id="gbs",a.src="javascript:''",b.parentNode.appendChild(a),q(document,"click",z));var c=b,b=0;"gb3"!=c.className&&(c=c.parentNode);a=c.getAttribute("aria-owns")||"gbi";var d=c.offsetWidth,f=20<c.offsetTop?46:24;document.getElementById("tphdr")&&(f-=3);var e=!1;do b+=c.offsetLeft||
0;while(c=c.offsetParent);var c=(document.documentElement.clientWidth||document.body.clientWidth)-b-d,h,d=document.body,k=document.defaultView;k&&k.getComputedStyle?(d=k.getComputedStyle(d,""))&&(h=d.direction):h=d.currentStyle?d.currentStyle.direction:d.style.direction;h="rtl"==h;if("gbi"==a){for(d=0;k=y[d++];)k();A(null,window.navExtra);h&&(b=c,e=!0)}else h||(b=c,e=!0);u!=a&&z();x(a,b,f,e)},z=function(){u&&x(u,0,0)},A=function(a,b){var c,d=document.getElementById("gbi"),f=a;f||(f=d.firstChild);
for(;b&&(c=b.pop());){var e=d,h=c,k=f;w||(w="gb2");e.insertBefore(h,k).className=w}},ka=function(a,b,c){if((b=document.getElementById(b))&&a){a.className="gb4";var d=document.createElement("span");d.appendChild(a);d.appendChild(document.createTextNode(" | "));d.id=c;b.appendChild(d)}},la=function(){return document.getElementById("gb_70")},ma=function(){return!!u};p("qs",n);p("setContinueCb",ea);p("pc",fa);p("tg",ja);p("close",z);p("addLink",ka);p("almm",A);p("si",la);p("adh",ia);p("op",ma);var B=function(){},C=function(){},F=function(a){var b=new Image,c=D;b.onerror=b.onload=b.onabort=function(){try{delete E[c]}catch(a){}};E[c]=b;b.src=a;D=c+1},E=[],D=0;p("logger",{il:C,ml:B,log:F});var G=window.gbar.logger;var H={},na={},I=[],oa=l.b("0.1",.1),pa=l.a("1",!0),qa=function(a,b){I.push([a,b])},ra=function(a,b){H[a]=b},sa=function(a){return a in H},J={},K=function(a,b){J[a]||(J[a]=[]);J[a].push(b)},ta=function(a){K("m",a)},L=function(a,b){var c=document.createElement("script");c.src=a;c.async=pa;Math.random()<oa&&(c.onerror=function(){c.onerror=null;B(Error("Bundle load failed: name="+(b||"UNK")+" url="+a))});(document.getElementById("xjsc")||document.getElementsByTagName("body")[0]||
document.getElementsByTagName("head")[0]).appendChild(c)},N=function(a){for(var b=0,c;(c=I[b])&&c[0]!=a;++b);!c||c[1].l||c[1].s||(c[1].s=!0,M(2,a),c[1].url&&L(c[1].url,a),c[1].libs&&m.d&&m.d(c[1].libs))},O=function(a){K("gc",a)},P=null,ua=function(a){P=a},M=function(a,b,c){if(P){a={t:a,b:b};if(c)for(var d in c)a[d]=c[d];try{P(a)}catch(f){}}};p("mdc",H);p("mdi",na);p("bnc",I);p("qGC",O);p("qm",ta);p("qd",J);p("lb",N);p("mcf",ra);p("bcf",qa);p("aq",K);p("mdd","");p("has",sa);
p("trh",ua);p("tev",M);var Q=l.b("0.1",.001),R=0;
function _mlToken(a,b){try{if(1>R){R++;var c,d=a,f=b||{},e=encodeURIComponent,h=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&jexpid=",e("17483"),"&srcpg=",e("prop=22"),"&jsr=",Math.round(1/Q),"&ogev=",e("NJHtU8mPKtCksQTZ2YD4Bg"),"&ogf=",g.bv.f,"&ogrp=",e("1"),"&ogv=",e("1407723702.0"),"&oggv="+e("es_plusone_gc_20140723.0_p0"),"&ogd=",e("com"),"&ogc=",e("USA"),"&ogl=",e("en")];f._sn&&(f._sn="og."+
f._sn);for(var k in f)h.push("&"),h.push(e(k)),h.push("="),h.push(e(f[k]));h.push("&emsg=");h.push(e(d.name+":"+d.message));var r=h.join("");S(r)&&(r=r.substr(0,2E3));c=r;var Aa=window.gbar.logger._aem(a,c);F(Aa)}}catch(Na){}}var S=function(a){return 2E3<=a.length},va=function(a,b){return b};function T(a){B=a;p("_itl",S,G);p("_aem",va,G);p("ml",B,G);a={};H.er=a}l.a("")?T(function(a){throw a;}):l.a("1")&&Math.random()<Q&&T(_mlToken);I.push(["m",{url:"//ssl.gstatic.com/gb/js/scm_7385cc5883250b43a39405734c1bea59.js"}]);g.mcf("c",{});g.sg={c:""};if(l.a("1")){var wa=l.a("");I.push(["gc",{auto:wa,url:"//ssl.gstatic.com/gb/js/abc/gci_91f30755d6a6b787dcc2a4062e6e9824.js",libs:"googleapis.client:plusone:gapi.iframes"}]);var xa={version:"gci_91f30755d6a6b787dcc2a4062e6e9824.js",index:"",lang:"en"};H.gc=xa;var U=function(a){window.googleapis&&window.iframes?a&&a():(a&&O(a),N("gc"))};p("lGC",U);l.a("1")&&p("lPWF",U)};window.__PVT="";if(l.a("1")&&l.a("1")){var V=function(a){U(function(){K("pw",a);N("pw")})};p("lPW",V);I.push(["pw",{url:"//ssl.gstatic.com/gb/js/abc/pwm_45f73e4df07a0e388b0fa1f3d30e7280.js"}]);var W=[],ya=function(a){W[0]=a},za=function(a,b){var c=b||{};c._sn="pw";B(a,c)},Ba={signed:W,elog:za,base:"https://plusone.google.com/u/0",loadTime:(new Date).getTime()};H.pw=Ba;var X=function(a,b){for(var c=b.split("."),d=function(){var b=arguments;a(function(){for(var a=g,d=0,e=c.length-1;d<e;++d)a=a[c[d]];a[c[d]].apply(a,b)})},f=g,e=0,h=c.length-1;e<h;++e)f=
f[c[e]]=f[c[e]]||{};return f[c[e]]=d};X(V,"pw.clk");X(V,"pw.hvr");p("su",ya,g.pw)};function Ca(){function a(){for(var b;(b=e[h++])&&"m"!=b[0]&&!b[1].auto;);b&&(M(2,b[0]),b[1].url&&L(b[1].url,b[0]),b[1].libs&&m.d&&m.d(b[1].libs));h<e.length&&setTimeout(a,0)}function b(){0<f--?setTimeout(b,0):a()}var c=l.a("1"),d=l.a(""),f=3,e=I,h=0,k=window.gbarOnReady;if(k)try{k()}catch(r){da(r,"ml","or")}d?p("ldb",a):c?q(window,"load",b):b()}p("rdl",Ca);var Da={D:1,H:2,da:3,p:4,W:5,M:6,F:7,g:8,ha:9,U:10,L:11,T:12,S:13,N:14,Q:15,P:16,fa:17,w:18,O:19,ga:20,ea:21,u:22,G:23,ja:24,ka:25,ia:26,A:27,j:28,o:29,k:30,ca:31,Z:32,$:33,J:34,K:35,ba:36,aa:37,Y:38,B:39,R:40,v:41,X:42,V:43,h:48,C:49,I:500},Y=[1,2,3,4,5,6,9,10,11,13,14,28,29,30,34,35,37,38,39,40,41,42,43,48,49,500];var Z=l.b("0.001",1E-4),Ea=l.b("1",1),Fa=!1,Ga=!1;if(l.a("1")){var Ha=Math.random();Ha<=Z&&(Fa=!0);Ha<=Ea&&(Ga=!0)}var Ia=Da,$=null;function Ja(){var a=0,b=function(b,d){l.a(d)&&(a|=b)};b(1,"");b(2,"");b(4,"");b(8,"");return a}
function Ka(a,b){var c=Z,d=Fa,f;f=a;if(!$){$={};for(var e=0;e<Y.length;e++){var h=Y[e];$[h]=!0}}if(f=!!$[f])c=Ea,d=Ga;if(d){d=encodeURIComponent;g.rp?(f=g.rp(),f="-1"!=f?f:"1"):f="1";c=["//www.google.com/gen_204?atyp=i&zx=",(new Date).getTime(),"&oge=",a,"&ogex=",d("17483"),"&ogev=",d("NJHtU8mPKtCksQTZ2YD4Bg"),"&ogf=",g.bv.f,"&ogp=",d("22"),"&ogrp=",d(f),"&ogsr=",Math.round(1/c),"&ogv=",d("1407723702.0"),"&oggv="+
d("es_plusone_gc_20140723.0_p0"),"&ogd=",d("com"),"&ogl=",d("en"),"&ogc=",d("USA"),"&ogus=",Ja()];if(b){"ogw"in b&&(c.push("&ogw="+b.ogw),delete b.ogw);var k;f=b;e=[];for(k in f)0!=e.length&&e.push(","),e.push(La(k)),e.push("."),e.push(La(f[k]));k=e.join("");""!=k&&(c.push("&ogad="),c.push(d(k)))}F(c.join(""))}}function La(a){"number"==typeof a&&(a+="");return"string"==typeof a?a.replace(".","%2E").replace(",","%2C"):a}C=Ka;p("il",C,G);var Ma={};H.il=Ma;setTimeout(function(){C(Ia.g)},0);}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var b=window.gbar.i.i;var c=window.gbar;var f=function(d){try{var a=document.getElementById("gbom");a&&d.appendChild(a.cloneNode(!0))}catch(e){b(e,"omas","aomc")}};c.aomc=f;}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{var a=window.gbar;a.mcf("pm",{p:""});}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
(function(){try{window.gbar.rdl();}catch(e){window.gbar&&gbar.logger&&gbar.logger.ml(e,{"_sn":"cfg.init"});}})();
if (window['_OC_timingAction']) {window['_OC_timingAction']('patents_refpage');}</script><style>#gbar,#guser{font-size:13px;padding-top:1px !important;}#gbar{float:left;height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}#gbs,.gbm{background:#fff;left:0;position:absolute;text-align:left;visibility:hidden;z-index:1000}.gbm{border:1px solid;border-color:#c9d7f1 #36c #36c #a2bae7;z-index:1001}.gb1{margin-right:.5em}.gb1,.gb3{zoom:1}.gb2{display:block;padding:.2em .5em}.gb2,.gb3{text-decoration:none !important;border-bottom:none}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb2,a.gb3,a.gb4{color:#00c !important}.gbi .gb3,.gbi .gb2,.gbi .gb4{color:#dd8e27 !important}.gbf .gb3,.gbf .gb2,.gbf .gb4{color:#900 !important}a.gb2:hover{background:#36c;color:#fff !important}#gbar .gbz0l{color:#000 !important;cursor:default;font-weight:bold;text-decoration:none !important}
#gbar { padding:.3em .6em !important;}</style></head><body  topmargin="3" marginheight="3"><div id=gbar><nobr><a onclick=gbar.qs(this);gbar.logger.il(1,{t:1}); class=gb1 id=gb_1 href="https://www.google.com/search?sa=N&tab=tw">Search</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:2}); class=gb1 id=gb_2 href="http://www.google.com/search?hl=en&tbm=isch&source=og&sa=N&tab=ti">Images</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:8}); class=gb1 id=gb_8 href="http://maps.google.com/maps?hl=en&sa=N&tab=tl">Maps</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:78}); class=gb1 id=gb_78 href="https://play.google.com/?hl=en&sa=N&tab=t8">Play</a> <a onclick=gbar.qs(this);gbar.logger.il(1,{t:36}); class=gb1 id=gb_36 href="http://www.youtube.com/results?sa=N&tab=t1">YouTube</a> <a onclick=gbar.logger.il(1,{t:5}); class=gb1 id=gb_5 href="http://news.google.com/nwshp?hl=en&tab=tn">News</a> <a onclick=gbar.logger.il(1,{t:23}); class=gb1 id=gb_23 href="https://mail.google.com/mail/?tab=tm">Gmail</a> <a onclick=gbar.logger.il(1,{t:25}); class=gb1 id=gb_25 href="https://drive.google.com/?tab=to">Drive</a> <a class=gb3 href="http://www.google.com/intl/en/options/" onclick="this.blur();gbar.tg(event);return !1" aria-haspopup=true><u>More</u> <small>&#9660;</small></a><div class=gbm id=gbi><a onclick=gbar.logger.il(1,{t:24}); class=gb2 id=gb_24 href="https://www.google.com/calendar?tab=tc">Calendar</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:51}); class=gb2 id=gb_51 href="http://translate.google.com/?hl=en&sa=N&tab=tT">Translate</a><a onclick=gbar.logger.il(1,{t:17}); class=gb2 id=gb_17 href="http://www.google.com/mobile/?hl=en&tab=tD">Mobile</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:10}); class=gb2 id=gb_10 href="http://www.google.com/search?hl=en&tbo=u&tbm=bks&source=og&sa=N&tab=tp">Books</a><a onclick=gbar.logger.il(1,{t:212}); class=gb2 id=gb_212 href="https://wallet.google.com/manage/?tab=ta">Wallet</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:6}); class=gb2 id=gb_6 href="http://www.google.com/search?hl=en&tbo=u&tbm=shop&source=og&sa=N&tab=tf">Shopping</a><a onclick=gbar.logger.il(1,{t:30}); class=gb2 id=gb_30 href="http://www.blogger.com/?tab=tj">Blogger</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:27}); class=gb2 id=gb_27 href="http://www.google.com/finance?sa=N&tab=te">Finance</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:31}); class=gb2 id=gb_31 href="https://plus.google.com/photos?sa=N&tab=tq">Photos</a><a onclick=gbar.qs(this);gbar.logger.il(1,{t:12}); class=gb2 id=gb_12 href="http://www.google.com/search?hl=en&tbo=u&tbm=vid&source=og&sa=N&tab=tv">Videos</a><div class=gb2><div class=gbd></div></div><a onclick=gbar.logger.il(1,{t:66}); href="http://www.google.com/intl/en/options/" class=gb2>Even more &raquo;</a></div></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=_top id=gb_70 href="https://www.google.com/accounts/Login?service=&continue=http://www.google.com/patents%3Fhl%3Den&hl=en" class=gb4>Sign in</a><div style="display: none"><div class=gbm id=gbd5 aria-owner=gbg5><div class=gbmc><ol id=gbom class=gbmcc></ol></div></div></div></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div role="alert" style="position: absolute; left: 0; right: 0;"><a href="http://www.google.com/patents/us7084859?hl=en&amp;output=html_text" title="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."><img border="0" src="http://www.google.com/images/cleardot.gif"alt="Screen reader users: click this link for accessible mode. Accessible mode has the same essential features but works better with your reader."></a></div><div id="guser"><nobr></nobr></div><div style="clear:both;"></div><div id="gb-top-search-box" class="gb-top-search-box-small gb-reset"><table><tr><td class="logo"><a href="http://www.google.com/patents" class="logo-link"><img class="logo-img" src="/intl/en/images/logos/google_logo_41.png" alt="Go to Google Books Home" height="41"/></a></td><td><form action="http://www.google.com/search" name="f" id="vheadf" method="get"><span id="hf"></span><input type="hidden" name="tbm" value="pts"/><input type="hidden" name="tbo" value="1"/><input type="hidden" name="hl" value="en"/><table><tr><td><div class="inputs"><table><tr><td><div class="text-input"><input type="text" name="q" id="vheadq" class="text" maxlength="2048" size="31" value="" title="Search Patents" accesskey="s" autocomplete="off"/><script>window._OC_autoDir &&window._OC_autoDir('vheadq', 'tia-vheadq');</script></div></td><td><div class="submit-input"><input name="btnG" class="submit" type="submit" value=""/></div></td></tr></table></div></td><td class="col-ext-links"><div class="ext-links"><a href="http://www.google.com/advanced_patent_search">&lt;nobr&gt;Advanced Patent Search&lt;/nobr&gt;</a></div></td></tr></table></form></td></tr></table></div><div class="kd-appbar"><h2 class="kd-appname"><a href="/patents">Patents</a></h2><div class="kd-buttonbar left" id="left-toolbar-buttons"><a id="appbar-write-review-link" href=""></a><a id="appbar-view-print-sample-link" href=""></a><a id="appbar-view-ebook-sample-link" href=""></a><a id="appbar-patents-prior-art-finder-link" href="https://www.google.com/patents/related/US7084859"></a><a id="appbar-patents-discuss-this-link" href="http://www.google.com/url?id=OHd1BAABERAJ&amp;q=http://patents.stackexchange.com/redirect/google-patents%3Fpatent%3DUS7084859&amp;usg=AFQjCNHu_VIbcKFeDIKpX4V_G1AZ8UUq2g" data-is-grant="true"></a><a id="appbar-read-patent-link" href="//docs.google.com/viewer?url=patentimages.storage.googleapis.com/pdfs/US7084859.pdf"></a><a id="appbar-download-pdf-link" href="//patentimages.storage.googleapis.com/pdfs/US7084859.pdf"></a></div><div class="kd-buttonbar right" id="right-toolbar-buttons"></div></div><div id="books-microdata" itemscope=""itemtype="http://schema.org/Book"itemid="http://www.google.com/patents/US7084859" style="display:none"><span itemprop="description">Disclosed are new methods and apparatus particularly suited for applications in a vehicle, to provide a wide range of information, and the safe input of data to a computer controlling the vehicle subsystems or “Telematic” communication using for example GM&#39;s “ONSTAR” or cellular based data sources....</span><span itemprop="url">http://www.google.com/patents/US7084859?utm_source=gb-gplus-share</span><span class="main-title" itemprop="name">Patent US7084859 - Programmable tactile touch screen displays and man-machine interfaces for improved vehicle instrumentation and telematics</span><img itemprop="image" src="http://www.google.com/patents?id=&amp;printsec=frontcover&amp;img=1&amp;zoom=1"alt="Patent US7084859 - Programmable tactile touch screen displays and man-machine interfaces for improved vehicle instrumentation and telematics" title="Patent US7084859 - Programmable tactile touch screen displays and man-machine interfaces for improved vehicle instrumentation and telematics"></div><div style="display: none"><ol id="ofe-gear-menu-contents" class="gbmcc"><li class="gbe gbmtc"><a class="gbmt goog-menuitem-content" id="" href="http://www.google.com/advanced_patent_search">Advanced Patent Search</a></li></ol></div><table id="viewport_table" cellpadding="0" style="clear:both" cellspacing="0"><tr><td id="viewport_td"><div class=vertical_module_list_row><div id=intl_patents class=about_content><div id=intl_patents_v><table class="patent-bibdata"><tr><td class="patent-bibdata-heading">Publication number</td><td class="single-patent-bibdata">US7084859 B1</td></tr><tr><td class="patent-bibdata-heading">Publication type</td><td class="single-patent-bibdata">Grant</td></tr><tr><td class="patent-bibdata-heading">Application number</td><td class="single-patent-bibdata">US 09/789,538</td></tr><tr><td class="patent-bibdata-heading">Publication date</td><td class="single-patent-bibdata">Aug 1, 2006</td></tr><tr><td class="patent-bibdata-heading">Filing date</td><td class="single-patent-bibdata">Feb 22, 2001</td></tr><tr><td class="patent-bibdata-heading">Priority date<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed."></span></td><td class="single-patent-bibdata">Sep 18, 1992</td></tr><tr><td class="patent-bibdata-heading">Fee status<span class="patent-tooltip-anchor patent-question-icon"data-tooltip-text="The fee status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status or dates listed."></span></td><td class="single-patent-bibdata">Paid</td></tr><tr class="patent-bibdata-list-row alternate-patent-number"><td class="patent-bibdata-heading">Publication number</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value">09789538, </span><span class="patent-bibdata-value">789538, </span><span class="patent-bibdata-value">US 7084859 B1, </span><span class="patent-bibdata-value">US 7084859B1, </span><span class="patent-bibdata-value">US-B1-7084859, </span><span class="patent-bibdata-value">US7084859 B1, </span><span class="patent-bibdata-value">US7084859B1</span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Inventors</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=ininventor:%22Timothy+R.+Pryor%22">Timothy R. Pryor</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Original Assignee</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="http://www.google.com/search?tbo=p&tbm=pts&hl=en&q=inassignee:%22Pryor+Timothy+R%22">Pryor Timothy R</a></span></span></td></tr><tr class="patent-bibdata-list-row "><td class="patent-bibdata-heading">Export Citation</td><td><span class="patent-bibdata-value-list"><span class="patent-bibdata-value"><a href="/patents/US7084859.bibtex">BiBTeX</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7084859.enw">EndNote</a>, </span><span class="patent-bibdata-value"><a href="/patents/US7084859.ris">RefMan</a></span></span></td></tr><tr class="patent-internal-links"><td colspan=2><span class="patent-bibdata-value"><a href="#backward-citations">Patent Citations</a> (33),</span> <span class="patent-bibdata-value"><a href="#forward-citations">Referenced by</a> (187),</span> <span class="patent-bibdata-value"><a href="#classifications">Classifications</a> (9),</span> <span class="patent-bibdata-value"><a href="#legal-events">Legal Events</a> (7)</span> </td></tr><tr><td colspan=2 class="patent-bibdata-external-link-spacer-top"></td></tr><tr class="patent-bibdata-external-link-spacer-bottom"></tr><tr><td colspan=2><span class="patent-bibdata-heading">External Links:&nbsp;</span><span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=OHd1BAABERAJ&q=http://patft.uspto.gov/netacgi/nph-Parser%3FSect2%3DPTO1%26Sect2%3DHITOFF%26p%3D1%26u%3D/netahtml/PTO/search-bool.html%26r%3D1%26f%3DG%26l%3D50%26d%3DPALL%26RefSrch%3Dyes%26Query%3DPN/7084859&usg=AFQjCNE737zz-qeLxnaKHGo79aRbFJTBiw">USPTO</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=OHd1BAABERAJ&q=http://assignments.uspto.gov/assignments/q%3Fdb%3Dpat%26pat%3D7084859&usg=AFQjCNFDdbtkQLRD-7zf2oLza2s3twbQ2w">USPTO Assignment</a>, </span><span class="patent-bibdata-value"><a href="http://www.google.com/url?id=OHd1BAABERAJ&q=http://worldwide.espacenet.com/publicationDetails/biblio%3FCC%3DUS%26NR%3D7084859B1%26KC%3DB1%26FT%3DD&usg=AFQjCNFCXn0UkWDHiZYQmZdXtSjlkkKElQ">Espacenet</a></span></span></td></tr><tr class="patent-bibdata-group-spacer"></tr></table><div class="number-and-title"><span class="patent-title"><invention-title mxw-id="PT55613259" lang="EN" load-source="patent-office">Programmable tactile touch screen displays and man-machine interfaces for improved vehicle instrumentation and telematics</invention-title></span><br><span class="patent-number">US 7084859 B1</span></div><div class="patent-section patent-abstract-section"><div class="patent-section-header"><span class="patent-section-title">Abstract</span></div><div class="patent-text"><abstract mxw-id="PA51016212" lang="EN" load-source="patent-office"> <div num="p-0001" class="abstract">Disclosed are new methods and apparatus particularly suited for applications in a vehicle, to provide a wide range of information, and the safe input of data to a computer controlling the vehicle subsystems or “Telematic” communication using for example GM's “ONSTAR” or cellular based data sources. Preferred embodiments utilize new programmable forms of tactile touch screens and displays employing tactile physical selection or adjustment means which utilize direct optical data input. A revolutionary form of dashboard or instrument panel results which is stylistically attractive, lower in cost, customizable by the user, programmable in both the tactile and visual sense, and with the potential of enhancing interior safety and vehicle operation. Non-automotive applications of the invention are also disclosed, for example means for general computer input using touch screens and home automation systems.</div>
  </abstract></div></div><div class="patent-section patent-drawings-section"><div class="patent-section-header"><span class="patent-section-title">Images<span class="patent-section-count">(33)</span></span></div><div class="patent-drawings-body"><div class="patent-drawings-carousel"><div class="drawings"><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00000.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00000.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00001.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00001.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00002.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00002.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00003.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00003.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00004.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00004.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00005.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00005.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00006.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00006.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00007.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00007.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00008.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00008.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00009.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00009.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00010.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00010.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00011.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00011.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00012.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00012.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00013.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00013.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00014.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00014.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00015.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00015.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00016.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00016.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00017.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00017.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00018.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00018.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00019.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00019.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00020.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00020.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00021.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00021.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00022.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00022.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00023.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00023.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00024.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00024.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00025.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00025.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00026.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00026.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00027.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00027.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00028.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00028.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00029.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00029.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00030.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00030.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00031.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00031.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div><div class="patent-image"><div class="patent-thumbnail"><a href="//patentimages.storage.googleapis.com/US7084859B1/US07084859-20060801-D00032.png"><img class="patent-thumbnail-image" alt="Patent Drawing"src="//patentimages.storage.googleapis.com/thumbnails/US7084859B1/US07084859-20060801-D00032.png" /></a></div><div class="patent-thumbnail-caption">&nbsp;</div></div></div></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img" alt="Previous page"src="/googlebooks/images/kennedy/page_left.png"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img" alt="Next page"src="/googlebooks/images/kennedy/page_right.png"width="21" height="21" /></div></div></div><div class="patent-post-drawings"></div><div class="patent-section patent-claims-section"><div class="patent-section-header"><span class="patent-section-title">Claims<span class="patent-section-count">(28)</span></span></div><div class="patent-text"><div mxw-id="PCLM9058005" lang="EN" load-source="patent-office" class="claims">
  <div class="claim"> <div id="CLM-00001" num="00001" class="claim">
    <div class="claim-text">1. Method of improving designation of displayed data using touch screen inputs, comprising the steps of
<div class="claim-text">providing a display of visually observable data on a touch screen,</div>
<div class="claim-text">touching said screen proximate to particular displayed data that it is desired to designate,</div>
<div class="claim-text">generating a displayed designating action using said computer, and</div>
<div class="claim-text">using said designating action and the result of further touch input, designating the data desired, wherein said further touch input utilizes two fingers moved relative to, or about, each other.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00002" num="00002" class="claim">
    <div class="claim-text">2. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> in which said designating action comprises a cursor.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00003" num="00003" class="claim">
    <div class="claim-text">3. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> in which said designating action comprises a bar, or knob.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00004" num="00004" class="claim">
    <div class="claim-text">4. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> in which said designating action results in de-magnified movement of said designated action.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00005" num="00005" class="claim">
    <div class="claim-text">5. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> in which said further touch input utilizes two fingers in contact with the touch screen.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00006" num="00006" class="claim">
    <div class="claim-text">6. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> in which said designating action comprises a bar.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00007" num="00007" class="claim">
    <div class="claim-text">7. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> in which said further touch is a sliding motion toward said data.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00008" num="00008" class="claim">
    <div class="claim-text">8. A method according to <claim-ref idref="CLM-00001">claim 1</claim-ref> in which said further touch input utilizes a finger to urge a designation action in a direction on the screen without substantially moving the location of said touch.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00009" num="00009" class="claim">
    <div class="claim-text">9. Method of improving designation of displayed data using touch screen inputs, comprising the steps of
<div class="claim-text">providing a display of visually observable data on a touch screen,</div>
<div class="claim-text">touching said screen proximate to particular displayed data that is desired to designate,</div>
<div class="claim-text">generating a displayed designating action using said computer, and</div>
<div class="claim-text">using said designating action and the result of further touch input, designating the data desired, in which said further touch input utilizes two fingers, simultaneously to bracket data desired.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00010" num="00010" class="claim">
    <div class="claim-text">10. Method of improving designation of displayed data using touch screen inputs, comprising the steps of
<div class="claim-text">providing a display of visually observable data on a touch screen,</div>
<div class="claim-text">touching said screen proximate to particular displayed data that it is desired to designate,</div>
<div class="claim-text">generating a displayed designating action using said computer, and</div>
<div class="claim-text">using said designating action and the result of further touch input, designating the data desired, in which said further touch input utilizes a finger to urge a designation, without moving the point of touch.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00011" num="00011" class="claim">
    <div class="claim-text">11. Method of improving designation of displayed data using touch screen inputs, comprising the steps of
<div class="claim-text">providing a display of visually observable data on a touch screen,</div>
<div class="claim-text">touching said screen proximate to particular displayed data that it is desired to designate,</div>
<div class="claim-text">generating a displayed designating action using said computer, and</div>
<div class="claim-text">using said designating action and the result of further touch input, designating the data desired, in which said further touch is a pinching motion closing in on said data.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00012" num="00012" class="claim">
    <div class="claim-text">12. A method according to <claim-ref idref="CLM-00011">claim 11</claim-ref> in which said pinching motion causes the program to cut said data from a document.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00013" num="00013" class="claim">
    <div class="claim-text">13. A method according to <claim-ref idref="CLM-00011">claim 11</claim-ref> in which withdrawal of said pinching motion causes the program to paste said data into a document.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00014" num="00014" class="claim">
    <div class="claim-text">14. Method for controlling a computer system, comprising the steps of
<div class="claim-text">providing a touch screen which is capable of displaying visually observable data and is effectively responsive to touch at a plurality of positions at once,</div>
<div class="claim-text">effectively touching said surface with at least two fingers, and</div>
<div class="claim-text">using the action of said fingers, causing a desired control action of said computer system, two fingers of said at least two fingers simultaneously being used to pinch or bracket displayed data on the screen.</div>
</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00015" num="00015" class="claim">
    <div class="claim-text">15. A method according to <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein one of said fingers is a thumb.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00016" num="00016" class="claim">
    <div class="claim-text">16. A method according to <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein said touch screen is controlled by said computer system.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00017" num="00017" class="claim">
    <div class="claim-text">17. A method according to <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the finger touch is in relation to an initial display condition.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00018" num="00018" class="claim">
    <div class="claim-text">18. A method according to <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein a subsequent display condition is in relation to an initial finger touch.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00019" num="00019" class="claim">
    <div class="claim-text">19. A method according to <claim-ref idref="CLM-00014">claim 14</claim-ref> further including the step of generating a signal to indicate to the touch that a action has been taken.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00020" num="00020" class="claim">
    <div class="claim-text">20. A method according to <claim-ref idref="CLM-00014">claim 14</claim-ref> further including the step of generating a signal to indicate the value of an action entered by said touching.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00021" num="00021" class="claim">
    <div class="claim-text">21. A method according to <claim-ref idref="CLM-00020">claim 20</claim-ref> wherein said signal is acoustic.</div>
  </div>
  </div> <div class="claim-dependent"> <div id="CLM-00022" num="00022" class="claim">
    <div class="claim-text">22. A method according to <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein said touch screen operates electro-optically.</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00023" num="00023" class="claim">
    <div class="claim-text">23. Method for controlling a computer system, comprising the steps of
<div class="claim-text">providing a touch screen which is capable of displaying visually observable data and is effectively responsive to touch at a plurality of positions at once,</div>
<div class="claim-text">effectively touching said surface with at least two fingers, and</div>
<div class="claim-text">using the action of said fingers, causing a desired control action of said computer system, wherein said desired action is to indicate a value desired of a variable by the degree of rotation of one finger around a stationary finger.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00024" num="00024" class="claim">
    <div class="claim-text">24. Method for controlling a computer system, comprising the steps of
<div class="claim-text">providing a touch screen which is capable of displaying visually observable data and is effectively responsive to touch at a plurality of positions at once,</div>
<div class="claim-text">effectively touching said surface with at least two fingers, and</div>
<div class="claim-text">using the action of said fingers, causing a desired control action of said computer system, wherein said desired action is to indicate a value desired of a variable by the degree of rotation of the two fingers in a twisting motion.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00025" num="00025" class="claim">
    <div class="claim-text">25. Method for controlling a computer system, comprising the steps of
<div class="claim-text">providing a touch screen which is capable of displaying visually observable data and is effectively responsive to touch at a plurality of positions at once,</div>
<div class="claim-text">effectively touching said surface with at least two fingers simultaneously, and</div>
<div class="claim-text">using the action of said fingers, causing a desired control action of said computer system, wherein said desired action is to indicate a value desired of a variable by the spacing of one finger with respect to another finger.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00026" num="00026" class="claim">
    <div class="claim-text">26. Method for controlling a computer system, comprising the steps of
<div class="claim-text">providing a touch screen which is capable of displaying visually observable data and is effectively responsive to touch at a plurality of positions at once,</div>
<div class="claim-text">effectively touching said surface with at least two fingers, and</div>
<div class="claim-text">using the action of said fingers, causing a desired control action of said computer system, two fingers of said at least two fingers simultaneously being used to pinch or bracket displayed data on the screen.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00027" num="00027" class="claim">
    <div class="claim-text">27. Method for controlling a computer system, comprising the steps of
<div class="claim-text">providing a touch screen which is capable of displaying visually observable data and is effectively responsive to touch at a plurality of positions at once,</div>
<div class="claim-text">effectively touching said surface with at least two fingers, and</div>
<div class="claim-text">using the action of said fingers, causing a desired control action of said computer system, wherein said action of said fingers is the rotation of one finger about a stationary finger.</div>
</div>
  </div>
  </div> <div class="claim"> <div id="CLM-00028" num="00028" class="claim">
    <div class="claim-text">28. Method for controlling a computer system, comprising the steps of
<div class="claim-text">providing a touch screen which is capable of displaying visually observable data and is effectively responsive to touch at a plurality of positions at once,</div>
<div class="claim-text">effectively touching said surface with at least two fingers simultaneously, and</div>
<div class="claim-text">using the action of said fingers, causing a desired control action of said computer system, wherein said desired action is to indicate a value desired of a variable by the movement of one finger with respect to another finger.</div>
</div>
  </div>
</div> </div></div></div><div class="patent-section patent-description-section"><div class="patent-section-header"><span class="patent-section-title">Description</span></div><div class="patent-text"><div mxw-id="PDES16080549" lang="EN" load-source="patent-office" class="description">
<p num="p-0002">This application claims benefit of U.S. Provisional Applications; SN 60/142,777 Tactile Touch Screens for Automobile Dashboards, Interiors and Other Applications, filed Feb. 22, 2000; and SN 60/183, 807 Man-Machine Interfaces for Vehicle Instrumentation and Telematics, filed Sep. 26, 2000.</p>
  <heading>CROSS REFERENCES TO RELATED APPLICATIONS BY THE INVENTOR</heading> <p num="p-0003">
    </p> <ul> <li id="ul0001-0001" num="0002">1. Useful Man Machine interfaces and applications, Ser. No. 09/138,339</li>
      <li id="ul0001-0002" num="0003">2. Touch TV and other Man Machine Interfaces Ser. No. 09/435,854</li>
      <li id="ul0001-0003" num="0004">3. Systems for Occupant Position Sensing, Ser. No. 08/968,114</li>
      <li id="ul0001-0004" num="0005">4. More Useful Man Machine interfaces and applications Ser. No. 09/433,297</li>
      <li id="ul0001-0005" num="0006">5. Picture Taking method and apparatus Ser. No. 09/568,552</li>
      <li id="ul0001-0006" num="0007">6. Methods and Apparatus for Man Machine Interfaces and Related Activity Ser. No. 09/568,554</li>
      <li id="ul0001-0007" num="0008">7. Camera Based Man-Machine Interfaces Ser. No. 09/612,225</li>
    </ul> <p num="p-0004">This application is a continuation-in-part of application Ser. No. 09/435,854 filed Nov. 8, 1999 which was a continuation of application Ser. No. 07/496,908 filed Sep. 18, 1992, now U.S. Pat. No. 5,982,352, which was a continuation-in-part of application Ser. No. 08/290,516, filed Aug. 15, 1994, now U.S. Pat. No. 6,008,000, which was a continuation of application Ser. No. 07/946,588, filed Sep. 18, 1992, now abandoned.</p>
  <p num="p-0005">This application is also a continuation in part of copending applications Ser. Nos. 09/138,339; 09/433,297; 09/568,554;</p>
  <p num="p-0006">Copies of the disclosures of the following U.S. patents and co-pending patent applications are also incorporated herein by reference:
</p> <ul> <li id="ul0002-0001" num="0000"> <ul> <li id="ul0003-0001" num="0012">1. U.S. Pat. No. 4,629,319 (Panel Surface Flaw inspection, which discloses a novel optical principle commonly called “D Sight”)</li> <li id="ul0003-0002" num="0013">2. U.S. Ser. No. 09/435,854 and U.S. Pat. No. 5,982,352, and U.S. Ser. No. 08/290,516 (“Man Machine Interfaces”), filed Aug. 15, 1994, now U.S. Pat. No. 6,008,000, the disclosure of both of which is contained in that of Ser. No. 09/435,854</li> <li id="ul0003-0003" num="0014">3. U.S. application Ser. No. 09/138,339 Useful man machine interfaces and applications</li> <li id="ul0003-0004" num="0015">4. U.S. application Ser. No. 09/433,297 More useful man machine interfaces and applications</li> <li id="ul0003-0005" num="0016">5. Systems for Occupant Position Sensing, U.S. Ser. No. 08/968,114, itself abandoned, but the disclosure of which is incorporated by reference in items 3 and 4 above</li> <li id="ul0003-0006" num="0017">6. Camera Based Applications of Man-Machine Interfaces U.S. Ser. No. 09/612,225</li> <li id="ul0003-0007" num="0018">7. Picture Taking method and apparatus(U.S. Ser. No. 09/568,552</li> <li id="ul0003-0008" num="0019">8. Methods and Apparatus for Man Machine Interfaces and Related Activity Ser. No. 09/568,554</li> </ul> </li> </ul> <heading>FEDERALLY SPONSORED R AND D STATEMENT</heading> <p num="p-0007">not applicable</p>
  <heading>MICROFICHE APPENDIX</heading> <p num="p-0008">not applicable</p>
  <heading>BACKGROUND OF THE INVENTION</heading> <p num="h-0005">Preface</p>
  <p num="p-0009">This invention seeks to dramatically increase the utility of car informational displays and controls, while at the same time enhancing safety by improving sensory data presentation and ease of interaction with vehicle controls and data sources. The programmable nature of the disclosed devices also creates new methods for how data is delivered and utilized.</p>
  <p num="p-0010">The disclosed invention, including co-pending applications incorporated by reference, contains unique embodiments which allow one to interact, by feel, with a display, called herein a “programmable tactile display”. It encompasses two main focus areas:
</p> <ul> <li id="ul0004-0001" num="0000"> <ul> <li id="ul0005-0001" num="0024">A display having features commonly associated with a touch screen, but in a new form which can be sensed in several tactile manners, as well as visually.</li> <li id="ul0005-0002" num="0025">A tactile selection or adjustment means, such as a knob, slider, or switch, programmable in its tactile or visual nature, and generally operated in conjunction with the touch screen just described.</li> </ul> </li> </ul> <p num="p-0011">These features in turn provide the basis for a automobile instrument panel (dashboard) or other control panel which can be operated without undue concentration on visually reading the display while working the controls. It serves as an alternative, or adjunct, to voice activated systems being considered today to allow increased functionality with safety of vehicle operation. In several embodiments, the force or other sensation felt can itself be programmably changed, adding to driver understanding, and enhancing safety.</p>
  <p num="p-0012">Because it resembles today's dashboards, and can be used for the basic control functions of the vehicle, the invention provides not only a potential means of telematic connectivity while driving (e.g. with the internet, cellular telephonic sources or the like), but a much more useful display and control system capable of many more functions—including the primary vehicle control functions, if desired.</p>
  <p num="p-0013">In addition, I feel that a dashboard incorporating the invention can be built at lower cost than a conventional dashboard, especially as vehicles become ever more loaded up with navigational systems and other electronic functions incidental to the control of the vehicle.</p>
  <p num="p-0014">There is no known prior art having the above characteristics.</p>
  <heading>SUMMARY OF THE CHALLENGE AND OPPORTUNITY</heading> <p num="p-0015">The automotive dashboard is today a confused array of switches, knobs, dials, gages, and other tactile physical selection or adjustment means and instruments. It is often hard to see and to understand, and can cause undo distraction to the driver. As a device, it is filled with different parts and thus expensive to manufacture both in serial quantity, and in redesign and tooling for new models. Furthermore, it is inflexible, and invariant once manufactured. It cannot be changed in its design by the user, and cannot be easily changed by the manufacturer or the dealer.</p>
  <p num="p-0016">In addition, the instrument panel in its current form it is at its limit in so far as its ability to present data which can be acted on. There isn't any more room on the dash to put more devices, and various safety issues such as airbag deployment and passive interior safety preclude many choices. Data which might be desired for action can be from the vehicles own controls and state, its surroundings such as from other vehicles or parked objects, or could include material downloaded from the internet, or navigational data from satellites.</p>
  <p num="p-0017">Because of this, Voice recognition techniques are now being researched in earnest in order to allow the user to interact with computer based functions. IBM VIA VOICE and DRAGON SOFTWARE Naturally Speaking products have already reached the general office market, and are reasonably accurate and effective if the surrounding environment is quiet and stable. But in the car, this is not the case, and even specialized limited capability Telematic systems such as Fonix Corporations Automatic Speech Recognition (ASR), L and H, etc, have lots of problems—for example:
</p> <ul> <li id="ul0006-0001" num="0000"> <ul> <li id="ul0007-0001" num="0033">Noise—the car is not an ideal environment under any circumstance, never mind with CD Player at full volume.</li> <li id="ul0007-0002" num="0034">Different drivers, with different accents, practices</li> <li id="ul0007-0003" num="0035">Limited or no teach in available to perfect the function of the recognition program</li> <li id="ul0007-0004" num="0036">Passenger disturbance, and conversation interruption (i.e. a background noise source which disturbs the recognition, or the actual intonation of the speech</li> </ul> </li> </ul> <p num="p-0018">These problems result in function which is possibly error prone or time delayed. Having to repeat or worse, having the wrong action, is not desirable in mission critical driving situations so to speak. The Cell phone problem is notorious. In the extreme, a form of rage can take over if the system frustrates the driver. For this reason, voice recognition systems used for actual vehicle functions (as opposed to dialing phones and the like) have in effect, backup controls—defeating the concept of increasing dashboard space, and control comprehension.</p>
  <p num="p-0019">In addition there is a sociological problem—talking to ones car in unacceptable to many, even if the voice recognition program works. For some this is always true, for others its true with passengers present.</p>
  <p num="p-0020">In addition, the presentation of data by the computer to the driver by text to speech programs is known to have unacceptable intonation or time latency in many circumstances by the average user, and in any case is woefully inadequate for graphical or tabular information, for example. The data to be presented to the user, even in a simple list of choices, has to be vocally spoken by the computer to the driver in sequence, taking a great deal of concentration to avoid missing an important point, and taking minutes to do what can visually be done in a glance.</p>
  <p num="p-0021">Voice aside, perhaps the only way more data can be presented to users (driver, and/or passengers), who might wish to engage in internet or cellular activity while driving, for example, is through the use of programmable displays, and programmable entry devices, which can be varied to suit the need of the user at the instant of use. Such displays are commonplace today in the computer world (e.g. “Windows”, “Web Browsers”), so why not in a car?</p>
  <p num="p-0022">It is noted that unbridled use of such displays by the driver is however limited by laws in several states as for example pointed out in U.S. Pat. No. 5,949,345 owned by MICROSOFT corporation, which desirably covers an operating system for the computer driving the display to allow legal operation. Also germane are Ser. No. 08/564,586 and Ser. No. 08/668,781 assigned to MICROSOFT.</p>
  <p num="p-0023">One problem is how to interact with the display, without having a keyboard—which is generally too cumbersome, switch filled, and space consuming for a car dash, armrest, or other interior location. And a mouse is pretty much impossible as well.</p>
  <p num="p-0024">Accordingly, some companies have proposed or developed conventional changeable computer window type systems using up/down/left/right buttons or even joysticks to select the screens and menus thereon in an attempt to solve this problem. Such devices are however, not intuitive and hard to use, for at least 95% of the populace I feel. Because of such problems, even where implemented, they are relegated to only non critical functions, such as navigation, climate control, cell phone dialing or audio system entertainment.</p>
  <p num="p-0025">If such computer based systems could be used by the general populace, without the limits of voice recognition, a very large application area comprising in its limit the total function of the vehicle could result. In addition, improved productivity could result as people in the USA spend over 500 million hours it is said in their cars, and much of this time is wasted, from a business point of view. Even to salvage 1% is 50 million hours, which corresponds to over a billion dollars saved.</p>
  <p num="p-0026">I feel a key part of the answer lies in a form of tactile display or touch screen with tactile properties not hereto fore seen. This display could, depending on its construction, occupy some, or even most of the dashboard (also called dash, or instrument panel). Alternatively and or in addition, it could occupy the region in the center of the car called the “center stack”. It should also be noted that the problem may also require a multi-sensor solution, which will be discussed herein.</p>
  <p num="p-0027">Some requirements of an Automobile display/touch screen could arguably be at least the following:
</p> <ul> <li id="ul0008-0001" num="0000"> <ul> <li id="ul0009-0001" num="0047">1. Sturdy—and conducive to interior passive safety, and regulations</li> <li id="ul0009-0002" num="0048">2. Should allow multiple functions to be undertaken simultaneously, either all by feel, by glance and feel, or by combinations of touch/feel. body positions, voice and other variables.</li> <li id="ul0009-0003" num="0049">3. Should provide enhanced measure of safety by requiring less concentration on controls than present with many controls today.</li> <li id="ul0009-0004" num="0050">4. Allow programmable screen operation by feel if possible, for ease of use while driving (or while engaged in other tasks) and safety. The “feel” sensation should be itself programmable, such that varying responses can be given the operator, depending on what screen and what function or what amount is being actuated or interrogated (that is you touch it to get a sensation first indicative of the current value or screen type)</li> <li id="ul0009-0005" num="0051">5. Activate functions easily</li> <li id="ul0009-0006" num="0052">6. Not affected by environmental issues, such as ambient light, contamination</li> <li id="ul0009-0007" num="0053">7. Not sacrifice airbag function—thus must not be where the airbag is, or the airbag/and screen module must be of another design (one of which is disclosed herein)</li> <li id="ul0009-0008" num="0054">8. High resolution, to allow graphics and dense information such as that downloaded from the internet to be displayed, when desired.</li> <li id="ul0009-0009" num="0055">9. It should desirably respond to z inputs (into the display screen itself) to give added dimension to function. (e.g. volume of sound, air, etc,) or combine a switching function with a turning, sliding or other function (similar to the heat knob of a Olds Aurora for example). And response to inputs at angles to the screen normal would also be desirable in some cases.</li> </ul> </li> </ul> <heading>PRIOR ART</heading> <p num="p-0028">Directly to the point, there are no touch screens known to me, other than my own, which have a physical tactile “feel” aspect to them—a feature which I consider essential for the vehicular control application of touch screens. Having said this, a review of conventional touch screen art for automobiles follows.</p>
  <p num="p-0029">Where limited functionality is all that is needed, conventional touch screens of limited size are an answer. To my knowledge, the application of touch screens for computer input in automobiles was first achieved in the 1988, in the General Motors Buick Riviera using a relatively small CRT display located in the center of the dashboard. Such a display is described in U.S. Pat. No. 4,787,040 by Ames, et al. entitled Display system for Automotive Vehicle issued to IBM Corporation Nov. 22, 1988.</p>
  <p num="p-0030">More recently, numerous manufacturers have done the same, for the purpose of navigation, using instant GPS data and stored maps. However, none of these displays is easy to use by the driver, being small, difficult to interact with physically, and out of the general line of sight of the driver. This is a problem as one has to see the screen to touch the box corresponding to the input function desired. And to see the screen, you have to take your eyes off the road for a significant time. There is no physical sensation associated with the function which would allow you to actuate the function “by feel”. Indeed because of this, many of the GPS systems being sold, are just displays, with function selection and the like achieved with the standard assortment of buttons, switches, dials, etc. And where touch boxes on screens are used, they by necessity have limited functions.</p>
  <p num="p-0031">Other problems with computer displays of the multiple windows type exist when these are used in vehicles—this is discussed in U.S. Pat. No. 5,995,104 by Kataoka, also aimed at a Navigation system. The instant invention can use such windows in the touch screens it provides, but preferably uses more physical “objects” representing typical dashboard instrumentation.</p>
  <p num="p-0032">Finally, as another example of display locations in vehicles, a recent U.S. Pat. No. 5,871,251 assigned to Prince Corp) discloses an LCD display located in the sun visor, for use by the driver, having no input capability which would allow the driver to make use of it in normal operation (even if he could see it well).</p>
  <p num="p-0033">Another problem, is that the touch screen gets really attractive, as you envision it as a replacement for a large measure of the automobile user instrumentation and control functions of today, a dramatic move, filled with other advantages in flexibility, ease of use and user benefits—plus potentially lower cost.</p>
  <p num="p-0034">What about then, a touch screen of the LCD type, with a surface acoustic wave (SAW)based touch cover screen? (e.g. Microtouch brand). One could just put one of these across the whole dashboard, at least in sections. SAW types do not have some objectionable properties of cover materials and the like which can get damaged which some other types of conventional screens possess.</p>
  <p num="p-0035">This use of a conventional LCD based (or alternatively plasma display or other type) flat panel touch screen has several problems (and is very expensive today). These are:
</p> <ul> <li id="ul0010-0001" num="0000"> <ul> <li id="ul0011-0001" num="0064">The requirement to see the function touched, with no physical feedback from doing so, as pointed out above</li> <li id="ul0011-0002" num="0065">A potential problem in large size with complying with interior passive safety regulations (should a persons head hit it, say)</li> <li id="ul0011-0003" num="0066">A problem in coexisting physically, and functionally, with dashboard or steering wheel installed airbags (also called SRS or supplemental restraint systems) and their actuation.</li> <li id="ul0011-0004" num="0067">A potential problem with durability, being able to survive hits by kids, feet on the dash, etc., as well as degradation of performance by contaminants (a major problem of conventional technologies recently addressed in U.S. Pat. No. 5,923,319 Bishop, et al.</li> <li id="ul0011-0005" num="0068">Possible thermal drift and resolution problems due to the extreme temperature situations in vehicles</li> </ul> </li> </ul> <p num="p-0036">The safety related issues are a significant stumbling block, but the ease of use without undue concentration issues are as well, and apply to other fields besides—indeed anywhere a touch screen would be desirable but not easy to use by someone who can not take the time to look, or to whom looking is inconvenient or uncomfortable, or in the case of the handicapped, difficult or impossible.</p>
  <p num="p-0037">It is also noted, that the objectionable requirement to touch what one sees(and thus take ones eyes significantly off the road in many cases), has undoubtedly led manufacturers of the most recent set of navigational screens, to use a fixed set of keys as inputs, with the data display being the only thing programmable. In this case the person can feel the switch, and only quickly glance at the screen for data once presented. But the keys and switches used are by necessity small, and cluttered.</p>
  <p num="p-0038">To conclude, conventional touch screens can work in vehicles, but in their form heretofore they have no tactile properties. Thus today they need to be augmented by voice recognition and/or tactile devices placed near them if they are to be used for important functions of the vehicle.</p>
  <heading>SUMMARY OF THE INVENTION</heading> <p num="p-0039">The instant invention disclosed herein seeks to provide a vast increase in flexibility of data presentation and manipulation, while obviating the problems of conventional technology, and doing so at a low price encouraging wide spread automotive application. (indeed it is thought to be less expensive to manufacture, and much less expensive to design and redesign, than a conventional dash with its various Input/Output devices). While the focus is on vehicular application, it is noted that the invention is not limited thereto.</p>
  <p num="p-0040">Safety is enhanced by providing a potentially safer car interior and one more friendly to elderly or handicapped drivers and those visually challenged (primarily due to the larger displays and control devices made practical, while at the same time having more functionality). In addition, it can enhance safety by promoting a degree of dash standardization between vehicles not now practical from a marketing point of view. This is because certain functions could be common, with the decorative and other portions customizable.</p>
  <p num="p-0041">The invention is also differentiated in that it expands the stylistic and after market business opportunities associated with vehicle sales. Just one example disclosed is a single use or mission specific accessories to enable safe driving while executing detailed specialized tasks, for example a project worked on ones way to work, or a trip taken to a specific place.</p>
  <p num="p-0042">There are many embodiments of the invention disclosed herein. One version, has a major advantage for near term commercialization in that it results in a dashboard and/or center stack which is as close as possible to today's cars while possessing a high degree of programmably variable operation needed for enhanced use by a driver.</p>
  <p num="p-0043">It is especially noted that the invention serves to eliminate problems brought about by reliance on voice communication between driver and vehicle, and telematic connections to same. Voice recognition has problems, as it often requires prompts for the driver, which are often time consuming, frustrating, and difficult to hear. Undue concentration on same can also cause unsafe driving (for example in the presence of sirens). In addition, voice is linear, and sequential. Only one message/action can be executed at once. And the whole message has to be understood by the driver, or vehicle or telematic connection—or all can be lost and the process required to be repeated, which can provide frustration and endanger safety in extreme circumstances.</p>
  <p num="p-0044">With the instant invention however, key functions can all be implemented with little or no voice input/output by the driver at all. And more than one function can be done at once, using multiple fingers, hands, head movements and a variety of alternative input mechanisms.</p>
  <p num="p-0045">The invention also contributes to safe operation in other ways. For example the dash can be built with a lack of protruding devices if desired, contributing to passive safety. In addition, the invention is also unique among touch screens, as it allows the touch screen to coexist so to speak, with an airbag. This in-turn may allow novel location of the invention in the steering wheel. This is the easiest to use of any location, as the drivers hands are normally on the wheel.</p>
  <p num="p-0046">Another good location for the driver, also largely unexploited heretofore is in the center console (also called center stack). Even the armrest can be used, desirably assisted by a heads up display on the windshield. Other embodiments of the invention describe display locations of use to passengers, or of use to the driver on an periodic basis only.</p>
  <p num="p-0047">Since the technology disclosed herein and in co-pending applications incorporated by reference is in many ways universal in its application, some applications are herein disclosed of use in the home and business as well. These often do not require “feel”, but benefit from the invention's similarity to familiar tactile physical selection or adjustment means found in the home or workplace—knobs, switches, levers, sliders, and the like.</p>
  <p num="p-0048">The basic aspects of some features and/or embodiments of the invention have been disclosed in provisional patent applications, co-pending applications and patents incorporated herein by reference. These are now briefly summarized.</p>
  <p num="p-0049">Co-pending application Ser. No. 09/435,854 by the inventor (and recently granted U.S. Patents related thereto) describes the use of unique touch devices for automotive dashboard related applications, and in addition, discloses new type of data entry device for computers and other electronic equipment generally in the category of digitizers and touch screens having several unique properties. This device, in the form usable here, is most generally based on the use of one or more TV cameras to view optically discernable data associated with the screen from the rear in order to determine the location in screen coordinates (and z in some cases as well) of one or more touch points. Primarily discussed therein are devices which use the electro-optical determination of temporary surface distortion caused by the physical input signal, or force, creating distortion (e.g. a finger “touch”). This is referred to as “surface distortion imaging” and depends on the ability to detect, and in some cases quantify, small distortions of or stresses in member over a large (by comparison) area. This technology has some unique advantages for automotive control and display applications, the subject of this disclosure, which elaborates on the versions shown previously. It is also highly useful for other applications, such as Kiosks, control panels in the home, and the like. And further disclosed are means to allow tactile physical selection or adjustment means such as knobs, levers and switches to be integrated with a touch screen, an area expanded on further in this application.</p>
  <p num="p-0050">In addition to the unique touch screen just described, other referenced patent applications by the inventor incorporated by reference describe novel means to determine other events which cooperatively or individually may a be imputed to a computer using electro-optically determinable datum's on persons or other entry means, typically objects such as steering wheels, switches, and the like in a vehicle. Included is data input using direct camera observation of the user, or datum's attached to the user, or objects held by the user. With the advent of low cost cameras, the use of such camera systems becomes practical for automotive interior application. And disclosed herein are other touch screen transduction methods which can be used alternatively or in combination with those of previous applications.</p>
  <p num="p-0051">The above features are very useful in answering the needs of potential applications in cars, trucks and other vehicles, as well as for certain other instrumentation and related applications—even in the home. In particular, the invention herein, even beyond previous disclosures will be further shown to have unique advantages in the following areas.
</p> <ul> <li id="ul0012-0001" num="0000"> <ul> <li id="ul0013-0001" num="0085">1. Feed back to the user of touch related activities</li> <li id="ul0013-0002" num="0086">2. Safety enhancement and compatibility with existing safety systems and norms.</li> <li id="ul0013-0003" num="0087">3. Ruggedness and ease of application in difficult environments</li> <li id="ul0013-0004" num="0088">4. Novelty in terms of the benefits for the user</li> <li id="ul0013-0005" num="0089">5. New business methods resulting from the data intensive activities made possible in the car</li> <li id="ul0013-0006" num="0090">6. Novel locations made possible for the screens</li> <li id="ul0013-0007" num="0091">7. Additional novel touch screen sensing approaches including those using TV cameras to view the user or objects.
<br>
Advantages of the Instant Invention
</li> </ul> </li> </ul> <p num="p-0052">The instant invention is aimed primarily, but not necessarily for dash mounted touch screen displays, which can literally fill the dash if desired, or at least provide a large display at any or all of these locations of primary interest:
</p> <ul> <li id="ul0014-0001" num="0000"> <ul> <li id="ul0015-0001" num="0093">In the center of the dash, or in the console (also called center stack) for use by driver and/or passenger</li> <li id="ul0015-0002" num="0094">Replacing the existing instrumentation cluster in front of the driver</li> <li id="ul0015-0003" num="0095">Located in the steering wheel, in addition to or in place of said cluster.</li> <li id="ul0015-0004" num="0096">In front of the passenger
<br>
Safety Advantages Include
</li> <li id="ul0015-0005" num="0097">Reduction in sources of anger or frustration caused by alternative voice based systems</li> <li id="ul0015-0006" num="0098">Large easy to read data—much larger than possible today, and ideally suited for just quick glances and elderly drivers, and drivers in general who are far sighted and don't have close up glasses. The large size is due to the ability to time share the dashboard space In addition the Focus of eye can be on objects outside the vehicle, as letters big enough to read even if out of focus.</li> <li id="ul0015-0007" num="0099">Can lead a degree of dash standardization between vehicles not now practical from a marketing point of view. One can in an extreme, carry your own dash program with you from one vehicle to another (e.g. a rental car)!</li> <li id="ul0015-0008" num="0100">Much more data available, in a wide expanse of screen. This also includes TV imagery of the surroundings of the vehicle (eliminating blind spots, aiding parking, etc). Novel too is the ability to provide a human assist, e.g. designation, to automatic vision and ir systems</li> <li id="ul0015-0009" num="0101">The tactile control of the computer system used can be customized not only by the user in general, but for specific purposes—what I call a “Mission specific” tactile and visual input and output. This makes it still safer by providing just the right tactile controls for the application at hand.</li> <li id="ul0015-0010" num="0102">Some exemplary versions can potentially be collocated with airbags, allowing placement in positions most easy to use, such as the steering wheel.</li> </ul> </li> </ul> <p num="p-0053">It is in addition, a goal of the invention to improve vehicle safety by
</p> <ul> <li id="ul0016-0001" num="0000"> <ul> <li id="ul0017-0001" num="0104">Providing an instrument panel which has larger and easier to read and comprehend controls</li> <li id="ul0017-0002" num="0105">Providing a means to inform drivers of hazardous conditions</li> <li id="ul0017-0003" num="0106">Providing a means to automatically cause the vehicle to take action in hazardous situations</li> <li id="ul0017-0004" num="0107">Providing a means for drivers to seek important safety related information</li> <li id="ul0017-0005" num="0108">Providing a dashboard of enhanced passive safety</li> <li id="ul0017-0006" num="0109">Providing a means for telematic connectivity which is conducive to safe driving</li> </ul> </li> </ul> <p num="p-0054">It is a also a goal of the invention to make possible the safe use of aesthetically pleasing large screen programmable displays by a driver as the primary means of data input and output. And to provide means for changing these displays and inputs to suit the needs and desires of the user.</p>
  <p num="p-0055">It is an added goal of the invention to provide tactile displays which allow a driver to “feel” the information desired as well as see it, similar in many ways to instrument panels of today but with many added benefits.</p>
  <p num="p-0056">It is a goal of the invention to provide a touch screen or other tactile display having a passive or active tactile feedback to a users touch.</p>
  <p num="p-0057">It is another goal of the invention to provide a method by making touch commands usable with just a glance from a driver to the region of a large easy to read display.</p>
  <p num="p-0058">It is a further goal to provide displays and actuation devices for vehicle functions which can be customized at a factory or dealership or otherwise (for example from a web site).</p>
  <p num="p-0059">It is also a goal of the invention to provide displays and inputs not just for the functions of the vehicle operation systems, but for a myriad of added functions, such as navigation, reconnaissance, communication, information download and retrieval, gaming, and even pleasing interior design and changes thereto. Many of these functions also can serve applications outside the vehicle as well, such as in the home or workplace.</p>
  <p num="p-0060">It is another goal of the invention to provide a touch screen having controls which indicate to the user via physical touch sensible signals, such as acoustic waves, a condition, such as the control device touched, the particular selection made of choices concerning same (e.g. heat or wipers), or the magnitude of the setting desired of the choice made. This can also indicate what the current value of the choice or function is at the time of touch, not just changes therein.</p>
  <p num="p-0061">It is a goal of the invention to further provide means by which complex instructions for use can be presented in easily readable form co-located with the control function whose instruction is needed.</p>
  <p num="p-0062">It is a goal of the invention to provide means by which multiple inputs and outputs (e.g. touch, voice and direct optical viewing of human or human actuated switch positions) can be used in concert.</p>
  <p num="p-0063">It is a goal of the invention to provide means for both z (direction into the display screen) and force based input of single or multiple human or object inputs.</p>
  <p num="p-0064">It is goal of the invention to provide a new form of flat panel touch screen having tactile relation of user touch to the screen.</p>
  <p num="p-0065">It is goal of the invention to provide a flat panel display having tactile physical selection and adjustment capability.</p>
  <p num="p-0066">It is goal of the invention to provide a touch screen which optically provides feed back to a driver in his line of sight of touch screen or touch pad input movements.</p>
  <p num="p-0067">It is goal of the invention to provide a device, which may also include a touch screen, equipped with interchangeable front panels which have specialized and if desired, purpose special physical devices for interaction with the sensing arrangement used.</p>
  <p num="p-0068">It is goal of the invention to provide a touch screen having a display and associated control function which moves around the point of touch, using for example a vector of touch, as opposed to requiring a person's touch to track a predetermined fixed display a task requiring much more visual concentration.</p>
  <p num="p-0069">It is a goal of the invention to provide methods for using a touch screen for fine manipulation of data on the screen, heretofore not possible.</p>
  <p num="p-0070">It is goal of the invention to provide a method for controlling a vehicle or other device which uses a combination of functions, particularly touch, voice and gestures.</p>
  <p num="p-0071">It is another goal of the invention to provide for methods of work improvement in use of computer tools such as spread sheets, word processing, computer aided design and the like.</p>
  <p num="p-0072">It is a further goal of the invention to provide improved controls for automation systems, including those in the home.</p>
  <p num="p-0073">It is noted that this invention presents fundamentally different concepts for touch screens that those which are menu driven as known in the art (though it can use those too). It calls for “Natural” functions more related to the existing instrumentation of the vehicle and the experience base of the driver, including physical implementation not known heretofore in the touch screen or control art, whether for cars or for any other purpose.</p>
  <p num="p-0074">These tactile methods, together with novel screen embodiments and other means for communication with automotive and other control functions are now disclosed.</p>
  <p num="p-0075">While described as displays or “screens” in typical computer parlance, there are in general, combinations of programmable displays, tactile touching aspects and gesture and voice, to make a synergistic whole. Also the screens can be as noted in front of the driver or passenger, or in the center stack (middle region of the dash) for example, as desired. They are discussed herein in the center stack as this is thought the first point of application due to non-interference with airbag location and other factors.</p>
  <p num="p-0076">Finally, it is possible that the disclosed display and instrumentation invention will prove safer for vehicle occupants than those of today. One reason is that protruding switches, knobs, stalks and the like can be eliminated, and these in some cases have caused injury, particularly when passengers have been thrown into the center part of the vehicle which has no protective airbag today.</p>
<description-of-drawings> <heading>BRIEF DESCRIPTION OF FIGURES</heading> <p num="p-0077">The invention will further be described in the following figures:</p>
    <p num="p-0078"> <figref idrefs="DRAWINGS">FIG. 1</figref> Illustrates conventional automotive practice, and arrangements of screens and input devices (touch or otherwise) which comprise various automotive operative control and information embodiments of the invention. Further illustrated is a preferred embodiment for near term commercialization, located in this example in the center stack. This embodiment discloses programmable tactile physical selection or adjustment means and is most like the dash operation in vehicles of today, but in addition has programmable visual and tactile aspects of high utility. Projection and LCD display examples with different types of optical sensing are illustrated, as are interchangeable tactile elements.</p>
    <p num="p-0079"> <figref idrefs="DRAWINGS">FIG. 2</figref> illustrates additional detail and features of the preferred embodiment for near term commercialization possessing a physical detail tactile feel, which may be also operated in conjunction with the surface distortion or other types of touch screens disclosed herein</p>
    <p num="p-0080"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a basic embodiment of a dashboard located touch screen, similar to that of <figref idrefs="DRAWINGS">FIG. 2</figref> of Ser. No. 09/435,854. Also illustrated in <figref idrefs="DRAWINGS">FIGS. 3</figref> <i>b </i>to <b>3</b> <i>d </i>are various examples of tactile-like manipulation of data using such a screen</p>
    <p num="p-0081"> <figref idrefs="DRAWINGS">FIG. 4</figref> a preferred arrangement for safety and other purposes, in which the illumination and image projection source are off-axis, such that an airbag (also called supplemental restraint system, or SRS) can be deployed on-axis. Both optical and computer correction techniques are used to make a presentable display.</p>
    <p num="p-0082"> <figref idrefs="DRAWINGS">FIG. 5</figref> illustrates a touch screen having indented or raised portions of the screen or overlays thereon to facilitate the driver finding the correct location for input action the touch screen may provide, to aid knob turning, slider moving, switch rocking, turn signaling, transmission mode selection and other usage of various tactile physical selection or adjustment means.</p>
    <p num="p-0083"> <figref idrefs="DRAWINGS">FIG. 6</figref> illustrates a combined touch, gesture and voice activated system</p>
    <p num="p-0084"> <figref idrefs="DRAWINGS">FIG. 7</figref> illustrates a touch screen on the dash, whose control is further (or alternatively) equipped or enhanced with an optional TV camera based human extremity (head, fingers, hands, etc) tracker in the roof and other features.</p>
    <p num="p-0085"> <figref idrefs="DRAWINGS">FIG. 8</figref> further illustrates methods provision and functionality of external data using a screen with TV images</p>
    <p num="p-0086"> <figref idrefs="DRAWINGS">FIG. 9</figref> illustrates another version, in which the steering wheel and airbag also encompasses the display and touch device.</p>
    <p num="p-0087"> <figref idrefs="DRAWINGS">FIG. 10</figref> illustrates an embodiment of the invention where a Screen display moves to or around a touch point touched by a user, using a vector of touch or a time based input.</p>
    <p num="p-0088"> <figref idrefs="DRAWINGS">FIG. 11</figref> is an alternate embodiment of the invention using the image of a users body portion or object, using light from a projection source, or from inside the car to back illuminate said portion.</p>
    <p num="p-0089"> <figref idrefs="DRAWINGS">FIG. 12</figref> is an alternate acoustic embodiment of the invention using a surface acoustic wave (SAW) conventional type touch screen or other acoustic means and employing a LCD Flat panel display</p>
    <p num="p-0090"> <figref idrefs="DRAWINGS">FIG. 13</figref> illustrates a unique “Heads up display” (HUD) of touch screen and touch position, to aid the driver to work the touch screen while driving.</p>
    <p num="p-0091"> <figref idrefs="DRAWINGS">FIG. 14</figref> illustrates a novel screen design for distortion based touch screens. Also illustrated is an Optical touch screen equipped with a force feedback signal which is acoustically generated by a piezo electric or other transducer providing an acoustic wave force pulse back to the user to signal that one is close to the point desired or has reached it for example. It can be pulsed, or of static or varying repetition rate or frequency, or of varying amplitude.</p>
    <p num="p-0092"> <figref idrefs="DRAWINGS">FIG. 15</figref> illustrates a display screen responsive to a laser pointer or like device used by driver or passengers to input data.</p>
    <p num="p-0093"> <figref idrefs="DRAWINGS">FIG. 16</figref> illustrates an embodiment of the invention having external cameras and cameras to sense the orientation of a drivers head</p>
    <p num="p-0094"> <figref idrefs="DRAWINGS">FIG. 17</figref> illustrates a front sensed switch overlay for conventional or other touch screens.</p>
    <p num="p-0095"> <figref idrefs="DRAWINGS">FIG. 18</figref> illustrates a ball joint or gimbaled physical selection or adjustment means in which position and orientation of the handle input is determined optically using for example a four point target set.</p>
    <p num="p-0096"> <figref idrefs="DRAWINGS">FIG. 19</figref> illustrates a multipurpose home application of the invention</p>
    <p num="p-0097"> <figref idrefs="DRAWINGS">FIG. 20</figref> illustrates an application for difficult environments, such as a home shower bath</p>
    <p num="p-0098"> <figref idrefs="DRAWINGS">FIG. 21</figref> illustrates another screen input embodiment, using cameras sensing humans or objects through the projection screen of the invention</p>
    <p num="p-0099"> <figref idrefs="DRAWINGS">FIG. 22</figref> illustrates further camera sensed controls of the invention</p>
    <p num="p-0100"> <figref idrefs="DRAWINGS">FIG. 23</figref> illustrates a keyboard embodiment of the invention</p>
    <p num="p-0101"> <figref idrefs="DRAWINGS">FIG. 24</figref> is an touch screen improvement whereby touch designation precision of small displayed features is assisted by use of two fingers and/or demagnified cursor movement controlled by ones finger movement.</p>
    <p num="p-0102"> <figref idrefs="DRAWINGS">FIG. 25</figref> illustrates further embodiments of the “touch mouse” of <figref idrefs="DRAWINGS">FIG. 24</figref> using a pinching or other action-in this case with respect to information in a spreadsheet cell or a word document word or letter.</p>
    <p num="p-0103"> <figref idrefs="DRAWINGS">FIG. 26</figref> further illustrates a touch mouse using a sliding lever or rotary knob or joystick representation.</p>
    <p num="p-0104"> <figref idrefs="DRAWINGS">FIG. 27</figref> illustrates embodiments having elastic touch screen properties and employing photo-elastic and other principles</p>
    <p num="p-0105"> <figref idrefs="DRAWINGS">FIG. 28</figref> illustrates an embodiment of the invention where one first touches a screen at any point desired or comfortable, and said touch is temporarily indented into the screen to serve as reference.</p>
  </description-of-drawings> <heading>EMBODIMENTS OF THE INVENTION</heading> <p num="p-0106">
    <figref idrefs="DRAWINGS">FIG. 1</figref> </p>
  <p num="p-0107">This invention is predominately aimed at providing a new form of instrument panel and related controls, based on displays, including touch screen displays, which have tactile attributes of considerable utility for the driver of the vehicle. By themselves, or in concert with voice techniques, these tactile displays enable the effective selection of functions in control related applications, not just in the vehicle, but in the home and elsewhere. Many of the disclosed embodiments are also thought to improve the safety of vehicle operation, while doing so at less manufacturing cost.</p>
  <p num="p-0108">In a vehicle, tactile displays of the invention can be located on the passenger side (mainly for amusement and internet communication), on the driver's side as a replacement for the existing instrument cluster, or in the middle (also called center stack) which is the easiest to implement today because of lack of requirement of airbag application and the fact that the steering wheel does not obstruct hand or eye communication with the display screen.</p>
  <p num="p-0109">Before providing detail on the specific technology employed, it is of interest to consider what sorts of new presentations of data to the driver and passengers of a vehicle might be, if such technology were employed, especially but not necessarily that of the invention. Only if displays having suitable tactile aspects are available (the primary goal of the invention herein), will these possibilities likely ever be realized, and least in the near to mid term.</p>
  <p num="p-0110"> <figref idrefs="DRAWINGS">FIGS. 1</figref> <i>a </i>and <i>b </i>illustrate one example of a conventional Automobile instrument panel of today, in this case of a SAAB 9-5. This dash includes many small buttons, displays and controls, and yet has none of the features desired for advanced forms of telematics, navigation and other features disclosed herein. It is a good example of a nice design at its limit.</p>
  <p num="p-0111"> <figref idrefs="DRAWINGS">FIG. 1</figref> <i>c </i>Illustrates one example of a conceptual vehicle basic dash board <b>1</b>, and center stack <b>5</b>, with tactile displays and/or touch screens <b>10</b>–<b>14</b>, made possible by this invention, incorporated in
</p> <ul> <li id="ul0018-0001" num="0000"> <ul> <li id="ul0019-0001" num="0168">The steering wheel center, <b>10</b> </li> <li id="ul0019-0002" num="0169">The center stack, <b>11</b>,<b>12</b> </li> <li id="ul0019-0003" num="0170">The console, <b>14</b> </li> <li id="ul0019-0004" num="0171">The passenger side of the dash board, <b>13</b> </li> <li id="ul0019-0005" num="0172">And possibly the instrument cluster itself Not here illustrated.</li> </ul> </li> </ul> <p num="p-0112">As will become apparent on consideration of this invention, it is possible to provide in an automobile dash, a highly effective touch screen or other display of large size having the necessary tactile feel, passive safety, and air bag accommodation where needed to make it acceptable from a safety point of view.</p>
  <p num="p-0113">This unique ability opens up a host of novel dashboard implementation embodiments presented here, which are sometimes also possible less preferably with other prior art touch screen devices in certain circumstances as well. The reason for this is the huge increase in flexibility presented by the tactile touch screen concept, and its ability to greatly increase the active information which may be made available to improve automotive safety, as well as the driving experience, at the same time in total synchronism with current trends to information availability at all times and places, including in ones vehicle. (a concept referred to as “Telematics”).</p>
  <p num="p-0114">Many of these advantages accrue because the same display can function as the controls, the vehicle operation information presentation, and the telematic function displays. And on top of that, can provide information from cameras and other inputs directly where needed by the driver (as such information can temporarily, as in time of stress or crisis, replace some or all of the functional displays, for example of speed or engine RPM).</p>
  <p num="p-0115">Some novel screens and other dash configurations and representations according to the invention, which give an idea of the breath of possibilities are given below.
</p> <ul> <li id="ul0020-0001" num="0000"> <ul> <li id="ul0021-0001" num="0177">a. Standard screen</li> <li id="ul0021-0002" num="0178">b. Highway screen</li> <li id="ul0021-0003" num="0179">c. City Traffic screen</li> <li id="ul0021-0004" num="0180">d. Audio enjoyment screen</li> <li id="ul0021-0005" num="0181">e. Night vision and fog screen</li> <li id="ul0021-0006" num="0182">f. A Difficult driving or “Crisis” screen, where all functions relate to navigating or recovering from a difficult situation—such as very heavy traffic, ice conditions, etc.</li> <li id="ul0021-0007" num="0183">g. Malfunction and alarm screen, including Automatic switch to a “trouble mode” so to speak when something wrong is detected</li> <li id="ul0021-0008" num="0184">h. Telephoning faxing and Email screen</li> <li id="ul0021-0009" num="0185">i. Climate control optimization screen</li> <li id="ul0021-0010" num="0186">j. Exaggerated character and input screen (for elderly or infirm persons, or as desired)</li> <li id="ul0021-0011" num="0187">k. Internet and communication screen (passenger)</li> <li id="ul0021-0012" num="0188">l. Internet and communication screen (driver moving, and driver stopped)</li> <li id="ul0021-0013" num="0189">m. A Techno screen, with graphs, charts, etc of interest (laws may prohibit some of these from being displayed to the driver while moving—in this case a set of abbreviated screens with “legal information” might just be displayed.</li> <li id="ul0021-0014" num="0190">n. A “preflight” checkout screen to allow the driver to ascertain correct functions and settings of various subsystems</li> <li id="ul0021-0015" num="0191">o. Preventive maintenance screen, with data, predicted service times, and even camera images of key parts of vehicle such as wheels, engine, etc.</li> <li id="ul0021-0016" num="0192">p. “Global” navigational screens, including GPS based MAP data as is now prevalent for guiding one to a destination</li> <li id="ul0021-0017" num="0193">q. “Local” navigational screen for parking, in which camera and other sensory data (microwave, ultrasound, IR lidar, etc) are greatly magnified and presented. To aid in parking or navigating in tight quarters.</li> <li id="ul0021-0018" num="0194">r. Parking spot screen, providing data on location of parking spots in for example, a garage, using data downloaded to the car upon entering or nearing said garage.</li> <li id="ul0021-0019" num="0195">s. Car subsystem amusement and education screen, for displaying, for example
        <ul> <li id="ul0022-0001" num="0196">a. 3D representation of airflow, with different HVAC and vent position settings,</li> <li id="ul0022-0002" num="0197">b. 3D representation of sound, with different settings, including if desired the monitoring of ambient sound</li> <li id="ul0022-0003" num="0198">c. 3D representation of rear and front viewing situations by various optical subsystems (mirrors, cameras, IR or other night vision, etc) and radar subsystems if used.</li> </ul> </li> </ul> </li> </ul> <p num="p-0116">The above screens can, if desired, occupy 60–70% of the width of the dash, in front of the driver and in the center. Or they can be in front of the driver, in the instrument cluster area, or on the steering wheel as disclosed herein. They are primarily for use by the driver, unassisted, but clearly have use for passengers as well. If they are not for the driver, many of the comments regarding vehicle functions may not apply (e.g. the need for a speedometer or other critical car function if any to be displayed somewhere at all times may not be needed).</p>
  <p num="p-0117">These screens do not represent the totality of novel functions which may be provided, but are ones that I feel are of considerable utility. Generically, these novel screens have one or more of these common threads.
</p> <ul> <li id="ul0023-0001" num="0000"> <ul> <li id="ul0024-0001" num="0201">1. An ability to switch automatically to a new safety oriented state on input of data from sensors which indicate such is called for. This state can be activated manually as well</li> <li id="ul0024-0002" num="0202">2. an ability to switch among several screens</li> <li id="ul0024-0003" num="0203">3. an ability to be switched from one to another, as well has have individual functions triggered from a variety of human command inputs, including touch related activities of various sorts, plus hand motions, head motions, finger motions, and the like.</li> <li id="ul0024-0004" num="0204">4.</li> </ul> </li> </ul> <p num="p-0118">Some more detail on the screens listed above appears in a sequence of <figref idrefs="DRAWINGS">FIGS. 29</figref> <i>a </i>to <b>29</b> <i>s </i> </p>
  <p num="p-0119"> <figref idrefs="DRAWINGS">FIG. 1</figref> <i>d </i>illustrates a preferred highly tactile center stack embodiment for near term commercialization, as it is closest to the instrument panel of today in terms of touch and feel, and visually, yet offers the full programmability required tomorrow and it is also thought to promote safer driving. The overall shape, in this example, is that of a SAAB 9000 and 9-5 which, like the Oldsmobile Aurora has a portion of the dash/center stack as its called, canted toward the driver, representing an almost ideal location for a touch screen or other embodiment of the invention.</p>
  <p num="p-0120">The base screen functions are depicted in the quasi rectangular portion of the center stack area in question, in keeping with the notion of a relatively simple projection display on a scattering screen in the same area.</p>
  <p num="p-0121">This embodiment utilizes “classical” tactile physical selection or adjustment means (such as knobs) common to dashboards today which may be also operated in conjunction with the surface distortion or other types screens disclosed herein. What makes it novel and exceedingly useful is that it has programmable visual and tactile aspects approaching and in some cases exceeding today's dashboards, while at the same time being programmably changeable as needed to provide added features and enhanced versatility. By doing so, safety is improved, and user value is enhance.</p>
  <p num="p-0122">Elements have been disclosed in the referenced co-pending applications. Additional details are provided in <figref idrefs="DRAWINGS">FIG. 2</figref> and other figures.</p>
  <p num="p-0123">As shown in figure, an instrument panel center stack <b>101</b> is equipped with a large panel <b>105</b> which is primarily a display screen having scattering characteristics to act as a rear projection screen for LCD, DLP, or other type of computer display projector <b>110</b> positioned behind it when viewed by driver <b>111</b> from the drivers side, and controlled by computer <b>120</b>.</p>
  <p num="p-0124">Clearly by known means computer data including messages and other communications down loaded to the computer from external sources, or the vehicle itself can be projected on the screen.</p>
  <p num="p-0125">In this embodiment however, knob <b>115</b> (and if desired other knobs or tactile physical selection or adjustment means such as <b>116</b>) is mounted on the screen so as to be rotatable thereon to various positions which are sensed (for example by optical means as shown in <figref idrefs="DRAWINGS">FIG. 2</figref>, such as camera <b>117</b> which looks at points on the back of the knobs which relate to their circumferential position) and reported to the computer <b>120</b> which in turn indicates the knob position, functions or other data by projecting same onto the screen. The rotation can optionally have a detent function, or another feeling sensation dependent on rotational position can be alternatively provided by an acoustic source such as <b>125</b> programmed by computer <b>120</b> and driven by drive electronics <b>127</b> which, on command sends acoustic waves <b>126</b> which can be felt by the user thru the screen panel <b>105</b>, which is made of Plexiglas or lexan.</p>
  <p num="p-0126">By sharing the display area between knob selection or adjustment functions (and their description/instructions, and display functions (e.g. display of navigational charts), space is saved on the dashboard, and larger knobs and lettering may be provided (especially given the programmable operation). This then promotes safety immediately by making it easier to see what is desired, or has been effected. The screen too can be a touch screen as well.</p>
  <p num="p-0127">Contributing even more to safety, the tactile feel of the knob, or other tactile physical selection or adjustment means, can itself be programmable, for example using programmable acoustic wave pulses or other means, giving many added benefits. Note that such a programmable tactile response can be programmed to change with function selected, and/or variable affected And can operate statically too, to give the driver a chance to tell the setting of the knob by feel alone. In addition, conventional cues to the driver such as the displayed values or computer generated speech can be used as well or instead.</p>
  <p num="p-0128">While described as a knob movable rotationally, other tactile physical selection or adjustment means like sliders, switches, levers or the like which are movable linearly, angularly, or in other manners can be used.</p>
  <p num="p-0129">Let us consider in more detail the operation of the preferred embodiment, also in consideration of the front view of <figref idrefs="DRAWINGS">FIG. 1</figref> <i>e</i>. As shown, there are two knobs, <b>115</b> and <b>116</b> preferably large for ease of use. While illustrated vertically, a horizontal or any other positioning of such knobs or other tactile selection and adjustment means can be used.</p>
  <p num="p-0130">A driver can look at this screen <b>105</b> and see the lettering displayed in big letters next to knob <b>115</b> indicative of its function, eg the word “HEAT” as shown. If he needs to keep his eyes on the road he can as he touches the knob receive a physical sensation due to the acoustic source <b>125</b>, which typically is a piezo-electric transducer known in the art. For example, if as he touches the knob <b>115</b> the computer <b>120</b> can cause the driver to feel a series of pulses 1 sec apart which indicate that the knob is a function selector knob in its current state of programmed operation. Or the one hertz pulses might mean that it was the heater function (if there was no doubt as to what the knob was for—ie having a sign with big lettering right next to it.) or perhaps it was permanently such a knob.</p>
  <p num="p-0131">Alternatively, the definition of the setting can also be actuated by voice. Say status, and the HEAT indication is displayed.</p>
  <p num="p-0132">In any case as the knob is moved, the lettering projected is changed to indicate the new position. For example in one program, as the knob is turned 20 degrees rotationally, the function changes to wipers from heat, and in so doing the acoustic source causes 2 hertz pulses, (or another choice of signal like alternating high low pulses etc) which signify the wiper function.</p>
  <p num="p-0133">The system is totally programmable—the degrees of turn to create a new choice of selection or function, the acoustic pulse choices, and the visual display of lettering (and language thereof too) is all programmable in computer control unit <b>120</b>. If desired a computer generated sound (which can be made to emanate from one or more of the radio loudspeakers can also be used to indicate position as well or instead of the acoustic wave tactile pulses (or other types).</p>
  <p num="p-0134">This system has the ability to have its data read in a quick visual glance and/or touch and visual confirmation too after a move is made. Voice input or output can also be used. For example when the knob is stopped from being turned, the computer can annunciate its position. Or the person can tell the system via a microphone connected to the computer equipped with a voice recognition program (not shown) what he wants the knob to be (eg heating and air) and the knob function and its associated display can be changed accordingly.</p>
  <p num="p-0135">When the knob is at the point desired in the circumferential direction, it can be left there and after a momentary dwell, the computer registers the reading desired (e.g. wipers at the second position). Or, in another exemplary mode of operation, the knob function can be changed. For example, when at the “wipers” position, the knob can be pushed in to register this choice (wipers) and then after that the knob function and the display associated with the knob changed to wiper speed delay and other wiper function settings at the different circumferential positions desired.</p>
  <p num="p-0136">Note that the unit may have different functions made possible for the knob or other devices when the car is stopped. For example the knob might indicate wipers, heat and radio when in motion, but additionally when stopped could have email, internet surfing and other functions which might be too dangerous while in motion.</p>
  <p num="p-0137">As an alternative to an acoustic source thru the screen to signal a feel of a variable to the user,, a sound generator loudspeaker <b>130</b> can send sub audible waves <b>131</b> against the whole screen <b>105</b> which can be sensed at the point touched.</p>
  <p num="p-0138">In the preferred embodiment shown, the portion <b>106</b> of the screen <b>105</b> is reserved for touch screen inputs for example using known resistive, capacitive or other means such as my distortion based types. Similarly as shown, the users finger <b>141</b> (<figref idrefs="DRAWINGS">FIG. 1</figref> <i>d</i>) touching screen <b>105</b> can input data, and information can also be communicated via acoustic source <b>125</b> controlled by computer <b>120</b>.(see also <figref idrefs="DRAWINGS">FIG. 14</figref>).</p>
  <p num="p-0139">It is noted too that the touch screen can desirably have indents or ridges such as circular depression <b>140</b> in screen <b>105</b>, in order to guide the users finger such as <b>141</b> to a certain location on the screen where for example the starting point of various command movements might be made. The indent or ridge (disclosed further in <figref idrefs="DRAWINGS">FIG. 5</figref> and elsewhere herein) can be shallow such that it can be felt, but not deep enough to cause refractive gradients which would disturb an image say of a map that might take up the whole screen surface in a navigational mode for example. Ridges and depressions in smooth surfaces of even 0.003 inches for example can work (thickness of human hair), while even 0.020 inches deep or high can be used with little optical effect in many cases.</p>
  <p num="p-0140">It should be noted that because the display and tactile physical selection or adjustment means providing control functions and feedback are both programmable, one can have programs which vary by driver. Indeed, one can even take your program with you, for example 1f renting a car having a similar display and a data input device for your to enter your program in (e.g. a CD Rom for example using a version of the CD player of the car). Or your program can be downloaded from remote sources such as your home computer or the internet (where a selection of programs might be found, say on the GM web site).</p>
  <p num="p-0141">The Sequence of actions undertaken then using the embodiment in one preferred version is
</p> <ul> <li id="ul0025-0001" num="0000"> <ul> <li id="ul0026-0001" num="0229">1. Glance,</li> <li id="ul0026-0002" num="0230">2. touch,</li> <li id="ul0026-0003" num="0231">3. move,</li> <li id="ul0026-0004" num="0232">4. confirm (tactily and/or visually—much like today's dash boards—but generally with even more tactile feel relating to the position, and larger lettering which can be better seen at a glance) Alternatively, one can do it entirely by feel, using the techniques described above where the programmable acoustic source is used to input to the user all data needed (even to include the initial starting point of the knob position such as a short pulse burst, with a long delay until the next one to indicate a first position, or the end point, which might be three short pulses with a long delay, to indicate position <b>3</b> for example. Other pulse frequencies or codes could signify different programmed knob functions if desired, like wipers or heat, or whatever. Or voice can be used as discussed, or combinations thereof.</li> </ul> </li> </ul> <p num="p-0142">Also gestures can be used, such as hand or finger position or movement, as disclosed in my copending applications for example.</p>
  <p num="p-0143">It should also be noted that using the computer controlled projector (or other screen) display a variety of visual cues can be used to signal a function or state has been reached. For example, Not only can one display a word such as “high” to indicate high heat, but one can also have it blink 3 times when reached. This could correspond to a acoustic or other tactile signal comprising three pulses as well.</p>
  <p num="p-0144">Also it is possible for the colors or patterns of the words or figures to be changed programmably. For example, in the <figref idrefs="DRAWINGS">FIG. 1</figref> case the whole knob, or its surroundings could be illuminated through use of an appropriate computer program for the display, in bright red when the highest heat position was chosen (with blue, for the lowest, for example). And for example, the knob surroundings could be projected on the display in polka-dots, if the knob was not in a position that actuated a function (e.g. a dead zone).</p>
  <p num="p-0145">Where desired (e.g. with elderly drivers) the writing near the knob, might be in large letters, which could even be so large that the words (such as “high”, for a heater blower speed selected, would need to be abbreviated as HI″ for example).</p>
  <p num="p-0146">It should be noted that the knobs or other tactile input devices shown could be contained on an interchangeable member. One can have a touch screen, equipped with interchangeable front panels of the type just described which have specialized physical devices for interaction with the sensing arrangement used. In this manner one can actually add in an interchangeable manner, overlays to your dash which can be used for different purposes. This can enable you to do tasks by feel not otherwise possible.</p>
  <p num="p-0147">For example consider a tactile display or touch screen of the type shown in <figref idrefs="DRAWINGS">FIG. 1</figref> or <b>2</b> using optically sensed details on the back of the various control items used. In this instance the control items are able to be interchanged. A group of levers could be on one screen, knobs on another, and so forth. Or one screen could have markings on its face for email and stocks, with levers and knobs to suit, etc.</p>
  <p num="p-0148">This overlay type of arrangement does not depend only on the touch screen types I have invented, but may work with touch screens employing other technology, for example the acoustic surface wave version of <figref idrefs="DRAWINGS">FIG. 12</figref>.</p>
  <p num="p-0149">The tactile control of the computer system used can in this manner, be customized not only by the user in general, but for specific purposes—what I call a “Mission specific” tactile and visual input and output. This makes it still safer by providing just the right tactile controls for the application at hand.</p>
  <p num="p-0150">To illustrate, consider optional removable (and interchangeable) tactile screen member <b>144</b>(dotted lines), in this case a piece of plastic shaped like a 3×5 inch card, and containing a knob <b>145</b> (also dotted lines) as described whose position can be determined (optically via camera <b>117</b>, or otherwise).</p>
  <p num="p-0151">This card member can be inserted to change the function of the tactile portions, or for example to include, as illustrated in this case, an additional knob as tactile physical selection or adjustment means. In one preferred embodiment, the card member can be placed in slot between guides <b>146</b> and <b>147</b> (which could be dovetailed to assist). Data concerning the function of the knob in this particular case can be provided to computer <b>120</b> by any means desired, including downloads from the internet, magnetic or optically encoded data on the card or whatever.</p>
  <p num="p-0152">The card as well can have printing on it, and may be transparent in areas desired to allow data from the display behind it to indicate data relating to the knob position or other variables. The use of data permanently printed on the card might befit a mission specific or one time use card, for a particular project, or a particular drive to work. Note that the card could also correspond to just one function, like an internet music source selection card, that you could use just for this purpose, removing and storing it when you wanted to put another card in, or free up the display area for other information.</p>
  <p num="p-0153">It should be noted that an acoustic source such as 125 providing programmable feel, can couple to this insert-able member as well. Such cards have a very interesting use as single use, or mission specific controls for certain purposes, and provide additional freedom in choice of tactile physical selection and adjustment means.</p>
  <p num="p-0154">It is noted that TV camera based systems are not the only way of determining position or orientation of tactile physical devices such as knobs in the invention. Other technologies may be used. For example of interest in rear projection display based applications of the invention are DLP or micro-mirror type chip projectors which can be used in a reverse mode as a scanning device, capable of determining, with a suitable detector in conjunction therewith, indications relating to say the location of a flag on a knob, corresponding to its rotational position. In some cases they can be used to see finger touch as well, by for example, the direct viewing method of <figref idrefs="DRAWINGS">FIG. 11</figref>.</p>
  <p num="p-0155"> <figref idrefs="DRAWINGS">FIG. 1</figref> <i>f </i>provides another illustration of such devices. Consider a version, in which a transparent member <b>150</b>, typically glass or Lexan plastic is placed in front of an LCD or other flat panel display (e.g. a plasma display) <b>155</b> having front surface <b>156</b> and rear illumination source <b>157</b> which directs light upward in the drawing. LCD displays operate by switching bi-refringent properties of individual elements such as <b>158</b>–<b>160</b> (of hundreds of thousands), whose size is exaggerated for clarity.</p>
  <p num="p-0156">Knob <b>165</b> is secured to transparent member <b>150</b> using pin <b>166</b>. On the rear of the knob, is reflector <b>170</b> which reflects light <b>174</b> from light source <b>157</b> passing thru the LCD elements such that light so reflected (or other wise deviated) back into member <b>150</b> reaches high gain photo detector <b>175</b> by bouncing between the front and rear surfaces <b>176</b> and <b>177</b> of member <b>150</b>. The reflector design can be such that it scatters or otherwise helps direct light to the detector.</p>
  <p num="p-0157">In operation, the location of reflector <b>170</b> is determined, in order to determine the circumferential location of knob <b>165</b>. In this example, the area of LCD pixel elements in the region of the knob are all initially dark (in this case a circular area), and one by one are sequentially opened up (under computer control) to allow light from source <b>157</b> to pass thru in order to illuminate the region in front of them. To illustrate, with the knob pointed in the position to the right of the drawing, as the pixel elements <b>158</b>–<b>160</b> are sequenced thru, it is noticed on graph <b>180</b> of detector amplifier <b>181</b> output, that at as element <b>159</b> was illuminated, that detector <b>175</b> received a signal considerably stronger than at other positions. This then indicates that the reflector was closest to that represented by LCD element <b>159</b>. Typically such a position would be an grouping of elements able to identify knob circumferential position within the tolerances needed (e.g. every 10 degrees, say).</p>
  <p num="p-0158">LCD elements can be very rapidly sequenced in many cases. Thus it may be that a sequential illumination scan such as just represented can be done so fast (e.g. in a millisecond) that it is unnoticed by a user—even if he is looking directly at the knob. (human eye integration time being 60 milliseconds.). To make the total scan even quicker, it is noted that it is only necessary to scan an annulus corresponding to expected reflector positions, if a point reflector is used on the periphery of the knob, say. Other reflector designs can also be used. And refractive, diffractive, or other elements to direct light can also be used.</p>
  <p num="p-0159">It is noted that transparent member <b>150</b> may be interchangeable as well, as disclosed above.</p>
  <p num="p-0160">As shown therefore the circumferential location of knob <b>165</b> has been determined. Similarly slider (or other tactile selection or adjustment means) axial locations can be determined as well. And in addition, so can finger touch locations in x and y dimension of the LCD panel surface.</p>
  <p num="p-0161">For example consider finger <b>190</b> touching front surface <b>177</b> of member <b>150</b> as shown. In this case the finger itself acts to reflect light <b>191</b> from individually sequenced LCD pixels (not shown for clarity) to the detector and its location can be determined by similar means.</p>
  <p num="p-0162">Finally it is of interest to consider optional ridge or other small protrusion <b>192</b> on LCD flat panel screen (or cover glass thereon, which as noted can be interchangable). This as noted is to provide a tactile relation between the users finger and the screen and display, an issue discussed in <figref idrefs="DRAWINGS">FIG. 5</figref>. This allows the finger to easily find a set point or line of action on the screen without looking. Typically the height of the ridge or alternatively an indentation is small, so as not to unduly disturb the light field of the screen. Generally preferable is when there is sharp slopes, so as to minimize lens effects.</p>
  <p num="p-0163">It should be noted that displayed data next to (or even within same, if room exists a knob or other tactile physical selection or adjustment means usually at a minimum displays the selection or adjustment choices—e.g. wipers, heat, air, cruise; or lo, med, hi for example. Alternatively or in addition, pictographs, colors or patterns may be used for example.</p>
  <p num="p-0164">In addition other information can also be displayed, such as more detail about what the choice means. This can even include effectively displaying the instruction manual for that function if desired.</p>
  <p num="p-0165">
    <figref idrefs="DRAWINGS">FIG. 2</figref> </p>
  <p num="p-0166"> <figref idrefs="DRAWINGS">FIG. 2</figref> illustrates in further detail the versions and features of the preferred embodiment for near term commercialization disclosed in <figref idrefs="DRAWINGS">FIG. 1</figref>. It is illustrated in the form of a rear projection display, but is not limited thereto. This embodiment senses positions of screen mounted knobs and other conventional tactile selection and adjustment means. Such sensing can be performed by a variety of electrical, magnetic, acoustic or other means known in the art, but here is preferably illustrated with optically based sensing means, which has the advantage of simplicity and non contact operation.</p>
  <p num="p-0167">Consider screen <b>200</b>, where a physical tactile selection or adjustment means such as a knob, (switch, slider, rocker, etc), <b>203</b>, is in one portion of the screen surface, and this knob has on its rear facing the camera a “pointer” or mark <b>210</b> which can be imaged and sensed by the camera <b>205</b> and its associated image processing computer <b>212</b> to tell the angular knob position, and pass this information to the display and vehicle control computer <b>215</b>. Camera <b>205</b>, can also be used to see screen distortion under touch like <b>340</b> in <figref idrefs="DRAWINGS">FIG. 3</figref>. This means that a combination touch screen and “regular” function dash can be provided for very low cost, which can be interchanged, since the camera <b>205</b> can be programmed to see any effect in any zone. Note that the light source of projector <b>218</b> even may be used to provide illumination of mark <b>210</b>. And since one can control the projector, one can choose the light projected for example to clearly illuminate the marker—which itself may be preferentially reflective of a certain color which may be instantly recognized in the color image obtained by camera <b>205</b>, which can be a solid state matrix TV camera. Identification of the marker is also made easier by the fact that its relative position is approximately known to be in a certain region of the screen.</p>
  <p num="p-0168">While the projector source may be used to light the markers of any knob slider, switch or other selection or adjustment means, a separate light source such as laser light source <b>219</b> can be used alternatively such that the camera using filter <b>221</b> (dotted lines)can for example see, using an interference filtered image responsive only to laser light, the knob mark reflection (where it too can have a dichroic mirror reflecting only laser light substantially). This makes the sensing of knob position independent of what is being projected, and it can operate with no projection at all. Its noted that if the laser (or alternatively for example an LED source) is in the near IR (e.g. 0.9 microns) the user will not see any indication of this.</p>
  <p num="p-0169">With proper construction, the knob or for example transparent slider <b>230</b>, having mark <b>231</b> (also viewable by camera <b>205</b>, or a different camera if desired) running in track <b>235</b> in transparent screen <b>200</b>, can be all transparent (however, scattering light from the screen face, <b>201</b>. Under computer control, the projector source <b>218</b> then can be used to change these devices to indicated different functions, both by projected lettering and color, and possibly other patterns or the like.</p>
  <p num="p-0170">For example consider the function of transparent slider <b>230</b>, illuminated by light projected through it, <b>231</b>. as shown this runs in dove tail groove track <b>235</b> in the outer portion of screen <b>200</b>, as shown in <figref idrefs="DRAWINGS">FIG. 2</figref> <i>b</i>. in one mode, shown in <figref idrefs="DRAWINGS">FIG. 2</figref> <i>c</i>, the word “HEAT” is projected on the screen over the top of it, by projector <b>218</b>, and further, the zone from a to b in the slider grove may be projected with a continuous band changing from red (hot) at point a, to blue (cold) at point b. Camera <b>205</b> senses the position of mark <b>231</b> and computer controls the heating and air-conditioning system of the vehicle accordingly.</p>
  <p num="p-0171">In another screen scenario however, the projector can turn this same slider into another function altogether, for example as shown in <figref idrefs="DRAWINGS">FIG. 2</figref> <i>d</i>, where the data projected is indicative of the viewing direction of a swiveling exterior rear view camera on the right side of the vehicle, which presents an image <b>240</b> of the rearward TV camera scene viewed on the same screen <b>200</b> (or alternatively, a different screen), by virtue of the projector <b>218</b> also projecting the image. The words right and left are projected in this case.</p>
  <p num="p-0172">In this manner, a relatively small number of knobs, sliders, switches, buttons, or whatever can be used to control a large number of functions, while in large measure still retaining a classical “feel” to which users of dashboards are accustomed. Not all portions of the screen zone need be transparent, allowing the projected image to be displayed at all points, but this feature allows maximum flexibility in the use of the screen. The surface distortion technique or other touch screen technologies as desired, may be combined with the direct viewing technique as can be appreciated, such that more data can be entered as well. For example in a particular combination, one can use the <figref idrefs="DRAWINGS">FIG. 2</figref> version to dial in a heat setting, and then push the knob in, creating a distortion type reading as taught in <figref idrefs="DRAWINGS">FIGS. 2–4</figref> above and co-pending applications indicating that the selection has been made. Or one can use stereo vision or other means to detector the push command—which also may be proportionally related, rather than just a switch.</p>
  <p num="p-0173">Its noted that not only then is the image displayed programmable, but the inputs are too. for example, if problems were detected, the Malfunction screen (or an applicable portion thereof, such as electrical system data, if an electrical problem was detected) could be called to be displayed, and the slider <b>230</b> would assume say, the function of turning the hazard lights on, and calling “ONSTAR”—if not automatically called.</p>
  <p num="p-0174">It is noted that the knob and slider examples, are those where the position of the device (angular or linear, in the case of knobs and sliders) is determined, and provides an analog or digital indicator (if converted, or if mechanical detents area used) to the control computer. If switches are used, the function can be as simple as on or off. In this case, shown in <figref idrefs="DRAWINGS">FIG. 2</figref> <i>e</i>, the button, such as <b>260</b>, might push in to the screen <b>265</b> to turn on, exposing as it does so a mark <b>270</b> to camera <b>275</b> (like <b>205</b>).</p>
  <p num="p-0175">It is useful to have tactile feel provided with ratchet clicks for example, when moving a knob or slider or other means of the invention. This provides a feedback to the user of how much movement has occurred. In addition, the system itself can provide a feedback by sound, or by visual indication on the screen which could be a change of pattern, color or the like.</p>
  <p num="p-0176"> <figref idrefs="DRAWINGS">FIG. 2</figref> <i>f </i>illustrates another example of a tactile conventional mechanism built into a screen—which itself may optionally be a touch screen. A big knob <b>277</b> as above is for example located in the middle of a large screen <b>278</b> in the central dash portion. Both the knob and the lettering on the screen are desirably large to allow most all drivers to read it easily in a glance without the aid of glasses. Optionally the print and control knob etc sizes could be user selectable. For example a young user might like four small knobs rather than one big one, with less need to go to secondary screens. Another user might want sliders rather than knobs. With an open computer system, programs for this could even be purchased in the after market.</p>
  <p num="p-0177">As shown there are 6 indications at different circumferential locations (as shown there are diversely illustrated as Heat/Air, Wipers, Stocks, Email, audio. It is possible to have more than 6, with each still being quite readable, especially if the screen is large.</p>
  <p num="p-0178">The user turns the knob to the desired indication and presses in the knob as appropriate to indicate his selection, which is detected by the touch screen apparatus or by a second camera capable of seeing the knob from a different angle and thus by matching with image from camera <b>205</b> determining depth movement of the knob by pressing. (see other drawings for examples).</p>
  <p num="p-0179">Then as shown in <figref idrefs="DRAWINGS">FIG. 2</figref> <i>g</i>, a new screen <b>285</b> (in this case a heat one selected in the first case <b>2</b> <i>f </i>for example) may be generated by the display control, indicative of that function. In this particular example a bar indicator <b>286</b> is displayed, and the user touches the point “P” on the bar which he wants, temperature wise that is. The point can be programmed by the display computer to be color indicated, or with actual temperature markings or whatever. Or he can use the knob to select a fan speed, rear air distribution, or something else.</p>
  <p num="p-0180"> <figref idrefs="DRAWINGS">FIG. 2</figref> <i>h </i>illustrates a version of <figref idrefs="DRAWINGS">FIG. 2</figref> <i>f </i>or <i>g </i>which can be achieved with a conventional display screen having no special capability at all. In this case a conventional knob <b>290</b> (or other tactile selection and adjustment means) is located below screen <b>291</b> and a large displayed surrogate knob <b>292</b>, like <b>277</b>, is displayed on the screen. As the user physically turns real knob <b>290</b>, the surrogate knob <b>292</b> moves to the positions indicated—just like in <figref idrefs="DRAWINGS">FIG. 2</figref> <i>f</i>, where it was physically turned. Again, one can push the real knob in if desired to indicate a choice. Or the choice can be indicated by just leaving the knob in position for a minimum time period.</p>
  <p num="p-0181">The “real” knob of <b>2</b> <i>f </i>(albeit virtually displayed), or the surrogate of <b>2</b> <i>h</i>, both allow displays having multiple choices of different variables or functions, and different gradations at different circumferential locations, and the use of different languages or the like.</p>
  <p num="p-0182">It should also be noted that such a conventional approach can be still more conventional, and that is to not display the surrogate knob <b>292</b>, but rather just display the status of the knob and/or the result of its movement or other action.</p>
  <p num="p-0183">For example, consider display of the word “Heat” <b>295</b> in big letters on the screen, as the knob <b>290</b> is touched and movement is sensed. As it reaches its next setting which could be “Wipers”, for example, the display would be changed accordingly. These large words could be supplemental to any smaller display words chosen. (particularly when the knob is on the display rather than below it or otherwise positioned).</p>
  <p num="p-0184">It is noted that the acoustic or other programmable tactile feedback to the user operating the conventional knob <b>290</b> can be provided as well.</p>
  <p num="p-0185">It is also noted that with TV camera sensing of a finger touch screen distortion or features of physical tactile devices such as knobs as described can be usefully accomplished by change detection, accomplished for example by subtracting images in a quiet previous state (i.e. static) from instant images. Both finger touch and knob turning represent change which can be so identified. And the region of change pinpointed.</p>
  <p num="p-0186">Its also noted that one can calibrate the camera system each time a measurement is made as well, if the movement is known—e.g. a knob turn to the next position</p>
  <p num="p-0187">
    <figref idrefs="DRAWINGS">FIG. 3</figref> </p>
  <p num="p-0188">Where surface distortion based transduction of touch events is desired, a preferred means of detecting surface distortion is that given in reference <b>1</b>, which discloses illumination of a surface and subsequent retroreflective re-illumination of the surface from which an enhanced image of the distortion in such surface are created. This method (and the products based thereon sold under the trade name “D-Sight.TM.”), is at once, simple, fast, and capable of intelligibly measuring minute distortions over large surface areas. All of these are advantages for the present disclosed invention, and “D-Sight. TM. is the preferred method (but not the only method) for determining such distortions. Other optical techniques are grid and moiré triangulation, also providing surface distortion data.</p>
  <p num="p-0189"> <figref idrefs="DRAWINGS">FIG. 3</figref> is a basic embodiment of one type of distortion/deflection based touch screen suitable of the invention, previously disclosed in reference <b>2</b>, <figref idrefs="DRAWINGS">FIG. 2</figref> (and others). Touch screen <b>301</b> located on a vehicle dashboard <b>302</b> in front of a passenger <b>305</b> for example, typically and desirably of significant size so as to make reading and touching easy, operates as follows. The screen surface <b>301</b> is illuminated by a rear projection TV projector <b>310</b>, in this case provided by a low cost micro-display such as an LCD matrix <b>311</b> or alternatively for example, a Texas instruments DLP device, illuminated by light source <b>309</b>, and imaged on screen <b>301</b> by lens <b>313</b>, and controlled by display computer <b>315</b>.</p>
  <p num="p-0190">A TV camera based transduction of the screen touch or other physical function is used as described in co-pending applications and further in this application. In this embodiment, surface distortion of the screen caused by touch or impact of one or more objects (e.g. thumb <b>320</b> and first finger <b>321</b> of the passenger) is a achieved using for example the D sight effect, and employing TV camera <b>340</b>, light source <b>345</b>, and retro-reflective screen <b>350</b>, together with microcomputer <b>360</b> (which may be the same as <b>315</b>) to process the data obtained as to the location, of any and all contacts in the xy plane of the screen, and their z direction or force function if desired. As was disclosed in the referenced application, it is the local screen distortion caused by the touch that is being sensed in this case.</p>
  <p num="p-0191">It is noted that camera <b>340</b> imaging distortion of screen <b>301</b> touched by finger <b>321</b> may also be used to image a projected image on the screen produced by image projector <b>310</b> and backscattered by the screen. Analysis by computer <b>360</b> attached to camera <b>340</b> directly correlates the point of touch “P” to a point determined by <b>360</b> in the image. For example 1f a test image is produced on the screen which has 100 vertical bars, and the finger touch hits between the 90<sup>th </sup>and 91<sup>st </sup>bar, it is then known that the reading of the touch determining apparatus corresponds to this location in the image. Similarly <b>100</b> vertical bars could be projected and the vertical distance calibrated.</p>
  <p num="p-0192">This correlation can be performed when ever it is desired to provide a test pattern. Thus the operation of the device is not dependent on stable image projection over time and a knowledge from the projection input, as to where the touch occurred. This kind of operation is useful if there is instability in the projection for example, or if there are problems with the mounting of projection devices, large amounts of thermal expansion or the like.</p>
  <p num="p-0193">The transparent screen material may be darkened if desired, to make the system innocuous and or invisible when not turned on. If desired, the display may be flat, or curved with reasonable positive or negative radius to correspond to aesthetic design considerations. (generally however, flat displays are easiest to use).</p>
  <p num="p-0194">While not the only form of touch screen which can be used with the invention herein, this distortion based system has several unique properties which are helpful in the car application. For example some of those given in reference <b>2</b> are:
</p> <ul> <li id="ul0027-0001" num="0000"> <ul> <li id="ul0028-0001" num="0286">A potential “four” and “five dimensional” capability, wherein the force vector direction as well as the magnitude of force is measured.</li> <li id="ul0028-0002" num="0287">An ability to have a data storage of a complete signature at once, physically or in memory.</li> <li id="ul0028-0003" num="0288">Robust and reliable. Many prior art touch screens are of specialized construction and would be quite expensive to replace if they were broken, especially as the size increases</li> <li id="ul0028-0004" num="0289">An ability to have the surface distortion or touching input means of any material, completely removed from the actual sensing of the input.</li> <li id="ul0028-0005" num="0290">Inexpensive, particularly for larger surfaces.</li> <li id="ul0028-0006" num="0291">Some embodiments give a desirable tactile feedback since it is the actual physical deformation (and the amount thereof) that is responsive. Thus the feedback to a finger (or other member) in terms of resistive force is proportional to the desired input. This tactile feedback is particularly desirable in for example the automobile where one should not take one's eyes off the road.</li> <li id="ul0028-0007" num="0292">High resolution data entry may be made directly on the screen with ordinary instruments or fingers. If desired the “beam” of the display can literally follow the touch point, just as a pencil line would follow a pencil. In this application the 3-D capability allows one to press harder and make a darker (or wider) line for example, just as one would do in normal practice.</li> <li id="ul0028-0008" num="0293">Multipoint Operation. The existing touch screen art is capable only of measuring one touch point in X and Y at a time. The invention is however, capable of multi-point operation or even simultaneous detection of complex area “signatures”, not just “points”. A further advantage of the invention's ability to detect multiple input signatures, etc. at any point on its face, therefore a keyboard, a piano keyboard, a joy stick can be artificially created at any point under computer control or simply by random human command. This is particularly desirable in a car where you cannot necessarily keep your eye on the data entry device. And it is of use for handicapped people who could not be expected to hit the right point on the device every time, but if they just hit the device anywhere, could make a move from that point in a manner that would be intelligible to a computer for example</li> <li id="ul0028-0009" num="0294">Variable and “Intelligent” orientation. It is also useful therefore to replace in many cases keyboards which have continuous arrays of keys, be they optical, mechanical, contact, electro mechanical or whatever. Unlike most keyboards the disclosed type can “float” (i.e. be at any zone on the surface) which is convenient for people who know how to type but cannot see the keys for example, while driving.</li> <li id="ul0028-0010" num="0295">Tactile feedback, including programmable. The unique tactile feedback application aspect of the invention allows one to essentially use a deformable member as a sort of miniature joy stick for each finger or to allow one to rock back and forth between one or more fingers to enter information, for example—such as seek controls on a car radio. In addition, programmable tactile feedback such as air blasts, vibration, etc., can also be added easily to the touch surface.</li> </ul> </li> </ul> <p num="p-0195">Another advantage of the invention is that it can detect a force or displacement signature such that the signature of someone can be used as a control enabler, or other verification tool—for example in authorizing the driver to start the car, or an internet transaction. In addition, for the handicapped, non-conventional inputs such as palms and the like can be used, rather than fingers.</p>
  <p num="p-0196">As was pointed out in copending applications, the distortion based screen, because it physically is perturbed by the finger(s) of the user, has some subtle tactile features all by itself. For example consider finger <b>320</b> and thumb <b>321</b> of person <b>305</b> who desires to “rotate” a virtual knob <b>365</b> (dotted lines in <figref idrefs="DRAWINGS">FIG. 3</figref> <i>c</i>) depicted on touch screen <b>301</b> (for example created by display computer <b>315</b>). The person can do so by pressing in on the screen <b>301</b>, and registering to the computer that a finger <b>320</b> and thumb <b>321</b> are touching in close proximity, just as one would pinch a small knob on a dash of today. This is illustrated in <figref idrefs="DRAWINGS">FIG. 3</figref> <i>b</i>. Then in a sequential motion, the screen sensing system senses that this knob is being turned an amount m, in direction theta, so to speak, as the fingers rotate their points of contact indication, <figref idrefs="DRAWINGS">FIG. 3</figref> <i>c</i>—effectively like a twisting motion. This amount of rotation is communicated to the car control system, and the heater output let us say, raised accordingly.</p>
  <p num="p-0197">In another example shown in <figref idrefs="DRAWINGS">FIG. 3</figref> <i>d</i>, an input of how much windshield wiper delay is desired. is achieved by just touching the screen <b>301</b> again in this case with finger and thumb, spaced d apart, in the general area of windshield wiper control functions (dotted lines for example). This could be a fixed zone of a particular displayed screen (such that the driver would know by heart, or could be projected on the screen for the moment in such large manner that it would be easy to see—and hit, so to speak. (so one wouldn't have to take eyes off road for but a fraction of a second). With fingers still resting on the screen, the driver for example, could move them a distance delta d to a new delay position, in order to decrease the delay, say.</p>
  <p num="p-0198">Alternatively the distance w of a finger touch point <b>338</b> with respect to the fixed base point ‘a’ (eg at the side of the screen, or from a ridge or other tactile reference) could give the delay for example.</p>
  <p num="p-0199">Consider too the action of finger <b>320</b> and thumb <b>321</b> which can both rest on the screen at once, and alternately push in and out, so as to “rock” back and forth, just as one might do today in selecting radio stations with a seek rocker switch, or moving outside mirror positions for example.</p>
  <p num="p-0200">While use of a finger and thumb are most natural, clearly any two things could do it in the above example. And some functions might use three or even more input points.</p>
  <p num="p-0201">More common is just one input, such as a forefinger. Here again the device of ref <b>2</b> is useful, as the forefinger force on the screen (also a z input) can be the function desired.</p>
  <p num="p-0202">For example, as shown in <figref idrefs="DRAWINGS">FIG. 3E</figref>, a box <b>370</b> can be displayed on screen <b>301</b>, and the person just pushes on the box at point ‘P’ with a force sufficient to cause an indentation or distortion ‘Z ’ which is sensed, and used to indicate that a high heater blower speed is desired (the Z value attained could also be indicated to the driver with a tone or other audio pulse, or a visual indication on the dash. For example a value of 0.010 inches could give a louder or higher pitched sound than a 005 inch movement. Or it could be fed to a voice program to just say HIGH, for example). This is easier than, and preferable to, the common touch screen alternative, of leaving ones finger on the box for a given amount of time t, during which the heater speed would ramp up. The latter is disadvantageous, in that one must wait—with ones finger still in place—for the desired speed or other desired result to be obtained.</p>
  <p num="p-0203">It should also noted that it is thought possible that one can push in on a surface distortion detected screen with one object, such as a forefinger, and not only detect the degree of indentation, but the degree with which one might “twist” ones finger in a rotation motion. This makes it easy for a driver to interact.</p>
  <p num="p-0204">It is also noted that one can also write on the display screen <b>301</b> with any object for example pen <b>375</b> and have the computer trace the line such as <b>376</b> on the screen, or store it memory, or transmit it (the same goes for all data inputted to the system by whatever means). One can also sense the head <b>380</b> of pen <b>375</b> with a camera pair <b>385</b> and <b>386</b> (shown here in the windshield header) such that the point of contact is known on the screen, and the head position is known, and thus the pointing angle to the screen is known which can be useful for various applications, such as pointing at data within a 3D virtual display.</p>
  <p num="p-0205">Note that as alternative to the twisting knob motion of <figref idrefs="DRAWINGS">FIGS. 3</figref> <i>b–c </i>above, one can also just trace with ones finger, a movement on the screen. For example, a small left to right linear movement starting at any point in at least a region of the screen, could indicate an increase in a variable such as heat, where as an arc shaped movement in the clockwise direction could indicate an increase in heater blower speed. The more the movement, the more the increase (and like wise decrease). The rate of movement of ones finger across the screen can also be sensed by comparing consecutive camera views, and one could use rate to give a value as well—a quick movement could be a quick blower speed, or a fast wiper speed, say.</p>
  <p num="p-0206">
    <figref idrefs="DRAWINGS">FIG. 4</figref> </p>
  <p num="p-0207"> <figref idrefs="DRAWINGS">FIG. 4</figref> illustrates the same arrangement as <figref idrefs="DRAWINGS">FIG. 3</figref>, but with the image projection device <b>410</b> located off axis at angle alpha to the normal <b>415</b> of screen <b>430</b> as shown, such at an airbag <b>440</b> can be deployed on axis, substantially along the normal. The display can be a simple display only, or can be part of a touch screen as disclosed above and elsewhere herein.</p>
  <p num="p-0208">The airbag when initiated moves the touch screen <b>430</b> out of the way. In this case it is desirable to secure the screen in such a way for example by providing it with a weakened tear line, that it blows down, or out of the way, and to make the screen of material which itself causes minimal harm. The ability to choose the screen material accordingly is advantage when using the screen distortion based touch sensing technology disclosed in ref <b>2</b>. However, other touch screen techniques known in the art can be used. For example a surface acoustic wave generator <b>441</b> can be used (with acoustic microphone pickups not shown for clarity), to triangulate on where the human touch <b>443</b> has modified the acoustic wave passing through the screen material.</p>
  <p num="p-0209">Both optical and computer correction techniques are used to make a presentable display. For example, the projection source may have a light source <b>450</b> illuminating a LCD matrix <b>451</b> driven by display computer <b>455</b>, where the matrix is tilted at an angle beta with respect to the axis of projection <b>460</b> such that the image is in focus at all points on screen <b>430</b>.</p>
  <p num="p-0210">Since this arrangement yields varying magnification at different points across the screen, the input to display computer <b>455</b> is advantageously corrected using equations processed by said computer to account for the variation in magnification and other issues as desired, in order to make the projected image appear uniform.</p>
  <p num="p-0211">As disclosed copending applications, the point <b>479</b> of an ordinary pen or other device held and manipulated by a person is shown here as an input, for drawing or other purposes using computer <b>485</b> connected to the accoustic sensing system <b>441</b>, or alternatively, electro-optically based sensing system <b>442</b>. This computer <b>485</b> also acts as input to display computer <b>455</b>.</p>
  <p num="p-0212">Finally it should be noted that another advantage of the surface distortion system is that the material can be anything sufficiently transparent that sufficiently deflects. If it is desired that the screen break apart, rather than fly off, swing down etc, when the airbag deploys against it, it can be slit or serrated, or for example screen <b>430</b> can be equipped with a weakened tear seam <b>491</b>. Other touch screen types may also be advantageously slit or serrated or otherwise induced to break or disintegrate as well, but many will not work properly in this mode due to disruption of acoustic or capacitive fields for example. Again a big advantage of the instant invention.</p>
  <p num="p-0213">This approach not only allows the airbag to deploy properly independent of the presence of the touch screen, but as well allows low cost of repair after a deployment. The screen surface isn't expensive, and is the only thing struck-less cost possibly than repairing dash upholstery.</p>
  <p num="p-0214">It is noted in <figref idrefs="DRAWINGS">FIG. 4</figref> that the image projector can be angled in the x z plane as opposed to the yz plane as shown, in order to miss the airbag deployment. Any touch screen sensing using cameras or whatever does not have to be in the same plane with the projection unit. In addition, where it makes sense, the airbag itself can be somewhat angled with respect to the touch screen as well.</p>
  <p num="p-0215">
    <figref idrefs="DRAWINGS">FIG. 5</figref> </p>
  <p num="p-0216">On a touch screen display, it is desirable to have tactile indication of where to touch, ideally so one would not have to take ones eyes off the road. <figref idrefs="DRAWINGS">FIG. 5</figref> illustrates an example using a screen wherein such a means for tactile relation comprises a local disturbance or relief of the screen surface.</p>
  <p num="p-0217">First let us consider a touch screen <b>500</b> which could be of any type, but is particularly easy to construct using the surface distortion type. The screen surface <b>501</b> is not flat in this case, but has transparent local protrusions such as <b>505</b>, or indentations such as <b>506</b> which make finding the zone near them on the screen possible by feel. Generally, it is preferable that the protrusions or indentations be small, on the order of 0.010 inches or less for example (with correspondingly small slopes if a smooth indentation), such that minimum discontinuity to the eye occurs when images are on the screen.</p>
  <p num="p-0218">For example, round indentation <b>506</b>, could be somewhat larger in diameter “d” than ones finger tip <b>507</b>, and be a position where z force would give the desired input—what ever it was. Thus if the driver switches screens to driver input functions, and his finger went over to touch the indentation in the approximate zone he knew, he would know he was touching heater speed, for example. If his finger went to the indentation on the other side <b>516</b>, he would know that was wiper speed, and so forth. He would not each time have to look at the screen.</p>
  <p num="p-0219">Similarly, if his finger found ridge <b>505</b>, which might be one of two ridges, the other being <b>515</b>, he would know from the displayed information “Heat” the bottom ridge <b>515</b> was for example heat temperature, and he would slide his finger along the ridge in the x direction accordingly to indicate how much heat was desired, for example. If he wanted wiper delay, he could slide along the upper ridge <b>505</b>. In either case, all functions once learned, (ideally possible by a very quick observation if one was driving a rental car for example)could be found by feel.</p>
  <p num="p-0220">With a distortion based screen, such indentations also serve another purpose, in that they cause more local distortion in the zone indented such as in <b>506</b> side view <figref idrefs="DRAWINGS">FIG. 5</figref> <i>b</i>, due to reduction in wall thickness, t, of the material of the screen (e.g. Plexiglas or Lexan). This is often useful to decrease the force required for a given resolution. A similar effect can be achieved as shown in <figref idrefs="DRAWINGS">FIG. 5</figref> <i>c </i>by substitution of material of the screen, for example 1f in screen <b>540</b> a zone <b>545</b> where contact was to be made (in response for example to displayed functions )could be of material <b>546</b> different than that of the rest of the screen <b>540</b>. In this case the screen could still be flat, and if both materials were equally transparent, likewise transparent. This is a particularly interesting thing, in that functions what ever they are could be memorized by the driver as always occurring in these spots on the screen, even though the screen could be used to display anything. And in some cases the totality of the screen at all points could be touched, even though the response would be different to forces at different points.</p>
  <p num="p-0221">Indeed the previous indented or raised screen version, if the indents did not overly optically change the information displayed, can serve the same purpose. This then leads to yet another alternative, discussed in reference <b>2</b>, that is to provide an overlay, on an otherwise uniform screen. This overlay can be of ridges, as just discussed for example, or can actually include various real knobs and switches, which themselves contact the screen. (previously disclosed in <figref idrefs="DRAWINGS">FIGS. 12 and 14</figref> of reference <b>2</b>).</p>
  <p num="p-0222">The idea of adding ridges or indentations to touch screens to improve their tactile function may be employed with other touch screens than my distortion based type, such as those of the resistive type comprising a matrix of elements which contact at a certain point touched when the force of touch presses the outer layer of contacts against the inner. But it may not be possible to easily provide such ridges and indentations on conventional touch screens of today employing capacitive or SAW technology. It would however, be possible for indirect sensing methods, for example the “touch screen” of <figref idrefs="DRAWINGS">FIG. 11</figref> in which ones finger image is sensed using cameras or other external means, independent of the screen itself.</p>
  <p num="p-0223">As shown in <figref idrefs="DRAWINGS">FIG. 5</figref> <i>d</i>, the action of the indented other material <b>546</b> for example, could “snap” to a new more indented location such that there would be a definite feel passed back to the person touching it. For example consider button <b>555</b> which is curved toward the user in its quiescent state, but when pushed in, snaps to position <b>560</b> (dotted lines).</p>
  <p num="p-0224">Also of use is a screen version having zones of more elastic behavior, either due to weaker wall thickness of the screen, or different material than other parts of the screen. This may also be in addition to the use of indented or raised portions of the screen to facilitate the driver finding the correct location for input action. Note that where desired the material may be of the substantially the same index of refraction as the base material but considerably softer, and more easily deflected. And it can be flush with the screen so that the total screen appears uniformly flat.</p>
  <p num="p-0225"> <figref idrefs="DRAWINGS">FIG. 5</figref> <i>e </i>illustrates a novel annular groove <b>565</b> in a translucent flat screen <b>570</b>, which is easy for a user to under stand by feel, placing his finger <b>571</b> in the groove. A raised portion in the groove, <b>572</b>, at the “9 O'clock position” indicates the start of the groove, and as one moves clockwise, the amount of the desired input increases (typically, it could be programmed otherwise). Optionally, small bumps (which for example, can also be coded as shown, two <b>575</b> at 12 O'clock, three <b>576</b> at 3 O'clock, and four <b>577</b> at 6 O'clock) can be used to further indicate to the user the location of his finger input by feel. As pointed out above, the immediate region of the groove can be color displayed or the like to better communicate visually to the user the result of his action. Again this groove need not be more than a few thousandths of inch deep in order to be felt by the user. If the sidewalls of the groove are aligned with a line from projector to the drivers eye, the effect of the groove on the displayed image will be minimal.</p>
  <p num="p-0226">The annular notch or groove is very “natural”. It has a neat feel, like the dial of an old style telephone in a way, and the analogy to a clock is known by everybody.</p>
  <p num="p-0227">Alternatively a circular or annular ridge sticking out from the screen can be made with some what the same effect.</p>
  <p num="p-0228">As noted, a desirable aspect of such ridges, grooves and the like, is that in linear form they can provide a “line of action”, so to speak, which the driver can trace his finger along, in the example from cold to hot, as one goes left to right. The ridge or groove can be under or on top, or surrounded by the display as desired.</p>
  <p num="p-0229">In <figref idrefs="DRAWINGS">FIG. 5</figref> <i>f</i>, a more sophisticated situation is illustrated wherein a plot of stock prices for the day has been displayed (in this case in the same region as the previous heat control, as a stock price selection has been made), and the driver with a quick glance can tactily slide his finger <b>590</b> along a horizontal tactile ridge or groove <b>591</b> to the point of the days trading (say 4 pm near the right of the chart) that he wants more information on. Such lines of action” can be vertical as well as horizontal, and more than one can be on a screen (e.g. <b>592</b> in the vertical direction). And as disclosed above, the line can be on a screen overlay which may be interchangeable, to accommodate different purposes.</p>
  <p num="p-0230">Finally it is of interest to consider in <figref idrefs="DRAWINGS">FIG. 1</figref> optional ridge <b>192</b> on LCD flat panel screen (or cover glass thereon, which as noted can be interchangable). This as noted is to provide a tactile relation between the users finger <b>190</b> and the screen and display. Of importance is that this is a very useful reference.</p>
  <p num="p-0231">However to execute this with many types of conventional touch screens is difficult. This is because one typically in the car dashboard case at least would rest ones finger on the tactile ridge, indentation or what ever. This would cause a permanent signal if a conductive grid type, and this may not be what is required. This can in one instance be solved by only looking for changes in location of the finger touch, and when change occurs in finger position, then begin tracking the finger to its final position—which final position then constitutes the signal desired. Alternatively, in some cases the path or other characteristic of the movement may constitute the answer.</p>
  <p num="p-0232">
    <figref idrefs="DRAWINGS">FIG. 6</figref> </p>
  <p num="p-0233"> <figref idrefs="DRAWINGS">FIG. 6</figref> illustrates a combined touch, gesture and voice activated system Which further illustrates the intimate connection of various functions of the invention, further including voice activation and response functions. The invention makes possible a multi-sensory dashboard data input/output system, including voice recognition and computer text to speech response, body part motion input (typically sensed optically, but not necessarily) and touch screen capability. Clearly it would be desirable to have such a device large, tactile, easy to read for all functions; and easy to add more functions—not achievable today with conventional dashboards.</p>
  <p num="p-0234"> <figref idrefs="DRAWINGS">FIG. 6</figref> <i>a </i>illustrates a touch screen display <b>620</b> in which two TV cameras <b>621</b> and <b>622</b> are located in the upper corners (or elsewhere) in order to observe a users finger <b>625</b>, for example in stereo to obtain 3D finger position if desired. In addition two microphones <b>628</b> and <b>629</b> are also located so as to be spaced apart by a baseline b, allowing stereo discrimination of a users voice location, as well as a voice command. The voice location is used to determine driver location, and optionally front seat passenger location for airbag deployment or other purposes.</p>
  <p num="p-0235">In a first version, the finger <b>625</b> touches the screen and registers a touch location in x and y by whatever means (conventional or as disclosed herein or in copending applications). The point touched by the finger, lets say the box <b>635</b> labeled “Heat” is detected and the coordinates of xy provided to the camera analysis module <b>640</b> by the touch screen determination module <b>645</b>. At this point, the camera system then knows the approximate location of the finger (or optionally part or whole hand) in a very localized region of space, and can immediately then look for its indication.</p>
  <p num="p-0236">For example, the finger can now be moved vertically up or down (or side to side), to indicate more or less heat depending on the finger position or movement. (heat having been the function selected with the touch screen).</p>
  <p num="p-0237">This is not the only indication of combined function. The user upon touch, can vocally speak the words “up” or “down”, with voice recognition module <b>655</b> discerning his command and executing same.</p>
  <p num="p-0238">It is also possible as another example, to do both, moving the finger up or down, and vocally indicating a command to the microphones and voice module <b>655</b>. In this manner, if one of the signals is faint, or suspect, the other can prevail. Or a co-incidence type of logic can be used where both the finger and the voice have to agree, to execute the function.</p>
  <p num="p-0239">Alternatively it would however be possible for finger location to be sensed directly by external means such as the cameras <b>621</b> and <b>622</b>, without any input from the touch screen sensing device.</p>
  <p num="p-0240">
    <figref idrefs="DRAWINGS">FIG. 7</figref> </p>
  <p num="p-0241"> <figref idrefs="DRAWINGS">FIG. 7</figref> illustrates a touch screen on the dash (either passenger or drivers side), whose control is further (or alternatively) equipped, as disclosed in co-pending application references, with an optional camera based head tracker and/or finger or hand tracker (which can include multiple cameras to improve accuracy and cover inputs from more than one occupant). As shown, in one example the camera <b>701</b> is located in the headliner over the windshield <b>705</b> of the vehicle, in this case positioned to look at the users face <b>710</b> (for face recognition security purposes for example), and/or to look at the users hands <b>711</b>, fingers <b>712</b>, etc. Alternatively or in conjunction, camera(s), such as <b>720</b> can be located in the dash <b>721</b> itself, which are also useful for augmentation of inputs of the touch screen by direct viewing of the fingers or hands of the user when near the screen.</p>
  <p num="p-0242">A stereo pair of cameras such as <b>701</b> (and another located into the plane of the paper and not shown for clarity), located in the roof can be used instead of a single camera. The camera or cameras can alternatively be located in the dash or elsewhere, and can also be used as taught in the copending references to provide depth information of points in their field of view. Indeed cameras can be located in the dash to view the occupants, or as shown in <figref idrefs="DRAWINGS">FIG. 7</figref>, can even be mounted in the touch screen itself, looking outward from corners thereof.</p>
  <p num="p-0243">For example, one mechanism is to use the upward movement of a persons hand, such as <b>711</b>, detected by camera <b>701</b> using image processor computer (eg a Cognex corp “insight” unit) <b>728</b>, to provide as signal to the display and screen input control computer <b>725</b> to change the displayed data on the screen <b>726</b> by projector <b>730</b>. One movement gesture of the hand (or just hand position) could be to change the screen from one type to another, while another movement could indicate which screen. Or as pointed out in co-pending cases, the positions or movements could themselves indicate the action to be taken. For example 1f a slider running left to right, corresponding cold, to hot of the heater was displayed, an movement of ones hand or finger near the slider from left to right could be used to provide an indication to the control computer to increase heat. In this case, a touch screen per se is not required, only a programmable screen. Thus this technique offers an alternative to use of specialized screens for touch input.</p>
  <p num="p-0244">Features of the face, such as the eyes can be used too. For example consider Camera <b>701</b> looking at the face of a person in the vehicle <b>710</b>. In one embodiment, the persons eyes found in the image for example using algorithms such as discussed in U.S. Pat. No. 5,859,921 by Suzuki. The persons eyes are separated a distance d which is determined using the camera computer system of the invention. As the face rotates, the distance d shrinks (foreshortens) as seen by the camera, and this reduction in d can be used to control either a function, like reducing the value of heat, or it can be used to switch consecutively for example for every incremental value of d, to an new screen choice or action choice within a screen (e.g. heat action, vs. the same slider representing fan speed action, say). The user could, for example, even choose in one embodiment via a pre switch selection, whether he wanted to control such things by voice, by hand gesture, or by face position/rotation.</p>
  <p num="p-0245">Indeed, the user can signal the system to change screens or some other function, just by the number of fingers he holds up, the distance between them or some other signal.</p>
  <p num="p-0246">As disclosed in referenced co-pending applications, by finding the eyes and nose, or eyes and chin, for example (e.g. 3 points), and using the locations so determined, one can arrive a complete single camera photogrammetric solution (if the original spacings are known, or learned by the system), such that x,y,z, roll pitch and yaw of the face relative to the camera can be determined. The person can thus nod up and down to make a selection as described above, using the camera to determine the variation in position of the plane of the face determined by 3 points such as the eyes and nose, or eyes and chin.</p>
  <p num="p-0247">Another method to change the screens from one display to the next, or to change the function of individual controls of a screen, is to provide in the dash (or alternatively on the windshield header or door header for example), a ranging sensor such as triangulation optical type <b>750</b> located in dash <b>721</b>, which looks outward in this case from the dash, and, as shown, measures the distance d, of a users hand <b>711</b> in front of the dash <b>721</b>. Sensors of this type are described in U.S. Pat. Nos. 5,362,970 5,880,459, 5,877,491, 5,734,172, 5,670,787 and others by the inventor and his colleagues.</p>
  <p num="p-0248">This sensor can be set to read that a particular hand position has been reached (as disclosed for example in U.S. Pat. No. 5,362,970), in which case the display for example can switch to a new screen—e.g. from highway screen <b>30</b> <i>b</i>, to traffic screen <b>30</b> <i>c</i>. Or in a more sophisticated mode of operation, the sensor can determine the actual distance to the users hand (or alternatively the change in distance plus or minus from some nominal mid point d<b>0</b>. For example, if the heater screen were touched, the distance delta d from d<b>0</b> in a positive direction could indicate the increase in heat from the nominal setting. There are clearly many variations on how such controls can be structured, but all have the advantage that they can operate in the car environment, and do not require one to deviate appreciably from important driving tasks. Its noted that with laser sources and solid state cameras (or PSD analog cameras) having large dynamic range, that operation in a car is assured.</p>
  <p num="p-0249">It should also be noted that cameras such as <b>720</b> in the dash, can be used with laser or LED sources, such as <b>750</b> in the triangulation mode, and at other times can be used for photogrammetry or recognition purposes.</p>
  <p num="p-0250">Other types of sensors can also be used to trigger screen changes, or other wise input data to the computer control systems of the invention. these can include, besides voice, other audio inputs (e.g. hand clap), proximity sensors and bulk mass sensors operating on inductive or capacitance principles, and the like.</p>
  <p num="p-0251">Sensors such as cameras or triangulation sensors such as can also be used as components of a “Smart” airbag control system to determine the characteristics and locations of occupants of the vehicle, as disclosed in reference <b>5</b>. It should be noted that by co-locating camera or cameras or other sensors with the touch screen operation and control “module” to be inserted into a dashboard, one can assure proper line up of camera viewing and touch screen display, using a factory calibrated module, without in situ registration requirements (such that a indication say of a finger pointing at a point on a screen, corresponds to the known projection of information at that point.</p>
  <p num="p-0252">It is noted the dashboard which may have a platform for a lap top computer to dock into under the passenger side of the dash, which can then allow the user to use the dashboard screen while driving. The use of hand or fingers or objects held in the hand to operate such a laptop and screen combination has been disclosed in the co-pending references. Clearly one could use normal mice, keyboards and the like which could plug in the laptop, or directly into a dash receptacle. Alternatively, or in addition, voice commands can also be used.</p>
  <p num="p-0253">
    <figref idrefs="DRAWINGS">FIG. 8</figref> </p>
  <p num="p-0254">Shown in <figref idrefs="DRAWINGS">FIG. 8</figref> are user assisted displays used to control machine vision systems associated with TV camera in puts of conditions external to the vehicle. Two embodiments are shown, a stereo vision based system capable of determining range to an object in the z direction, as well as its xy location, and a driver designated feature search, using a screen of the invention.</p>
  <p num="p-0255">As shown TV color camera pair <b>801</b> and <b>802</b> are positioned in close proximity to the windshield <b>805</b> with a baseline B between them to provide range information as to range R of objects in front of the vehicle when images from each camera are matched in computer <b>810</b> using known stereo photogrammetric triangulation principles. (see for example, A paper by Dr. H. F. L. Pinckney entitled Theory and Development of an on line 30 Hz video photogrammetry system for real-time 3 dimensional control presented at the Symposium of Commission V Photogrammetry for Industry, Stockholm, August 1978, together with many of the references referred to therein gives many of the underlying equations of solution of photogrammetry particularly with a single camera. Another reference relating to use of two or more cameras, is Development of Stereo Vision for Industrial Inspection, Dr. S. F. El-Hakim, Proceedings of the Instrument Society of America (ISA) Symposium, Calgary Alta, Apr. 3–5 1989. This paper too has several useful references to the photogrammetry art.).</p>
  <p num="p-0256">The color (e.g. red) and shape (e.g. round) of an object such as the traffic light <b>808</b> can be used to help discrimination against background in the scene, as can their location (either above or to the side of the road—which can be inputted to the system automatically given a GPS signal to locate what jurisdiction the car is in (such jurisdiction having known traffic light positions).</p>
  <p num="p-0257">In one embodiment, <figref idrefs="DRAWINGS">FIG. 8</figref> <i>a</i>, the computer <b>810</b> is programmed to simply look for red or yellow traffic light signal images, such as <b>815</b> and <b>816</b> in the lower portion of respective image fields <b>820</b> and <b>821</b> (due to lens inversion), and provide to the operator of the vehicle a signal as to their presence and distance. Optionally, such signals can be used to actuate the brakes or other functions as well. If only presence is needed, then a single camera data can give this information.</p>
  <p num="p-0258">In an alternate and similar embodiment, the signal looked for is brake lights of vehicles ahead, or tail lights. It is noted that a single camera can be used to roughly estimate range to taillights since most are on relatively similar base line widths (almost car width, and are this baseline is nearly horizontal at all times). Similarly oncoming headlights can be detected and range determined if desired in this manner.</p>
  <p num="p-0259">In another embodiment of the invention, the user himself, can designate things of interest for the camera system to sense and help keep him aware of. For example as shown in <figref idrefs="DRAWINGS">FIG. 8</figref> <i>b</i>, consider large touch screen display on the dashboard <b>840</b> on which a TV camera image of the zone ahead of the car is displayed using one of the screen options available. The driver, with his finger <b>845</b> can touch the screen to designate a region of interest, and the computer camera analysis system can consider criteria in this region. For example, the user could point out a car ahead, an intersection with traffic lights, a shoulder of the road such as <b>855</b>, a line marker in the middle of the road, railway crossings such as <b>852</b>, and so forth. Known machine vision programs such as Vision Bloks by Integral vision corporation can be used to process the TV image data to continue to monitor the position of a crossing gate <b>850</b> in crossing and to warn if it begins to go down for example. In this manner Machine vision can be of assist to the driver, without the necessity of processing complex scenes. It is particularly true if the driver can say intersection, or light, and point to the approximate region of the image to monitor.</p>
  <p num="p-0260">In an auxiliary implementation, the car can further include one or more infrared light projectors to illuminate retro-reflectors (also generally in known locations) of vehicles and highway signs ahead or to the sides of the vehicle. In this case ir is used to keep from disturbing other drivers, and near IR (e.g. 0.9 microns wavelength) can still be detected by conventional cameras.</p>
  <p num="p-0261">It is noted that the driver could alternatively ask the system via microphone <b>860</b> using voice recognition to look for a certain condition, such as “stoplight”. But voice recognition is not as effective to determine where to look in the image field.</p>
  <p num="p-0262">In this example it is often desirable to find the traffic light which is lit, so to speak (e.g. red green or yellow), since the lit one is the most distinguishable. A problem exists that if nothing is seen, it still doesn't mean nothing is lit. (not fail safe). Thus operator assist by designation is useful, to define a region that surely has something lit, such that the sensitivity can be ramped up until it is detected)</p>
  <p num="p-0263">
    <figref idrefs="DRAWINGS">FIG. 9</figref> </p>
  <p num="p-0264"> <figref idrefs="DRAWINGS">FIG. 9</figref> illustrates an embodiment building on that of <figref idrefs="DRAWINGS">FIG. 4</figref>, in which the steering wheel <b>901</b> includes in its center portion a display and touch device, such as touch screen <b>910</b>, also with airbag <b>915</b> behind it. As in <figref idrefs="DRAWINGS">FIG. 4</figref>, the image projector <b>920</b> and sensing means <b>925</b> for the touch screen is preferably off the axis of the airbag deployment. Both are included, with the airbag <b>915</b> in housing <b>930</b>.</p>
  <p num="p-0265">In this manifestation, the housing for the sensing means and projector becomes larger in diameter than currently in today's steering columns, however Since the touch screen in the wheel can allow virtually all essential controls and functions to be viewed or actuated, this eliminates the need for many of the switches and levers on the column needed today for windshield wipers, cruise control, transmission and the like, which also pose a safety hazard, as do any switches which can be impacted by passengers. In this example, there are no protruding devices of any kind required.</p>
  <p num="p-0266">It is not necessary to use a rear projection based touch screen in the steering wheel, but given current laws, an airbag is virtually required in the steering wheel. And too a projection device can be fixed in position in some cases, while the wheel moves. While this avoids wrapping of wires, it does mean the display would not look right on a rotated wheel, without computer compensation of the projection to account for same.</p>
  <p num="p-0267">Alternatively, or in addition, A camera overhead or to the side such as shown in <figref idrefs="DRAWINGS">FIG. 23</figref> as well as referenced co-pending applications also can be used to input commands from human positions, or small switches knobs or other tactile selection and adjustment means and controls actuated by the human which are on the steering wheel. For example, in this case if the controls are done this way, the touch screen <b>910</b>, need only be a display screen without the touch capability, since control actuation by the user can be done by use of the cameras as shown, or by conventional devices.</p>
  <p num="p-0268">
    <figref idrefs="DRAWINGS">FIG. 10</figref> </p>
  <p num="p-0269"> <figref idrefs="DRAWINGS">FIG. 10</figref> illustrates an embodiment of the invention where a Screen display moves around a touch point using a vector of touch, rather than where the touched point is moved by the person touching to different points on a fixed display. This is radically different than any known touch screen operation I am aware of, and is achieved herein by having a display responsive to the vector of touch, which allows an urging of the touched point in a chosen direction to command the display on the screen.</p>
  <p num="p-0270">It should be noted that in <figref idrefs="DRAWINGS">FIG. 14</figref>, the finger <b>1410</b> by pressing in on the pliable screen outer surface, undergoes resistance which opposes movement from side to side, but does not inhibit it. This resistance can be helpful in tactilely determining a direction of movement. For example as ones finger touches the screen at a point “P”, there is felt in the finger is a resistance when one wishes to push to one side or another. As soon as one begins to push, say to the right, the display begins to move accordingly. But the finger itself need not be moved across the screen, as the force of the finger against the resistance acts as gas pedal so to speak to cause the display to move.</p>
  <p num="p-0271">For example consider <figref idrefs="DRAWINGS">FIG. 10</figref> <i>a </i>where a screen <b>1000</b> is touched at point “P” by finger <b>1010</b>. The display is controlled to put a slider right on the finger point indicative of its position along a sliding control path. At a future time, after pushing with ones finger a given amount, the display looks like that of <figref idrefs="DRAWINGS">FIG. 10</figref> <i>b</i>. Note that the finger is in the same xy location on the screen, but the display has moved, and is now indicative of an increase in the position along the path, and a corresponding increase in a variable, in this case heater blower speed. The previous position of the heater control slider is shown in dotted lines.</p>
  <p num="p-0272">This same approach can be followed not only by conventional knobs and the like, but also by more complex items, such as displayed seat icons shaped like seats (such as typically found in Oldsmobile aurora and Mercedes vehicles). These icons can be pushed in any number of different directions, not just x and y related.</p>
  <p num="p-0273">Position and orientation of the image can thus be in proportion to the absolute value of a control variable known—which modifies what the image looks like as it jumps to a logical position at the point of touch.</p>
  <p num="p-0274">By having a display and associated control function which moves around the point of an arbitrary touch, this embodiment requires much less visual concentration than conventional touch screens requiring the touch to track a predetermined fixed display.</p>
  <p num="p-0275">It is noted that the operation shown in <figref idrefs="DRAWINGS">FIG. 10</figref> a and b can also be achieved using more conventional touch screen technology, but simply causing the movement of the display shown between condition a and b to occur by leaving ones finger on the display for a length of time, said time related to the rate of display movement and the rate of change of the control settings.</p>
  <p num="p-0276">However in this case, the direction of change is not clear, just by touching the screen at a point. Thus some other indication has to be given (which the touch vector version provides). This direction indication can come from moving slightly in the direction desired, and then leaving ones finger sit. Or it can be provided by voice input. Or some other method.</p>
  <p num="p-0277">
    <figref idrefs="DRAWINGS">FIG. 11</figref> </p>
  <p num="p-0278"> <figref idrefs="DRAWINGS">FIG. 11</figref> is an alternate embodiment of the invention using the image of a users body portion or object, using light from a projection source, or from inside the car to back illuminate said portion.</p>
  <p num="p-0279">For example, consider, finger <b>1100</b> of a user touches screen <b>1102</b> which is sufficiently transparent (though still able to scatter light projected on its surface). TV Camera <b>1105</b>, located behind the screen and out of the way of the displayed image projector device <b>1110</b>, is used to view the screen <b>1102</b> from the rear. The Camera image is processed in system computer <b>1107</b>.</p>
  <p num="p-0280">In a first case, the camera sees the darker image of the finger <b>1100</b> touching the screen, when back illuminated by light <b>1111</b> from the users side (in this case light inside the car. This works well during daylight and in relatively well defined situations, but poorly at night unless auxiliary lighting is provided inside the vehicle (e.g. From IR LED's which can be placed for example in the roof of the vehicle behind the front seats, which are not disturbing as they are invisible). The camera is ideally used to see the finger touch location in x and y screen coordinates when the projector source is dark eliminating background noise from the screen. The projector can in some cases be switched off or its displayed image darkened only momentarily, to avoid the impression of it being off, which can disturb the user.</p>
  <p num="p-0281">As an alternative solution to the night illumination problem, a reverse situation can be used, and is generally preferred. Here, in one embodiment, the finger tip <b>1101</b> is front illuminated through the screen instead using light <b>1115</b> from the projector <b>1110</b> (or optionally by a separate source such as IR LED's <b>1116</b> (dotted lines) located behind the screen, whose light, like that of the projector, passes through the screen from the rear. The reflection <b>1120</b> from the finger, is sensed through the screen by the camera <b>1105</b> as shown.</p>
  <p num="p-0282">Given the screen scattering properties, generally required to make the projected display image visible to users in variant positions, the user's finger (or fingers) need be close or touching the screen for best results. (when touching it may act to frustrate the scattering effect, and work even better).</p>
  <p num="p-0283">For best results in the presence of strong projection images, anti reflection coating <b>1121</b> is applied to the back of the screen to prevent backscatter reaching the camera from the projection source, and the camera is purposely located off the angle of direct reflection off the screen from the source as well. If a separate quasi monochromatic source such as IR LED <b>1116</b> is used, a band pass filter at that wavelength such as <b>1126</b> (dotted lines) can be placed in front of the TV camera <b>1105</b> in order to pass largely light from the special source, and not from the projector (which could also include a band blocking filter at that wavelength-desirable in most cases anyway to limit heat reaching the screen.</p>
  <p num="p-0284">It is noted that if the screen is to the right of the driver, as it would be if located to the right of the steering wheel on the dash or center stack, the finger will generally approach the screen at an angle as shown—this can make it easier to discern the point of finger contact as a longer stretch of the finger edges can be used to perform the calculation from the TV image.</p>
  <p num="p-0285">The screen if desired, could have raised or indented portions, as described in <figref idrefs="DRAWINGS">FIG. 5</figref>, such as indent <b>1130</b> where the users finger would touch.</p>
  <p num="p-0286">The image of finger tip <b>1101</b> can be further distinguished by other methods. For example the camera <b>1105</b> is typically a color camera, and if the light projected by projector is white in nature, the color of the imaged light from the finger tip will be flesh colored, and only flesh colored images can be looked for by image analysis software in computer <b>1107</b>. Since everyone's flesh may be different in color, one can teach the camera computer system during a setup phase, by simply putting ones finger on a square or squares on which white (or another color) is projected. It is thus desired to match the return from one or more projected colors with an indication of the persons finger.</p>
  <p num="p-0287">Another means of distinguishing ones finger is by image sharpness. Since the surface of the screen <b>1102</b> is typically scattering, it is clear that objects that aren't in direct contact with the screen will be seen less clearly—and in fact become undiscern—able if the finger or other object is too far from the screen in the direction away from the camera. Thus a criteria for determining finger presence on the screen is both sharpness and color, as well as degree of light return and size (most finger touches being within a size range on the order of 10 millimeters on the screen).</p>
  <p num="p-0288">To avoid having the camera system exposed to images which don't represent a touch, it is possible to sense independently that a touch condition has occurred, for example with piezoelectric transducers such as <b>1180</b>, and <b>1181</b> whose information is processed by computer <b>1107</b> and then use this signal to cause the camera computer to analyze images on the screen. When a force or other touch condition is detected the system is programmed to look, minimizing the chance of false signals due to unusual lighting conditions.</p>
  <p num="p-0289">Another variant is to see the deflection of the screen or other indicator of finger location, and using this knowledge, then localize the search for the finger image. Conversely, one can get a rough finger image, and then localize the location of search thru other means, which could be acoustic, optical or whatever.</p>
  <p num="p-0290"> <figref idrefs="DRAWINGS">FIG. 11</figref> <i>b </i>illustrates an annular groove <b>1155</b> in screen <b>1102</b> which can be traced around with finger tip <b>1101</b> in order to guide finger tip to the desired clock wise position indicative of a variable or quantity desired. The finger image, is discerned best when the finger is fully in the groove, as it would be when one was “feeling” for same and guiding ones finger around it, in a fashion similar to a old style telephone dial, or knob.</p>
  <p num="p-0291"> <figref idrefs="DRAWINGS">FIG. 11</figref> <i>c </i>illustrates a thresholded finger image <b>1190</b> as seen by camera <b>1105</b> when the finger is in direct contact with a outer scattering screen surface, while finger image <b>1191</b> is that seen with a finger 3 mm away from the scattering surface. In each case the images are thresholded to the same intensity value.</p>
  <p num="p-0292">Note too that the edge contrast as illustrated in the intensity profile <b>1195</b> of image <b>1190</b> is much higher than the corresponding profile <b>1196</b> of <b>1191</b>, which contrast can also be used, besides size at a given threshold intensity value, to discriminate the two conditions.</p>
  <p num="p-0293">While the discussion above has been concerned with finger or other images illuminated with light in the visible or near IR wavelengths, alternatively, the Self generated radiation of the body can be used, detected with IR TV (eg pyroelectric) cameras <b>1105</b> operating in longer wavelength regions.</p>
  <p num="p-0294">
    <figref idrefs="DRAWINGS">FIG. 12</figref> </p>
  <p num="p-0295"> <figref idrefs="DRAWINGS">FIG. 12</figref> is an alternate embodiment of the invention using a SAW type touch screen <b>1200</b>, such as a Mass Multimedia/Elo brand, model M14-SAW employed with an LCD display <b>1201</b>. (note it can also use the micro-display projection unit of <figref idrefs="DRAWINGS">FIG. 2</figref>). And the display screen may be equipped with eschelle prisms, rectilinear screens, or other optics <b>1210</b>, to direct the light preferentially, for example toward the driver or in another direction as desired.</p>
  <p num="p-0296"> <figref idrefs="DRAWINGS">FIG. 12</figref> <i>b </i>illustrates another form of ultrasonic based touch screen, in which an acoustic imaging transducer array <b>1230</b> is located behind an LCD display screen <b>1235</b> having glass cover <b>1236</b> is capable of detecting the location of a finger touch <b>1240</b> on the front of the screen, <b>1236</b>. The array <b>1230</b> is composed of 100 individual acoustic transducers (individually not shown for clarity,) which are read by signal generator and receiver electronics module <b>1245</b> switched to each position in succession by multiplexor <b>1250</b>. Switching time for 5 Mhz signals from each transducer is about 0.1 seconds, but can be less.</p>
  <p num="p-0297">This embodiment can produce an acoustic image of the surface of the screen, which as disclosed above can have ridges on it, or other tactile relations as desired. The finger position and degree of indentation if desired of the glass plate can be determined by computer module <b>1260</b> which processes the time of flight of the ultrasonic wave and looks for changes in the norm. Even though the signals from the various layers of the LCD screen are complex, the change in reflected signal due to the finger touch on glass <b>1236</b> is clearly discernable, and pinpoints location in xy and distance z. For example in this case shown in <figref idrefs="DRAWINGS">FIG. 12</figref> c the signal back from transducer A and transducer B is different, with transducer B being stronger, indicating that the finger touch <b>1240</b> is closer to B than A. These signals can be calibrated to give a lookup table which allows one to compute the xy location of touch in the computer <b>1260</b>. Typical beam spreads of the acoustic send and receive modules are 20 degrees.</p>
  <p num="p-0298">It should be noted that the xy location is not just limited to resolution of 10×10 where 100 transducers are used, since interpolation between signals of adjacent transducers can be used to isolate the position to at least 4× better in x and y. This gives a 40×40 matrix, sufficient for many touch screen applications in cars.</p>
  <p num="p-0299">Ultrasound can also be used in air in such a manner to monitor a screen such as in <figref idrefs="DRAWINGS">FIG. 2</figref>, illuminated by a projection display for example.</p>
  <p num="p-0300">
    <figref idrefs="DRAWINGS">FIG. 13</figref> </p>
  <p num="p-0301"> <figref idrefs="DRAWINGS">FIG. 13</figref> illustrates a related embodiment of the invention in which a simple LCD display <b>1300</b> is located in the armrest portion <b>1305</b> between the two front seats (or optionally between two rear seats). This display may optionally be equipped with touch screen capability.</p>
  <p num="p-0302">The display is positioned to be easy for the driver <b>1328</b> to use with his right (left in the UK and Japan) hand, and can be seen by him by looking down and to the right. The touch screen or other display may optionally be rotate-able so that the passenger can use it as well. When the display is rotated, the display may be varied to meet applicable safety laws, for example by not presenting graphics or perhaps internet surfing information to the driver while moving.</p>
  <p num="p-0303">In a preferred example of an embodiment with many desirable advantages, TV camera <b>1320</b> in the roof, for example obtains an image of the touched screen and the finger <b>1310</b> touching it (in essence creating a touch screen, by noting the position in the camera computer <b>1321</b> of the display as well). This image <b>1330</b> is then displayed for the driver on the windshield <b>1340</b> of the vehicle in or near his normal line of sight, using a heads up display (HUD) employing holographic or other lens technology known in the art. This provides the driver a very desirable display of what is happening, so to speak on the display/touch screen, along with his vision of the road. Its noted that this approach works wherever the display, touch screen or pad exists—as long as the camera can obtain its image.</p>
  <p num="p-0304">For those touch screens or pads capable of outputting the locations of the human finger(s), the display can create these without the camera, by using the image drive circuitry of the touch screen to drive the heads up display as well.</p>
  <p num="p-0305">For example <figref idrefs="DRAWINGS">FIG. 13B</figref> illustrates what a direct image taken with an external camera such as <b>1320</b> would look like when projected. The person is touching point a virtual slider <b>1360</b> for heat control in this example, which is moved by the driver to position B, from an initial position A. This is something like one sees with ones eyes, when one works a touch screen type display normally—i.e., you see your hand and finger near or at the screen. In either case you can get a much better feel for what you have to do to make a reading register successfully, by watching your finger approach the point in question, and watching the display say change color (if programmed to do so) when you correctly touch it. Alternatively, in other embodiments herein an acoustic signal, or force pulse to your finger can also or alternatively be used to indicate correct contact so to speak. This issue though, is how do you approach the point, with only a glance, at the screen, and without taking your eyes off the road for anything other than an instant.)</p>
  <p num="p-0306">This concept is different than conventional touch screens which do not have an auxiliary display (in this case via an HUD, or “heads up” display) displaying not only the results of the touch in a box on a screen, but to actually display the position of ones finger on the screen with the screen information. This is to allow one to “See what one is doing” without taking ones eyes off the road—a new concept in its own right, which could be extrapolated to other tasks in the car—or in other applications such as the home or business as well. Indeed the auxiliary display, could be the only display. The touch screen itself can be of the standard type such as in <figref idrefs="DRAWINGS">FIG. 12</figref>, or preferably with one or more tactile features as in the invention herein disclosed.</p>
  <p num="p-0307">It is noted that because of the unique properties of the heads up display of such touch screen inputs, that one can maintain a conventional instrument cluster ahead of the driver such as in <figref idrefs="DRAWINGS">FIG. 1</figref> if desired while still having all the advantages of the touch screens of <figref idrefs="DRAWINGS">FIG. 1</figref> and more besides. Data entered from the touch screen can be used to override conventional controls if any which are still present. Or priority can be given to the conventional controls.</p>
  <p num="p-0308">Note that to make it easier for the driver, and use up less space on the windshield, one might only display portions of his finger and the slider, not the whole screen image acquired by camera <b>1320</b>.</p>
  <p num="p-0309">
    <figref idrefs="DRAWINGS">FIG. 14</figref> </p>
  <p num="p-0310"> <figref idrefs="DRAWINGS">FIG. 14</figref> illustrates another embodiment of my distortion touch screen invention As discussed above A TV or computer display image projector such as a Sharp brand LCD based projector <b>1401</b> can be used to project a desired display image on the screen <b>1403</b>. In this embodiment, distortion of the screen material <b>1403</b> (for example a plastic sheet with scattering properties on its front surface <b>1405</b>) occurs primarily by virtue of the compressible supporting material <b>1406</b> that allows the relatively thin sheet member <b>1403</b> to locally distort or otherwise be changed under action of a touch, for example by finger <b>1410</b>,</p>
  <p num="p-0311">As shown, the screen member <b>1403</b> is separated from rigid Plexiglas backing member <b>1415</b> by refractive medium <b>1406</b> of thickness t, which is compressed or displaced by the force of finger <b>1410</b>. Alternatively, the medium <b>1406</b> can be a liquid such as oil, glycol, or water, that is compressed or displaced by the finger push(raising the surface elsewhere).</p>
  <p num="p-0312">Light <b>1430</b> from light source <b>1431</b> from the rear passes through backing member <b>1415</b> and is reflected off the back surface <b>1404</b> of screen member <b>1405</b>. Desirably, the index of refraction of member <b>1415</b> and material <b>1406</b> are closely matched such that little refraction occurs, Ideally anti-reflection coatings eliminate most of the reflections from the various surfaces other than that from the back surface <b>1404</b> of member <b>1403</b> desired.</p>
  <p num="p-0313">Light reflected from surface <b>1404</b> reflects back toward camera <b>1435</b> (after retro reflection by expansive retro reflector <b>1440</b> made of scotch light <b>7615</b> glass bead material) for example effectively used to determine the distortion of <b>1403</b> due to the retro reflective based “D sight” image effect or another optical phenomena such as disclosed in copending reference <b>1</b> and <b>2</b>.</p>
  <p num="p-0314">Its noted that the Dsight system operates independently of the projector, and is not affected by its operation. If desired, laser sources instead of a conventional light source <b>1431</b> can be used, with appropriate filters on the TV camera to assure function. This holds true of all optical methods such as grid projection that might alternatively be used to determine the change in screen <b>1403</b>.</p>
  <p num="p-0315">It is also noted that in another embodiment, the material of <b>1403</b> may itself be in contact with the rigid (relatively at least) member <b>1415</b>. In this case the material of <b>1403</b> should be compressible, and closely matched where possible in its index of refraction to that of <b>1415</b>. In this case the reflection comes from the front surface of <b>1403</b> facing the user. The refractive medium <b>1406</b> in this case doesn't exist, or can be considered of t=zero.</p>
  <p num="p-0316">Also illustrated in <figref idrefs="DRAWINGS">FIG. 14</figref> is the use of a force feedback signal to the users finger in which an acoustic wave in the screen signals to the users finger that an event has occurred It can be pulsed, or of static or varying repetition rate or frequency, or of varying amplitude and is an alternative to providing the user a beep tone for example to indicate that an action has registered.</p>
  <p num="p-0317">As the user finger <b>1410</b> presses on the screen <b>1403</b>, a strong acoustic wave <b>1465</b> is generated by transducer <b>1470</b> in material <b>1406</b>. The transducer input is triggered by the sensor of what ever type is used in determining the screen has been touched (in the case here, it is signaled by computer <b>1446</b> analyzing the image from TV camera <b>1435</b> which determines a touch at a given screen coordinate has occurred(and its magnitude if desired. This then is compared to the screen projection data, to determine what the touch has registered, if anything. When some sort of registration data is desired, The acoustic wave is generated and impacts finger <b>1410</b> which then feels the event. The degree of touching in the z or force direction can be used to control the amplitude or the frequency of the acoustic wave <b>1465</b> and thus signal to the user the degree of z actuation.</p>
  <p num="p-0318">It should be noted that the finger <b>1410</b> by pressing in on the pliable screen outer surface <b>1405</b>, undergoes resistance from sidewalls <b>1411</b> and <b>1412</b> for example of the indent, which opposes movement from side to side (and similarly, up and down), but does not inhibit it. This resistance can be helpful in tactilely determining a direction of movement. It can also be actually increased or enhanced in some manner by using an acoustic source to provide a pulse or other signal to the finger in proportion to movement or direction, generated in the medium <b>1406</b> by acoustic wave sources such as <b>1470</b>.</p>
  <p num="p-0319">A useful display with such as screen is when the display is caused to “jump” to the point touched, using that point as a new reference for the displayed data (eg a heat control bar). This is discussed relative to figures below</p>
  <p num="p-0320">
    <figref idrefs="DRAWINGS">FIG. 15</figref> </p>
  <p num="p-0321">The TV camera based rear projection touch screens and other user signal detecting aspects of the invention allow one to use a laser pointer indication on the screen as well for control, entertainment or other purposes. <figref idrefs="DRAWINGS">FIG. 15</figref> illustrates a method according to the invention using a laser pointer <b>1501</b> which is aimed by user <b>1502</b> at the touch screen <b>1510</b> on the cars dashboard <b>1518</b> producing spot <b>1515</b> or other indication. This signal on the backside is visible to TV camera <b>1520</b> which may be desirably equipped with a band pass interference filter known in the art centered on the laser pointer wavelength (e.g. 670 nm), <b>1521</b>, to increase signal to noise. This can be easily combined with the same camera sensing touch distortion, if the light illuminating the screen for this purpose <b>1530</b> is also from a laser or other source of the same wavelength (assuming the filter is used). Use of lasers helps in high sunlight environments in the car, but is not necessary for operation in general.</p>
  <p num="p-0322">Computer <b>1550</b> processes the image from camera <b>1520</b> to determine the x,y location of the spot <b>1515</b> on the screen and any other data such the spot shape or coded signature, etc, as disclosed in my copending application Ser. No. 09/568,554 incorporated by reference</p>
  <p num="p-0323">
    <figref idrefs="DRAWINGS">FIG. 16</figref> </p>
  <p num="p-0324"> <figref idrefs="DRAWINGS">FIG. 16</figref> illustrates further camera based embodiments of the invention, several of which are discussed as well in other forms in the copending referenced applications above.</p>
  <p num="p-0325">For example, these applications and previously granted patents by the inventor such as U.S. Pat. No. 5,982,352 have discussed the use of cameras either single, or stereo pairs and the like to view persons to determine positions and motions of their extremities or objects held by the person. When data is determined from such activity to be of concern for example, alarms can be provided (if out of position occupants are determined say), or to signal Onstar (GMs name for a navigational and help aid).</p>
  <p num="p-0326">As pointed out previously, such camera inputs (eg from camera <b>701</b>) as to rotational position of a head such as <b>710</b>, can cue the appropriate screen display to moves as head moves—not just for viewpoint changes, but to help the person view the display (for example 1f one is looking a bit to the side, the critical data on the instrument cluster ahead of the driver can be shifted slightly sideways.) In addition, as pointed out before, if one moves one head, this could cause the display to change function, or if rear view images were presented on the display (from rear facing cameras for example), the display <b>1600</b> could change views of the cameras.</p>
  <p num="p-0327">For example, with cameras having large numbers of camera pixels at low cost, a single camera with for example 2000×2000 pixels such as <b>1601</b> looking rearward from above the rear window <b>1602</b> as shown, may be above to cover with a suitable wide angle lens <b>1605</b>, the total angular field of interest “K” behind the vehicle <b>1610</b>. However what is displayed on a screen on the dash <b>1625</b>, for example in zone <b>1630</b> allotted for the rear view camera image, might be only a fraction of the field (ie only a fraction of the pixels displayed at once corresponding to an angular region less than K)—and this swept, by simply turning ones head (just as one does when one looks in the rear view mirror).</p>
  <p num="p-0328">To aid camera viewing of the human, it is often desirable to have retro reflective or other discernable marks rather than arbitrary human features. For example, glove <b>1640</b> with targeted fingertip <b>1645</b> and thumb <b>1646</b>.</p>
  <p num="p-0329">As taught in the copending applications referenced, similar cameras can also be used to inexpensively see objects which the human moves for signaling purposes or to cue the control system, for example as to like switch positions, or heater vent locations, or stalk positions etc. For example one could have interchangeable dash overlays, not only for certain screen functions or markings, but also sliders, raised or other wise which could be optically sensed by TV cameras, for example with the camera <b>701</b> of <figref idrefs="DRAWINGS">FIG. 7</figref> located in the headliner.</p>
  <p num="p-0330">It should be noted that where features on the human can be discerned by the computer and camera system, that such features themselves can provide the input. This too can include the fingers and hands, etc. Such features can also be watched for safety purposes, to assure a door isn't slammed on a child's hand for example.</p>
  <p num="p-0331">Also pointed out in copending references, objects in the car can also be viewed by cameras, or themselves have cameras, and communicate with other things in the car like fax machines, phones and the like.</p>
  <p num="p-0332">The system can aid recognition of the drivers face, hands, eyes etc. for theft security or other purposes. The art is full of such things these days (See for example Suzuki U.S. Pat. No. 5,859,921 Apparatus for processing an image of a face). Such devices can also observe the state of the driver, to determine driver attentiveness and ability to drive. Note too however, that such may be further augmented using the invention, by having a keyboard for example screen presentation, in which certain commands have to be typed in order to start the car—these could assist in determining the drivers state or authenticity (via codes typed in).</p>
  <p num="p-0333">A camera of the system can also recognize shape of face, and particularly the pointing direction of the eyes, in order to rotate a presented view on the screen, for example a rear view obtained with exterior mounted cameras (see for example U.S. Pat. No. 5,949,331 by Schofield, et al.), or a night vision view out the front using IR cameras or scanning microwave equipment. Indeed such views, when sensed by the car display and actuation system of the invention to be needed or called on by the driver input can be presented on the display <b>1405</b> of dash along with the minimum amount of normal data (speed etc) needed, for example as expressed by a digital speed indication.</p>
  <p num="p-0334">In one embodiment, discussed above, when the user is seen by a camera such as <b>701</b> and associated image processing computer to be looking in a rearward direction, for example by turning ones head slightly, the dash instrumentation may be programmed to immediately convert to a mode where the rear display is presented as shown on the dash.</p>
  <p num="p-0335">
    <figref idrefs="DRAWINGS">FIG. 17</figref> </p>
  <p num="p-0336">The disclosed camera based object position sensing and control system of the co-pending applications incorporated by reference can be used to turn any part of a car into a control mechanism without wires or levers etc. Detected can be common parts of the car in any position where a conveniently mounted camera(s) can get a reliable view—vent positions, turn signal indicators, shift levers, knobs, buttons, ashtrays foot pedals, etc. Isn't just the point you actuate. Its also a point you move to a new position. For example 1f you move a vent to a new angular position. Knobs, window and lock switches, etc. seat positions, all can be camera sensed. Thus such controls can be inexpensive, as there is nothing that need connect to them, as all transduction is non contact—and there is an economy of scale of multiple things in field of view of the camera(s).</p>
  <p num="p-0337">For example. Consider overlay <b>1700</b> on flat panel display screen <b>1708</b>. The overlay contains knob <b>1710</b> with pointer <b>1715</b> As shown in <figref idrefs="DRAWINGS">FIGS. 1 and 2</figref>, such an overlay on the screen can be read from the rear or projection side (if a projection display is used). But alternatively an overlay can be constructed to have a pointer or other marker on the front of the knob as shown, for observation in a front viewing mode.</p>
  <p num="p-0338">For operation in a front viewing mode as shown, the persons hand (not shown for clairity) moving the knob, slider or other control can get in the way of the camera ,<b>1720</b> used to see the knob and its pointer location at least for some camera and person positions. For this reason it is desirable that a certainly latency be acceptable. For example, if one moves the heater slider to a new position hotter than the last, it is generally alright to have the sensing of the new slider position be performed after the hand is retracted (and thus not obscuring the view). This can be sensed by the camera <b>1720</b> and its computer system <b>1740</b>, as a situation where a standard view or “image signature” of the slider <b>1750</b> (or other object to be sensed) is present and not blocked.</p>
  <p num="p-0339">For example 1f in the field of view of the camera, the slider mark <b>1765</b> occupied an area of 12 dark pixels in the processed camera image <b>1760</b> surrounded by bright ones (which could be the case if it was a dark mark on an overlay on a backlit touch screen), and such a situation wasn't seen, then no action would be taken until the situation once again appeared. At that time the new position of the centroid say, of the 12 pixels would be compared either to the absolute position of the dash, or to the previous position of the slider, and appropriate action taken. (this same logic also works for rear viewing as in other embodiments above, where the problem is not necessarily obscuration, but noise from projected image variations in an instant image situation).</p>
  <p num="p-0340">The processed image of the knob pointer <b>1770</b> is shown as well.</p>
  <p num="p-0341">Most controls in a car today fit this scenario. Only some things like adjusting something in real time to “get it right” with no other cues would require constant monitoring of control position.</p>
  <p num="p-0342">
    <figref idrefs="DRAWINGS">FIG. 18</figref> </p>
  <p num="p-0343"> <figref idrefs="DRAWINGS">FIG. 18</figref> illustrates a ball joint or gimbaled physical selection or adjustment means in which position and orientation of the handle input is determined optically using for example TV Camera <b>1800</b> to view a four point target set <b>1805</b> (two targets shown for clarity) on ball member <b>1810</b> secured in ball joint clamp <b>1825</b> Joystick <b>1820</b> is attached to member <b>1810</b>.</p>
  <p num="p-0344">Using the invention of Pinkney et al, U.S. Pat. No. 4,219,847, or the devices of the El-Hakim reference, 6 axes of position and orientation of the member <b>1810</b> can be determined using program in computer <b>1830</b> connected to the TV camera. This axes data can then be used by computer <b>1830</b> to determine a control function desired (e.g. mirror rotation) and an appropriate display or voice or other response if desired. The camera used in this case, can if desired be the same one used to see knob positions, if such is used.</p>
  <p num="p-0345">Of considerable interest however is that this system can be mounted right to the dash, even to the display screen <b>1850</b> itself, as shown. In this case data selected by any of the up to 6 degrees of motion inputted by the user, can be displayed right next to the joystick by control of the display means such as a computer controlled projector projecting data on the display screen.</p>
  <p num="p-0346">If the ball joint is sufficiently tight, the joystick will stay where it is positioned. Alternatively it can be spring loaded to return to home center position, for example. Or it may be affixed with force feedback capability known in the art (eg from Immersion corporation) to provide a haptic response to the user as well, indicative of actions taken, or data to be fed back as a result thereof.</p>
  <p num="p-0347">Note that such a physical selection joystick device can also be built in the form of a palm resting ball or other. And such devices can be used as game controllers—in the car or home, for example</p>
  <p num="p-0348">
    <figref idrefs="DRAWINGS">FIG. 19</figref> </p>
  <p num="p-0349">The invention can be used in the home or work, besides in the car, as was previously disclosed as well. For example either the on axis or off axis version (<figref idrefs="DRAWINGS">FIG. 2</figref>) of the projection device can be used, the off axis being more complex, but able to occupy a bit different space (realizing the on axis version can be used with mirrors to make it taller rather than deeper, just as projection home TVs are today. Since the display surface, if build of the surface distortion type, can be chosen from different materials, one could use it as a home black board or cork board, putting family notes on, keeping records, drawing pictures or diagrams, etc.</p>
  <p num="p-0350">The advent of high definition projection sources, and large computer memories at low cost, allows one to project images of fine detail from a variety of sources. This HDTV type of image capability, in turn makes it practical to get close to a large screen projection, which here to fore has been too crude (pixilated) to view at close range. At this point the projection TV becomes essentially possessing the detail of a fine photograph, or drawing.</p>
  <p num="p-0351">Clearly this is useful on the car dash. But interesting too is in the home where such resolution makes it possible to display pictures scenes and the like, still or moving, in the sense of interior decoration.</p>
  <p num="p-0352">Shown here is an All Purpose touch screen display for the home, incorporating controls for heating and lighting and appliance systems plus entertainment and internet features, plus interior decorating and other features.</p>
  <p num="p-0353">As shown a large screen HDTV (High definition television) capable display <b>1900</b> is on wall <b>1901</b>. It is constructed in this case using a projection micro-display, in a manner similar to the figures above. For the moment, it is shown with the display on axis, but it could be off axis like <figref idrefs="DRAWINGS">FIG. 3</figref> as well, which could help in some cases to make the display less deep. As can be appreciated from the above disclosure, this display could at one time display beautiful pictures, say of a scene from Venice in the 1700's (static, or moving) to decorate a Venetian decor living room for example, while at the next moment provide a touch screen to act as home control center, for various functions such as heating, air conditioning, lighting, burglar alarms, appliances, and any and all other desired home functions.</p>
  <p num="p-0354">Then at the next moment, it could be used for entertainment (e.g. DVD discs, TV entertainment) or to communicate with the internet. Even images for the interior decoration can be downloaded from the internet. For example 1f one wanted to have a Venetian masquerade party, one could download from of the internet Carnevale pictures of a real party of that type in a Venetian setting, which could be sequentially played during the guests visit.</p>
  <p num="p-0355">In addition, one could use it to write on, for example notes to other family members could be entered directly just as one might do today with a blackboard in the kitchen. The written images, could be transmitted as well to other locations in the home, or to family members in the field.</p>
  <p num="p-0356">The screen can also be used for games, as has been discussed at length in other copending applications. Camera <b>1920</b>, or stereo Camera pair <b>1921</b> and <b>1925</b> located near the display can be used to sense inputs from humans or objects in the room.</p>
  <p num="p-0357">If the display were large sized, covering a good portion of a wall, what would be the effect? The use of such displays was discussed at length in the referenced Co-pending applications, for CAD Systems, Games, Education, and the like. Of particular interest is Life sized displays to give a “natural” feel to the data displayed. People, if life size displayed, become “Guests” in ones house, so to speak. One can in a natural manner use ones hands to wave at them (sensed by cameras of the invention too). And one can use the computer and screen to provide to the people in the home life sized digital images of themselves in various clothes they might wish to buy over the internet for example. And they can modify those images using a computer, to suit how they feel they might look under different weight reduction or enhancing regimens for example. Such digital data as to their initial shape can be obtained using laser triangulation techniques disclosed by the inventor and his colleagues as mentioned in U.S. Pat. Nos. 5,362,970 5,880,459, 5,877,491, 5,734,172, 5,670,787 and others.</p>
  <p num="p-0358">It isn't just the home where such a combination control and image entertainment function could be of use. Ones office is another example where the room functions, and pleasing decor, and perhaps ones own computer functions can all be combined. A desk of this type in the context of a CAD terminal is shown in co-pending application U.S. Ser. No. 09/435,854. In this case too, one can write on the screen for presentation purposes, or to transmit such data to others via a network for example. One can display pictures or drawings, and alter them and transmit or store the altered data as well. Such a display screen in a home or office, could become the main focal point for activity, since it allows all functions of a computer to be used, (and could have a plug in keyboard or mouse if desired), as well as entertainment and the like.</p>
  <p num="p-0359">The life size aspect of such a potential screen in a business context may relate most to dealing with objects that ones business is comprised of. For example in t he context of car parts, the thing has a real dimension.</p>
  <p num="p-0360">But the real benefit is dealing with people, who can be portrayed actual size. Or in dealing with things people use or interact with, which can also be of real size relative to the people in question.</p>
  <p num="p-0361">A good example is the “Digital You” “mirror” of the fashion related co-pending application, in which the touch screen—or even just plain display screen, is in the form of a full length fashion mirror for clothes trying. you can also have a perfect ballerina model, executing a perfect digital move of a particular ballet step.</p>
  <p num="p-0362">It is also possible to present to a user, a life size surrogate person for interaction, in this case in the context of clothes design where one tries different designs on a digital person, and touches them just as described in the copending application—in this case however, not to alter the garment, but to actually change its original design dimensions. And not necessarily fit related, put perhaps style related.</p>
  <p num="p-0363">
    <figref idrefs="DRAWINGS">FIG. 20</figref> </p>
  <p num="p-0364"> <figref idrefs="DRAWINGS">FIG. 20</figref> illustrates another advantage of the invention, namely the ability to locate the touch screen in areas of high electrical problem or contamination. In this case, it is in a shower bath <b>2000</b>, where located at end <b>2001</b> of the shower opposite the shower head <b>2005</b> is a touch screen <b>2010</b> of the invention, located in the wall of the shower. In this case the unit is used to allow the user to access the internet and all desired audio visual sources while in the shower, but at the same time by selection of a different screen it can control the functions in the home as described in <figref idrefs="DRAWINGS">FIG. 17</figref>, to include the plumbing, and the various features (heat, pressure, pulsation, etc) of the shower itself.</p>
  <p num="p-0365">The touch screen can operate at low voltage but of interest is that the total electrical part of the screen can be shielded easily from the using part attached to the shower. For example consider rubber gasket material <b>2040</b> which isolates the electrical components such as projection section <b>2045</b>, computer control section <b>2050</b> from screen input section <b>2053</b>—in this example a direct camera view <b>2058</b> of a screen face <b>2054</b> having sliding input or in this case rotating knob <b>2055</b>, and rotating knob <b>2057</b> viewed from the rear as disclosed in FIG. <b>2</b>/<b>3</b> above).</p>
  <p num="p-0366">For point of illustration, a completely different screen face <b>2065</b> having other knobs and input devices such as pushbutton <b>2068</b> and <b>2070</b> respectively, can be interchanged with the screen face <b>2054</b> as desired by the user. In one preferred embodiment, the user may reprogram the system to have these physical functions represent what ever variables he wishes.</p>
  <p num="p-0367">
    <figref idrefs="DRAWINGS">FIG. 21</figref> </p>
  <p num="p-0368"> <figref idrefs="DRAWINGS">FIG. 21</figref> illustrates an additional embodiment of the invention directed at optically sensed actuation of screens, even by pointing at them. Direct detection by one or more TV cameras of the pointing vector of ones finger (or other object) has been discussed in co-pending applications. A laser pointer application has been described above and in co-pending applications as well. However in this case, ones finger is viewed through the screen of a projection display, which for the moment in a car application is assumed to be in a location not requiring an airbag application though the screen (which however could be affected by angling the camera and sensing system here disclosed as in <figref idrefs="DRAWINGS">FIG. 3</figref>, so as to be out of the way of the airbag deployment.</p>
  <p num="p-0369">In this example, let us assume that a stereo pair of cameras <b>2101</b> and <b>2102</b> is located as shown in <figref idrefs="DRAWINGS">FIG. 21</figref> <i>a</i>, which conveniently is outside the deployment path of the airbag, if present. If the screen <b>2105</b> was perfectly transparent, this would be the same arrangement shown in the numerous examples for computer inputs using human features and objects shown in my co-pending applications, where the cameras are looking directly at the user. However, what if the screen is diffuse, as it would have to be to some degree for use in conjunction with image projector <b>2120</b>, in order that viewers could see the information displayed on the screen?</p>
  <p num="p-0370">For objects such as finger <b>2130</b> located a distance “d” from the screen <b>2105</b> as shown, I have found that it is possible to obtain useful x,y, and in some cases, z (depth direction) informational data concerning the finger or other object such as a pen tip, even if the screen is diffuse, which is a novel result of the invention (discussed also in <figref idrefs="DRAWINGS">FIG. 11</figref>). It is noted that an IR LED or LED Array, operating at for example a wavelength of 0.9 um, <b>2140</b> can be used to illuminate the users finger or other object through the screen. If desired, band pass filters <b>2131</b> and <b>2132</b> passing the 0.9 um radiation can filter light reaching the cameras <b>2101</b> and <b>2102</b> so as to substantially block light from the projector source (which also could include a beam blocking filter <b>2121</b> at the same wavelength, or even more preferably, to block all IR outside the visible region from the projector lamp (thus reducing heat on the screen and remaining portions of the device).</p>
  <p num="p-0371">Alternatively, in another useful arrangement, the screen <b>2105</b> illuminated by projector <b>2120</b>, is preferentially scattering at each elemental point on the screen in the lateral, or “x”, direction across the vehicle (or across a room in the <figref idrefs="DRAWINGS">FIG. 20</figref> case), but less diffuse if at all in the vertical direction (out of the plane of the drawing). This is the case if a lectilinear screen with vertical diffusing lines is used for example.</p>
  <p num="p-0372">If in this case, the stereo pair of cameras <b>2101</b> and <b>2102</b> are spaced with their baseline in the vertical direction out of the plane of the drawing, then they have an substantially unmodified view of the object situation in that direction, and accurate Z locations can be determined (and y data as well—though x data is modified by the screen effects).</p>
  <p num="p-0373">Another variation utilizes a screen <b>2105</b> equipped with a Digilens Company or other type switch able device <b>2155</b> (dotted lines) to control the holographic diffuser <b>2160</b> (dotted lines). On command of computer <b>2165</b>, the diffuser switch and the projector <b>2120</b> are switched off, and the cameras <b>2101</b> and <b>2102</b> look directly through a now transparent screen at the user (typically hands or fingers or head), <b>2130</b>. If the switching occurs every 1/60 sec or so, flicker of the image to the eye is minimal. The cameras output is analyzed by image processing program computer <b>2165</b> to obtain a stereo image of the edges of the object such as users finger head, or the like and the output made accordingly to control the functions of the vehicle and of the display as desired</p>
  <p num="p-0374">
    <figref idrefs="DRAWINGS">FIG. 22</figref> </p>
  <p num="p-0375">The above control aspects can in some forms be used in a car as well even with a small display, or in some cases without the display.</p>
  <p num="p-0376">For example, consider car steering wheel rim <b>2200</b> in <figref idrefs="DRAWINGS">FIG. 22</figref>. In particular, consider hinged targeted switch <b>2210</b> (likely in a cluster of several switches) on or near the top of the wheel, when the car is pointed straight ahead, and actuated by the thumb of the driver <b>2211</b>. A camera <b>2220</b> located in the headliner <b>2222</b>, and read out by microcomputer <b>2225</b> senses representative target <b>2230</b> on switch <b>2210</b>, when the switch is moved to a up position exposing the target to the camera. (or one could cover the target with ones fingers, and when you take a finger off, it is exposed, or conversely one can cover the target to actuate the action).</p>
  <p num="p-0377">The camera senses that target <b>2210</b> is desired to be signaled and accordingly computer <b>2225</b> assures this function, such as turning on the radio. As long as the switch stays in the position, the radio is on. However other forms of control can be used where the switch and target snap back to an original position, and the next actuation, turns the radio off. And too, the time the switch is actuated can indicate a function, such as increasing the volume of the radio until one lets off the switch, and the target is sensed to have swung back to its original position and the increase in volume thus terminated.</p>
  <p num="p-0378">In operating the invention in this manner, one can see position, velocity, orientation, excursion, or any other attribute of actuation desired. Because of the very low cost involved in incremental additions of functions, all kinds of things not normally sensed can be. For example the position of a datum on a plastic air outlet <b>2241</b> in the dashboard of <figref idrefs="DRAWINGS">FIG. 1</figref> <i>c </i>can be sensed, indicative of the direction of airflow. The computer can combine this with other data concerning driver or passenger wishes, other outlets, air temperature and the like, to perfect control of the ambiance of the car interior.</p>
  <p num="p-0379">It is also noted that the same TV camera used to sense switch positions, wheel position, duct position, seat position head rest position and a variety of other aspects of physical positions or motions of both the car controls and the driver or passengers. And it can do this without wires or other complicating devices such as rotary encoders which otherwise add to the service complexity and cost.</p>
  <p num="p-0380">When the camera is located as shown, it can also see other things of interest on the dashboard—and indeed the human driver himself, for example his head. This latter aspect has significance in that it can be used to determine numerous aspects
</p> <ul> <li id="ul0029-0001" num="0482">1. The identity of the driver. For example, if a certain band of height isn't reached, such as point P on the drivers head, the ignition can be interlocked. Much simpler than face recognition, but effective if properly interlocked to prevent repeated retries in a short time period.</li> <li id="ul0029-0002" num="0483">2. The position of the head of the driver in case of an accident. As detailed in reference <b>4</b>, a camera or cameras can be used to determine head location, and indeed location of the upper torso if the field of view is large enough. This information can be used to control airbag deployment, or head rest position prior to or during an accident (noting too that headrest position can also be monitored without adding any hardware). Particularly of interest is that the pixel addressing camera of the invention can have the frequency response to be useful in a crash, sensing the movement of the person (particularly severe if unrestrained) within a millisecond or two, and providing a measure of the position for airbag deployment. Additional cameras may also be used to aid the determination.</li> </ul> <p num="p-0381">Using a pixel addressing camera for camera <b>2220</b> confers additional advantages. For example consider the image of the car interior In the first instance one can confine the window of view of a certain group of pixels of camera to be only in the region of the steering wheel. This allows much faster readout of the more limited number of pixels, and thus of the steering wheel switches, at the expense of not seeing anywhere else in that particular reading. But this may be desirable in some cases, since it may only be required to scan for heater controls or seat positions, every 10 seconds say, while scanning for other more immediate items a hundred times per second or more. A good example are safety related functions. 5 per second might suffice for seeing where the turn signal or windshield washer control was, as an example.</p>
  <p num="p-0382">Scans in certain areas of the image can also depend on information obtained. For example one may initiate a scan of a control position, based on the increasing—or decreasing—frequency of an event occurrence. For example 1f the persons head is in a different location for a significant number of scans made at 15 second intervals for example, then in case of a crash, this data could be considered unreliable. Thus the camera window corresponding to pixels in the zone of the head could be scanned more frequently henceforward, either until the car stopped, or until such action settled down for example. Such action is often the case of a person listening to rock music, for example.</p>
  <p num="p-0383">Similarly, if someone is detected operating the heater controls, a scan of predominately heater function controls and related zones like air outlets can be initiated. Thus while normal polling of heater controls might be every 2 seconds say, once action is detected, polling can increase in the window(s) in question to 40 times per second for example.</p>
  <p num="p-0384">Scans in certain areas of the image can also depend on information obtained in other areas of scan, or be initiated by other control actions or by voice. For example, if hard de-acceleration was detected by an accelerometer, but before a crash occurred, the camera could immediately be commanded to begin scanning as fast as possible in the region of the image occupied by the driver and/or any other humans in its field of view. This would be for the purpose of monitoring movements in a crash, if a crash came, in order to deploy an airbag for example.</p>
  <p num="p-0385">While illustrated on the steering wheel where it is readily at hand, it can be appreciated that the position of switch or indication devices for the purpose at hand could be elsewhere than the wheel, for example on a stalk or on a piece of the dash, or other interior component—indeed wherever a camera of the invention can view them without excessive obscuration by persons or things in the car. It need not be on a car either, controls of this type can be in the home or elsewhere. Indeed a viewable control datum can even be on a portable component such as ones key chain, phone, or article of clothing apparel, or whatever. Similarly the camera <b>2220</b> can view these items for other purposes as well.</p>
  <p num="p-0386">The teach ability of the invention is achieved by showing the camera the code marker in question (e.g. a short bar located on the wheel), and in the computer recording this data along with what it is supposed to signify as a control function—for example, turn rear wiper on to first setting. This added functionality of being easily changed after manufacture is an important advantage in some cases, as for example, today after-market addition of wired in accessories is difficult.</p>
  <p num="p-0387">
    <figref idrefs="DRAWINGS">FIG. 22</figref> </p>
  <p num="p-0388"> <figref idrefs="DRAWINGS">FIG. 22</figref> illustrates a keyboard embodiment of the invention, in this case in the car touch screen, but certainly not limited to car applications.</p>
  <p num="p-0389">Taking the embodiments such as <figref idrefs="DRAWINGS">FIG. 5</figref> above one step further, suppose all keys were just circular depression type indents in a transparent piece—either the screen such as depicted here as screen surface <b>2200</b> (itself for example similar in function to screen <b>1403</b> of the invention), or an overlay thereon, such as member <b>2201</b> dotted lines. One could have all of the keys of the QWERTY Keyboard for example, represented as indents such as indent <b>2205</b> having projected on it the letter “Q” (Alternatively the letter could be just projected on the screen using the display alone, with no tactile feel). Despite the number of keys, the effect on other images presented on the screen (such as a graphical map of a navigational strategy to reach a location in the car, for example) would not be unduly affected, as the screen typically has a surface finish which scatters light from its outer surface (plus directs it in the case of the prismatic or rectilinear screen.</p>
  <p num="p-0390">Clearly the goal is to provide, on the dash, or other interior portion of the vehicle such as an arm rest, a keyboard not unlike those familiar today. This can be used for all the computer input functions known in the art. But it can also be used for different things as well. For example with one or more fingers at once or in succession, one can make a heater do something (Multi-finger at once capability is the key advantage of the distortion based screen, not shared with many other touch screen/pad technologies). One other example is that if a QUERTY keyboard with 40 indents, say, was used, a separate illumination of the indents might show up heater and air conditioning choices.</p>
  <p num="p-0391">For example, consider again screen <b>2200</b> which has numerous indents, only 6 of which are shown for clarity, <b>2205</b>–<b>2210</b>. One moment the keyboard letters of the QWERTY keyboard can be projected as discussed above, but the next moment on command as needed, shown here in the representative case, the projected numbers are 1,2,3,4,5, corresponding to 5 heater blower speeds.</p>
  <p num="p-0392">The command to change projections, and thus the screen characters and their meaning for touch action can be made for example, either by touch of some change area of the screen, or by voice, using voice recognition to bring up the new screen. Alternatively as discussed elsewhere in this application, for a keyboard is in a car, a camera located in the dash or headliner, for example, can image and using suitable image processing and interpretation algorithms, determine the position or velocity of a persons hand, finger or head for example to get such a switch command, or at a more complex gesture (movement sequence of the person).</p>
  <p num="p-0393">Note that the screen surface, or an overlay thereon, can be changed to create different indented or raised styles to suit different user needs. These could be specified when ordering from the factory, or dealer installed, or even user interchangeable. They can also incorporate customized printed on writing or designs, or contains specially added features, such as more knobs, or bigger knobs or keys (for elderly drivers, say). Similarly, the projected images could be bigger too.</p>
  <p num="p-0394">It should be noted that the device shown in <figref idrefs="DRAWINGS">FIG. 14</figref> provides useful tactile feel in such a keyboard touch screen system. The transparent medium displaced, creates a force resisting the action of the finger. This force can be increased by use of a pressurizing device, which device can be on all the time, or which may be actuated when the action of the finger on the screen is sensed by the camera or other sensing device used to sense touch on the screen.</p>
  <p num="p-0395">In a similar vein, it is noted that rather than create a local slope on a flat screen, one can use the invention in a method by which slope is reduced or removed to provide the indication desired. (in which case, the surface of the screen is flattened out so to speak). For example consider <figref idrefs="DRAWINGS">FIG. 22</figref> <i>b </i>which shows a touch screen having a transparent medium <b>2240</b> pressing (in this case upward) against a transparent overlay <b>2245</b>—urged by the pressure of liquid <b>2250</b> pressurized between transparent rear member <b>2255</b>. In this case the finger locations are round bumps such as <b>2260</b>, which are touched and pressed down against the action of the pressure by for example finger tip <b>2265</b>.</p>
  <p num="p-0396">Touch screens with high resolution and other useful functions are now described.</p>
  <p num="p-0397">
    <figref idrefs="DRAWINGS">FIG. 24</figref> </p>
  <p num="p-0398"> <figref idrefs="DRAWINGS">FIG. 24</figref> is an touch screen improvement whereby touch designation precision of small displayed features is assisted by use of two fingers and/or demagnified cursor movement controlled by ones finger movement.</p>
  <p num="p-0399">The pros and cons of various types of conventional computer inputs (mice, trackballs, touch screens etc) are well known. Touch screens are touted as superior and more naturally intuitive, as you can point right at the thing desired, such as an icon, word or whatever.</p>
  <p num="p-0400">However, in prior art touch screens, spatial resolution (in xy on the screen face) is often limited due to touch screen resolution, but more importantly, finger size and other finger characteristics, such as force, capacitance, impedance or the like. Even in my distortion based touch screen inventions, resolution is difficult to attain if the screen doesn't significantly distort or undergo stress upon a given force of touch.</p>
  <p num="p-0401">So while touch screens are great for input, unless one uses big characters (which are natural human wise in size only on a proportionately big screen), there is a lack of spatial resolution due to the size of your finger, which limits the size of displayed object which can be touched accurately. Thus in touch screens you see in Kiosks, for example, you touch a box, or a big icon—but you seldom are asked to touch to select a letter in a word, as you might do with a mouse and screen pointer.</p>
  <p num="p-0402">To improve spatial resolution, now disclosed is a significant improvement on my touch screen invention, and to at least some forms of conventional touch screens. Importantly, it solves the problem of designation of small displayed data items such as letter or characters using “large” fingers operating a touch screen. And it thus allows a touch screen to become an excellent generalized computer input.</p>
  <p num="p-0403">There are several manifestations to this. The first concerns use of a touch local to the point desired to “steer” a displayed screen cursor to the point, in a de-magnified manner in which a large finger movement causes a smaller cursor movement—often much smaller, e.g. 1:5. Said another way, I can first put my finger on the screen local to the letter say at a point at which a cursor is then displayed near my finger. I then can move my finger around at the point of touch to aim the cursor at the letter.</p>
  <p num="p-0404">For example, as shown in <figref idrefs="DRAWINGS">FIG. 24</figref> <i>a</i>, a persons finger <b>2401</b> touches touch screen <b>2405</b> at point <b>2410</b> near in both x and y, and perhaps an inch away from a letter “a” that it is desired to designate (typically the letter is in a word which is part of a sentence, none of which is shown for clarity). The letter is typically smaller than the persons finger. At this point one of several alternatives may be executed.</p>
  <p num="p-0405">In a first alternative, a cursor <b>2415</b> (in this case an X shaped cursor) is caused to appear under control of the display control computer on the screen <b>2405</b> a distance of lets say ½ inch above the finger point of touch <b>2410</b>. (Using known programming techniques which vary the vertical deflection of the screen raster, this amount can be user set, if other amounts are more preferable). A demagnification of movement can be employed, if desired, in sensing and cursor display system such that the cursor moves slowly either laterally or rotationally toward the letter “a” <b>2420</b> desired to be indicated for a given touch movement. In this case you would touch somewhat farther away, and “inch” toward the point desired. As the persons finger moves to point P′, the cursor has inched upward and over to the letter “a”. to illustrate, a movement of the finger on the screen of 0.5 inch in x to the right, would for example move the cursor only 0.1 inch (a 5:1 demagnification of movement, making it easier to move the cursor in small amounts needed to more precisely designate small items or precise points displayed.</p>
  <p num="p-0406">Shown in <figref idrefs="DRAWINGS">FIG. 24</figref> <i>b </i>is a second example, usable on those touch screens capable of input with two fingers. In this case thumb <b>2441</b> touches the screen, causing the cursor to be established at point P as before. But in this case finger <b>2450</b> touches the screen as well at point “Q”, and by moving the finger around the thumb point of touch, the cursor is caused to rotate around point p. And by moving finger <b>2450</b> radial-ly toward or away the thumb, the cursor <b>2455</b> is caused to extend toward the letter a as desired (the first point of cursor location having been determined for example, by the initial spacing “S” between the thumb and forefinger on the screen).</p>
  <p num="p-0407">It is also possible as shown that the finger <b>2450</b> and thumb <b>2441</b> can also be used to bracket the desired point “a” to be designated, with then a movement of the finger relative to the thumb can cause the cursor <b>2455</b> to move along the line <b>2470</b> between thumb and finger. When the cursor hits the desired point, one stops the finger motion. For a screen which can accept z inputs, one then just can push in with the thumb or finger in order to register the selection. Other wise one could use a voice command or hit a key or whatever.</p>
  <p num="p-0408">Finally, the radically different touch vector approach of <figref idrefs="DRAWINGS">FIG. 29</figref> can also be used for this purpose, but no other touch screens than my own are known to be able to operate in this fashion.</p>
  <p num="p-0409">
    <figref idrefs="DRAWINGS">FIG. 25</figref> </p>
  <p num="p-0410">To recap, shown in <figref idrefs="DRAWINGS">FIG. 24</figref> above are at least the following ways to interact with a touch screen.
</p> <ul> <li id="ul0030-0001" num="0000"> <ul> <li id="ul0031-0001" num="0514">1. Touch at one point and move—see change in local position from starting point, or possibly distance from absolute start</li> <li id="ul0031-0002" num="0515">2. Touch with two fingers. Move one relative to other—linear or rotationally</li> </ul> </li> </ul> <p num="p-0411"> <figref idrefs="DRAWINGS">FIG. 25</figref> now illustrates further embodiments of the “touch mouse” of <figref idrefs="DRAWINGS">FIG. 24</figref> using a pinching or other action-in this case with respect to information in a spreadsheet cell or a word document word or letter.</p>
  <p num="p-0412">As shown displayed on a screen <b>2505</b>, is a cell <b>2510</b> of the many cells of an “Excel” or other spreadsheet. A user, with a quick move can go right to the cell of interest and with his thumb <b>2515</b> and forefinger <b>2516</b>, “pinch” the data in the cell <b>2510</b>, in this case the dollar amount $38.10. The pinching action, for example can be programmed to cause the Excel program, to “cut” the data in the cell, for “pasting” elsewhere. The user can then move it, by sliding it on the screen while pinching it, to empty cell <b>2530</b>. When movement stops and the fingers un-pinch the program in this case would paste the data there. This move is totally natural, just as one would pinch any small object and move it to somewhere else and deposit it. This is considerably faster than using a mouse or touch pad, which needs to incrementally move a cursor across the sheet, highlight the item, and then drag it and drop it in the cell</p>
  <p num="p-0413">
    <figref idrefs="DRAWINGS">FIG. 26</figref> </p>
  <p num="p-0414"> <figref idrefs="DRAWINGS">FIG. 26</figref> further illustrates an embodiment of the invention using for example a sliding lever, rotary knob or joystick representation on the display screen. This is another variant, where what ever point you first touch becomes the origin for subsequent movement of the displayed bar, knob or the like.</p>
  <p num="p-0415">When used in a car, a driver can just glance over at the screen, touch the screen at a first point desired, which then signals the computer to provide a display on the screen, of a rotary knob, lever or whatever. The display is preferably oriented so that the starting point or frame of reference of its movement is established at the point of finger touch. Alternatively, one can touch the screen at a first point, and then indicate by pushing in on the display or other means that the last point the finger reached is to be considered the first point in this context.</p>
  <p num="p-0416">Such a display is preferably big so that one can then easily see the initial position of the knob or slider In the case of a knob, you then can turn it with ones fingers using a touch screen which has a twist motion response. (see <figref idrefs="DRAWINGS">FIG. 3</figref>). As it is twisted in this virtual manner, a movement of the knob dial with twist may be displayed, along with possibly other changes on the screen such as colors to indicate heat and the like as desired.</p>
  <p num="p-0417">It should be noted that in some cases it may be desirable to have an indent in the screen for one finger or other to locate in. This can be handy when driving if it is desirable that the display of data and other information be in the same place on the screen. Alternatively a small ridge in the screen, including linear or annular one can be used. (discussed also in <figref idrefs="DRAWINGS">FIGS. 1</figref>, <b>5</b> and elsewhere).</p>
  <p num="p-0418">For a slider, you just touch and move the lever, with your finger—or better pinch it and pull it back and forth as desired in a virtual motion.</p>
  <p num="p-0419">A screen there fore could then provide the ability to at various times display and actuate touch like dials, knobs, etc on the screen, and still have them be conveniently the size of a human hand and fingers. Could just display data in a big area—driver just tries to hit anywhere in the area and receives an indication back like clicks from acoustic pulse or other indicator to his finger.</p>
  <p num="p-0420">Such a device can also be used at the side of a page of typewritten data in order to designate a letter in a paragraph say. For example, consider <figref idrefs="DRAWINGS">FIG. 26</figref>, in which a persons finger <b>2650</b> touches point P on the side of a line of writing <b>2651</b> on displayed page <b>2655</b>, and a slider bar <b>2660</b> is displayed on the side of the page with a lever <b>2665</b> also displayed, which acts to slide a cursor <b>2670</b> toward a letter “a” to be designated. Often desired for accurate designation of the letter or other object, is that the movement of the cursor be de-magnified from the movement of the lever.</p>
  <p num="p-0421">
    <figref idrefs="DRAWINGS">FIG. 27</figref> </p>
  <p num="p-0422"> <figref idrefs="DRAWINGS">FIG. 27</figref> illustrates improved screen distortion and stress based optical touch screen embodiments, useful in my previous and co-pending applications referenced.</p>
  <p num="p-0423">Illustrated, is a clear plastic touch screen <b>2700</b> which is reinforced with fine steel wires such as <b>2705</b> and <b>2706</b> supported by rectangular ring member <b>2710</b> (typically of steel or aluminum) which provide elasticity to the screen and basically carry the load caused by a touch to the screen by finger <b>2720</b>. This has the effect too of making the screen not have too much deflection in the middle, often a problem when large forces are used with an unsupported screen clamped only at the edges. Additionally cross wires can be used to further strengthen the screen such as <b>2715</b> and <b>2716</b>.</p>
  <p num="p-0424">An Image projector as before, projects information onto the screen from the rear which is however, blocked in small regions by the wires. If the blockage is small relative to the data presented, it is not objectionable. In addition, or alternatively, the projector control <b>2730</b> may be imputed with data concerning the wire location on the screen, and the display program created or modified to keep critical data away from these locations, or at least to make data presented at the wire locations of large enough size as to not be impacted. This is particularly of use at the wire junctures, such as <b>2735</b> where crossed wires intersect.</p>
  <p num="p-0425">
    <figref idrefs="DRAWINGS">FIG. 28</figref> </p>
  <p num="p-0426"> <figref idrefs="DRAWINGS">FIG. 28</figref> <i>a </i>illustrates an embodiment of the invention where one first touches the screen at any point desired or comfortable, and said touch is temporarily indented into the screen. This indentation, for example in a soft plastic screen, has the latency to later serve as reference to tactilely come back to in a particular short session of touch command activity.</p>
  <p num="p-0427">For example consider finger tip <b>2801</b> which has just contacted screen <b>2810</b> at point ″P, and left an indentation <b>2815</b> in the screen at the point in question (shown also in depth in section A—A). The purpose of touching the screen in this instance was to hit a large projected box icon <b>2820</b> displayed on the screen which cued the system to switch to a new screen.</p>
  <p num="p-0428">As illustrated in <figref idrefs="DRAWINGS">FIG. 28</figref> <i>b</i>, the new image such as <b>2850</b> displayed by the computer controlled projector, would have its control bar <b>2851</b> displayed such that the control lever <b>2860</b> would be at the same point “P” of touch, that is at a point whose position was determined by the initial and variant touch location. Put another way, if you hit the big box (e.g. a heat icon)anywhere, the next control of a temperature bar is displayed where the starting point from its present temperature position (e.g. “74 degrees”) would be the same point “P” on the screen as you just touched and indented. Thus one can come back to this point, without looking at the screen at all, and move the lever left or right, to increase or decrease temp from 74. The desired setting numbers can be displayed in large letters to make it easy to see.</p>
  <p num="p-0429">In another instance it may be desired to come back to point P, and drag the icon box to a new location toward the side of the display, which tells the system a command depending on which side is desired (for example up down, left right for mirror positions). The driver can do this by feeling for the point previously touched—without taking his eyes off the road. And he can further feel the edges of the display, or alternatively, ridges or other indentations in the display as taught herein, if he wishes to explore the limits of the movement possible.</p>
  <p num="p-0430">A useful display with such as screen is when the display is caused to “jump” to the point touched. This is discussed relative to <figref idrefs="DRAWINGS">FIG. 10</figref> as well.</p>
  <p num="p-0431">The touch screen and related aspects of the invention open up new vistas for adding useful functions for improved control or enjoyment. And the completely programmable nature of the device, provides a future ability to insert new devices.</p>
  <p num="p-0432">However, it is often difficult for persons to learn how to use the functionality deliverable. This is particularly true if the person has just gotten into a rental car say. Illustrated a useful embodiment of the invention, wherein the instructions for operation of a device (e.g. the heat control knob and associated computer display screen or screens) is displayed right along with the device on the tactile display or touch screen of the invention.</p>
  <p num="p-0433">For example consider in <figref idrefs="DRAWINGS">FIG. 1</figref> or <b>2</b> for example a table of instructions for use can be down loaded from computer to image projector or other display mechanism and displayed right next the knob or slider or whatever is being used. This makes the understanding much easier, than for example the simple storing of a manual in the computer for later presentation. This is particularly possible when the display is large, and the instructions, and the controls easy to see.</p>
  <p num="p-0434">For example, as the person touches the knob, the instructions appear—if an instruction mode is activated in the control computer. The instructions and knob are preferably large enough so that no difficulty is encountered seeing both together in one view.</p>
  <p num="p-0435">This aspect of the invention is also very helpful with often complex functions. For example, when the knob is turned to heat as in <figref idrefs="DRAWINGS">FIG. 1</figref> or <b>2</b>, and then pushed in, a thermal bar graph display for example is activated which then itself is energized and used to set the temperature. The instructions then for this function are subsequently displayed to allow the user to at all times be informed as to what is desired.</p>
  <p num="p-0436">Further background information and commentary on the Automotive control application.</p>
  <p num="p-0437">The driver of the vehicle today, interacts with the controls of the vehicle primarily in three ways
</p> <ul> <li id="ul0032-0001" num="0000"> <ul> <li id="ul0033-0001" num="0543">1. by sight only</li> <li id="ul0033-0002" num="0544">2. by touch/feel only (possibly accompanied by a sound, such as a click-corresponding to a feeling sensation in many cases, e.g. a detent)</li> <li id="ul0033-0003" num="0545">3. by a quick glance, and then touch. Possibly followed by another quick glance to check a final setting of a knob, slider control, etc.</li> </ul> </li> </ul> <p num="p-0438">Typically while driving, only methods 2 and 3 above can be effected, as the control functions are too small to read (radio buttons are notorious) and require too much concentration while driving. Indeed for many users, these functions are effectively completely inoperable while driving, as they cannot be worked by touch only due to the crowded nature of their placement, and they are too hard to see in a glance—especially for those who cannot easily correct their vision away from the far sighted vision needed to drive.</p>
  <p num="p-0439">Conversely, some functions, such as turn signal stalks, transmission levers, and some knobs are big enough and/or simple enough such that only touch is needed.</p>
  <p num="p-0440">The addition of further desired functions such as internet communication to such an already difficult situation, constitutes a major challenge, and is one of the issues addressed by this invention. For such functions, designers today have heretofore envisioned only voice recognition as the medium for actuation and data retrieval.</p>
  <p num="h-0011">Problems of Voice Recognition Alone.</p>
  <p num="p-0441">In theory one can simply talk to your car. You say wipers on, and they come on, thanks to a program like IBM “via voice” or the like. But this has great difficulty in execution. Noise, unnatural activity, and a need for prompts in anything more than the simplest action, limit the effectiveness. In addition, such action is sequential. You can only do one voice activated thing at a time, and often you must do this slowly.</p>
  <p num="p-0442">Consider the extreme frustration of trying to carry out such actions in a noisy environment. You ask to retrieve emails. A voice prompt says from which account, you speak a little softly, it then repeats the request, asking you to speak more slowly/clearly saying it cant under stand. All this takes your mind off the driving and any other task needed. In extreme examples one can become enraged at the system, not a good thing while driving. The more you try to do this way, the worse it is.</p>
  <p num="p-0443">Consider too that it may not work well for another driver of the vehicle, as what it has learned with time may not match.</p>
  <p num="p-0444">In addition to all of the above, there is a general shortage of dashboard space on which to display data—be it from the vehicle subsystems, or from external sources. Some types of data one would wish to display could be (see also discussion above)</p>
  <p num="h-0012">Data Useful for Driving</p>
  <p num="p-0445">
    </p> <ul> <li id="ul0034-0001" num="0000">
        <ul> <li id="ul0035-0001" num="0553">Graphical data such as navigational maps, other Navigational information</li>
          <li id="ul0035-0002" num="0554">Picture data useful for front and rear vision and other peripheral viewing.
<br>
Data Useful for Car Functions
</li>
          <li id="ul0035-0003" num="0555">Maintenance issues</li>
          <li id="ul0035-0004" num="0556">Performance issues</li>
          <li id="ul0035-0005" num="0557">etc
<br>
Data Useful for General Information
</li>
          <li id="ul0035-0006" num="0558">stock price charts,</li>
          <li id="ul0035-0007" num="0559">hotel information</li>
          <li id="ul0035-0008" num="0560">tourist information
<br>
Data Useful for Specific Information
</li>
          <li id="ul0035-0009" num="0561">Email messages for the user</li>
          <li id="ul0035-0010" num="0562">Emergency data from outside sources</li>
        </ul> </li>
    </ul> <p num="p-0446">Entertainment—such as TV, games, or internet for the passengers</p>
  <p num="p-0447">Tactile Displays and Touch screens with physical attributes</p>
  <p num="p-0448">A touch screen and/or tactile display such as disclosed above could be used to replace all the display and actuation functions of the conventional dashboard, while at the same time vastly augmenting the potential scope of their application, as it can be reprogrammed to serve different needs in the vehicle at different times. One moment it can be used for heat and radio volume, the next for internet surfing, or camera image viewing (e.g. of the road behind the car) etc. The things to touch are programmably displayed.</p>
  <p num="p-0449">Particularly of interest are functions which the touch screen or tactile display may provide in a manner that is “Natural” to the user, such as knob turning, slider moving, switch rocking, turn signaling, transmission mode selection and the like.</p>
  <p num="p-0450">The application and adoption by the public of such a radically different dash approach, is probably aided by maintaining as much similarity as possible to the dash of today. And the more tactile, the better, not only for this purpose, but to allow control functions to be found and engaged with the least possible requirement to view the screen while driving in order to engage the needed functions.</p>
  <p num="p-0451">The issue of “Glance” and degree thereof required</p>
  <p num="p-0452">Lets consider the “Glance” needed, if any, of the driver to operate today's dash controls.</p>
  <p num="p-0453">A worst case is the increasing need for added buttons and the like for the ever multiplying electronic functions of the vehicle or communication systems. With fixed dash space, these become smaller and more complex. And most are not easily arrayed within the drivers line of sight.</p>
  <p num="p-0454">In addition, many require not just a first glance, but a continued stare and often a squint—small radio buttons come to mind. Many accidents have been registered as a result, even though the little button once found, may be tactile in nature. Some functions even require one to stare at changing numbers in an awkward location. Others require reading very small print (often impossible for those requiring glasses—a real dilemma for middle aged and old particularly).</p>
  <p num="p-0455">A touch screen can free up a whole expanse of screen to work with one function, for example a seat adjustment, or radio. Lettering can be big enough to read without glasses. And by doing all functions this way, a saving in manufacturing cost results as well.</p>
  <p num="p-0456">But the touch screen historically has a drawback as noted above, that it too may require of the driver more than a glance to operate successfully. This is because there is nothing on a conventional touch screen to reference ones self too—except the displayed data itself, which then has to be continually read. One means to do this by a “Heads up Display of sorts, displaying data on the windshield is disclosed herein which does not require one to take ones eyes of f the road. Another means is to use computer generated speech commands to for example tell the driver that he has touched, or moved a touch command button to a new position. In this case the Computer voice could say “74 Degrees” for the new setting of the heat control for example.</p>
  <p num="p-0457">Another approach is to create a touch screen or conventional display having tactile features, a concept discussed in considerable detail in the invention herein. It can thus be seen that the invention overcomes the problems of conventional touch screens for automotive or other applications in several ways, which can even be combined in some cases.</p>
  <p num="p-0458">A note on the viewing and reading of email and other messages.</p>
  <p num="p-0459">If email can only be read or spoken, then output of email while driving has to be readable by driver in plain view. But this may be illegal in most jurisdictions. Alternatively, the email message can be spoken to the driver by a text to voice program in the car information presentation computer which also controls the displays of the invention. A suitable program for example is the Text-to-speech (TTS) program of Fonix Corporation.</p>
  <p num="p-0460">The read back of email, is something that can be repeated as opportune, or safe. It also can just read who sent the message or other data, with a hand or finger control of the invention used to select whatever further info is desired from the computer and the email source.</p>
  <p num="p-0461">A bigger problem on the input side, is for a driver to command actions to a voice recognition program while trying to do something else (e.g. talk on a cell phone) or have conversation going with others. For this function, a tactile based system, possibly combined with voice is considered superior.</p>
  <p num="h-0013">Method of Manufacture and Sale</p>
  <p num="p-0462">The invention herein would appear to be less costly to manufacture than conventional dashboards, since two or three displays of the invention, which can be installed as modules along with the airbags (at least where airbags are used), can contain all means necessary to control the vehicle, plus provide a large number of exciting further functions and attributes. This means that only a few connections need be made to the other wiring and computers in the car as well. Heating, defrosting and air conditioning ducts and other related HVAC devices such as mixing doors etc are still required.</p>
  <p num="p-0463">It is possible to assemble the displays/airbag modules by inserting them from the front (toward the inside of the car) in a finished vehicle. This keeps them from damage in the line type assembly process. From a trim point however, assembly from the rear allows perhaps a more esthetically pleasing dash. Or the placement of a final trim panel over the displays once assembled can be made.</p>
  <p num="p-0464">Indeed the invention lends itself to dealer or after market installation as well. In one scenario, cars could be delivered to dealers without these components, and customers would go into a special room at the dealership and chose their instrumentation to their own tastes. This then would be programmed, any overlays or trim made, and installed in the vehicle. This tends to build dealership traffic, which otherwise is threatened due to the internet purchasing of vehicles trend, possible if all vehicles are commodities.</p>
  <p num="p-0465">It is noted that new display and touch functions can even be downloaded from an internet site such as that of the manufacturer—directly even to the user. This allows a continuous upgrade path for approved “accessories” of a software nature.</p>
  <p num="h-0014">The Problem of Driver Viewing of Prohibited Screens.</p>
  <p num="p-0466">U.S. Pat. No. 5,949,345 assigned to Microsoft, addresses the issue of what screens on a conventional computer display can be presented to the driver. This is a useful concept to incorporate with the instant invention, in this case making the total touch screen presentation generated in accordance with the ideas of U.S. Pat. No. 5,949,345 or those herein.</p>
  <p num="p-0467">There is another problem, that is how to sift through information contained in different screens? U.S. Pat. No. 5,995,3404 addresses one solution to the problem of making the driver sequentially view different screens until he gets to the one of interest, and describes the man machine interface devices such as joysticks needed to operate it. This reference describes a 3D screen presentation where all data is visible (supposedly) in one glance. This too can be incorporated with the touch screen invention here, but is thought to be at least somewhat objectionable as it departs from the classical presentation of dashboards in cars, or instrument panels in planes, which other wise the instant invention herein desirably resembles—a major advantage over presentation of “Computer-like” displays.</p>
  <p num="h-0015">Other Ideas</p>
  <p num="p-0468">In the touch screen devices above, the various screen selections have been switched by either actuating a switch (which might be gesture activated), or by voice or gesture command—that is say HEAT (or say, point at the heater), and the heat touch screen appears (or a screen having heat controls among other essential things, such as a speedometer display).</p>
  <p num="p-0469">One approach too is to have substantially every screen presented, such as the myriad shown in <figref idrefs="DRAWINGS">FIG. 34</figref>, have a section common to all, which contains a selection control to switch screens.</p>
  <p num="p-0470">The screens of the invention can as disclosed be used in numerous positions on the dash or elsewhere, and in some cases can cover the near total width of the dash. It is possible to envision that the image computers creating the displays for such screens could have multiple programs stored in the vehicle, loaded in by CD ROMs or DVD Discs, or down load new programs over the internet or other communication channels from remote data bases.</p>
  <p num="p-0471">Some novel screens could be for example to change the language of the screen, English to Spanish for example. But not only that, but to change the motif of the screen—to a more Spanish preferred layout if desired. OR the motif could depend on personal rather than regional tastes, for example a dash with a GO BLUE theme, if the driver was a die hard University of Michigan supporter for example. An Impressionist art lover, could have Van Goghs and Renoir's decorating his dash, and so on. In the extreme case of flexibility, not only could the language change and the decoration change, but also the actual placement and type of controls. For example ones standard and other screens of <figref idrefs="DRAWINGS">FIG. 34</figref> could be changed to suit, limited by any laws that require certain data at all times, for example. Business interests could include stock tickers and the like, with data received off the internet by cell phone.</p>
  <p num="p-0472">And the display could be dynamic, with display of at least data varying with time, for example pictures varying with music</p>
  <p num="p-0473">The same arguments also apply for the screen in ones home, or in ones office such as disclosed in <figref idrefs="DRAWINGS">FIGS. 19 and 20</figref> for example.</p>
  <p num="p-0474">It should be noted that when the sensing of a finger used to touch a screen is performed independently of the screen itself, for example as described in <figref idrefs="DRAWINGS">FIG. 6</figref> <i>c</i>, then the programmable screen “feel can be anything which is transparent enough for the projected image, without regard to whether the screen itself has the ability itself to sense touch.</p>
  <p num="p-0475">The touch screen and other sensing aspects of the invention above are useful for navigational purposes as well. A large screen presentation is available which is easy to see, and the driver can immediately designate a desired blow up of a route or other aspect on a map by rapidly touching it. The driver can also quickly draw on the map a possible alternative route, which the computer of the map system (local or remote) can then explore, and suggest possible avenues.</p>
  <p num="h-0016">A note on New Business Models</p>
  <p num="p-0476">In addition to the improved vehicle safety and control possibilities provided by the invention, there are many “New Business” possibilities opened up by instrument panels build according to the invention. From an internet business point of view, one for example, could customize ones own dash over the internet from the manufacturers or sellers web site, designing just how you wanted it presented. Perhaps you would want it all touch screen, or perhaps with some “real” knobs or the like. Then when the car was delivered it would be special for you, also with your name and address on the screen (a theft deterrent too!) for example. If you wanted to change it later, it could be reprogrammed by the dealer (or possibly yourself), and if you wanted major change, you could go back to the web site and design another one which could be put in. This could also done, just by changing an overlay and/the program for projecting images (if a projection display) and interpreting the touch screen and any other inputs such as cameras, microphones (for voice input or other purposes), etc.</p>
  <p num="p-0477">It should be noted that the programmable tactile features, such as knobs of the invention can have their use changed by the user, or by download of new functions from some source such as a car company (g GM) web site.</p>
  <p num="p-0478">Because the functions are shared, so much more data can be presented—even down to minor things like giving heater outlet air blowing regimens and graphs—any data avialabale indeed where one can easily go back to important functions and data when needed.</p>
  <p num="p-0479">The foregoing discussion should be understood as illustrative and should not be considered to be limiting in any sense. While this invention has been particularly shown and described with references to preferred embodiments thereof, it will be understood by those skilled in the art that various changes in form and details may</p>
  <p num="h-0017">be made therein without departing from the spirit and scope of the invention</p>
</div></div></div><div class="patent-section patent-tabular-section"><a id="backward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Patent Citations</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Cited Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US3825730">US3825730</a></td><td class="patent-data-table-td patent-date-value">Nov 7, 1972</td><td class="patent-data-table-td patent-date-value">Jul 23, 1974</td><td class="patent-data-table-td ">Searle Medidata Inc</td><td class="patent-data-table-td ">Two-way projection system for data input</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4561017">US4561017</a></td><td class="patent-data-table-td patent-date-value">Aug 19, 1983</td><td class="patent-data-table-td patent-date-value">Dec 24, 1985</td><td class="patent-data-table-td ">Richard Greene</td><td class="patent-data-table-td ">Graphic input apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4631525">US4631525</a></td><td class="patent-data-table-td patent-date-value">Apr 11, 1983</td><td class="patent-data-table-td patent-date-value">Dec 23, 1986</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Digital fader or like device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US4787040">US4787040</a></td><td class="patent-data-table-td patent-date-value">Dec 22, 1986</td><td class="patent-data-table-td patent-date-value">Nov 22, 1988</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Display system for automotive vehicle</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5418760">US5418760</a></td><td class="patent-data-table-td patent-date-value">Aug 2, 1993</td><td class="patent-data-table-td patent-date-value">May 23, 1995</td><td class="patent-data-table-td ">Casio Computer Co., Ltd.</td><td class="patent-data-table-td ">Electronic devices with a liquid crystal display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5483261">US5483261</a></td><td class="patent-data-table-td patent-date-value">Oct 26, 1993</td><td class="patent-data-table-td patent-date-value">Jan 9, 1996</td><td class="patent-data-table-td ">Itu Research, Inc.</td><td class="patent-data-table-td ">Graphical input controller and method with rear screen image detection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5530456">US5530456</a></td><td class="patent-data-table-td patent-date-value">Dec 7, 1994</td><td class="patent-data-table-td patent-date-value">Jun 25, 1996</td><td class="patent-data-table-td ">Nec Corporation</td><td class="patent-data-table-td ">Position information input method and device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5572239">US5572239</a></td><td class="patent-data-table-td patent-date-value">Apr 10, 1995</td><td class="patent-data-table-td patent-date-value">Nov 5, 1996</td><td class="patent-data-table-td ">Jaeger; Denny</td><td class="patent-data-table-td ">Operator/circuit interface with integrated display screen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5709219">US5709219</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 1, 1996</td><td class="patent-data-table-td patent-date-value">Jan 20, 1998</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and apparatus to create a complex tactile sensation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5712661">US5712661</a></td><td class="patent-data-table-td patent-date-value">Aug 26, 1996</td><td class="patent-data-table-td patent-date-value">Jan 27, 1998</td><td class="patent-data-table-td ">Intertactile Technologies Corporation</td><td class="patent-data-table-td ">Operator/circuit interface with integrated display screen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5726685">US5726685</a></td><td class="patent-data-table-td patent-date-value">Jun 30, 1995</td><td class="patent-data-table-td patent-date-value">Mar 10, 1998</td><td class="patent-data-table-td ">Siemens Aktiengesellschaft</td><td class="patent-data-table-td ">Input unit for a computer</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5805145">US5805145</a></td><td class="patent-data-table-td patent-date-value">May 10, 1996</td><td class="patent-data-table-td patent-date-value">Sep 8, 1998</td><td class="patent-data-table-td ">Intertactile Technologies Corporation</td><td class="patent-data-table-td ">Circuit control panel displaying changeable graphics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5805146">US5805146</a></td><td class="patent-data-table-td patent-date-value">Dec 10, 1996</td><td class="patent-data-table-td patent-date-value">Sep 8, 1998</td><td class="patent-data-table-td ">Intertactile Technologies Corporation</td><td class="patent-data-table-td ">Integrated display screen and slidable control for electrical circuits</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5841428">US5841428</a></td><td class="patent-data-table-td patent-date-value">Sep 20, 1996</td><td class="patent-data-table-td patent-date-value">Nov 24, 1998</td><td class="patent-data-table-td ">Intertactile Technologies Corporation</td><td class="patent-data-table-td ">Rotary circuit control devices with changeable graphics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5859631">US5859631</a></td><td class="patent-data-table-td patent-date-value">Dec 19, 1996</td><td class="patent-data-table-td patent-date-value">Jan 12, 1999</td><td class="patent-data-table-td ">Siemens Elema Ab</td><td class="patent-data-table-td ">Apparatus front panel allowing indicia on an indicia-bearing element to be read therethrough</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5867149">US5867149</a></td><td class="patent-data-table-td patent-date-value">Aug 14, 1995</td><td class="patent-data-table-td patent-date-value">Feb 2, 1999</td><td class="patent-data-table-td ">Intertactile Technologies Corporation</td><td class="patent-data-table-td ">Switch key image display and operator/circuit interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5871251">US5871251</a></td><td class="patent-data-table-td patent-date-value">Mar 14, 1996</td><td class="patent-data-table-td patent-date-value">Feb 16, 1999</td><td class="patent-data-table-td ">Prince Corporation</td><td class="patent-data-table-td ">For displaying information to an occupant of the vehicle</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5923319">US5923319</a></td><td class="patent-data-table-td patent-date-value">Nov 7, 1997</td><td class="patent-data-table-td patent-date-value">Jul 13, 1999</td><td class="patent-data-table-td ">International Business Machines Corporation</td><td class="patent-data-table-td ">Front cover assembly for touch sensitive device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5936613">US5936613</a></td><td class="patent-data-table-td patent-date-value">Aug 11, 1997</td><td class="patent-data-table-td patent-date-value">Aug 10, 1999</td><td class="patent-data-table-td ">Intertactile Technologies Corporation</td><td class="patent-data-table-td ">Rotary circuit control devices with changeable graphics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5949345">US5949345</a></td><td class="patent-data-table-td patent-date-value">May 27, 1997</td><td class="patent-data-table-td patent-date-value">Sep 7, 1999</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Displaying computer information to a driver of a vehicle</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5982355">US5982355</a></td><td class="patent-data-table-td patent-date-value">Aug 25, 1997</td><td class="patent-data-table-td patent-date-value">Nov 9, 1999</td><td class="patent-data-table-td ">Jaeger; Denny</td><td class="patent-data-table-td ">Multiple purpose controls for electrical systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US5995104">US5995104</a></td><td class="patent-data-table-td patent-date-value">Jul 16, 1996</td><td class="patent-data-table-td patent-date-value">Nov 30, 1999</td><td class="patent-data-table-td ">Yazaki Corporation</td><td class="patent-data-table-td ">Vehicle display unit with three-dimensional menu controlled by an input device which has two joysticks</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6057540">US6057540</a></td><td class="patent-data-table-td patent-date-value">Apr 30, 1998</td><td class="patent-data-table-td patent-date-value">May 2, 2000</td><td class="patent-data-table-td ">Hewlett-Packard Co</td><td class="patent-data-table-td ">Mouseless optical and position translation type screen pointer control for a computer system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6066075">US6066075</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 29, 1997</td><td class="patent-data-table-td patent-date-value">May 23, 2000</td><td class="patent-data-table-td ">Poulton; Craig K.</td><td class="patent-data-table-td ">Direct feedback controller for user interaction</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6219035">US6219035</a></td><td class="patent-data-table-td patent-date-value">Oct 27, 1998</td><td class="patent-data-table-td patent-date-value">Apr 17, 2001</td><td class="patent-data-table-td ">Siemens Elema Ab</td><td class="patent-data-table-td ">Apparatus panel</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6256020">US6256020</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 1998</td><td class="patent-data-table-td patent-date-value">Jul 3, 2001</td><td class="patent-data-table-td ">G &amp; R Associates Incorporated</td><td class="patent-data-table-td ">Computer-telephony integration employing an intelligent keyboard and method for same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6278441">US6278441</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 14, 1999</td><td class="patent-data-table-td patent-date-value">Aug 21, 2001</td><td class="patent-data-table-td ">Virtouch, Ltd.</td><td class="patent-data-table-td ">Tactile interface system for electronic data display system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6414672">US6414672</a></td><td class="patent-data-table-td patent-date-value">Jul 6, 1998</td><td class="patent-data-table-td patent-date-value">Jul 2, 2002</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Information input apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6421042">US6421042</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 9, 1999</td><td class="patent-data-table-td patent-date-value">Jul 16, 2002</td><td class="patent-data-table-td ">Ricoh Company, Ltd.</td><td class="patent-data-table-td ">Coordinate position inputting/detecting device, a method for inputting/detecting the coordinate position, and a display board system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6421046">US6421046</a></td><td class="patent-data-table-td patent-date-value">Nov 23, 1998</td><td class="patent-data-table-td patent-date-value">Jul 16, 2002</td><td class="patent-data-table-td ">Saab Automobile Ab</td><td class="patent-data-table-td ">Control panel</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6441806">US6441806</a></td><td class="patent-data-table-td patent-date-value">Aug 26, 1996</td><td class="patent-data-table-td patent-date-value">Aug 27, 2002</td><td class="patent-data-table-td ">Intertact Corporation</td><td class="patent-data-table-td ">Operator/circuit interface with integrated display screen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US6563492">US6563492</a></td><td class="patent-data-table-td patent-date-value">Feb 8, 2000</td><td class="patent-data-table-td patent-date-value">May 13, 2003</td><td class="patent-data-table-td ">Yazaki Corporation</td><td class="patent-data-table-td ">Multi-function switch unit and function indicating method of the same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20020130839">US20020130839</a></td><td class="patent-data-table-td patent-date-value">Mar 16, 2001</td><td class="patent-data-table-td patent-date-value">Sep 19, 2002</td><td class="patent-data-table-td ">Hugh Wallace</td><td class="patent-data-table-td ">Optical screen pointing device with inertial properties</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="forward-citations"></a><div class="patent-section-header"><span class="patent-section-title">Referenced by</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Citing Patent</th><th class="patent-data-table-th">Filing date</th><th class="patent-data-table-th">Publication date</th><th class="patent-data-table-th">Applicant</th><th class="patent-data-table-th">Title</th></tr></thead><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7138986">US7138986</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 26, 2003</td><td class="patent-data-table-td patent-date-value">Nov 21, 2006</td><td class="patent-data-table-td ">Smk Corporation</td><td class="patent-data-table-td ">Touch-type input apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7379562">US7379562</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2004</td><td class="patent-data-table-td patent-date-value">May 27, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Determining connectedness and offset of 3D objects relative to an interactive surface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7389319">US7389319</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 22, 2002</td><td class="patent-data-table-td patent-date-value">Jun 17, 2008</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Adaptive connection routing over multiple communication channels</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7394459">US7394459</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 29, 2004</td><td class="patent-data-table-td patent-date-value">Jul 1, 2008</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Interaction between objects and a virtual environment display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7405651">US7405651</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 13, 2005</td><td class="patent-data-table-td patent-date-value">Jul 29, 2008</td><td class="patent-data-table-td ">Siemens Aktiengesellschaft</td><td class="patent-data-table-td ">Operating element for a heating and air conditioning system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7406421">US7406421</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 14, 2002</td><td class="patent-data-table-td patent-date-value">Jul 29, 2008</td><td class="patent-data-table-td ">Intellisist Inc.</td><td class="patent-data-table-td ">Systems and methods for reviewing informational content in a vehicle</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7466843">US7466843</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 19, 2005</td><td class="patent-data-table-td patent-date-value">Dec 16, 2008</td><td class="patent-data-table-td ">Pryor Timothy R</td><td class="patent-data-table-td ">Multi-functional control and entertainment systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7499027">US7499027</a></td><td class="patent-data-table-td patent-date-value">Apr 29, 2005</td><td class="patent-data-table-td patent-date-value">Mar 3, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Using a light pointer for input on an interactive display surface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7515143">US7515143</a></td><td class="patent-data-table-td patent-date-value">Feb 28, 2006</td><td class="patent-data-table-td patent-date-value">Apr 7, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Uniform illumination of interactive display panel</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7519223">US7519223</a></td><td class="patent-data-table-td patent-date-value">Jun 28, 2004</td><td class="patent-data-table-td patent-date-value">Apr 14, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Recognizing gestures and using gestures for interacting with software applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7525538">US7525538</a></td><td class="patent-data-table-td patent-date-value">Jun 28, 2005</td><td class="patent-data-table-td patent-date-value">Apr 28, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Using same optics to image, illuminate, and project</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7535463">US7535463</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 15, 2005</td><td class="patent-data-table-td patent-date-value">May 19, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Optical flow-based manipulation of graphical objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7538662">US7538662</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 4, 2005</td><td class="patent-data-table-td patent-date-value">May 26, 2009</td><td class="patent-data-table-td ">Innovate! Technology, Inc.</td><td class="patent-data-table-td ">Programmable vehicle gauge apparatus, system, and method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7542995">US7542995</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 11, 2006</td><td class="patent-data-table-td patent-date-value">Jun 2, 2009</td><td class="patent-data-table-td ">Sun Microsystems, Inc.</td><td class="patent-data-table-td ">Method and apparatus for detecting a change-point in a time-series of computer telemetry signals</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7546188">US7546188</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 22, 2004</td><td class="patent-data-table-td patent-date-value">Jun 9, 2009</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">In-vehicle apparatus and control method of in-vehicle apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7551814">US7551814</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 21, 2006</td><td class="patent-data-table-td patent-date-value">Jun 23, 2009</td><td class="patent-data-table-td ">National Semiconductor Corporation</td><td class="patent-data-table-td ">Optical detection of user interaction based on external light source</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7574020">US7574020</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 7, 2008</td><td class="patent-data-table-td patent-date-value">Aug 11, 2009</td><td class="patent-data-table-td ">Gesturetek, Inc.</td><td class="patent-data-table-td ">Detecting and tracking objects in images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7576725">US7576725</a></td><td class="patent-data-table-td patent-date-value">Oct 19, 2004</td><td class="patent-data-table-td patent-date-value">Aug 18, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Using clear-coded, see-through objects to manipulate virtual objects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7593593">US7593593</a></td><td class="patent-data-table-td patent-date-value">Jun 16, 2004</td><td class="patent-data-table-td patent-date-value">Sep 22, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for reducing effects of undesired signals in an infrared imaging system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7613358">US7613358</a></td><td class="patent-data-table-td patent-date-value">Apr 21, 2008</td><td class="patent-data-table-td patent-date-value">Nov 3, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for reducing effects of undesired signals in an infrared imaging system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7677978">US7677978</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 21, 2005</td><td class="patent-data-table-td patent-date-value">Mar 16, 2010</td><td class="patent-data-table-td ">Nintendo Co., Ltd.</td><td class="patent-data-table-td ">Game apparatus and game program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7708641">US7708641</a></td><td class="patent-data-table-td patent-date-value">Jan 21, 2005</td><td class="patent-data-table-td patent-date-value">May 4, 2010</td><td class="patent-data-table-td ">Nintendo Co., Ltd.</td><td class="patent-data-table-td ">Game program for touch control hand-held game device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7714800">US7714800</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 2, 2004</td><td class="patent-data-table-td patent-date-value">May 11, 2010</td><td class="patent-data-table-td ">Pioneer Corporation</td><td class="patent-data-table-td ">Information display apparatus, information display method and information recording medium for information display apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7743348">US7743348</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 30, 2004</td><td class="patent-data-table-td patent-date-value">Jun 22, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Using physical objects to adjust attributes of an interactive display application</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7750840">US7750840</a></td><td class="patent-data-table-td patent-date-value">Dec 4, 2007</td><td class="patent-data-table-td patent-date-value">Jul 6, 2010</td><td class="patent-data-table-td ">Raytheon Company</td><td class="patent-data-table-td ">Method and apparatus for assessing contact clusters</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7760188">US7760188</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 2, 2004</td><td class="patent-data-table-td patent-date-value">Jul 20, 2010</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Information processing system, remote maneuvering unit and method thereof, control unit and method thereof, program, and recording medium</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7787706">US7787706</a></td><td class="patent-data-table-td patent-date-value">Jun 14, 2004</td><td class="patent-data-table-td patent-date-value">Aug 31, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method for controlling an intensity of an infrared source used to detect objects adjacent to an interactive display surface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7801731">US7801731</a></td><td class="patent-data-table-td patent-date-value">Oct 30, 2007</td><td class="patent-data-table-td patent-date-value">Sep 21, 2010</td><td class="patent-data-table-td ">Intellisist, Inc.</td><td class="patent-data-table-td ">Systems and methods for processing voice instructions in a vehicle</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7817020">US7817020</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 17, 2006</td><td class="patent-data-table-td patent-date-value">Oct 19, 2010</td><td class="patent-data-table-td ">Gentex Corporation</td><td class="patent-data-table-td ">Optical user interface system for automotive modules</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7840347">US7840347</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 7, 2006</td><td class="patent-data-table-td patent-date-value">Nov 23, 2010</td><td class="patent-data-table-td ">Xanavi Informatics Corporation</td><td class="patent-data-table-td ">Navigation system and route setting method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7853041">US7853041</a></td><td class="patent-data-table-td patent-date-value">Jan 6, 2006</td><td class="patent-data-table-td patent-date-value">Dec 14, 2010</td><td class="patent-data-table-td ">Gesturetek, Inc.</td><td class="patent-data-table-td ">Detecting and tracking objects in images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7872636">US7872636</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 31, 2006</td><td class="patent-data-table-td patent-date-value">Jan 18, 2011</td><td class="patent-data-table-td ">Marvell International Ltd.</td><td class="patent-data-table-td ">Virtual pointing devices for displays</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7885508">US7885508</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 10, 2006</td><td class="patent-data-table-td patent-date-value">Feb 8, 2011</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">Input apparatus, input method, input control program, reproduction apparatus, reproduction control method, and reproduction control program</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7887222">US7887222</a></td><td class="patent-data-table-td patent-date-value">May 9, 2008</td><td class="patent-data-table-td patent-date-value">Feb 15, 2011</td><td class="patent-data-table-td ">Yazaki North America, Inc.</td><td class="patent-data-table-td ">Display device with changeable display background</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7889187">US7889187</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 20, 2007</td><td class="patent-data-table-td patent-date-value">Feb 15, 2011</td><td class="patent-data-table-td ">Kohler Co.</td><td class="patent-data-table-td ">User interface for controlling a bathroom plumbing fixture</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7898436">US7898436</a></td><td class="patent-data-table-td patent-date-value">Jun 10, 2008</td><td class="patent-data-table-td patent-date-value">Mar 1, 2011</td><td class="patent-data-table-td ">Alpine Electronics, Inc.</td><td class="patent-data-table-td ">On-vehicle position detection system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7904220">US7904220</a></td><td class="patent-data-table-td patent-date-value">Jul 22, 2005</td><td class="patent-data-table-td patent-date-value">Mar 8, 2011</td><td class="patent-data-table-td ">GM Global Technology Operations LLC</td><td class="patent-data-table-td ">Vehicular multifunction control system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7907128">US7907128</a></td><td class="patent-data-table-td patent-date-value">Apr 25, 2008</td><td class="patent-data-table-td patent-date-value">Mar 15, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Interaction between objects and a virtual environment display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7911444">US7911444</a></td><td class="patent-data-table-td patent-date-value">Aug 31, 2005</td><td class="patent-data-table-td patent-date-value">Mar 22, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Input method for surface of interactive display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7920102">US7920102</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 23, 2006</td><td class="patent-data-table-td patent-date-value">Apr 5, 2011</td><td class="patent-data-table-td ">Automotive Technologies International, Inc.</td><td class="patent-data-table-td ">Vehicular heads-up display system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7925996">US7925996</a></td><td class="patent-data-table-td patent-date-value">Nov 18, 2004</td><td class="patent-data-table-td patent-date-value">Apr 12, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for providing multiple input connecting user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7928859">US7928859</a></td><td class="patent-data-table-td patent-date-value">Nov 21, 2008</td><td class="patent-data-table-td patent-date-value">Apr 19, 2011</td><td class="patent-data-table-td ">Yazaki North America, Inc.</td><td class="patent-data-table-td ">Full angle laser illuminate instrument cluster</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US7954965">US7954965</a></td><td class="patent-data-table-td patent-date-value">Apr 13, 2009</td><td class="patent-data-table-td patent-date-value">Jun 7, 2011</td><td class="patent-data-table-td ">Yazaki North America, Inc.</td><td class="patent-data-table-td ">Method for multiple gauges in a scanning laser based display device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8022942">US8022942</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 25, 2007</td><td class="patent-data-table-td patent-date-value">Sep 20, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Dynamic projected user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8026827">US8026827</a></td><td class="patent-data-table-td patent-date-value">May 4, 2009</td><td class="patent-data-table-td patent-date-value">Sep 27, 2011</td><td class="patent-data-table-td ">Yazaki North America, Inc.</td><td class="patent-data-table-td ">Virtual push button switch</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8032264">US8032264</a></td><td class="patent-data-table-td patent-date-value">Oct 30, 2007</td><td class="patent-data-table-td patent-date-value">Oct 4, 2011</td><td class="patent-data-table-td ">Automotive Technologies International, Inc.</td><td class="patent-data-table-td ">Vehicular heads-up display system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8032280">US8032280</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 17, 2008</td><td class="patent-data-table-td patent-date-value">Oct 4, 2011</td><td class="patent-data-table-td ">Aisin Aw. Co., Ltd.</td><td class="patent-data-table-td ">Switch control device and switch control method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8041460">US8041460</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 26, 2009</td><td class="patent-data-table-td patent-date-value">Oct 18, 2011</td><td class="patent-data-table-td ">Visteon Global Technologies, Inc.</td><td class="patent-data-table-td ">User interface for adjusting parameters for climate control systems in motor vehicles</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8060840">US8060840</a></td><td class="patent-data-table-td patent-date-value">Dec 29, 2005</td><td class="patent-data-table-td patent-date-value">Nov 15, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Orientation free user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8068942">US8068942</a></td><td class="patent-data-table-td patent-date-value">Oct 30, 2007</td><td class="patent-data-table-td patent-date-value">Nov 29, 2011</td><td class="patent-data-table-td ">Automotive Technologies International, Inc.</td><td class="patent-data-table-td ">Vehicular heads-up display system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8094131">US8094131</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 11, 2008</td><td class="patent-data-table-td patent-date-value">Jan 10, 2012</td><td class="patent-data-table-td ">National Taiwan University</td><td class="patent-data-table-td ">Touch control virtual screen apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8098856">US8098856</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 22, 2006</td><td class="patent-data-table-td patent-date-value">Jan 17, 2012</td><td class="patent-data-table-td ">Sony Ericsson Mobile Communications Ab</td><td class="patent-data-table-td ">Wireless communications devices with three dimensional audio systems</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8103057">US8103057</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 22, 2010</td><td class="patent-data-table-td patent-date-value">Jan 24, 2012</td><td class="patent-data-table-td ">Smart Technologies Ulc</td><td class="patent-data-table-td ">System and method for capturing images of a target area on which information is recorded</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8125442">US8125442</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 12, 2008</td><td class="patent-data-table-td patent-date-value">Feb 28, 2012</td><td class="patent-data-table-td ">Immersion Corporation</td><td class="patent-data-table-td ">System and method for manipulation of sound data using haptic feedback</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8142030">US8142030</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 28, 2008</td><td class="patent-data-table-td patent-date-value">Mar 27, 2012</td><td class="patent-data-table-td ">Visteon Global Technologies, Inc.</td><td class="patent-data-table-td ">Reconfigurable center stack with touch sensing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8155832">US8155832</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 7, 2009</td><td class="patent-data-table-td patent-date-value">Apr 10, 2012</td><td class="patent-data-table-td ">Denso Corporation</td><td class="patent-data-table-td ">Apparatus for remote operation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8159682">US8159682</a></td><td class="patent-data-table-td patent-date-value">Nov 12, 2008</td><td class="patent-data-table-td patent-date-value">Apr 17, 2012</td><td class="patent-data-table-td ">Intellectual Ventures Holding 67 Llc</td><td class="patent-data-table-td ">Lens system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8164574">US8164574</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 10, 2007</td><td class="patent-data-table-td patent-date-value">Apr 24, 2012</td><td class="patent-data-table-td ">Denso Corporation</td><td class="patent-data-table-td ">Touch panel input system for vehicle</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8165422">US8165422</a></td><td class="patent-data-table-td patent-date-value">Jun 26, 2009</td><td class="patent-data-table-td patent-date-value">Apr 24, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Method and system for reducing effects of undesired signals in an infrared imaging system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8169410">US8169410</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 20, 2005</td><td class="patent-data-table-td patent-date-value">May 1, 2012</td><td class="patent-data-table-td ">Nintendo Co., Ltd.</td><td class="patent-data-table-td ">Gesture inputs for a portable display device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8170281">US8170281</a></td><td class="patent-data-table-td patent-date-value">Aug 10, 2009</td><td class="patent-data-table-td patent-date-value">May 1, 2012</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Detecting and tracking objects in images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8179378">US8179378</a></td><td class="patent-data-table-td patent-date-value">Dec 27, 2010</td><td class="patent-data-table-td patent-date-value">May 15, 2012</td><td class="patent-data-table-td ">Kyocera Corporation</td><td class="patent-data-table-td ">Input apparatus and control method of input apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8201109">US8201109</a></td><td class="patent-data-table-td patent-date-value">Sep 30, 2008</td><td class="patent-data-table-td patent-date-value">Jun 12, 2012</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Methods and graphical user interfaces for editing on a portable multifunction device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8212857">US8212857</a></td><td class="patent-data-table-td patent-date-value">Jan 26, 2007</td><td class="patent-data-table-td patent-date-value">Jul 3, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Alternating light sources to reduce specular reflection</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8218154">US8218154</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 29, 2007</td><td class="patent-data-table-td patent-date-value">Jul 10, 2012</td><td class="patent-data-table-td ">Flatfrog Laboratories Ab</td><td class="patent-data-table-td ">System and a method of determining a position of a scattering/reflecting element on the surface of a radiation transmissive element</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8219936">US8219936</a></td><td class="patent-data-table-td patent-date-value">Aug 21, 2008</td><td class="patent-data-table-td patent-date-value">Jul 10, 2012</td><td class="patent-data-table-td ">Lg Electronics Inc.</td><td class="patent-data-table-td ">User interface for a mobile device using a user&#39;s gesture in the proximity of an electronic device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8228304">US8228304</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 11, 2009</td><td class="patent-data-table-td patent-date-value">Jul 24, 2012</td><td class="patent-data-table-td ">Smart Technologies Ulc</td><td class="patent-data-table-td ">Size/scale orientation determination of a pointer in a camera-based touch system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8229603">US8229603</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 8, 2008</td><td class="patent-data-table-td patent-date-value">Jul 24, 2012</td><td class="patent-data-table-td ">Kabushiki Kaisha Tokai Rika Denki Seisakusho</td><td class="patent-data-table-td ">In-vehicle equipment control device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8248375">US8248375</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 11, 2008</td><td class="patent-data-table-td patent-date-value">Aug 21, 2012</td><td class="patent-data-table-td ">Fih (Hong Kong) Limited</td><td class="patent-data-table-td ">Input device for complex control signal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8249880">US8249880</a></td><td class="patent-data-table-td patent-date-value">Aug 18, 2010</td><td class="patent-data-table-td patent-date-value">Aug 21, 2012</td><td class="patent-data-table-td ">Intellisist, Inc.</td><td class="patent-data-table-td ">Real-time display of system instructions</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8255830">US8255830</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 2009</td><td class="patent-data-table-td patent-date-value">Aug 28, 2012</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Methods and graphical user interfaces for editing on a multifunction device with a touch screen display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8264455">US8264455</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 3, 2009</td><td class="patent-data-table-td patent-date-value">Sep 11, 2012</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Mapping of physical controls for surface computing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8294669">US8294669</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 19, 2007</td><td class="patent-data-table-td patent-date-value">Oct 23, 2012</td><td class="patent-data-table-td ">Palo Alto Research Center Incorporated</td><td class="patent-data-table-td ">Link target accuracy in touch-screen mobile devices by layout adjustment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8330737">US8330737</a></td><td class="patent-data-table-td patent-date-value">Dec 27, 2010</td><td class="patent-data-table-td patent-date-value">Dec 11, 2012</td><td class="patent-data-table-td ">Kyocera Corporation</td><td class="patent-data-table-td ">Input apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8339369">US8339369</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 18, 2007</td><td class="patent-data-table-td patent-date-value">Dec 25, 2012</td><td class="patent-data-table-td ">Sony Corporation</td><td class="patent-data-table-td ">In-vehicle apparatus and control method of in-vehicle apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8352884">US8352884</a></td><td class="patent-data-table-td patent-date-value">Oct 7, 2009</td><td class="patent-data-table-td patent-date-value">Jan 8, 2013</td><td class="patent-data-table-td ">Sony Computer Entertainment Inc.</td><td class="patent-data-table-td ">Dynamic reconfiguration of GUI display decomposition based on predictive model</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8363866">US8363866</a></td><td class="patent-data-table-td patent-date-value">Jan 30, 2009</td><td class="patent-data-table-td patent-date-value">Jan 29, 2013</td><td class="patent-data-table-td ">Panasonic Automotive Systems Company Of America</td><td class="patent-data-table-td ">Audio menu navigation method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8370736">US8370736</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 2009</td><td class="patent-data-table-td patent-date-value">Feb 5, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Methods and graphical user interfaces for editing on a multifunction device with a touch screen display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8375295">US8375295</a></td><td class="patent-data-table-td patent-date-value">Oct 7, 2009</td><td class="patent-data-table-td patent-date-value">Feb 12, 2013</td><td class="patent-data-table-td ">Sony Computer Entertainment Inc.</td><td class="patent-data-table-td ">Customization of GUI layout based on history of use</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8411040">US8411040</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 14, 2007</td><td class="patent-data-table-td patent-date-value">Apr 2, 2013</td><td class="patent-data-table-td ">Volkswagen Ag</td><td class="patent-data-table-td ">Operating device of a motor vehicle and method for recording user inputs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8427445">US8427445</a></td><td class="patent-data-table-td patent-date-value">Jun 22, 2010</td><td class="patent-data-table-td patent-date-value">Apr 23, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Visual expander</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8432365">US8432365</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 29, 2008</td><td class="patent-data-table-td patent-date-value">Apr 30, 2013</td><td class="patent-data-table-td ">Lg Electronics Inc.</td><td class="patent-data-table-td ">Apparatus and method for providing feedback for three-dimensional touchscreen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8434003">US8434003</a></td><td class="patent-data-table-td patent-date-value">Oct 7, 2009</td><td class="patent-data-table-td patent-date-value">Apr 30, 2013</td><td class="patent-data-table-td ">Sony Computer Entertainment Inc.</td><td class="patent-data-table-td ">Touch control with dynamically determined buffer region and active perimeter</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8446367">US8446367</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 17, 2009</td><td class="patent-data-table-td patent-date-value">May 21, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Camera-based multi-touch mouse</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8446381">US8446381</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 15, 2008</td><td class="patent-data-table-td patent-date-value">May 21, 2013</td><td class="patent-data-table-td ">Delphi Technologies, Inc.</td><td class="patent-data-table-td ">Control panels for onboard instruments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8451245">US8451245</a></td><td class="patent-data-table-td patent-date-value">Dec 13, 2011</td><td class="patent-data-table-td patent-date-value">May 28, 2013</td><td class="patent-data-table-td ">Immersion Corporation</td><td class="patent-data-table-td ">Multi-touch device having dynamic haptic effects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8482534">US8482534</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 10, 2009</td><td class="patent-data-table-td patent-date-value">Jul 9, 2013</td><td class="patent-data-table-td ">Timothy R. Pryor</td><td class="patent-data-table-td ">Programmable tactile touch screen displays and man-machine interfaces for improved vehicle instrumentation and telematics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8483437">US8483437</a></td><td class="patent-data-table-td patent-date-value">Mar 20, 2012</td><td class="patent-data-table-td patent-date-value">Jul 9, 2013</td><td class="patent-data-table-td ">Qualcomm Incorporated</td><td class="patent-data-table-td ">Detecting and tracking objects in images</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8483907">US8483907</a></td><td class="patent-data-table-td patent-date-value">Feb 23, 2010</td><td class="patent-data-table-td patent-date-value">Jul 9, 2013</td><td class="patent-data-table-td ">Paccar Inc</td><td class="patent-data-table-td ">Customizable graphical display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8488042">US8488042</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 29, 2009</td><td class="patent-data-table-td patent-date-value">Jul 16, 2013</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">Systems for capturing images through a display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8490005">US8490005</a></td><td class="patent-data-table-td patent-date-value">Feb 23, 2010</td><td class="patent-data-table-td patent-date-value">Jul 16, 2013</td><td class="patent-data-table-td ">Paccar Inc</td><td class="patent-data-table-td ">Visual enhancement for instrument panel</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8493366">US8493366</a></td><td class="patent-data-table-td patent-date-value">Jul 13, 2011</td><td class="patent-data-table-td patent-date-value">Jul 23, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Dynamic projected user interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8502789">US8502789</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 11, 2010</td><td class="patent-data-table-td patent-date-value">Aug 6, 2013</td><td class="patent-data-table-td ">Smart Technologies Ulc</td><td class="patent-data-table-td ">Method for handling user input in an interactive input system, and interactive input system executing the method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8503861">US8503861</a></td><td class="patent-data-table-td patent-date-value">Aug 29, 2008</td><td class="patent-data-table-td patent-date-value">Aug 6, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Systems and methods for receiving, storing, and rendering digital video, music, and pictures on a personal media player</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8508710">US8508710</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 2, 2004</td><td class="patent-data-table-td patent-date-value">Aug 13, 2013</td><td class="patent-data-table-td ">Hewlett-Packard Development Company, L.P.</td><td class="patent-data-table-td ">Display panel</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8510665">US8510665</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 2009</td><td class="patent-data-table-td patent-date-value">Aug 13, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Methods and graphical user interfaces for editing on a multifunction device with a touch screen display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8519952">US8519952</a></td><td class="patent-data-table-td patent-date-value">Feb 23, 2011</td><td class="patent-data-table-td patent-date-value">Aug 27, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Input method for surface of interactive display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8531398">US8531398</a></td><td class="patent-data-table-td patent-date-value">Jan 18, 2011</td><td class="patent-data-table-td patent-date-value">Sep 10, 2013</td><td class="patent-data-table-td ">Marvell International Ltd.</td><td class="patent-data-table-td ">Method and apparatus for using a virtual pointer to select an object in a display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8537134">US8537134</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 20, 2012</td><td class="patent-data-table-td patent-date-value">Sep 17, 2013</td><td class="patent-data-table-td ">Smart Technologies Ulc</td><td class="patent-data-table-td ">Size/scale and orientation determination of a pointer in a camera-based touch system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8542216">US8542216</a></td><td class="patent-data-table-td patent-date-value">Dec 13, 2011</td><td class="patent-data-table-td patent-date-value">Sep 24, 2013</td><td class="patent-data-table-td ">Immersion Corporation</td><td class="patent-data-table-td ">Multi-touch device having dynamic haptic effects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8547345">US8547345</a></td><td class="patent-data-table-td patent-date-value">May 4, 2009</td><td class="patent-data-table-td patent-date-value">Oct 1, 2013</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Touch sensitive LCD panel</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8552980">US8552980</a></td><td class="patent-data-table-td patent-date-value">Apr 29, 2010</td><td class="patent-data-table-td patent-date-value">Oct 8, 2013</td><td class="patent-data-table-td ">Gregory A. Shaver</td><td class="patent-data-table-td ">Computer input devices and associated computing devices, software, and methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8552999">US8552999</a></td><td class="patent-data-table-td patent-date-value">Sep 28, 2010</td><td class="patent-data-table-td patent-date-value">Oct 8, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Control selection approximation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8560975">US8560975</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 6, 2012</td><td class="patent-data-table-td patent-date-value">Oct 15, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Touch event model</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8564538">US8564538</a></td><td class="patent-data-table-td patent-date-value">Oct 21, 2008</td><td class="patent-data-table-td patent-date-value">Oct 22, 2013</td><td class="patent-data-table-td ">Belkin International, Inc.</td><td class="patent-data-table-td ">Touch screen overlays and methods for manufacturing the same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8566044">US8566044</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2011</td><td class="patent-data-table-td patent-date-value">Oct 22, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Event recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8566045">US8566045</a></td><td class="patent-data-table-td patent-date-value">Mar 31, 2011</td><td class="patent-data-table-td patent-date-value">Oct 22, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Event recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8570278">US8570278</a></td><td class="patent-data-table-td patent-date-value">Oct 24, 2007</td><td class="patent-data-table-td patent-date-value">Oct 29, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Portable multifunction device, method, and graphical user interface for adjusting an insertion point marker</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8577487">US8577487</a></td><td class="patent-data-table-td patent-date-value">Feb 23, 2010</td><td class="patent-data-table-td patent-date-value">Nov 5, 2013</td><td class="patent-data-table-td ">Paccar Inc</td><td class="patent-data-table-td ">Customized instrument evaluation and ordering tool</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8584050">US8584050</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 2009</td><td class="patent-data-table-td patent-date-value">Nov 12, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Methods and graphical user interfaces for editing on a multifunction device with a touch screen display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8587422">US8587422</a></td><td class="patent-data-table-td patent-date-value">Mar 29, 2011</td><td class="patent-data-table-td patent-date-value">Nov 19, 2013</td><td class="patent-data-table-td ">Tk Holdings, Inc.</td><td class="patent-data-table-td ">Occupant sensing system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8599147">US8599147</a></td><td class="patent-data-table-td patent-date-value">May 23, 2008</td><td class="patent-data-table-td patent-date-value">Dec 3, 2013</td><td class="patent-data-table-td ">Vortant Technologies, Llc</td><td class="patent-data-table-td ">Computer interface for navigating graphical user interface by touch</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8600761">US8600761</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 9, 2008</td><td class="patent-data-table-td patent-date-value">Dec 3, 2013</td><td class="patent-data-table-td ">The Boeing Company</td><td class="patent-data-table-td ">Hands-free and non-visually occluding object information interaction system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8610671">US8610671</a></td><td class="patent-data-table-td patent-date-value">Dec 27, 2007</td><td class="patent-data-table-td patent-date-value">Dec 17, 2013</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Insertion marker placement on touch sensitive display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8610677">US8610677</a></td><td class="patent-data-table-td patent-date-value">Apr 25, 2008</td><td class="patent-data-table-td patent-date-value">Dec 17, 2013</td><td class="patent-data-table-td ">Jens Martin Jensen</td><td class="patent-data-table-td ">Touch-sensitive pointing device with guiding lines</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8639819">US8639819</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 5, 2004</td><td class="patent-data-table-td patent-date-value">Jan 28, 2014</td><td class="patent-data-table-td ">Nokia Corporation</td><td class="patent-data-table-td ">Ad-hoc connection between electronic devices</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8643481">US8643481</a></td><td class="patent-data-table-td patent-date-value">Oct 5, 2010</td><td class="patent-data-table-td patent-date-value">Feb 4, 2014</td><td class="patent-data-table-td ">Johnson Controls Technology Company</td><td class="patent-data-table-td ">Interior rearview mirror assembly with integrated indicator symbol</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8645827">US8645827</a></td><td class="patent-data-table-td patent-date-value">Mar 4, 2008</td><td class="patent-data-table-td patent-date-value">Feb 4, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Touch event model</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8650507">US8650507</a></td><td class="patent-data-table-td patent-date-value">Mar 4, 2008</td><td class="patent-data-table-td patent-date-value">Feb 11, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Selecting of text using gestures</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8661339">US8661339</a></td><td class="patent-data-table-td patent-date-value">Sep 23, 2011</td><td class="patent-data-table-td patent-date-value">Feb 25, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Devices, methods, and graphical user interfaces for document manipulation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8661362">US8661362</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 2009</td><td class="patent-data-table-td patent-date-value">Feb 25, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Methods and graphical user interfaces for editing on a multifunction device with a touch screen display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8661363">US8661363</a></td><td class="patent-data-table-td patent-date-value">Apr 22, 2013</td><td class="patent-data-table-td patent-date-value">Feb 25, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Application programming interfaces for scrolling operations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8670632">US8670632</a></td><td class="patent-data-table-td patent-date-value">Mar 15, 2012</td><td class="patent-data-table-td patent-date-value">Mar 11, 2014</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">System for reducing effects of undesired signals in an infrared imaging system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8677232">US8677232</a></td><td class="patent-data-table-td patent-date-value">Sep 23, 2011</td><td class="patent-data-table-td patent-date-value">Mar 18, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Devices, methods, and graphical user interfaces for document manipulation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8682602">US8682602</a></td><td class="patent-data-table-td patent-date-value">Sep 14, 2012</td><td class="patent-data-table-td patent-date-value">Mar 25, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Event recognition</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8686922">US8686922</a></td><td class="patent-data-table-td patent-date-value">Mar 13, 2008</td><td class="patent-data-table-td patent-date-value">Apr 1, 2014</td><td class="patent-data-table-td ">American Vehicular Sciences Llc</td><td class="patent-data-table-td ">Eye-location dependent vehicular heads-up display system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8698030">US8698030</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 4, 2012</td><td class="patent-data-table-td patent-date-value">Apr 15, 2014</td><td class="patent-data-table-td ">Alps Electric Co., Ltd.</td><td class="patent-data-table-td ">Input device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8698773">US8698773</a></td><td class="patent-data-table-td patent-date-value">Nov 8, 2013</td><td class="patent-data-table-td patent-date-value">Apr 15, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Insertion marker placement on touch sensitive display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8717305">US8717305</a></td><td class="patent-data-table-td patent-date-value">Mar 4, 2008</td><td class="patent-data-table-td patent-date-value">May 6, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Touch event model for web pages</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8719695">US8719695</a></td><td class="patent-data-table-td patent-date-value">Sep 23, 2011</td><td class="patent-data-table-td patent-date-value">May 6, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Devices, methods, and graphical user interfaces for document manipulation</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8723822">US8723822</a></td><td class="patent-data-table-td patent-date-value">Jun 17, 2011</td><td class="patent-data-table-td patent-date-value">May 13, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Touch event model programming interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8725230">US8725230</a></td><td class="patent-data-table-td patent-date-value">Apr 1, 2011</td><td class="patent-data-table-td patent-date-value">May 13, 2014</td><td class="patent-data-table-td ">Tk Holdings Inc.</td><td class="patent-data-table-td ">Steering wheel with hand sensors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8756534">US8756534</a></td><td class="patent-data-table-td patent-date-value">Sep 24, 2009</td><td class="patent-data-table-td patent-date-value">Jun 17, 2014</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Methods and graphical user interfaces for editing on a multifunction device with a touch screen display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US8758135">US8758135</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 12, 2010</td><td class="patent-data-table-td patent-date-value">Jun 24, 2014</td><td class="patent-data-table-td ">Revolutionary Concepts, Inc.</td><td class="patent-data-table-td ">Child&#39;s car seat assembly enabling access to gaming and communications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20060119798">US20060119798</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 2, 2004</td><td class="patent-data-table-td patent-date-value">Jun 8, 2006</td><td class="patent-data-table-td ">Huddleston Wyatt A</td><td class="patent-data-table-td ">Display panel</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20070273670">US20070273670</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 8, 2007</td><td class="patent-data-table-td patent-date-value">Nov 29, 2007</td><td class="patent-data-table-td ">Mats Nordahl</td><td class="patent-data-table-td ">User identification for multi-user touch screens</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20080249668">US20080249668</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 8, 2008</td><td class="patent-data-table-td patent-date-value">Oct 9, 2008</td><td class="patent-data-table-td ">C/O Kabushiki Kaisha Tokai Rika Denki Seisakusho</td><td class="patent-data-table-td ">In-vehicle equipment control device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20080276191">US20080276191</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 26, 2007</td><td class="patent-data-table-td patent-date-value">Nov 6, 2008</td><td class="patent-data-table-td ">Automotive Technologies International, Inc.</td><td class="patent-data-table-td ">Vehicular Heads-Up Display System</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090058829">US20090058829</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 29, 2008</td><td class="patent-data-table-td patent-date-value">Mar 5, 2009</td><td class="patent-data-table-td ">Young Hwan Kim</td><td class="patent-data-table-td ">Apparatus and method for providing feedback for three-dimensional touchscreen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090082951">US20090082951</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 26, 2007</td><td class="patent-data-table-td patent-date-value">Mar 26, 2009</td><td class="patent-data-table-td ">Apple Inc.</td><td class="patent-data-table-td ">Intelligent Restriction of Device Operations</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090128505">US20090128505</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 19, 2007</td><td class="patent-data-table-td patent-date-value">May 21, 2009</td><td class="patent-data-table-td ">Partridge Kurt E</td><td class="patent-data-table-td ">Link target accuracy in touch-screen mobile devices by layout adjustment</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090189373">US20090189373</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 10, 2005</td><td class="patent-data-table-td patent-date-value">Jul 30, 2009</td><td class="patent-data-table-td ">Schramm Michael R</td><td class="patent-data-table-td ">Steering Apparatus</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090231019">US20090231019</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 11, 2008</td><td class="patent-data-table-td patent-date-value">Sep 17, 2009</td><td class="patent-data-table-td ">Mobinnova Hong Kong Limited</td><td class="patent-data-table-td ">Input device for complex control signal</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090231281">US20090231281</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 11, 2008</td><td class="patent-data-table-td patent-date-value">Sep 17, 2009</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Multi-touch virtual keyboard</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090256813">US20090256813</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Mar 31, 2009</td><td class="patent-data-table-td patent-date-value">Oct 15, 2009</td><td class="patent-data-table-td ">Amici Alan J</td><td class="patent-data-table-td ">Vehicle moving-image interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20090267921">US20090267921</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 10, 2009</td><td class="patent-data-table-td patent-date-value">Oct 29, 2009</td><td class="patent-data-table-td ">Pryor Timothy R</td><td class="patent-data-table-td ">Programmable tactile touch screen displays and man-machine interfaces for improved vehicle instrumentation and telematics</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100060613">US20100060613</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 11, 2009</td><td class="patent-data-table-td patent-date-value">Mar 11, 2010</td><td class="patent-data-table-td ">Smart Technologies Ulc</td><td class="patent-data-table-td ">Size/scale orientation determination of a pointer in a camera-based touch system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100063821">US20100063821</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 9, 2008</td><td class="patent-data-table-td patent-date-value">Mar 11, 2010</td><td class="patent-data-table-td ">Marsh Joseph C</td><td class="patent-data-table-td ">Hands-Free and Non-Visually Occluding Object Information Interaction System</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100188204">US20100188204</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 30, 2008</td><td class="patent-data-table-td patent-date-value">Jul 29, 2010</td><td class="patent-data-table-td ">Reiko Okada</td><td class="patent-data-table-td ">Display device for vehicle</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100188343">US20100188343</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 29, 2009</td><td class="patent-data-table-td patent-date-value">Jul 29, 2010</td><td class="patent-data-table-td ">Edward William Bach</td><td class="patent-data-table-td ">Vehicular control system comprising touch pad and vehicles and methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100188349">US20100188349</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 15, 2008</td><td class="patent-data-table-td patent-date-value">Jul 29, 2010</td><td class="patent-data-table-td ">Yannick Molard</td><td class="patent-data-table-td ">Control panels for onboard instruments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100188548">US20100188548</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 29, 2009</td><td class="patent-data-table-td patent-date-value">Jul 29, 2010</td><td class="patent-data-table-td ">Robinson Ian N</td><td class="patent-data-table-td ">Systems for Capturing Images Through A Display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100194677">US20100194677</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 3, 2009</td><td class="patent-data-table-td patent-date-value">Aug 5, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Mapping of physical controls for surface computing</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100199228">US20100199228</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Feb 23, 2009</td><td class="patent-data-table-td patent-date-value">Aug 5, 2010</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Gesture Keyboarding</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100277576">US20100277576</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 29, 2010</td><td class="patent-data-table-td patent-date-value">Nov 4, 2010</td><td class="patent-data-table-td ">David Fattal</td><td class="patent-data-table-td ">Systems for Capturing Images Through a Display</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20100308983">US20100308983</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 5, 2009</td><td class="patent-data-table-td patent-date-value">Dec 9, 2010</td><td class="patent-data-table-td ">Conte Thomas M</td><td class="patent-data-table-td ">Touch Screen with Tactile Feedback</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110050394">US20110050394</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 27, 2009</td><td class="patent-data-table-td patent-date-value">Mar 3, 2011</td><td class="patent-data-table-td ">Symbol Technologies, Inc.</td><td class="patent-data-table-td ">Systems and methods for pressure-based authentication of an input on a touch screen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110074739">US20110074739</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 4, 2010</td><td class="patent-data-table-td patent-date-value">Mar 31, 2011</td><td class="patent-data-table-td ">Microsoft Corporation</td><td class="patent-data-table-td ">Light-transmissive key and optically-recognizable signature</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110109578">US20110109578</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 7, 2009</td><td class="patent-data-table-td patent-date-value">May 12, 2011</td><td class="patent-data-table-td ">Waeller Christoph</td><td class="patent-data-table-td ">Display and control device for a motor vehicle and method for operating the same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110157047">US20110157047</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 20, 2010</td><td class="patent-data-table-td patent-date-value">Jun 30, 2011</td><td class="patent-data-table-td ">Canon Kabushiki Kaisha</td><td class="patent-data-table-td ">Information processing apparatus and control method therefor</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110169308">US20110169308</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 12, 2010</td><td class="patent-data-table-td patent-date-value">Jul 14, 2011</td><td class="patent-data-table-td ">Ron Carter</td><td class="patent-data-table-td ">Child&#39;s car seat assembly enabling access to gaming and communications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20110169748">US20110169748</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 11, 2010</td><td class="patent-data-table-td patent-date-value">Jul 14, 2011</td><td class="patent-data-table-td ">Smart Technologies Ulc</td><td class="patent-data-table-td ">Method for handling user input in an interactive input system, and interactive input system executing the method</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120032899">US20120032899</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 25, 2010</td><td class="patent-data-table-td patent-date-value">Feb 9, 2012</td><td class="patent-data-table-td ">Volkswagen Ag</td><td class="patent-data-table-td ">Method for operating a motor vehicle having a touch screen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120044090">US20120044090</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Aug 18, 2011</td><td class="patent-data-table-td patent-date-value">Feb 23, 2012</td><td class="patent-data-table-td ">GM Global Technology Operations LLC</td><td class="patent-data-table-td ">Motor vehicle with digital projectors</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120056804">US20120056804</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 14, 2011</td><td class="patent-data-table-td patent-date-value">Mar 8, 2012</td><td class="patent-data-table-td ">Nokia Corporation</td><td class="patent-data-table-td ">Apparatus, Methods And Computer Program Products Providing Finger-Based And Hand-Based Gesture Commands For Portable Electronic Device Applications</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120261243">US20120261243</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 4, 2012</td><td class="patent-data-table-td patent-date-value">Oct 18, 2012</td><td class="patent-data-table-td ">Alps Electric Co., Ltd.</td><td class="patent-data-table-td ">Input device</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120280943">US20120280943</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 20, 2012</td><td class="patent-data-table-td patent-date-value">Nov 8, 2012</td><td class="patent-data-table-td ">Smart Technologies Ulc</td><td class="patent-data-table-td ">Size/scale and orientation determination of a pointer in a camera-based touch system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20120287050">US20120287050</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 9, 2012</td><td class="patent-data-table-td patent-date-value">Nov 15, 2012</td><td class="patent-data-table-td ">Fan Wu</td><td class="patent-data-table-td ">System and method for human interface in a vehicle</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130024071">US20130024071</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jul 22, 2011</td><td class="patent-data-table-td patent-date-value">Jan 24, 2013</td><td class="patent-data-table-td ">Clas Sivertsen</td><td class="patent-data-table-td ">Steering Wheel Input Device Having Gesture Recognition and Angle Compensation Capabilities</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130069899">US20130069899</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Nov 6, 2012</td><td class="patent-data-table-td patent-date-value">Mar 21, 2013</td><td class="patent-data-table-td ">Jason Clay Beaver</td><td class="patent-data-table-td ">Touch Event Model</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130179811">US20130179811</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 5, 2012</td><td class="patent-data-table-td patent-date-value">Jul 11, 2013</td><td class="patent-data-table-td ">Visteon Global Technologies, Inc.</td><td class="patent-data-table-td ">Projection dynamic icon knobs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/US20130346892">US20130346892</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 8, 2013</td><td class="patent-data-table-td patent-date-value">Dec 26, 2013</td><td class="patent-data-table-td ">Google Inc.</td><td class="patent-data-table-td ">Graphical user interface element expansion and contraction using a rotating gesture</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE102008025122A1?cl=en">DE102008025122A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">May 26, 2008</td><td class="patent-data-table-td patent-date-value">Dec 3, 2009</td><td class="patent-data-table-td ">Volkswagen Ag</td><td class="patent-data-table-td ">Anzeigeverfahren für ein Anzeigesystem, Anzeigesystem und Bedienverfahren für ein Navigationssystem eines Fahrzeugs</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE102008061987A1?cl=en">DE102008061987A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Dec 12, 2008</td><td class="patent-data-table-td patent-date-value">Jun 17, 2010</td><td class="patent-data-table-td ">Volkswagen Ag</td><td class="patent-data-table-td ">Bedienverfahren und Bedieneinrichtung</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/DE102009002328A1?cl=en">DE102009002328A1</a></td><td class="patent-data-table-td patent-date-value">Apr 9, 2009</td><td class="patent-data-table-td patent-date-value">Nov 26, 2009</td><td class="patent-data-table-td ">Visteon Global Technologies, Inc., Van Buren Township</td><td class="patent-data-table-td ">Rekonfigurierbare Zentralkonsole mit Berührungserfassung</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP1905635A1?cl=en">EP1905635A1</a></td><td class="patent-data-table-td patent-date-value">Aug 18, 2007</td><td class="patent-data-table-td patent-date-value">Apr 2, 2008</td><td class="patent-data-table-td ">Volkswagen Aktiengesellschaft</td><td class="patent-data-table-td ">Multi-functional operating device and method for operating a multi-function operating device for a motor vehicle with a slidable operating element</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2003421A1?cl=en">EP2003421A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jun 13, 2007</td><td class="patent-data-table-td patent-date-value">Dec 17, 2008</td><td class="patent-data-table-td ">Alpine Electronics, Inc.</td><td class="patent-data-table-td ">On-vehicle position detection system</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2203798A1?cl=en">EP2203798A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 8, 2008</td><td class="patent-data-table-td patent-date-value">Jul 7, 2010</td><td class="patent-data-table-td ">Immersion Corporation</td><td class="patent-data-table-td ">Multi-touch device having dynamic haptic effects</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2273353A1?cl=en">EP2273353A1</a></td><td class="patent-data-table-td patent-date-value">Jul 7, 2009</td><td class="patent-data-table-td patent-date-value">Jan 12, 2011</td><td class="patent-data-table-td ">Ford Global Technologies Inc.</td><td class="patent-data-table-td ">Improved human machine interface</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/EP2350788A2?cl=en">EP2350788A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 21, 2009</td><td class="patent-data-table-td patent-date-value">Aug 3, 2011</td><td class="patent-data-table-td ">Samsung Electronics Co., Ltd.</td><td class="patent-data-table-td ">Interface apparatus for generating control command by touch and motion, interface system including the interface apparatus, and interface method using the same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2008145124A2?cl=en">WO2008145124A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 25, 2008</td><td class="patent-data-table-td patent-date-value">Dec 4, 2008</td><td class="patent-data-table-td ">Martin Pointing Devices</td><td class="patent-data-table-td ">Touch-sensitive pointing device with guiding lines</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2009047437A2?cl=en">WO2009047437A2</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Sep 15, 2008</td><td class="patent-data-table-td patent-date-value">Apr 16, 2009</td><td class="patent-data-table-td ">Delphi Tech Inc</td><td class="patent-data-table-td ">Control panels for onboard instruments</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2010048338A1?cl=en">WO2010048338A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Oct 21, 2009</td><td class="patent-data-table-td patent-date-value">Apr 29, 2010</td><td class="patent-data-table-td ">Belkin International, Inc.</td><td class="patent-data-table-td ">Touch screen overlays and methods for manufacturing the same</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2010087988A1?cl=en">WO2010087988A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 29, 2010</td><td class="patent-data-table-td patent-date-value">Aug 5, 2010</td><td class="patent-data-table-td ">Panasonic Automotive Systems Company Of America</td><td class="patent-data-table-td ">Menu navigation method for user of audio headphones</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2010089036A1?cl=en">WO2010089036A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Jan 25, 2010</td><td class="patent-data-table-td patent-date-value">Aug 12, 2010</td><td class="patent-data-table-td ">Volkswagen Aktiengesellschaft</td><td class="patent-data-table-td ">Method for operating a motor vehicle having a touch screen</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2010127207A1?cl=en">WO2010127207A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 30, 2010</td><td class="patent-data-table-td patent-date-value">Nov 4, 2010</td><td class="patent-data-table-td ">Shaver Gregory A</td><td class="patent-data-table-td ">Computer input devices and associated computing devices, software, and methods</td></tr><tr><td class="patent-data-table-td citation-patent"><a href="/patents/WO2013174580A1?cl=en">WO2013174580A1</a><span class='patent-tooltip-anchor' data-tooltip-text="Cited by examiner"> *</span></td><td class="patent-data-table-td patent-date-value">Apr 17, 2013</td><td class="patent-data-table-td patent-date-value">Nov 28, 2013</td><td class="patent-data-table-td ">Volkswagen Aktiengesellschaft</td><td class="patent-data-table-td ">Device for controlling lighting in a vehicle interior</td></tr></table><div class="patent-section-footer">* Cited by examiner</div></div><div class="patent-section patent-tabular-section"><a id="classifications"></a><div class="patent-section-header"><span class="patent-section-title">Classifications</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th"> </th><th class="patent-data-table-th"> </th></tr></thead><tr><td class="patent-data-table-td ">U.S. Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=OHd1BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc345/defs345.htm&usg=AFQjCNF0b52M2HqQQp5rThx3mQ75nwjbGg#C345S173000">345/173</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OHd1BAABERAJ&q=http://www.uspto.gov/web/patents/classification/uspc345/defs345.htm&usg=AFQjCNF0b52M2HqQQp5rThx3mQ75nwjbGg#C345S156000">345/156</a></span></td></tr><tr><td class="patent-data-table-td ">International Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=OHd1BAABERAJ&q=http://web2.wipo.int/ipcpub/&usg=AFQjCNER44F5jlVoswCkvW3YEcB5lW4moA#refresh=page&notion=scheme&version=20130101&symbol=G09G0005000000">G09G5/00</a></span></td></tr><tr><td class="patent-data-table-td ">Cooperative Classification</td><td class="patent-data-table-td "><span class="nested-value"><a href="http://www.google.com/url?id=OHd1BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G01C21/3664">G01C21/3664</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OHd1BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F3/0488">G06F3/0488</a></span>, <span class="nested-value"><a href="http://www.google.com/url?id=OHd1BAABERAJ&q=http://worldwide.espacenet.com/classification&usg=AFQjCNGs5WqSrPE3A4ZP63zGuM6PRNfEFA#!/CPC=G06F3/0421">G06F3/0421</a></span></td></tr><tr><td class="patent-data-table-td ">European Classification</td><td class="patent-data-table-td "><span class="nested-value">G01C21/36K</span>, <span class="nested-value">G06F3/0488</span>, <span class="nested-value">G06F3/042B</span></td></tr></table><div class="patent-section-footer"></div></div><div class="patent-section patent-tabular-section"><a id="legal-events"></a><div class="patent-section-header"><span class="patent-section-title">Legal Events</span></div><table class="patent-data-table"><thead class="patent-data-table-thead"><tr class="patent-data-table"><th class="patent-data-table-th">Date</th><th class="patent-data-table-th">Code</th><th class="patent-data-table-th">Event</th><th class="patent-data-table-th">Description</th></tr></thead><tr><td class="patent-data-table-td patent-date-value">Jan 2, 2014</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">8</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 17, 2011</td><td class="patent-data-table-td ">B1</td><td class="patent-data-table-td ">Reexamination certificate first reexamination</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">CLAIMS 1, 5 AND 22 ARE CANCELLED. CLAIMS 2-3, 6-16, 20, 21 AND 23-28 ARE DETERMINED TO BE PATENTABLE AS AMENDED. CLAIMS 17-19, DEPENDENT ON AN AMENDED CLAIM, ARE DETERMINED TO BE PATENTABLE. NEW CLAIMS 29-46 ARE ADDED AND DETERMINED TO BE PATENTABLE. CLAIM 4 WAS NOT REEXAMINED.</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">May 3, 2010</td><td class="patent-data-table-td ">AS</td><td class="patent-data-table-td ">Assignment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">APPLE INC.,CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:PRYOR, TIMOTHY R.;US-ASSIGNMENT DATABASE UPDATED:20100504;REEL/FRAME:24320/642</span></div><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20100330</span></div><div class="nested-key-value"><span class="nested-key">Owner name: </span><span class="nested-value">APPLE INC., CALIFORNIA</span></div><div class="nested-key-value"><span class="nested-key">Free format text: </span><span class="nested-value">ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:PRYOR, TIMOTHY R.;REEL/FRAME:024320/0642</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Jan 5, 2010</td><td class="patent-data-table-td ">CC</td><td class="patent-data-table-td ">Certificate of correction</td><td class="patent-data-table-td "></td></tr><tr><td class="patent-data-table-td patent-date-value">Nov 5, 2009</td><td class="patent-data-table-td ">FPAY</td><td class="patent-data-table-td ">Fee payment</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Year of fee payment: </span><span class="nested-value">4</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Aug 25, 2009</td><td class="patent-data-table-td ">RR</td><td class="patent-data-table-td ">Request for reexamination filed</td><td class="patent-data-table-td "><div class="nested-key-value"><span class="nested-key">Effective date: </span><span class="nested-value">20090610</span></div></td></tr><tr><td class="patent-data-table-td patent-date-value">Sep 2, 2008</td><td class="patent-data-table-td ">CC</td><td class="patent-data-table-td ">Certificate of correction</td><td class="patent-data-table-td "></td></tr></table><div class="patent-section-footer"></div></div><div class="modal-dialog" id="patent-images-lightbox"><div class="patent-lightbox-controls"><div class="patent-lightbox-rotate-controls"><div class="patent-lightbox-rotation-text">Rotate</div><div class="rotate-icon rotate-ccw-icon"></div><div class="rotate-icon rotate-cw-icon"></div></div><div class="patent-lightbox-index-counter"></div><a class="patent-lightbox-fullsize-link" target="_blank">Original Image</a><div class="patent-drawings-control patent-drawings-next"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_right.png" alt="Next page"width="21" height="21" /></div><div class="patent-drawings-control patent-drawings-prev"><img class="patent-drawings-button-img"src="/googlebooks/images/kennedy/page_left.png" alt="Previous page"width="21" height="21" /></div></div><div class="modal-dialog-content"><div class="patent-lightbox-image-holder"><div class="patent-lightbox-placeholder"></div></div></div></div><script>_OC_initPatentsAtb({image_not_available_html: " Image not available"});</script></div></div></div></td></tr></table><script>(function() {var href = window.location.href;if (href.indexOf('?') !== -1) {var parameters = href.split('?')[1].split('&');for (var i = 0; i < parameters.length; i++) {var param = parameters[i].split('=');if (param[0] == 'focus') {var elem = document.getElementById(param[1]);if (elem) {elem.focus();}}}}})();</script><script>_OC_addFlags({LockSrc:"/books/javascript/lock_8a2b04e7bf975d5171d8e4c0b6365c7a.js", Host:"http://www.google.com/", IsBooksRentalEnabled:1, IsWebstoreDisplayCaseEnabled:1, IsObfuscationEnabled:1, IsBrowsingHistoryEnabled:1, IsWebReaderSvgEnabled:0, IsGeoLayerEnabled:1, IsImageModeNotesEnabled:1, IsCopyMenuItemEnabled:1, IsGiftingEnabled:0, IsWebReaderUniversalPaginatorEnabled:0, IsOfflineBubbleEnabled:1, IsReaderEnabledForPlayRequests:1, IsFutureOnSaleVolumesEnabled:1, IsOfflineRestrictedCopyEnabled:1, IsBooksUnifiedLeftNavEnabled:1, IsRestrictedCopyEnabled:1, IsZipitFolderCollectionEnabled:1, IsEndOfSampleRecommendationsEnabled:1, IsRatingsOnBookcardsEnabled:1, IsAdsDisabled:0, IsIframePageDisplayEnabled:0, IsEmbeddedMediaEnabled:1, IsImageModeAnnotationsEnabled:1, IsMyLibraryGooglePlusEnabled:1, IsImagePageProviderEnabled:0, IsBookcardListPriceSmall:0, IsInternalUser:0, IsBooksShareButtonEnabled:0, IsPreOrdersEnabled:0, IsDisabledRandomBookshelves:0, WebstoreDisplayCasePosition:3});_OC_Run({"enable_p13n":false,"add_vol_to_collection_base_url":"http://www.google.com/patents?op=add\u0026sig=ACfU3U1zW00iOHS242CPQt9Fa3AM2fqRaA\u0026id=OHd1BAABERAJ","remove_vol_from_collection_base_url":"http://www.google.com/patents?op=remove\u0026sig=ACfU3U3qdhNLyg-WYBvnnmZYDL0VkUWlbA\u0026id=OHd1BAABERAJ","logged_in":false,"p13n_save_user_settings_url":"http://www.google.com/patents?op=edit_user_settings\u0026sig=ACfU3U2jiWNq8BsJ1y7HfzoAbWuvTyd47A","is_cobrand":false,"sign_in_url":"https://www.google.com/accounts/Login?service=\u0026continue=http://www.google.com/patents%3Fhl%3Den\u0026hl=en","is_play_enabled":true}, {"volume_id":"","is_ebook":true,"volumeresult":{"has_flowing_text":false,"has_scanned_text":true,"can_download_pdf":false,"can_download_epub":false,"is_pdf_drm_enabled":false,"is_epub_drm_enabled":false,"download_pdf_url":"http://www.google.com/patents/download/Programmable_tactile_touch_screen_displa.pdf?id=OHd1BAABERAJ\u0026output=pdf\u0026sig=ACfU3U3nncZMfGiLQoOzMTNp9bu0C9xglA"},"sample_url":"http://www.google.com/patents/reader?id=OHd1BAABERAJ\u0026printsec=frontcover\u0026output=reader\u0026source=gbs_atb_hover","is_browsable":true,"is_public_domain":true}, {});</script><div id="footer_table" style="font-size:83%;text-align:center;position:relative;top:20px;height:4.5em;margin-top:2em"><div style="margin-bottom:8px"><a href=http://www.google.com/><nobr>Google&nbsp;Home</nobr></a> - <a href=//www.google.com/patents/sitemap/><nobr>Sitemap</nobr></a> - <a href=http://www.google.com/googlebooks/uspto.html><nobr>USPTO Bulk Downloads</nobr></a> - <a href=/intl/en/privacy/><nobr>Privacy Policy</nobr></a> - <a href=/intl/en/policies/terms/><nobr>Terms of Service</nobr></a> - <a href=https://support.google.com/faqs/answer/2539193?hl=en><nobr>About Google Patents</nobr></a> - <a href="http://www.google.com/tools/feedback/intl/en/error.html" onclick="try{_OC_startFeedback({productId: '72792',locale: 'en'});return false;}catch(e){}"><nobr>Send Feedback</nobr></a></div><span>Data provided by IFI CLAIMS Patent Services</span><br><span >&copy;2012 Google</span></div> <script type="text/javascript">var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));</script><script type="text/javascript">var pageTracker = _gat._getTracker("UA-27188110-1");pageTracker._setCookiePath("/patents/");pageTracker._trackPageview();</script> </body></html>